<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136667114690000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907045%26RESULT%3d1%26SIGN%3dnYBo5fL5lSnBIjZiRiq0IJkSNPE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907045&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907045&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907045&amp;v=MjUwNDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVjcvUEx6N0JkN0c0SDlqTXFJOUJZWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#55" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="1 生成式对抗网络 ">1 生成式对抗网络</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="2 词向量 ">2 词向量</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="3 模型网络结构 ">3 模型网络结构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="3.1 &lt;b&gt;模型结构&lt;/b&gt;">3.1 <b>模型结构</b></a></li>
                                                <li><a href="#80" data-title="3.2 &lt;b&gt;损失计算&lt;/b&gt;">3.2 <b>损失计算</b></a></li>
                                                <li><a href="#85" data-title="3.3 &lt;b&gt;标注排序&lt;/b&gt;">3.3 <b>标注排序</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#87" data-title="4 实验 ">4 实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="4.1 &lt;b&gt;数据集&lt;/b&gt;">4.1 <b>数据集</b></a></li>
                                                <li><a href="#91" data-title="4.2 &lt;b&gt;评估方法&lt;/b&gt;">4.2 <b>评估方法</b></a></li>
                                                <li><a href="#93" data-title="4.3 &lt;b&gt;标注结果&lt;/b&gt;">4.3 <b>标注结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="图1 GAN模型">图1 GAN模型</a></li>
                                                <li><a href="#79" data-title="图2 本文模型结构">图2 本文模型结构</a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;数据集信息&lt;/b&gt;"><b>表</b>1 <b>数据集信息</b></a></li>
                                                <li><a href="#96" data-title="图3 两个数据集上阈值影响">图3 两个数据集上阈值影响</a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同模型的性能对比&lt;/b&gt;"><b>表</b>2 <b>不同模型的性能对比</b></a></li>
                                                <li><a href="#104" data-title="图4 Corel 5K数据集上模型实际标注效果">图4 Corel 5K数据集上模型实际标注效果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="140">


                                    <a id="bibliography_1" title=" FENG S L, MANMATHA R, LAVRENKO V.Multiple Bernoulli relevance models for image and video annotation[C]// Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2004:1002-1009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiple bemouli relevance models for image and video annotation">
                                        <b>[1]</b>
                                         FENG S L, MANMATHA R, LAVRENKO V.Multiple Bernoulli relevance models for image and video annotation[C]// Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2004:1002-1009.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_2" title=" JEON J, LAVRENKO V, MANMATHA R.Automatic image annotation and retrieval using cross-media relevance models[C]// Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2003:119-126." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic image annotation and retrieval using cross-media relevance models">
                                        <b>[2]</b>
                                         JEON J, LAVRENKO V, MANMATHA R.Automatic image annotation and retrieval using cross-media relevance models[C]// Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2003:119-126.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_3" title=" MORAN S, LAVRENKO V.A sparse kernel relevance model for automatic image annotation[J].Journal of Multimedia Information Retrieval, 2014, 3 (4) :209-229." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14101500003780&amp;v=MjkzMzJvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXNRYWhVPU5qN0Jhcks4SDlITnFvOUZaT3NNQzNRNQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         MORAN S, LAVRENKO V.A sparse kernel relevance model for automatic image annotation[J].Journal of Multimedia Information Retrieval, 2014, 3 (4) :209-229.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_4" title=" MAKADIA A, PAVLOVIC V, KUMAR S.Baselines for image annotation[J].International Journal of Computer Vision, 2010, 90 (1) :88-105." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003704828&amp;v=MDMyMzA5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFZMck1JRms9Tmo3QmFyTzRIdEhQcUk5QmJPa0hZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         MAKADIA A, PAVLOVIC V, KUMAR S.Baselines for image annotation[J].International Journal of Computer Vision, 2010, 90 (1) :88-105.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_5" title=" VERMA Y, JAWAHAR C V.Image annotation using metric learning in semantic neighborhoods[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer, 2012:836-849." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image annotation using metric learning in semantic neighborhoods">
                                        <b>[5]</b>
                                         VERMA Y, JAWAHAR C V.Image annotation using metric learning in semantic neighborhoods[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer, 2012:836-849.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_6" title=" GUILLAUMIN M, MENSINK T, VERBEEK J, et al.TagProp:discriminative metric learning in nearest neighbor models for image auto-annotation[C]// Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:309-316." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tagprop: Discriminative metric learningin nearest neighbor models for image auto-annotation">
                                        <b>[6]</b>
                                         GUILLAUMIN M, MENSINK T, VERBEEK J, et al.TagProp:discriminative metric learning in nearest neighbor models for image auto-annotation[C]// Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:309-316.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_7" title=" CHANG E, GOH K, SYCHAY G, et al.CBSA:content-based soft annotation for multimodal image retrieval using Bayes point machines [J].IEEE Transactions on Circuits and Systems for Video Technology, 2003, 13 (1) :26-38." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CBSA: content-based soft annotation for multimodal image retrieval using Bayes point machines">
                                        <b>[7]</b>
                                         CHANG E, GOH K, SYCHAY G, et al.CBSA:content-based soft annotation for multimodal image retrieval using Bayes point machines [J].IEEE Transactions on Circuits and Systems for Video Technology, 2003, 13 (1) :26-38.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_8" title=" GRANGIER D, BENGIO S.A discriminative kernel-based approach to rank images from text queries[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (8) :1371-1384." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A discriminative kernel-based approach to rank images from text queries">
                                        <b>[8]</b>
                                         GRANGIER D, BENGIO S.A discriminative kernel-based approach to rank images from text queries[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (8) :1371-1384.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_9" title=" YANG C, DONG M, HUA J.Region-based image annotation using asymmetrical support vector machine-based multiple-instance learning[C]// Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2006:2057-2063." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Region-based image annotation using asymmetrical support vector machine-based multipleinstance learning">
                                        <b>[9]</b>
                                         YANG C, DONG M, HUA J.Region-based image annotation using asymmetrical support vector machine-based multiple-instance learning[C]// Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2006:2057-2063.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_10" title=" 黎健成, 袁春, 宋友.基于卷积神经网络的多标签图像自动标注[J].计算机科学, 2016, 43 (7) :41-45. (LI J C, YUAN C, SONG Y.Multi-label image annotation based on convolutional neural network[J].Computer Science, 2016, 43 (7) :41-45.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201607009&amp;v=MTk5NjJDVVI3cWZadVpzRnkvZ1Y3L1BMejdCYjdHNEg5Zk1xSTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         黎健成, 袁春, 宋友.基于卷积神经网络的多标签图像自动标注[J].计算机科学, 2016, 43 (7) :41-45. (LI J C, YUAN C, SONG Y.Multi-label image annotation based on convolutional neural network[J].Computer Science, 2016, 43 (7) :41-45.) 
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_11" title=" 高耀东, 侯凌燕, 杨大利.基于多标签学习的卷积神经网络的图像标注方法[J].计算机应用, 2017, 37 (1) :228-232. (GAO Y D, HOU L Y, YANG D L.Automatic image annotation method using multi-label learning convolutional neural network[J].Journal of Computer Applications, 2017, 37 (1) :228-232.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201701041&amp;v=MTU5MTdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dWNy9QTHo3QmQ3RzRIOWJNcm85Qlo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         高耀东, 侯凌燕, 杨大利.基于多标签学习的卷积神经网络的图像标注方法[J].计算机应用, 2017, 37 (1) :228-232. (GAO Y D, HOU L Y, YANG D L.Automatic image annotation method using multi-label learning convolutional neural network[J].Journal of Computer Applications, 2017, 37 (1) :228-232.) 
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_12" title=" 汪鹏, 张奥帆, 王利琴, 等.基于迁移学习与多标签平滑策略的图像自动标注[J].计算机应用, 2018, 38 (11) :3199-3203. (WANG P, ZHANG A F, WANG L Q, et al.Image automatic annotation based on transfer learning and multi-label smoothing strategy[J].Journal of Computer Applications, 2018, 38 (11) :3199-3203.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811027&amp;v=MjY4MDRSN3FmWnVac0Z5L2dWNy9QTHo3QmQ3RzRIOW5Ocm85SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         汪鹏, 张奥帆, 王利琴, 等.基于迁移学习与多标签平滑策略的图像自动标注[J].计算机应用, 2018, 38 (11) :3199-3203. (WANG P, ZHANG A F, WANG L Q, et al.Image automatic annotation based on transfer learning and multi-label smoothing strategy[J].Journal of Computer Applications, 2018, 38 (11) :3199-3203.) 
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_13" title=" 李志欣, 郑永哲, 张灿龙, 等.结合深度特征与多标记分类的图像语义标注[J].计算机辅助设计与图形学学报, 2018, 30 (2) :318-326. (LI Z X, ZHENG Y Z, ZHANG C L, et al.Combining deep feature and multi-label classification for semantic image annotation[J].Journal of Computer-Aided Design and Computer Graphics, 2018, 30 (2) :318-326.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201802013&amp;v=Mjg2NTJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dWNy9QTHo3QmFMRzRIOW5Nclk5RVo0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         李志欣, 郑永哲, 张灿龙, 等.结合深度特征与多标记分类的图像语义标注[J].计算机辅助设计与图形学学报, 2018, 30 (2) :318-326. (LI Z X, ZHENG Y Z, ZHANG C L, et al.Combining deep feature and multi-label classification for semantic image annotation[J].Journal of Computer-Aided Design and Computer Graphics, 2018, 30 (2) :318-326.) 
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_14" title=" GOODFELLOW I, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[C]// Proceedings of the 2014 Conference on Advances in Neural Information Processing Systems 27.Montreal:Curran Associates, 2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Networks">
                                        <b>[14]</b>
                                         GOODFELLOW I, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[C]// Proceedings of the 2014 Conference on Advances in Neural Information Processing Systems 27.Montreal:Curran Associates, 2014:2672-2680.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_15" title=" 王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332. (WANG K F, GOU C, DUAN Y J, et al.Generative adversarial networks:the state of the art and beyond[J].Acta Automatica Sinica, 2017, 43 (3) :321-332.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MDc0OTZWNy9QS0NMZlliRzRIOWJNckk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2c=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332. (WANG K F, GOU C, DUAN Y J, et al.Generative adversarial networks:the state of the art and beyond[J].Acta Automatica Sinica, 2017, 43 (3) :321-332.) 
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_16" title=" MIRZA M, OSINDERO S.Conditional generative adversarial nets[J].ArXiv Preprint, 2014, 2014:1411.1784." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">
                                        <b>[16]</b>
                                         MIRZA M, OSINDERO S.Conditional generative adversarial nets[J].ArXiv Preprint, 2014, 2014:1411.1784.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_17" title=" ARJOVSKY M, CHINTALA S, BOTTOU L.Wasserstein GAN[J].ArXiv Preprint, 2017, 2017:1701.07875." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wasserstein GAN">
                                        <b>[17]</b>
                                         ARJOVSKY M, CHINTALA S, BOTTOU L.Wasserstein GAN[J].ArXiv Preprint, 2017, 2017:1701.07875.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_18" title=" GULRAJANI I, AHMED F, ARJOVSKY M, et al.Improved training of Wasserstein GANs[C]// Proceedings of the 30th Advances in Neural Information Processing Systems.Long Beach, CA:NIPS, 2017:5769-5779." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved training of wasserstein gans">
                                        <b>[18]</b>
                                         GULRAJANI I, AHMED F, ARJOVSKY M, et al.Improved training of Wasserstein GANs[C]// Proceedings of the 30th Advances in Neural Information Processing Systems.Long Beach, CA:NIPS, 2017:5769-5779.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_19" title=" SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, inception-ResNet and the impact of residual connections on learning[C]// Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:4278-4284." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the impact of residual connections on learning">
                                        <b>[19]</b>
                                         SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, inception-ResNet and the impact of residual connections on learning[C]// Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:4278-4284.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_20" title=" FU H, ZHANG Q, QIU G.Random forest for image annotation[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer, 2012:86-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Random forest for image annotation">
                                        <b>[20]</b>
                                         FU H, ZHANG Q, QIU G.Random forest for image annotation[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer, 2012:86-99.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_21" title=" VERMA Y, JAWAHAR C.Exploring SVM for image annotation in presence of confusing labels[C]// Proceedings of the 24th British Machine Vision Conference.Durham:BMVA Press, 2013:1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploring SVM for image annotation in presence of confusing labels">
                                        <b>[21]</b>
                                         VERMA Y, JAWAHAR C.Exploring SVM for image annotation in presence of confusing labels[C]// Proceedings of the 24th British Machine Vision Conference.Durham:BMVA Press, 2013:1-11.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_22" title=" KASHANI M M, AMIRI S H.Leveraging deep learning representation for search-based image annotation[C]// Proceedings of 2017 Artificial Intelligence and Signal Processing Conference.Piscataway, NJ:IEEE, 2017:156-161." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Leveraging deep learning representation for search-based image annotation">
                                        <b>[22]</b>
                                         KASHANI M M, AMIRI S H.Leveraging deep learning representation for search-based image annotation[C]// Proceedings of 2017 Artificial Intelligence and Signal Processing Conference.Piscataway, NJ:IEEE, 2017:156-161.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_23" title=" MURTHY V N, MAJI S, MANMATHA R.Automatic image annotation using deep learning representations[C]// Proceedings of the 5th ACM on International Conference on Multimedia Retrieval.New York:ACM, 2015:603-606." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic image annotation using deep learning representations">
                                        <b>[23]</b>
                                         MURTHY V N, MAJI S, MANMATHA R.Automatic image annotation using deep learning representations[C]// Proceedings of the 5th ACM on International Conference on Multimedia Retrieval.New York:ACM, 2015:603-606.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_24" title=" 周铭柯, 柯逍, 杜明智.基于数据均衡的增进式深度自动图像标注[J].软件学报, 2017, 28 (7) :1862-1880. (ZHOU M K, KE X, DU M Z.Enhanced deep automatic image annotation based on data equalization[J].Journal of Software, 2017, 28 (7) :1862-1880.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201707015&amp;v=MDE1MDF5ZlRiTEc0SDliTXFJOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVjcvUE4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         周铭柯, 柯逍, 杜明智.基于数据均衡的增进式深度自动图像标注[J].软件学报, 2017, 28 (7) :1862-1880. (ZHOU M K, KE X, DU M Z.Enhanced deep automatic image annotation based on data equalization[J].Journal of Software, 2017, 28 (7) :1862-1880.) 
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_25" title=" 柯逍, 周铭柯, 牛玉贞.融合深度特征和语义邻域的自动图像标注[J].模式识别与人工智能, 2017, 30 (3) :193-203. (KE X, ZHOU M K, NIU Y Z.Automatic image annotation combining semantic neighbors and deep features[J].Pattern Recognition and Artificial Intelligence, 2017, 30 (3) :193-203.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201703001&amp;v=MDMzNTBac0Z5L2dWNy9QS0Q3WWJMRzRIOWJNckk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         柯逍, 周铭柯, 牛玉贞.融合深度特征和语义邻域的自动图像标注[J].模式识别与人工智能, 2017, 30 (3) :193-203. (KE X, ZHOU M K, NIU Y Z.Automatic image annotation combining semantic neighbors and deep features[J].Pattern Recognition and Artificial Intelligence, 2017, 30 (3) :193-203.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-02-27 12:37</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),2129-2133 DOI:10.11772/j.issn.1001-9081.2018112400            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于生成式对抗网络的图像自动标注</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8E%E7%95%99%E6%88%90&amp;code=41685056&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">税留成</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%8D%AB%E5%BF%A0&amp;code=07597773&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘卫忠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E5%8D%93%E6%98%8E&amp;code=07585250&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯卓明</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%AD%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%85%89%E5%AD%A6%E4%B8%8E%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0045381&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华中科技大学光学与电子信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对基于深度学习的图像标注模型输出层神经元数目与标注词汇量成正比, 导致模型结构因词汇量的变化而改变的问题, 提出了结合生成式对抗网络 (GAN) 和Word2vec的新标注模型。首先, 通过Word2vec将标注词汇映射为固定的多维词向量;其次, 利用GAN构建神经网络模型——GAN-W模型, 使输出层神经元数目与多维词向量维数相等, 与词汇量不再相关;最后, 通过对模型多次输出结果的排序来确定最终标注。GAN-W模型分别在Corel 5K和IAPRTC-12图像标注数据集上进行实验, 在Corel 5K数据集上, GAN-W模型准确率、召回率和F1值比卷积神经网络回归 (CNN-R) 方法分别提高5、14和9个百分点;在IAPRTC-12数据集上, GAN-W模型准确率、召回率和F1值比两场<i>K</i>最邻近 (2P<i>K</i>NN) 模型分别提高2、6和3个百分点。实验结果表明, GAN-W模型可以解决输出神经元数目随词汇量改变的问题, 同时每幅图像标注的标签数目自适应, 使得该模型标注结果更加符合实际标注情形。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%87%AA%E5%8A%A8%E6%A0%87%E6%B3%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像自动标注;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成式对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E6%B3%A8%E5%90%91%E9%87%8F%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标注向量化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *税留成 (1992—) , 男, 四川成都人, 硕士研究生, 主要研究方向:计算机视觉、图像标注;电子邮箱597799047@qq.com;
                                </span>
                                <span>
                                    刘卫忠 (1972—) , 男, 湖北荆州人, 副教授, 博士, 主要研究方向:多媒体信源编码、机器学习;;
                                </span>
                                <span>
                                    冯卓明 (1970—) , 男, 湖北荆州人, 讲师, 博士, 主要研究方向:无线通信。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-05</p>

            </div>
                    <h1><b>Automatic image annotation based on generative adversarial network</b></h1>
                    <h2>
                    <span>SHUI Liucheng</span>
                    <span>LIU Weizhong</span>
                    <span>FENG Zhuoming</span>
            </h2>
                    <h2>
                    <span>School of Optical and Electronic Information, Huazhong University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem that the number of output neurons in deep learning-based image annotation model is directly proportionate to the labeled vocabulary, which leads the change of model structure caused by the change of vocabulary, a new annotation model combining Generative Adversarial Network (GAN) and Word2 vec was proposed. Firstly, the labeled vocabulary was mapped to the fixed multidimensional word vector through Word2 vec. Secondly, a neural network model called GAN-W (GAN-Word2 vec annotation) was established based on GAN, making the number of neurons in model output layer equal to the dimension of multidimensional word vector and no longer relevant to the vocabulary. Finally, the annotation result was determined by sorting the multiple outputs of model. Experiments were conducted on the image annotation datasets Corel 5 K and IAPRTC-12. The experimental results show that on Corel 5 K dataset, the accuracy, recall and F1 value of the proposed model are increased by 5, 14 and 9 percentage points respectively compared with those of Convolutional Neural Network Regression (CNN-R) ; on IAPRTC-12 dataset, the accuracy, recall and F1 value of the proposed model are 2, 6 and 3 percentage points higher than those of Two-Pass <i>K</i>-Nearest Neighbor (2 P<i>K</i>NN) . The experimental results show that GAN-W model can solve the problem of neuron number change in output layer with vocabulary. Meanwhile, the number of labels in each image is self-adaptive, making the annotation results of the proposed model more suitable for actual annotation situation.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=automatic%20image%20annotation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">automatic image annotation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Network%20(GAN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Network (GAN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=label%20vectorization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">label vectorization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=transfer%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">transfer learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SHUI Liucheng, born in 1992, M. S. candidate. His research interests include computer vision, image annotation. ;
                                </span>
                                <span>
                                    LIU Weizhong, born in 1972, Ph. D. , associate professor. His research interests include multimedia source coding, machine learning.;
                                </span>
                                <span>
                                    FENG Zhuoming, born in 1970, Ph. D. , lecturer. His research interest includes wireless communication.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-05</p>
                            </div>


        <!--brief start-->
                        <h3 id="55" name="55" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="56">随着图像数据的快速增长, 通过人工对图像进行标注已经变得不可取, 迫切需要对图像内容进行自动标注, 以实现对图像的有效管理与检索, 更加高效利用庞大的图像信息。目前, 主要的标注方法是通过机器学习构建一个图像标注模型, 通过学习图像与其对应标注之间的潜在联系, 给未知图像添加描述其内容的关键词, 实现对未知图像的标注。</p>
                </div>
                <div class="p1">
                    <p id="57">基于机器学习的图像标注模型大致分为3类:生成模型、最邻近模型及判别模型。生成模型首先提取图像特征, 然后计算图像特征与图像标签之间的联合概率, 最后根据测试图像的特征计算各标签的概率, 确定图像对应的标签;代表方法有:多贝努利相关模型 (Multiple Bernoulli Relevance Model, MBRM) <citation id="190" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、跨媒体相关模型 (Cross Media Relevance Model, CMRM) <citation id="191" type="reference"><link href="142" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>及SKL-CRM (Sparse Kernel Learning Continuous Relevance Model) <citation id="192" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。最邻近模型首先根据某些基于图像特征的距离找到多幅与预测图像相似的图像, 然后根据这些相似图像的标注确定预测图像的标注;代表方法有:JEC (Joint Equal Contribution) 模型<citation id="193" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、2P<i>K</i>NN (Two-Pass <i>K</i>-Nearest Neighbor) 模型<citation id="194" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>及TagProp_ML (Tag Propagation Metric Learning) 模型<citation id="195" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="58">判别模型是将图像标签视作图像的一个分类, 因此图像标注可以看成是对图像的多分类, 通过图像的分类结果确定图像的标签;代表方法有:CBSA (Content-Based Soft Annotation) 模型<citation id="196" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、PAMIR (Passive-Aggressive Model for Image Retrieval) <citation id="197" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、ASVM-MIL (Asymmetrical Support Vector Machine-based Multiple Instance Learning algorithm) 模型<citation id="198" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。近几年, 随着深度学习在图像分类上取得良好效果, 深度学习的方法也逐渐应用于图像标注任务中。例如2016年黎健成等<citation id="199" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在CNN (Convolutional Neural Network) 模型基础上增加基于Softmax层的多标签排名损失函数, 提出Multi-label CNN标注模型;2017年高耀东等<citation id="200" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出基于均方误差损失的CNN-MSE (CNN-Mean Squared Error) 模型;2018年汪鹏等<citation id="201" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出基于多标签平滑单元的CNN-MLSU (CNN-Multi-Label Smoothing Unit) 模型;李志欣等<citation id="202" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出结合深度卷积神经网络和集成分类器链的CNN-ECC (CNN-Ensemble of Classifier Chains) 模型。这些模型在图像标注任务上均取得了良好的效果, 性能较传统的标注方法有明显的提高。</p>
                </div>
                <div class="p1">
                    <p id="59">然而, 这些深度学习标注模型有一个共同的特点, 即模型输出层神经元 (或分类器) 数目与标注词汇量成正比。这将导致2个问题:1) 随着数据集标注词汇量的增加, 输出层神经元数目会成比例地增加。当数据集词汇量较小时, 对模型几乎没有影响, 但是如果选择较大词汇量的数据集时, 模型输出层神经元数目将将变得非常庞大, 如选择Open Images数据集神经元数目将超过2万。庞大的输出层神经元数目将导致很难设计出一个合理的神经网络结构, 并且会导致模型参数量的骤增, 增加模型训练难度的同时使得模型权重文件的大小骤增, 不利于模型的实际应用。2) 当标注的词汇量发生变化时, 即使只是增删某个词汇, 由于模型输出神经元数目与词汇量成正比, 所以也需要对模型网络结构进行修改。在实际应用中新增词汇几乎是不可避免的, 这将使得模型结构将会被频繁修改, 导致模型稳定性较差。</p>
                </div>
                <div class="p1">
                    <p id="60">针对此问题, 本文将生成式对抗网络 (Generative Adversarial Net, GAN) <citation id="203" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和自然语言处理中的Word2vec模型相结合, 构建一种新的图像标注模型——GAN-W (GAN-Word2vec annotation) 模型。模型的主要步骤是:首先, 利用Word2vec将标签转换为一个固定维数的多维空间向量, 多维空间向量的维数自由选择, 模型输出层神经元数目将只与多维向量的维数相关, 不再与标注词汇量相关。另外, 当词汇量发生较小变化时, 只需要修改Word2vec的词向量转换表即可, 不再需要修改模型结构。其次, 标注模型不再一次性输出图像对应所有标注, 而是利用GAN网络每次输出一个候选标注对应的多维空间向量。通过GAN网络中随机噪声的扰动, 使得GAN网络每次可以输出与图像相关并且不同的候选标注对应的多维空间向量。最终根据模型多次输出结果筛选出图像的最终标注。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag">1 生成式对抗网络</h3>
                <div class="p1">
                    <p id="62">生成式对抗网络 (GAN) 的核心思想源于博弈论的纳什均衡<citation id="204" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 其模型如图1所示, 主要由一个生成器 (<i>G</i>) 和一个判别器 (<i>D</i>) 构成, 生成器通过随机噪声生成接近数据集分布的假数据, 判别器则需要辨别输入其中的数据是来源于生成器还是数据集。</p>
                </div>
                <div class="p1">
                    <p id="63">GAN的目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext>E</mtext><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">z</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">z</mi></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">GAN网络训练时需要交替优化生成器与判别器, 优化生成器时, 最小化目标函数<i>V</i> (<i>D</i>, <i>G</i>) , 使生成的数据<i>G</i> (<b><i>z</i></b>) 愈加接近数据集, 经过判别器后的输出<i>D</i> (<i>G</i> (<b><i>z</i></b>) ) 越来越接近于1, 即判别器无法辨别生成数据<i>G</i> (<b><i>z</i></b>) 和真实数据<b><i>x</i></b>;优化判别器时, 最大化<i>V</i> (<i>D</i>, <i>G</i>) , 使得<i>D</i> (<i>G</i> (<b><i>z</i></b>) ) 接近于0, 同时<i>D</i> (<b><i>x</i></b>) 接近于1, 即让判别器尽可能准确判断输入数据是来自于数据集的真实数据<b><i>x</i></b>还是来自于生成器生成的数据<i>G</i> (<b><i>z</i></b>) 。通过多次交替优化生成器和判别器, 分别提升其性能, 最终生成器与判别器性能达到纳什均衡, 使得生成器生成的数据分布近似于原数据集的分布。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907045_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 GAN模型" src="Detail/GetImg?filename=images/JSJY201907045_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 GAN模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907045_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 GAN model</p>

                </div>
                <div class="p1">
                    <p id="67">随机噪声<b><i>z</i></b>使得生成结果具有不确定性, 给GAN的生成结果带来了多样性, 与此同时, 由于缺乏约束常导致生成结果不可控。为解决这个问题, Mirza等<citation id="205" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出条件生成对抗网络 (Conditional Generative Adversarial Net, CGAN) , 在生成器输入噪声<b><i>z</i></b>的同时输入一个条件<b><i>c</i></b>, 并且将真实数据<b><i>x</i></b>和条件<b><i>c</i></b>作为判别器的输入, 利用条件<b><i>c</i></b>对GAN的生成结果进行限制。CGAN的目标函数<i>V</i> (<i>D</i>, <i>G</i>) , 如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">c</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext>E</mtext><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">z</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">z</mi></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo>, </mo><mi mathvariant="bold-italic">c</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">原始GAN具有训练不稳定、模式崩溃等问题, 对此Arjovsky等<citation id="206" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出Wasserstein-GAN (WGAN) 对GAN进行改进, 去掉判别器 (<i>D</i>) 最后sigmoid层, 损失函数不取log, 并且对更新后的权重强制截取到一定范围。WGAN减小了GAN网络的训练难度, 但是WGAN强制截取权重容易导致模型梯度消失或者梯度爆炸。对此, Gulrajani等<citation id="207" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出Improved WGAN对WGAN进一步改进, 使用梯度惩罚代替强制截取梯度。Improved WGAN网络的目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mtext> </mtext><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>-</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">z</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">z</mi></msub></mrow></msub><mo stretchy="false">[</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mtext>E</mtext><msub><mrow></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover></msub></mrow></msub><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mo>∇</mo><msub><mrow></mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover></msub><mi>D</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中, <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>E</mtext><msub><mrow></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover></msub></mrow></msub><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mo>∇</mo><msub><mrow></mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover></msub><mi>D</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo></mrow></math></mathml>为训练判别器 (<i>D</i>) 时梯度惩罚带来的损失, <i>λ</i>为梯度惩罚损失的系数, 一般取值为10。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">2 词向量</h3>
                <div class="p1">
                    <p id="74">由于神经网络无法直接处理文本数据, 所以需要对文本数据进行数值转换。传统的方法是将文本数据转换成<i>one</i>-<i>hot</i>词向量, 即词向量维数与词汇量相等, 所有单词均分别与向量某一维对应, 并且如果单词存在, 则对应维度取值为1, 否则只能为0, 如在5维的词向量中<i>cat</i>可能表示为[0 0 0 1 0 0], <i>dog</i>为[0 1 0 0 0 0]。<i>one</i>-<i>hot</i>表示方法是一种高维稀疏的方法, 词向量维度与词汇量成正比, 计算效率低而且每一维度互相正交, 无法体现词之间的语义关系。</p>
                </div>
                <div class="p1">
                    <p id="75">2013年<i>Google</i>开源一款新词向量生成工具<i>Word</i>2<i>vec</i>可以将词汇映射成为多维空间向量, 如<i>cat</i>可能表示为[0.1, 0.25, 0.3, 0.01, 0.9, 0.6], 目前<i>Word</i>2<i>vec</i>被大量应用于自然语言处理 (<i>Natural Language Processing</i>, <i>NLP</i>) 任务当中。<i>Word</i>2<i>vec</i>的主要思想是具有相同或相似上下文的词汇, 可能具有相似的语义, 通过学习文本语料, 根据词汇上下文, 将文本中的每个词汇映射到一个统一N维词汇空间, 并使语义上相近的词汇在该空间中的位置相近, 如<i>cat</i>和<i>kitten</i>对应词向量之间的空间距离小于<i>cat</i>和<i>iPhone</i>之间的距离, 从而体现词汇之间的关系, 从而避免<i>one</i>-<i>hot</i>词向量的缺点。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">3 模型网络结构</h3>
                <h4 class="anchor-tag" id="77" name="77">3.1 <b>模型结构</b></h4>
                <div class="p1">
                    <p id="78">本文采用的模型结构如图2所示。模型整体框架采用<i>CGAN</i>网络架构, 输入图像大小统一为 (299, 299, 3) , 图像对应的<i>N</i>维特征向量作为条件, 真实标注对应的<i>M</i>维词向量作为真实数据, 根据条件和100维随机噪声, 生成器输出<i>M</i>维向量作为生成数据。其中CNN特征提取模型选择Inception-ResNetV2<citation id="208" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>模型, 并在ImageNet数据集上进行预训练, 去除最后分类器层后采用迁移学习的方法应用到模型中;Word2vec功能采用genism库的Word2vec模块实现, 生成的词向量维数统一为500维, 生成器和判别器均采用全连接层, 将特征向量和随机噪声/词向量分别全连接映射到不同维数后拼接, 重复操作2次后映射到输出全连接层, 输出全连接层神经元数目与词向量维数相等。本文训练GAN采用Improved WGAN模型, 所以判别器输出层去除sigmoid激活层。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907045_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文模型结构" src="Detail/GetImg?filename=images/JSJY201907045_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907045_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Structure of the proposed model</p>

                </div>
                <h4 class="anchor-tag" id="80" name="80">3.2 <b>损失计算</b></h4>
                <div class="p1">
                    <p id="81">在图像标注领域, 标注词汇的分布不均匀是一个常见的问题, 有些标注如<i>cafe</i>、<i>butterfly</i>在<i>Corel</i> 5<i>K</i>数据集中只出现过2次, 而<i>water</i>、<i>sky</i>、<i>tree</i>等标注出现次数多于800次。由于标注中不同词汇的词频差异巨大, 如果不进行处理, 模型容易忽略低频标签的影响, 导致对低频词汇标注的准确率下降, 影响模型性能。针对标注分布不均衡问题, 本模型对损失函数进行优化, 对不同标注的损失乘以一个平衡系数, 使得词频低的标注具有更大权重的损失, 另外使用<i>L</i>2正则化减小模型过拟合。修改后的损失为:</p>
                </div>
                <div class="p1">
                    <p id="82"><i>Loss</i>_<i>G</i>=-E<sub><b><i>z</i></b>～<i>P</i><sub><b><i>z</i></b></sub></sub>[<i>D</i> (<i>G</i> (<b><i>z</i></b>) ) *<i>α</i>]+<i>βL</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>_</mo><mi>D</mi><mo>=</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">z</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">z</mi></msub></mrow></msub><mo stretchy="false">[</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>*</mo><mi>α</mi><mo stretchy="false">]</mo><mo>-</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>*</mo><mi>α</mi><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mtext>E</mtext><msub><mrow></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover></msub></mrow></msub><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mo>∇</mo><msub><mrow></mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover></msub><mi>D</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <i>α</i>=10/<i>k</i>, <i>k</i>为标签对应的图像数目, 10倍是防止loss过小, <i>L</i>为生成标签向量与真实标签向量的均方误差 (L2正则化损失) , <i>β</i>为<i>L</i>的系数。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">3.3 <b>标注排序</b></h4>
                <div class="p1">
                    <p id="86">由于本文模型每次输出一个图像对应的候选标注词向量, 所以本文的标注排序方法采用出现次数排序, 具体过程为:1) 通过已训练模型对图像进行<i>N</i>次预测, 获得<i>N</i>个词向量;2) 对于每个词向量, 通过Word2vec模型获取与其对应最接近的<i>M</i>个候选标注词及每个标注词对应的概率;3) 以标注词对应的概率作为标注词对应的出现次数, 统计所有候选标注词出现次数, 通过阈值筛选出现次数大于阈值的候选标注作为该图像最终标注。</p>
                </div>
                <h3 id="87" name="87" class="anchor-tag">4 实验</h3>
                <h4 class="anchor-tag" id="88" name="88">4.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="89">本文实验的数据集为图像标注领域常用数据集:<i>Corel</i> 5<i>K</i>和<i>IAPRTC</i>- 12数据集。<i>Corel</i> 5<i>K</i>数据集是由科雷尔 (<i>Corel</i>) 公司收集整理的5 000张图片, 该数据集常用于图像分类、检索等科学图像实验, 是图像实验的标准数据集。<i>IAPRTC</i>- 12数据集最初用于跨语言检索任务, 每张图像有英语、德语及西班牙语三种语言的图像描述, 在研究人员用自然语言处理技术提取图形描述中的常用名词作为图像标签后, 也被作为图像标注任务的常用数据集。<i>Corel</i> 5<i>K</i>和<i>IAPRTC</i>- 12数据集的详细信息统计如表1。</p>
                </div>
                <div class="area_img" id="90">
                    <p class="img_tit"><b>表</b>1 <b>数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Datasets information</i></p>
                    <p class="img_note"></p>
                    <table id="90" border="1"><tr><td><br />数据集</td><td>图片数</td><td>标签数</td><td>测试集</td><td>训练集</td><td>平均标签数</td></tr><tr><td><br /><i>Corel</i> 5<i>K</i></td><td>5 000</td><td>260</td><td>500</td><td>4 500</td><td>3.4</td></tr><tr><td><br /><i>IAPRTC</i>- 12</td><td>19 627</td><td>291</td><td>1 962</td><td>17 665</td><td>5.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">4.2 <b>评估方法</b></h4>
                <div class="p1">
                    <p id="92">实验采用的评价方法是计算数据集中每个标签的准确率 (Precision, <i>P</i>) 和召回率 (Recall, <i>R</i>) 及<i>F</i>1值。假设一个标签在测试集中相关图像为<i>N</i>, 测试时模型预测出的相关图像为<i>N</i>1, 其中预测正确的相关图像数量为<i>N</i>2, 那么, 准确率<i>P</i>=<i>N</i>2/<i>N</i>1, 召回率<i>R</i>=<i>N</i>2/<i>N</i>及<i>F</i>1=2*<i>P</i>*<i>R</i>/ (<i>P</i>+<i>R</i>) 。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">4.3 <b>标注结果</b></h4>
                <h4 class="anchor-tag" id="94" name="94">4.3.1 不同阈值对图像标注的影响</h4>
                <div class="p1">
                    <p id="95">不同标注阈值对本文模型的最终标注性能有巨大影响, 为了进一步探究不同阈值与标注性能的关系, 本文对不同阈值下的模型的标注性能进行测试。图3为模型标注的准确率、召回率、F1值与阈值的关系。测试时, 模型预测次数为128, 每次选出最接近输出向量的5个候选标注, 统计所有候选标注, 选出出现次数大于阈值的标注作为图像最终标注。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907045_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 两个数据集上阈值影响" src="Detail/GetImg?filename=images/JSJY201907045_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 两个数据集上阈值影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907045_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Threshold effect on two datasets</i></p>

                </div>
                <div class="p1">
                    <p id="97">从图3可以看出:标注的准确率<i>P</i>随阈值先上升后下降, 召回率<i>R</i>随阈值上升而下降, <i>F</i>1值基本上随阈值略微上涨后下降。出现这种现象的原因为:模型可以学到图像特征与标签向量之间的映射关系, 通过对模型的训练, 模型有了一定的标注能力, 对于大多数标签的预测结果中, 正确的预测对应的出现次数一般较高。当阈值特别小时, 标签对应的出现一般次数大于阈值, 标签的预测结果基本没有被阈值过滤, 标注准确率<i>P</i>和召回率<i>R</i>都不变;阈值增加到一定值时, 部分错误的预测被逐渐过滤, 正确的预测因为出现次数较大, 基本不受影响, 准确率<i>P</i>上升, 召回率<i>R</i>基本不变。阈值继续增加, 正确的预测也开始被过滤, 但是由于正确的预测情形多集中于出现次数较高的情形, 因此阈值的增加对正确的预测影响更大, 正确预测的部分被过滤的速度大于错误预测的部分, 最终使得标注准确率<i>P</i>和召回率<i>R</i>都减小, 直到正确的预测被阈值完全过滤掉, 标注准确率<i>P</i>和召回率<i>R</i>都为0。<i>F</i>1值的变化由准确率<i>P</i>和召回率<i>R</i>的变化共同确定。模型性能随阈值变化, 为了和其他模型标注性能进行对比及模型实际标注效果展示, 需要确定模型的最佳阈值。由于<i>F</i>1值能兼顾准确率<i>P</i>和召回率<i>R</i>, 所以<i>F</i>1值作为模型最佳阈值选取的参考, 选取<i>F</i>1值最大时的阈值作为模型最佳阈值。由于不同数据集之间存在差异导致对于不同数据集模型的最佳阈值也不相同, 所以对于Corel 5K和IAPRTC- 12数据集, 在模型预测次数为128的情况下, 模型分别选择75和50作为模型的最佳阈值。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">4.3.2 不同模型标注性能对比</h4>
                <div class="p1">
                    <p id="99">本文将<i>GAN</i>-<i>W</i>模型与其他经典的标注方进行对比, 来验证本文所提出模型的有效性。这里涉及的方法包括:传统模型方法<i>RF</i>-<i>opt</i> (<i>Random Forest</i>-<i>optimize</i>) <citation id="209" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、2<i>P</i>K<i>NN</i><citation id="210" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、2<i>P</i>K<i>NN</i>-<i>ML</i> (2<i>P</i>K<i>NN</i>-<i>Metric Learning</i>) <citation id="211" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、<i>SKL</i>-<i>CRM</i><citation id="212" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、<i>KSVM</i>-<i>VT</i><citation id="213" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和使用深度卷积神经网络的方法<i>NN</i>-<i>CNN</i> (<i>Nearest Neighbor</i>-<i>CNN</i>) <citation id="214" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、<i>CNN</i>-<i>R</i> (<i>CNN</i>-<i>Regression</i>) <citation id="215" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、<i>ADA</i> (<i>Attribute Discrimination Annotation</i>) <citation id="216" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、<i>SNDF</i> (<i>automatic image annotation combining Semantic Neighbors and Deep Features</i>) <citation id="217" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、<i>CNN</i>-<i>MSE</i><citation id="218" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、<i>CNN</i>-<i>MLSU</i><citation id="219" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。表2显示本文<i>GAN</i>-<i>W</i>模型与其他模型在<i>Corel</i> 5<i>K</i>和<i>IAPRTC</i>- 12数据集上标注性能的对比。</p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit"><b>表</b>2 <b>不同模型的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Performance comparison of different models</i></p>
                    <p class="img_note">%</p>
                    <table id="100" border="1"><tr><td rowspan="2"><br />模型</td><td colspan="3"><br /><i>Corel</i> 5<i>K</i></td><td rowspan="2"></td><td colspan="3"><br /><i>IAPRTC</i>- 12</td></tr><tr><td><br />R</td><td>P</td><td>F1</td><td><br />R</td><td>P</td><td>F1</td></tr><tr><td><i>RF</i>-<i>opt</i></td><td>40</td><td>29</td><td>34</td><td></td><td>31</td><td>44</td><td>36</td></tr><tr><td><br />2<i>P</i>K<i>NN</i></td><td>40</td><td>39</td><td>39</td><td></td><td>32</td><td>49</td><td>39</td></tr><tr><td><br />2<i>P</i>K<i>NN</i>-<i>ML</i></td><td>46</td><td>41</td><td>43</td><td></td><td>32</td><td>53</td><td>40</td></tr><tr><td><br /><i>SKL</i>-<i>CRM</i></td><td>46</td><td>39</td><td>42</td><td></td><td>32</td><td>51</td><td>39</td></tr><tr><td><br /><i>KSVM</i>-<i>VT</i></td><td>42</td><td>32</td><td>44</td><td></td><td>29</td><td>47</td><td>36</td></tr><tr><td><br /><i>NN</i>-<i>CNN</i></td><td>45</td><td>42</td><td>44</td><td></td><td>32</td><td>54</td><td>41</td></tr><tr><td><br /><i>CNN</i>-<i>R</i></td><td>41</td><td>32</td><td>37</td><td></td><td>31</td><td>49</td><td>38</td></tr><tr><td><br /><i>ADA</i></td><td>40</td><td>32</td><td>36</td><td></td><td>30</td><td>42</td><td>35</td></tr><tr><td><br /><i>SNDF</i></td><td>39</td><td>37</td><td>38</td><td></td><td>30</td><td>48</td><td>37</td></tr><tr><td><br /><i>CNN</i>-<i>MSE</i></td><td>35</td><td>41</td><td>38</td><td></td><td>35</td><td>40</td><td>37</td></tr><tr><td><br /><i>CNN</i>-<i>MLSU</i></td><td><b>49</b></td><td>37</td><td>42</td><td></td><td><b>38</b></td><td>44</td><td>41</td></tr><tr><td><br />GAN-W</td><td>46</td><td><b>46</b></td><td><b>46</b></td><td></td><td>34</td><td><b>55</b></td><td><b>42</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="101">通过表2可以看出, 本文提出的GAN-W模型在Corel 5K数据集上, 性能较传统方法有了较大提高, 召回率取得并列第二, 高于RF-opt方法6个百分点, 准确率和<i>F</i>1值均为第一, 比RF-opt方法分别提高17和12个百分点, 在使用卷积模型的方法中, 召回率比CNN-MSE方法提高了11个百分点, 取得第二高的召回率, 准确率和<i>F</i>1值均为第一。在IAPRTC- 12数据集上, 模型也有良好表现, 准确率和<i>F</i>1值均为第一, 召回率也取得不错效果。综合GAN-W模型在Corel 5K和IAPRTC- 12数据集上的性能指标数据可以得出, GAN-W模型与其他的方法相比, 虽然召回率低于CNN-MLSU方法未取得最高值, 但是效果依然良好, 同时模型准确率和<i>F</i>1值均取得较大提升, 取得最佳效果, 模型的综合性能与其他模型相比具有明显的提高。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">4.3.3 模型实际标注效果</h4>
                <div class="p1">
                    <p id="103">图4中给出模型自动标注的实际结果, 模型统一预测次数为一个batch_size, 128次, 测试Corel 5K数据集时选择的阈值为75, 每幅图像选取出现次数大于阈值的标注作为该图形最终标注。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907045_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Corel 5K数据集上模型实际标注效果" src="Detail/GetImg?filename=images/JSJY201907045_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Corel 5K数据集上模型实际标注效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907045_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Actual annotation effect of model on Corel 5K dataset</p>

                </div>
                <div class="p1">
                    <p id="105">从图4中可以看出:</p>
                </div>
                <div class="p1">
                    <p id="106">1) 与大部分标注模型固定每幅图像的标注数目不同, 本文模型对每幅图像的标注数目不是定值, 不同图像可能有不同的标注数目, 更符合实际标注情况。通过对GAN-W模型的训练, 模型可以学到图像特征与标签向量之间的映射关系, 在每次预测新图像时, 模型就会根据被预测图像的视觉特征中的某种特征输出一个与之对应的标签向量。对于语义简单的图像, 其图像视觉特征只包含某个的标签对应的特征, 所以模型每次输出的向量基本上都接近该标签, 使得该标签对应的出现次数较高, 而其他标签出现次数小于阈值被过滤掉, 模型最终标注数目较少;对于复杂的图像, 其图像视觉特征可能包含多个标签对应的特征, 经过随机噪声的扰动, 使得多个标签中每个标签都有较大概率成为模型输出标签, 所以通过多次测试之后, 多个标签中的每个标签出现次数都不会太小, 模型最终的标注数目较多。</p>
                </div>
                <div class="p1">
                    <p id="107">2) 某些标注虽然与原标注不符合, 但是可能与测试图像的语义相符或者相关, 这是因为某些标注之间 (如tundra与bear、snow、polar) 在数据集中共现频率较高, 使得这些标注在使用Word2vec进行向量化时, 它们对应的多维向量之间的距离很近, 所以在获取输出向量对应最接近的标注词时常一起出现, 并且标注词之间对应的概率相差很小, 导致某些标注虽然不是原始标注, 但是最终统计次数时出现次数依然很大, 被确定为图像标注之一。同时, 由于在数据集中这些标注经常一起出现, 证明在现实中它们之间的联系较深, 所以在新的测试图像中, 这些常与原始标注一起出现的标签依然有较大概率与测试图像相关。例如图4中的tundra不在原始标注中, 但是tundra在数据集中多与bear、snow、polar一起出现, 所以tundra被作为最终输出之一, 依然与图像内容有联系。</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="109">针对基于深度学习的图像自动标注模型其结构受标注词汇量影响的问题, 本文基于生成式对抗网络和词向量模型提出一种新标注模型——GAN-W, 通过在Corel 5K和IAPRTC- 12数据集上的实验结果表明GAN-W模型的准确率<i>P</i>、召回率<i>R</i>及<i>F</i>1值较其他模型有明显的提高, 证明本文模型能够较好地应用于图像标注任务, 标注结果更加符合实际标注情况。然而, 模型存在一些值得改进和研究的方面:1) 词向量的训练结果缺乏一个较好的评判标准;2) 生成器和判别器的网络模型需要进行进一步优化;3) 选择更优的特征提取模型和标签平衡系数。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="140">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiple bemouli relevance models for image and video annotation">

                                <b>[1]</b> FENG S L, MANMATHA R, LAVRENKO V.Multiple Bernoulli relevance models for image and video annotation[C]// Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2004:1002-1009.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic image annotation and retrieval using cross-media relevance models">

                                <b>[2]</b> JEON J, LAVRENKO V, MANMATHA R.Automatic image annotation and retrieval using cross-media relevance models[C]// Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2003:119-126.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14101500003780&amp;v=MTUyODJadEZpbmxVcjNJSjFzUWFoVT1OajdCYXJLOEg5SE5xbzlGWk9zTUMzUTVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> MORAN S, LAVRENKO V.A sparse kernel relevance model for automatic image annotation[J].Journal of Multimedia Information Retrieval, 2014, 3 (4) :209-229.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003704828&amp;v=MDM0NDdsVkxyTUlGaz1OajdCYXJPNEh0SFBxSTlCYk9rSFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rmly&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> MAKADIA A, PAVLOVIC V, KUMAR S.Baselines for image annotation[J].International Journal of Computer Vision, 2010, 90 (1) :88-105.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image annotation using metric learning in semantic neighborhoods">

                                <b>[5]</b> VERMA Y, JAWAHAR C V.Image annotation using metric learning in semantic neighborhoods[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer, 2012:836-849.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tagprop: Discriminative metric learningin nearest neighbor models for image auto-annotation">

                                <b>[6]</b> GUILLAUMIN M, MENSINK T, VERBEEK J, et al.TagProp:discriminative metric learning in nearest neighbor models for image auto-annotation[C]// Proceedings of the 12th IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:309-316.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CBSA: content-based soft annotation for multimodal image retrieval using Bayes point machines">

                                <b>[7]</b> CHANG E, GOH K, SYCHAY G, et al.CBSA:content-based soft annotation for multimodal image retrieval using Bayes point machines [J].IEEE Transactions on Circuits and Systems for Video Technology, 2003, 13 (1) :26-38.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A discriminative kernel-based approach to rank images from text queries">

                                <b>[8]</b> GRANGIER D, BENGIO S.A discriminative kernel-based approach to rank images from text queries[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (8) :1371-1384.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Region-based image annotation using asymmetrical support vector machine-based multipleinstance learning">

                                <b>[9]</b> YANG C, DONG M, HUA J.Region-based image annotation using asymmetrical support vector machine-based multiple-instance learning[C]// Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2006:2057-2063.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201607009&amp;v=MTYzOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVjcvUEx6N0JiN0c0SDlmTXFJOUZiWVE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 黎健成, 袁春, 宋友.基于卷积神经网络的多标签图像自动标注[J].计算机科学, 2016, 43 (7) :41-45. (LI J C, YUAN C, SONG Y.Multi-label image annotation based on convolutional neural network[J].Computer Science, 2016, 43 (7) :41-45.) 
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201701041&amp;v=MTE5MDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1Y3L1BMejdCZDdHNEg5Yk1ybzlCWllRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 高耀东, 侯凌燕, 杨大利.基于多标签学习的卷积神经网络的图像标注方法[J].计算机应用, 2017, 37 (1) :228-232. (GAO Y D, HOU L Y, YANG D L.Automatic image annotation method using multi-label learning convolutional neural network[J].Journal of Computer Applications, 2017, 37 (1) :228-232.) 
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811027&amp;v=MDU3OTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dWNy9QTHo3QmQ3RzRIOW5Ocm85SFk0UUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 汪鹏, 张奥帆, 王利琴, 等.基于迁移学习与多标签平滑策略的图像自动标注[J].计算机应用, 2018, 38 (11) :3199-3203. (WANG P, ZHANG A F, WANG L Q, et al.Image automatic annotation based on transfer learning and multi-label smoothing strategy[J].Journal of Computer Applications, 2018, 38 (11) :3199-3203.) 
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201802013&amp;v=MTI2MDRhTEc0SDluTXJZOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVjcvUEx6N0I=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 李志欣, 郑永哲, 张灿龙, 等.结合深度特征与多标记分类的图像语义标注[J].计算机辅助设计与图形学学报, 2018, 30 (2) :318-326. (LI Z X, ZHENG Y Z, ZHANG C L, et al.Combining deep feature and multi-label classification for semantic image annotation[J].Journal of Computer-Aided Design and Computer Graphics, 2018, 30 (2) :318-326.) 
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Networks">

                                <b>[14]</b> GOODFELLOW I, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[C]// Proceedings of the 2014 Conference on Advances in Neural Information Processing Systems 27.Montreal:Curran Associates, 2014:2672-2680.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MjQ2MjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dWNy9QS0NMZlliRzRIOWJNckk5RlpZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 王坤峰, 苟超, 段艳杰, 等.生成式对抗网络GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332. (WANG K F, GOU C, DUAN Y J, et al.Generative adversarial networks:the state of the art and beyond[J].Acta Automatica Sinica, 2017, 43 (3) :321-332.) 
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">

                                <b>[16]</b> MIRZA M, OSINDERO S.Conditional generative adversarial nets[J].ArXiv Preprint, 2014, 2014:1411.1784.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wasserstein GAN">

                                <b>[17]</b> ARJOVSKY M, CHINTALA S, BOTTOU L.Wasserstein GAN[J].ArXiv Preprint, 2017, 2017:1701.07875.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved training of wasserstein gans">

                                <b>[18]</b> GULRAJANI I, AHMED F, ARJOVSKY M, et al.Improved training of Wasserstein GANs[C]// Proceedings of the 30th Advances in Neural Information Processing Systems.Long Beach, CA:NIPS, 2017:5769-5779.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the impact of residual connections on learning">

                                <b>[19]</b> SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, inception-ResNet and the impact of residual connections on learning[C]// Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:4278-4284.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Random forest for image annotation">

                                <b>[20]</b> FU H, ZHANG Q, QIU G.Random forest for image annotation[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer, 2012:86-99.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploring SVM for image annotation in presence of confusing labels">

                                <b>[21]</b> VERMA Y, JAWAHAR C.Exploring SVM for image annotation in presence of confusing labels[C]// Proceedings of the 24th British Machine Vision Conference.Durham:BMVA Press, 2013:1-11.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Leveraging deep learning representation for search-based image annotation">

                                <b>[22]</b> KASHANI M M, AMIRI S H.Leveraging deep learning representation for search-based image annotation[C]// Proceedings of 2017 Artificial Intelligence and Signal Processing Conference.Piscataway, NJ:IEEE, 2017:156-161.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic image annotation using deep learning representations">

                                <b>[23]</b> MURTHY V N, MAJI S, MANMATHA R.Automatic image annotation using deep learning representations[C]// Proceedings of the 5th ACM on International Conference on Multimedia Retrieval.New York:ACM, 2015:603-606.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201707015&amp;v=MTIyODU3cWZadVpzRnkvZ1Y3L1BOeWZUYkxHNEg5Yk1xSTlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> 周铭柯, 柯逍, 杜明智.基于数据均衡的增进式深度自动图像标注[J].软件学报, 2017, 28 (7) :1862-1880. (ZHOU M K, KE X, DU M Z.Enhanced deep automatic image annotation based on data equalization[J].Journal of Software, 2017, 28 (7) :1862-1880.) 
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201703001&amp;v=MjQ1MTlHRnJDVVI3cWZadVpzRnkvZ1Y3L1BLRDdZYkxHNEg5Yk1ySTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 柯逍, 周铭柯, 牛玉贞.融合深度特征和语义邻域的自动图像标注[J].模式识别与人工智能, 2017, 30 (3) :193-203. (KE X, ZHOU M K, NIU Y Z.Automatic image annotation combining semantic neighbors and deep features[J].Pattern Recognition and Artificial Intelligence, 2017, 30 (3) :193-203.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907045" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907045&amp;v=MjUwNDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVjcvUEx6N0JkN0c0SDlqTXFJOUJZWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
