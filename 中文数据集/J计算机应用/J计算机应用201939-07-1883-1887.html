<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136661638908750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907003%26RESULT%3d1%26SIGN%3dCBV6zyG11MIEkKfShUfag89zhTc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907003&amp;v=MTgxODc0SDlqTXFJOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVkw3Qkx6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="1 近邻样本删减策略 ">1 近邻样本删减策略</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="2 代价敏感的改进算法——CSD-&lt;i&gt;K&lt;/i&gt;NN ">2 代价敏感的改进算法——CSD-<i>K</i>NN</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="3 集成代价敏感的CSD-&lt;i&gt;K&lt;/i&gt;NN算法 ">3 集成代价敏感的CSD-<i>K</i>NN算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#153" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#155" data-title="4.1 &lt;b&gt;度量标准&lt;/b&gt;">4.1 <b>度量标准</b></a></li>
                                                <li><a href="#159" data-title="4.2 &lt;b&gt;样本重采样实验&lt;/b&gt;">4.2 <b>样本重采样实验</b></a></li>
                                                <li><a href="#163" data-title="4.3 &lt;b&gt;算法性能对比&lt;/b&gt;">4.3 <b>算法性能对比</b></a></li>
                                                <li><a href="#170" data-title="4.4 CSD-&lt;i&gt;K&lt;/i&gt;NN&lt;b&gt;算法集成测试&lt;/b&gt;">4.4 CSD-<i>K</i>NN<b>算法集成测试</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#176" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#157" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;数据集分类结果混淆矩阵&lt;/b&gt;"><b>表</b>1 <b>数据集分类结果混淆矩阵</b></a></li>
                                                <li><a href="#161" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;删减前后少数类所占比例&lt;/b&gt;"><b>表</b>2 <b>删减前后少数类所占比例</b></a></li>
                                                <li><a href="#165" data-title="&lt;b&gt;表&lt;/b&gt;3 CSD-&lt;i&gt;K&lt;/i&gt;NN&lt;b&gt;与其他分类器的平均误分代价对比&lt;/b&gt;"><b>表</b>3 CSD-<i>K</i>NN<b>与其他分类器的平均误分代价对比</b></a></li>
                                                <li><a href="#167" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同算法的召回率对比&lt;/b&gt;"><b>表</b>4 <b>不同算法的召回率对比</b></a></li>
                                                <li><a href="#172" data-title="图1 4种算法在不同数据集上的分类准确率对比">图1 4种算法在不同数据集上的分类准确率对比</a></li>
                                                <li><a href="#173" data-title="图2 3种算法的代价敏感性能对比">图2 3种算法的代价敏感性能对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="196">


                                    <a id="bibliography_1" title=" 熊冰妍, 王国胤, 邓维斌.基于样本权重的不平衡数据欠抽样方法[J].计算机研究与发展, 2016, 53 (11) :2613-2622. (XIONG B Y, WANG G Y, DENG W B.Under-sampling method based on sample weight for imbalanced data [J].Journal of Computer Research and Development, 2016, 53 (11) :2613-2622.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201611017&amp;v=MjUwNjU3cWZadVpzRnkvblZMN0FMeXZTZExHNEg5Zk5ybzlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         熊冰妍, 王国胤, 邓维斌.基于样本权重的不平衡数据欠抽样方法[J].计算机研究与发展, 2016, 53 (11) :2613-2622. (XIONG B Y, WANG G Y, DENG W B.Under-sampling method based on sample weight for imbalanced data [J].Journal of Computer Research and Development, 2016, 53 (11) :2613-2622.) 
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     CHENG F, ZHANG J, WEN C.Cost-sensitive large margin distribution machine for imbalanced data classification [J].Pattern Recognition Letters, 2016, 80:107-112.</a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_3" title=" CAO C J, WANG Z.IMCStacking:cost-sensitive stacking learning with feature inverse mapping for imbalanced problems [J].Knowledge-Based Systems, 2018, 150:27-37." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES163B975DE0422E2DA894120FBC36D737&amp;v=MzEwMDhmYksrSGFQRnFJb3hFZXNMRG41TXpXSmk0alo1U1gzaTJtQkdlclRnUXJtWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cTR3YUE9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         CAO C J, WANG Z.IMCStacking:cost-sensitive stacking learning with feature inverse mapping for imbalanced problems [J].Knowledge-Based Systems, 2018, 150:27-37.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_4" title=" PINAR T, LALE O, SINEM K, et al.A cost-sensitive classification algorithm:BEE-miner [J].Knowledge-Based Systems, 2016, 95:99-113." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB3C2A610625F5E91DA8AF44D3D452B82&amp;v=MDQ3MjRQbnZtMkJGQmZiZVdON0tkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxNHdhQT1OaWZPZmNHN2JkTzlxWTVGWXVrS2VubE14aGRubXpjTQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         PINAR T, LALE O, SINEM K, et al.A cost-sensitive classification algorithm:BEE-miner [J].Knowledge-Based Systems, 2016, 95:99-113.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_5" title=" LOMAX S, VADERA S.A survey of cost-sensitive decision tree induction algorithms [J].ACM Computing Surveys, 2013, 45 (2) :16-50." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000009793&amp;v=MDk3OTVJWTdLN0h0ak5yNDlGWk9zR0MzVTZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXdUYXhvPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         LOMAX S, VADERA S.A survey of cost-sensitive decision tree induction algorithms [J].ACM Computing Surveys, 2013, 45 (2) :16-50.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_6" title=" 陈永辉, 岳丽华.特征敏感的点云重采样算法[J].小型微型计算机系统, 2017, 38 (5) :1086-1090. (CHEN Y H, YUE L H.Point cloud resampling algorithm of feature-sensitivity [J].Journal of Chinese Computer Systems, 2017, 38 (5) :1086-1090.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201705034&amp;v=MDU0ODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTDdBUFRYY2RyRzRIOWJNcW85R1lJUUs=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         陈永辉, 岳丽华.特征敏感的点云重采样算法[J].小型微型计算机系统, 2017, 38 (5) :1086-1090. (CHEN Y H, YUE L H.Point cloud resampling algorithm of feature-sensitivity [J].Journal of Chinese Computer Systems, 2017, 38 (5) :1086-1090.) 
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_7" title=" 陈海鹏, 申铉京, 龙建武.采用高斯拟合的全局阈值算法阈值优化框架[J].计算机研究与发展, 2016, 53 (4) :892-903. (CHEN H P, SHEN X J, LONG J W.Threshold optimization framework of global thresholding algorithms using Gaussian fitting [J].Journal of Computer Research and Development, 2016, 53 (4) :892-903.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201604017&amp;v=MTA4NjNTZExHNEg5Zk1xNDlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblZMN0FMeXY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         陈海鹏, 申铉京, 龙建武.采用高斯拟合的全局阈值算法阈值优化框架[J].计算机研究与发展, 2016, 53 (4) :892-903. (CHEN H P, SHEN X J, LONG J W.Threshold optimization framework of global thresholding algorithms using Gaussian fitting [J].Journal of Computer Research and Development, 2016, 53 (4) :892-903.) 
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_8" title=" 李勇, 刘战东, 张海军.不平衡数据的集成分类算法综述[J].计算机应用研究, 2014, 31 (5) :1287-1291. (LI Y, LIU Z D, ZHANG H J.A survey of integrated classification algorithms for unbalanced data [J].Application Research of Computers, 2014, 31 (5) :1287-1291.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201405003&amp;v=MjM5MDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVkw3QUx6N1NaTEc0SDlYTXFvOUZaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         李勇, 刘战东, 张海军.不平衡数据的集成分类算法综述[J].计算机应用研究, 2014, 31 (5) :1287-1291. (LI Y, LIU Z D, ZHANG H J.A survey of integrated classification algorithms for unbalanced data [J].Application Research of Computers, 2014, 31 (5) :1287-1291.) 
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     DOMINGOS P.MetaCost:a general method for making classifiers cost-sensitive[C]// Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:155-164.</a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_10" title=" 周宇航, 周志华.代价敏感大间隔分布学习机[J].计算机研究与发展, 2016, 53 (9) :1964-1970. (ZHOU Y H, ZHOU Z H.Cost sensitive large interval distribution learning machine [J].Journal of Computer Research and Development, 2016, 53 (9) :1964-1970.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201609007&amp;v=MjUxMTMzenFxQnRHRnJDVVI3cWZadVpzRnkvblZMN0FMeXZTZExHNEg5Zk1wbzlGWTRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         周宇航, 周志华.代价敏感大间隔分布学习机[J].计算机研究与发展, 2016, 53 (9) :1964-1970. (ZHOU Y H, ZHOU Z H.Cost sensitive large interval distribution learning machine [J].Journal of Computer Research and Development, 2016, 53 (9) :1964-1970.) 
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_11" title=" BAHNSEN A C, AOUADA D, OTTERSTEN B.Example-dependent cost-sensitive decision trees [J].Expert Systems with Applications, 2015, 42 (19) :6609-6619." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES120D7CC8DF6B69F1927FF7C0270AF733&amp;v=MjQ2MjBXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxNHdhQT1OaWZPZmJLNkhxWEwzUHhORUowSmZub3d1UmNhNkRnTFBuaVJyQkF5ZWNQaVFybWNDT052RlNpVw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         BAHNSEN A C, AOUADA D, OTTERSTEN B.Example-dependent cost-sensitive decision trees [J].Expert Systems with Applications, 2015, 42 (19) :6609-6619.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_12" title=" GHAZIKHANI A, MONSEFI R, YAZDI H S.Online cost-sensitive neural network classifiers for non-stationary and imbalanced data streams [J].Neural Computing and Applications, 2013, 23 (5) :1283-1295." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13101000016317&amp;v=MjExMzNpbmxVcjNJSjF3VGF4bz1OajdCYXJLN0g5SE5yNDlGWk9vSkQzMCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0Rg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         GHAZIKHANI A, MONSEFI R, YAZDI H S.Online cost-sensitive neural network classifiers for non-stationary and imbalanced data streams [J].Neural Computing and Applications, 2013, 23 (5) :1283-1295.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_13" title=" 付忠良.多标签代价敏感分类集成学习算法[J].自动化学报, 2014, 40 (6) :1075-1085. (FU Z L.Multi-tag cost sensitive classification integrated learning algorithm [J].Acta Automatica Sinica, 2014, 40 (6) :1075-1085.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201406005&amp;v=MTY5MTFzRnkvblZMN0FLQ0xmWWJHNEg5WE1xWTlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         付忠良.多标签代价敏感分类集成学习算法[J].自动化学报, 2014, 40 (6) :1075-1085. (FU Z L.Multi-tag cost sensitive classification integrated learning algorithm [J].Acta Automatica Sinica, 2014, 40 (6) :1075-1085.) 
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_14" title=" 王茜, 杨正宽.一种基于加权KNN的大数据集下离群检测算法[J].计算机科学, 2011, 38 (10) :177-180. (WANG Q, YANG Z K.An outlier detection algorithm for big data sets based on weighted KNN [J].Computer Science, 2011, 38 (10) :177-180.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201110041&amp;v=MTQ5MTdCdEdGckNVUjdxZlp1WnNGeS9uVkw3QUx6N0JiN0c0SDlETnI0OUJaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         王茜, 杨正宽.一种基于加权KNN的大数据集下离群检测算法[J].计算机科学, 2011, 38 (10) :177-180. (WANG Q, YANG Z K.An outlier detection algorithm for big data sets based on weighted KNN [J].Computer Science, 2011, 38 (10) :177-180.) 
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     FREUND Y, IYER R, SCHAPIRE R, et al.An efficient boosting algorithm for combining preferences [J].Journal of Machine Learning Research, 2003, 4 (6) :170-178.</a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_16" title=" 胡小生, 钟勇.基于边界样本选择的支持向量机加速算法[J].计算机工程与应用, 2017, 53 (3) :169-173. (HU X S, ZHONG Y.Support vector machine acceleration algorithm based on boundary sample selection [J].Computer Engineering and Applications, 2017, 53 (3) :169-173.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201703031&amp;v=MjgwMDBGckNVUjdxZlp1WnNGeS9uVkw3QUx6N01hYkc0SDliTXJJOUdaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         胡小生, 钟勇.基于边界样本选择的支持向量机加速算法[J].计算机工程与应用, 2017, 53 (3) :169-173. (HU X S, ZHONG Y.Support vector machine acceleration algorithm based on boundary sample selection [J].Computer Engineering and Applications, 2017, 53 (3) :169-173.) 
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_17" title=" 蒋盛益, 谢照青, 余雯.基于代价敏感的朴素贝叶斯不平衡数据分类研究[J].计算机研究与发展, 2011, 48 (增刊I) :387-390. (JIANG S Y, XIE Z Q, YU W.Cost-sensitive naive Bayesian unbalanced data classification [J].Journal of Computer Research and Development, 2011, 48 (Suppl I) :387-390.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ2011S1066&amp;v=Mjg5MzJybzlEWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblZMN0FMeXZTZExHNEg5Q3Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         蒋盛益, 谢照青, 余雯.基于代价敏感的朴素贝叶斯不平衡数据分类研究[J].计算机研究与发展, 2011, 48 (增刊I) :387-390. (JIANG S Y, XIE Z Q, YU W.Cost-sensitive naive Bayesian unbalanced data classification [J].Journal of Computer Research and Development, 2011, 48 (Suppl I) :387-390.) 
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_18" title=" SUN Y, KAMEL M S, WONG A K, et al.Cost-sensitive boosting for classification of imbalanced data [J].Pattern Recognition, 2007, 40 (12) :3358-3378." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739373&amp;v=MzI0MjYzSUoxd1RheG89TmlmT2ZiSzdIdEROcVk5RlkrZ0dEM3M2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         SUN Y, KAMEL M S, WONG A K, et al.Cost-sensitive boosting for classification of imbalanced data [J].Pattern Recognition, 2007, 40 (12) :3358-3378.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                     SUN Y, WONG A K, WANG Y.Parameter inference of cost-sensitive boosting algorithms [C]// Proceedings of the 4th International Conference on Machine Learning and Data Mining in Pattern Recognition.Berlin:Springer, 2005:21-30.</a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_20" title=" 曹莹, 苗启广, 刘家辰, 等.具有Fisher一致性的代价敏感Boosting算法[J].软件学报, 2013, 24 (11) :2584-2596. (CAO Y, MIAO Q G, LIU J C, et al.Fisher consistent cost sensitive Boosting algorithm [J].Journal of Software, 2013, 24 (11) :2584-2596.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201311011&amp;v=MzIxODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTDdBTnlmVGJMRzRIOUxOcm85RVpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         曹莹, 苗启广, 刘家辰, 等.具有Fisher一致性的代价敏感Boosting算法[J].软件学报, 2013, 24 (11) :2584-2596. (CAO Y, MIAO Q G, LIU J C, et al.Fisher consistent cost sensitive Boosting algorithm [J].Journal of Software, 2013, 24 (11) :2584-2596.) 
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     周志华.机器学习[M].北京:清华大学出版社, 2018:30-33. (ZHOU Z H.Machine Learning [M].Beijing:Tsinghua University Press, 2018:30-33.) </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-29 07:32</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),1883-1887 DOI:10.11772/j.issn.1001-9081.2018122483            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于<i>K</i>最近邻样本平均距离的代价敏感算法的集成</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%B5%A9&amp;code=34912196&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨浩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AE%87&amp;code=07166892&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%B8%AD%E5%8E%9F&amp;code=42202177&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张中原</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E6%B5%B7%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=1044544&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河海大学计算机与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%BB%91%E9%93%81%E5%8D%A2%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB&amp;code=0010785&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">滑铁卢大学计算机系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了解决不均衡数据集的分类问题和一般的代价敏感学习算法无法扩展到多分类情况的问题, 提出了一种基于<i>K</i>最近邻 (<i>K</i>NN) 样本平均距离的代价敏感算法的集成方法。首先, 根据最大化最小间隔的思想提出一种降低决策边界样本密度的重采样方法;接着, 采用每类样本的平均距离作为分类结果的判断依据, 并提出一种符合贝叶斯决策理论的学习算法, 使得改进后的算法具备代价敏感性;最后, 对改进后的代价敏感算法按<i>K</i>值进行集成, 以代价最小为原则, 调整各基学习器的权重, 得到一个以总体误分代价最低为目标的代价敏感AdaBoost算法。实验结果表明, 与传统的<i>K</i>NN算法相比, 改进后的算法在平均误分代价上下降了31.4个百分点, 并且代价敏感性能更好。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">代价敏感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%A4%A7%E5%8C%96%E6%9C%80%E5%B0%8F%E9%97%B4%E9%9A%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最大化最小间隔;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%B7%E6%9C%AC%E9%97%B4%E8%B7%9D%E7%A6%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">样本间距离;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贝叶斯决策理论;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *杨浩 (1995—) , 男, 江苏淮安人, 硕士研究生, 主要研究方向:代价敏感学习、异常检测;电子邮箱2676622701@qq.com;
                                </span>
                                <span>
                                    王宇 (1979—) , 男, 云南昆明人, 副研究员, 博士, 主要研究方向:云计算、数据管理;;
                                </span>
                                <span>
                                    张中原 (1997—) , 男, 江苏南京人, 主要研究方向:人工智能、生物信息学。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然青年科学基金资助项目 (61103017);</span>
                                <span>中国科学院感知中国先导专项子课题项目 (XDA06040504);</span>
                    </p>
            </div>
                    <h1><b>Integration of cost-sensitive algorithms based on average distance of <i>K</i>-nearest neighbor samples</b></h1>
                    <h2>
                    <span>YANG Hao</span>
                    <span>WANG Yu</span>
                    <span>ZHANG Zhongyuan</span>
            </h2>
                    <h2>
                    <span>College of Computer and Information, Hohai University</span>
                    <span>Department of Computer Science, Waterloo University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To solve the problem of classification of unbalanced data sets and the problem that the general cost-sensitive learning algorithm can not be applied to multi-classification condition, an integration method of cost-sensitive algorithm based on average distance of <i>K</i>-Nearest Neighbor (<i>K</i>NN) samples was proposed. Firstly, according to the idea of maximizing the minimum interval, a resampling method for reducing the density of decision boundary samples was proposed. Then, the average distance of each type of samples was used as the basis of judgment of classification results, and a learning algorithm based on Bayesian decision-making theory was proposed, which made the improved algorithm cost sensitive. Finally, the improved cost-sensitive algorithm was integrated according to the <i>K</i> value. The weight of each base learner was adjusted according to the principle of minimum cost, obtaining the cost-sensitive AdaBoost algorithm aiming at the minimum total misclassification cost. The experimental results show that compared with traditional <i>K</i>NN algorithm, the improved algorithm reduces the average misclassification cost by 31.4 percentage points and has better cost sensitivity.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cost-sensitive&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cost-sensitive;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=maximization%20of%20minimum%20interval&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">maximization of minimum interval;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=distance%20between%20samples&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">distance between samples;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Bayesian%20decision-making%20theory&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Bayesian decision-making theory;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=integration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">integration;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YANG Hao, born in 1995, M. S. candidate. His research interests include cost sensitive learning, anomaly detection. ;
                                </span>
                                <span>
                                    WANG Yu, born in 1979, Ph. D. , associate researcher fellow. His research interests include cloud computing, data management. ;
                                </span>
                                <span>
                                    ZHANG Zhongyuan, born in 1997. His research interests include artificial intelligence, bioinformatics.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-17</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Youth Science Fund Project (61103017);</span>
                                <span>the Chinese Academy of Sciences Perceives China&#39;s Pilot Special Sub-Project (XDA06040504);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">在机器学习研究过程中, 经常存在着样本类别分布不均衡的情况, 传统的分类器注重于提高分类的准确率, 对不均衡数据集的分类结果更倾向于多数类<citation id="238" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 这种分类方式默认两种分类错误的代价是相等的。然而在很多领域, 比如入侵检测、医疗诊断、欺诈检测等, 少数类的误分类代价十分巨大, 在此类情况中, 人们主要关心少数类的分类准确率。传统的算法无法满足此类数据的分类需要, 于是代价敏感学习的思想被提出并广泛应用, 代价敏感学习方法是解决不均衡数据集分类问题的一个重要方法<citation id="239" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。代价敏感学习是指对不同的误分类结果赋予不同的代价, 得到一个在对未知样本进行分类时误分代价最小的分类器<citation id="240" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。常见的误分类代价包括基于类别的代价和基于样本的代价。在基于类别的代价中, 代价只与类别有关, 而在基于样本的代价中, 误分类代价与每一个样本有关, 在现实场景中, 基于样本的代价很难获得, 一般使用基于类别的代价。</p>
                </div>
                <div class="p1">
                    <p id="49">一直以来, 代价敏感学习算法在国际上都是机器学习和数据挖掘领域的研究热点。现有的代价敏感学习方法分为两种<citation id="241" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>:一是基于特定算法的代价敏感学习方法, 将优化目标变为得到期望代价最小的假设。例如在决策树算法中使用代价敏感的叶节点分裂准则以及代价敏感的剪枝策略<citation id="242" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。二是代价敏感学习元方法, 将使期望误差率最小的学习算法转变为得到期望代价最小的代价敏感学习算法, 常见方法包括重采样法<citation id="243" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、阈值移动法<citation id="244" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、集成学习法<citation id="245" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。这种方法具备良好的通用性, 文献<citation id="246" type="reference">[<a class="sup">9</a>]</citation>基于贝叶斯风险最小化原理提出了一种可以将任意的分类器算法转化为代价敏感算法的MetaCost算法, 根据样本属于每个类的概率及误分类代价之积选取出分类代价最小的类别作为样本分类结果, 达到最小误分代价。目前, 一些常见的分类算法, 如支持向量机 (Support Vector Machine, SVM) 、决策树、神经网络和AdaBoost都有对应的代价敏感算法<citation id="247" type="reference"><link href="214" rel="bibliography" /><link href="216" rel="bibliography" /><link href="218" rel="bibliography" /><link href="220" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。MetaCost算法可以将传统的分类算法转化为代价敏感学习算法, 并且适用于任何数目的样本类别和任意代价矩阵。</p>
                </div>
                <div class="p1">
                    <p id="50">但是, 目前的代价敏感学习算法主要针对二分类问题, 对于代价敏感的多分类问题的研究不多, 一些常见的算法无法扩展到多分类场景中。<i>K</i>最邻近 (<i>K</i>-Nearest Neighbor, <i>K</i>NN) 分类算法作为一种成熟的算法, 具有鲁棒性、概念清晰等优点, 算法以<i>K</i>个近邻样本的投票数来对未知样本进行分类, 可以直接扩展到多分类场景中。尽管<i>K</i>NN算法的优势十分明显, 但是它的缺点也不容忽视。<i>K</i>NN算法基于空间向量模型 (Vector Space Model, VSM) 模型, 利用欧氏距离或余弦距离度量样本的距离, 但权重不变, 这与实际情况不符, 一种改进的方法是加权<i>K</i>NN算法, 根据样本点之间的距离来分配权重, 权重的大小随距离的减小而增大<citation id="248" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。同时<i>K</i>NN算法在<i>K</i>值的设定方面依赖于经验, 存在着<i>K</i>值单一的情况, 而Boosting算法可以集成多个具备不同<i>K</i>值的<i>K</i>NN分类器, 有效解决了这一问题<citation id="249" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="51">针对上述问题, 本文提出了一种用于不平衡数据集上的近邻样本删减策略以及基于近邻样本间距的代价敏感 (Cost Sensitive based on average Distance of <i>K</i>-Nearest Neighbor, CSD-<i>K</i>NN) 算法, 并在此基础上对此算法按一定策略进行集成。首先, 针对多数类的边界进行选择性删减, 在新的数据集上算法得到样本点与<i>K</i>个近邻样本的距离, 并计算出每一个类中样本与测试样本点之间的平均距离, 以此作为输入进行代价敏感变换, 得到期望代价最小的代价敏感分类器, 最后在具备代价权重初值的AdaBoost算法上集成这种改进后的算法, 使之误分代价最小。在UCI数据集上的测试结果表明改进后算法的平均误分代价更低。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">1 近邻样本删减策略</h3>
                <div class="p1">
                    <p id="53">不均衡数据集会弱化分类器对少数类的分类效果, 对样本集的修改策略是重构数据集, 调整样本分布, 使得多数类与少数类的样本比例趋于1∶1。已有成果表明对分类结果有较大影响的样本处于样本边界<citation id="250" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 降低边界处的多数类样本密度比减少多数类的样本数量更为切实有效。受支持向量机最大化最小间隔形式化目标的启发, 本文对多数类的边界样本进行筛选, 减小边界处多数类样本的密度, 加大多数类与少数类的样本间隔, 降低分类结果受少数类样本稀疏性的影响。</p>
                </div>
                <div class="p1">
                    <p id="54">假设样本集<b><i>D</i></b> (|<b><i>D</i></b>|=<i>n</i>) ={<b><i>X</i></b><sub>1</sub>, <b><i>X</i></b><sub>2</sub>, …, <b><i>X</i></b><sub><i>n</i></sub>}, 每个样本<b><i>X</i></b><sub><i>i</i></sub>含有<i>z</i>个特征{<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>z</i></sub>}, <i>Y</i><sub><i>i</i></sub>∈{1, -1}为样本所属分类。多数类集为<b><i>D</i></b><sub>1</sub> (|<b><i>D</i></b><sub>1</sub>|=<i>n</i><sub>1</sub>) , 属性集<i>Y</i><sub><i>i</i></sub>=1, 少数类集为<b><i>D</i></b><sub>2</sub> (|<b><i>D</i></b><sub>2</sub>|=<i>n</i><sub>2</sub>) , 属性集<i>Y</i><sub><i>i</i></sub>=-1, 其中<i>n</i><sub>1</sub>+<i>n</i><sub>2</sub>=<i>n</i>。<i>d</i><sub><i>ij</i></sub>为样本<b><i>X</i></b><sub><i>i</i></sub>与样本<b><i>X</i></b><sub><i>j</i></sub>的欧氏距离:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>z</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><msub><mrow></mrow><mrow><msub><mrow></mrow><mi>m</mi></msub></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><msub><mrow></mrow><mrow><msub><mrow></mrow><mi>m</mi></msub></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">定义样本与自身的距离为∞。算法的思想是:确保少数类样本的最近邻样本仍为少数类, 即遍历少数类中的所有样本, 判断其最近邻样本点是否属于少数类, 若不是, 则删除此样本的最近邻点, 并继续对此样本进行判断, 直到此样本点的最近邻样本也属于少数类。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">2 代价敏感的改进算法——CSD-<i>K</i>NN</h3>
                <div class="p1">
                    <p id="58">当样本被错分时会产生代价, 分为两种情况:多数类误分为少数类的代价, 以及少数类误分为多数类的代价。当样本正确分类时, 代价值为0。传统的学习算法默认两种误分类情况是等价的, 但是在实际情况中, 两者必须区分开来。代价敏感思想基于期望代价最小的原则对分类器作出调整。<i>MetaCost</i>方法根据样本属于每一类的概率与其对应的误分代价值之积, 得到一个具有最小期望代价的分类结果。本文改进了传统K<i>NN</i>算法中每个样本权重相等的弊端, 基于每一类中近邻点与样本之间的平均距离将样本的误分代价值以函数的形式表现出来。假设待测样本<b><i>X</i></b><sub><i>i</i></sub>与邻近样本<b><i>X</i></b><sub><i>j</i></sub> (<i>j</i>=1, 2, …, <i>K</i>) 的距离为<i>d</i><sub><i>ij</i></sub>, 为了便于说明, 这里的<i>d</i><sub><i>ij</i></sub>取归一化后的数值, 属性的归一化方法为:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>min</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mrow><mi>max</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>-</mo><mi>min</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">将每一类中近邻样本与测试样本之间的平均距离作为具体的自变量因子, 通过对数函数的形式表示基于距离的代价函数, 距离越小, 误分代价值越大, 并且随着距离的缩小, 样本的误分类代价值以指数形式上升。通过样本属性维度值<i>w</i>将对数函数的自变量取值范围控制在0到1之间, 避免函数值为负。在这种思想的指导下构造出基于距离的代价函数如下所示:</p>
                </div>
                <div class="p1">
                    <p id="61"><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mspace width="0.25em" /><mrow><mi>log</mi></mrow><msub><mrow></mrow><mi>α</mi></msub><mspace width="0.25em" /><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><mo>∑</mo><mi>d</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo>/</mo><mo stretchy="false"> (</mo><mi>m</mi><msqrt><mi>w</mi></msqrt><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></math></mathml>; 0&lt;<i>α</i>&lt;1      (3) </p>
                </div>
                <div class="p1">
                    <p id="63">其中:<i>m</i>为某一类近邻样本点的个数;<i>α</i>值为距离对样本分类的影响因子, 值越小表明距离对样本分类结果影响越大;<i>c</i>为少数类与多数类误分类代价的比重。</p>
                </div>
                <div class="p1">
                    <p id="64">基于最小风险的贝叶斯决策的形式化目标<citation id="251" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>为:</p>
                </div>
                <div class="p1">
                    <p id="65"><i>R</i> (<i>y</i><sub><i>i</i></sub>|<i>x</i>) =∑<i>P</i> (<i>y</i><sub><i>i</i></sub>|<i>x</i>) <i>F</i> (<i>y</i><sub><i>i</i></sub>, <i>y</i><sub><i>j</i></sub>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="66">其中:<i>R</i> (<i>y</i><sub><i>i</i></sub>|<i>x</i>) 为样本<i>x</i>分类到<i>y</i><sub><i>i</i></sub>中的风险构造函数, <i>F</i> (<i>y</i><sub><i>i</i></sub>, <i>y</i><sub><i>j</i></sub>) 为类别<i>y</i><sub><i>i</i></sub>误分为<i>y</i><sub><i>j</i></sub>的代价, <i>P</i> (<i>y</i><sub><i>i</i></sub>|<i>x</i>) 为样本<i>x</i>属于类别<i>y</i><sub><i>i</i></sub>的后验概率。</p>
                </div>
                <div class="p1">
                    <p id="67"><b>定理</b>1 在<i>K</i>NN算法中, 若<i>K</i>为近邻样本的数量, <i>m</i>为样本中某一类的总体数量, 则样本属于该类的概率为<i>m</i>/<i>K</i>。</p>
                </div>
                <div class="p1">
                    <p id="68">证明</p>
                </div>
                <div class="p1">
                    <p id="69"><i>P</i> (<i>y</i><sub><i>i</i></sub>|<i>x</i>) =<i>P</i> (<i>x</i>|<i>y</i><sub><i>i</i></sub>) <i>P</i> (<i>y</i><sub><i>i</i></sub>) /<i>P</i> (<i>x</i>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="70">以样本比例逼近概率:<i>V</i>为<i>K</i>个样本点包围的最小超球的体积, <i>m</i>为数据集中类别<i>y</i><sub><i>i</i></sub>的个数, 得到类条件概率密度:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mi>m</mi><mrow><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>*</mo><mfrac><mn>1</mn><mi>V</mi></mfrac></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mi>Κ</mi><mi>Ν</mi></mfrac><mo>*</mo><mfrac><mn>1</mn><mi>V</mi></mfrac></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub><mo>/</mo><mi>Ν</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中<i>N</i>为样本总数。</p>
                </div>
                <div class="p1">
                    <p id="73">证得:</p>
                </div>
                <div class="p1">
                    <p id="74"><i>P</i> (<i>y</i><sub><i>i</i></sub>|<i>x</i>) =<i>m</i>/<i>K</i>      (6) </p>
                </div>
                <div class="p1">
                    <p id="75">因此得到符合贝叶斯决策理论的代价敏感算法——CSD-<i>K</i>NN, 表现形式为:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><mrow><mi>max</mi></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>c</mi><mfrac><mi>m</mi><mi>Κ</mi></mfrac><mtext> </mtext><mrow><mi>log</mi></mrow><msub><mrow></mrow><mi>α</mi></msub><mfrac><mrow><mstyle displaystyle="true"><mo>∑</mo><mi>d</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mi>m</mi><msqrt><mi>w</mi></msqrt></mrow></mfrac><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">根据文中描述的分类器, 具体的实现过程如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="78">算法1 代价敏感的改进算法——CSD-<i>K</i>NN。</p>
                </div>
                <div class="area_img" id="255">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201907003_25500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="116" name="116" class="anchor-tag">3 集成代价敏感的CSD-<i>K</i>NN算法</h3>
                <div class="p1">
                    <p id="117"><i>Boosting</i>算法的基本思想是对每一轮迭代得到的分类器赋予不同的权重, 使得分类器更注重分类难度大的样本, 最终得到一个基分类器<i>h</i><sub><i>t</i><sub> (<i>x</i>) </sub></sub>的线性集合<i>H</i><sub> (<i>x</i>) </sub>。计算方式为<i>H</i><sub><i>t</i>+1<sub> (<i>x</i>) </sub></sub>=<i>H</i><sub><i>t</i><sub> (<i>x</i>) </sub></sub>+<i>α</i>*<i>t</i>*<i>h</i><sub><i>t</i><sub> (<i>x</i>) </sub></sub>, 传统的Boosting算法的最大特点是随着迭代次数的增长, 分类错误率以指数速度下降。将Boosting算法应用到代价敏感学习中, 可以得到以错分类代价最低为目标的代价敏感分类器, 代价敏感的Boosting算法是代价敏感学习方法中的一个重要组成部分。目前人们已经提出了AdaCost、AdaC3<citation id="253" type="reference"><link href="230" rel="bibliography" /><link href="232" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>等代价敏感Boosting算法, 但是这些算法通过启发式策略向AdaBoost算法的加权投票因子中加入代价因子, 有可能破坏算法的Boosting特性<citation id="252" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。AdaBoost算法具备非对称学习能力, 在算法进行迭代之前, 根据样本类别赋予样本代价权重:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>/</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">根据式 (8) 可以实现代价敏感的Boosting算法。本文对此过程进行了改进, 在赋予样本初始化代价权重后, 对代价敏感的基分类器进行训练, 实验结果表明, 集成改进后的代价敏感算法, 减小了总体误分代价, 代价敏感性更好。</p>
                </div>
                <div class="p1">
                    <p id="120">算法2 集成代价敏感的CSD-<i>K</i>NN算法。</p>
                </div>
                <div class="area_img" id="256">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201907003_25600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="256">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201907003_25601.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="153" name="153" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="154">实验测试数据集为<i>UCI</i>官网上的公开数据集 (<i>http</i>://<i>archive</i>.<i>ics</i>.<i>uci</i>.<i>edu</i>/<i>ml</i>/<i>index</i>.<i>php</i>) , 在其中选取了5个典型的不均衡的分类数据集, 实验中将非数字型数据用数值表示, 并对所有数据进行归一化处理, 使之成为能够被K<i>NN</i>算法加载的数据集。对每个数据集进行了10折交叉法, 每次取其中9个数据集作为样本集, 剩余1个数据集作为测试集, 实验结果取其平均值。</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">4.1 <b>度量标准</b></h4>
                <div class="p1">
                    <p id="156">代价敏感的学习过程中, 提高高代价样本的分类准确率显得更为重要, 通过对少数类的召回率 (<i>Recall</i>) 、平均误分代价 (<i>AvgCost</i>) 和高代价错误率 (记为<i>High</i>-<i>rate</i>) 进行代价敏感性能比较, 将多数类作为正例得到的混淆矩阵<citation id="254" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>如表1所示。</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表</b>1 <b>数据集分类结果混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Classification result confusion matrix of dataset</i></p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td rowspan="2"><br />真实情况</td><td colspan="2"><br />预测结果</td></tr><tr><td><br />正例</td><td>反例</td></tr><tr><td><br />正例</td><td>TP (<i>True Positive</i>) </td><td>FN (<i>False Negative</i>) </td></tr><tr><td><br />反例</td><td>FP (<i>False Positive</i>) </td><td>TN (<i>True Negative</i>) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="158" class="code-formula">
                        <mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>A</mi><mi>v</mi><mi>g</mi><mi>C</mi><mi>o</mi><mi>s</mi><mi>t</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>Ν</mi><mo>+</mo><mi>c</mi><mi>F</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi><mo>+</mo><mi>Τ</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Η</mi><mi>i</mi><mi>g</mi><mi>h</mi><mo>-</mo><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi><mo>+</mo><mi>Τ</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="159" name="159">4.2 <b>样本重采样实验</b></h4>
                <div class="p1">
                    <p id="160">第一个实验是将数据集按照近邻原则进行删减, 降低多数类样本的边界密度, 使得样本分布趋于平衡。表2为样本集删减前后少数类的比重对比, 其中的rate为少数类在整个样本中所占的比重, <b><i>Before</i></b>、<b><i>Later</i></b>为数据删减前后的样本数与维数的向量表示。</p>
                </div>
                <div class="area_img" id="161">
                    <p class="img_tit"><b>表</b>2 <b>删减前后少数类所占比例</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Proportion of minority before and after sample deleting</p>
                    <p class="img_note"></p>
                    <table id="161" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="2"><br /><b><i>Before</i></b></td><td rowspan="2"></td><td colspan="2"><br /><b><i>Later</i></b></td></tr><tr><td><br /> (样本数, 维数) </td><td><i>rate</i></td><td><br /> (样本数, 维数) </td><td><i>rate</i></td></tr><tr><td>breast-cancer</td><td> (699, 9) </td><td>0.344 8</td><td></td><td> (573, 9) </td><td>0.420 6</td></tr><tr><td><br />haberman</td><td> (306, 4) </td><td>0.264 7</td><td></td><td> (148, 4) </td><td>0.547 3</td></tr><tr><td><br />glass</td><td> (214, 10) </td><td>0.350 5</td><td></td><td> (165, 10) </td><td>0.454 5</td></tr><tr><td><br />diabetes</td><td> (768, 9) </td><td>0.349 0</td><td></td><td> (466, 9) </td><td>0.575 1</td></tr><tr><td><br />balance</td><td> (625, 5) </td><td>0.400 0</td><td></td><td> (442, 5) </td><td>0.565 6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="162">实验过后, 少数类的比重<i>rate</i>较原先的数据集提升了近17个百分点, 表2中的结果表明, 删减过后的样本分布更加均衡, 少数类和多数类的比例接近1∶1, 近邻删减法可以有效降低多数类的样本密度。</p>
                </div>
                <h4 class="anchor-tag" id="163" name="163">4.3 <b>算法性能对比</b></h4>
                <div class="p1">
                    <p id="164">在第二个实验中, 将<i>CSD</i>-K<i>NN</i>算法与传统的K<i>NN</i>和贝叶斯分类器进行代价敏感性能比较, 并综合分析算法的分类准确率。整个实验中的数据是以<i>K</i>=5, <i>α</i>=0.5, 误分代价比重<i>c</i>=3得出的数据, 其中CSD-<i>K</i>NN (new) 表示在按近邻策略删减之后的样本集上进行性能分析的结果。表4中的衡量指标为Recall值, 少数类的召回率表示少数类的分类准确率, Recall值越大, 表示算法对少数类的分类精确率越高。</p>
                </div>
                <div class="area_img" id="165">
                    <p class="img_tit"><b>表</b>3 CSD-<i>K</i>NN<b>与其他分类器的平均误分代价对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Comparison of average misclassification cost among CSD-<i>K</i>NN and other classifiers</p>
                    <p class="img_note"></p>
                    <table id="165" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="4"><br />不同算法的平均误分代价</td></tr><tr><td><br /><i>K</i>NN</td><td>Naive Bayes</td><td>CSD-<i>K</i>NN</td><td>CSD-<i>K</i>NN (new) </td></tr><tr><td><br />Breast-cancer</td><td>0.065 77</td><td>1.033 66</td><td>0.055 73</td><td>0.021 02</td></tr><tr><td><br />haberman</td><td>0.629 35</td><td>0.793 55</td><td>0.603 54</td><td>0.425 24</td></tr><tr><td><br />glass</td><td>0.596 36</td><td>0.763 81</td><td>0.556 02</td><td>0.465 91</td></tr><tr><td><br />diabetes</td><td>0.694 81</td><td>0.687 45</td><td>0.564 72</td><td>0.386 03</td></tr><tr><td><br />balance</td><td>1.034 46</td><td>0.928 57</td><td>0.624 32</td><td>0.420 91</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="166">从表3中可以得出, 基于距离的CSD-<i>K</i>NN算法的平均误分代价相比<i>K</i>NN算法下降了12.33个百分点, 对数据集应用近邻删减法和代价敏感改进, 算法的平均误分代价下降了37.87个百分点。实验结果表明, 与传统的<i>K</i>NN算法和朴素贝叶斯算法相比, 改进后算法的代价敏感性能明显优于传统算法。</p>
                </div>
                <div class="area_img" id="167">
                    <p class="img_tit"><b>表</b>4 <b>不同算法的召回率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Comparison of recall rate among different algorithms</p>
                    <p class="img_note"></p>
                    <table id="167" border="1"><tr><td><br />数据集</td><td><i>K</i>NN</td><td>Naive Bayes</td><td>CSD-<i>K</i>NN</td><td>CSD-<i>K</i>NN (new) </td></tr><tr><td><br />Breast-cancer</td><td>0.947 54</td><td>0.862 33</td><td>0.975 05</td><td>0.992 38</td></tr><tr><td><br />haberman</td><td>0.355 16</td><td>0.171 43</td><td>0.619 84</td><td>0.923 48</td></tr><tr><td><br />glass</td><td>0.580 16</td><td>0.668 10</td><td>0.726 59</td><td>0.864 08</td></tr><tr><td><br />diabetes</td><td>0.541 94</td><td>0.766 67</td><td>0.748 33</td><td>0.836 95</td></tr><tr><td><br />balance</td><td>0.206 82</td><td>0.352 46</td><td>0.834 14</td><td>0.956 97</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="168">从表4中可以直观地看到, 改进后的算法对少数类的正确分类更为注重, 具备更强的代价敏感特性。改进后的算法是代价敏感的, 并且根据<i>K</i>NN进行近邻删减后的数据集也可以提升算法的代价敏感性。</p>
                </div>
                <div class="p1">
                    <p id="169">从图1中可以发现, CSD-<i>K</i>NN算法是一种牺牲了部分分类准确率达到对少数类的高召回率的代价敏感分类算法。相比<i>K</i>NN算法:改进后的算法的分类准确率降低了约6.4个百分点, 对于整体的分类效果影响不大;而对于少数类的召回率提高了约25.4个百分点, 性能明显优于传统的<i>K</i>NN算法。同时, 基于样本删减策略的CSD-<i>K</i>NN算法可以有效提高算法的分类准确率, 相比<i>K</i>NN算法, 改进后算法的分类准确率提升了约0.8个百分点, 同时召回率也明显提升。实验证明, CSD-<i>K</i>NN算法具备代价敏感性, 并且对于整体分类准确率的影响也较小, 而样本删减策略可以有效地减少分类错误。基于样本删减策略的CSD-<i>K</i>NN算法在性能上明显优于传统的<i>K</i>NN算法。</p>
                </div>
                <h4 class="anchor-tag" id="170" name="170">4.4 CSD-<i>K</i>NN<b>算法集成测试</b></h4>
                <div class="p1">
                    <p id="171">在第3个实验中, 集成算法在调整权重之前, 给每个样本赋予代价初值使得集成后的算法具备代价敏感性, 将K<i>NN</i>算法与集成的K<i>NN</i>算法以及集成代价敏感的<i>CSD</i>-K<i>NN</i>算法进行性能比较, 分析对代价敏感的基学习器进行集成能否得到代价敏感性能更好的集成算法, 实验结果如图2所示。</p>
                </div>
                <div class="area_img" id="172">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907003_172.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 4种算法在不同数据集上的分类准确率对比" src="Detail/GetImg?filename=images/JSJY201907003_172.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 4种算法在不同数据集上的分类准确率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907003_172.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 1 <i>Comparison of classification accuracy of</i><i>four algorithms on different datasets</i></p>

                </div>
                <div class="area_img" id="173">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907003_173.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 3种算法的代价敏感性能对比" src="Detail/GetImg?filename=images/JSJY201907003_173.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 3种算法的代价敏感性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907003_173.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Comparison of cost sensitive performance of three algorithms</i></p>

                </div>
                <div class="p1">
                    <p id="174">实验结果表明, 对权重赋代价初值可以得到代价敏感的集成算法, 而相比集成K<i>NN</i>算法, 对<i>CSD</i>-K<i>NN</i>算法进行集成的高代价错误率降低了4.01个百分点, 证明对代价敏感的基分类器进行集成, 对于降低平均误分类代价效果更好。</p>
                </div>
                <div class="p1">
                    <p id="175">在整个实验中对于少数类的分类准确率, 改进后的算法相比K<i>NN</i>算法提升了38.8个百分点, 同时在整体性能上也优于原先的算法, 而集成这种代价敏感的基分类算法使得算法的代价敏感性能更好, 高代价错误率降低了14.01个百分点, 平均误分代价降低了31.35个百分点。</p>
                </div>
                <h3 id="176" name="176" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="177">在实际的应用中, 针对不同的类别情况赋予不同的代价显得更为可行, 代价敏感学习算法在对不均衡数据集进行分类时性能实际意义明显优于传统的分类算法。本文提出的<i>CSD</i>-K<i>NN</i>集成算法不仅提出了一种应用于不均衡数据集的样本删减策略, 同时还提出了一种代价敏感算法, 通过不断的迭代使得分类器的错分代价降至最低, 相比K<i>NN</i>算法, 本文算法的高代价错误率和平均误分代价都显著降低了, 同时整体分类性能更好。</p>
                </div>
                <div class="p1">
                    <p id="178">需要指出, <i>CSD</i>-K<i>NN</i>算法在软件缺陷预测、文本分类、聚类分析等诸多领域有着良好的效果, 但是本文研究也存在不足之处, 在进行样本选择时, 压缩样本数量、选取典型样本、大幅度减少样本集数量将是下一阶段的研究目标。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="196">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201611017&amp;v=Mjk1NzN5L25WTDdBTHl2U2RMRzRIOWZOcm85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 熊冰妍, 王国胤, 邓维斌.基于样本权重的不平衡数据欠抽样方法[J].计算机研究与发展, 2016, 53 (11) :2613-2622. (XIONG B Y, WANG G Y, DENG W B.Under-sampling method based on sample weight for imbalanced data [J].Journal of Computer Research and Development, 2016, 53 (11) :2613-2622.) 
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 CHENG F, ZHANG J, WEN C.Cost-sensitive large margin distribution machine for imbalanced data classification [J].Pattern Recognition Letters, 2016, 80:107-112.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES163B975DE0422E2DA894120FBC36D737&amp;v=MTQ0NDFhQT1OaWZPZmJLK0hhUEZxSW94RWVzTERuNU16V0ppNGpaNVNYM2kybUJHZXJUZ1FybVlDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3E0dw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> CAO C J, WANG Z.IMCStacking:cost-sensitive stacking learning with feature inverse mapping for imbalanced problems [J].Knowledge-Based Systems, 2018, 150:27-37.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB3C2A610625F5E91DA8AF44D3D452B82&amp;v=MTQ2NjFudm0yQkZCZmJlV043S2RDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3E0d2FBPU5pZk9mY0c3YmRPOXFZNUZZdWtLZW5sTXhoZG5temNNUA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> PINAR T, LALE O, SINEM K, et al.A cost-sensitive classification algorithm:BEE-miner [J].Knowledge-Based Systems, 2016, 95:99-113.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000009793&amp;v=MTY1ODl4bz1OaWZJWTdLN0h0ak5yNDlGWk9zR0MzVTZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXdUYQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> LOMAX S, VADERA S.A survey of cost-sensitive decision tree induction algorithms [J].ACM Computing Surveys, 2013, 45 (2) :16-50.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201705034&amp;v=MjUwMzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTDdBUFRYY2RyRzRIOWJNcW85R1k=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 陈永辉, 岳丽华.特征敏感的点云重采样算法[J].小型微型计算机系统, 2017, 38 (5) :1086-1090. (CHEN Y H, YUE L H.Point cloud resampling algorithm of feature-sensitivity [J].Journal of Chinese Computer Systems, 2017, 38 (5) :1086-1090.) 
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201604017&amp;v=MDU2ODBGckNVUjdxZlp1WnNGeS9uVkw3QUx5dlNkTEc0SDlmTXE0OUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 陈海鹏, 申铉京, 龙建武.采用高斯拟合的全局阈值算法阈值优化框架[J].计算机研究与发展, 2016, 53 (4) :892-903. (CHEN H P, SHEN X J, LONG J W.Threshold optimization framework of global thresholding algorithms using Gaussian fitting [J].Journal of Computer Research and Development, 2016, 53 (4) :892-903.) 
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201405003&amp;v=MTQ0MTVGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblZMN0FMejdTWkxHNEg5WE1xbzk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 李勇, 刘战东, 张海军.不平衡数据的集成分类算法综述[J].计算机应用研究, 2014, 31 (5) :1287-1291. (LI Y, LIU Z D, ZHANG H J.A survey of integrated classification algorithms for unbalanced data [J].Application Research of Computers, 2014, 31 (5) :1287-1291.) 
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 DOMINGOS P.MetaCost:a general method for making classifiers cost-sensitive[C]// Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:155-164.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201609007&amp;v=MTczNTNvOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVkw3QUx5dlNkTEc0SDlmTXA=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 周宇航, 周志华.代价敏感大间隔分布学习机[J].计算机研究与发展, 2016, 53 (9) :1964-1970. (ZHOU Y H, ZHOU Z H.Cost sensitive large interval distribution learning machine [J].Journal of Computer Research and Development, 2016, 53 (9) :1964-1970.) 
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES120D7CC8DF6B69F1927FF7C0270AF733&amp;v=MzE5NDFKMEpmbm93dVJjYTZEZ0xQbmlSckJBeWVjUGlRcm1jQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxNHdhQT1OaWZPZmJLNkhxWEwzUHhORQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> BAHNSEN A C, AOUADA D, OTTERSTEN B.Example-dependent cost-sensitive decision trees [J].Expert Systems with Applications, 2015, 42 (19) :6609-6619.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13101000016317&amp;v=Mjc4NDRRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VGF4bz1OajdCYXJLN0g5SE5yNDlGWk9vSkQzMCtvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> GHAZIKHANI A, MONSEFI R, YAZDI H S.Online cost-sensitive neural network classifiers for non-stationary and imbalanced data streams [J].Neural Computing and Applications, 2013, 23 (5) :1283-1295.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201406005&amp;v=MDExMTdCdEdGckNVUjdxZlp1WnNGeS9uVkw3QUtDTGZZYkc0SDlYTXFZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 付忠良.多标签代价敏感分类集成学习算法[J].自动化学报, 2014, 40 (6) :1075-1085. (FU Z L.Multi-tag cost sensitive classification integrated learning algorithm [J].Acta Automatica Sinica, 2014, 40 (6) :1075-1085.) 
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201110041&amp;v=MDc1NTQvblZMN0FMejdCYjdHNEg5RE5yNDlCWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 王茜, 杨正宽.一种基于加权KNN的大数据集下离群检测算法[J].计算机科学, 2011, 38 (10) :177-180. (WANG Q, YANG Z K.An outlier detection algorithm for big data sets based on weighted KNN [J].Computer Science, 2011, 38 (10) :177-180.) 
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 FREUND Y, IYER R, SCHAPIRE R, et al.An efficient boosting algorithm for combining preferences [J].Journal of Machine Learning Research, 2003, 4 (6) :170-178.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201703031&amp;v=MTE4MzFNckk5R1pZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTDdBTHo3TWFiRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 胡小生, 钟勇.基于边界样本选择的支持向量机加速算法[J].计算机工程与应用, 2017, 53 (3) :169-173. (HU X S, ZHONG Y.Support vector machine acceleration algorithm based on boundary sample selection [J].Computer Engineering and Applications, 2017, 53 (3) :169-173.) 
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ2011S1066&amp;v=MTMyNzJybzlEWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblZMN0FMeXZTZExHNEg5Q3Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 蒋盛益, 谢照青, 余雯.基于代价敏感的朴素贝叶斯不平衡数据分类研究[J].计算机研究与发展, 2011, 48 (增刊I) :387-390. (JIANG S Y, XIE Z Q, YU W.Cost-sensitive naive Bayesian unbalanced data classification [J].Journal of Computer Research and Development, 2011, 48 (Suppl I) :387-390.) 
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739373&amp;v=MjIzMTE3SHRETnFZOUZZK2dHRDNzNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1RheG89TmlmT2ZiSw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> SUN Y, KAMEL M S, WONG A K, et al.Cost-sensitive boosting for classification of imbalanced data [J].Pattern Recognition, 2007, 40 (12) :3358-3378.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                 SUN Y, WONG A K, WANG Y.Parameter inference of cost-sensitive boosting algorithms [C]// Proceedings of the 4th International Conference on Machine Learning and Data Mining in Pattern Recognition.Berlin:Springer, 2005:21-30.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201311011&amp;v=MzIzNDZHNEg5TE5ybzlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblZMN0FOeWZUYkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 曹莹, 苗启广, 刘家辰, 等.具有Fisher一致性的代价敏感Boosting算法[J].软件学报, 2013, 24 (11) :2584-2596. (CAO Y, MIAO Q G, LIU J C, et al.Fisher consistent cost sensitive Boosting algorithm [J].Journal of Software, 2013, 24 (11) :2584-2596.) 
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 周志华.机器学习[M].北京:清华大学出版社, 2018:30-33. (ZHOU Z H.Machine Learning [M].Beijing:Tsinghua University Press, 2018:30-33.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907003" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907003&amp;v=MTgxODc0SDlqTXFJOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVkw3Qkx6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
