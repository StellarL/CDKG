<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136466473565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909012%26RESULT%3d1%26SIGN%3dVNRPsC1d2az1A%252bYYcjN9w%252bMnolI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909012&amp;v=MjI5NTR6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXJyTEx6N0JkN0c0SDlqTXBvOUVab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="1.1 RNP&lt;b&gt;模型&lt;/b&gt;">1.1 RNP<b>模型</b></a></li>
                                                <li><a href="#55" data-title="1.2 &lt;b&gt;协同正则化最近点模型&lt;/b&gt;(CRNP)">1.2 <b>协同正则化最近点模型</b>(CRNP)</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="2 本文方法 ">2 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="2.1 &lt;b&gt;目标函数的设计&lt;/b&gt;">2.1 <b>目标函数的设计</b></a></li>
                                                <li><a href="#75" data-title="2.2 &lt;b&gt;优化求解&lt;/b&gt;">2.2 <b>优化求解</b></a></li>
                                                <li><a href="#99" data-title="2.3 &lt;b&gt;分类&lt;/b&gt;">2.3 <b>分类</b></a></li>
                                                <li><a href="#110" data-title="2.4 &lt;b&gt;算法时间复杂度&lt;/b&gt;">2.4 <b>算法时间复杂度</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="3 实验 ">3 实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#114" data-title="3.1 &lt;b&gt;实验环境搭建&lt;/b&gt;">3.1 <b>实验环境搭建</b></a></li>
                                                <li><a href="#119" data-title="3.2 &lt;b&gt;识别精度评估&lt;/b&gt;">3.2 <b>识别精度评估</b></a></li>
                                                <li><a href="#123" data-title="3.3 &lt;b&gt;鲁棒性评估&lt;/b&gt;">3.3 <b>鲁棒性评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="图1 SRNPC模型">图1 SRNPC模型</a></li>
                                                <li><a href="#98" data-title="图2 绝对误差和熵尺度的比较示意图">图2 绝对误差和熵尺度的比较示意图</a></li>
                                                <li><a href="#118" data-title="图3 人脸数据库部分样本">图3 人脸数据库部分样本</a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同数据集上的分类精度和标准差 单位&lt;/b&gt;:%"><b>表</b>1 <b>不同数据集上的分类精度和标准差 单位</b>:%</a></li>
                                                <li><a href="#125" data-title="图4 被椒盐噪声污染的人脸(噪声百分比分别为0%～50%)">图4 被椒盐噪声污染的人脸(噪声百分比分别为0%～50%)</a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;i&gt;YTC&lt;/i&gt;&lt;b&gt;图像在不同椒盐噪声百分比下的 分类精度 单位&lt;/b&gt;:%"><b>表</b>2 <i>YTC</i><b>图像在不同椒盐噪声百分比下的 分类精度 单位</b>:%</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="167">


                                    <a id="bibliography_1" title=" UZAIR M,SHAFAIT F,GHANEM B,et al.Representation learning with deep extreme learning machines for efficient image set classification[J].Neural Computing and Applications,2018,30(4):1211-1223." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Representation learning with deep extreme learning machines for efficient image set classification">
                                        <b>[1]</b>
                                         UZAIR M,SHAFAIT F,GHANEM B,et al.Representation learning with deep extreme learning machines for efficient image set classification[J].Neural Computing and Applications,2018,30(4):1211-1223.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_2" title=" 李光早,王士同.基于稀疏表示和弹性网络的人脸识别[J].计算机应用,2017,37(3):901-905.(LI G Z,WANG S T.Face recognition based on sparse representation and elastic network [J].Journal of Computer Applications,2017,37(3):901-905.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201703052&amp;v=MTI2Njc0SDliTXJJOUFab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXJyTEx6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         李光早,王士同.基于稀疏表示和弹性网络的人脸识别[J].计算机应用,2017,37(3):901-905.(LI G Z,WANG S T.Face recognition based on sparse representation and elastic network [J].Journal of Computer Applications,2017,37(3):901-905.)
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_3" title=" 刘天赐,史泽林,刘云鹏,等.基于Grassmann流形几何深度网络的图像集识别方法[J].红外与激光工程,2018,47(7):25-31.(LIU T C,SHI Z L,LIU Y P,et al.Geometry deep network image-set recognition method based on Grassmann manifolds [J].Infrared and Laser Engineering,2018,47(7):25-31.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWYJ201807004&amp;v=MjI2NzJGeWpsVXJyTExUclNaTEc0SDluTXFJOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         刘天赐,史泽林,刘云鹏,等.基于Grassmann流形几何深度网络的图像集识别方法[J].红外与激光工程,2018,47(7):25-31.(LIU T C,SHI Z L,LIU Y P,et al.Geometry deep network image-set recognition method based on Grassmann manifolds [J].Infrared and Laser Engineering,2018,47(7):25-31.)
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_4" title=" KIM T,KITTLER J,CIPOLLA R.Discriminative learning and recognition of image set classes using canonical correlations [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2007,29(6):1005-1018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative Learning and Recognition of Image Set Classes Using Canonical Correlations">
                                        <b>[4]</b>
                                         KIM T,KITTLER J,CIPOLLA R.Discriminative learning and recognition of image set classes using canonical correlations [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2007,29(6):1005-1018.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_5" title=" HAYAT M,BENNAMOUN M,AN S.Learning non-linear reconstruction models for image set classification [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2014:1907-1914." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning non-linear reconstruction models for image set classification">
                                        <b>[5]</b>
                                         HAYAT M,BENNAMOUN M,AN S.Learning non-linear reconstruction models for image set classification [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2014:1907-1914.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_6" title=" 郑萍萍,李波,丁玉琳.基于局部邻域多流形度量的人脸识别[J].计算机应用研究,2018,35(4):1250-1253.(ZHENG P P,LI B,DING Y L.Local neighborhood based multi-manifold metric learning for face recognition[J].Application Research of Computers,2018,35(4):1250-1253.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201804063&amp;v=MDcyNjhIOW5NcTQ5RFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVcnJMTHo3U1pMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         郑萍萍,李波,丁玉琳.基于局部邻域多流形度量的人脸识别[J].计算机应用研究,2018,35(4):1250-1253.(ZHENG P P,LI B,DING Y L.Local neighborhood based multi-manifold metric learning for face recognition[J].Application Research of Computers,2018,35(4):1250-1253.)
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_7" title=" WANG R,GUO H,DAVIS L S,et al.Covariance discriminative learning:A natural and efficient approach to image set classification[C]// Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:2496-2503." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Covariance discriminative learning:a natural and efficient approach to image set classification C]">
                                        <b>[7]</b>
                                         WANG R,GUO H,DAVIS L S,et al.Covariance discriminative learning:A natural and efficient approach to image set classification[C]// Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:2496-2503.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_8" title=" HU Y,MIAN A S,OWENS R.Face recognition using sparse approximated nearest points between image sets [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(10):1992-2004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face Recognition Using Sparse Approximated Nearest Points between Image Sets">
                                        <b>[8]</b>
                                         HU Y,MIAN A S,OWENS R.Face recognition using sparse approximated nearest points between image sets [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(10):1992-2004.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_9" title=" YANG M,ZHU P,van GOOL L,et al.Face recognition based on regularized nearest points between image sets [C]// Proceedings of the 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Piscataway,NJ:IEEE,2013:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition based on regularized nearest points between image sets">
                                        <b>[9]</b>
                                         YANG M,ZHU P,van GOOL L,et al.Face recognition based on regularized nearest points between image sets [C]// Proceedings of the 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Piscataway,NJ:IEEE,2013:1-7.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_10" title=" WU Y,MINOH M,MUKUNOKI M.Collaboratively regularized nearest points for set based recognition [C]// Proceedings of the 2013 British Machine Vision Conference.Durham:BMVA Press,2013:No.0134." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Collaboratively regularized nearest points for set based recognition">
                                        <b>[10]</b>
                                         WU Y,MINOH M,MUKUNOKI M.Collaboratively regularized nearest points for set based recognition [C]// Proceedings of the 2013 British Machine Vision Conference.Durham:BMVA Press,2013:No.0134.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_11" title=" 唐宋,陈利娟,陈志贤,等.基于目标域局部近邻几何信息的域自适应图像分类方法[J].计算机应用,2017,37(4):1164-1168.(TANG S,CHEN L J,CHEN Z X,et al.Domain adaptation image classification based on target local-neighbor geometrical information [J].Journal of Computer Applications,2017,37(4):1164-1168.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201704045&amp;v=MDYwNDZxZlp1WnNGeWpsVXJyTEx6N0JkN0c0SDliTXE0OUJZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         唐宋,陈利娟,陈志贤,等.基于目标域局部近邻几何信息的域自适应图像分类方法[J].计算机应用,2017,37(4):1164-1168.(TANG S,CHEN L J,CHEN Z X,et al.Domain adaptation image classification based on target local-neighbor geometrical information [J].Journal of Computer Applications,2017,37(4):1164-1168.)
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_12" title=" CAO F,YANG Z,REN J,et al.Sparse representation-based augmented multinomial logistic extreme learning machine with weighted composite features for spectral-spatial classification of hyperspectral images [J].IEEE Transactions on Geoscience and Remote Sensing,2018,56(11):6263-6279." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse representation-based augmented multinomial logistic extreme learning machine with weighted composite features for spectral-spatial classification of hyperspectral images">
                                        <b>[12]</b>
                                         CAO F,YANG Z,REN J,et al.Sparse representation-based augmented multinomial logistic extreme learning machine with weighted composite features for spectral-spatial classification of hyperspectral images [J].IEEE Transactions on Geoscience and Remote Sensing,2018,56(11):6263-6279.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_13" title=" YANG M,LIU W,SHEN L,et al.Joint regularized nearest points for image set based face recognition [C]// Proceedings of the 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Washington,DC:IEEE Computer Society,2015:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint regularized nearest points for image set based face recognition">
                                        <b>[13]</b>
                                         YANG M,LIU W,SHEN L,et al.Joint regularized nearest points for image set based face recognition [C]// Proceedings of the 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Washington,DC:IEEE Computer Society,2015:1-7.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_14" title=" LIU W,POKHAREL P P,PRINCIPE J C.Correntropy:properties and applications in non-Gaussian signal processing [J].IEEE Transactions on Signal Processing,2007,55(11):5286-5298." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Correntropy: Properties and Applications in Non-Gaussian Signal Processing">
                                        <b>[14]</b>
                                         LIU W,POKHAREL P P,PRINCIPE J C.Correntropy:properties and applications in non-Gaussian signal processing [J].IEEE Transactions on Signal Processing,2007,55(11):5286-5298.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_15" title=" CHEN B,WANG X,LU N,et al.Mixture correntropy for robust learning[J].Pattern Recognition,2018,79:318-327." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9C0205EFBC7CB0D0FF334A9029824FBE&amp;v=MTUzNjJzPU5pZk9mYnJMSHRQTXF2b3pGcGdJZnc0NXV4WmxuRHgrVEE3cnJCQThjYkNRTThqcUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMaSt4YQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         CHEN B,WANG X,LU N,et al.Mixture correntropy for robust learning[J].Pattern Recognition,2018,79:318-327.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_16" title=" GOU G,SHI J,XIONG G,et al.Image-set based collaborative representation for face recognition in videos [C]// Proceedings of the 2017 Pacific Rim Conference on Multimedia,LNCS 10736.Berlin:Springer,2017:498-507." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image-set based collaborative representation for face recognition in videos">
                                        <b>[16]</b>
                                         GOU G,SHI J,XIONG G,et al.Image-set based collaborative representation for face recognition in videos [C]// Proceedings of the 2017 Pacific Rim Conference on Multimedia,LNCS 10736.Berlin:Springer,2017:498-507.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_17" title=" HAYAT M,KHAN S H,BENNAMOUN M.Empowering simple binary classifiers for image set based face recognition [J].International Journal of Computer Vision,2017,123(3):479-498." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDC123F4FDBA247EF3425817FB32D10CDE&amp;v=MDM2MDBxL2t4RnBvTkNIdE11UlVYNkRwMVNYaVUzaEUzRGJPVU5zN3FDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TGkreGFzPU5qN0Jhc0M1SE5LNg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         HAYAT M,KHAN S H,BENNAMOUN M.Empowering simple binary classifiers for image set based face recognition [J].International Journal of Computer Vision,2017,123(3):479-498.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_18" title=" FERRARI C,BERRETTI S,del BIMBO A.Extended YouTube faces:a dataset for heterogeneous open-set face identification [C]// Proceedings of the 2018 24th International Conference on Pattern Recognition.Piscataway,NJ:IEEE,2018:3408-3413." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extended YouTube faces:a dataset for heterogeneous open-set face identification">
                                        <b>[18]</b>
                                         FERRARI C,BERRETTI S,del BIMBO A.Extended YouTube faces:a dataset for heterogeneous open-set face identification [C]// Proceedings of the 2018 24th International Conference on Pattern Recognition.Piscataway,NJ:IEEE,2018:3408-3413.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-08 10:47</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2547-2551 DOI:10.11772/j.issn.1001-9081.2019030463            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于熵自加权联合正则化最近点的图像集分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%BB%E7%8F%8D%E6%96%87&amp;code=28443210&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">任珍文</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%98%8E%E5%A8%9C&amp;code=41184102&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴明娜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%AD%A6%E9%99%A2&amp;code=0189969&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南科技大学国防科技学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0077991&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京理工大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>图像集分类算法通过充分利用图像的集合信息来提高识别性能,得到了广泛的关注。但是现有的图像集分类算法存在如下问题:1)需要样本满足某种概率统计分布;2)忽略了图库集类与类之间的互斥性;3)对非高斯噪声不具备鲁棒性。为了解决上述问题,提出了一种基于熵自加权联合正则化最近点的图像集分类算法(SRNPC)。首先在测试集中寻找唯一的全局联合正则化最近点,同时最小化该点与每个图库集中正则化最近点之间的距离;然后,为了增强类之间的判别力以及对非高斯噪声的鲁棒性,引入一种基于熵尺度的自加权策略来迭代更新测试集与各个图库集合之间的熵加权权重,得到的权重能够直接反映测试集与每个图库集之间相关性的高低;最后,利用测试集和每个图库集之间的最小残差值获得分类结果。通过在UCSD/Honda、CMU Mobo和YouTube这三个公开数据集上与当前主流的算法进行的对比实验结果表明,所提出的算法具有更高的分类精度和更强的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E9%9B%86%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像集分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%80%E8%BF%91%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">正则化最近点;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%AF%B9%E7%86%B5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相对熵;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模式识别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *任珍文(1987—),男,四川南充人,讲师,博士,主要研究方向:机器学习、计算机视觉、压缩感知;电子邮箱rzw@njust.edu.cn;
                                </span>
                                <span>
                                    吴明娜(1998—),女,安徽宿州人,主要研究方向:机器学习、计算机视觉。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61673220);</span>
                                <span>国家国防科技工业局项目(JCKY2017209B010,JCKY2018209B001);</span>
                                <span>四川省省工办项目(ZYF-2018-106);</span>
                                <span>西南科技大学大学生创新基金资助项目(cx18-029);</span>
                    </p>
            </div>
                    <h1><b>Correntropy self-weighted based joint regularized nearest points for images set classification algorithm</b></h1>
                    <h2>
                    <span>REN Zhenwen</span>
                    <span>WU Mingna</span>
            </h2>
                    <h2>
                    <span>School of National Defence Science and Technology, Southwest University of Science and Technology</span>
                    <span>School of Computer Science and Engineering, Nanjing University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Image set classification algorithms, which make full use of the image set information to improve the recognition performance, have gained much attention. However, existing image set classification algorithms have the following problems: 1) samples need to obey a certain probability and statistical distribution; 2) ignoring the mutual exclusion between classes in the gallery set; 3) without robustness against non-Gaussian noise. In order to solve the above problems, an image set classification algorithm based on Correntropy Self-weighted based joint Regularization of Nearest Points(SRNPC) was proposed. Firstly, the unique global joint regularization nearest point in the test set was found and the distance between this point and the regularization nearest point in each gallery set was minimized simultaneously. Then, to enhance the discrimination between classes and the robustness against non-Gaussian noise, a self-weighting strategy based on correntropy scale was introduced to update the correntropy weight between the test set and each gallery set iteratively. And the obtained weight was able to directly reflect the correlation between the test set and each gallery set. Finally, the classification result was obtained by using the minimum residual value between the test set and each gallery set. Experimental results on three open datasets UCSD/Honda, CMU Mobo and YouTube show that SRNPC has higher classification accuracy and better robustness than many state-of-the-art image classification algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20set%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image set classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=regularized%20nearest%20point&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">regularized nearest point;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correntropy&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correntropy;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pattern%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pattern recognition;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    REN Zhenwen, born in 1987, Ph. D. , lecturer. His research interests include machine learning, computer vision, compressed sensing. ;
                                </span>
                                <span>
                                    WU Mingna, born in 1998. Her research interests include machine learning, computer vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-20</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61673220);</span>
                                <span>the Project of State Administration of Science,Technology and Industry for National Defense(JCKY2017209B010,JCKY2018209B001);</span>
                                <span>the Project of Sichuan Office of Science,Technology and Industry for National Defense(ZYF-2018-106);</span>
                                <span>the Innovation Foundation for College Students of Southwest University of Science and Technology(cx18-029);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="40">基于图像集的图像识别技术已经成为计算机视觉领域和模式识别领域的研究热点之一。与传统的基于单幅静态图像的识别技术相比,基于图像集的识别算法以目标对象的多幅图像或视频序列构成集合为单位,通过利用图像集附加的集合信息更全面地获取对象特征,进而提高图像识别的识别精度和鲁棒性,在实际的人脸识别与目标识别中得到了广泛的应用<citation id="203" type="reference"><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="41">总体来讲,基于图像集的分类涉及两个关键问题:如何有效地对图像集建模;如何适当地度量两个集合之间相似度。为了解决这两个问题,现有的图像集分类方法可分为有参方法和无参方法。由于有参方法参数估计困难且无法很好地拟合实际情况,因此当前的图像识别技术更倾向于无参方法<citation id="204" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。现有的先进的无参方法大多基于子空间、流形、仿射包或协方差矩阵。文献<citation id="205" type="reference">[<a class="sup">4</a>]</citation>通过同时最大化类内离散度,最小化类间离散度来获取投影矩阵,利用典型相关性来度量集合之间的相似性。文献<citation id="206" type="reference">[<a class="sup">5</a>]</citation>利用流形将每个图像集建模为一组局部子空间,同时定义图像集之间的距离为相应子空间之间距离的加权平均值。文献<citation id="207" type="reference">[<a class="sup">6</a>]</citation>将图像集建模成仿射包,图像集之间的相似度被定义为两个仿射包中最近点之间的距离。文献<citation id="208" type="reference">[<a class="sup">7</a>]</citation>利用协方差矩阵表示图像集,通过黎曼流形到欧氏空间的映射来度量图像集之间的相似度。随着压缩感知的深入学习和广泛应用,近期,Hu等<citation id="209" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出基于稀疏表示的分类器(Sparse Approximated Nearest Points, SANP)来度量集合与集合之间的相似度,但是SANP涉及较多的参数和未知辅助变量,求解较复杂。为了降低SANP模型的复杂性,文献<citation id="210" type="reference">[<a class="sup">9</a>]</citation>提出一种简化的模型——正则最近点(Regularized Nearest Points, RNP)模型,该方法将每个图像集建模为一个正则化的仿射包,利用正则化的最近点对来测量图像集之间的相似性。但在RNP中,对于不同的训练集,测试集中与之对应的正则化最近点不唯一,很容易出现过度拟合,导致分类不稳定。在RNP和协同稀疏表示的启发下,文献<citation id="211" type="reference">[<a class="sup">10</a>]</citation>提出协同正则化最近点(Collaboratively Regularized Nearest Points, CRNP)模型,基于协同表示同时找到所有的正则化最近点,但该算法只考虑测试集和整个图库集之间的相似度而忽略了测试集和每个图库集间的相关性,未能充分利用集合之间的判别信息。</p>
                </div>
                <div class="p1">
                    <p id="42">随着图像集分类算法的进一步研究,加权的思想在越来越多的算法中得到应用<citation id="215" type="reference"><link href="187" rel="bibliography" /><link href="189" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。其核心思想是通过加权策略使得正确的类具有高权重,错误类具有低权重,进而增强测试集与正确类之间的相关性以及与错误类之间的互斥性。从广义上来说,在SANP、RNP和CRNP中,测试集与每个图库集之间的加权权重均设置为1,即认为每个图库集同等重要。为了增强测试集与正确类之间的相关性,联合正则化最近点(Joint Regularized Nearest Points, JRNP)模型<citation id="212" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>通过计算测试集与每个图库集之间的欧氏距离来获得不同的权重。但实际的图像集分类任务中往往存在多种随机噪声(如:非高斯噪声),上述算法对该类噪声的鲁棒性较低<citation id="213" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。随着对信息理论的不断学习和扩展,Liu等<citation id="214" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出通过熵尺度能够有效地处理非高斯噪声对图像识别的影响,从而提高算法的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="43">针对以上问题,本文提出了一种基于熵自加权联合正则化最近点的图像集分类方法(Correntropy Self-weighted based joint Regularized Nearest Points, SRNPC)。该方法首先在测试集中寻找唯一的全局联合正则化最近点的同时,最小化测试集与每个图库集之间的距离。在熵尺度的启发下,进一步引入了一种基于熵的自适应加权策略来增强类间判别力和对非高斯噪声的鲁棒性。接着发展了一种交替迭代的优化求解算法快速有效地得到目标函数的最优解。最后,采用残差判别函数完成图像分类。通过在UCSD/Honda、CMU Mobo和YouTube数据集上与多种其他流行图像集分类算法(典型相关法(Discriminative learning and recognition using Canonical Correlations, DCC)<citation id="216" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、流行判别分析法(Manifold-Manifold Distance, MMD)<citation id="217" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、基于图像集的人脸识别AHISD/CHISD(Affine/Convex Hull Image Set Distance)算法<citation id="218" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、SANP<citation id="219" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、RHP<citation id="220" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、CRNP<citation id="221" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、JRNP<citation id="222" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>)的对比实验证实了SRNPC具有更高的识别精度和鲁棒性。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="45" name="45">1.1 RNP<b>模型</b></h4>
                <div class="p1">
                    <p id="46">在正则化最近点(RNP)模型中,每个图像集被建模成一个正则化的仿射包:</p>
                </div>
                <div class="p1">
                    <p id="47" class="code-formula">
                        <mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>A</mi><mi>Η</mi><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">x</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">α</mi><mo stretchy="false">|</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mn>1</mn><mo>,</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mi>σ</mi><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="48">其中:<b><i>X</i></b><sub><i>i</i></sub>=[<b><i>x</i></b><sub><i>i</i>,1</sub>,<b><i>x</i></b><sub><i>i</i>,2</sub>,…,<b><i>x</i></b><sub><i>i</i>,<i>n</i><sub><i>i</i></sub></sub>]表示为图库集中第<i>i</i>类样本集合,<i>n</i><sub><i>i</i></sub>为第<i>i</i>类的图像个数,<b><i>x</i></b><sub><i>i</i>,<i>k</i></sub>为第<i>i</i>类的第<i>k</i>个特征向量。对于给定的测试集<b><i>Q</i></b>和图库集<b><i>X</i></b><sub><i>i</i></sub>,RNP算法通过解决如下目标函数来得到最近点:</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">α</mi><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">β</mi></mrow></munder><mspace width="0.25em" /><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Q</mi><mi mathvariant="bold-italic">α</mi><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mn>1</mn><mo>,</mo><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>β</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>,</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mi>σ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mspace width="0.25em" /><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mi>σ</mi><msub><mrow></mrow><mn>2</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">其中: <i>β</i><sub><i>i</i>,<i>k</i></sub>为<i>β</i><sub><i>i</i></sub>的第<i>k</i>项,<b><i>Q</i></b><i>α</i>和<b><i>X</i></b><sub><i>i</i></sub><i>β</i><sub><i>i</i></sub>分别为在测试集<b><i>Q</i></b>和图库集<b><i>X</i></b><sub><i>i</i></sub>中对应的正则化最近点。</p>
                </div>
                <div class="p1">
                    <p id="51">计算得到<i>α</i><sup>*</sup>和<i>β</i><sup>*</sup>后,图像集<b><i>Q</i></b>和<b><i>X</i></b><sub><i>i</i></sub>之间的距离为:</p>
                </div>
                <div class="p1">
                    <p id="52"><i>d</i><sub><i>i</i></sub> = (‖<b><i>Q</i></b>‖<sub>*</sub> + ‖<b><i>X</i></b><sub><i>i</i></sub>‖<sub>*</sub>)*‖<b><i>Q</i></b><i>α</i><sup>*</sup>-<b><i>X</i></b><sub><i>i</i></sub><i>β</i><sup>*</sup><sub><i>i</i></sub>‖<mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="54">其中:‖<b><i>Q</i></b>‖<sub>*</sub>是<b><i>Q</i></b>的核范数,即:<b><i>Q</i></b>的奇异值之和。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">1.2 <b>协同正则化最近点模型</b>(CRNP)</h4>
                <div class="p1">
                    <p id="56">与RNP不同,对于给定的测试集<b><i>Q</i></b>和图库集<b><i>X</i></b>=[<b><i>X</i></b><sub>1</sub>,<b><i>X</i></b><sub>2</sub>,…,<b><i>X</i></b><sub><i>c</i></sub>],CRNP直接最小化测试集<b><i>Q</i></b>与整个图库集<b><i>X</i></b>的距离:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">α</mi><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">β</mi></mrow></munder><mo stretchy="false">{</mo><mo stretchy="false">∥</mo><mrow><mi mathvariant="bold-italic">Q</mi><mi mathvariant="bold-italic">α</mi></mrow><mo>-</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">β</mi></mrow><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">β</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">}</mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mn>1</mn><mo>,</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>β</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup><mo>=</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中:<i>λ</i><sub>1</sub>、<i>λ</i><sub>2</sub>为平衡参数, <i>β</i>=[<i>β</i><sub>1</sub>; <i>β</i><sub>2</sub>;…; <i>β</i><sub><i>n</i></sub>],<i>β</i><sub><i>i</i></sub>为与<b><i>X</i></b><sub><i>i</i></sub>对应的子仿射系数向量。<b><i>Q</i></b><i>α</i>和<b><i>X</i></b><i>β</i>分别为在测试集和整个图库集中的正则化最近点。</p>
                </div>
                <div class="p1">
                    <p id="59">得到系数<i>α</i><sup>*</sup>和<i>β</i><sup>*</sup>后,<b><i>Q</i></b>和<b><i>X</i></b><sub><i>i</i></sub>之间的类间距离为:</p>
                </div>
                <div class="p1">
                    <p id="60"><i>d</i><sub><i>i</i></sub>=(‖<b><i>Q</i></b>‖<sub>*</sub>+‖<b><i>X</i></b><sub><i>i</i></sub>‖<sub>*</sub>)·‖<b><i>Q</i></b><i>α</i><sup>*</sup>-<b><i>X</i></b><sub><i>i</i></sub><i>β</i><sup>*</sup><sub><i>i</i></sub>‖<mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>/‖<i>β</i><sup>*</sup><sub><i>i</i></sub>‖<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="63">尽管与其他的图像集分类方法相比,RNP和CRNP已经取得了令人满意的性能,但在这两种方法中仍然存在一些问题需要解决(详见引言)。为了解决这些问题,本文提出一种基于熵自加权联合正则化最近点的图像集分类算法(SRNPC)。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">2 本文方法</h3>
                <h4 class="anchor-tag" id="65" name="65">2.1 <b>目标函数的设计</b></h4>
                <div class="p1">
                    <p id="66">对于一个给定的测试集<b><i>Y</i></b>=[<b><i>y</i></b><sub>1</sub>,<b><i>y</i></b><sub>2</sub>,…,<b><i>y</i></b><sub><i>n</i><sub><i>y</i></sub></sub>](<b><i>y</i></b><sub><i>k</i></sub>表示第<i>k</i>个特征向量)和整个图像集<b><i>X</i></b>=[<b><i>X</i></b><sub>1</sub>,<b><i>X</i></b><sub>2</sub>,…,<b><i>X</i></b><sub><i>c</i></sub>],其中<b><i>X</i></b><sub><i>i</i></sub>=[<b><i>x</i></b><sub><i>i</i>,1</sub>,<b><i>x</i></b><sub><i>i</i>,2</sub>,…,<b><i>x</i></b><sub><i>i</i>,<i>n</i><sub><i>i</i></sub></sub>]。为了充分利用测试集与整个图库集以及测试集与每个图库集之间的相关性和差异性,本文首先在<b><i>Y</i></b>中寻找一个唯一的正则化最近点。该点不仅靠近整个图库集<b><i>X</i></b>中的协同正则化最近点,还靠近正确类<b><i>X</i></b><sub><i>i</i></sub>中的正则化最近点。学习模型如下:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">α</mi><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">β</mi></mrow></munder><mo stretchy="false">{</mo><mo stretchy="false">∥</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">β</mi></mrow><mo>-</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">α</mi></mrow><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mn>1</mn></mstyle><mo>—</mo></mover><msub><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>y</mi></msub></mrow></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">∥</mo><mi>β</mi><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mn>1</mn></mstyle><mo>—</mo></mover><msub><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>x</mi></msub></mrow></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>γ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mo stretchy="false">∥</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">β</mi></mrow><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>β</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中:<i>λ</i><sub>1</sub>、<i>λ</i><sub>2</sub>和<i>γ</i>为平衡参数,<i>α</i>和<i>β</i>表示描述系数。<i>N</i><sub><i>y</i></sub>和<i>N</i><sub><i>x</i></sub>分别是测试集<b><i>Y</i></b>和图库集<b><i>X</i></b>中的样本数量,<i>c</i>为图像集中类的个数。1<sub><i>N</i></sub>表示元素全为1的<i>N</i>维列向量,<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mn>1</mn></mstyle><mo>—</mo></mover><msub><mrow></mrow><mi>Ν</mi></msub><mo>=</mo><mn>1</mn><msub><mrow></mrow><mi>Ν</mi></msub><mo>/</mo><mi>Ν</mi></mrow></math></mathml>。项‖<b><i>Y</i></b><i>β</i>-<b><i>X</i></b><i>α</i>‖<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>表示<b><i>Y</i></b><i>β</i>与整个图库集的协同正则化最近点之间的距离,<mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mn>1</mn></mstyle><mo>—</mo></mover><msub><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>y</mi></msub></mrow></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">β</mi><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mn>1</mn></mstyle><mo>—</mo></mover><msub><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>x</mi></msub></mrow></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>保证描述系数<i>α</i>和<i>β</i>趋近于样本图心,使得获得的最近点更接近真实样本。最后一项<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">β</mi><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>则是<b><i>Y</i></b><i>β</i>与每个图库集中正则化最近点之间距离。其中<i>w</i><sub><i>i</i></sub>为熵尺度加权权重,它的引入进一步确保<b><i>Y</i></b><i>β</i>更加靠近所对应的正确类。仿射约束<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>β</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mn>1</mn></mrow></math></mathml>可以避免出现平凡解(如:<i>α</i>=<i>β</i>=0)。算法模型图如图1所示,可以直观得出,相比RNP,该模型对于不同的图库集,测试集中的正则化最近点<b><i>Y</i></b><i>β</i>始终保持不变,这样避免了RNP中过度拟合情况的出现;相比CRNP,该模型明确地最小化了<b><i>Y</i></b><i>β</i>和每个类之间的距离,很大程度上提高了类之间的判别能力;相比JRNP,该模型利用熵尺度加权权重来度量测试集与每个图库集之间的相似度,使得该模型对非高斯噪声鲁棒。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909012_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SRNPC模型" src="Detail/GetImg?filename=images/JSJY201909012_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SRNPC模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909012_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Model of SRNPC</p>

                </div>
                <h4 class="anchor-tag" id="75" name="75">2.2 <b>优化求解</b></h4>
                <div class="p1">
                    <p id="76">在优化求解过程中,松弛对<i>β</i>的约束,即<mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>β</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>≈</mo><mn>1</mn></mrow></math></mathml>,同时令<mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">R</mi><mo>=</mo><mover><mstyle mathsize="140%" displaystyle="true"><mn>1</mn></mstyle><mo>—</mo></mover><msub><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>y</mi></msub></mrow></msub><mo>,</mo><mi mathvariant="bold-italic">G</mi><mo>=</mo><mover><mstyle mathsize="140%" displaystyle="true"><mn>1</mn></mstyle><mo>—</mo></mover><msub><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>x</mi></msub></mrow></msub></mrow></math></mathml>,则式(6)重写为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">α</mi><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">β</mi></mrow></munder><mo stretchy="false">{</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo stretchy="true">¯</mo></mover><mi mathvariant="bold-italic">β</mi><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo stretchy="true">¯</mo></mover><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo>-</mo><mi mathvariant="bold-italic">R</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">β</mi><mo>-</mo><mi mathvariant="bold-italic">G</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>γ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo stretchy="true">¯</mo></mover><mi mathvariant="bold-italic">β</mi><mo>-</mo><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中:<b><i>z</i></b>=[0;<mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo stretchy="false">]</mo><mo>,</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo stretchy="true">¯</mo></mover><mo>=</mo><mo stretchy="false">[</mo><mo>-</mo><mi mathvariant="bold-italic">Y</mi></mrow></math></mathml>;<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>,</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo stretchy="true">¯</mo></mover><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi></mrow></math></mathml>;<mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>,</mo><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;0]。列向量0,1的大小根据图像集的大小决定。为了快速得到<i>α</i>、 <i>β</i>和<b><i>w</i></b>的最优解,本文提出一种有效的交替迭代优化算法来得到闭式解集,算法更新规则如下:</p>
                </div>
                <div class="p1">
                    <p id="84">当<i>β</i>、<b><i>w</i></b>固定时,<i>α</i>更新为:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula"><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">α</mi><mo>=</mo><mi mathvariant="bold-italic">Ρ</mi><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo stretchy="true">¯</mo></mover><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo stretchy="true">¯</mo></mover><mi mathvariant="bold-italic">β</mi><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mi mathvariant="bold-italic">R</mi><mo stretchy="false">)</mo></mrow></math></mathml>;<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>=</mo><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo stretchy="true">¯</mo></mover><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo stretchy="true">¯</mo></mover><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="88">当<i>α</i>、<b><i>w</i></b>固定时,<i>β</i>更新为:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">β</mi><mo>=</mo><mi mathvariant="bold-italic">Q</mi><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo stretchy="true">¯</mo></mover><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo stretchy="true">¯</mo></mover><mi mathvariant="bold-italic">α</mi><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">G</mi><mo stretchy="false">)</mo></mrow></math></mathml>;<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Q</mi><mo>=</mo><mo stretchy="false">(</mo><mn>2</mn><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo stretchy="true">¯</mo></mover><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo stretchy="true">¯</mo></mover><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="92">为了在不同的类之间引入竞争,从而保证<i><b>Y</b></i><i>β</i>与所对应的正确类之间的逻辑加权距离最短。本文利用一种基于熵的自适应加权策略来更新<i>w</i><sub><i>i</i></sub>。具体来说,熵是一种度量任意两个随机变量(<i>u</i>,<i>v</i>)之间相似度的方法,该尺度对非高斯噪声和冲击噪声鲁棒<citation id="223" type="reference"><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。定义如下:</p>
                </div>
                <div class="p1">
                    <p id="93"><i>V</i><sub><i>σ</i></sub>(<i>u</i>,<i>v</i>)=<i>E</i>[<i>k</i><sub><i>σ</i></sub>(<i>e</i>)]      (10)</p>
                </div>
                <div class="p1">
                    <p id="94">其中:<i>e</i>=<i>u</i>-<i>v</i>,<i>E</i>[·]表示期望值,<i>k</i><sub><i>σ</i></sub>(·)为核函数,本文仅仅考虑高斯核函数<i>k</i><sub><i>σ</i></sub>(<i>e</i>)=exp(-<i>e</i><sup>2</sup>/2<i>σ</i><sup>2</sup>)。<i>σ</i>表示核宽度。</p>
                </div>
                <div class="p1">
                    <p id="95">基于式(10),本文提出一种新颖的自适应加权策略来更新<b><i>w</i></b>。即:当<i>α</i>、 <i>β</i>固定时,<b><i>w</i></b>更新为:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mi>c</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>k</mi></mstyle><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">β</mi></mrow><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">文中初始化<i>β</i><sub>0</sub>=1/<i>N</i><sub><i>y</i></sub>,<i>w</i><sub>0</sub>=1/<i>c</i>。图2为绝对误差和熵尺度的比较示意图。可以看出,熵是一种局部度量方法,当集合之间的误差较小时,熵近似于绝对误差(|Δ<i>x</i>|),当误差较大时,熵收敛于1。因此本文提出的基于熵的自适应加权策略能够有效地抑制离群样本和噪声的影响,尤其在非高斯噪声和冲击噪声存在的情况下表现出更强鲁棒性。换句话说,若<b><i>Y</i></b><i>β</i>和<b><i>X</i></b><sub><i>i</i></sub><i>α</i><sub><i>i</i></sub>之间的熵权重越小,说明相似度越高,<i>w</i><sub><i>i</i></sub>则越大。反之,<i>w</i><sub><i>i</i></sub>则越小。此外,由于损失函数有下限(≥0)且是<i>α</i>和<i>β</i>联合凸面,因此迭代过程中损失函数的值会不断减小,最终收敛于全局最优解。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 绝对误差和熵尺度的比较示意图" src="Detail/GetImg?filename=images/JSJY201909012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 绝对误差和熵尺度的比较示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Comparison of absolute error and correntropy</p>

                </div>
                <h4 class="anchor-tag" id="99" name="99">2.3 <b>分类</b></h4>
                <div class="p1">
                    <p id="100">得到最优的描述系数<i>α</i><sup>*</sup>、 <i>β</i><sup>*</sup>后,SRNPC定义集合间的距离为:</p>
                </div>
                <div class="p1">
                    <p id="101"><i>d</i><sub><i>i</i></sub>=(‖<b><i>X</i></b><sub><i>i</i></sub>‖<sub>*</sub>+‖<b><i>Y</i></b>‖<sub>*</sub>)·(‖<b><i>X</i></b><i>α</i><sup>*</sup>-<b><i>Y</i></b><i>β</i><sup>*</sup>‖<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>+</p>
                </div>
                <div class="p1">
                    <p id="103">‖<b><i>X</i></b><sub><i>i</i></sub><i>α</i><sup>*</sup><sub><i>i</i></sub>-<b><i>Y</i></b><i>β</i><sup>*</sup>‖<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>)/‖<i>α</i><sup>*</sup><sub><i>i</i></sub>‖<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="106">其中:项‖<i><b>X</b></i><sub><i>i</i></sub>‖<sub>*</sub>+‖<i><b>Y</b></i>‖<sub>*</sub>用于移除与类无关信息的干扰。例如:与正确的类相比,错误的类由于具有更多的样本导致‖<i><b>X</b></i><sub><i>i</i></sub><i>α</i><sup>*</sup><sub><i>i</i></sub>-<i><b>Y</b></i><i>β</i><sup>*</sup>‖<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>的值较低,但凸松弛矩阵<i><b>X</b></i><sub><i>i</i></sub>和<i><b>Y</b></i>的秩‖<i><b>X</b></i><sub><i>i</i></sub>‖<sub>*</sub>+‖<i><b>Y</b></i>‖<sub>*</sub>却能增强图像集的表示能力<citation id="224" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。此外,式(12)融合了RNP和CRNP中对距离的测量方法,因此SRNPC在分类上兼备两者的优势。</p>
                </div>
                <div class="p1">
                    <p id="108">最后,确定测试集<b><i>Y</i></b>的类别标签为:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>i</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>i</mi></munder><mspace width="0.25em" /><mo stretchy="false">{</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">2.4 <b>算法时间复杂度</b></h4>
                <div class="p1">
                    <p id="111">假设<i>n</i>为图库集的样本总数,<i>t</i>为算法迭代更新次数。对于测试集<i><b>Y</b></i>,利用离线计算即可得到图库集对应投影矩阵<i><b>P</b></i>和<i><b>P</b></i><sub><i>i</i></sub>。式(9)中对<i><b>Q</b></i>的计算涉及矩阵的逆运算,其时间复杂度可粗略地等效为SANP中求解<i><b>Y</b></i>时的时间复杂度。从文献<citation id="225" type="reference">[<a class="sup">8</a>]</citation>可知,SANP通过SVD得到标准正交基,因此在SRNPC中计算<i><b>Q</b></i>时的时间复杂度近似为<i>O</i>(<i>n</i><sup>3</sup>)。此外,算法每次更新<i>α</i>和<i>β</i>的时间复杂度均为<i>O</i>(<i>n</i><sup>3</sup>),每次更新<i>w</i><sub><i>i</i></sub>的时间复杂度为<i>O</i>(<i>n</i>)。所以整个交替迭代更新策略的总的时间复杂度为<i>O</i>(<i>t</i>·(3<i>n</i><sup>3</sup>+<i>n</i>))。</p>
                </div>
                <h3 id="112" name="112" class="anchor-tag">3 实验</h3>
                <div class="p1">
                    <p id="113">为了验证本文算法的有效性和鲁棒性,本文在<i>UCSD</i>/<i>Honda</i><citation id="226" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、<i>CMU Mobo</i><citation id="227" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和<i>YouTube celebrities</i>(<i>YTC</i>)<citation id="228" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>这三个公开数据集上进行了<i>SRNPC</i>与多种先进的图像集分类方法的对比实验。这些方法包括:<i>DCC</i><citation id="229" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、<i>MMD</i><citation id="230" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、<i>AHISD</i><citation id="231" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、<i>CHISD</i><citation id="232" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、<i>SANP</i><citation id="233" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、<i>RNP</i><citation id="234" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、<i>CRNP</i><citation id="235" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和<i>JRNP</i><citation id="236" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。所有方法的相关参数均按照对应文献中推荐的最佳参数值进行配置。对于<i>SRNPC</i>,参数<i>λ</i><sub>1</sub>=0.001,<i>λ</i><sub>2</sub>=0.1,采用5倍交叉验证获得<i>γ</i>的值。在本文的实验中,每个图像集都由随机选择的50、100或200张人脸图片组成集合,所有实验结果的分类精度与标准差均为进行10次实验后求均值得到。实验运行环境为Win10 64位 Matlab 2016a, Intel Core i5-3210M CPU 2.5 GHz,8 GB RAM。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">3.1 <b>实验环境搭建</b></h4>
                <div class="p1">
                    <p id="115"><i>UCSD</i>/<i>Honda</i>数据集包含20名不同的对象共59个视频序列,视频中的每个图像均发生了姿态、光照和表情的变化。图3(<i>a</i>)展示了该数据集的部分样本。仿照文献<citation id="237" type="reference">[<a class="sup">16</a>]</citation>中的实验设置,调整图像尺寸为20×20,并进行直方图均衡化处理。实验中,随机选取20个视频序列作为图库集,剩下的39个视频序列为测试集。</p>
                </div>
                <div class="p1">
                    <p id="116"><i>CMU Mobo</i>数据集包含了来自24名对象的96个视频序列,每名对象包含4个视频序列,每个序列分别对应着一种行走模式(部分样本见图3(<i>b</i>))。为了简化运算,提高运行速度,本文将人脸调整为30×30的灰度图像,在每次实验中,从每个对象的视频中随机选取一个图像集作为图库集,剩下的作为测试集。</p>
                </div>
                <div class="p1">
                    <p id="117"><i>YTC</i>数据集是一个来自现实环境的数据集,由47位名人的1 910个视频序列组成。该数据集中的视频大多分辨率低、姿态变化大、光照强度各异。相比前两个数据集该数据集更具有挑战性。对于该数据集,本文使用前29个的视频序列,并调整脸部区域尺寸为20×20(如图3(<i>c</i>)所示)。对于每个对象,随机选取3个序列作为图库集,从剩余序列中再随机选择3个序列用作测试集。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909012_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 人脸数据库部分样本" src="Detail/GetImg?filename=images/JSJY201909012_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 人脸数据库部分样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909012_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Part samples in face databases</i></p>

                </div>
                <h4 class="anchor-tag" id="119" name="119">3.2 <b>识别精度评估</b></h4>
                <div class="p1">
                    <p id="120">为了验证<i>SRNPC</i>和其他流行的图像集分类算法相比具有更高的识别精度,本文依次在<i>UCSD</i>/<i>Honda</i>、<i>CMU Mobo</i>、<i>YTC</i>数据集上分别获得测试集大小为50、100和200时的平均分类精度和标准差。实验结果如表1所示,其中最高分类精度加粗显示。</p>
                </div>
                <div class="area_img" id="121">
                                            <p class="img_tit">
                                                <b>表</b>1 <b>不同数据集上的分类精度和标准差 单位</b>:%
                                                    <br />
                                                <i>Tab</i>. 1 <i>Classification accuracies and standard deviations on different datasets</i><i>unit</i>: %
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909012_12100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909012_12100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909012_12100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 不同数据集上的分类精度和标准差 单位:%" src="Detail/GetImg?filename=images/JSJY201909012_12100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="122">通过观察表1,本文得出以下3个结论:1)同预期一致,在所有的情况下,本文提出的算法<i>SRNPC</i>均表现出最佳的分类效果,验证了本文所提出算法的有效性。特别是在<i>UCSD</i>/<i>Honda</i>数据集上,当图像集大小为100,200时,<i>SRNPC</i>达到了100%的分类精度。2)当样本不充分时(&lt;50),<i>DCC</i>,<i>MMD</i>在所有数据集上的分类精度均小于70%,这可能和判别信息提取和流形分析依赖于图像集样本充足的事实有关。3)当样本充分时(&gt;100),在<i>UCSD</i>/<i>Honda</i>和<i>CMU Mobo</i>数据集上所有的算法均表现出较好的识别性能,在<i>YTC</i>数据集上,<i>SRNPC</i>仍然实现了最高的识别精度,但与其他算法之间的差距较小。这是因为<i>YTC</i>数据集是自然条件下采集的图像,图像质量不高,含有噪声,与前两个数据集相比更具有挑战性。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123">3.3 <b>鲁棒性评估</b></h4>
                <div class="p1">
                    <p id="124">本节中,再次利用<i>YTC</i>数据集来评估鲁棒性。与4.2节中的实验不同,在该实验中,首先固定<i>YTC</i>图像集的大小为200,并在人脸图像中添加不同百分比的椒盐噪声研究表明,对于人眼来说,一张破坏超过50%的图片便很难被识别<citation id="238" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。因此该实验仅测试在添加0%～50%的椒盐噪声时,各个算法的鲁棒性。图4为添加不同百分比噪声时的<i>YTC</i>人脸图像。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909012_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 被椒盐噪声污染的人脸(噪声百分比分别为0%～50%)" src="Detail/GetImg?filename=images/JSJY201909012_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 被椒盐噪声污染的人脸(噪声百分比分别为0%～50%)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909012_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Face damaged by salt</i>-<i>and</i>-<i>pepper noise</i>, <i>with percentage from</i> 0% <i>to</i> 50%</p>

                </div>
                <div class="p1">
                    <p id="126">实验结果如表2所示,在所有的实验中,<i>SRNPC</i>仍然具有最好的分类精度;同时,随着噪声百分比的增加,其他算法与<i>SRNPC</i>之间的差距越来越大。与次优的算法<i>JRNP</i>相比,<i>SRNPC</i>在分类精度上平均实现了3.1%的提高。由此,充分验证了本文提出算法的鲁棒性。但是,当图像包含40%以上的噪声时,由于判别信息不充分,所有算法(包括<i>SRNPC</i>)的分类精度都低于50%。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表</b>2 <i>YTC</i><b>图像在不同椒盐噪声百分比下的 分类精度 单位</b>:% <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Classification accuracy of YTC images with</i><i>different percentage of salt</i>-<i>and</i>-<i>pepper noise</i><i>unit</i>: %</p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="6"><br />噪声百分比</td></tr><tr><td><br />0</td><td>10</td><td>20</td><td>30</td><td>40</td><td>50</td></tr><tr><td><br /><i>DCC</i></td><td>75.9</td><td>73.6</td><td>69.5</td><td>59.8</td><td>49.2</td><td>26.7</td></tr><tr><td><br /><i>MMD</i></td><td>76.1</td><td>72.8</td><td>59.7</td><td>41.8</td><td>29.6</td><td>20.3</td></tr><tr><td><br /><i>AHISD</i></td><td>67.5</td><td>61.2</td><td>57.1</td><td>47.4</td><td>36.3</td><td>27.9</td></tr><tr><td><br /><i>CHISD</i></td><td>75.7</td><td>73.4</td><td>60.3</td><td>49.2</td><td>38.2</td><td>22.1</td></tr><tr><td><br /><i>SANP</i></td><td>78.4</td><td>76.1</td><td>72.7</td><td>61.9</td><td>55.4</td><td>45.6</td></tr><tr><td><br /><i>RNP</i></td><td>77.3</td><td>75.3</td><td>70.2</td><td>62.1</td><td>53.5</td><td>47.2</td></tr><tr><td><br /><i>CRNP</i></td><td>83.9</td><td>74.5</td><td>72.4</td><td>67</td><td>42.6.</td><td>38.1</td></tr><tr><td><br /><i>JRNP</i></td><td>84.1</td><td>78.7</td><td>73.8</td><td>69.9</td><td>54.2</td><td>47.3</td></tr><tr><td><br /><i>SRNPC</i></td><td>84.7</td><td>80.4</td><td>76.3</td><td>70.1</td><td>65.8</td><td>48.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="128" name="128" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="129">本文提出一种用于图像集分类的熵自加权联合正则化最近点算法(<i>SRNPC</i>)。该算法融合协同表示和正则化最近点的优势,同时最小化测试集与整个图像集以及每个训练集之间的距离。为了避免过度拟合,测试集中的全局联合最近点始终保持不变。为了增强集合之间的判别能力和对非高斯噪声的鲁棒性,提出了一种基于熵的自加权策略,根据每个图库集与测试集的相似度分配给每个图库集不同的权重。最后通过计算测试集和每个图库集之间的最小残差值完成分类。在三种公开数据集上与目前先进的图像集分类算法进行了对比实验,验证了<i>SRNPC</i>在图像集分类任务上具有更高的识别精度和更强的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="130">下一步工作中,将引入低秩约束模型来增强对损坏图像的恢复,从而能够更进一步地提高算法的分类精度与鲁棒性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="167">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Representation learning with deep extreme learning machines for efficient image set classification">

                                <b>[1]</b> UZAIR M,SHAFAIT F,GHANEM B,et al.Representation learning with deep extreme learning machines for efficient image set classification[J].Neural Computing and Applications,2018,30(4):1211-1223.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201703052&amp;v=MDU2MDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFVyckxMejdCZDdHNEg5Yk1ySTlBWm9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 李光早,王士同.基于稀疏表示和弹性网络的人脸识别[J].计算机应用,2017,37(3):901-905.(LI G Z,WANG S T.Face recognition based on sparse representation and elastic network [J].Journal of Computer Applications,2017,37(3):901-905.)
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWYJ201807004&amp;v=MDQ0NDRaTEc0SDluTXFJOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXJyTExUclM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 刘天赐,史泽林,刘云鹏,等.基于Grassmann流形几何深度网络的图像集识别方法[J].红外与激光工程,2018,47(7):25-31.(LIU T C,SHI Z L,LIU Y P,et al.Geometry deep network image-set recognition method based on Grassmann manifolds [J].Infrared and Laser Engineering,2018,47(7):25-31.)
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative Learning and Recognition of Image Set Classes Using Canonical Correlations">

                                <b>[4]</b> KIM T,KITTLER J,CIPOLLA R.Discriminative learning and recognition of image set classes using canonical correlations [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2007,29(6):1005-1018.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning non-linear reconstruction models for image set classification">

                                <b>[5]</b> HAYAT M,BENNAMOUN M,AN S.Learning non-linear reconstruction models for image set classification [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2014:1907-1914.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201804063&amp;v=MzI1ODNTWkxHNEg5bk1xNDlEWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFVyckxMejc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 郑萍萍,李波,丁玉琳.基于局部邻域多流形度量的人脸识别[J].计算机应用研究,2018,35(4):1250-1253.(ZHENG P P,LI B,DING Y L.Local neighborhood based multi-manifold metric learning for face recognition[J].Application Research of Computers,2018,35(4):1250-1253.)
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Covariance discriminative learning:a natural and efficient approach to image set classification C]">

                                <b>[7]</b> WANG R,GUO H,DAVIS L S,et al.Covariance discriminative learning:A natural and efficient approach to image set classification[C]// Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:2496-2503.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face Recognition Using Sparse Approximated Nearest Points between Image Sets">

                                <b>[8]</b> HU Y,MIAN A S,OWENS R.Face recognition using sparse approximated nearest points between image sets [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(10):1992-2004.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition based on regularized nearest points between image sets">

                                <b>[9]</b> YANG M,ZHU P,van GOOL L,et al.Face recognition based on regularized nearest points between image sets [C]// Proceedings of the 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Piscataway,NJ:IEEE,2013:1-7.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Collaboratively regularized nearest points for set based recognition">

                                <b>[10]</b> WU Y,MINOH M,MUKUNOKI M.Collaboratively regularized nearest points for set based recognition [C]// Proceedings of the 2013 British Machine Vision Conference.Durham:BMVA Press,2013:No.0134.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201704045&amp;v=MDIzMDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVcnJMTHo3QmQ3RzRIOWJNcTQ5QllZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 唐宋,陈利娟,陈志贤,等.基于目标域局部近邻几何信息的域自适应图像分类方法[J].计算机应用,2017,37(4):1164-1168.(TANG S,CHEN L J,CHEN Z X,et al.Domain adaptation image classification based on target local-neighbor geometrical information [J].Journal of Computer Applications,2017,37(4):1164-1168.)
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse representation-based augmented multinomial logistic extreme learning machine with weighted composite features for spectral-spatial classification of hyperspectral images">

                                <b>[12]</b> CAO F,YANG Z,REN J,et al.Sparse representation-based augmented multinomial logistic extreme learning machine with weighted composite features for spectral-spatial classification of hyperspectral images [J].IEEE Transactions on Geoscience and Remote Sensing,2018,56(11):6263-6279.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint regularized nearest points for image set based face recognition">

                                <b>[13]</b> YANG M,LIU W,SHEN L,et al.Joint regularized nearest points for image set based face recognition [C]// Proceedings of the 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Washington,DC:IEEE Computer Society,2015:1-7.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Correntropy: Properties and Applications in Non-Gaussian Signal Processing">

                                <b>[14]</b> LIU W,POKHAREL P P,PRINCIPE J C.Correntropy:properties and applications in non-Gaussian signal processing [J].IEEE Transactions on Signal Processing,2007,55(11):5286-5298.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9C0205EFBC7CB0D0FF334A9029824FBE&amp;v=MjE1MzFPZmJyTEh0UE1xdm96RnBnSWZ3NDV1eFpsbkR4K1RBN3JyQkE4Y2JDUU04anFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TGkreGFzPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> CHEN B,WANG X,LU N,et al.Mixture correntropy for robust learning[J].Pattern Recognition,2018,79:318-327.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image-set based collaborative representation for face recognition in videos">

                                <b>[16]</b> GOU G,SHI J,XIONG G,et al.Image-set based collaborative representation for face recognition in videos [C]// Proceedings of the 2017 Pacific Rim Conference on Multimedia,LNCS 10736.Berlin:Springer,2017:498-507.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDC123F4FDBA247EF3425817FB32D10CDE&amp;v=MjA5Njh4TGkreGFzPU5qN0Jhc0M1SE5LNnEva3hGcG9OQ0h0TXVSVVg2RHAxU1hpVTNoRTNEYk9VTnM3cUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> HAYAT M,KHAN S H,BENNAMOUN M.Empowering simple binary classifiers for image set based face recognition [J].International Journal of Computer Vision,2017,123(3):479-498.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extended YouTube faces:a dataset for heterogeneous open-set face identification">

                                <b>[18]</b> FERRARI C,BERRETTI S,del BIMBO A.Extended YouTube faces:a dataset for heterogeneous open-set face identification [C]// Proceedings of the 2018 24th International Conference on Pattern Recognition.Piscataway,NJ:IEEE,2018:3408-3413.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909012&amp;v=MjI5NTR6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXJyTEx6N0JkN0c0SDlqTXBvOUVab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
