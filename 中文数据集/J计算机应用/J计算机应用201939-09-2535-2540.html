<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136466377783750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909010%26RESULT%3d1%26SIGN%3ddI%252bU5HbF5%252fU4uSEgINDD57ELOJo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909010&amp;v=MzA3MzZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXIzS0x6N0JkN0c0SDlqTXBvOUU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#57" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="1 细边缘检测模型 ">1 细边缘检测模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="1.1 RCF&lt;b&gt;网络&lt;/b&gt;">1.1 RCF<b>网络</b></a></li>
                                                <li><a href="#65" data-title="1.2 &lt;b&gt;本模型&lt;/b&gt;">1.2 <b>本模型</b></a></li>
                                                <li><a href="#76" data-title="1.3 &lt;b&gt;精细边缘检测模型训练&lt;/b&gt;">1.3 <b>精细边缘检测模型训练</b></a></li>
                                                <li><a href="#97" data-title="1.4 &lt;b&gt;与&lt;/b&gt;RCF&lt;b&gt;比较&lt;/b&gt;">1.4 <b>与</b>RCF<b>比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="2 实验 ">2 实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#101" data-title="2.1 &lt;b&gt;图像金字塔技术&lt;/b&gt;">2.1 <b>图像金字塔技术</b></a></li>
                                                <li><a href="#103" data-title="2.2 &lt;b&gt;实验分析&lt;/b&gt;">2.2 <b>实验分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#114" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="图1 RCF结构">图1 RCF结构</a></li>
                                                <li><a href="#73" data-title="图2 本文模型结构">图2 本文模型结构</a></li>
                                                <li><a href="#74" data-title="图3 &lt;i&gt;SE&lt;/i&gt;结构">图3 <i>SE</i>结构</a></li>
                                                <li><a href="#75" data-title="图4 特征融合模块">图4 特征融合模块</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;本文算法与其他算法比较&lt;/b&gt;"><b>表</b>1 <b>本文算法与其他算法比较</b></a></li>
                                                <li><a href="#107" data-title="图5 本文模型输出的边缘图与原图对比">图5 本文模型输出的边缘图与原图对比</a></li>
                                                <li><a href="#110" data-title="图6 本文模型与&lt;i&gt;HED&lt;/i&gt;、&lt;i&gt;RCF&lt;/i&gt;各个&lt;i&gt;stage&lt;/i&gt;输出边缘图的对比">图6 本文模型与<i>HED</i>、<i>RCF</i>各个<i>stage</i>输出边缘图的对比</a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;各模块改进效果的对比&lt;/b&gt;"><b>表</b>2 <b>各模块改进效果的对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="153">


                                    <a id="bibliography_1" title=" REN X F,BO L F.Discriminatively trained sparse code gradients for contour detection[C]// NIPS 2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012,1:584-592." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminatively Trained Sparse Code Gradients for ContourDetection">
                                        <b>[1]</b>
                                         REN X F,BO L F.Discriminatively trained sparse code gradients for contour detection[C]// NIPS 2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012,1:584-592.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_2" title=" 张广燕,王俊平,邢润森,等.PSLIP新模型及在边缘检测和图像增强中的应用[J].电子学报,2015,43(2):377-382.(ZHANG G Y,WANG J P,XING R S,et al.A new PSLIP model and its application in edge detection and image enhancement[J].Acta Electronica Sinica,2015,43(2):377-382.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201502026&amp;v=MTkwNDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVcjNLSVRmVGU3RzRIOVRNclk5SFlvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         张广燕,王俊平,邢润森,等.PSLIP新模型及在边缘检测和图像增强中的应用[J].电子学报,2015,43(2):377-382.(ZHANG G Y,WANG J P,XING R S,et al.A new PSLIP model and its application in edge detection and image enhancement[J].Acta Electronica Sinica,2015,43(2):377-382.)
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_3" title=" KOHLI P,LADICKY L,TORR P H S.Robust higher order potentials for enforcing label consistency [J].International Journal of Computer Vision,2009,82(3):302-324." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003427423&amp;v=MDUyNTA3QmFyTzRIdEhQcTQxQ1lPa01ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpcmxVNy9KSWx3PU5q&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         KOHLI P,LADICKY L,TORR P H S.Robust higher order potentials for enforcing label consistency [J].International Journal of Computer Vision,2009,82(3):302-324.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_4" title=" 石美红,李青,赵雪青,等.一种基于保角相位的图像边缘检测新方法[J].电子与信息学报,2015,37(11):2594-2600.(SHI M H,LI Q,ZHAO X Q,et al.A new approach for image edge detection based on conformal phase [J].Journal of Electronics and Information Technology,2015,37(11):2594-2600.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201511008&amp;v=MTExNTVsVXIzS0lUZlNkckc0SDlUTnJvOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         石美红,李青,赵雪青,等.一种基于保角相位的图像边缘检测新方法[J].电子与信息学报,2015,37(11):2594-2600.(SHI M H,LI Q,ZHAO X Q,et al.A new approach for image edge detection based on conformal phase [J].Journal of Electronics and Information Technology,2015,37(11):2594-2600.)
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_5" title=" PANTOFARU C,SCHMID C,HERBERT M.Object recognition by integrating multiple image segmentations [C]//ECCV 2008:Proceedings of the 10th European Conference on Computer Vision,LNCS 5304.Berlin:Springer,2008:481-494." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object recognition by integrating multiple imagesegmentation">
                                        <b>[5]</b>
                                         PANTOFARU C,SCHMID C,HERBERT M.Object recognition by integrating multiple image segmentations [C]//ECCV 2008:Proceedings of the 10th European Conference on Computer Vision,LNCS 5304.Berlin:Springer,2008:481-494.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_6" title=" FELDMAN J A,FELDMAN G M,FALK G,et al.The Stanford hand-eye project [C]// IJCAI &#39;69:Proceedings of the 1st International Joint Conference on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann,1969:521-526." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Stanford hand-eye project">
                                        <b>[6]</b>
                                         FELDMAN J A,FELDMAN G M,FALK G,et al.The Stanford hand-eye project [C]// IJCAI &#39;69:Proceedings of the 1st International Joint Conference on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann,1969:521-526.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_7" title=" CANNY J.A computational approach to edge detection [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1986,8(6):679-698." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A computational approach to edge detection">
                                        <b>[7]</b>
                                         CANNY J.A computational approach to edge detection [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1986,8(6):679-698.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_8" title=" KONISHI S,YUILLE A L,COUGHLAN J M,et al.Statistical edge detection:learning and evaluating edge cues [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2003,25(1):57-74." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Statistical edge detection: learning and evaluating edge cues">
                                        <b>[8]</b>
                                         KONISHI S,YUILLE A L,COUGHLAN J M,et al.Statistical edge detection:learning and evaluating edge cues [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2003,25(1):57-74.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_9" title=" MARTIN D R,FOWLKES C C,MALIK J.Learning to detect natural image boundaries using local brightness,color,and texture cues[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2004,26(5):530-549." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to detect natural image boundaries using local brightness, color, and texture cues">
                                        <b>[9]</b>
                                         MARTIN D R,FOWLKES C C,MALIK J.Learning to detect natural image boundaries using local brightness,color,and texture cues[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2004,26(5):530-549.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_10" title=" GANIN Y,LEMPITSKY V.N&lt;sup&gt;4&lt;/sup&gt;-Fields:neural network nearest neighbor fields for image transforms [C]// Proceedings of the 2014 Asian Conference on Computer Vision,LNCS 9004.Berlin:Springer,2014:536-551." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=N4-fields:Neural network nearest neighbor fields for image transforms">
                                        <b>[10]</b>
                                         GANIN Y,LEMPITSKY V.N&lt;sup&gt;4&lt;/sup&gt;-Fields:neural network nearest neighbor fields for image transforms [C]// Proceedings of the 2014 Asian Conference on Computer Vision,LNCS 9004.Berlin:Springer,2014:536-551.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_11" title=" SHEN W,WANG X G,WANG Y,et al.DeepContour:a deep convolutional feature learned by positive-sharing loss for contour detection[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:3982-3991." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepcontour:A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection">
                                        <b>[11]</b>
                                         SHEN W,WANG X G,WANG Y,et al.DeepContour:a deep convolutional feature learned by positive-sharing loss for contour detection[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:3982-3991.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_12" title=" BERTASIUS G,SHI J,TORRESANI L.DeepEdge:a multi-scale bifurcated deep network for top-down contour detection [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:4380-4389." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepedge:A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection">
                                        <b>[12]</b>
                                         BERTASIUS G,SHI J,TORRESANI L.DeepEdge:a multi-scale bifurcated deep network for top-down contour detection [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:4380-4389.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_13" title=" XIE S,TU Z.Holistically-nested edge detection [J].International Journal of Computer Vision,2017,125(1/2/3):3-18." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD6BB00EB93AC104C12EB55DFC97247B75&amp;v=MDcxNzg3QmFyWEtiTkhNMnYxTVo1cDhEWHc5dkJjUm4wMTRUUXVVM3hzeWU3YVRONzJhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeExpK3dxbz1Oag==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         XIE S,TU Z.Holistically-nested edge detection [J].International Journal of Computer Vision,2017,125(1/2/3):3-18.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_14" title=" SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[14]</b>
                                         SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_15" title=" LEE C-Y,XIE S,GALLAGHER P,et al.Deeply-supervised nets [EB/OL].[2019- 01- 02].https://arxiv.org/pdf/1409.5185.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deeply-supervised nets">
                                        <b>[15]</b>
                                         LEE C-Y,XIE S,GALLAGHER P,et al.Deeply-supervised nets [EB/OL].[2019- 01- 02].https://arxiv.org/pdf/1409.5185.pdf.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_16" title=" LIU Y,CHENG M,HU X,et al.Richer convolutional features for edge detection [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:5872-5881." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Richer convolutional features for edge detection">
                                        <b>[16]</b>
                                         LIU Y,CHENG M,HU X,et al.Richer convolutional features for edge detection [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:5872-5881.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_17" title=" SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL]">
                                        <b>[17]</b>
                                         SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_18" title=" HU J,SHEN L,ALBANIE S,et al.Squeeze-and-excitation networks [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1709.01507.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Squeeze-and-excitation networks[OL]">
                                        <b>[18]</b>
                                         HU J,SHEN L,ALBANIE S,et al.Squeeze-and-excitation networks [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1709.01507.pdf.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_19" title=" YU F,KOLTUN V.Multi-scale context aggregation by dilated convolutions [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1511.07122.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">
                                        <b>[19]</b>
                                         YU F,KOLTUN V.Multi-scale context aggregation by dilated convolutions [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1511.07122.pdf.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_20" title=" HE K M,ZHANG X Y,REN S Q,et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[20]</b>
                                         HE K M,ZHANG X Y,REN S Q,et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770-778.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_21" title=" NAIR V,HINTON G E.Rectified linear units improve restricted Boltzmann machines [C]// ICML &#39;10:Proceedings of the 27th International Conference on Machine Learning.Madison,WI:Omnipress,2010:807-814." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted boltzmann machines">
                                        <b>[21]</b>
                                         NAIR V,HINTON G E.Rectified linear units improve restricted Boltzmann machines [C]// ICML &#39;10:Proceedings of the 27th International Conference on Machine Learning.Madison,WI:Omnipress,2010:807-814.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_22" title=" MARTIN D R,FOWLKES C C,TAL D,et al.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics [C]// ICCV 2001:Proceedings of the 8th IEEE International Conference on Computer Vision.Washington DC:IEEE Computer Society,2001,2:416-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A database of human segmented natural images and its application to evaluating seg- mentation algorithms and measuring ecological statistics.In: Werner B, ed">
                                        <b>[22]</b>
                                         MARTIN D R,FOWLKES C C,TAL D,et al.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics [C]// ICCV 2001:Proceedings of the 8th IEEE International Conference on Computer Vision.Washington DC:IEEE Computer Society,2001,2:416-423.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_23" title=" MOTTAGHI R,CHEN X,LIU X,et al.The role of context for object detection and semantic segmentation in the wild [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington DC:IEEE Computer Society,2014:891-898." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The role of context for object detection and semantic segmentation in the wild">
                                        <b>[23]</b>
                                         MOTTAGHI R,CHEN X,LIU X,et al.The role of context for object detection and semantic segmentation in the wild [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington DC:IEEE Computer Society,2014:891-898.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_24" title=" FARABET C,COUPRIE C,NAJMAN L,et al.Learning hierarchical features for scene labeling [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(8):1915-1929." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning hierarchical features for scene labeling">
                                        <b>[24]</b>
                                         FARABET C,COUPRIE C,NAJMAN L,et al.Learning hierarchical features for scene labeling [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(8):1915-1929.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_25" title=" 刘胜男,宁纪锋.基于超像素的点互信息边界检测算法[J].计算机应用,2016,36(8):2296-2300.(LIU S N,NING J F.Super-pixel based pointwise mutual information boundary detection algorithm[J].Journal of Computer Applications,2016,36(8):2296-2300.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201608044&amp;v=MTAxOTlLTHo3QmQ3RzRIOWZNcDQ5QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amxVcjM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         刘胜男,宁纪锋.基于超像素的点互信息边界检测算法[J].计算机应用,2016,36(8):2296-2300.(LIU S N,NING J F.Super-pixel based pointwise mutual information boundary detection algorithm[J].Journal of Computer Applications,2016,36(8):2296-2300.)
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_26" title=" DOLL&#193;R P,ZITNICK C L.Fast edge detection using structured forests [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1558-1570." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast edge detection using structured forests">
                                        <b>[26]</b>
                                         DOLL&#193;R P,ZITNICK C L.Fast edge detection using structured forests [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1558-1570.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_27" title=" ZITNICK C L,DOLL&#193;R P.Edge boxes:locating object proposals from edges [C]// Proceedings of the 2014 European Conference on Computer Vision,LNCS 8693.Berlin:Springer,2014:391-405." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Edge boxes:locating object proposals from edges">
                                        <b>[27]</b>
                                         ZITNICK C L,DOLL&#193;R P.Edge boxes:locating object proposals from edges [C]// Proceedings of the 2014 European Conference on Computer Vision,LNCS 8693.Berlin:Springer,2014:391-405.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-19 16:07</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2535-2540 DOI:10.11772/j.issn.1001-9081.2019030462            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于RCF的精细边缘检测模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%99%AF%E5%B9%B4%E6%98%AD&amp;code=42779637&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">景年昭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E7%BB%B4&amp;code=06320505&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨维</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0111847&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京交通大学电子信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对目前基于深度学习的边缘检测技术生成的边缘粗糙及模糊等问题,提出一种基于更丰富特征的边缘检测(RCF)模型的端到端的精细边缘检测模型。该模型以RCF模型为基础,在主干网络中引入“注意力”机制,采用SE模块提取图像边缘特征,并且去掉主干网络部分下采样,避免细节信息过度丢失,使用扩张卷积技术增大模型感受野,并利用残差结构将不同尺度的边缘图进行融合。对伯克利分割数据集(BSDS500)进行增强,使用一种多步骤的训练方式在BSDS500和PASCAL VOC Context数据集上进行训练,并用BSDS500进行测试实验。实验结果表明,该模型将全局最佳(ODS)和单图最佳(OIS)指标分别提高到了0.817和0.838,在不影响实时性的前提下可以输出更精细的边缘,同时还具有较好的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">边缘检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9B%B4%E4%B8%B0%E5%AF%8C%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">更丰富的卷积特征检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%89%A9%E5%BC%A0%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">扩张卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *景年昭(1994—),男,山东济南人,硕士研究生,主要研究方向:计算机视觉、信息处理;电子邮箱947392052@qq.com;
                                </span>
                                <span>
                                    杨维(1964—),男,北京人,教授,博士生导师,博士,主要研究方向:无线通信、信息处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目(2016YFC0801800);</span>
                    </p>
            </div>
                    <h1><b>Fine edge detection model based on RCF</b></h1>
                    <h2>
                    <span>JING Nianzhao</span>
                    <span>YANG Wei</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Information Engineering, Beijing Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the roughness and blur of edges generated by edge detection technology based on deep learning, an end-to-end fine edge detection model based on RCF(Richer Convolutional Features for edge detection) was proposed. In this model based on RCF model, attention mechanism was introduced in the backbone network, Squeeze-and-Excitation(SE) module was used to extract image edge features. In order to avoid excessive loss of detail information, two subsampling in the backbone network were removed. In order to increase the receptive field of the model, dilation convolution was used in the backbone. A residual module was used to fuse the edge images in different scales. The model was trained on the Berkeley Segmentation Data Set(BSDS500)and PASCAL VOC Context dataset by a multi-step training approach and was tested on the BSDS500. The experimental results show that the model improves the ODS(Optimal Dataset Scale) and OIS(Optimal Image Scale) to 0.817 and 0.838 respectively, and it not only generates finer edges without affecting real-time performance but also has better robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=edge%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">edge detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Richer%20Convolutional%20Features%20for%20edge%20detection(RCF)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Richer Convolutional Features for edge detection(RCF);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dilation%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dilation convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JING Nianzhao, born in 1994, M. S. candidate. His research interests include computer vision, information processing. ;
                                </span>
                                <span>
                                    YANG Wei, born in 1964, Ph. D. , professor. His research interests include wireless communication, information processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-20</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Key Research and Development Program of China(2016YFC0801800);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="57" name="57" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="58">图像的边缘是图像的重要特征之一,准确的边缘检测是图像分割、目标区域识别、区域形状提取等图像分析工作的基础<citation id="207" type="reference"><link href="153" rel="bibliography" /><link href="155" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>,是机器视觉系统中必不可少的重要环节<citation id="208" type="reference"><link href="157" rel="bibliography" /><link href="159" rel="bibliography" /><link href="161" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="59">早期的边缘检测算法建立在图像梯度运算的基础上,利用图像的一阶或二阶梯度信息提取图像的边缘,代表方法有Sobel算子<citation id="209" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、Canny算子<citation id="210" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等。这一类基于梯度的方法实时性好,但鲁棒性不强,容易受噪声、光照等因素的影响。随着统计学和信息理论的引入以及机器学习领域的发展,许多基于手工特征的方法被提出。Konishi等<citation id="211" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>基于数据驱动技术,将边缘检测表述为统计推断,利用图像特征的联合概率分布实现边缘的提取;Martin等<citation id="212" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>把图像的亮度、光照、纹理等局部特征输入到逻辑回归分类器中进行边缘的判定。这一类方法基于手工提取的图像特征,利用滤波器技术或分类器技术进行边缘的检测,性能比传统方法有了很大的提升,但是其成本高,步骤繁琐,实时性不好。后来,随着神经网络的复兴以及计算机算力的提升,基于深度学习的算法成为了该领域的主流方法。Ganin等<citation id="213" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出将卷积神经网络与最近邻算法结合起来进行边缘检测,该方法先利用卷积神经网络(Convolutional Neural Network, CNN)自动提取图像特征,然后再利用最近邻算法实现边缘的聚类;Shen等<citation id="214" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>将边缘检测看作一个多分类问题,根据边缘的不同形态将边缘分成多个子类,利用CNN进行边缘子类的检测,之后再利用随机森林技术将子类聚合形成最终轮廓图;Bertasius等<citation id="215" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出使用更多的高级特征可以提升模型的性能,将参与分类任务的预训练模型迁移到边缘检测模型中,并将网络分为两个支路,利用多尺度技术分别进行边缘的分类学习和回归学习,使模型的鲁棒性得到很大提升;Xie等<citation id="216" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了第一个端到端的边缘检测模型HED(Holistically-nested Edge Detection),该模型基于全卷积神经网络<citation id="217" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>框架并尝试利用深监督技术<citation id="218" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>和多尺度学习技术解决边缘的模糊问题;Liu等<citation id="219" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>在HED模型的基础上提出基于更丰富特征的边缘检测(Richer Convolutional Features for edge detection, RCF)模型,通过融合更丰富的特征使输出的边缘更精细。</p>
                </div>
                <div class="p1">
                    <p id="60">由于其强大的特征自动提取能力,基于深度学习的边缘检测技术比之前的算法在性能上有很大提升,对光照、阴影等影响因素的鲁棒性也随之增强。但是,像RCF或HED这类端到端的边缘检测模型以VGG16(Visual Geometry Group)<citation id="220" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等传统分类网络为基础,不能提取图片的全局特征,因此表达能力受到一定的影响。另外,这一类模型采用过多的下采样并且没有充分融合多尺度特征,导致图像中的许多细节信息丢失,使输出的边缘线条粗糙且模糊。针对这些问题,本文提出一种基于RCF的精细边缘检测模型。该模型在RCF的基础上,引入SE(Squeeze-and-Excitation)结构<citation id="221" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,提取全局特征,去掉主干网络的后两个下采样,避免细节信息多度丢失,并使用扩张卷积技术<citation id="222" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>增加主干网络的感受野,提高主干网络的表达能力,然后使用一个残差结构<citation id="223" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>充分融合深监督模块输出的多尺度特征,生成最终的边缘图。此外,该模型使用一种多阶段的训练方式在增强的伯克利分割数据集(Berkeley Segmentation Data Set, BSDS500)<citation id="224" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和PASCAL VOC Context数据集<citation id="225" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>上进行训练,并在测试时使用图像金字塔技术,使模型性能进一步提升。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag">1 细边缘检测模型</h3>
                <h4 class="anchor-tag" id="62" name="62">1.1 RCF<b>网络</b></h4>
                <div class="p1">
                    <p id="63">RCF<citation id="226" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>以HED<citation id="227" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>为基础,实现了端到端边缘检测,是目前性能最好的边缘检测算法之一。如图1所示,RCF的结构分为三大部分:主干网络、深监督模块(Deeply-Supervised nets, DS)和特征融合模块(fuse)。RCF使用VGG16<citation id="228" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>的全部卷积层作为自己的主干网络,分为5个stage。通过这种全卷积结构<citation id="229" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,主干网络实现了边缘特征的自动提取。RCF的深监督模块对每一个stage进行监督学习,并使每个stage输出一张边缘图,使该模型可以更好更快地收敛。RCF的融合模块则利用一个1×1的卷积层将深监督模块输出的5张边缘图融合并输出最终的边缘图。RCF输出的边缘图融合了主干网络每一层的特征,其效果要好于只使用部分特征的HED网络。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909010_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 RCF结构" src="Detail/GetImg?filename=images/JSJY201909010_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 RCF结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909010_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure of RCF</p>

                </div>
                <h4 class="anchor-tag" id="65" name="65">1.2 <b>本模型</b></h4>
                <div class="p1">
                    <p id="66">本模型的设计思想来源于RCF网络,并对RCF各个模块进行了针对性的改进。如图2所示,本模型可大致分为主干网络、深监督模块、特征融合模块三部分。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">1.2.1 基于SE结构的主干网络</h4>
                <div class="p1">
                    <p id="68">如图2中的虚线左边部分所示,本模型的主干网络以RCF的主干网络为基础,也分为5个stage,负责边缘特征的自动提取。然而RCF的主干网络采用的是VGG16的传统卷积结构(convolution, conv),由多个卷积层简单的堆叠而成。这种传统的卷积结构只能对局部感受野中的空间维度和特征维度的信息进行处理,不能提取图片的全局特征,表达能力会受到限制。因此,本模型引入可提取全局特征的SE结构并将主干网络分为前后两部分:前一部分(前两个stage)与RCF一样,采用传统的卷积和下采样组合的结构,充分提取边缘的低级特征;后一部分(后面三个stage)则采用类似“注意力机制”的SE(Squeeze-and-Excitation)结构<citation id="230" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,提取更多的语义特征和全局特征。如图3所示,每个SE结构先通过全局平均池化操作<i>F</i><sub>sq</sub>把卷积层<i><b>IN</b></i>的每个二维的feature map压缩成一个实数<i>a</i><sub><i>i</i></sub>,这个实数<i>a</i><sub><i>i</i></sub>具有全局感受野,然后利用可学习的参数<i><b>W</b></i>为每一个实数<i>a</i><sub><i>i</i></sub>生成权重,通过可学习的方式显式地建模feature map间的相关性,最后用学习后的全局特征<i><b>B</b></i>与原始feature map进行元素相乘,使<i><b>OUT</b></i>中每一个feature map的特征包含全局的信息<sup></sup><citation id="231" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。使用这种SE结构后,本模型可以提取全局特征来衡量feature map的重要程度,通过学习的方式提升有用的特征并抑制对当前任务用处不大的特征,从而提升主干网络的表达能力。另外,RCF主干网络有4次下采样,模型输出的图片精度只有原始图片的1/16,导致图片的许多细节信息丢失,使输出的边缘模糊不清。因此,本模型去掉主干网络后2个下采样,使用3×3的池化(pool)并将步长设为1。这样,本模型只有2次下采样,输出图片的精度是原始图片的1/4,保留了更多的细节信息,使输出的边缘更精细。与此同时,为了解决去掉下采样后感受野变小的问题,本模型在后两个stage中使用扩张卷积技术(dilation, dil)<citation id="232" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,将扩张参数分别设为2和4,在不增加参数的情况下增大模型的感受野。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">1.2.2 深监督模块</h4>
                <div class="p1">
                    <p id="70">图2中的主干网络在提取特征的过程中,由于自身参数过多,训练时网络收敛速度慢,收敛效果也不太理想。因此,为了使主干网络能够更高效地提取特征,改善模型训练效果,本模型使用与<i>RCF</i>相同的深监督技术。如图2中的虚线右侧部分所示,将主干网络中的每一层特征通过1×1的卷积层进行特征压缩,并以<i>stage</i>为单位相加,然后再使用1×1的卷积进一步压缩特征,最后通过利用双线性插值技术初始化的反卷积(<i>deconvolution</i>, <i>deconv</i>)实现上采样,使每一个<i>stage</i>都输出一张边缘图;并且本模型对每一个<i>stage</i>输出的边缘图都进行监督学习,实现深监督学习。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">1.2.3 基于残差结构的特征融合</h4>
                <div class="p1">
                    <p id="72">图2中的特征融合模块负责对深监督模块输出的5个边缘图进行融合处理。每个<i>stage</i>生成的边缘图都使用了不同层次的特征,第1个<i>stage</i>生成的边缘图使用了大量的低级特征和很少的语义特征,而第5个<i>stage</i>生成的边缘图则使用了大量的语义特征和很少的细节特征。因此将每个<i>stage</i>输出的边缘图融合起来,可以更充分地使用模型特征,使输出的边缘更精确。然而<i>RCF</i>只采用一层1×1的卷积简单地按比例融合,对多尺度的信息处理得不充分,输出的边缘比较粗。另外,通过实验发现后两个<i>stage</i>输出的边缘图与标签图(<i>ground truth</i>)已经很相近,这就意味着后两个<i>stage</i>输出的边缘图与标签图之间的残差已经很小,因此,本模型的融合模块使用一种残差结构<citation id="233" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>,通过对每个<i>stage</i>输出的边缘图与标签图之间的残差进行处理,使多尺度特征更高效地融合。如图4所示,融合模块的输入是深监督模块输出的5张边缘图,将输入分成两个分支,其中一个分支通过3个卷积层对残差进行充分处理,然后与输入相加,实现多尺度信息的融合,再经过一个1×1的卷积进行特征压缩后便输出最终的边缘图。融合后的边缘图既保留大量的细节信息,在语义上也是完整的。此外,通过实验发现,在特征融合模块中使用非线性结构,比如<i>ReLU</i>(<i>Rectified Linear Units</i>)<citation id="234" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>等,会损失一些细节信息,导致输出的边缘图包含噪声,因此,该模块没有添加任何非线性结构。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909010_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文模型结构" src="Detail/GetImg?filename=images/JSJY201909010_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909010_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Structure of proposed model</i></p>

                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909010_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 SE结构" src="Detail/GetImg?filename=images/JSJY201909010_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>SE</i>结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909010_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Structure of Squeeze</i>-<i>and</i>-<i>Excitation</i></p>

                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909010_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 特征融合模块" src="Detail/GetImg?filename=images/JSJY201909010_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 特征融合模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909010_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Module of feature fusion</i></p>

                </div>
                <h4 class="anchor-tag" id="76" name="76">1.3 <b>精细边缘检测模型训练</b></h4>
                <h4 class="anchor-tag" id="77" name="77">1.3.1 数据集增强</h4>
                <div class="p1">
                    <p id="78">本模型使用增强版的<i>BSDS</i>500<citation id="235" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>数据集和<i>PASCAL VOC Context</i><citation id="236" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>数据集进行训练。传统的边缘检测数据集<i>BSDS</i>500由训练集、验证集和测试集三部分组成。其中,训练集包含200张图片,验证集包含100张图片,测试集包含200张图片。为了防止模型出现过拟合现象,对<i>BSDS</i>500数据集进行了增强处理。通过使用<i>OpenCV</i>工具包,对训练集和验证集的300张图片进行旋转、扩大、剪裁等操作,将数据集增强到28 800张。另外,由于本模型对输入图片的大小信息不敏感,为了减少模型训练时的内存消耗,并实现批量训练,将所有的训练图片剪裁为209×273的统一大小。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">1.3.2 像素级分类的损失函数</h4>
                <div class="p1">
                    <p id="80">针对边缘检测这种像素级的分类问题,通常将图片的每个像素点的分类看作是一个二分类问题(边缘点与非边缘点),因此,本模型使用交叉熵作为每个像素点分类的代价函数。另外, <i>BSDS</i>500数据集中的每一个标签图(<i>ground truth</i>)都是由多个标记者标注而成,由于每个标记者对于边缘的认知不统一,因此该数据集中的边缘含有一些噪声。为了提高模型鲁棒性,本模型使用文献<citation id="237" type="reference">[<a class="sup">16</a>]</citation>中阈值法,即对标签图中的像素值进行归一化处理,将标签图变为边缘的概率图,并将概率值大于阈值η的像素点作为边缘点,将概率值为0的像素点作为非边缘点,而其他像素点作为有争议点,排除在外。与文献<citation id="238" type="reference">[<a class="sup">16</a>]</citation>相同,本模型每个像素点的损失函数表示为:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>l</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow></msub><mo>;</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo>=</mo><mspace width="0.25em" /><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>α</mi><mo>⋅</mo><mtext>l</mtext><mtext>b</mtext><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>;</mo><mtext> </mtext><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mn>0</mn><mspace width="0.25em" /><mo>&lt;</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≤</mo><mi>η</mi></mtd></mtr><mtr><mtd><mi>β</mi><mo>⋅</mo><mtext>l</mtext><mtext>b</mtext><mo stretchy="false">(</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&gt;</mo><mi>η</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>α</mi><mo>=</mo><mi>λ</mi><mo>⋅</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>+</mo></msup></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>+</mo></msup></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>-</mo></msup></mrow><mo>|</mo></mrow></mrow></mfrac></mtd></mtr><mtr><mtd><mi>β</mi><mo>=</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>-</mo></msup></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>+</mo></msup></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>-</mo></msup></mrow><mo>|</mo></mrow></mrow></mfrac></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中:|<b><i>Y</i></b><sup>+</sup>|和|<b><i>Y</i></b><sup>-</sup>|分别代表正负样本的数量,<i>α</i>、 <i>β</i>和超参数<i>λ</i>用来平衡正负样本数量差,<b><i>X</i></b><sub><i>i</i></sub>代表神经网络的激活值,<i>y</i><sub><i>i</i></sub>代表标签图中像素点<i>i</i>是边缘点的概率值,<b><i>W</i></b>代表神经网络中可学习的参数。这种损失函数可以促使模型作出最有把握的判断,将有争议的像素点排除在外,提高模型的鲁棒性。另外,通过实验发现:如果将超参数<i>η</i>设为0.5或更大的值,有些标签图出现边缘不连续的现象;而将超参数<i>η</i>减小时,有些标签图的边缘开始受到噪声的影响。因此,超参数<i>η</i>大小的设定要根据该模型具体的使用场景而定。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">1.3.3 多阶段训练方式</h4>
                <div class="p1">
                    <p id="84"><i>RCF</i>使用单阶段的训练方式,将深监督模块每个<i>stage</i>的损失与融合模块的损失之和作为整个模型的损失函数。然而,这两部分损失的重要程度是不一样的,深监督模块输出边缘图并计算损失只是为了辅助主干网络更好地优化,起辅助作用;而融合模块输出的边缘图为模型最终的输出,这部分损失起决定作用。<i>RCF</i>的这种简单相加的损失函数并没有体现出融合模块损失的重要性,因此,本模型使用一种多阶段的训练方式,先使用<i>RCF</i>的训练方式使网络达到一个较优的状态,然后去掉深监督模块的损失,只使用融合模块的损失继续进行训练,使模型达到最优的状态。通过这种多阶段训练方式,显式地提升融合模块输出的边缘图的重要性。具体过程如下:</p>
                </div>
                <div class="p1">
                    <p id="85">1)先采用<i>RCF</i>的训练方式,损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="86"><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">Ι</mi><mo>|</mo></mrow></mrow></munderover><mo stretchy="false">(</mo></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>l</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>;<i>W</i>)+<i>l</i>(<b><i>X</i></b><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msubsup></mrow></math></mathml>;<b><i>W</i></b>))      (3)</p>
                </div>
                <div class="p1">
                    <p id="89">其中:<b><i>X</i></b><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>代表第<i>k</i>个stage输出图片中的第<i>i</i>个像素点的激励值,而<b><i>X</i></b><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msubsup></mrow></math></mathml>代表融合模块输出的图片中的第<i>i</i>个像素点的激励值,|<b><i>I</i></b>|代表每张图片像素点的总数,<i>K</i>代表主干网络stage的数量,在这里<i>K</i> =5。利用该损失函数通过随机梯度下降算法训练本模型10个epoch,<i>batch size</i>设为16,基准学习率设为1E-6,每一层的学习率略有差异,<i>momentum</i>设为0.9,<i>weight decay</i>设为0.000 2。另外,与HED和RCF网络不同,本模型在训练过程中没有使用任何预训练模型,只对参数进行Gaussian分布的初始化。</p>
                </div>
                <div class="p1">
                    <p id="92">2)在第一步的基础上,只使用融合模块的损失继续对模型进行优化,此时本模型的损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="93"><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">Ι</mi><mo>|</mo></mrow></mrow></munderover><mi>l</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msubsup></mrow></math></mathml>;<b><i>W</i></b>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="95">其中:<i>l</i>(<b><i>X</i></b><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mrow><mi mathvariant="bold">f</mi><mi mathvariant="bold">u</mi><mi mathvariant="bold">s</mi><mi mathvariant="bold">e</mi></mrow></msubsup></mrow></math></mathml>;<b><i>W</i></b>)代表融合模块输出的图片中的第<i>i</i>个像素点的损失函数,|<b><i>I</i></b>|代表每张图片像素点的总数。用该损失函数继续训练模型直至收敛,显式地提升融合模块输出的边缘图的重要性,基准学习率设为1E-7,其他的参数设置与第1)步相同。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97">1.4 <b>与</b>RCF<b>比较</b></h4>
                <div class="p1">
                    <p id="98">本模型与<i>RCF</i>的区别主要体现在3个方面:首先,<i>RCF</i>的主干网络采用传统的<i>VGG</i>16的全部卷积层,而本模型将主干网络分为两部分:前半部分采用下采样结构充分提取边缘特征;后半部分考虑到图片的精度问题不进行下采样,采用空洞卷积技术增大感受野并且采用<i>SE</i>结构提取图片全局特征。然后,在特征融合部分,<i>RCF</i>只使用一层1×1卷积进行特征融合,而本模型使用一种多层的残差结构,实现多尺度特征的充分融合。最后,<i>RCF</i>使用的是单阶段的训练方式,而本模型使用了一种多阶段的训练方式。通过上述几点的改进,本模型性能比<i>RCF</i>网络有了一定程度的提升。</p>
                </div>
                <h3 id="99" name="99" class="anchor-tag">2 实验</h3>
                <div class="p1">
                    <p id="100">本文中的实验以<i>Pytorch</i> 0.4.0作为开发框架,系统为<i>Linux</i>,硬件设备包括一块<i>NVIDIA TITAN Xp GPU</i>,显卡内存为12 <i>GB</i>和一块<i>Intel i</i>7-6800<i>k CPU</i>,内存为64 <i>GB</i>。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">2.1 <b>图像金字塔技术</b></h4>
                <div class="p1">
                    <p id="102">图像金字塔技术是计算机视觉领域中的一种重要技术,文献<citation id="239" type="reference">[<a class="sup">16</a>]</citation>和<citation id="240" type="reference">[<a class="sup">24</a>]</citation>均使用该技术并取得了很好的效果。在测试阶段,本模型也使用图像金字塔技术,将测试集图片分别剪裁为0.8×、1.0×、1.2×三种规模,分别输入到模型,然后将同一图片三种不同规模的结果加权求平均,得出最终的边缘图,增强模型的鲁棒性。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">2.2 <b>实验分析</b></h4>
                <div class="p1">
                    <p id="104">边缘检测模型的检测指标主要包括:全局最佳(<i>Optimal</i><i>Dataset Scale</i>, <i>ODS</i>)和单图最佳(<i>Optimal Image Scale</i>, <i>OIS</i>)等。其中:<i>ODS</i>指测试集中所有图片使用固定同一阈值时的检测结果;<i>OIS</i>指对每一幅图像使用针对当前图片最佳阈值时的检测结果<citation id="241" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。本文模型输出的边缘图通过非最大抑制<citation id="242" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>处理后,用<i>Edge Box</i><citation id="243" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>工具包进行指标测量,各项指标与其他相关算法的对比如表1所示。</p>
                </div>
                <div class="area_img" id="105">
                    <p class="img_tit"><b>表</b>1 <b>本文算法与其他算法比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Comparison of the proposed algorithm with other algorithms</i></p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td><br />算法</td><td><i>ODS</i></td><td><i>OIS</i></td></tr><tr><td><br /><i>Canny</i><sup>[7]</sup></td><td>0.611</td><td>0.676</td></tr><tr><td><br /><i>SE</i><sup>[26]</sup></td><td>0.742</td><td>0.750</td></tr><tr><td><br /><i>N</i><sup>4</sup>-<i>Fields</i><sup>[10]</sup></td><td>0.753</td><td>0.769</td></tr><tr><td><br /><i>DeepEdge</i><sup>[12]</sup></td><td>0.753</td><td>0.772</td></tr><tr><td><br /><i>HED</i><sup>[13]</sup></td><td>0.788</td><td>0.804</td></tr><tr><td><br /><i>RCF</i><sup>[16]</sup></td><td>0.811</td><td>0.830</td></tr><tr><td><br />本文算法</td><td>0.817</td><td>0.838</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="106">通过对比可以看出,本文模型通过对<i>RCF</i>和<i>HED</i>的改进,性能比<i>RCF</i>和<i>HED</i>有了一定的提高。本文模型的<i>ODS</i>分别比<i>RCF</i>和<i>HED</i>提高0.6%和2.9%,而<i>OIS</i>分别比<i>RCF</i>和<i>HED</i>提高0.8%和3.4%。另外,本文模型输出的边缘图与原始图片的对比如图5所示。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 本文模型输出的边缘图与原图对比" src="Detail/GetImg?filename=images/JSJY201909010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 本文模型输出的边缘图与原图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Comparison between original image and edge image</i><i>output by the proposed model</i></p>

                </div>
                <div class="p1">
                    <p id="108">通过对比可知,本文模型可以精确地将图片中物体的边缘提取出来,并且在未使用图像金字塔技术和未进行非最大抑制处理的情况下,本文模型生成的边缘线条也非常清晰,达到了目前靠前的水平。</p>
                </div>
                <div class="p1">
                    <p id="109">为了进一步查看本文模型各模块的改进效果,进行了相关实验,实验结果如表2所示。针对主干网络,将使用<i>SE</i>结构和空洞卷积的模型与<i>RCF</i>进行对比实验,实验结果表明使用<i>SE</i>结构和空洞卷积的模型分别使<i>ODS</i>和<i>OIS</i>指标提升了0.33%和0.42%,这也证明了本模型的主干网络能够提取更多的全局信息并且会保留更多的边缘细节信息。另外,本模型主干网络每个<i>stage</i>输出的边缘图与<i>HED</i>和<i>RCF</i>的对比如图6所示。从左到右,前两列为<i>HED</i>与本模型各个<i>stage</i>输出边缘图的对比,后两列为<i>RCF</i>与本模型各个<i>stage</i>输出边缘图的对比。从上到下,每一列的5张图分别为主干网络1～5 <i>stage</i>输出的边缘图。通过对比可以看出,<i>HED</i>和<i>RCF</i>各个<i>stage</i>输出的边缘图线条粗糙且模糊,对细节的处理欠佳;而本文模型生成的边缘图保留了更多细节信息,线条更清晰。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909010_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 本文模型与HED、RCF各个stage输出边缘图的对比" src="Detail/GetImg?filename=images/JSJY201909010_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 本文模型与<i>HED</i>、<i>RCF</i>各个<i>stage</i>输出边缘图的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909010_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Comparison of output edge images between</i><i>the proposed model and other models at each stage</i></p>

                </div>
                <div class="p1">
                    <p id="111">针对融合模块,在使用残差结构后,模型的<i>ODS</i>和<i>OIS</i>指标分别提升了0.21%和0.28%,这说明该融合模块可以更充分地融合多尺度的特征。另外,当去掉残差结构中的<i>shortcut</i>支路而只保留层叠结构时,发现模型收敛情况变差,边缘图出现很多噪声并且变得不清晰。这也证明了残差结构可以使网络参数更好地优化,避免梯度消失和梯度爆炸等问题。此外,当对融合模块增添<i>ReLU</i>等非线性元素时,边缘图会变得模糊,这说明该模块的非线性结构会损坏很多有用的边缘信息,造成网络性能下降。</p>
                </div>
                <div class="area_img" id="112">
                    <p class="img_tit"><b>表</b>2 <b>各模块改进效果的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Comparison of improvement effect of each module</i></p>
                    <p class="img_note"></p>
                    <table id="112" border="1"><tr><td><br /><i>Rank</i></td><td><i>SE</i>+<i>Dilation</i></td><td><i>Residual</i></td><td><i>Multi</i>-<i>step</i></td><td><i>ODS</i></td><td><i>OIS</i></td></tr><tr><td><br />1</td><td></td><td></td><td></td><td>0.811 0</td><td>0.830 0</td></tr><tr><td><br />2</td><td>√</td><td></td><td></td><td>0.814 3</td><td>0.834 2</td></tr><tr><td><br />3</td><td>√</td><td>√</td><td></td><td>0.816 4</td><td>0.837 0</td></tr><tr><td><br />4</td><td>√</td><td>√</td><td>√</td><td>0.817 3</td><td>0.838 2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="113">最后,对本模型的多阶段训练方式进行实验,发现使用多阶段的训练方式后,深监督模块每个<i>stage</i>输出的边缘图会受到一定的影响,变得不清晰,但融合模块输出的边缘图却更精确,<i>ODS</i>和<i>OIS</i>分别提升了0.09%和0.12%,这也符合预期。</p>
                </div>
                <h3 id="114" name="114" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="115">本文提出了一个新的边缘检测模型。该模型基于<i>RCF</i>和<i>HED</i>的思想,在主干网络上加入<i>SE</i>模块,提升主干网络的表达能力,舍弃部分下采样,避免图片精度过度降低;采用空洞卷积技术提升网络的感受野,采用一种残差结构使多尺度特征充分融合;最后采用多阶段训练的方式使模型性能进一步提升。实验表明,本模型可以生成高质量的边缘图。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="153">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminatively Trained Sparse Code Gradients for ContourDetection">

                                <b>[1]</b> REN X F,BO L F.Discriminatively trained sparse code gradients for contour detection[C]// NIPS 2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012,1:584-592.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201502026&amp;v=MTI4NzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXIzS0lUZlRlN0c0SDlUTXJZOUhZb1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 张广燕,王俊平,邢润森,等.PSLIP新模型及在边缘检测和图像增强中的应用[J].电子学报,2015,43(2):377-382.(ZHANG G Y,WANG J P,XING R S,et al.A new PSLIP model and its application in edge detection and image enhancement[J].Acta Electronica Sinica,2015,43(2):377-382.)
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003427423&amp;v=MzEwMDY5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFU3L0pJbHc9Tmo3QmFyTzRIdEhQcTQxQ1lPa01ZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> KOHLI P,LADICKY L,TORR P H S.Robust higher order potentials for enforcing label consistency [J].International Journal of Computer Vision,2009,82(3):302-324.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201511008&amp;v=MjUzNjJDVVI3cWZadVpzRnlqbFVyM0tJVGZTZHJHNEg5VE5ybzlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 石美红,李青,赵雪青,等.一种基于保角相位的图像边缘检测新方法[J].电子与信息学报,2015,37(11):2594-2600.(SHI M H,LI Q,ZHAO X Q,et al.A new approach for image edge detection based on conformal phase [J].Journal of Electronics and Information Technology,2015,37(11):2594-2600.)
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object recognition by integrating multiple imagesegmentation">

                                <b>[5]</b> PANTOFARU C,SCHMID C,HERBERT M.Object recognition by integrating multiple image segmentations [C]//ECCV 2008:Proceedings of the 10th European Conference on Computer Vision,LNCS 5304.Berlin:Springer,2008:481-494.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Stanford hand-eye project">

                                <b>[6]</b> FELDMAN J A,FELDMAN G M,FALK G,et al.The Stanford hand-eye project [C]// IJCAI '69:Proceedings of the 1st International Joint Conference on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann,1969:521-526.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A computational approach to edge detection">

                                <b>[7]</b> CANNY J.A computational approach to edge detection [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1986,8(6):679-698.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Statistical edge detection: learning and evaluating edge cues">

                                <b>[8]</b> KONISHI S,YUILLE A L,COUGHLAN J M,et al.Statistical edge detection:learning and evaluating edge cues [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2003,25(1):57-74.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to detect natural image boundaries using local brightness, color, and texture cues">

                                <b>[9]</b> MARTIN D R,FOWLKES C C,MALIK J.Learning to detect natural image boundaries using local brightness,color,and texture cues[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2004,26(5):530-549.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=N4-fields:Neural network nearest neighbor fields for image transforms">

                                <b>[10]</b> GANIN Y,LEMPITSKY V.N<sup>4</sup>-Fields:neural network nearest neighbor fields for image transforms [C]// Proceedings of the 2014 Asian Conference on Computer Vision,LNCS 9004.Berlin:Springer,2014:536-551.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepcontour:A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection">

                                <b>[11]</b> SHEN W,WANG X G,WANG Y,et al.DeepContour:a deep convolutional feature learned by positive-sharing loss for contour detection[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:3982-3991.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepedge:A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection">

                                <b>[12]</b> BERTASIUS G,SHI J,TORRESANI L.DeepEdge:a multi-scale bifurcated deep network for top-down contour detection [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:4380-4389.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD6BB00EB93AC104C12EB55DFC97247B75&amp;v=MjIyNjdCYXJYS2JOSE0ydjFNWjVwOERYdzl2QmNSbjAxNFRRdVUzeHN5ZTdhVE43MmFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TGkrd3FvPU5qNw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> XIE S,TU Z.Holistically-nested edge detection [J].International Journal of Computer Vision,2017,125(1/2/3):3-18.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[14]</b> SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deeply-supervised nets">

                                <b>[15]</b> LEE C-Y,XIE S,GALLAGHER P,et al.Deeply-supervised nets [EB/OL].[2019- 01- 02].https://arxiv.org/pdf/1409.5185.pdf.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Richer convolutional features for edge detection">

                                <b>[16]</b> LIU Y,CHENG M,HU X,et al.Richer convolutional features for edge detection [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:5872-5881.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition[C/OL]">

                                <b>[17]</b> SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Squeeze-and-excitation networks[OL]">

                                <b>[18]</b> HU J,SHEN L,ALBANIE S,et al.Squeeze-and-excitation networks [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1709.01507.pdf.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">

                                <b>[19]</b> YU F,KOLTUN V.Multi-scale context aggregation by dilated convolutions [EB/OL].[2018- 08- 12].https://arxiv.org/pdf/1511.07122.pdf.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[20]</b> HE K M,ZHANG X Y,REN S Q,et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted boltzmann machines">

                                <b>[21]</b> NAIR V,HINTON G E.Rectified linear units improve restricted Boltzmann machines [C]// ICML '10:Proceedings of the 27th International Conference on Machine Learning.Madison,WI:Omnipress,2010:807-814.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A database of human segmented natural images and its application to evaluating seg- mentation algorithms and measuring ecological statistics.In: Werner B, ed">

                                <b>[22]</b> MARTIN D R,FOWLKES C C,TAL D,et al.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics [C]// ICCV 2001:Proceedings of the 8th IEEE International Conference on Computer Vision.Washington DC:IEEE Computer Society,2001,2:416-423.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The role of context for object detection and semantic segmentation in the wild">

                                <b>[23]</b> MOTTAGHI R,CHEN X,LIU X,et al.The role of context for object detection and semantic segmentation in the wild [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington DC:IEEE Computer Society,2014:891-898.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning hierarchical features for scene labeling">

                                <b>[24]</b> FARABET C,COUPRIE C,NAJMAN L,et al.Learning hierarchical features for scene labeling [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(8):1915-1929.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201608044&amp;v=MjQxNjZxZlp1WnNGeWpsVXIzS0x6N0JkN0c0SDlmTXA0OUJZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 刘胜男,宁纪锋.基于超像素的点互信息边界检测算法[J].计算机应用,2016,36(8):2296-2300.(LIU S N,NING J F.Super-pixel based pointwise mutual information boundary detection algorithm[J].Journal of Computer Applications,2016,36(8):2296-2300.)
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast edge detection using structured forests">

                                <b>[26]</b> DOLLÁR P,ZITNICK C L.Fast edge detection using structured forests [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1558-1570.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Edge boxes:locating object proposals from edges">

                                <b>[27]</b> ZITNICK C L,DOLLÁR P.Edge boxes:locating object proposals from edges [C]// Proceedings of the 2014 European Conference on Computer Vision,LNCS 8693.Berlin:Springer,2014:391-405.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909010" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909010&amp;v=MzA3MzZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXIzS0x6N0JkN0c0SDlqTXBvOUU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
