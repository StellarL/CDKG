<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136444487940000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911006%26RESULT%3d1%26SIGN%3dub0cU9L22XqscMsQZToRjOFka%252fA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911006&amp;v=MjA2ODdmWnVac0Z5bm5WcjdOTHo3QmQ3RzRIOWpOcm85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="1.1 SVDD&lt;b&gt;相关理论&lt;/b&gt;">1.1 SVDD<b>相关理论</b></a></li>
                                                <li><a href="#88" data-title="1.2 Bagging">1.2 Bagging</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="2 基于概率的支持向量数据描述方法 ">2 基于概率的支持向量数据描述方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="2.1 &lt;b&gt;概率函数&lt;/b&gt;">2.1 <b>概率函数</b></a></li>
                                                <li><a href="#107" data-title="2.2 P-SVDD">2.2 P-SVDD</a></li>
                                                <li><a href="#117" data-title="2.3 P-SVDD&lt;b&gt;算法实现&lt;/b&gt;">2.3 P-SVDD<b>算法实现</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#136" data-title="3.1 &lt;b&gt;数据集&lt;/b&gt;">3.1 <b>数据集</b></a></li>
                                                <li><a href="#139" data-title="3.2 &lt;b&gt;度量标准&lt;/b&gt;">3.2 <b>度量标准</b></a></li>
                                                <li><a href="#151" data-title="3.3 &lt;b&gt;实验结果及分析&lt;/b&gt;">3.3 <b>实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="图1 SVDD示意图">图1 SVDD示意图</a></li>
                                                <li><a href="#90" data-title="图2 集成学习示意图">图2 集成学习示意图</a></li>
                                                <li><a href="#100" data-title="图3 Bagging算法流程">图3 Bagging算法流程</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;实验数据集描述&lt;/b&gt;"><b>表</b>1 <b>实验数据集描述</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;分类结果混淆矩阵&lt;/b&gt;"><b>表</b>2 <b>分类结果混淆矩阵</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同&lt;/b&gt;&lt;i&gt;T&lt;/i&gt;&lt;b&gt;值下两种算法的准确率&lt;/b&gt;"><b>表</b>3 <b>不同</b><i>T</i><b>值下两种算法的准确率</b></a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同&lt;/b&gt;&lt;i&gt;T&lt;/i&gt;&lt;b&gt;值下两种算法的&lt;/b&gt;&lt;i&gt;F&lt;/i&gt;1&lt;b&gt;值&lt;/b&gt;"><b>表</b>4 <b>不同</b><i>T</i><b>值下两种算法的</b><i>F</i>1<b>值</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;i&gt;T&lt;/i&gt;=10&lt;b&gt;时两种算法的性能比较&lt;/b&gt;"><b>表</b>5 <i>T</i>=10<b>时两种算法的性能比较</b></a></li>
                                                <li><a href="#161" data-title="图4 两种方法的&lt;i&gt;F&lt;/i&gt;1值变化曲线">图4 两种方法的<i>F</i>1值变化曲线</a></li>
                                                <li><a href="#163" data-title="图5 两种方法的准确率变化曲线">图5 两种方法的准确率变化曲线</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="195">


                                    <a id="bibliography_1" title=" GHAHRAMANI Z.Probabilistic machine learning and artificial intelligence[J].Nature,2015,521(7553):452-459." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic machine learning and artificial intelligence">
                                        <b>[1]</b>
                                         GHAHRAMANI Z.Probabilistic machine learning and artificial intelligence[J].Nature,2015,521(7553):452-459.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_2" title=" HINTON G,DENG L,YU D,et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine,2012,29(6):82-97." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups">
                                        <b>[2]</b>
                                         HINTON G,DENG L,YU D,et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine,2012,29(6):82-97.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_3" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E,et al.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM,2017,60(6):84-90." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=MjU4OTBNMUQ3U1NSOGlmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeGJxNndhMD1OaWZJWThlL0g5SFByUG96Yko1NkRYZ3d1UjRhN2s1OFBuN2xyMg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E,et al.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM,2017,60(6):84-90.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_4" title=" 黄凯奇,任伟强,谭铁牛.图像物体分类与检测算法综述[J].计算机学报,2014,36(6):1-18.(HUANG K Q,REN W Q,TAN T N.A review on image object classification and detection[J].Chinese Journal of Computers,2014,36(6):1-18.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201406001&amp;v=MjE3MTg3Tkx6N0Jkckc0SDlYTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         黄凯奇,任伟强,谭铁牛.图像物体分类与检测算法综述[J].计算机学报,2014,36(6):1-18.(HUANG K Q,REN W Q,TAN T N.A review on image object classification and detection[J].Chinese Journal of Computers,2014,36(6):1-18.)
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_5" title=" KANDOLA E J,HOFMANN T,POGGIO T,et al.A neural probabilistic language model[J].Journal of Machine Learning Research,2006,194:137-186." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Neural Probabilistic Language Model">
                                        <b>[5]</b>
                                         KANDOLA E J,HOFMANN T,POGGIO T,et al.A neural probabilistic language model[J].Journal of Machine Learning Research,2006,194:137-186.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_6" title=" TAX D M J,DUIN R P W.Support vector data description[J].Machine Learning,2004,54(1):45-66." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340727&amp;v=MTQ5NDI5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFVyM05JVnM9Tmo3QmFyTzRIdEhOckl0Rlkra0lZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         TAX D M J,DUIN R P W.Support vector data description[J].Machine Learning,2004,54(1):45-66.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_7" title=" LEE K Y,KIM D W,LEE D,et al.Improving support vector data description using local density degree[J].Pattern Recognition,2005,38(10):1768-1771." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740085&amp;v=MzE2OTRLN0h0RE5xWTlGWSs4UERIUThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJVndSYXhjPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         LEE K Y,KIM D W,LEE D,et al.Improving support vector data description using local density degree[J].Pattern Recognition,2005,38(10):1768-1771.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_8" title=" KIM P J,CHANG H J,SONG D S,et al.Fast support vector data description using k-means clustering[C]// Proceedings of the 4th International Symposium on Neural Networks.Berlin:Springer,2007:506-514." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast support vector data description using k-means clustering">
                                        <b>[8]</b>
                                         KIM P J,CHANG H J,SONG D S,et al.Fast support vector data description using k-means clustering[C]// Proceedings of the 4th International Symposium on Neural Networks.Berlin:Springer,2007:506-514.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_9" title=" LUO J,LI B,WU C,et al.A fast SVDD algorithm based on decomposition and combination for fault detection[C]// Proceedings of the 2010 International Conference on Control and Automation.Piscataway:IEEE,2010:1924-1928." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fast SVBD algorithm based ondecomposition and combination for fault detection">
                                        <b>[9]</b>
                                         LUO J,LI B,WU C,et al.A fast SVDD algorithm based on decomposition and combination for fault detection[C]// Proceedings of the 2010 International Conference on Control and Automation.Piscataway:IEEE,2010:1924-1928.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_10" title=" HUANG G,CHEN H,ZHOU Z,et al.Two-class support vector data description[J].Pattern Recognition,2011,44(2):320-329." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738297&amp;v=MjkwODJud1plWnRGaW5sVXIzSUlWd1JheGM9TmlmT2ZiSzdIdEROcVk5RlkrZ0hEblUrb0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         HUANG G,CHEN H,ZHOU Z,et al.Two-class support vector data description[J].Pattern Recognition,2011,44(2):320-329.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_11" title=" 李勇,刘战东,张海军.不平衡数据的集成分类算法综述[J].计算机应用研究,2014,31(5):1287-1291.(LI Y,LIU Z D,ZHANG H J.Summary of integrated classification algorithm for unbalanced data[J].Application Research of Computers,2014,31(5):1287-1291.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201405003&amp;v=MjkwMDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcjdOTHo3U1pMRzRIOVhNcW85Rlo0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         李勇,刘战东,张海军.不平衡数据的集成分类算法综述[J].计算机应用研究,2014,31(5):1287-1291.(LI Y,LIU Z D,ZHANG H J.Summary of integrated classification algorithm for unbalanced data[J].Application Research of Computers,2014,31(5):1287-1291.)
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_12" title=" FREUND Y,SCHAPIRE R E.A decision-theoretic generalization of on-line learning and an application to boosting[J].Journal of Computer and System Sciences,1997,55(1):119-139." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601258112&amp;v=MjQ3MjhUTW53WmVadEZpbmxVcjNJSVZ3UmF4Yz1OaWZPZmJLN0h0RE5xWTlFWnU0SERYMDdvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         FREUND Y,SCHAPIRE R E.A decision-theoretic generalization of on-line learning and an application to boosting[J].Journal of Computer and System Sciences,1997,55(1):119-139.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_13" title=" BREIMAN L.Bagging predictors[J].Machine Learning,1996,24(2):123-140." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339495&amp;v=MjExMTJVcjNOSVZzPU5qN0Jhck80SHRITnJJeE1ZT0lLWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJs&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         BREIMAN L.Bagging predictors[J].Machine Learning,1996,24(2):123-140.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_14" title=" BREIMAN L.Random forests[J].Machine Learning,2001,45(1):5-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340271&amp;v=MjM3NTdJdEZadXdPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJsVXIzTklWcz1OajdCYXJPNEh0SE5y&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         BREIMAN L.Random forests[J].Machine Learning,2001,45(1):5-32.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_15" title=" 周志华.机器学习[M].北京:清华大学出版社,2016:171-189.(ZHOU Z H.Mechine Learning[M].Beijing:Tsinghua University Press,2016:171-189.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MDAwOTRkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3N0tKVjhSWEZxekdiQzRITlhPckkxTlkrc1BEQk04enhVU21E&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         周志华.机器学习[M].北京:清华大学出版社,2016:171-189.(ZHOU Z H.Mechine Learning[M].Beijing:Tsinghua University Press,2016:171-189.)
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_16" title=" QIAN Y,LI F,LIANG J,et al.Space structure and clustering of categorical data[J].IEEE Transactions on Neural Networks &amp;amp; Learning Systems,2016,27(10):2047-2059." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Space structure and clustering of categorical data">
                                        <b>[16]</b>
                                         QIAN Y,LI F,LIANG J,et al.Space structure and clustering of categorical data[J].IEEE Transactions on Neural Networks &amp;amp; Learning Systems,2016,27(10):2047-2059.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_17" title=" ZHOU Z.Ensemble Methods:Foundations and Algorithms[M].Boca Raton,FL:Taylor &amp;amp; Francis Group,2012:1-22." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble Methods:Foundations and Algorithms">
                                        <b>[17]</b>
                                         ZHOU Z.Ensemble Methods:Foundations and Algorithms[M].Boca Raton,FL:Taylor &amp;amp; Francis Group,2012:1-22.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_18" title=" QIAN Y,LI F,LIANG J,et al.Fusing monotonic decision trees[J].IEEE Transactions on Knowledge and Data Engineering,2015,27(10):2717-2728." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fusing monotonic decision trees">
                                        <b>[18]</b>
                                         QIAN Y,LI F,LIANG J,et al.Fusing monotonic decision trees[J].IEEE Transactions on Knowledge and Data Engineering,2015,27(10):2717-2728.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_19" title=" EFRON B.Bootstrap methods:another look at the jackknife[J].Breakthroughs in Statistics,1979,7(1):569-593." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bootstrap methods; another look at the jackknife">
                                        <b>[19]</b>
                                         EFRON B.Bootstrap methods:another look at the jackknife[J].Breakthroughs in Statistics,1979,7(1):569-593.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_20" title=" CAWLEY G,TALBOT N.Gunnar Raetsch&#39;s benchmark datasets [DB/OL].[2018- 11- 20].http://theoval.cmp.uea.ac.uk/～gcc/matlab/default.html#benchmarks." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gunnar Raetsch&amp;#39;&amp;#39;s benchmark datasets">
                                        <b>[20]</b>
                                         CAWLEY G,TALBOT N.Gunnar Raetsch&#39;s benchmark datasets [DB/OL].[2018- 11- 20].http://theoval.cmp.uea.ac.uk/～gcc/matlab/default.html#benchmarks.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_21" title=" NAGANJANEYULU S,KUPPA M R.A novel framework for class imbalance learning using intelligent under-sampling[J].Progress in Artificial Intelligence,2013,2(1):73-84." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel framework for class imbalance learning using intelligent under-sampling">
                                        <b>[21]</b>
                                         NAGANJANEYULU S,KUPPA M R.A novel framework for class imbalance learning using intelligent under-sampling[J].Progress in Artificial Intelligence,2013,2(1):73-84.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_22" title=" ZHANG X,SONG Q,WANG G,et al.A dissimilarity-based imbalance data classification algorithm[J].Applied Intelligence,2015,42(3):544-565." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A dissimilarity-based imbalance data classification algorithm">
                                        <b>[22]</b>
                                         ZHANG X,SONG Q,WANG G,et al.A dissimilarity-based imbalance data classification algorithm[J].Applied Intelligence,2015,42(3):544-565.
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_23" title=" JIANG K,LU J,XIA K.A novel algorithm for imbalance data classification based on genetic algorithm improved SMOTE[J].Arabian Journal for Science and Engineering,2016,41(8):3255-3266." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Novel Algorithm for Imbalance Data Classification Based on Genetic Algorithm Improved SMOTE">
                                        <b>[23]</b>
                                         JIANG K,LU J,XIA K.A novel algorithm for imbalance data classification based on genetic algorithm improved SMOTE[J].Arabian Journal for Science and Engineering,2016,41(8):3255-3266.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-19 14:15</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3134-3139 DOI:10.11772/j.issn.1001-9081.2019050823            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于概率的支持向量数据描述方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%99%A8&amp;code=08409648&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%A9%95%E5%A9%B7&amp;code=32464908&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王婕婷</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%A3%9E%E6%B1%9F&amp;code=31528567&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李飞江</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%B1%E5%AE%87%E5%8D%8E&amp;code=08454095&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钱宇华</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E4%BA%A7%E4%B8%9A%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=0176514&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学大数据科学与产业研究院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E4%B8%8E%E4%B8%AD%E6%96%87%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算智能与中文信息处理教育部重点实验室(山西大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学计算机与信息技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对目前概率机器学习方法在解决概率问题时具有较高的复杂度,而传统的支持向量数据描述(SVDD)作为一种核密度估计方法只能判断测试样本是否属于该类等问题,提出一种基于概率的支持向量数据描述方法。首先,利用传统的SVDD方法分别得到两类数据的数据描述,计算测试样本到超球体的距离;然后,构造一个将距离转换为概率的函数,提出一种基于概率的SVDD方法;同时,使用Bagging算法进行集成,进一步提高数据描述的性能。借鉴分类场景,将所提方法与传统的SVDD方法在Gunnar Raetsch的13种基准数据集上进行实验,实验结果表明,所提方法在准确率和F1值上优于传统的SVDD方法,并且其数据描述的性能有所提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A6%82%E7%8E%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">概率机器学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E6%8F%8F%E8%BF%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">支持向量数据描述;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">不确定性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    杨晨(1996—),女,山西临汾人,硕士研究生,主要研究方向:统计机器学习理论;;
                                </span>
                                <span>
                                    王婕婷(1991—),女,山西临汾人,博士研究生,CCF会员,主要研究方向:统计机器学习理论、强化学习;;
                                </span>
                                <span>
                                    李飞江(1990—),男,山西晋城人,博士研究生,CCF会员,主要研究方向:集群学习、无监督学习;;
                                </span>
                                <span>
                                    *钱宇华(1976—),男,山西晋城人,教授,博士,CCF会员,主要研究方向:机器学习、复杂网络、粗糙集理论、粒计算,电子邮箱jinchengqyh@126.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61672332);</span>
                                <span>山西省拔尖创新人才支持计划项目;</span>
                                <span>山西青年三晋学者项目;</span>
                                <span>山西省海外归国人员研究项目(2017023);</span>
                    </p>
            </div>
                    <h1><b>Support vector data description method based on probability</b></h1>
                    <h2>
                    <span>YANG Chen</span>
                    <span>WANG Jieting</span>
                    <span>LI Feijiang</span>
                    <span>QIAN Yuhua</span>
            </h2>
                    <h2>
                    <span>Research Institute of Big Data Science and Industry, Shanxi University</span>
                    <span>Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education (Shanxi University)</span>
                    <span>School of Computer and Information Technology, Shanxi University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In view of the high complexity of current probabilistic machine learning methods in solving probability problems, and the fact that traditional Support Vector Data Description(SVDD), as a kernel density estimation method, can only estimate whether the test samples belong to this class, a probability-based SVDD method was proposed. Firstly, the traditional SVDD method was used to obtain the data descriptions of two types of data, and the distance between the test sample and the hypersphere was calculated. Then, a function was constructed to convert the distance into probability, and an SVDD method based on probability was proposed. At the same time, Bagging algorithm was used for the integration to further improve the performance of data description. By referring to classification scenarios, the proposed method was compared with the traditional SVDD method on 13 kinds of benchmark datasets of Gunnar Raetsch. The experimental results show that the proposed method is better than the traditional SVDD method on accuracy and F1-value, and its performance of data description is improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=probabilistic%20machine%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">probabilistic machine learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Support%20Vector%20Data%20Description(SVDD)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Support Vector Data Description(SVDD);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ensemble&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ensemble;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=uncertainty&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">uncertainty;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YANG Chen, born in 1996, M. S. candidate. Her research interests include statistical machine learning theory. ;
                                </span>
                                <span>
                                    WANG Jieting, born in 1991, Ph. D. candidate. Her research interests include statistical machine learning theory, reinforcement learning. ;
                                </span>
                                <span>
                                    LI Feijiang, born in 1990, Ph. D. candidate. His research interests include group learning, unsupervised learning. ;
                                </span>
                                <span>
                                    QIAN Yuhua, born in 1976, Ph. D., professor. His research interests include machine learning, complex network, rough set theory, granular computing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-06</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China(61672332);</span>
                                <span>the Program for the Outstanding Innovative Teams of Higher Learning Institutions of Shanxi;</span>
                                <span>the Program for the San Jin Young Scholars of Shanxi;</span>
                                <span>the Overseas Returnee Research Program of Shanxi Province(2017023);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="50">机器学习主要是计算机通过已知数据训练模型,运用模型对未知数据进行预测。机器学习面临的未来的数据和将来的行为引发的结果是不确定的, 而概率机器学习<citation id="241" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>针对这种不确定性提供了一个概率框架,对模型和预测的不确定性进行表示和控制,因此在很多领域发挥着重要的作用,比如语音识别<citation id="242" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、图像分类<citation id="246" type="reference"><link href="199" rel="bibliography" /><link href="201" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>、文本预测<citation id="243" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等。Ghahramani<citation id="244" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>在2015年指出概率机器学习框架的主要思想是,学习可以被认为是通过推断合理的模型来解释预测数据,然后机器使用模型来预测未来的数据,并且根据这些预测作出合理的决策,不确定性在这一切中起着至关重要的作用,对于给定的数据而言,哪个模型是合适的尚不确定;同样,对未来数据和未来行动结果的预测也是不确定的,因此使用概率论描述这种不确定性,概率论为不确定性建模提供了一个框架<citation id="245" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。相对于二值输出而言,以概率作为输出的结果能够提供更多的信息,预测结果更加准确。</p>
                </div>
                <div class="p1">
                    <p id="51">目前已有的概率机器学习方法主要有贝叶斯学习、高斯过程和深度学习等,其中:神经网络是具有许多参数的可调非线性函数,深度学习的概念源于人工神经网络的研究,但是也有一些重要的改变,比如新的架构和算法创新、有更大的数据集、大规模的计算资源等。神经网络和深度学习系统在许多基本测试任务上都有很高的性能,但是也有一些局限性,比如需要大量的数据、需要大量的计算资源进行训练等,因此这些方法在解决概率问题时具有较高的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="52">近年来,数据描述问题得到了大量的研究。在域描述领域中,数据描述的任务不是以“区分不同的类”为目标的分类问题,也不是以“对每一个样本产生一个期望输出”为目标的回归问题,而是给出一个关于训练样本集的描述,同时检测那些与训练样本集相似的新样本。该描述应该覆盖训练样本集的所有样本,同时,在理想情况下,该描述应该能够将样本空间中其他所有可能的异常样本排除在外。</p>
                </div>
                <div class="p1">
                    <p id="53">20世纪末,Tax等<citation id="247" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在支持向量机(Support Vector Machine, SVM)的基础上提出了一种数据描述方法,即支持向量数据描述(Support Vector Data Description, SVDD)。SVDD方法是一种基于边界数据(即支持向量)的描述方法,其思想是寻找一个几乎包含所有目标样本并且体积最小的超球体。SVDD方法相对于贝叶斯学习、高斯过程和深度学习等具有算法复杂性低、扩展性强、对数据规模需求低等优点。Lee等<citation id="248" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了一种改进的局部密度支持向量数据描述方法(Density-induced Support Vector Data Description, D-SVDD),结果表明,D-SVDD的性能优于SVDD和<i>k</i>近邻数据描述方法。Kim等<citation id="249" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>针对SVDD在处理大数据集时存在一定的局限性提出了一种基于<i>k</i>均值聚类的快速SVDD算法,该方法与原始SVDD方法有相同的结果,并且降低了计算成本。Luo等<citation id="250" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>针对矩阵操作的空间复杂性,提出了一种基于分解和组合策略的快速SVDD算法,该算法在实现样本描述方面优于原SVDD算法,尤其是在大规模样本数据集上。然而在现实生活中,目标数据集往往包含一类以上的对象,每一类对象都需要同时进行描述和区分,在这种情况下,传统的SVDD只能给出一个描述目标数据集,不区分不同的目标类数据集,或者给目标数据集中每个类的对象一个描述,Huang等<citation id="251" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了一种改进的支持向量数据描述方法——两类支持向量数据描述(Two-Class Support Vector Data Description, TC-SVDD)。</p>
                </div>
                <div class="p1">
                    <p id="54">传统的SVDD方法作为一种核密度估计方法,对未知数据进行预测主要是先对已知样本进行数据描述,然后判断测试样本是否属于该类,是二值输出;相对于二值输出而言,以概率作为输出的结果能够提供更多的信息,获得更准确的数据描述。而传统的SVDD方法判断测试样本是否属于该类,主要是计算测试数据到超球体中心的距离,但是该距离无法得知测试数据所属类别的概率。因此,本文提出了一个将距离转换为概率的函数,在本文第2章详细解释。</p>
                </div>
                <div class="p1">
                    <p id="55">集成学习算法<citation id="252" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是通过集成多个学习器共同决策的机器学习技术,通过不同的样本集训练有差异的个体学习器,然后使用某种规则把个体学习器的结果进行集成,得到的集成分类器可以有效提高单个分类器的学习效果。根据个体学习器的生成方式,目前的集成学习方法大致分为两大类,即个体学习器间存在强依赖关系、必须串行生成的序列化方法,以及个体学习器间不存在强依赖关系、可同时生成的并行化方法。前者的代表是Boosting<citation id="253" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,后者的代表是Bagging<citation id="254" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和随机森林(Random Forest)<citation id="255" type="reference"><link href="221" rel="bibliography" /><link href="223" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。借鉴分类场景,本文主要使用Bagging集成算法来提高传统SVDD方法的数据描述能力。</p>
                </div>
                <div class="p1">
                    <p id="56">为了解决目前深度学习等概率机器学习在解决概率问题时具有较高的复杂度,而且传统的SVDD方法只能判断测试样本是否属于该类等问题,本文首先介绍了SVDD方法和Bagging集成算法的相关知识;然后构造了一个将距离转换为概率的函数,提出了一种基于概率的支持向量数据描述方法(SVDD method based on Probability, P-SVDD);最后,实验验证了P-SVDD方法的有效性。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="58">本文算法是基于概率的支持向量数据描述方法,因此在本章简要回顾了SVDD方法和Bagging集成算法的相关知识。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.1 SVDD<b>相关理论</b></h4>
                <div class="p1">
                    <p id="60">支持向量数据描述(SVDD)是一种对目标数据集进行球形描述并用于离群点检测或分类的数据描述方法。它的基本思想是通过利用核函数将样本从低维空间映射到高维特征空间,并寻求一个超球体,尽可能在使超球体的半径小的情况下把所有训练样本包围起来,并以最小超球体的边界对数据进行描述和分类。在测试阶段,判断测试样本与所构造的超球体的球心之间的距离是否小于半径:如果小于超球体半径则认为是目标点;若大于超球体的半径则被认为是异常点。SVDD示意图如图1所示。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911006_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SVDD示意图" src="Detail/GetImg?filename=images/JSJY201911006_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SVDD示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911006_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic diagram of SVDD</p>

                </div>
                <div class="p1">
                    <p id="62">设<i>X</i>={<i><b>x</b></i><sub><i>i</i></sub>,<i>i</i>=1,2,…,<i>n</i>}为<b>R</b><sup><i>d</i></sup>空间中的训练数据集,<i><b>a</b></i>和<i>R</i>分别表示球面的中心和半径。该目标被表述为一个二次规划问题:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>R</mi><mo>,</mo><mi mathvariant="bold-italic">a</mi><mo>,</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mo stretchy="false">(</mo><mi>R</mi><mo>,</mo><mi mathvariant="bold-italic">a</mi><mo>,</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>C</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>ξ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mspace width="0.25em" /></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">s.t.  ‖<i><b>x</b></i><sub><i>i</i></sub>-<i><b>a</b></i>‖<sup>2</sup>≤<i>R</i><sup>2</sup>+<i>ξ</i><sub><i>i</i></sub>, <i>ξ</i><sub><i>i</i></sub>≥0, <i>i</i>=1,2,…,<i>n</i></p>
                </div>
                <div class="p1">
                    <p id="65">其中<i>ξ</i><sub><i>i</i></sub>是一个松弛变量,允许训练数据集中出现异常值的可能性; <i>C</i>为误差惩罚系数,用来平衡错分误差和球体体积。为了该二次规划问题进行求解,引入拉格朗日乘子<i>α</i><sub><i>i</i></sub>,<i>γ</i><sub><i>i</i></sub>构造拉格朗日函数:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi>R</mi><mo>,</mo><mi mathvariant="bold-italic">a</mi><mo>,</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>γ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>C</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>ξ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">[</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">a</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>γ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中拉格朗日乘子<i>α</i><sub><i>i</i></sub>≥0,<i>γ</i><sub><i>i</i></sub>≥0。将<i>R</i>,<i><b>a</b></i>,<i>ξ</i><sub><i>i</i></sub>偏导数设为零得到约束条件:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula"><mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>R</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>:<mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula"><mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">a</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>:<mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">a</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="70"><mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>: <i>C</i>-<i>α</i><sub><i>i</i></sub>-<i>γ</i><sub><i>i</i></sub>=0 ⇒ 0≤<i>α</i><sub><i>i</i></sub>≤<i>C</i>      (5)</p>
                </div>
                <div class="p1">
                    <p id="71">将式(3)～(5)代入式(2)得到式(1)的对偶问题:</p>
                </div>
                <div class="area_img" id="72">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201911006_07200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="72">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201911006_07201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="74">其中(<i><b>x</b></i>·<i><b>y</b></i>)是<i><b>x</b></i>和<i><b>y</b></i>的内积。在第2章引入核函数代替内积。</p>
                </div>
                <div class="p1">
                    <p id="75">训练对象<i><b>x</b></i><sub><i>i</i></sub>及其对应的<i>α</i><sub><i>i</i></sub>满足以下三个条件之一:</p>
                </div>
                <div class="p1">
                    <p id="76">‖<i><b>x</b></i><sub><i>i</i></sub>-<i><b>a</b></i>‖<sup>2</sup>&lt;<i>R</i><sup>2</sup> ⇒ <i>α</i><sub><i>i</i></sub>=0      (7)</p>
                </div>
                <div class="p1">
                    <p id="77">‖<i><b>x</b></i><sub><i>i</i></sub>-<i><b>a</b></i>‖<sup>2</sup>=<i>R</i><sup>2</sup> ⇒ 0&lt;<i>α</i><sub><i>i</i></sub>&lt;<i>C</i>      (8)</p>
                </div>
                <div class="p1">
                    <p id="78">‖<i><b>x</b></i><sub><i>i</i></sub>-<i><b>a</b></i>‖<sup>2</sup>&gt;<i>R</i><sup>2</sup> ⇒ <i>α</i><sub><i>i</i></sub>=<i>C</i>      (9)</p>
                </div>
                <div class="p1">
                    <p id="79">系数为<i>α</i><sub><i>i</i></sub>≥0的对象称为支持向量。可以从上面的关系看出,在球面的描述中只需要支持向量。球面中心可由式(4)计算得到,通过计算球面中心到任意0&lt;<i>α</i><sub><i>i</i></sub>&lt;<i>C</i>的支持向量的距离得到球面半径<i>R</i>。根据定义,<i>R</i><sup>2</sup>是从球面中心<i><b>a</b></i>到边界(任意一个支持向量)的距离。因此:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mn>2</mn><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中<i><b>x</b></i><sub><i>k</i></sub>∈<i>SV</i><sub>&lt;</sub><sub><i>C</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="82">为了确定测试样本<i><b>z</b></i>是否在球体内,必须计算<i><b>z</b></i>到球体中心的距离。当该距离小于半径时,则接受测试对象<i><b>z</b></i>,即:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">a</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>⋅</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>-</mo><mn>2</mn><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>≤</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">因此,分类器的数学表示为:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>V</mtext><mtext>D</mtext><mtext>D</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>;</mo><mi mathvariant="bold-italic">α</mi><mo>,</mo><mi>R</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ι</mi><mo stretchy="false">(</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">a</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>≤</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>⋅</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>-</mo><mn>2</mn><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>≤</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中<i>Ι</i>为示性函数:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>当</mtext><mi>A</mi><mtext>成</mtext><mtext>立</mtext></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">1.2 Bagging</h4>
                <div class="p1">
                    <p id="89">集成学习是一种重要的数据挖掘方法<citation id="256" type="reference"><link href="225" rel="bibliography" /><link href="227" rel="bibliography" /><link href="229" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>,通过构造并结合多个学习器来解决问题,能够显著提高学习系统的泛化能力。图2显示出集成学习算法的一般结构:先产生一组个体学习器,再用某种策略将它们结合起来。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911006_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 集成学习示意图" src="Detail/GetImg?filename=images/JSJY201911006_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 集成学习示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911006_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Schematic diagram of ensemble learning</i></p>

                </div>
                <div class="p1">
                    <p id="91"><i>Bagging</i>算法是集成学习方法最著名的代表,是一种基于<i>Bootstrap</i><citation id="257" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>的统计方法,其基本原理是对数据集进行随机有放回的重复采样,得到不同的训练集,然后在每个训练集训练出一个基分类器,接着对测试样本进行预测,最后采用投票的方式获得测试结果。整体算法如算法1所示。</p>
                </div>
                <div class="area_img" id="193">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201911006_19300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="193">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201911006_19301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="99">图3是Bagging算法的整体流程,对多个基分类器同时进行训练,因此Bagging算法具有运行效率高的特点。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911006_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Bagging算法流程" src="Detail/GetImg?filename=images/JSJY201911006_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Bagging算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911006_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Flowchart of Bagging algorithm</p>

                </div>
                <h3 id="101" name="101" class="anchor-tag">2 基于概率的支持向量数据描述方法</h3>
                <div class="p1">
                    <p id="102">本文提出基于概率的支持向量数据描述方法<i>P</i>-<i>SVDD</i>,基本思想如下:利用传统的<i>SVDD</i>方法分别对两类数据进行训练,得到两个超球体的数据描述,然后计算测试样本到超球体的距离,并提出一个将距离转换为概率的函数,将<i>SVDD</i>方法的测试距离映射为概率,得到测试样本分别属于每类的概率,同时利用<i>Bagging</i>集成算法,进一步提升<i>SVDD</i>方法的数据描述能力。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">2.1 <b>概率函数</b></h4>
                <div class="p1">
                    <p id="104"><i>SVDD</i>方法作为一种核密度估计方法,对未知数据预测的结果主要是二值输出, 但是以概率作为输出的结果能够提供更多的信息,获得更准确的数据描述。而传统的<i>SVDD</i>方法判断测试样本是否属于该类,主要是计算测试数据到超球体中心的距离,但是该距离无法得知测试数据所属类别的概率,因此,提出一个将距离转换为概率的函数,即:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false">(</mo><mi>d</mi><mo>-</mo><mi>R</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>1</mn><mo>/</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">通过证明该公式可以知道:当<i>d</i>逐渐增大时,<i>P</i>值逐渐减小;当<i>d</i>=0时,<i>P</i>趋近于1;当<i>d</i>=<i>R</i>时,<i>P</i>等于0.5;当<i>d</i>趋近于无穷时,<i>P</i>趋近于0。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">2.2 P-SVDD</h4>
                <div class="p1">
                    <p id="108">给定一个数据集<i>S</i>,对其进行有放回的抽样,构成训练集<i><b>D</b></i>,其中<i><b>D</b></i>=<i><b>D</b></i><sup>+</sup>∪<i><b>D</b></i><sup>-</sup>,<i><b>D</b></i><sup>+</sup>表示正类样本,<i><b>D</b></i><sup>-</sup>表示负类样本,利用传统的SVDD方法分别对<i><b>D</b></i><sup>+</sup>、 <i><b>D</b></i><sup>-</sup>进行训练,在这里引入核函数<i>K</i>(·),得到超球体中心<i><b>a</b></i>和半径<i>R</i>:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">a</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mn>2</mn><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">然后计算测试数据<i><b>z</b></i>到超球体的距离:</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">a</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>⋅</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>-</mo><mn>2</mn><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">因此,</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>V</mtext><mtext>D</mtext><mtext>D</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>;</mo><mi mathvariant="bold-italic">α</mi><mo>,</mo><mi>R</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>⋅</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>-</mo><mn>2</mn><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>≤</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">其中<i>K</i>(·)使用高斯核函数,即:</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mo>-</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>s</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">利用式(13)计算测试样本所属类别概率<i>P</i><sub>1</sub>、<i>P</i><sub>2</sub>,最后判断测试数据所属类别。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">2.3 P-SVDD<b>算法实现</b></h4>
                <div class="p1">
                    <p id="118">算法2 基于概率的支持向量数据描述方法</p>
                </div>
                <div class="area_img" id="194">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201911006_19400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="134" name="134" class="anchor-tag">3 实验与分析</h3>
                <div class="p1">
                    <p id="135">为了验证基于概率的支持向量数据描述方法的效果,本文在<i>Gunnar Raetsch</i><citation id="258" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的13种基准数据集上进行了实验,将其与传统的<i>SVDD</i>方法进行了比较。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136">3.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="137">本文将在<i>Gunnar Raetsch</i>的13种基准数据集上对本文方法和传统的<i>SVDD</i>方法进行了测试,13种数据集详细信息描述如表1所示。</p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表</b>1 <b>实验数据集描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Description of experimental datasets</i></p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td><br />数据集</td><td>属性数</td><td>样本数</td><td>正类样本数</td><td>负类样本数</td></tr><tr><td><i>banana</i></td><td>2</td><td>5 300</td><td>2 376</td><td>2 924</td></tr><tr><td><br /><i>breast</i>_<i>cancer</i></td><td>9</td><td>263</td><td>77</td><td>186</td></tr><tr><td><br /><i>diabetis</i></td><td>8</td><td>768</td><td>268</td><td>500</td></tr><tr><td><br /><i>flare</i>_<i>solar</i></td><td>9</td><td>144</td><td>94</td><td>50</td></tr><tr><td><br /><i>german</i></td><td>20</td><td>1 000</td><td>300</td><td>700</td></tr><tr><td><br /><i>heart</i></td><td>13</td><td>270</td><td>120</td><td>150</td></tr><tr><td><br /><i>image</i></td><td>18</td><td>2 086</td><td>1 188</td><td>898</td></tr><tr><td><br /><i>ringnorm</i></td><td>20</td><td>7 400</td><td>3 664</td><td>3 736</td></tr><tr><td><br /><i>splice</i></td><td>60</td><td>2 991</td><td>1 344</td><td>1 647</td></tr><tr><td><br /><i>thyroid</i></td><td>5</td><td>215</td><td>65</td><td>150</td></tr><tr><td><br /><i>titanic</i></td><td>3</td><td>24</td><td>14</td><td>10</td></tr><tr><td><br /><i>twonorm</i></td><td>20</td><td>7 400</td><td>3703</td><td>3 697</td></tr><tr><td><br /><i>waveform</i></td><td>21</td><td>5 000</td><td>1647</td><td>3 353</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="139" name="139">3.2 <b>度量标准</b></h4>
                <div class="p1">
                    <p id="140">分类问题中,经常使用混淆矩阵<citation id="260" type="reference"><link href="235" rel="bibliography" /><link href="237" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>进行度量,如表2所示。其中<i>TP</i>(<i>True Positives</i>)表示正类样本被正确分类的数量,<i>FN</i>(<i>False Negatives</i>)表示正类样本被错误分类的数量,<i>FP</i>(<i>False Positives</i>)表示负类样本被错误分类的数量,<i>TN</i>(<i>True Negatives</i>)表示负类样本被正确分类的数量<citation id="259" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit"><b>表</b>2 <b>分类结果混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Confusion matrix of classification results</i></p>
                    <p class="img_note"></p>
                    <table id="141" border="1"><tr><td rowspan="2"><br />真实情况</td><td colspan="2"><br />预测结果</td></tr><tr><td><br />正例</td><td>反例</td></tr><tr><td><br />正例</td><td>真正例(<i>TP</i>)</td><td>假反例(<i>FN</i>)</td></tr><tr><td><br />反例</td><td>假正例(<i>FP</i>)</td><td>真反例(<i>TN</i>)</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="142" name="142">1)查准率:</h4>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="144" name="144">2)查全率:</h4>
                <div class="p1">
                    <p id="145" class="code-formula">
                        <mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="146" name="146">3)准确率:</h4>
                <div class="p1">
                    <p id="147" class="code-formula">
                        <mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="148" name="148">4)基于查准率与查全率的调和平均定义的<i>F</i>1:</h4>
                <div class="p1">
                    <p id="149" class="code-formula">
                        <mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>×</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="150">本文选用<i>accuracy</i>、<i>F</i>1值作为评价算法性能的标准。</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151">3.3 <b>实验结果及分析</b></h4>
                <div class="p1">
                    <p id="152">实验使用<i>Matlab</i>作为仿真环境并将<i>LIBSVM</i> 工具箱作为实现工具。首先,对所有数据进行极差归一化处理,使所有数据取值在[0,1]内;然后,将数据集按7∶3的比例随机划分为训练集与测试集。在实验过程中,把本文所提的<i>P</i>-<i>SVDD</i>方法与原始的 <i>SVDD</i>方法作为数据描述学习算法,然后把10次独立重复实验的结果进行比较。 表3和表4分别列出了当数据描述的集成个数<i>T</i>为10、20、30时,两种算法在13种数据集的准确率和<i>F</i>1值。</p>
                </div>
                <div class="area_img" id="153">
                                            <p class="img_tit">
                                                <b>表</b>3 <b>不同</b><i>T</i><b>值下两种算法的准确率</b>
                                                    <br />
                                                Tab. 1 Accuracies of two algorithms under different <i>T</i> values
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911006_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201911006_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911006_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 不同T值下两种算法的准确率" src="Detail/GetImg?filename=images/JSJY201911006_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="154">从表3可以看出,当数据描述的集成个数<i>T</i>取不同值时,在借鉴分类场景的情况下,基于概率的支持向量数据描述方法的准确率明显高于传统的SVDD方法。比如在banana数据集上,P-SVDD方法的准确率比传统的SVDD方法高约6个百分点。</p>
                </div>
                <div class="area_img" id="155">
                    <p class="img_tit"><b>表</b>4 <b>不同</b><i>T</i><b>值下两种算法的</b><i>F</i>1<b>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 <i>F</i>1-values of two algorithms under different <i>T</i> values</p>
                    <p class="img_note"></p>
                    <table id="155" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="3"><br />P-SVDD</td><td rowspan="2"></td><td colspan="3"><br />SVDD</td></tr><tr><td><br /><i>T</i>=10</td><td><i>T</i>=20</td><td><i>T</i>=30</td><td><br /><i>T</i>=10</td><td><i>T</i>=20</td><td><i>T</i>=30</td></tr><tr><td>banana</td><td>0.895 2</td><td>0.897 7</td><td>0.891 1</td><td></td><td>0.793 9</td><td>0.799 3</td><td>0.797 7</td></tr><tr><td><br />breast_cancer</td><td>0.500 7</td><td>0.492 6</td><td>0.521 9</td><td></td><td>0.304 1</td><td>0.351 2</td><td>0.302 3</td></tr><tr><td><br />diabetis</td><td>0.524 2</td><td>0.529 5</td><td>0.506 0</td><td></td><td>0.322 6</td><td>0.297 5</td><td>0.316 2</td></tr><tr><td><br />flare_solar</td><td>0.825 6</td><td>0.805 3</td><td>0.800 8</td><td></td><td>0.313 8</td><td>0.289 3</td><td>0.267 6</td></tr><tr><td><br />german</td><td>0.498 0</td><td>0.511 0</td><td>0.520 0</td><td></td><td>0.263 7</td><td>0.267 2</td><td>0.411 2</td></tr><tr><td><br />heart</td><td>0.789 4</td><td>0.799 3</td><td>0.764 7</td><td></td><td>0.310 7</td><td>0.351 2</td><td>0.323 8</td></tr><tr><td><br />image</td><td>0.965 1</td><td>0.964 5</td><td>0.960 6</td><td></td><td>0.717 3</td><td>0.712 1</td><td>0.692 9</td></tr><tr><td><br />ringnorm</td><td>0.924 6</td><td>0.916 3</td><td>0.919 6</td><td></td><td>0.880 3</td><td>0.880 1</td><td>0.876 7</td></tr><tr><td><br />splice</td><td>0.524 8</td><td>0.524 7</td><td>0.519 3</td><td></td><td>0.400 3</td><td>0.366 9</td><td>0.371 2</td></tr><tr><td><br />thyroid</td><td>0.903 8</td><td>0.899 3</td><td>0.890 7</td><td></td><td>0.657 4</td><td>0.627 7</td><td>0.592 2</td></tr><tr><td><br />titanic</td><td>0.714 2</td><td>0.703 5</td><td>0.730 9</td><td></td><td>0.508 5</td><td>0.469 8</td><td>0.431 2</td></tr><tr><td><br />twonorm</td><td>0.977 2</td><td>0.975 9</td><td>0.977 8</td><td></td><td>0.807 8</td><td>0.813 1</td><td>0.813 4</td></tr><tr><td><br />waveform</td><td>0.588 9</td><td>0.584 0</td><td>0.596 9</td><td></td><td>0.462 3</td><td>0.471 1</td><td>0.472 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="156">从表4可以看出,基于概率的支持向量数据描述方法的效果明显好于传统的SVDD方法。同样在banana数据集上,P-SVDD方法的<i>F</i>1值比传统的SVDD方法高约10个百分点。</p>
                </div>
                <div class="p1">
                    <p id="157">同时从表3和表4可以看出,当集成个数<i>T</i>取不同值时,两种算法的准确率和<i>F</i>1值差别并不是很大。 因此,为了更全面地评价基于概率的支持向量数据描述方法,对比分析了当数据描述的集成个数为10时,基于概率的支持向量数据描述方法与传统的SVDD方法对应10次实验的平均值(Mean)、最大值(Max)、最小值(Min)以及标准差(Std),结果如表5所示。</p>
                </div>
                <div class="p1">
                    <p id="158">从表5可以得到与表3和表4相同的结果,当数据描述的集成个数<i>T</i>=10时,基于概率的支持向量数据描述方法的效果明显优于传统的SVDD方法,仅在个别数据集上,本文所提方法的标准差高于传统的SVDD方法,但是差别不大。因此,在借鉴分类场景的情况下,本文所提方法有效地提高了传统的SVDD方法的描述性能。</p>
                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表</b>5 <i>T</i>=10<b>时两种算法的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Performance comparison of two algorithms when <i>T</i>=10</p>
                    <p class="img_note"></p>
                    <table id="159" border="1"><tr><td rowspan="2"><br />序号</td><td rowspan="2">数据集</td><td colspan="4"><br />P-SVDD</td><td rowspan="2"></td><td colspan="4"><br />SVDD</td></tr><tr><td><br />Mean</td><td>Max</td><td>Min</td><td>Std</td><td><br />Mean</td><td>Max</td><td>Min</td><td>Std</td></tr><tr><td>1</td><td>banana</td><td>0.904 9</td><td>0.9189</td><td>0.894 3</td><td>0.008 7</td><td></td><td>0.842 0</td><td>0.861 6</td><td>0.815 7</td><td>0.014 0</td></tr><tr><td><br />2</td><td>breast_cancer</td><td>0.815 2</td><td>0.860 8</td><td>0.746 8</td><td>0.044 3</td><td></td><td>0.776 0</td><td>0.835 4</td><td>0.734 2</td><td>0.037 3</td></tr><tr><td><br />3</td><td>diabetis</td><td>0.786 5</td><td>0.817 4</td><td>0.747 8</td><td>0.020 9</td><td></td><td>0.731 7</td><td>0.752 2</td><td>0.708 7</td><td>0.014 1</td></tr><tr><td><br />4</td><td>flare_solar</td><td>0.741 9</td><td>0.837 2</td><td>0.627 9</td><td>0.073 9</td><td></td><td>0.404 7</td><td>0.511 6</td><td>0.232 6</td><td>0.087 8</td></tr><tr><td><br />5</td><td>german</td><td>0.801 3</td><td>0.836 7</td><td>0.780 0</td><td>0.016 3</td><td></td><td>0.749 7</td><td>0.803 3</td><td>0.696 7</td><td>0.032 0</td></tr><tr><td><br />6</td><td>heart</td><td>0.822 2</td><td>0.876 5</td><td>0.765 4</td><td>0.033 0</td><td></td><td>0.663 0</td><td>0.728 4</td><td>0.580 3</td><td>0.047 7</td></tr><tr><td><br />7</td><td>image</td><td>0.959 3</td><td>0.968 1</td><td>0.944 1</td><td>0.008 4</td><td></td><td>0.745 5</td><td>0.774 8</td><td>0.702 9</td><td>0.024 5</td></tr><tr><td><br />8</td><td>ringnorm</td><td>0.919 3</td><td>0.928 4</td><td>0.908 6</td><td>0.008 0</td><td></td><td>0.891 5</td><td>0.902 7</td><td>0.878 8</td><td>0.008 2</td></tr><tr><td><br />9</td><td>splice</td><td>0.709 5</td><td>0.744 7</td><td>0.687 9</td><td>0.018 5</td><td></td><td>0.662 2</td><td>0.685 6</td><td>0.632 1</td><td>0.017 4</td></tr><tr><td><br />10</td><td>thyroid</td><td>0.942 2</td><td>0.984 4</td><td>0.875 0</td><td>0.041 0</td><td></td><td>0.837 5</td><td>0.906 3</td><td>0.734 4</td><td>0.057 1</td></tr><tr><td><br />11</td><td>titanic</td><td>0.642 9</td><td>0.857 1</td><td>0.428 6</td><td>0.181 3</td><td></td><td>0.428 6</td><td>0.714 3</td><td>0.142 9</td><td>0.233 3</td></tr><tr><td><br />12</td><td>twonorm</td><td>0.976 9</td><td>0.990 5</td><td>0.946 0</td><td>0.013 0</td><td></td><td>0.838 9</td><td>0.864 0</td><td>0.826 6</td><td>0.011 8</td></tr><tr><td><br />13</td><td>waveform</td><td>0.809 3</td><td>0.832 0</td><td>0.792 7</td><td>0.012 6</td><td></td><td>0.769 7</td><td>0.783 3</td><td>0.754 7</td><td>0.010 5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="160">为了更好地对比本文所提方法相对于传统SVDD方法的优势,图4绘制了当数据描述的集成个数<i>T</i>=10时,在13种数据集上两种方法<i>F</i>1值的测试结果趋势曲线。</p>
                </div>
                <div class="area_img" id="161">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911006_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 两种方法的F1值变化曲线" src="Detail/GetImg?filename=images/JSJY201911006_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 两种方法的<i>F</i>1值变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911006_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Variation curves of <i>F</i>1-value of two methods</p>

                </div>
                <div class="p1">
                    <p id="162">图5绘制了<i>T</i>=10时,在13种数据集上两种方法的平均值(Mean)、最大值(Max)以及最小值(Min)的测试结果趋势曲线。通过这两个图可以看出借鉴分类场景,本文所提出的方法相对于传统的SVDD方法而言,描述性能有所上升。</p>
                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911006_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 两种方法的准确率变化曲线" src="Detail/GetImg?filename=images/JSJY201911006_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 两种方法的准确率变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911006_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Variation curves of accuracy of two methods</p>

                </div>
                <h3 id="164" name="164" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="165">为了解决目前深度学习等概率机器学习在解决概率问题时具有较高的复杂度,而且传统的<i>SVDD</i>方法只能判断测试样本是否属于该类等问题,本文提出了基于概率的支持向量数据描述方法<i>P</i>-<i>SVDD</i>,通过使用传统的<i>SVDD</i>方法对两类数据分别进行训练,并且使用本文提出的概率函数计算测试样本所属类别的概率,同时使用了<i>Bagging</i>集成算法,最终在借鉴分类场景的情况下,传统的<i>SVDD</i>方法的描述性能得到了提升。本文主要是将概率知识应用到<i>SVDD</i>方法上,而且在算法集成阶段,仅使用传统的<i>Bagging</i>算法,因此,将概率知识应用到其他算法以及寻找一个更适合<i>SVDD</i>方法的集成算法,将是未来重点研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="195">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic machine learning and artificial intelligence">

                                <b>[1]</b> GHAHRAMANI Z.Probabilistic machine learning and artificial intelligence[J].Nature,2015,521(7553):452-459.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups">

                                <b>[2]</b> HINTON G,DENG L,YU D,et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine,2012,29(6):82-97.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD71033EF8EE149F894A1F173A0F662B0&amp;v=MjYyODhZZk9HUWxmQnJMVTA1dHBoeGJxNndhMD1OaWZJWThlL0g5SFByUG96Yko1NkRYZ3d1UjRhN2s1OFBuN2xyMk0xRDdTU1I4aWZDT052RlNpV1dyN0pJRnBtYUJ1SA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E,et al.ImageNet classification with deep convolutional neural networks[J].Communications of the ACM,2017,60(6):84-90.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201406001&amp;v=MTM5Nzg3Tkx6N0Jkckc0SDlYTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 黄凯奇,任伟强,谭铁牛.图像物体分类与检测算法综述[J].计算机学报,2014,36(6):1-18.(HUANG K Q,REN W Q,TAN T N.A review on image object classification and detection[J].Chinese Journal of Computers,2014,36(6):1-18.)
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Neural Probabilistic Language Model">

                                <b>[5]</b> KANDOLA E J,HOFMANN T,POGGIO T,et al.A neural probabilistic language model[J].Journal of Machine Learning Research,2006,194:137-186.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340727&amp;v=MDcyNzNNSDdSN3FkWitadUZpcmxVcjNOSVZzPU5qN0Jhck80SHRITnJJdEZZK2tJWTNrNXpCZGg0ajk5U1hxUnJ4b3hj&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> TAX D M J,DUIN R P W.Support vector data description[J].Machine Learning,2004,54(1):45-66.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740085&amp;v=MDQwNDRRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSVZ3UmF4Yz1OaWZPZmJLN0h0RE5xWTlGWSs4UERIUThvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> LEE K Y,KIM D W,LEE D,et al.Improving support vector data description using local density degree[J].Pattern Recognition,2005,38(10):1768-1771.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast support vector data description using k-means clustering">

                                <b>[8]</b> KIM P J,CHANG H J,SONG D S,et al.Fast support vector data description using k-means clustering[C]// Proceedings of the 4th International Symposium on Neural Networks.Berlin:Springer,2007:506-514.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fast SVBD algorithm based ondecomposition and combination for fault detection">

                                <b>[9]</b> LUO J,LI B,WU C,et al.A fast SVDD algorithm based on decomposition and combination for fault detection[C]// Proceedings of the 2010 International Conference on Control and Automation.Piscataway:IEEE,2010:1924-1928.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738297&amp;v=MDU4NTZIdEROcVk5RlkrZ0hEblUrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSVZ3UmF4Yz1OaWZPZmJLNw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> HUANG G,CHEN H,ZHOU Z,et al.Two-class support vector data description[J].Pattern Recognition,2011,44(2):320-329.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201405003&amp;v=MTE5OTFzRnlublZyN05MejdTWkxHNEg5WE1xbzlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 李勇,刘战东,张海军.不平衡数据的集成分类算法综述[J].计算机应用研究,2014,31(5):1287-1291.(LI Y,LIU Z D,ZHANG H J.Summary of integrated classification algorithm for unbalanced data[J].Application Research of Computers,2014,31(5):1287-1291.)
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601258112&amp;v=MTE0MDZadEZpbmxVcjNJSVZ3UmF4Yz1OaWZPZmJLN0h0RE5xWTlFWnU0SERYMDdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> FREUND Y,SCHAPIRE R E.A decision-theoretic generalization of on-line learning and an application to boosting[J].Journal of Computer and System Sciences,1997,55(1):119-139.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339495&amp;v=MTUxMDlyTzRIdEhOckl4TVlPSUtZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpcmxVcjNOSVZzPU5qN0Jh&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> BREIMAN L.Bagging predictors[J].Machine Learning,1996,24(2):123-140.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340271&amp;v=MTIxNTBybFVyM05JVnM9Tmo3QmFyTzRIdEhOckl0Rlp1d09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZp&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> BREIMAN L.Random forests[J].Machine Learning,2001,45(1):5-32.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MDM3Mjkrc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bm5VNzdLSlY4UlhGcXpHYkM0SE5YT3JJMU5Z&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 周志华.机器学习[M].北京:清华大学出版社,2016:171-189.(ZHOU Z H.Mechine Learning[M].Beijing:Tsinghua University Press,2016:171-189.)
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Space structure and clustering of categorical data">

                                <b>[16]</b> QIAN Y,LI F,LIANG J,et al.Space structure and clustering of categorical data[J].IEEE Transactions on Neural Networks &amp; Learning Systems,2016,27(10):2047-2059.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble Methods:Foundations and Algorithms">

                                <b>[17]</b> ZHOU Z.Ensemble Methods:Foundations and Algorithms[M].Boca Raton,FL:Taylor &amp; Francis Group,2012:1-22.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fusing monotonic decision trees">

                                <b>[18]</b> QIAN Y,LI F,LIANG J,et al.Fusing monotonic decision trees[J].IEEE Transactions on Knowledge and Data Engineering,2015,27(10):2717-2728.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bootstrap methods; another look at the jackknife">

                                <b>[19]</b> EFRON B.Bootstrap methods:another look at the jackknife[J].Breakthroughs in Statistics,1979,7(1):569-593.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gunnar Raetsch&amp;#39;&amp;#39;s benchmark datasets">

                                <b>[20]</b> CAWLEY G,TALBOT N.Gunnar Raetsch's benchmark datasets [DB/OL].[2018- 11- 20].http://theoval.cmp.uea.ac.uk/～gcc/matlab/default.html#benchmarks.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel framework for class imbalance learning using intelligent under-sampling">

                                <b>[21]</b> NAGANJANEYULU S,KUPPA M R.A novel framework for class imbalance learning using intelligent under-sampling[J].Progress in Artificial Intelligence,2013,2(1):73-84.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A dissimilarity-based imbalance data classification algorithm">

                                <b>[22]</b> ZHANG X,SONG Q,WANG G,et al.A dissimilarity-based imbalance data classification algorithm[J].Applied Intelligence,2015,42(3):544-565.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Novel Algorithm for Imbalance Data Classification Based on Genetic Algorithm Improved SMOTE">

                                <b>[23]</b> JIANG K,LU J,XIA K.A novel algorithm for imbalance data classification based on genetic algorithm improved SMOTE[J].Arabian Journal for Science and Engineering,2016,41(8):3255-3266.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911006" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911006&amp;v=MjA2ODdmWnVac0Z5bm5WcjdOTHo3QmQ3RzRIOWpOcm85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
