<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136662090627500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907013%26RESULT%3d1%26SIGN%3dAYzrAgchHORMt4ffqVSwCpJXLA4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907013&amp;v=MDEwOTQ5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTHZLTHo3QmQ3RzRIOWpNcUk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="1.1 &lt;b&gt;特征选择&lt;/b&gt;">1.1 <b>特征选择</b></a></li>
                                                <li><a href="#76" data-title="1.2 &lt;b&gt;半监督深度学习&lt;/b&gt;">1.2 <b>半监督深度学习</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="2 基于特征选择和深度信念网络的分类算法 ">2 基于特征选择和深度信念网络的分类算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="2.1 &lt;b&gt;特征提取&lt;/b&gt;">2.1 <b>特征提取</b></a></li>
                                                <li><a href="#96" data-title="2.2 &lt;b&gt;数据划分&lt;/b&gt;">2.2 <b>数据划分</b></a></li>
                                                <li><a href="#101" data-title="2.3 &lt;b&gt;特征选择和特征约减&lt;/b&gt;">2.3 <b>特征选择和特征约减</b></a></li>
                                                <li><a href="#104" data-title="2.4 &lt;b&gt;模型训练和测试&lt;/b&gt;">2.4 <b>模型训练和测试</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="3 实验以及讨论 ">3 实验以及讨论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="3.1 &lt;b&gt;实验设置&lt;/b&gt;">3.1 <b>实验设置</b></a></li>
                                                <li><a href="#116" data-title="3.2 &lt;b&gt;参数设置&lt;/b&gt;">3.2 <b>参数设置</b></a></li>
                                                <li><a href="#118" data-title="3.3 &lt;b&gt;性能评测指标&lt;/b&gt;">3.3 <b>性能评测指标</b></a></li>
                                                <li><a href="#128" data-title="3.4 &lt;b&gt;实验结果&lt;/b&gt;">3.4 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#146" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="图1 RBM网络模型">图1 RBM网络模型</a></li>
                                                <li><a href="#87" data-title="图2 部分的DBN">图2 部分的DBN</a></li>
                                                <li><a href="#88" data-title="图3 完整的DBN">图3 完整的DBN</a></li>
                                                <li><a href="#92" data-title="图4 FSDBN程序流程">图4 FSDBN程序流程</a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;混淆矩阵&lt;/b&gt;"><b>表</b>1 <b>混淆矩阵</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同特征保留率的分类准确性对比&lt;/b&gt;"><b>表</b>2 <b>不同特征保留率的分类准确性对比</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表&lt;/b&gt;3 FSDBN&lt;b&gt;模型的特征保留率&lt;/b&gt;"><b>表</b>3 FSDBN<b>模型的特征保留率</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;半监督文本情感分类算法的&lt;/b&gt;F&lt;b&gt;值&lt;/b&gt;"><b>表</b>4 <b>半监督文本情感分类算法的</b>F<b>值</b></a></li>
                                                <li><a href="#143" data-title="图5 不同算法的训练时间对比">图5 不同算法的训练时间对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="170">


                                    <a id="bibliography_1" title=" HINTON G, OSINDERO S, TEH Y.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MjIxNzNiSzlIdGpNcW85RlpPb05DWDh4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VGJoYz1OaWZKWg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         HINTON G, OSINDERO S, TEH Y.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_2" title=" ZHOU S, CHEN Q, WANG X, et al.Hybrid deep belief networks for semi-supervised sentiment classification[C]// Proceeding of the 2014 25th International Conference on Computational Linguistic.Stroudsburg, PA:Association for Computational Linguistics, 2014:1341-1349." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hybrid deep belief networks for semi-supervised sentiment classification">
                                        <b>[2]</b>
                                         ZHOU S, CHEN Q, WANG X, et al.Hybrid deep belief networks for semi-supervised sentiment classification[C]// Proceeding of the 2014 25th International Conference on Computational Linguistic.Stroudsburg, PA:Association for Computational Linguistics, 2014:1341-1349.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_3" title=" ZHOU S, CHEN Q, WANG X.Active deep networks for semisupervised sentiment classification.[C]// Proceedings of the 2010 23rd International Conference on Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2010:1515-1523." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active deep networks for semi-supervised sentiment classification">
                                        <b>[3]</b>
                                         ZHOU S, CHEN Q, WANG X.Active deep networks for semisupervised sentiment classification.[C]// Proceedings of the 2010 23rd International Conference on Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2010:1515-1523.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_4" title=" SOCHER R, PERELYGIN A, WU J J.et al.Recursive deep models for semantic compositionality over a sentiment treebank [C]// Proceedings of the 2013 International Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2013:1631-1642." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recursive deep models for semantic compositionality over a sentiment treebank">
                                        <b>[4]</b>
                                         SOCHER R, PERELYGIN A, WU J J.et al.Recursive deep models for semantic compositionality over a sentiment treebank [C]// Proceedings of the 2013 International Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2013:1631-1642.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_5" title=" DASGUPTA, S, NG V.Mine the easy, classify the hard:a semisupervised approach to automatic sentiment classification [C]// Proceedings of the 2009 47th International Conference on Annual Meeting of the Association for Computational Linguistics and Proceedings of the 2009/4th International Joint Conference on Natural Language of the Asian Federation of Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2009:701-709." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mine the easy classify the hard:a semisupervised approach to automatic sentiment classification">
                                        <b>[5]</b>
                                         DASGUPTA, S, NG V.Mine the easy, classify the hard:a semisupervised approach to automatic sentiment classification [C]// Proceedings of the 2009 47th International Conference on Annual Meeting of the Association for Computational Linguistics and Proceedings of the 2009/4th International Joint Conference on Natural Language of the Asian Federation of Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2009:701-709.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_6" title=" PANG B, LEE L, VAITHYANATHAN S.Thumbs up?:sentiment classification using machine learning techniques [C]// Proceedings of the 2002 International Conference on Association for Computational Linguistics on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2002:79-86." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Thumbs up? sentiment classification using machine learning techniques">
                                        <b>[6]</b>
                                         PANG B, LEE L, VAITHYANATHAN S.Thumbs up?:sentiment classification using machine learning techniques [C]// Proceedings of the 2002 International Conference on Association for Computational Linguistics on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2002:79-86.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_7" title=" FORMAN G.An extensive empirical study of feature selection metrics for text classification[J].The Journal of Machine Learning Research, 2003:1289-1305." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An extensive empirical study of feature selection metrics for text classification">
                                        <b>[7]</b>
                                         FORMAN G.An extensive empirical study of feature selection metrics for text classification[J].The Journal of Machine Learning Research, 2003:1289-1305.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_8" title=" YANG Y, PEDERSEN J O.A comparative study on feature selection in text categorization[C]// Proceedings of the 1997 14th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann, 1997:412-420." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Comparative Study on Feature Selection in Text Categorization">
                                        <b>[8]</b>
                                         YANG Y, PEDERSEN J O.A comparative study on feature selection in text categorization[C]// Proceedings of the 1997 14th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann, 1997:412-420.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_9" title=" 周茜, 赵明生, 扈旻.中文文本分类中的特征选择研究[J].中文信息学报, 2004, 18 (3) :17-23. (ZHOU Q, ZHAO M S, HU M.Research on feature selection in Chinese text classification [J].Journal of Chinese Information Processing, 2004, 18 (3) :17-23.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS200403002&amp;v=MTk3NjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblZMdk5LQ2pZZmJHNEh0WE1ySTlGWm9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         周茜, 赵明生, 扈旻.中文文本分类中的特征选择研究[J].中文信息学报, 2004, 18 (3) :17-23. (ZHOU Q, ZHAO M S, HU M.Research on feature selection in Chinese text classification [J].Journal of Chinese Information Processing, 2004, 18 (3) :17-23.) 
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_10" title=" 吴金源, 冀俊忠, 赵学武, 等.基于特征选择技术的情感词权重计算[J].北京工业大学学报, 2016, 42 (1) :142-151. (WU J Y, JI J Z, ZHAO X W, et al.Weight calculation of affective words based on feature selection technique[J].Journal of Beijing University of Technology, 2016, 42 (1) :142-151.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJGD201601022&amp;v=MjI5MzdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTHZOSnlmTWFyRzRIOWZNcm85SFo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         吴金源, 冀俊忠, 赵学武, 等.基于特征选择技术的情感词权重计算[J].北京工业大学学报, 2016, 42 (1) :142-151. (WU J Y, JI J Z, ZHAO X W, et al.Weight calculation of affective words based on feature selection technique[J].Journal of Beijing University of Technology, 2016, 42 (1) :142-151.) 
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_11" title=" 周爱武, 马那那, 刘慧婷.基于卡方统计的情感文本分类[J].微电子学与计算机, 2017, 34 (8) :57-61. (ZHOU A W, MA N N, LIU H T.Emotional text classification based on chi-square statistics [J].Microelectronics and Computer, 2017, 34 (8) :57-61.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201708012&amp;v=MzEwMTQvblZMdk5NalhTWkxHNEg5Yk1wNDlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         周爱武, 马那那, 刘慧婷.基于卡方统计的情感文本分类[J].微电子学与计算机, 2017, 34 (8) :57-61. (ZHOU A W, MA N N, LIU H T.Emotional text classification based on chi-square statistics [J].Microelectronics and Computer, 2017, 34 (8) :57-61.) 
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_12" title=" 裴英博, 刘晓霞.文本分类中改进型CHI特征选择方法的研究[J].计算机工程与应用, 2011, 47 (4) :128-130. (PEI Y B, LIU X X.Research on improved CHI feature selection method in text classification [J].Computer Engineering and Application, 2011, 47 (4) :128-130.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201104037&amp;v=MTk2MTdMdk5MejdNYWJHNEg5RE1xNDlHWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         裴英博, 刘晓霞.文本分类中改进型CHI特征选择方法的研究[J].计算机工程与应用, 2011, 47 (4) :128-130. (PEI Y B, LIU X X.Research on improved CHI feature selection method in text classification [J].Computer Engineering and Application, 2011, 47 (4) :128-130.) 
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_13" title=" BAGHERI A, SARAEE M, de JONG F.Sentiment classification in Persian:introducing a mutual information-based method for feature selection[C]// Proceedings of the 2013 21th International Conference on Electrical Engineering.Piscataway, NJ:IEEE, 2013:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sentiment classification in Persian introducing a mutual information-based method for feature selection">
                                        <b>[13]</b>
                                         BAGHERI A, SARAEE M, de JONG F.Sentiment classification in Persian:introducing a mutual information-based method for feature selection[C]// Proceedings of the 2013 21th International Conference on Electrical Engineering.Piscataway, NJ:IEEE, 2013:1-6.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_14" title=" BLIZER J, DREDZE M, PEREIRA F.Biographies, bollywood, boomboxes and blenders:domain adaptation for sentiment classification[C]// Proceedings of the 2007 International Conference on Association for Computational Linguistic.Stroudsburg, PA:Association for Computational Linguistics, 2007:440-447." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Biographies,bollywood,boom-boxes and blenders:domain adaptation for sentiment classifica-tion">
                                        <b>[14]</b>
                                         BLIZER J, DREDZE M, PEREIRA F.Biographies, bollywood, boomboxes and blenders:domain adaptation for sentiment classification[C]// Proceedings of the 2007 International Conference on Association for Computational Linguistic.Stroudsburg, PA:Association for Computational Linguistics, 2007:440-447.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_15" title=" LOPES N, RIBEIRO B, GON&#199;ALVES J.Restricted Boltzmann machines and deep belief networks on multi-core processors [C]// Proceedings of the 2012 International Joint Conference on Neural Networks Piscataway, NJ:IEEE, 2012:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RestrictedBoltzmann machines and deep belief networks on multi-coreprocessors">
                                        <b>[15]</b>
                                         LOPES N, RIBEIRO B, GON&#199;ALVES J.Restricted Boltzmann machines and deep belief networks on multi-core processors [C]// Proceedings of the 2012 International Joint Conference on Neural Networks Piscataway, NJ:IEEE, 2012:1-7.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_16" title=" 张庆庆, 刘西林.基于深度信念网络的文本情感分类研究[J].西北工业大学学报 (社会科学版) , 2016, 36 (1) :62-66. (ZHANG Q Q, LIU X L.Research on text emotion classification based on deep belief network[J].Journal of Northwest Polytechnic University (Social Science Edition) , 2016, 36 (1) :62-66.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDSH201601013&amp;v=MzEzNjBGckNVUjdxZlp1WnNGeS9uVkx2Tklpbllackc0SDlmTXJvOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         张庆庆, 刘西林.基于深度信念网络的文本情感分类研究[J].西北工业大学学报 (社会科学版) , 2016, 36 (1) :62-66. (ZHANG Q Q, LIU X L.Research on text emotion classification based on deep belief network[J].Journal of Northwest Polytechnic University (Social Science Edition) , 2016, 36 (1) :62-66.) 
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_17" title=" 伊尔夏提&#183;吐尔贡, 吾守尔&#183;斯拉木, 热西旦木&#183;吐尔洪太, 等.维吾尔文情感语料库的构建与分析[J].计算机与现代化, 2017 (4) :67-72. (TUERGONG Y, SILAMU W, TUSERHONGTAI R, et al.Construction and analysis of Uighur affective corpus [J].Computer and Modernization, 2017 (4) :67-72.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201704014&amp;v=MzE3MTMzenFxQnRHRnJDVVI3cWZadVpzRnkvblZMdk5MelRUWnJHNEg5Yk1xNDlFWUlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         伊尔夏提&#183;吐尔贡, 吾守尔&#183;斯拉木, 热西旦木&#183;吐尔洪太, 等.维吾尔文情感语料库的构建与分析[J].计算机与现代化, 2017 (4) :67-72. (TUERGONG Y, SILAMU W, TUSERHONGTAI R, et al.Construction and analysis of Uighur affective corpus [J].Computer and Modernization, 2017 (4) :67-72.) 
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_18" title=" KAMVAR S D, DAN K, MANNING C D.Spectral learning[C]// Proceedings of the 2003 International Joint Conference on Artificial Intelligence.San Francisco, CA:Morgan Kaufmann, 2003:561-566." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral learning">
                                        <b>[18]</b>
                                         KAMVAR S D, DAN K, MANNING C D.Spectral learning[C]// Proceedings of the 2003 International Joint Conference on Artificial Intelligence.San Francisco, CA:Morgan Kaufmann, 2003:561-566.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_19" title=" COLLOBERT R, SINZ F, WESTON J, et al.Large scale transductive SVMs[J].The Journal of Machine Learning Research, 2006, 7:1687-1712." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large scale transductive SVMs">
                                        <b>[19]</b>
                                         COLLOBERT R, SINZ F, WESTON J, et al.Large scale transductive SVMs[J].The Journal of Machine Learning Research, 2006, 7:1687-1712.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_20" title=" LI S, HUANG C R, ZHOU G, et al.Employing personal/impersonal views in supervised and semi-supervised sentiment classification[C]// Proceedings of the 2010 48th International Joint Conference on Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2010:414-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Employing personal/impersonal views in supervised and semi-supervised sentiment classification">
                                        <b>[20]</b>
                                         LI S, HUANG C R, ZHOU G, et al.Employing personal/impersonal views in supervised and semi-supervised sentiment classification[C]// Proceedings of the 2010 48th International Joint Conference on Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2010:414-423.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_21" title=" RUANGKANOKMAS P, ACHALAKUL T, AKKARAJITSAKUL K.Deep belief networks with feature selection for sentiment classification[C]// Proceedings of the 2017 48th International Conference on Intelligent Systems, Modelling and Simulation.Piscataway, NJ:IEEE, 2017:9-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep belief networks with feature selection for sentiment classification">
                                        <b>[21]</b>
                                         RUANGKANOKMAS P, ACHALAKUL T, AKKARAJITSAKUL K.Deep belief networks with feature selection for sentiment classification[C]// Proceedings of the 2017 48th International Conference on Intelligent Systems, Modelling and Simulation.Piscataway, NJ:IEEE, 2017:9-14.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-02-27 12:39</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),1942-1947 DOI:10.11772/j.issn.1001-9081.2018112363            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于特征选择和深度信念网络的文本情感分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%91%E8%BF%9B%E5%8B%87&amp;code=39760363&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">向进勇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%96%87%E5%BF%A0&amp;code=09256021&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨文忠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%BE%E5%AE%88%E5%B0%94%C2%B7%E6%96%AF%E6%8B%89%E6%9C%A8&amp;code=17705001&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吾守尔·斯拉木</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%96%B0%E7%96%86%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0181515&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">新疆大学信息科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%96%B0%E7%96%86%E5%A4%9A%E8%AF%AD%E7%A7%8D%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E6%96%B0%E7%96%86%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">新疆多语种信息技术重点实验室(新疆大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于人类语言的复杂性, 文本情感分类算法大多都存在因为冗余而造成的词汇量过大的问题。深度信念网络 (DBN) 通过学习输入语料中的有用信息以及它的几个隐藏层来解决这个问题。然而对于大型应用程序来说, DBN是一个耗时且计算代价昂贵的算法。针对这个问题, 提出了一种半监督的情感分类算法, 即基于特征选择和深度信念网络的文本情感分类算法 (FSDBN) 。首先使用特征选择方法 (文档频率 (DF) 、信息增益 (IG) 、卡方统计 (CHI) 、互信息 (MI) ) 过滤掉一些不相关的特征从而使词汇表的复杂性降低;然后将特征选择的结果输入到DBN中, 使得DBN的学习阶段更加高效。将所提算法应用到中文以及维吾尔语中, 实验结果表明在酒店评论数据集上, FSDBN在准确率方面比DBN提高了1.6%, 在训练时间上比DBN缩短一半。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度信念网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征选择;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8A%E7%9B%91%E7%9D%A3%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">半监督的情感分类算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%97%E9%99%90%E6%B3%A2%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">受限波尔兹曼机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本情感分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    向进勇 (1992—) , 男, 新疆伊犁人, 硕士研究生, 主要研究方向:文本情感分类;;
                                </span>
                                <span>
                                    *杨文忠 (1973—) , 男, 河南洛阳人, 副教授, 博士, CCF会员, 主要研究方向:网络安全、文本情感分析;电子邮箱ywz_xy@163.com;
                                </span>
                                <span>
                                    吾守尔·斯拉木 (1942—) , 男, 新疆伊犁人, 中国工程院院士, 教授, CCF会员, 主要研究方向:自然语言处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (U1603115, XJEDU2017T002, U1435215);</span>
                    </p>
            </div>
                    <h1><b>Text sentiment classification algorithm based on feature selection and deep belief network</b></h1>
                    <h2>
                    <span>XIANG Jinyong</span>
                    <span>YANG Wenzhong</span>
                    <span>SILAMU·Wushouer</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Engineering, Xinjiang University</span>
                    <span>Xinjiang Laboratory of Multi-Language Information Technology, Xinjiang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Because of the complexity of human language, text sentiment classification algorithms mostly have the problem of excessively huge vocabulary due to redundancy. Deep Belief Network (DBN) can solve this problem by learning useful information in the input corpus and its hidden layers. However, DBN is a time-consuming and computationally expensive algorithm for large applications. Aiming at this problem, a semi-supervised sentiment classification algorithm called text sentiment classification algorithm based on Feature Selection and Deep Belief Network (FSDBN) was proposed. Firstly, the feature selection methods including Document Frequency (DF) , Information Gain (IG) , CHI-square statistics (CHI) and Mutual Information (MI) were used to filter out some irrelevant features to reduce the complexity of vocabulary. Then, the results of feature selection were input into DBN to make the learning phase of DBN more efficient. The proposed algorithm was applied to Chinese and Uygur language. The experimental results on hotel review dataset show that the accuracy of FSDBN is 1.6% higher than that of DBN and the training time of FSDBN halves that of DBN.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Belief%20Network%20(DBN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Belief Network (DBN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Learning%20(DL)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Learning (DL) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20Selection%20(FS)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature Selection (FS) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semi-supervised%20sentiment%20classification%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semi-supervised sentiment classification algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Restricted%20Boltzmann%20Machine%20(RBM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Restricted Boltzmann Machine (RBM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text%20sentiment%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text sentiment classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XIANG Jinyong, born in 1992, M. S. candidate. His research interest includes text sentiment classification. ;
                                </span>
                                <span>
                                    YANG Wenzhong, born in 1973, Ph. D. , associate professor. His research interests include network security, text sentiment classification. ;
                                </span>
                                <span>
                                    SILAMU·Wushouer, born in 1942, academician of the Chinese Academy of Engineering, professor. His research interest includes natural language processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (U1603115, XJEDU2017T002, U1435215);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="46">如今, 互联网上社交媒体的数据量大幅度增长。个人和组织试图从这些大型数据集中提取有用的信息, 以便作出更好的判断并提高客户满意度。例如, 在决定是否购买产品或服务之前, 客户会查看其他人对产品的评论。同样, 产品的制造商也使用这些信息来提高产品的服务和质量。然而, 由于网络上有大量的可用数据, 人们手工的去标注这些数据是不现实的, 因此, 文本情感分类 (确定文档中表达的情绪是积极的、中立的还是消极的) 将对商业智能应用程序、推荐系统和消息过滤应用程序有帮助和益处。</p>
                </div>
                <div class="p1">
                    <p id="47">为了构建一个准确的情感性分类器, 在过去的几年里, 许多研究者尝试将深度学习算法与机器学习算法相结合<citation id="213" type="reference"><link href="170" rel="bibliography" /><link href="172" rel="bibliography" /><link href="174" rel="bibliography" /><link href="176" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。深度学习算法具有处理数百万个参数的能力, 可以极大地提高模型预测的能力。其中一个典型的例子是Socher等<citation id="212" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出的基于情感树的递归神经网络训练, 它能准确预测文本情感, 准确率达85%以上;然而在有监督训练方法中需要大量的标记训练数据, 手工标记这些数据常常是困难和费时的。</p>
                </div>
                <div class="p1">
                    <p id="48">文献<citation id="214" type="reference">[<a class="sup">5</a>]</citation>中提出了半监督学习的新方法, 它的目的是利用大量的未标记数据和标记数据构建文本情感分类器。文献<citation id="215" type="reference">[<a class="sup">6</a>]</citation>称半监督深度学习模型可以在获得良好的性能的同时避免上述问题。然而目前的深度学习算法对于大型应用程序来说计算成本很高。</p>
                </div>
                <div class="p1">
                    <p id="49">此外, 大多数分类算法使用固定大小的数字特征向量作为输入, 而不是使用原始的变长文本文档, 因此, 有必要将一个文档语料库转换为一个矩阵, 每个文档表示一行, 每个词语表示一列, 每一列表示词语在语料库中发生的情况。由于人类语言的复杂性, 特征项的维度可能超过1万维度, 而其中大多数是嘈杂的或冗余的。这将导致分类错误的数量和计算时间的增加。</p>
                </div>
                <div class="p1">
                    <p id="50">为了克服上述缺陷, 使学习阶段更高效、更准确, 必须进行有效的特征选择, 其目的是过滤训练集中出现的非必要特征项, 只选择有意义的特征项。Forman<citation id="216" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了多种特征选择的方法。结果表明, 采用特征选择方法可以通过消除噪声特征来减少输入数据的维数, 从而提高分类算法的性能, 因此, 本文提出的方法可以更快地训练出分类模型, 减少内存消耗, 并得到更高的结果精度。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="52">在本章中, 介绍相关特征选择算法和半监督深度学习的理论背景。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53">1.1 <b>特征选择</b></h4>
                <div class="p1">
                    <p id="54">特征选择是通过选择相关特征的子集来简化模型构建的过程。它有两个主要角色:第一个作用是通过减少词汇输入的大小来提高分类器的训练过程的效率;第二个作用是通过过滤不重要的术语或噪声特征来提高预测精度, 因此, 可以缩短训练时间, 也可以得到更好的模型表示。特征选择是一种比较常用的特征维数约减的方法, 选择出更具有代表特征。特征选择的好坏对情感文本分类的效果有很大的影响。</p>
                </div>
                <div class="p1">
                    <p id="55">基本上, 特征选择技术可以分为三类:过滤技术、包装技术和嵌入技术。在学习算法之前, 使用基于过滤的技术作为预处理步骤。根据一些标准对特征进行排序, 如果它们的分数超过适当的预先定义的阈值, 就会进行选择。包装器技术使用一种学习算法来选择和评估所有特性中的一个子集。嵌入式技术作为训练过程的一部分进行特征选择。</p>
                </div>
                <div class="p1">
                    <p id="56">在这三种类型中, 基于过滤的方法是最适合的, 因为它简单、快速, 并且独立于分类器。它具有良好的可扩展性, 可以有效地应用于大型应用。文献<citation id="219" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>对特征选择的方法进行了研究, 基于过滤器的技术有文档频率 (Document Frequency, DF) <citation id="220" type="reference"><link href="186" rel="bibliography" /><link href="188" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>、信息增益 (Information Gain, IG) <citation id="217" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、卡方统计 (CHI-square statistics, CHI) <citation id="221" type="reference"><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>、互信息 (Mutual Information, MI) <citation id="218" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>等, 在所有特征选择算法中, 本文使用文档频率、信息增益、卡方统计、互信息作为基于特征选择和深度信念网络的文本情感分类算法 (text sentiment classification algorithm based on Feature Selection and Deep Belief Network, FSDBN) 框架中的特征选择方法, 用实验证明哪种特征选择方法与深度信念网络 (Deep Belief Network, DBN) 结合可以取得最好的文本情感分类效果。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57">1.1.1 文档频率</h4>
                <div class="p1">
                    <p id="58">文档频率方法的基本思想是:统计每个词的文档频率值, 根据预先设定的最小文档频率值以及最大的文档频率值来除去一些没有代表的特征, 如果特征<i>t</i>的文档频率值在最小和最大阈值之间, 则保留此特征<i>t</i>;否则就删去此特征。这种方法理解与实现起来比较简单, 适用于大规模数据集, 阈值的设定可能会影响到特征选择的效果, 从而影响到文本分类的效果。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.1.2 信息增益</h4>
                <div class="p1">
                    <p id="60">信息增益 (IG) 能够度量某个特征包含类别信息的多少, 一个特征词的信息增益等于该特征出现前后的信息熵之差, 通常会对某一文档、类别或是整个数据集来计算, 根据IG方法的定义, 某个特征的信息增益越大说明该特征对于分类就越重要。</p>
                </div>
                <div class="p1">
                    <p id="61">针对某个具体的类别<i>c</i><sub><i>i</i></sub>, 对于特征<i>t</i>的信息增益如式 (1) :</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ι</mi><mi>G</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>t</mi><mo stretchy="false">) </mo><mo>⋅</mo></mtd></mtr><mtr><mtd><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>t</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>t</mi><mo>¯</mo></mover><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mover accent="true"><mi>t</mi><mo>¯</mo></mover><mo stretchy="false">) </mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mover accent="true"><mi>t</mi><mo>¯</mo></mover><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>t</mi><mo>¯</mo></mover></math></mathml>表示特征<i>t</i>不出现, 那么特征<i>t</i>对于整个训练集的信息增益如式 (2) :</p>
                </div>
                <div class="p1">
                    <p id="65"><i>IG</i> (<i>t</i>) =∑<i>IG</i> (<i>c</i><sub><i>i</i></sub>, <i>t</i>)      (2) </p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.1.3 卡方统计</h4>
                <div class="p1">
                    <p id="67">卡方统计 (CHI) 能表示两个变量的相关性, CHI兼顾了特征存在与特征不存在时的情况。根据CHI的定义以及公式可知特征与类别的CHI值越大, 就表示这个特征就越重要。</p>
                </div>
                <div class="p1">
                    <p id="68">根据CHI的公式:对于某个具体的类别<i>c</i><sub><i>i</i></sub>特征<i>t</i>的CHI统计值如式 (3) :</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>Η</mi><mi>Ι</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mover accent="true"><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mo>, </mo><mover accent="true"><mi>t</mi><mo>¯</mo></mover><mo stretchy="false">) </mo><mo>-</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mover accent="true"><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mover accent="true"><mi>t</mi><mo>¯</mo></mover><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mover accent="true"><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>t</mi><mo>¯</mo></mover><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">文献<citation id="222" type="reference">[<a class="sup">10</a>]</citation>分别根据卡方统计 (CHI) 以及互信息 (MI) 的特征选择方法给语料中的中文特征词赋予了一定的权重。</p>
                </div>
                <div class="p1">
                    <p id="71">有的研究人员也使用了改进的卡方特征选择技术<citation id="223" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>来避免卡方统计量的缺点“低频词缺陷问题” (它只统计文档是否出现词而不管出现几次) 。这会使得它对低频词有所偏袒 (因为它夸大了低频词的作用) , 甚至它会出现一些情况, 一个词在一类文章的每篇文档中只出现一次, 其卡方值却大过了在该类文章99%的文档中出现了10次的词, 其实后面的词才是更具代表性的, 但只因为它出现的文档数比前面的文档中词少了“1”, 特征选择的时候就可能筛掉后面的词而保留前者) 。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">1.1.4 互信息</h4>
                <div class="p1">
                    <p id="73">互信息 (MI) 是信息论中的一个概念, 表示的是一个随机变量中包含另一个随机变量的信息量。在文本情感分析任务中, 特征项跟情感类别之间的互信息量可以表示特征项推测出情感类别的能力, 若特征项与情感类别的互信息为0, 这就可以表示特征项不包含任何的特征信息, 对文本情感分类没有任何的贡献。如果互信息越大, 表示特征项包含的情感信息越大, 类别间的区分程度也就越大。特征项<i>t</i>与情感类别<i>C</i>之间的互信息量计算公式 (4) 如下:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>Ι</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>*</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">不管使用什么样的特征选择技术, 最终的目的就是减少特征的空间, 消除噪声从而提高分类器的性能。本文使用这四种特征选择方法与深度信念网络结合实现半监督的文本情感分类。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">1.2 <b>半监督深度学习</b></h4>
                <div class="p1">
                    <p id="77">半监督深度学习是机器学习的一个分支, 它利用少量的标记数据和大量的未标记数据。半监督深度学习算法的一个著名例子是深度信念网络 (DBN) , 它是由Hinton等最近提出的。DBN是由限制玻尔兹曼机 (Restricted Boltzmann Machine, RBM) 构造的许多隐藏层组成的。该模型利用大量未标注数据, 克服领域依赖和缺乏标注数据缺陷, 同时获得良好的性能<citation id="224" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">1.2.1 受限玻尔兹曼机</h4>
                <div class="p1">
                    <p id="79">受限玻尔兹曼机 (RBM) 是一种基于能量的生成随机模型, 其目标是了解其输入集的概率分布。它由一个输入层 (可见层) 和一个隐藏层组成, 通过对称加权连接, 但同一层神经元之间没有连接。图1显示了RBM的网络模型。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907013_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 RBM网络模型" src="Detail/GetImg?filename=images/JSJY201907013_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 RBM网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907013_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 RBM network model</p>

                </div>
                <div class="p1">
                    <p id="81">为了训练一个网络模型, 最广泛使用的算法被称为对比散度 (Contrastive Divergence, CD) 。训练RBM的目的是优化网络中的权向量, 以最小化误差。为了在尽可能保持输入数据分布的同时降低网络的能量, 在训练数据的对数似然时应用了随机梯度上升, 关于方程的更多细节可以参阅文献<citation id="225" type="reference">[<a class="sup">15</a>]</citation>。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">1.2.2 深度信念网络</h4>
                <div class="p1">
                    <p id="83">为了获取更好的性能, 一组受限制的玻尔兹曼机器可以定义为深度信念网络 (DBN) 。为了构建DBN, 本文可以按照以下步骤<citation id="226" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="84">利用一组RBMs作为构建块, 贪婪地分层无监督学习构造DBN。该学习算法有效地利用了未标记数据, 在输入数据中提供了大量的模式, 从而比随机数据产生更好的初始权值。</p>
                </div>
                <div class="p1">
                    <p id="85">利用基于梯度下降算法的有监督学习方法, 根据指数损失函数训练DBN。模型的权值通过标记数据来细化, 目的是进行模式分类。</p>
                </div>
                <div class="p1">
                    <p id="86">图2显示了具有一个输入层的DBN的贪婪分层无监督训练过程, 即输入层<i>x</i>和三个隐藏层即从下到上分别是<i>h</i>1、<i>h</i>2和<i>h</i>3。较浅的颜色层代表已经过训练的层, 而较深的颜色层则是经过训练的层。经过贪婪的分层无监督学习, <i>h</i>3 (<i>x</i>) 是<i>x</i>的表示。然后, 在顶部添加一个输出层, 并使用标记评论来细化权重, 以提高识别能力。图3显示了完整的DBN。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 部分的DBN" src="Detail/GetImg?filename=images/JSJY201907013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 部分的DBN  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Part of DBN</p>

                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907013_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 完整的DBN" src="Detail/GetImg?filename=images/JSJY201907013_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 完整的DBN  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907013_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Complete DBN</p>

                </div>
                <div class="p1">
                    <p id="89">在文本情感分类研究中, DBN采用以词袋模型表示的矩阵作为输入。前几层期望从输入中提取低层次的特性, 而上一层则需要提炼以前学过的特性模式, 从而生成更复杂的特征。最后, 在输出层预测评论的情绪傾向, 无论是积极的还是消极的。</p>
                </div>
                <h3 id="90" name="90" class="anchor-tag">2 基于特征选择和深度信念网络的分类算法</h3>
                <div class="p1">
                    <p id="91">本章将介绍基于特征选择的深度信念网络 (FSDBN) 的主要设计流程。图4是显示执行文本情感分类任务的框架。大多数任务与其他文本分类方法相似, 分别包括特征提取、数据分隔、模型训练和测试。然而, 在本文题注的算法中有两个新的任务, 即特征选择和缩减。每个任务的细节如下所述。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907013_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 FSDBN程序流程" src="Detail/GetImg?filename=images/JSJY201907013_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 FSDBN程序流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907013_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Flow chart of FSDBN program</p>

                </div>
                <h4 class="anchor-tag" id="93" name="93">2.1 <b>特征提取</b></h4>
                <div class="p1">
                    <p id="94">在文本情感分类应用中, 需要将变长文档转换为适合于分类算法的固定大小的数字特征向量。对基于文本的输入进行特征提取的常用技术是词袋技术, 该技术通过单词出现来描述文档, 而忽略了单词的相对位置信息。有两个主要任务, 包括特征定义和加权分数计算。在本文的模型中, 本文算法将标记的词语应用为“特征” (中文使用结巴进行分词、维吾尔语提取unigram) 。</p>
                </div>
                <div class="p1">
                    <p id="95">想要提取特性, 首先进行分词 (中文使用结巴进行分词、维吾尔语提取unigram) , 然后把所有的特征生成一个字典, 在一个文档中, 如果该特征出现就赋值为1;否则为0。此外, 根据每个词汇表降序排列的次数排序的前2%词汇表将被删除, 因为它们中的大多数都是停止词 (例如“的”“是”) , 或者它们可能是特定领域或通用的词 (例如, 酒店评论中的“酒店”) 。从理论上讲, 停用词可以出现在正训练数据集或负训练数据集中, 而不带有任何情绪信息, 这可能会增加分类错误率, 因为它们的情绪含糊不清。在此过程之后, 一个文档语料库就形成了一个二进制值矩阵, 每个文档表示一行, 每个特性或标记在语料库中表示一列。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">2.2 <b>数据划分</b></h4>
                <div class="p1">
                    <p id="97">为了使得到的结果更加具有说服力, 在中文文本情感分类中, 本文选取酒店评论语料、京东上获取的书籍评论语料、淘宝上获取的电脑评论语料。</p>
                </div>
                <div class="p1">
                    <p id="98">为了验证模型FSDBN在维吾尔语文本情感分类中的有效性。相对于中文和英文来说维吾尔语的文本情感开放语料较少, 本实验所使用的维吾尔语文本情感语料是由伊尔夏提·吐尔贡等<citation id="227" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>在这三个维文网站 (Alkuy、TianShan、Putbal) 上收集的评论数据, 然后经过维吾尔族大学生进行手工筛选, 最终形成了pos类5 000多条句子以及neg类5 000多条句子。</p>
                </div>
                <div class="p1">
                    <p id="99">在这四个数据集中, 本文选取3 000个标签评论 (即在每个数据集中, 有1 500个正面和1 500个负面) 。</p>
                </div>
                <div class="p1">
                    <p id="100">由于所提出的分类器是一种半监督学习算法, 利用未标记和标记数据构造分类器, 因此, 本文方法将数据集划分为三个集, 包括未标记训练集、标记训练集和标记测试集。将每个3 000个评论的数据集随机划分为10个大小相等的文档, 同时为了交叉验证的目的, 仍然在每份文档中保持平衡的类分布。在每一轮中, 本文算法选择一份文档作为标记数据集, 然后随机选择这份文档中的一半评论作为标记训练数据集, 另一半作为标记测试数据集。其余9个折叠被用作未标记的数据集。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">2.3 <b>特征选择和特征约减</b></h4>
                <div class="p1">
                    <p id="102">为了提高分类精度, 本文的目标是通过特征选择和特征约减来消除可能导致分类误差的噪声特征。在本文的框架中, 本文使用文档频率、信息增益、卡方统计、互信息特征选择来确定哪些特征与情感分类最相关, 以获得最高的分类准确性。</p>
                </div>
                <div class="p1">
                    <p id="103">本文提出的分类算法经过特征选择算法来选择前<i>n</i>%的特征来构建文本情感分类模型, 而其他的则不用于分析。每个数据集的最佳<i>n</i>百分比值见3.4节实验。值得注意的是, 本文之所以使用带标签的训练数据集来进行特征选择, 仅仅是因为该算法基于监督学习方法, 并且避免了测试集出现过拟合问题。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">2.4 <b>模型训练和测试</b></h4>
                <div class="p1">
                    <p id="105">本文框架中使用的情感分类模型是基于深度信念网络的。首先, 学习算法使用无标记的训练评论执行贪婪的分层无监督学习;然后, 利用基于梯度下降法的监督学习方法, 通过带指数损失函数的训练来改进模型的权值;在完全构建预测模型后, 利用标记数据对模型进行测试;最后的分类结果使用十倍交叉验证在F值方面的平均。第3章将介绍DBN的学习参数和结构。</p>
                </div>
                <h3 id="106" name="106" class="anchor-tag">3 实验以及讨论</h3>
                <div class="p1">
                    <p id="107">在本章中, 为了验证本文提出的FSDBN算法的性能, 将其在准确率和训练时间方面与其他算法半监督学习算法<citation id="228" type="reference"><link href="170" rel="bibliography" /><link href="176" rel="bibliography" /><link href="204" rel="bibliography" /><link href="206" rel="bibliography" /><link href="208" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">4</a>,<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>进行比较。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">3.1 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="109">本文实验的目的是评估本文提出的框架与5种半监督学习情感分类器在准确性以及训练时间方面的性能表现, 这5种半监督的文本情感分类器分别是:深度信念网络 (DBN) 、混合深信念网络 (Hybrid Deep Belief Network, HDBN) <citation id="229" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、半监督谱学习 (semi-supervised spectral learning) <citation id="230" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、转换式支持向量机 (Transductive Support Vector Machine, TSVM) <citation id="231" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、个人/非个人观点 (Personal/Impersonal Views, PIV) <citation id="232" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。分类器具体概述如下。</p>
                </div>
                <div class="p1">
                    <p id="110">谱学习利用数据矩阵特征向量中的信息检测结构, 进行半监督聚类和分类。</p>
                </div>
                <div class="p1">
                    <p id="111">TSVM利用未标记数据提高了支持向量机 (Support Vector Machine, SVM) 的泛化精度。与SVM类似, 它使用带标记的训练数据学习了一个大范围的超平面分类器, 但同时迫使这个超平面远离未带标记的数据。谱学习和TSVM方法是半监督情绪分类的两种基本方法。</p>
                </div>
                <div class="p1">
                    <p id="112">PIV采用个人和非个人的观点构建半监督分类器。个人观点包括直接表达说话人对目标对象的感受和偏好的句子, 而非个人观点则集中于对目标对象的陈述进行评价。</p>
                </div>
                <div class="p1">
                    <p id="113">DBN是在第1章中提出的经典深度学习方法。</p>
                </div>
                <div class="p1">
                    <p id="114">HDBN是RBM和卷积RBM (Convolutional RBM, CRBM) 的混合深度网络结构, 底层由RBMs构造, 可以快速减少尺寸, 提取输入信息;然后, 使用CRBM在上层抽象更复杂的信息;最后, 利用基于梯度下降法的有监督学习方法对整个网络进行了指数损失函数的微调。</p>
                </div>
                <div class="p1">
                    <p id="115">同样, 用于评估的5种文本情感分类器的数据集是酒店评论数据、书籍评论数据、电脑评论数据以及维吾尔语评论数据。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">3.2 <b>参数设置</b></h4>
                <div class="p1">
                    <p id="117">为了比较结果, 本文提出的模型中使用的学习参数与文献<citation id="233" type="reference">[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">21</a>]</citation>相同。在训练前的步骤中, 本文对所有的隐藏层和输出层执行贪婪的分层无监督学习, <i>epoch</i>=30。在有监督学习阶段, 将<i>epoch</i>设置为30, 学习率为0.1, 每个<i>epoch</i>设置为0.9。使用的DBN结构为100- 100- 200- 2, 表示三个隐藏层中的神经元单元数分别为100、100和200, 输出层中的神经元单元数分别为2 (正、负) 。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">3.3 <b>性能评测指标</b></h4>
                <div class="p1">
                    <p id="119">根据混淆矩阵可以得出文本情感分类的三大性能指标, 混淆矩阵如表1所示。</p>
                </div>
                <div class="area_img" id="120">
                                            <p class="img_tit">
                                                <b>表</b>1 <b>混淆矩阵</b>
                                                    <br />
                                                Tab. 1 Confusion matrix
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907013_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201907013_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907013_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 混淆矩阵" src="Detail/GetImg?filename=images/JSJY201907013_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="121">性能评价指标准确率 (Precision, <i>P</i>) 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>F</mi><mi>Ν</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow></mfrac></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">性能评价指标召回率 (Recall, <i>R</i>) 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mi>R</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow></mfrac></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">性能评价指标F值 (F-measure) 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mn>2</mn><mo>*</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub><mi>R</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub><mo>+</mo><mi>R</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext></mrow></msub></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">从公式中可以看出来F值与准确率以及召回率都有关系, 所以在后面的实验中本文使用F值作为性能评测指标。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128">3.4 <b>实验结果</b></h4>
                <div class="p1">
                    <p id="129">在从分类准确率和训练时间方面详细分析实验结果和分析结果之前, 首先要论证特征选择和缩减对输入维数大小的影响。</p>
                </div>
                <div class="p1">
                    <p id="130">不同的特征保留率对文本情感分类的准确性以及模型的训练时间有着一定的影响, 在4个文本情感分析数据集上, 特征保留率对文本情感分类准确性的影响。如表2所示。</p>
                </div>
                <div class="area_img" id="131">
                    <p class="img_tit"><b>表</b>2 <b>不同特征保留率的分类准确性对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Classification accuracy comparison of different feature retention rates </p>
                    <p class="img_note">%</p>
                    <table id="131" border="1"><tr><td rowspan="2"><br />特征选<br />择方法</td><td rowspan="2">数据集</td><td colspan="5"><br />特征保留率</td></tr><tr><td><br />20%</td><td>30%</td><td>40%</td><td>50%</td><td>60%</td></tr><tr><td rowspan="4"><br />文档<br />频率<br /> (DF) </td><td><br />酒店评论</td><td>67.2</td><td>68.9</td><td><b>70.3</b></td><td>68.6</td><td>69.7</td></tr><tr><td><br />书籍评论</td><td>72.7</td><td>73.5</td><td><b>74.2</b></td><td>73.9</td><td>72.5</td></tr><tr><td><br />电脑评论</td><td>69.4</td><td>69.7</td><td>69.9</td><td><b>71.5</b></td><td>70.2</td></tr><tr><td><br />维吾尔语评论</td><td>63.1</td><td><b>64.6</b></td><td>63.9</td><td>61.2</td><td>61.3</td></tr><tr><td rowspan="4"><br />信息<br />增益<br /> (IG) </td><td><br />酒店评论</td><td>68.1</td><td>71.9</td><td><b>73.6</b></td><td>72.3</td><td>67.5</td></tr><tr><td><br />书籍评论</td><td>74.3</td><td><b>76.6</b></td><td>75.2</td><td>74.1</td><td>71.2</td></tr><tr><td><br />电脑评论</td><td>69.5</td><td>72.5</td><td><b>73.1</b></td><td>72.6</td><td>70.2</td></tr><tr><td><br />维吾尔语评论</td><td><b>65.9</b></td><td>63.7</td><td>61.3</td><td>61.2</td><td>61.6</td></tr><tr><td rowspan="4"><br />卡方<br />统计<br /> (CHI) </td><td><br />酒店评论</td><td>71.5</td><td><b>73.9</b></td><td>69.2</td><td>68.3</td><td>67.5</td></tr><tr><td><br />书籍评论</td><td>73.2</td><td>74.2</td><td><b>75.3</b></td><td>74.9</td><td>73.6</td></tr><tr><td><br />电脑评论</td><td>69.2</td><td>71.2</td><td><b>72.9</b></td><td>71.3</td><td>70.9</td></tr><tr><td><br />维吾尔语评论</td><td>63.2</td><td><b>64.8</b></td><td>63.1</td><td>62.5</td><td>62.3</td></tr><tr><td rowspan="4"><br />互信息<br /> (MI) </td><td><br />酒店评论</td><td>71.3</td><td>72.1</td><td><b>72.3</b></td><td>71.6</td><td>70.9</td></tr><tr><td><br />书籍评论</td><td>73.2</td><td>74.7</td><td>75.2</td><td><b>75.4</b></td><td>74.3</td></tr><tr><td><br />电脑评论</td><td>71.4</td><td><b>72.1</b></td><td>71.5</td><td>70.6</td><td>71.1</td></tr><tr><td><br />维吾尔语评论</td><td><b>64.9</b></td><td>63.2</td><td>63.5</td><td>62.8</td><td>62.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="132">从表2中可以看出使用不同的特征选择方法进行特征选择以及特征约减得到的F值不同。比如说使用文档频率方法在4个数据集上特征保留率为40%、40%、50%以及30%取得最好的分类效果。故根据表2可以得到表3 FSDBN模型在不同特征选择方法下的特征保留率。</p>
                </div>
                <div class="p1">
                    <p id="133">使用特征选择可使得特征数目减少。它展示了本文提出的降维方法的性能, 该方法可以过滤掉大多数无用的特征。表3显示了在4个数据集上以及4种特征选择方法特征减少到原特征的多少时文本情感的分类效果最好。</p>
                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表</b>3 FSDBN<b>模型的特征保留率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Feature retention rate of FSDBN model</p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td rowspan="2">数据集</td><td rowspan="2">原来<br />特征<br />数</td><td colspan="2"><br />文档频率<br /> (DF) </td><td rowspan="2"></td><td colspan="2"><br />信息增益<br /> (IG) </td><td rowspan="2"></td><td colspan="2"><br />卡方统计<br /> (CHI) </td><td rowspan="2"></td><td colspan="2"><br />互信息<br /> (MI) </td></tr><tr><td><br />特征<br />保留<br />率/%</td><td>后来<br />特征<br />数</td><td><br />特征<br />保留<br />率/%</td><td>后来<br />特征<br />数</td><td><br />特征<br />保留<br />率/%</td><td>后来<br />特征<br />数</td><td><br />特征<br />保留<br />率/%</td><td>后来<br />特征<br />数</td></tr><tr><td>酒店评论</td><td>15 062</td><td>40</td><td>6 024</td><td></td><td>40</td><td>6 024</td><td></td><td>30</td><td>4 518</td><td></td><td>40</td><td>6 024</td></tr><tr><td><br />书籍评论</td><td>18 352</td><td>40</td><td>7 340</td><td></td><td>30</td><td>5 505</td><td></td><td>40</td><td>7 340</td><td></td><td>50</td><td>9 176</td></tr><tr><td><br />电脑评论</td><td>17 565</td><td>50</td><td>8 782</td><td></td><td>35</td><td>6 147</td><td></td><td>40</td><td>7 026</td><td></td><td>30</td><td>5 269</td></tr><tr><td><br />维吾尔语评论</td><td>22 486</td><td>30</td><td>6 745</td><td></td><td>20</td><td>4 497</td><td></td><td>20</td><td>4 497</td><td></td><td>20</td><td>4 497</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">3.4.1 F值</h4>
                <div class="p1">
                    <p id="136">采用半监督学习方法对4个情感分类数据集进行10倍交叉验证的分类准确性结果如表4所示。</p>
                </div>
                <div class="p1">
                    <p id="137">实验证明了在三个数据集上本文提出的方法FSDNBN在准确率上都有所提升并且在两个数据集上的特征选择算法都是信息增益。</p>
                </div>
                <div class="p1">
                    <p id="138">从表4可以看出, FSDBN在三个标记为粗体的数据集中执行了最好的精度结果。另一方面, DBN、HDBN和FSDBN可以在所有数据集中执行得很好。结果证明了深层架构在情感分类中的强大功能。</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139">3.4.2 训练时间</h4>
                <div class="p1">
                    <p id="140">DBN与FSDBN的训练时间 (从执行特征提取到构建完整模型) 结果如图5所示。比较而言, 除了要训练的特性数量和网络结构外, 本文在相同的环境和相同的参数设置中运行它们。对于要训练的特性数量, 对于DBN, 本文使用了完整的特性集, 而不进行任何的特性选择和减少, 但是对于FSDBN, 本文采用了第2章中描述的方法。</p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit"><b>表</b>4 <b>半监督文本情感分类算法的</b>F<b>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 F values of semi-supervised text sentiment classification algorithms </p>
                    <p class="img_note">%</p>
                    <table id="141" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />数据集</td></tr><tr><td><br />酒店评论</td><td>书籍评论</td><td>电脑评论</td><td>维吾尔语评论</td></tr><tr><td><br />Spectral</td><td>68.5</td><td>62.9</td><td>60.2</td><td>59.6</td></tr><tr><td><br />TSVM</td><td>69.3</td><td>64.3</td><td>62.3</td><td>60.2</td></tr><tr><td><br />PIV</td><td>70.2</td><td><b>77.1</b></td><td>70.3</td><td>63.4</td></tr><tr><td><br />DBN</td><td>72.3</td><td>74.6</td><td>65.2</td><td>63.7</td></tr><tr><td><br />HDBN</td><td>73.5</td><td>76.8</td><td>72.9</td><td>64.6</td></tr><tr><td><br />FSDBN (DF) </td><td>70.3</td><td>74.2</td><td>71.5</td><td>64.6</td></tr><tr><td><br />FSDBN (IG) </td><td>73.6</td><td>76.6</td><td><b>73.2</b></td><td><b>65.9</b></td></tr><tr><td><br />FSDBN (CHI) </td><td><b>73.9</b></td><td>75.3</td><td>72.9</td><td>64.8</td></tr><tr><td><br />FSDBN (MI) </td><td>72.3</td><td>75.4</td><td>72.1</td><td>64.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="142">在图5中, 可以看到FSDBN在所有数据集中花费的训练时间比DBN少得多。FSDBN的平均速度可以提高2倍多, 在使用维吾尔语评论数据集时几乎可以提高4倍。这是因为维吾尔语评论数据集比其他3个数据集具有更多的特性, 因此, 使用FSDBN, 删除了更多的特征, 这使得与DBN相比, 训练时间更短, 同时仍然保持了良好的准确率。</p>
                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907013_143.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同算法的训练时间对比" src="Detail/GetImg?filename=images/JSJY201907013_143.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同算法的训练时间对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907013_143.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Training time comparison of different algorithms</p>

                </div>
                <div class="p1">
                    <p id="144">特别地, 显著提高FSDBN训练时间的主要因素是它更简单的深层结构, 它替换了几个隐藏层, 但是增加了本文提出的特征选择方法。</p>
                </div>
                <div class="p1">
                    <p id="145">根据实验结果, 可以看出本文提出的FSDBN比其他半监督情绪分类算法更快、更准确。</p>
                </div>
                <h3 id="146" name="146" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="147">为了解决文本情感分类问题, 本文提出了一种名为FSDBN文本情感分类算法, 使用基于过滤器的特征选择技术替换了DBNs中的几个隐藏层, 然后, 过滤掉不必要的特性, 只选择有意义的特性。实验结果表明, 在特征选择和约简的辅助下, FSDBN的学习阶段效率更高。FSDBN的分类精度高于基本的半监督学习算法, 如光谱学习、TSVM和个人PIV。此外, FSDBN的性能略好于其他深度学习算法DBN和混合深度信念网络 (HDBN) 。此外, 还可以观察到, 与传统的DBN相比, FSDBN花费的训练时间要少得多。在未来的工作中, 计划将本文的算法并行化并在GPU平台上运行, 以加速其计算。目标是解决现实世界中的大量问题, 具有更好的标度能力, 同时仍能保持良好的分类准确率。这里的特征选择方法只使用了文档频率、信息增益、卡方统计以及互信息, 还可以使用其他的特征选择方法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="170">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MDI3OTlNcW85RlpPb05DWDh4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VGJoYz1OaWZKWmJLOUh0ag==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> HINTON G, OSINDERO S, TEH Y.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hybrid deep belief networks for semi-supervised sentiment classification">

                                <b>[2]</b> ZHOU S, CHEN Q, WANG X, et al.Hybrid deep belief networks for semi-supervised sentiment classification[C]// Proceeding of the 2014 25th International Conference on Computational Linguistic.Stroudsburg, PA:Association for Computational Linguistics, 2014:1341-1349.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active deep networks for semi-supervised sentiment classification">

                                <b>[3]</b> ZHOU S, CHEN Q, WANG X.Active deep networks for semisupervised sentiment classification.[C]// Proceedings of the 2010 23rd International Conference on Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2010:1515-1523.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recursive deep models for semantic compositionality over a sentiment treebank">

                                <b>[4]</b> SOCHER R, PERELYGIN A, WU J J.et al.Recursive deep models for semantic compositionality over a sentiment treebank [C]// Proceedings of the 2013 International Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2013:1631-1642.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mine the easy classify the hard:a semisupervised approach to automatic sentiment classification">

                                <b>[5]</b> DASGUPTA, S, NG V.Mine the easy, classify the hard:a semisupervised approach to automatic sentiment classification [C]// Proceedings of the 2009 47th International Conference on Annual Meeting of the Association for Computational Linguistics and Proceedings of the 2009/4th International Joint Conference on Natural Language of the Asian Federation of Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2009:701-709.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Thumbs up? sentiment classification using machine learning techniques">

                                <b>[6]</b> PANG B, LEE L, VAITHYANATHAN S.Thumbs up?:sentiment classification using machine learning techniques [C]// Proceedings of the 2002 International Conference on Association for Computational Linguistics on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2002:79-86.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An extensive empirical study of feature selection metrics for text classification">

                                <b>[7]</b> FORMAN G.An extensive empirical study of feature selection metrics for text classification[J].The Journal of Machine Learning Research, 2003:1289-1305.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Comparative Study on Feature Selection in Text Categorization">

                                <b>[8]</b> YANG Y, PEDERSEN J O.A comparative study on feature selection in text categorization[C]// Proceedings of the 1997 14th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann, 1997:412-420.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS200403002&amp;v=MTgzMzZxQnRHRnJDVVI3cWZadVpzRnkvblZMdk5LQ2pZZmJHNEh0WE1ySTlGWm9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 周茜, 赵明生, 扈旻.中文文本分类中的特征选择研究[J].中文信息学报, 2004, 18 (3) :17-23. (ZHOU Q, ZHAO M S, HU M.Research on feature selection in Chinese text classification [J].Journal of Chinese Information Processing, 2004, 18 (3) :17-23.) 
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJGD201601022&amp;v=MTA2NzFzRnkvblZMdk5KeWZNYXJHNEg5Zk1ybzlIWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 吴金源, 冀俊忠, 赵学武, 等.基于特征选择技术的情感词权重计算[J].北京工业大学学报, 2016, 42 (1) :142-151. (WU J Y, JI J Z, ZHAO X W, et al.Weight calculation of affective words based on feature selection technique[J].Journal of Beijing University of Technology, 2016, 42 (1) :142-151.) 
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201708012&amp;v=MzEyNDFyQ1VSN3FmWnVac0Z5L25WTHZOTWpYU1pMRzRIOWJNcDQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 周爱武, 马那那, 刘慧婷.基于卡方统计的情感文本分类[J].微电子学与计算机, 2017, 34 (8) :57-61. (ZHOU A W, MA N N, LIU H T.Emotional text classification based on chi-square statistics [J].Microelectronics and Computer, 2017, 34 (8) :57-61.) 
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201104037&amp;v=MTAzOTBETXE0OUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVkx2Tkx6N01hYkc0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 裴英博, 刘晓霞.文本分类中改进型CHI特征选择方法的研究[J].计算机工程与应用, 2011, 47 (4) :128-130. (PEI Y B, LIU X X.Research on improved CHI feature selection method in text classification [J].Computer Engineering and Application, 2011, 47 (4) :128-130.) 
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sentiment classification in Persian introducing a mutual information-based method for feature selection">

                                <b>[13]</b> BAGHERI A, SARAEE M, de JONG F.Sentiment classification in Persian:introducing a mutual information-based method for feature selection[C]// Proceedings of the 2013 21th International Conference on Electrical Engineering.Piscataway, NJ:IEEE, 2013:1-6.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Biographies,bollywood,boom-boxes and blenders:domain adaptation for sentiment classifica-tion">

                                <b>[14]</b> BLIZER J, DREDZE M, PEREIRA F.Biographies, bollywood, boomboxes and blenders:domain adaptation for sentiment classification[C]// Proceedings of the 2007 International Conference on Association for Computational Linguistic.Stroudsburg, PA:Association for Computational Linguistics, 2007:440-447.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RestrictedBoltzmann machines and deep belief networks on multi-coreprocessors">

                                <b>[15]</b> LOPES N, RIBEIRO B, GONÇALVES J.Restricted Boltzmann machines and deep belief networks on multi-core processors [C]// Proceedings of the 2012 International Joint Conference on Neural Networks Piscataway, NJ:IEEE, 2012:1-7.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDSH201601013&amp;v=Mjk4MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVkx2Tklpbllackc0SDlmTXJvOUVaNFE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 张庆庆, 刘西林.基于深度信念网络的文本情感分类研究[J].西北工业大学学报 (社会科学版) , 2016, 36 (1) :62-66. (ZHANG Q Q, LIU X L.Research on text emotion classification based on deep belief network[J].Journal of Northwest Polytechnic University (Social Science Edition) , 2016, 36 (1) :62-66.) 
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201704014&amp;v=MDk4NDVyRzRIOWJNcTQ5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTHZOTHpUVFo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 伊尔夏提·吐尔贡, 吾守尔·斯拉木, 热西旦木·吐尔洪太, 等.维吾尔文情感语料库的构建与分析[J].计算机与现代化, 2017 (4) :67-72. (TUERGONG Y, SILAMU W, TUSERHONGTAI R, et al.Construction and analysis of Uighur affective corpus [J].Computer and Modernization, 2017 (4) :67-72.) 
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral learning">

                                <b>[18]</b> KAMVAR S D, DAN K, MANNING C D.Spectral learning[C]// Proceedings of the 2003 International Joint Conference on Artificial Intelligence.San Francisco, CA:Morgan Kaufmann, 2003:561-566.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large scale transductive SVMs">

                                <b>[19]</b> COLLOBERT R, SINZ F, WESTON J, et al.Large scale transductive SVMs[J].The Journal of Machine Learning Research, 2006, 7:1687-1712.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Employing personal/impersonal views in supervised and semi-supervised sentiment classification">

                                <b>[20]</b> LI S, HUANG C R, ZHOU G, et al.Employing personal/impersonal views in supervised and semi-supervised sentiment classification[C]// Proceedings of the 2010 48th International Joint Conference on Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2010:414-423.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep belief networks with feature selection for sentiment classification">

                                <b>[21]</b> RUANGKANOKMAS P, ACHALAKUL T, AKKARAJITSAKUL K.Deep belief networks with feature selection for sentiment classification[C]// Proceedings of the 2017 48th International Conference on Intelligent Systems, Modelling and Simulation.Piscataway, NJ:IEEE, 2017:9-14.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907013" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907013&amp;v=MDEwOTQ5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTHZLTHo3QmQ3RzRIOWpNcUk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
