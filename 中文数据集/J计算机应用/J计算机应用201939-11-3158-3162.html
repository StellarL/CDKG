<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136444687783750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911010%26RESULT%3d1%26SIGN%3dSb7LaLEhEiUMctLJrKF1b4ka2Po%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911010&amp;v=MTYxNTBqTnJvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnJ6Tkx6N0JkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="1 改进ORB特征点匹配方法 ">1 改进ORB特征点匹配方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="1.1 &lt;b&gt;特征点提取&lt;/b&gt;">1.1 <b>特征点提取</b></a></li>
                                                <li><a href="#50" data-title="1.2 &lt;b&gt;特征点相似度量&lt;/b&gt;">1.2 <b>特征点相似度量</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="2 实验结果分析 ">2 实验结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="2.1 &lt;b&gt;尺度变化图像特征点匹配&lt;/b&gt;">2.1 <b>尺度变化图像特征点匹配</b></a></li>
                                                <li><a href="#82" data-title="2.2 &lt;b&gt;模糊图像特征点匹配&lt;/b&gt;">2.2 <b>模糊图像特征点匹配</b></a></li>
                                                <li><a href="#90" data-title="2.3 &lt;b&gt;光照变化图像特征点匹配&lt;/b&gt;">2.3 <b>光照变化图像特征点匹配</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="3 基于特征点匹配的图像拼接 ">3 基于特征点匹配的图像拼接</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#114" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="图1 尺度变化图像">图1 尺度变化图像</a></li>
                                                <li><a href="#77" data-title="图2 尺度变化图像特征点匹配结果">图2 尺度变化图像特征点匹配结果</a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;尺度变化图像的特征点正确匹配率&lt;/b&gt;"><b>表</b>1 <b>尺度变化图像的特征点正确匹配率</b></a></li>
                                                <li><a href="#84" data-title="图3 模糊图像">图3 模糊图像</a></li>
                                                <li><a href="#86" data-title="图4 模糊图像特征点匹配结果">图4 模糊图像特征点匹配结果</a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;模糊图像的特征点正确匹配率&lt;/b&gt;"><b>表</b>2 <b>模糊图像的特征点正确匹配率</b></a></li>
                                                <li><a href="#92" data-title="图5 光照变化测试图像">图5 光照变化测试图像</a></li>
                                                <li><a href="#94" data-title="图6 光照变化图像特征点匹配结果">图6 光照变化图像特征点匹配结果</a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;光照变化图像的特征点正确匹配率&lt;/b&gt;"><b>表</b>3 <b>光照变化图像的特征点正确匹配率</b></a></li>
                                                <li><a href="#101" data-title="图7 测试图像">图7 测试图像</a></li>
                                                <li><a href="#103" data-title="图8 特征点匹配正确率变化曲线">图8 特征点匹配正确率变化曲线</a></li>
                                                <li><a href="#106" data-title="图9 三种方法的特征点匹配结果">图9 三种方法的特征点匹配结果</a></li>
                                                <li><a href="#107" data-title="图10 三种方法的图像拼接结果">图10 三种方法的图像拼接结果</a></li>
                                                <li><a href="#109" data-title="图11 图像拼接结果放大图">图11 图像拼接结果放大图</a></li>
                                                <li><a href="#112" data-title="图12 本文方法最终图像拼接结果">图12 本文方法最终图像拼接结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="124">


                                    <a id="bibliography_1" title=" 左川,庞春江.基于改进ORB的抗视角变换快速图像匹配算法[J].传感技术学报,2018,31(11):98-104.(ZUO C,PANG C J.Fast image matching algorithm based on improved ORB for anti-viewing transformation[J].Journal of Sensing Technology,2018,31(11):98-104.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CGJS201811017&amp;v=MDc3ODBGckNVUjdxZlp1WnNGeW5uVnJ6TkppckJmYkc0SDluTnJvOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         左川,庞春江.基于改进ORB的抗视角变换快速图像匹配算法[J].传感技术学报,2018,31(11):98-104.(ZUO C,PANG C J.Fast image matching algorithm based on improved ORB for anti-viewing transformation[J].Journal of Sensing Technology,2018,31(11):98-104.)
                                    </a>
                                </li>
                                <li id="126">


                                    <a id="bibliography_2" title=" 董强,刘晶红,周前飞.用于遥感图像拼接的改进SURF算法[J].吉林大学学报(工学版),2017,47(5):1644-1652.(DONG Q,LIU J H,ZHOU Q F.Improved SURF algorithm for remote sensing image stitching[J].Journal of Jilin University (Engineering Edition),2017,47(5):1644-1652.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201705042&amp;v=MTk5MTRublZyek5MeUhNZDdHNEg5Yk1xbzlCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         董强,刘晶红,周前飞.用于遥感图像拼接的改进SURF算法[J].吉林大学学报(工学版),2017,47(5):1644-1652.(DONG Q,LIU J H,ZHOU Q F.Improved SURF algorithm for remote sensing image stitching[J].Journal of Jilin University (Engineering Edition),2017,47(5):1644-1652.)
                                    </a>
                                </li>
                                <li id="128">


                                    <a id="bibliography_3" title=" MENTZER N,PAYA-VAYA G,BLUME H.Analyzing the performance-hardware trade-off of an ASIP-based sift feature extraction[J].Journal of Signal Processing Systems,2016,85(1):83-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analyzing the performance-hardware trade-off of an ASIP-based sift feature extraction">
                                        <b>[3]</b>
                                         MENTZER N,PAYA-VAYA G,BLUME H.Analyzing the performance-hardware trade-off of an ASIP-based sift feature extraction[J].Journal of Signal Processing Systems,2016,85(1):83-99.
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_4" title=" 邹承明,侯小碧,马静.基于几何学图像配准的SIFT图像拼接算法[J].华中科技大学学报(自然科学版),2016,44(4):32-36.(ZOU C M,HOU X B,MA J.SIFT image stitching algorithm based on geometric image registration[J].Journal of Huazhong University of Science and Technology (Natural Science Edition),2016,44(4):32-36.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HZLG201604007&amp;v=MjAwNjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublZyek5MVGZIYWJHNEg5Zk1xNDlGWTRRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         邹承明,侯小碧,马静.基于几何学图像配准的SIFT图像拼接算法[J].华中科技大学学报(自然科学版),2016,44(4):32-36.(ZOU C M,HOU X B,MA J.SIFT image stitching algorithm based on geometric image registration[J].Journal of Huazhong University of Science and Technology (Natural Science Edition),2016,44(4):32-36.)
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_5" title=" 张勇,王志锋,马文.基于改进SIFT特征点匹配的图像拼接算法研究[J].微电子学与计算机,2016,33(3):60-64.(ZHANG Y,WANG Z F,MA W.Research on image stitching algorithm based on improved SIFT feature point matching[J].Microelectronics &amp;amp; Computer,2016,33(3):60-64.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201603013&amp;v=MzE4MDc0SDlmTXJJOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnJ6Tk1qWFNaTEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         张勇,王志锋,马文.基于改进SIFT特征点匹配的图像拼接算法研究[J].微电子学与计算机,2016,33(3):60-64.(ZHANG Y,WANG Z F,MA W.Research on image stitching algorithm based on improved SIFT feature point matching[J].Microelectronics &amp;amp; Computer,2016,33(3):60-64.)
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_6" title=" 曹君宇,周浩,高志山,等.基于SURF的图像拼接过程中配准算法的改进[J].云南大学学报(自然科学版),2016,38(6):845-852.(CAO J Y,ZHOU H,GAO Z S,et al.Improvement of registration algorithm in image stitching process based on SURF[J].Journal of Yunnan University (Natural Science Edition),2016,38(6):845-852.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YNDZ201606003&amp;v=MDQwODJQUGRMRzRIOWZNcVk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcnpOUEM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         曹君宇,周浩,高志山,等.基于SURF的图像拼接过程中配准算法的改进[J].云南大学学报(自然科学版),2016,38(6):845-852.(CAO J Y,ZHOU H,GAO Z S,et al.Improvement of registration algorithm in image stitching process based on SURF[J].Journal of Yunnan University (Natural Science Edition),2016,38(6):845-852.)
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_7" title=" 张博,韩广良.基于Mask R-CNN的ORB去误匹配方法[J].液晶与显示,2018,33(8):69-75.(ZHANG B,HAN G L.Based on Mask R-CNN ORB error matching method[J].LCD and Display,2018,33(8):69-75.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YJYS201808009&amp;v=MTQ0MDJmU2ZiRzRIOW5NcDQ5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcnpOUEM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         张博,韩广良.基于Mask R-CNN的ORB去误匹配方法[J].液晶与显示,2018,33(8):69-75.(ZHANG B,HAN G L.Based on Mask R-CNN ORB error matching method[J].LCD and Display,2018,33(8):69-75.)
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_8" title=" 王健,于鸣,任洪娥.一种用于图像拼接的改进ORB算法[J].液晶与显示,2018,33(6):73-80.(WANG J,YU M,REN H E.An improved ORB algorithm for image stitching[J].LCD and Display,2018,33(6):73-80.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YJYS201806011&amp;v=MDg5MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcnpOUENmU2ZiRzRIOW5NcVk5RVpZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         王健,于鸣,任洪娥.一种用于图像拼接的改进ORB算法[J].液晶与显示,2018,33(6):73-80.(WANG J,YU M,REN H E.An improved ORB algorithm for image stitching[J].LCD and Display,2018,33(6):73-80.)
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_9" title=" WANG D,LIU H,CHENG X.A miniature binocular endoscope with local feature matching and stereo matching for 3D measurement and 3D reconstruction[J].Sensors,2018,18(7):2243-2250." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A miniature binocular endoscope with local feature matching and stereo matching for 3D measurement and 3D reconstruction">
                                        <b>[9]</b>
                                         WANG D,LIU H,CHENG X.A miniature binocular endoscope with local feature matching and stereo matching for 3D measurement and 3D reconstruction[J].Sensors,2018,18(7):2243-2250.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_10" title=" SHAN Y,LI S.Descriptor matching for a discrete spherical image with a convolutional neural network[J].IEEE Access,2018,6:20748-20755." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Descriptor matching for a discrete spherical image with a convolutional neural network">
                                        <b>[10]</b>
                                         SHAN Y,LI S.Descriptor matching for a discrete spherical image with a convolutional neural network[J].IEEE Access,2018,6:20748-20755.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_11" title=" YU H,FU Q,YANG Z,et al.Robust robot pose estimation for challenging scenes with an RGB-D camera[J].IEEE Sensors Journal,2019,19(6):2217-2229." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust robot pose estimation for challenging scenes with an RGB-D camera">
                                        <b>[11]</b>
                                         YU H,FU Q,YANG Z,et al.Robust robot pose estimation for challenging scenes with an RGB-D camera[J].IEEE Sensors Journal,2019,19(6):2217-2229.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_12" title=" ZHANG F,WAH B W.Fundamental principles on learning new features for effective dense matching[J].IEEE Transactions on Image Processing,2017,27(2):822-836." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fundamental principles on learning new features for effective dense matching">
                                        <b>[12]</b>
                                         ZHANG F,WAH B W.Fundamental principles on learning new features for effective dense matching[J].IEEE Transactions on Image Processing,2017,27(2):822-836.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_13" title=" LEE W T,CHEN H I,CHEN M S,et al.High-resolution 360 video foveated stitching for real-time VR[J].Computer Graphics Forum,2017,36(7):115-123." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution 360video foveated stitching for real-time VR">
                                        <b>[13]</b>
                                         LEE W T,CHEN H I,CHEN M S,et al.High-resolution 360 video foveated stitching for real-time VR[J].Computer Graphics Forum,2017,36(7):115-123.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_14" title=" KIM Y,JUNG H.Reconfigurable hardware architecture for faster descriptor extraction in surf[J].Electronics Letters,2018,54(4):210-212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reconfigurable hardware architecture for faster descriptor extraction in SURF">
                                        <b>[14]</b>
                                         KIM Y,JUNG H.Reconfigurable hardware architecture for faster descriptor extraction in surf[J].Electronics Letters,2018,54(4):210-212.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_15" title=" BORGES F A S,FERNANDES R A S,SILVA I N,et al.Feature extraction and power quality disturbances classification using smart meters signals[J].IEEE Transactions on Industrial Informatics,2017,12(2):824-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature extraction and power quality disturbances classification using smart meters signals">
                                        <b>[15]</b>
                                         BORGES F A S,FERNANDES R A S,SILVA I N,et al.Feature extraction and power quality disturbances classification using smart meters signals[J].IEEE Transactions on Industrial Informatics,2017,12(2):824-833.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_16" title=" GARCIA J,GARDEL A,BRAVO I,et al.Multiple view oriented matching algorithm for people reidentification[J].IEEE Transactions on Industrial Informatics,2017,10(3):1841-1851." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiple view oriented matching algorithm for people reidentification">
                                        <b>[16]</b>
                                         GARCIA J,GARDEL A,BRAVO I,et al.Multiple view oriented matching algorithm for people reidentification[J].IEEE Transactions on Industrial Informatics,2017,10(3):1841-1851.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_17" title=" SRINIVASA K G,DEVI B N S.GPU based n-gram string matching algorithm with score table approach for string searching in many documents[J].Journal of the Institution of Engineers,2017,98(2):1-10." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD9498F21FD7ED631A92E1C94D7567D168&amp;v=MjA0MDZmQnJMVTA1dHBoeGJxNnc2MD1OajdCYXJxOEY5bTZyWTR6RU94NmVIbzZ6bWNhNkVwOE8zYm0yQlV3ZjdYZ1JMeVhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         SRINIVASA K G,DEVI B N S.GPU based n-gram string matching algorithm with score table approach for string searching in many documents[J].Journal of the Institution of Engineers,2017,98(2):1-10.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_18" title=" 曾庆化,陈艳,王云舒,等.一种基于ORB的快速大视角图像匹配算法[J].控制与决策,2017,32(12):2233-2239.(ZENG Q H,CHEN Y,WANG Y S,et al.A fast large angle view image matching algorithm based on ORB[J].Control and Decision,2017,32(12):2233-2239.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201712017&amp;v=MzI0MDRiYkc0SDliTnJZOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnJ6TkxqZlM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         曾庆化,陈艳,王云舒,等.一种基于ORB的快速大视角图像匹配算法[J].控制与决策,2017,32(12):2233-2239.(ZENG Q H,CHEN Y,WANG Y S,et al.A fast large angle view image matching algorithm based on ORB[J].Control and Decision,2017,32(12):2233-2239.)
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_19" title=" 杜承垚,袁景凌,陈旻骋,等.GPU加速与L-ORB特征提取的全景视频实时拼接[J].计算机研究与发展,2017,54(6):1316-1325.(DU C Y,YUAN J L,CHEN M C,et al.GPU acceleration and L-ORB feature extraction for panoramic video real-time stitching[J].Journal of Computer Research and Development,2017,54(6):1316-1325.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201706014&amp;v=MDgwNzMzenFxQnRHRnJDVVI3cWZadVpzRnlublZyek5MeXZTZExHNEg5Yk1xWTlFWUlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         杜承垚,袁景凌,陈旻骋,等.GPU加速与L-ORB特征提取的全景视频实时拼接[J].计算机研究与发展,2017,54(6):1316-1325.(DU C Y,YUAN J L,CHEN M C,et al.GPU acceleration and L-ORB feature extraction for panoramic video real-time stitching[J].Journal of Computer Research and Development,2017,54(6):1316-1325.)
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-23 12:55</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3158-3162 DOI:10.11772/j.issn.1001-9081.2019051180            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于距离融合的图像特征点匹配方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BF%AE%E6%98%A5%E6%B3%A2&amp;code=10826096&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">修春波</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E4%BA%91%E8%8F%B2&amp;code=43224061&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马云菲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%BD%98%E8%82%96%E6%A5%A0&amp;code=42145258&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">潘肖楠</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0050423&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津工业大学电气工程与自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%94%B5%E5%B7%A5%E7%94%B5%E8%83%BD%E6%96%B0%E6%8A%80%E6%9C%AF%E5%A4%A9%E6%B4%A5%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%A4%A9%E6%B4%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">电工电能新技术天津市重点实验室(天津工业大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对ORB算法中特征点缺乏尺度不变性导致算法误匹配率高,以及二进制鲁棒独立基本特征(BRIEF)算法的描述子易受噪声影响的问题,提出了改进的特征点匹配方法。采用加速的具有鲁棒性的特征(SURF)算法进行特征点提取,利用带有方向信息的BRIEF算法进行特征点描述;在特征点邻域内选取随机点对,并对随机点对的灰度大小比较和相似度比较分别进行编码,采用汉明距离计算两种编码的差异;利用自适应加权融合的方式实现特征点相似性距离度量。实验结果表明,改进方法对于尺度变化、光照变化以及模糊变化的图像具有更好的适应性,与传统ORB特征点匹配方法相比能够获得更高的特征点正确匹配率,且该特征点匹配方法可用于改善图像拼接的性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%82%B9%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征点匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E6%8B%BC%E6%8E%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像拼接;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E6%9D%83%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加权融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">汉明距离;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%BA%E5%BA%A6%E4%B8%8D%E5%8F%98%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尺度不变性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *修春波(1978—),男,黑龙江大庆人,教授,博士,主要研究方向:神经网络、系统建模、混沌控制,电子邮箱,xiuchunbo@tjpu.edu.cn;
                                </span>
                                <span>
                                    马云菲(1995—),女,河北石家庄人,硕士研究生,主要研究方向:图像处理、模式识别;;
                                </span>
                                <span>
                                    潘肖楠(1992—),女,甘肃庆阳人,硕士研究生,主要研究方向:神经网络、目标跟踪。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>天津市自然科学基金资助项目(18JCYBJC88300,18JCYBJC88400);</span>
                    </p>
            </div>
                    <h1><b>Image feature point matching method based on distance fusion</b></h1>
                    <h2>
                    <span>XIU Chunbo</span>
                    <span>MA Yunfei</span>
                    <span>PAN Xiaonan</span>
            </h2>
                    <h2>
                    <span>School of Electrical Engineering and Automation, Tianjin Polytechnic University</span>
                    <span>Tianjin Key Laboratory of Advanced Electrical Engineering and Energy Technology (Tianjin Polytechnic University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to reduce the matching error rate of ORB(Oriented FAST and Rotated BRIEF) method caused by the scale invariance of the feature points in the algorithm and enhance the robustness of the descriptors of Binary Robust Independent Elementary Features(BRIEF) algorithm to noise, an improved feature point matching method was proposed. Speeded-Up Robust Features(SURF) algorithm was used to extract feature points, and BRIEF algorithm with direction information was used to describe the feature points. Random pixel pairs in the neighborhood of the feature point were selected, the comparison results of the grayscales and the similarity of pixel pairs were encoded respectively, and Hamming distance was used to calculate the differences between the two codes. The similarity between the feature points were measured by the adaptive weighted fusion method. Experimental results show that the improved method has better adaptability to the scale variance, illumination variance and blurred variance of images, can obtain a higher feature point correct matching rate compared with the conventional ORB method, and can be used to improve the performance of image stitching.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20point%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature point matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20stitching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image stitching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weighted%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weighted fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hamming%20distance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hamming distance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scale%20invariance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scale invariance;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XIU Chunbo, born in 1978, Ph. D., professor. His research interests include neural network, system modeling, chaos control. ;
                                </span>
                                <span>
                                    MA Yunfei, born in 1995, M. S. candidate. Her research interests include image processing, pattern recognition. ;
                                </span>
                                <span>
                                    PAN Xiaonan, born in 1992, M. S. candidate. Her research interests include neural network, target tracking.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Tianjin Natural Science Foundation(18JCYBJC88300,18JCYBJC88400);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">图像匹配是计算机视觉领域的核心技术问题之一<citation id="164" type="reference"><link href="124" rel="bibliography" /><link href="126" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>,在图像拼接、智能视觉诊断、机器人视觉<citation id="165" type="reference"><link href="128" rel="bibliography" /><link href="130" rel="bibliography" /><link href="132" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>等工程领域中有着重要的应用价值。常用的图像匹配算法有基于块匹配的方法和基于特征点的匹配方法两类。块匹配方法通常不具有旋转不变性,且当图像块较小时匹配结果易受噪声影响,而当图像块较大时图像之间的差异会导致匹配效果变差。因此,基于特征点的匹配方法成为当前研究的主流方法<citation id="162" type="reference"><link href="134" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。常见的特征点提取方法有尺度不变特征转换(Scale-Invariant Feature Transform, SIFT)算法、加速段测试的特征(Features From Accelerated Segment Test, FAST)算法和加速的具有鲁棒性的特征(Speeded-Up Robust Features, SURF)等方法<citation id="166" type="reference"><link href="136" rel="bibliography" /><link href="138" rel="bibliography" /><link href="140" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。SIFT方法计算量较大,不适用于实时性要求较高的场合<citation id="167" type="reference"><link href="142" rel="bibliography" /><link href="144" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>; FAST方法计算速度较快,在ORB(Oriented FAST and Rotated BRIEF)描述方法中得到了应用,但FAST方法检测的特征点不具有尺度不变性,因此,误匹配率较高<citation id="163" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>; SURF方法是SIFT方法的改进,不但具有较快的计算速度,而且所检测的特征点稳定性较好<citation id="168" type="reference"><link href="148" rel="bibliography" /><link href="150" rel="bibliography" /><link href="152" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>; 传统的ORB方法采用带有方向信息的二进制鲁棒独立基本特征(Binary Robust Independent Elementary Features, BRIEF)描述子计算特征点领域内随机点对灰度大小比较的编码,但BRIEF特征描述子对噪声比较敏感,因此采用基于BRIEF特征描述子进行特征点匹配的效果有时并不理想<citation id="169" type="reference"><link href="154" rel="bibliography" /><link href="156" rel="bibliography" /><link href="158" rel="bibliography" /><link href="160" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="43">为此,本文提出了改进的ORB方法进行特征点匹配。为了克服FAST算法不具有尺度不变性的缺点,采用SURF算法进行特征点提取,并在BRIEF特征描述子中加入了特征点邻域内随机点对的灰度相似度的比较信息,分别采用汉明距离计算点对灰度大小和相似度比较的距离,将两个距离的自适应加权融合结果作为特征点的相似性度量,以此实现图像的特征点匹配,从而提高特征点匹配的正确率, 改善图像拼接等技术性能。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag">1 改进ORB特征点匹配方法</h3>
                <h4 class="anchor-tag" id="45" name="45">1.1 <b>特征点提取</b></h4>
                <div class="p1">
                    <p id="46">传统的ORB算法采用FAST算法进行特征点提取,利用带有方向信息的BRIEF特征描述子对特征点进行描述,并利用汉明距离计算特征点的匹配度。由于FAST算法所检测的特征点不具有尺度不变性,为此,提出采用SURF算法进行特征点提取的改进ORB方法。</p>
                </div>
                <div class="p1">
                    <p id="47">通常,图像中的像素点(<i>x</i>,<i>y</i>)的Hessian矩阵计算为:</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>f</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mtd><mtd><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>f</mi></mrow><mrow><mo>∂</mo><mi>x</mi><mo>∂</mo><mi>y</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>f</mi></mrow><mrow><mo>∂</mo><mi>x</mi><mo>∂</mo><mi>y</mi></mrow></mfrac></mtd><mtd><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>f</mi></mrow><mrow><mo>∂</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">为了获得具有尺度不变性的特征点,选用不同尺度的滤波器对图像进行高斯滤波,然后计算Hessian矩阵。将Hessian矩阵判别式取极值的点确定为特征点。</p>
                </div>
                <h4 class="anchor-tag" id="50" name="50">1.2 <b>特征点相似度量</b></h4>
                <div class="p1">
                    <p id="51"><i>BRIEF</i>特征描述子根据特征点邻域内随机点对的灰度值大小比较组成编码。例如,在特征点邻域内选取<i>n</i>对随机点对,设<i>f</i>(<i>x</i><sub><i>i</i></sub>)和<i>f</i>(<i>y</i><sub><i>i</i></sub>)分别为特征点邻域内两个像素点<i>x</i><sub><i>i</i></sub>和<i>y</i><sub><i>i</i></sub>对应的灰度值。定义测试函数<i>τ</i>():</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>τ</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>,</mo></mtd><mtd columnalign="left"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>f</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>≥</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">对于<i>n</i>对随机点对(<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>),可生成长度为<i>n</i>的二进制字符串,由此得到BRIEF特征描述子的编码为:</p>
                </div>
                <div class="p1">
                    <p id="54"><i><b>T</b></i><sub><i>s</i></sub>=[<i>τ</i>(<i>x</i><sub>1</sub>,<i>y</i><sub>1</sub>),<i>τ</i>(<i>x</i><sub>2</sub>,<i>y</i><sub>2</sub>),…,<i>τ</i>(<i>x</i><sub><i>n</i></sub>,<i>y</i><sub><i>n</i></sub>)]      (3)</p>
                </div>
                <div class="p1">
                    <p id="55">式(3)是根据特征点邻域内随机点对灰度大小比较所得的BRIEF特征描述子。当两个随机点对的灰度接近时,图像间光照的微弱变化等干扰会对像素点对灰度的大小关系产生较大影响,从而导致特征点匹配的准确率降低。为了提高特征描述子对干扰变化的鲁棒性,可在上述描述子的基础上融合随机点对灰度相似度的比较信息,弥补点对灰度大小比较信息的不足。定义相似度比较测试函数为:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>τ</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>,</mo></mtd><mtd columnalign="left"><mrow><mo>|</mo><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>f</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mo>&gt;</mo><mi>θ</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">其中:<i>θ</i>为相似度比较阈值,如果像素点对的灰度值差值大于<i>θ</i>,表示像素点对灰度差异较大;否则,表示像素点对灰度接近。因此,<i>θ</i>可选择最大灰度值的5%以内。</p>
                </div>
                <div class="p1">
                    <p id="58">相似度比较描述子的编码为:</p>
                </div>
                <div class="p1">
                    <p id="59"><i><b>T</b></i><sub><i>m</i></sub>=[<i>τ</i>′(<i>x</i><sub>1</sub>,<i>y</i><sub>1</sub>),<i>τ</i>′(<i>x</i><sub>2</sub>,<i>y</i><sub>2</sub>),…,<i>τ</i>′(<i>x</i><sub><i>n</i></sub>,<i>y</i><sub><i>n</i></sub>)]      (5)</p>
                </div>
                <div class="p1">
                    <p id="60">其中: <i><b>T</b></i><sub><i>m</i></sub>编码值描述了随机点对中两个像素点灰度相似度的差异,即当相似度差异较大时,编码为1; 否则编码为0。<i><b>T</b></i><sub><i>m</i></sub>能够实现对<i><b>T</b></i><sub><i>s</i></sub>的信息补充,克服光照变化等干扰引起的编码不稳定。</p>
                </div>
                <div class="p1">
                    <p id="61">特征点的相似性采用汉明距离进行度量。设两幅图像中待匹配特征点邻域内点对灰度大小比较的编码分别为<i><b>T</b></i><sub><i>s</i></sub>和<i><b>T</b></i><sub><i>s</i></sub>′,二者汉明距离为<i>D</i><sub><i>s</i></sub>,灰度相似度比较的编码分别为<i><b>T</b></i><sub><i>m</i></sub>和<i><b>T</b></i><sub><i>m</i></sub>′,二者汉明距离为<i>D</i><sub><i>m</i></sub>,其中,汉明距离可采用两个编码按位异或求和进行计算:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>t</mi></mstyle><msub><mrow></mrow><mrow><mi>s</mi><mi>i</mi></mrow></msub><mo>⊕</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>i</mi></mrow></msub><mo>´</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>t</mi></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mi>i</mi></mrow></msub><mo>⊕</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>i</mi></mrow></msub><mo>´</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中: <i>t</i><sub><i>si</i></sub>、<i>t</i><sub><i>si</i></sub>′分别为<i><b>T</b></i><sub><i>s</i></sub>和<i><b>T</b></i><sub><i>s</i></sub>′的第<i>i</i>位数值,<i>t</i><sub><i>mi</i></sub>、<i>t</i><sub><i>mi</i></sub>′分别为<i><b>T</b></i><sub><i>m</i></sub>和<i><b>T</b></i><sub><i>m</i></sub>′的第<i>i</i>位数值。将<i>D</i><sub><i>s</i></sub>与<i>D</i><sub><i>m</i></sub>两个汉明距离进行融合可求得待匹配特征点的相似性度量。常用的融合方法可采用加权求和的方式,即:</p>
                </div>
                <div class="p1">
                    <p id="65"><i>D</i>=<i>αD</i><sub><i>s</i></sub>+(1-<i>α</i>)<i>D</i><sub><i>m</i></sub>      (8)</p>
                </div>
                <div class="p1">
                    <p id="66">其中<i>D</i>为融合后的特征点相似性度量值。这种融合方式虽然简单,但由于缺少先验知识,融合系数不易确定。为此,提出具有自适应性的加权融合方式,即设:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>α</i>=<i>D</i><sub><i>m</i></sub>/(<i>D</i><sub><i>s</i></sub>+<i>D</i><sub><i>m</i></sub>)      (9)</p>
                </div>
                <div class="p1">
                    <p id="68">则:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>D</i>=2<i>D</i><sub><i>s</i></sub><i>D</i><sub><i>m</i></sub>/(<i>D</i><sub><i>s</i></sub>+<i>D</i><sub><i>m</i></sub>)      (10)</p>
                </div>
                <div class="p1">
                    <p id="70">采用上述距离融合方式,较小的编码距离可获得较大的权重,而较大的编码距离则获得较小的权重,由此可有效均衡两种编码方式的作用强度,有利于提高特征点相似性度量的合理性。</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag">2 实验结果分析</h3>
                <div class="p1">
                    <p id="72">为了验证改进方法的有效性,将本文方法与传统<i>ORB</i>特征点匹配方法以及固定权值特征点匹配方法进行实验对比分析。其中,传统<i>ORB</i>特征点匹配方法以及固定权值特征点匹配方法均采用<i>FAST</i>特征点提取方法,本文方法采用<i>SURF</i>特征点提取方法。为了验证融合随机点对灰度相似度比较信息的作用,将固定权值特征点匹配方法的权值<i>α</i>选为0.75,相似度比较阈值<i>θ</i>=5。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">2.1 <b>尺度变化图像特征点匹配</b></h4>
                <div class="p1">
                    <p id="74">图1为三张具有尺度大小变化的图像。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 尺度变化图像" src="Detail/GetImg?filename=images/JSJY201911010_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 尺度变化图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 1 <i>Images with scale variance</i></p>

                </div>
                <div class="p1">
                    <p id="76">图1中,图像1与图像3的特征点匹配结果如图2所示。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 尺度变化图像特征点匹配结果" src="Detail/GetImg?filename=images/JSJY201911010_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 尺度变化图像特征点匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Feature point matching results of images with scale variance</i></p>

                </div>
                <div class="p1">
                    <p id="78">由图2可见,传统<i>ORB</i>特征点匹配方法以及固定权值特征点匹配方法均有较多明显的错误匹配点对,而本文特征点匹配方法对图像尺度的变化具有更好的鲁棒性,因此匹配正确率较高。</p>
                </div>
                <div class="p1">
                    <p id="79">图像1中的特征点分别与图像2和图像3中的特征点进行匹配,所得特征点正确匹配率如表1所示。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表</b>1 <b>尺度变化图像的特征点正确匹配率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Correct matching rate of</i><i>feature points for scale variance images</i></p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="2"><br />特征点正确匹配率/%</td></tr><tr><td><br />图像1与图像2</td><td>图像1与图像3</td></tr><tr><td><br />传统<i>ORB</i>匹配</td><td>75</td><td>70</td></tr><tr><td><br />固定权值匹配</td><td>90</td><td>80</td></tr><tr><td><br />本文方法</td><td>95</td><td>90</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="81">由表1可见,随着缩放尺度变化的加大,三种方法的特征点匹配正确率均有所下降,固定权值特征点匹配方法由于融入了灰度相似度比较信息,因此特征点正确匹配率较传统<i>ORB</i>方法有所提升,而本文方法由于采用尺度不变的<i>SURF</i>算法进行特征点提取,因此特征点正确匹配率能够得到进一步提升。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">2.2 <b>模糊图像特征点匹配</b></h4>
                <div class="p1">
                    <p id="83">图3为测试集<i>bikes</i>(<i>blur</i>)组中的4张具有不同模糊程度的测试图像,图像的模糊程度逐渐增加。</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 模糊图像" src="Detail/GetImg?filename=images/JSJY201911010_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 模糊图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Blurred test images</i></p>

                </div>
                <div class="p1">
                    <p id="85">采用三种方法对<i>bikes</i>1和<i>bikes</i>2进行特征点匹配,所得结果如图4所示。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 模糊图像特征点匹配结果" src="Detail/GetImg?filename=images/JSJY201911010_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 模糊图像特征点匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Feature point matching results of blurred images</i></p>

                </div>
                <div class="p1">
                    <p id="87">从匹配结果可见,本文方法的特征点正确匹配率仍然高于传统<i>ORB</i>匹配方法和固定权值匹配方法。各图像间的特征点正确匹配率如表2所示。</p>
                </div>
                <div class="area_img" id="88">
                    <p class="img_tit"><b>表</b>2 <b>模糊图像的特征点正确匹配率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Correct matching rate of</i><i>feature points for blurred images</i></p>
                    <p class="img_note"></p>
                    <table id="88" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="3"><br />特征点正确匹配率/%</td></tr><tr><td><br /><i>bikes</i>1与<i>bikes</i>2</td><td><i>bikes</i>1与<i>bikes</i>3</td><td><i>bikes</i>1与<i>bikes</i>4</td></tr><tr><td><br />传统<i>ORB</i>匹配</td><td>86</td><td>82</td><td>80</td></tr><tr><td><br />固定权值匹配</td><td>90</td><td>86</td><td>84</td></tr><tr><td><br />本文方法</td><td>94</td><td>90</td><td>86</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="89">由表2可见,随着模糊程度的增加,特征点的正确匹配率也逐渐减小,但本文方法采用自适应加权的方式实现了特征点邻域内点对的灰度大小比较和相似度比较的距离融合,并以此衡量特征点的相似性,因此具有更高的特征点正确匹配率,也说明本文方法对模糊图像具有一定的适应性。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">2.3 <b>光照变化图像特征点匹配</b></h4>
                <div class="p1">
                    <p id="91">图5为测试集<i>leuven</i>(<i>light</i>)组中光照变化的五张图像,光照强度逐渐变弱。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 光照变化测试图像" src="Detail/GetImg?filename=images/JSJY201911010_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 光照变化测试图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Test images with illumination variance</i></p>

                </div>
                <div class="p1">
                    <p id="93">图6 给出了<i>leuven</i>1与<i>leuven</i>2中的特征点匹配结果。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 光照变化图像特征点匹配结果" src="Detail/GetImg?filename=images/JSJY201911010_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 光照变化图像特征点匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Feature point matching results of images with illumination variance</i></p>

                </div>
                <div class="p1">
                    <p id="95">从图6可见,虽然两幅图像的光照差异并不大,但光照变化对匹配结果仍然有着一定的影响:传统的<i>ORB</i>方法对光照变化比较敏感,误匹配的特征点数量最多;固定权值匹配方法通过融入点对的相似度比较信息能够在一定程度上降低光照变化对匹配结果的影响;本文方法特征点的正确匹配率最高,对光照变化具有更好的适应能力。</p>
                </div>
                <div class="p1">
                    <p id="96">上述图像间的特征点匹配正确率如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="97">由表3可见,随着光照变化程度的增加,三种方法的正确匹配率均有所下降,但本文方法具有更高的正确匹配率,匹配结果对光照变化具有更好的鲁棒性。</p>
                </div>
                <div class="area_img" id="98">
                    <p class="img_tit"><b>表</b>3 <b>光照变化图像的特征点正确匹配率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Correct matching rate of feature points</i><i>for illumination variance images</i></p>
                    <p class="img_note"></p>
                    <table id="98" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="4"><br />特征点正确匹配率/%</td></tr><tr><td><br /><i>leuven</i> 1与<br /><i>leuven</i> 2</td><td><i>leuven</i> 1与<br /><i>leuven</i> 3</td><td><i>leuven</i> 1与<br /><i>leuven</i> 4</td><td><i>leuven</i> 1与<br /><i>leuven</i> 5</td></tr><tr><td><br />传统<i>ORB</i>匹配</td><td>95</td><td>94</td><td>92</td><td>87</td></tr><tr><td><br />固定权值匹配</td><td>97</td><td>95</td><td>94</td><td>91</td></tr><tr><td><br />本文方法</td><td>99</td><td>96</td><td>95</td><td>93</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="99" name="99" class="anchor-tag">3 基于特征点匹配的图像拼接</h3>
                <div class="p1">
                    <p id="100">图7为测试集(<i>input</i>- 42-<i>data</i>)中的两张图像。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 测试图像" src="Detail/GetImg?filename=images/JSJY201911010_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 测试图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Test images</i></p>

                </div>
                <div class="p1">
                    <p id="102">采用上述三种匹配方法对图7中的两张图像进行特征点匹配,随着特征点数量的增加,特征点匹配正确率的变化曲线如图8所示。</p>
                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 特征点匹配正确率变化曲线" src="Detail/GetImg?filename=images/JSJY201911010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 特征点匹配正确率变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Curve of correct matching rate of feature points</i></p>

                </div>
                <div class="p1">
                    <p id="104">由图8可见,随着特征点数量的增加,三种特征点匹配方法的正确匹配率逐渐下降,但本文方法的匹配正确率明显高于其他两种对比方法。当特征点数量为160时,三种方法的特征点匹配结果如图9所示。</p>
                </div>
                <div class="p1">
                    <p id="105">图9中,本文方法的特征点能够实现特征点的全部正确匹配,固定权值匹配方法有少量特征点发生匹配错误,而<i>ORB</i>匹配方法错误匹配特征点数量较多。基于上述匹配结果,实现的图像拼接效果如图10所示。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 三种方法的特征点匹配结果" src="Detail/GetImg?filename=images/JSJY201911010_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 三种方法的特征点匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 9 <i>Feature point matching results of three methods</i></p>

                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 三种方法的图像拼接结果" src="Detail/GetImg?filename=images/JSJY201911010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 三种方法的图像拼接结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 10 <i>Image stitching results of three methods</i></p>

                </div>
                <div class="p1">
                    <p id="108">上述拼接结果的局部放大图如图11所示。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 图像拼接结果放大图" src="Detail/GetImg?filename=images/JSJY201911010_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 图像拼接结果放大图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 11 <i>Magnification of image stitching results</i></p>

                </div>
                <div class="p1">
                    <p id="110">从图10和图11可见:基于<i>ORB</i>匹配的拼接图像,由于特征点正确匹配率低,其拼接效果较差,有明显的几何错位; 固定权值匹配方法的正确率较高,所得拼接效果也得到改善,虽仍有几何错位,但并不太明显; 而本文方法能够实现全部特征点的正确匹配,因此拼接效果最好,无明显的几何错位。</p>
                </div>
                <div class="p1">
                    <p id="111">进一步采用基于距离加权的亮度融合方法,能够消除因亮度差异产生的拼接缝隙,得到最终的图像拼接结果如图12所示。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911010_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 本文方法最终图像拼接结果" src="Detail/GetImg?filename=images/JSJY201911010_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 本文方法最终图像拼接结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911010_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 12 <i>Final image stitching result of the proposed method</i></p>

                </div>
                <div class="p1">
                    <p id="113">由此可见,本文方法能够有效提高图像特征点的正确匹配率,因此可改善图像拼接质量,抑制拼接产生的几何错位现象,提高图像拼接性能。</p>
                </div>
                <h3 id="114" name="114" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="115">本文针对传统<i>ORB</i>算法特征点匹配错误率较高、易受光照变化等干扰的问题,提出了改进的特征点匹配方法。采用<i>SURF</i>算法提取特征点,在<i>BRIEF</i>特征描述子中,融合了特征点邻域内随机点对的灰度相似度比较信息,并提出了灰度大小比较和相似度比较的编码距离自适应加权融合方式,提高了特征点匹配的正确率。实验结果表明,本文方法对图像尺度变化、光照变化以及模糊变化等均具有良好的适应能力,能够改善图像拼接的效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="124">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CGJS201811017&amp;v=MDIyMjRmYkc0SDluTnJvOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnJ6TkppckI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 左川,庞春江.基于改进ORB的抗视角变换快速图像匹配算法[J].传感技术学报,2018,31(11):98-104.(ZUO C,PANG C J.Fast image matching algorithm based on improved ORB for anti-viewing transformation[J].Journal of Sensing Technology,2018,31(11):98-104.)
                            </a>
                        </p>
                        <p id="126">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201705042&amp;v=MTEwMjhadVpzRnlublZyek5MeUhNZDdHNEg5Yk1xbzlCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 董强,刘晶红,周前飞.用于遥感图像拼接的改进SURF算法[J].吉林大学学报(工学版),2017,47(5):1644-1652.(DONG Q,LIU J H,ZHOU Q F.Improved SURF algorithm for remote sensing image stitching[J].Journal of Jilin University (Engineering Edition),2017,47(5):1644-1652.)
                            </a>
                        </p>
                        <p id="128">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analyzing the performance-hardware trade-off of an ASIP-based sift feature extraction">

                                <b>[3]</b> MENTZER N,PAYA-VAYA G,BLUME H.Analyzing the performance-hardware trade-off of an ASIP-based sift feature extraction[J].Journal of Signal Processing Systems,2016,85(1):83-99.
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HZLG201604007&amp;v=MDkzNTlOTFRmSGFiRzRIOWZNcTQ5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5Wcno=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 邹承明,侯小碧,马静.基于几何学图像配准的SIFT图像拼接算法[J].华中科技大学学报(自然科学版),2016,44(4):32-36.(ZOU C M,HOU X B,MA J.SIFT image stitching algorithm based on geometric image registration[J].Journal of Huazhong University of Science and Technology (Natural Science Edition),2016,44(4):32-36.)
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201603013&amp;v=MjY1MjRaTEc0SDlmTXJJOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnJ6Tk1qWFM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 张勇,王志锋,马文.基于改进SIFT特征点匹配的图像拼接算法研究[J].微电子学与计算机,2016,33(3):60-64.(ZHANG Y,WANG Z F,MA W.Research on image stitching algorithm based on improved SIFT feature point matching[J].Microelectronics &amp; Computer,2016,33(3):60-64.)
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YNDZ201606003&amp;v=MjM0NjVMRzRIOWZNcVk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcnpOUENQUGQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 曹君宇,周浩,高志山,等.基于SURF的图像拼接过程中配准算法的改进[J].云南大学学报(自然科学版),2016,38(6):845-852.(CAO J Y,ZHOU H,GAO Z S,et al.Improvement of registration algorithm in image stitching process based on SURF[J].Journal of Yunnan University (Natural Science Edition),2016,38(6):845-852.)
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YJYS201808009&amp;v=MDk0MzM0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnJ6TlBDZlNmYkc0SDluTXA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 张博,韩广良.基于Mask R-CNN的ORB去误匹配方法[J].液晶与显示,2018,33(8):69-75.(ZHANG B,HAN G L.Based on Mask R-CNN ORB error matching method[J].LCD and Display,2018,33(8):69-75.)
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YJYS201806011&amp;v=MDA2MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcnpOUENmU2ZiRzRIOW5NcVk5RVpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 王健,于鸣,任洪娥.一种用于图像拼接的改进ORB算法[J].液晶与显示,2018,33(6):73-80.(WANG J,YU M,REN H E.An improved ORB algorithm for image stitching[J].LCD and Display,2018,33(6):73-80.)
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A miniature binocular endoscope with local feature matching and stereo matching for 3D measurement and 3D reconstruction">

                                <b>[9]</b> WANG D,LIU H,CHENG X.A miniature binocular endoscope with local feature matching and stereo matching for 3D measurement and 3D reconstruction[J].Sensors,2018,18(7):2243-2250.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Descriptor matching for a discrete spherical image with a convolutional neural network">

                                <b>[10]</b> SHAN Y,LI S.Descriptor matching for a discrete spherical image with a convolutional neural network[J].IEEE Access,2018,6:20748-20755.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust robot pose estimation for challenging scenes with an RGB-D camera">

                                <b>[11]</b> YU H,FU Q,YANG Z,et al.Robust robot pose estimation for challenging scenes with an RGB-D camera[J].IEEE Sensors Journal,2019,19(6):2217-2229.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fundamental principles on learning new features for effective dense matching">

                                <b>[12]</b> ZHANG F,WAH B W.Fundamental principles on learning new features for effective dense matching[J].IEEE Transactions on Image Processing,2017,27(2):822-836.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution 360video foveated stitching for real-time VR">

                                <b>[13]</b> LEE W T,CHEN H I,CHEN M S,et al.High-resolution 360 video foveated stitching for real-time VR[J].Computer Graphics Forum,2017,36(7):115-123.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reconfigurable hardware architecture for faster descriptor extraction in SURF">

                                <b>[14]</b> KIM Y,JUNG H.Reconfigurable hardware architecture for faster descriptor extraction in surf[J].Electronics Letters,2018,54(4):210-212.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature extraction and power quality disturbances classification using smart meters signals">

                                <b>[15]</b> BORGES F A S,FERNANDES R A S,SILVA I N,et al.Feature extraction and power quality disturbances classification using smart meters signals[J].IEEE Transactions on Industrial Informatics,2017,12(2):824-833.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiple view oriented matching algorithm for people reidentification">

                                <b>[16]</b> GARCIA J,GARDEL A,BRAVO I,et al.Multiple view oriented matching algorithm for people reidentification[J].IEEE Transactions on Industrial Informatics,2017,10(3):1841-1851.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD9498F21FD7ED631A92E1C94D7567D168&amp;v=MDA2ODFCVXdmN1hnUkx5WENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhicTZ3NjA9Tmo3QmFycThGOW02clk0ekVPeDZlSG82em1jYTZFcDhPM2JtMg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> SRINIVASA K G,DEVI B N S.GPU based n-gram string matching algorithm with score table approach for string searching in many documents[J].Journal of the Institution of Engineers,2017,98(2):1-10.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201712017&amp;v=MTA2Nzdyek5MamZTYmJHNEg5Yk5yWTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 曾庆化,陈艳,王云舒,等.一种基于ORB的快速大视角图像匹配算法[J].控制与决策,2017,32(12):2233-2239.(ZENG Q H,CHEN Y,WANG Y S,et al.A fast large angle view image matching algorithm based on ORB[J].Control and Decision,2017,32(12):2233-2239.)
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201706014&amp;v=MDk1NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublZyek5MeXZTZExHNEg5Yk1xWTlFWUk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 杜承垚,袁景凌,陈旻骋,等.GPU加速与L-ORB特征提取的全景视频实时拼接[J].计算机研究与发展,2017,54(6):1316-1325.(DU C Y,YUAN J L,CHEN M C,et al.GPU acceleration and L-ORB feature extraction for panoramic video real-time stitching[J].Journal of Computer Research and Development,2017,54(6):1316-1325.)
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911010" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911010&amp;v=MTYxNTBqTnJvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnJ6Tkx6N0JkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
