<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136662062346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907012%26RESULT%3d1%26SIGN%3dUFDruFwfN%252bT6hxPuG%252fzYk5mTdio%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907012&amp;v=Mjg0OTVuVkx2SUx6N0JkN0c0SDlqTXFJOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#73" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="1.1 &lt;b&gt;传统情感分类方法&lt;/b&gt;">1.1 <b>传统情感分类方法</b></a></li>
                                                <li><a href="#86" data-title="1.2 &lt;b&gt;基于深度学习的情感分类方法&lt;/b&gt;">1.2 <b>基于深度学习的情感分类方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="2 模型介绍 ">2 模型介绍</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="2.1 &lt;b&gt;词嵌入向量和&lt;/b&gt;N-grams&lt;b&gt;特征&lt;/b&gt;">2.1 <b>词嵌入向量和</b>N-grams<b>特征</b></a></li>
                                                <li><a href="#100" data-title="2.2 &lt;b&gt;长短期记忆网络&lt;/b&gt;">2.2 <b>长短期记忆网络</b></a></li>
                                                <li><a href="#108" data-title="2.3 &lt;b&gt;基于卷积神经网络的文本情感分类&lt;/b&gt;">2.3 <b>基于卷积神经网络的文本情感分类</b></a></li>
                                                <li><a href="#129" data-title="2.4 &lt;b&gt;基于一维卷积的混合网络模型&lt;/b&gt;">2.4 <b>基于一维卷积的混合网络模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#155" data-title="3 实验及结果分析 ">3 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#157" data-title="3.1 &lt;b&gt;情感分类数据集&lt;/b&gt;">3.1 <b>情感分类数据集</b></a></li>
                                                <li><a href="#161" data-title="3.2 &lt;b&gt;参数设置&lt;/b&gt;">3.2 <b>参数设置</b></a></li>
                                                <li><a href="#163" data-title="3.3 &lt;b&gt;结果对比&lt;/b&gt;">3.3 <b>结果对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#175" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#110" data-title="图1 &lt;i&gt;CNN&lt;/i&gt;-&lt;i&gt;non&lt;/i&gt;-&lt;i&gt;static&lt;/i&gt;网络结构">图1 <i>CNN</i>-<i>non</i>-<i>static</i>网络结构</a></li>
                                                <li><a href="#131" data-title="图2 1&lt;i&gt;D&lt;/i&gt;-&lt;i&gt;CLSTM&lt;/i&gt;网络结构">图2 1<i>D</i>-<i>CLSTM</i>网络结构</a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;数据集统计信息&lt;/b&gt;"><b>表</b>1 <b>数据集统计信息</b></a></li>
                                                <li><a href="#165" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同算法的分类准确度对比&lt;/b&gt;"><b>表</b>2 <b>不同算法的分类准确度对比</b></a></li>
                                                <li><a href="#171" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;两种算法的分类准确度对比&lt;/b&gt;"><b>表</b>3 <b>两种算法的分类准确度对比</b></a></li>
                                                <li><a href="#173" data-title="&lt;b&gt;表&lt;/b&gt;4 3&lt;b&gt;种算法的分类准确度对比&lt;/b&gt;"><b>表</b>4 3<b>种算法的分类准确度对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="228">


                                    <a id="bibliography_1" title=" 周立柱, 贺宇凯, 王建勇.情感分析研究综述[J].计算机应用, 2008, 28 (11) :2725-2728. (ZHOU L Z, HE Y K, WANG J Y.Survey on research of sentiment analysis [J].Journal of Computer Applications, 2008, 28 (11) :2725-2728.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200811000&amp;v=MjQwMTBuTnJvOUZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVkx2SUx6N0JkN0c0SHQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         周立柱, 贺宇凯, 王建勇.情感分析研究综述[J].计算机应用, 2008, 28 (11) :2725-2728. (ZHOU L Z, HE Y K, WANG J Y.Survey on research of sentiment analysis [J].Journal of Computer Applications, 2008, 28 (11) :2725-2728.) 
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_2" title=" 赵妍妍, 秦兵, 刘挺.文本情感分析[J].软件学报, 2010, 21 (8) :1834-1848. (ZHAO Y Y, QIN B, LIU T.Sentiment analysis [J].Journal of Software, 2010, 21 (8) :1834-1848.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201008009&amp;v=Mjk4MDhadVpzRnkvblZMdklOeWZUYkxHNEg5SE1wNDlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         赵妍妍, 秦兵, 刘挺.文本情感分析[J].软件学报, 2010, 21 (8) :1834-1848. (ZHAO Y Y, QIN B, LIU T.Sentiment analysis [J].Journal of Software, 2010, 21 (8) :1834-1848.) 
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_3" title=" ZHANG Y, WALLACE B.A sensitivity analysis of (and practitioners&#39; guide to) convolutional neural networks for sentence classification[EB/OL]. (2016- 04- 06) [2018- 06- 07].https://arxiv.org/abs/1510.03820." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A sensitivity analysis of (and practitioners&amp;#39;&amp;#39;guide to)convolutional neural networks for sentence classification">
                                        <b>[3]</b>
                                         ZHANG Y, WALLACE B.A sensitivity analysis of (and practitioners&#39; guide to) convolutional neural networks for sentence classification[EB/OL]. (2016- 04- 06) [2018- 06- 07].https://arxiv.org/abs/1510.03820.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_4" title=" KIM Y.Convolutional neural networks for sentence classification [EB/OL]. (2014- 09- 03) [2018- 06- 01].https://arxiv.org/abs/1408.5882." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Sentence Classification">
                                        <b>[4]</b>
                                         KIM Y.Convolutional neural networks for sentence classification [EB/OL]. (2014- 09- 03) [2018- 06- 01].https://arxiv.org/abs/1408.5882.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_5" title=" ZHANG L, WANG S, LIU B.Deep learning for sentiment analysis:a survey[J].Wiley Interdisciplinary Reviews:Data Mining and Knowledge Discovery, 2018, 8 (4) :e1253." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD26FFFB5EBB77055235F755653217FCAF&amp;v=Mjc4MTlrcVJFM2VMWGlOc3ZwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxNHhLZz1OaWZjYXJHK2FLZTYzWW93RnBrSUMzdzh5aFFRNzBsNlRYcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         ZHANG L, WANG S, LIU B.Deep learning for sentiment analysis:a survey[J].Wiley Interdisciplinary Reviews:Data Mining and Knowledge Discovery, 2018, 8 (4) :e1253.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_6" title=" KIM S-M, HOVY E.Extracting opinions, opinion holders, and topics expressed in online news media text[C]// Proceedings of the 2006 Workshop on Sentiment and Subjectivity in Text.Stroudsburg, PA:Association for Computational Linguistics, 2006:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting opinions, opinion holders, and topics expressed in online newsmedia text">
                                        <b>[6]</b>
                                         KIM S-M, HOVY E.Extracting opinions, opinion holders, and topics expressed in online news media text[C]// Proceedings of the 2006 Workshop on Sentiment and Subjectivity in Text.Stroudsburg, PA:Association for Computational Linguistics, 2006:1-8.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_7" title=" TURNEY P D.Thumbs up or thumbs down?:semantic orientation applied to unsupervised classification of reviews[C]// Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2002:417-424." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Thumbs up or thumbs down?Semantic orientation applied to unsupervised classification of reviews">
                                        <b>[7]</b>
                                         TURNEY P D.Thumbs up or thumbs down?:semantic orientation applied to unsupervised classification of reviews[C]// Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2002:417-424.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_8" title=" HU M, LIU B.Mining and summarizing customer reviews[C]// Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2004:168-177." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mining and summarizing customer reviews">
                                        <b>[8]</b>
                                         HU M, LIU B.Mining and summarizing customer reviews[C]// Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2004:168-177.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     PANG B, LEE L, VAITHYANATHAN S.Thumbs up?:sentiment classification using machine learning techniques[C]// Proceedings of the ACL- 02 Conference on Empirical Methods in Natural Language Processing-Volume 10.Stroudsburg, PA:Association for Computational Linguistics, 2002:79-86.</a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_10" title=" MOHAMMAD S M, KIRITCHENKO S, ZHU X.NRC-Canada:building the state-of-the-art in sentiment analysis of tweets[EB/OL]. (2013- 08- 28) [2018- 07- 02].https://arxiv.org/abs/1308.6242." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NRC-Canada:building the state-of-the-art in sentiment analysis of tweets">
                                        <b>[10]</b>
                                         MOHAMMAD S M, KIRITCHENKO S, ZHU X.NRC-Canada:building the state-of-the-art in sentiment analysis of tweets[EB/OL]. (2013- 08- 28) [2018- 07- 02].https://arxiv.org/abs/1308.6242.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_11" title=" KIM S-M, HOVY E.Automatic identification of pro and con reasons in online reviews[C]// Proceedings of the 2006 COLING/ACL on Main Conference Poster Sessions.Stroudsburg, PA:Association for Computational Linguistics, 2006:483-490." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic identification of pro and con reasons in online reviews">
                                        <b>[11]</b>
                                         KIM S-M, HOVY E.Automatic identification of pro and con reasons in online reviews[C]// Proceedings of the 2006 COLING/ACL on Main Conference Poster Sessions.Stroudsburg, PA:Association for Computational Linguistics, 2006:483-490.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_12" title=" MEDHAT W, HASSAN A, KORASHY H.Sentiment analysis algorithms and applications:a survey [J].Ain Shams Engineering Journal, 2014, 5 (4) :1093-1113." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700305623&amp;v=MjY2NTU9TmlmT2ZiSzhIdGZOcUk5Rlorc0tDbjQ2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VGJoSQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         MEDHAT W, HASSAN A, KORASHY H.Sentiment analysis algorithms and applications:a survey [J].Ain Shams Engineering Journal, 2014, 5 (4) :1093-1113.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     BENGIO Y, DUCHARME R, VINCENT P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3:1137-1155.</a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_14" title=" MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[C]// NIPS&#39;13:Proceedings of the 26th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2013:3111-3119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">
                                        <b>[14]</b>
                                         MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[C]// NIPS&#39;13:Proceedings of the 26th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2013:3111-3119.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_15" title=" PENNINGTON J, SOCHER R, MANNING C.GloVe:global vectors for word representation[C]// Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2014:1532-1543." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Glove:Global vectors for word representation">
                                        <b>[15]</b>
                                         PENNINGTON J, SOCHER R, MANNING C.GloVe:global vectors for word representation[C]// Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2014:1532-1543.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_16" title=" SOCHER R, PERELYGIN A, WU J, et al.Recursive deep models for semantic compositionality over a sentiment treebank[C]// Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2013:1631-1642." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recursive deep models for semantic compositionality over a sentiment treebank">
                                        <b>[16]</b>
                                         SOCHER R, PERELYGIN A, WU J, et al.Recursive deep models for semantic compositionality over a sentiment treebank[C]// Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2013:1631-1642.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_17" title=" SOCHER R, PENNINGTON J, HUANG E H, et al.Semi-supervised recursive autoencoders for predicting sentiment distributions[C]// Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2011:151-161." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised recursive autoencoders for predicting sentiment distributions">
                                        <b>[17]</b>
                                         SOCHER R, PENNINGTON J, HUANG E H, et al.Semi-supervised recursive autoencoders for predicting sentiment distributions[C]// Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2011:151-161.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_18" title=" QIAN Q, TIAN B, HUANG M, et al.Learning tag embeddings and tag-specific composition functions in recursive neural network[C]// Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2015:1365-1374." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning tag embeddings and tag-specific composition functions in recursive neural network">
                                        <b>[18]</b>
                                         QIAN Q, TIAN B, HUANG M, et al.Learning tag embeddings and tag-specific composition functions in recursive neural network[C]// Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2015:1365-1374.
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_19" title=" TAI K S, SOCHER R, MANNING C D.Improved semantic representations from tree-structured long short-term memory networks[EB/OL]. (2015- 05- 30) [2018- 08- 10].https://arxiv.org/abs/1503.00075." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved semantic representations from tree-structured long short-term memory networks">
                                        <b>[19]</b>
                                         TAI K S, SOCHER R, MANNING C D.Improved semantic representations from tree-structured long short-term memory networks[EB/OL]. (2015- 05- 30) [2018- 08- 10].https://arxiv.org/abs/1503.00075.
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_20" title=" IRSOY O, CARDIE C.Opinion mining with deep recurrent neural networks[C]// Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2014:720-728." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Opinion Mining with Deep Recurrent Neural Networks">
                                        <b>[20]</b>
                                         IRSOY O, CARDIE C.Opinion mining with deep recurrent neural networks[C]// Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2014:720-728.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_21" title=" LIU P, QIU X, HUANG X.Recurrent neural network for text classification with multi-task learning[EB/OL]. (2016- 05- 17) [2018- 08- 01].https://arxiv.org/abs/1605.05101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent Neural Network for Text Classification with Multi-Task Learning">
                                        <b>[21]</b>
                                         LIU P, QIU X, HUANG X.Recurrent neural network for text classification with multi-task learning[EB/OL]. (2016- 05- 17) [2018- 08- 01].https://arxiv.org/abs/1605.05101.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_22" title=" QIAN Q, HUANG M, LEI J, et al.Linguistically regularized LSTMs for sentiment classification [EB/OL]. (2017- 04- 25) [2018- 08- 15].https://arxiv.org/abs/1611.03949." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Linguistically regularized LSTMs for sentiment classification">
                                        <b>[22]</b>
                                         QIAN Q, HUANG M, LEI J, et al.Linguistically regularized LSTMs for sentiment classification [EB/OL]. (2017- 04- 25) [2018- 08- 15].https://arxiv.org/abs/1611.03949.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_23" title=" KALCHBRENNER N, GREFENSTETTE E, BLUNSOM P.A convolutional neural network for modelling sentences [EB/OL]. (2014- 04- 08) [2018- 07- 16].https://arxiv.org/abs/1404.2188." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Convolutional Neural Network for Modelling Sentences">
                                        <b>[23]</b>
                                         KALCHBRENNER N, GREFENSTETTE E, BLUNSOM P.A convolutional neural network for modelling sentences [EB/OL]. (2014- 04- 08) [2018- 07- 16].https://arxiv.org/abs/1404.2188.
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_24" title=" ZHOU C, SUN C, LIU Z, et al.A C-LSTM neural network for text classification[EB/OL]. (2015- 11- 30) [2018- 08- 22].https://arxiv.org/abs/1511.08630." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A C-LSTM neural network for text classification">
                                        <b>[24]</b>
                                         ZHOU C, SUN C, LIU Z, et al.A C-LSTM neural network for text classification[EB/OL]. (2015- 11- 30) [2018- 08- 22].https://arxiv.org/abs/1511.08630.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_25" title=" COLLOBERT R, WESTON J, BOTTOU L, et al.Natural lan-guage processing (almost) from scratch[J].Journal of Machine Learning Research, 2011, 12:2493-2537." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Natural language processing (almost) from scratch">
                                        <b>[25]</b>
                                         COLLOBERT R, WESTON J, BOTTOU L, et al.Natural lan-guage processing (almost) from scratch[J].Journal of Machine Learning Research, 2011, 12:2493-2537.
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_26" title=" HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTYxOTdvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1RiaEk9TmlmSlpiSzlIdGpNcQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_27" title=" GRAVES A, JAITLY N, MOHAMED A.Hybrid speech recognition with deep bidirectional LSTM[C]// Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2013:273-278." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hybrid speech recognition with deep bidirectional LSTM">
                                        <b>[27]</b>
                                         GRAVES A, JAITLY N, MOHAMED A.Hybrid speech recognition with deep bidirectional LSTM[C]// Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2013:273-278.
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_28" >
                                        <b>[28]</b>
                                     MIKOLOV T, CHEN K, CORRADO G, et al.Efficient estimation of word representations in vector space[EB/OL]. (2013- 09- 07) [2018- 09- 02].https://arxiv.org/abs/1301.3781.</a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_29" title=" LIU B.Sentiment Analysis and Opinion Mining[M].San Rafael, CA:Morgan and Claypool Publishers, 2012:1-167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sentiment Analysis and Opinion Mining">
                                        <b>[29]</b>
                                         LIU B.Sentiment Analysis and Opinion Mining[M].San Rafael, CA:Morgan and Claypool Publishers, 2012:1-167.
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_30" title=" McCANN B, BRADBURY J, XIONG C, et al.Learned in translation:contextualized word vectors[C]// NIPS 2017:Proceedings of the 31st Annual Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2017:6297-6308." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learned in translation:contextualized word vectors">
                                        <b>[30]</b>
                                         McCANN B, BRADBURY J, XIONG C, et al.Learned in translation:contextualized word vectors[C]// NIPS 2017:Proceedings of the 31st Annual Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2017:6297-6308.
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_31" title=" PETERS M E, NEUMANN M, IYYER M et al.Deep contextualized word representations[EB/OL]. (2018- 03- 22) [2018- 10- 21].https://arxiv.org/abs/1802.05365." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep contextualized word representations">
                                        <b>[31]</b>
                                         PETERS M E, NEUMANN M, IYYER M et al.Deep contextualized word representations[EB/OL]. (2018- 03- 22) [2018- 10- 21].https://arxiv.org/abs/1802.05365.
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_32" title=" HOWARD J, RUDER S.Universal language model fine-tuning for text classification[C]// Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2018:328-339." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Universal language model fine-tuning for text classification">
                                        <b>[32]</b>
                                         HOWARD J, RUDER S.Universal language model fine-tuning for text classification[C]// Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2018:328-339.
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_33" title=" RADFORD A, NARASIMHAN K, SALIMANS T, et al.Improving language understanding by generative pre-training[EB/OL]. (2018- 06- 11) [2018- 10- 22].https://blog.openai.com/language-unsupervised/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving language understanding by generative pre-training">
                                        <b>[33]</b>
                                         RADFORD A, NARASIMHAN K, SALIMANS T, et al.Improving language understanding by generative pre-training[EB/OL]. (2018- 06- 11) [2018- 10- 22].https://blog.openai.com/language-unsupervised/.
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_34" title=" VASWANI A, SHAZEER N, PARMAR N, et al.Attention is all you need[C]// NIPS 2017:Proceedings of the 31st Annual Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2017:5998-6008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention is All You Need">
                                        <b>[34]</b>
                                         VASWANI A, SHAZEER N, PARMAR N, et al.Attention is all you need[C]// NIPS 2017:Proceedings of the 31st Annual Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2017:5998-6008.
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_35" title=" DEVLIN J, CHANG M-W, LEE K, et al.BERT:pre-training of deep bidirectional transformers for language understanding[EB/OL]. (2018- 10- 11) [2018- 11- 13].https://arxiv.org/abs/1810.04805." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding">
                                        <b>[35]</b>
                                         DEVLIN J, CHANG M-W, LEE K, et al.BERT:pre-training of deep bidirectional transformers for language understanding[EB/OL]. (2018- 10- 11) [2018- 11- 13].https://arxiv.org/abs/1810.04805.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-29 17:13</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),1936-1941 DOI:10.11772/j.issn.1001-9081.2018122477            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于一维卷积混合神经网络的文本情感分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E9%83%91%E6%B7%8F&amp;code=42202190&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈郑淏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E7%BF%B1&amp;code=39026688&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯翱</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E5%98%89&amp;code=33976052&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何嘉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%88%90%E9%83%BD%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=1699079&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">成都信息工程大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对情感分类中传统二维卷积模型对特征语义信息的损耗以及时序特征表达能力匮乏的问题, 提出了一种基于一维卷积神经网络 (CNN) 和循环神经网络 (RNN) 的混合模型。首先, 使用一维卷积替换二维卷积以保留更丰富的局部语义特征;再由池化层降维后进入循环神经网络层, 整合特征之间的时序关系;最后, 经过softmax层实现情感分类。在多个标准英文数据集上的实验结果表明, 所提模型在SST和MR数据集上的分类准确率与传统统计方法和端到端深度学习方法相比有1至3个百分点的提升, 而对网络各组成部分的分析验证了一维卷积和循环神经网络的引入有助于提升分类准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">情感分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">循环神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%8D%E5%90%91%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">词向量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈郑淏 (1993—) , 男, 四川成都人, 硕士研究生, CCF会员, 主要研究方向:自然语言处理、深度学习;;
                                </span>
                                <span>
                                    冯翱 (1978—) , 男, 四川广安人, 副教授, 博士, CCF会员, 主要研究方向:信息检索、数据挖掘、机器学习;;
                                </span>
                                <span>
                                    *何嘉 (1968—) , 女, 四川成都人, 教授, 博士, CCF会员, 主要研究方向:计算智能、人工智能。电子邮箱hejia@cuit.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>四川省科技厅应用基础重点项目 (2017JY0011);</span>
                    </p>
            </div>
                    <h1><b>Text sentiment classification based on</b> 1<b>D convolutional hybrid neural network</b></h1>
                    <h2>
                    <span>CHEN Zhenghao</span>
                    <span>FENG Ao</span>
                    <span>HE Jia</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Chengdu University of Information Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Traditional 2 D convolutional models suffer from loss of semantic information and lack of sequential feature expression ability in sentiment classification. Aiming at these problems, a hybrid model based on 1 D Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) was proposed. Firstly, 2 D convolution was replaced by 1 D convolution to retain richer local semantic features. Then, a pooling layer was used to reduce data dimension and the output was put into the recurrent neural network layer to extract sequential information between the features. Finally, softmax layer was used to realize the sentiment classification. The experimental results on multiple standard English datasets show that the proposed model has 1-3 percentage points improvement in classification accuracy compared with traditional statistical method and end-to-end deep learning method. Analysis of each component of network verifies the value of introduction of 1 D convolution and recurrent neural network for better classification accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sentiment%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sentiment classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Recurrent%20Neural%20Network%20(RNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Recurrent Neural Network (RNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=word%20embedding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">word embedding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Zhenghao, born in 1993, M. S. candidate. His research interests include natural language processing, deep learning. ;
                                </span>
                                <span>
                                    FENG Ao, born in 1978, Ph. D. , associate professor. His research interests include information retrieval, data mining, machine learning. ;
                                </span>
                                <span>
                                    HE Jia, born in 1968, Ph. D. , professor. Her research interests include computing intelligence, artificial intelligence.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-17</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Key Project of Applied Basic Research of Sichuan Science and Technology Department (2017JY0011);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="73" name="73" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="74">随着互联网和社交媒体的蓬勃发展, 网络用户不再局限于浏览信息, 更多的人开始表达自己的观点, 分享知识并创作内容。互联网中出现了大量用户产生的信息, 例如对热点新闻事件的讨论、对特定商品的评价、对电影的评分等, 这些评论信息都包含了各种复杂的情感色彩或情感倾向, 如赞同、否定和喜怒哀乐等, 企业、机构或个人希望整合这些带有主观色彩的评论, 来解析和跟踪大众舆论对于某一对象或某一事件的看法。对主观评论的分析在业界被广泛应用于股票价格预测、产品分析、商品推荐等领域, 在政府部门常常被应用于舆情监测、民意调研、异常检测等方面。</p>
                </div>
                <div class="p1">
                    <p id="75">由于网络上这类评论信息激增, 仅靠人工难以在海量的非结构文本数据中收集和挖掘出有价值的情感信息, 因此需要设计情感分析算法, 利用计算机来帮助用户快速、有效地获取对于某一对象的情感倾向, 这就是情感分析 (Sentiment Analysis) 的主要任务<citation id="298" type="reference"><link href="228" rel="bibliography" /><link href="230" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。本文研究的是语句级情感分析, 即通过算法判断给定句子的情感倾向, 对应于机器学习领域的文本分类问题。</p>
                </div>
                <div class="p1">
                    <p id="76">深度学习逐渐成为一种性能优异的主流机器学习方法, 它通过学习数据的多层特征表示, 在很多领域取得优于传统方法的结果。近年来, 随着深度学习在图像处理、机器翻译等领域的成功, 深度学习也被用于文本情感分类任务中。卷积神经网络 (Convolutional Neural Network, CNN) 是一种常见的多层神经网络, 由于能够有效地捕获时间和空间结构的相关性, 被广泛应用于计算机视觉和自然语言处理等领域。在文本情感分类任务中, CNN逐渐取代传统基于文本统计信息的机器学习方法, 成为主流模型之一。</p>
                </div>
                <div class="p1">
                    <p id="77">目前, 基于CNN的文本情感分类方法大多是利用文本局部的最大语义特征进行情感极性判别<citation id="299" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。Kim<citation id="300" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出的CNN模型是较为经典的一种, 它通过二维卷积获取文本N-grams特征, 再经过最大池化层筛选出最显著的语义特征, 使用全连接层组合多个特征来判断情感倾向。然而, 对输入文本使用二维卷积会压缩特征图 (Feature Map) 的维度, 损失大量原始文本中的语义信息。从语言学的角度理解, 该模型仅从文本中挖掘出包含情感的关键词或词组, 对关键词或词组赋予不同的权重来判断情感倾向, 并未考虑文本的语序结构。</p>
                </div>
                <div class="p1">
                    <p id="78">针对上述CNN模型的缺陷, 本文提出了一种基于一维卷积神经网络和循环神经网络 (Recurrent Neural Network, RNN) 的混合模型。首先, 使用一维卷积替换二维卷积来提取文本特征, 在获取文本N-grams特征的过程中保持特征维度不变, 将语义信息的损失降到最低。其次, 通过RNN获取高层特征之间的时序关系, 理解文本的全局语义。相比文献<citation id="301" type="reference">[<a class="sup">4</a>]</citation>的CNN模型, 该混合模型具有更强的特征提取能力, 且提取到的特征维度更高, 种类更丰富。实验结果表明, 该模型在多个标准数据集上的情感分类性能相比传统统计方法和主流端到端深度学习模型均有明显的提升。本文还在该模型的基础上, 对不同网络层的特征提取模式进行改进, 对比多种网络结构的效果, 验证了一维卷积在文本情感分类任务上的有效性。</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="80" name="80">1.1 <b>传统情感分类方法</b></h4>
                <div class="p1">
                    <p id="81">传统的情感分类方法主要分为基于情感词典和基于机器学习的方法<citation id="302" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">1.1.1 基于情感词典的分类方法</h4>
                <div class="p1">
                    <p id="83">该方法主要通过构建情感词典以及一系列的规则来判断文本的情感极性。Kim等<citation id="303" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>使用已有的情感词典, 根据词语的情感得分加和的方法来判断文本的情感极性。Turney<citation id="304" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>利用点互信息方法扩展正面和负面的种子词汇, 使用形容词种子集作为语句中词的评分, 根据多个固定的句式结构来判断语句情感倾向。Hu等<citation id="305" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>通过词汇语义网WordNet的同义词、反义词关系, 得到形容词情感词典, 并根据该词典与一些简单规则判断语句的情感极性。这类方法严重依赖情感词典的质量, 并且词典的维护需要耗费大量的人力物力, 随着新词的不断涌现, 己经不能满足应用需求。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">1.1.2 基于机器学习的分类方法</h4>
                <div class="p1">
                    <p id="85">该方法首先从文本中筛选出一组具有统计意义的特征, 然后使用机器学习算法构建分类模型判断文本的情感极性。Pang等<citation id="306" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>最早使用机器学习方法对电影评论进行情感极性判别, 对比了朴素贝叶斯 (Naïve Bayes, NB) 、最大熵 (Maximum Entropy, ME) 和支持向量机 (Support Vector Machine, SVM) 这三种机器学习算法在情感分类中的效果。Mohammad等<citation id="307" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在Pang等<citation id="308" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>的方法基础上, 加入通过情感词典构建的特征, 取得了更高的分类准确度。Kim等<citation id="309" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>引入词汇、位置和评价词作为特征, 使用SVM作为分类模型进行情感极性判别。这类方法对不同的数据集选取不同的人工特征, 模型泛化能力差, 而且特征大多来自于文本的个别词汇统计数据, 无法体现词语间的关系以及上下文信息<citation id="310" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">1.2 <b>基于深度学习的情感分类方法</b></h4>
                <div class="p1">
                    <p id="87">在文本表示方面, Bengio等<citation id="311" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>最早提出用神经网络学习语言模型, 训练得到词在同一个向量空间上的表示, 与one-hot表示方式相比维数大幅度降低。由于该模型参数较多, 训练效率低下, Mikolov等<citation id="312" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在此基础上使用层次化Softmax进行优化, 提出了Word2vec框架, 使训练时间大幅度缩短的同时, 学习得到的词向量在一定程度上可体现深层的语法和语义特征。</p>
                </div>
                <div class="p1">
                    <p id="88">随着Word2vec<citation id="313" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和GloVe<citation id="314" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>等词的分布式表示方法的出现, 词向量表示维度大幅度降低, 且包含更多明确的语义信息, 这使深度学习方法逐渐成为情感分类任务的主流方法。深度学习相比传统算法, 具有更强的语义表达能力, 在不需要人工进行特征选择的情况下取得了较好的性能。</p>
                </div>
                <div class="p1">
                    <p id="89">基于深度学习的分类方法是一种多层表示学习方法。该方法可以利用深层神经网络, 从低层的文本表示中自动学习高层的文本语义特征。基于深度学习的方法主要包含如下几种。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">1.2.1 基于递归神经网络</h4>
                <div class="p1">
                    <p id="91">Socher等<citation id="317" type="reference"><link href="258" rel="bibliography" /><link href="260" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>提出了多个递归神经网络 (Recursive Neural Network) 模型, 将文本的句法结构加入到神经网络模型中。Qian等<citation id="315" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出两种基于短语词性标签的递归网络模型:标签引导递归神经网络 (Tag-Guided Recursive Neural Network, TG-RNN) 和标签嵌入递归神经网络 (Tag Embedded Recursive Neural Network, TE-RNN) , 前者根据词性标签选择一个用于子节点词嵌入的合成函数, 后者学习词性标签的词嵌入, 并与子节点的词嵌入结合。Tai等<citation id="316" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>将长短期记忆 (Long Short-Term Memory, LSTM) 网络改造为树状结构, 相比线性的LSTM结构效果有明显提升。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">1.2.2 基于循环神经网络</h4>
                <div class="p1">
                    <p id="93">Irsoy等<citation id="318" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>利用循环神经网络获取文本序列特征, 对比了不同层数RNN对性能的影响;Liu等<citation id="319" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出了3种可用于多种数据集共同训练的RNN模型, 将分布相似的数据集同时作为输入, 提高了模型的泛化能力;Qian等<citation id="320" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>在LSTM的基础上对情感词添加了正则化, 不同类型情感词的出现会调整当前时刻情感表示的概率分布。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94">1.2.3 基于卷积神经网络</h4>
                <div class="p1">
                    <p id="95">Kim<citation id="321" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>使用CNN捕获文本的局部语义特征, 结合静态或微调的词向量实现句子级情感分类;Kalchbrenner等<citation id="322" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>使用一维CNN获取文本的N-grams特征, 尽可能保留了原文本的语义信息, 再经过动态<i>K</i>-Max池化筛选出最显著的<i>k</i>个特征;Zhou等<citation id="323" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>提出了一种结合CNN和RNN的混合模型, 利用CNN提取文本的N-grams特征, RNN整合文本的全局信息。</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag">2 模型介绍</h3>
                <h4 class="anchor-tag" id="97" name="97">2.1 <b>词嵌入向量和</b>N-grams<b>特征</b></h4>
                <div class="p1">
                    <p id="98">自然语言处理任务中大多数深度学习模型都需要词向量作为神经网络的输入特征<citation id="324" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。词嵌入 (Word Embedding) 是一种用于语言建模和特征学习的技术, 它将词汇表中的单词转换成连续的实数向量。例如, 单词“cat”在一个5维词嵌入模型中可表示为向量 (0.15, 0.23, 0.41, 0.63, 0.45) 。该技术能将高维稀疏的向量空间 (例如one-hot向量空间, 只有一个维度的值取1, 其他全取0) 转换为低维稠密的向量空间。词向量可编码一定的语言学规则和模式, 包含语义特征。</p>
                </div>
                <div class="p1">
                    <p id="99">词嵌入向量仅仅涵盖了词级别 (word level) 的特征表示, 无法表达固定短语和词组的语义, 对文本的表示能力不足。在传统统计模型中, N-grams特征常被用于解决此类问题。神经网络模型中的卷积操作也能达到类似的效果, 用于提升文本的语义表达能力。例如, “I get up early”这句话通过大小为2的卷积核处理后会得到Bigram特征:“I get”“get up”“up early”, 其中“get up”为固定的短语搭配, 经过反向传播后该特征能分配到更高的权重。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">2.2 <b>长短期记忆网络</b></h4>
                <div class="p1">
                    <p id="101">由于原始的循环神经网络存在严重的梯度消失或梯度爆炸问题, 对于较长距离的特征处理能力弱, 网络的处理单元出现了若干的变体, 其中长短期记忆 (LSTM) 网络<citation id="325" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>已被广泛应用于自然语言处理领域。简单地说, 在LSTM中, 隐含状态<b><i>h</i></b><sub><i>t</i></sub>和存储单元<b><i>c</i></b><sub><i>t</i></sub>是它们前一时刻的状态<b><i>c</i></b><sub><i>t</i>-1</sub>、<b><i>h</i></b><sub><i>t</i>-1</sub>和当前时刻输入向量<b><i>x</i></b><sub><i>t</i></sub>的函数, 其表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="102"><b><i>c</i></b><sub><i>t</i></sub>, <b><i>h</i></b><sub><i>t</i></sub>=<i>g</i><sup> (LSTM) </sup> (<b><i>c</i></b><sub><i>t</i>-1</sub>, <b><i>h</i></b><sub><i>t</i>-1</sub>, <b><i>x</i></b><sub><i>t</i></sub>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="103">隐含状态<b><i>h</i></b><sub><i>t</i></sub>∈<b>R</b><sup><i>d</i></sup>是隐含层<i>t</i>时刻的向量表示, 同时也编码该时刻之前的上下文信息。</p>
                </div>
                <div class="p1">
                    <p id="104">在单向LSTM中, 每个位置<b><i>h</i></b><sub><i>t</i></sub>的隐藏状态只保留了前向的上下文信息, 而没有考虑反向的上下文信息。双向长短期记忆 (Bi-directional Long Short-Term Memory, BiLSTM) 网络<citation id="326" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>利用两个并行通道 (前向和后向) 和两个LSTM的隐藏状态级联作为每个位置的表示。前向和后向LSTM分别表述如下:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">c</mi><mo>→</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>→</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>g</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>L</mtext><mtext>S</mtext><mtext>Τ</mtext><mtext>Μ</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">c</mi><mo>→</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>→</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">c</mi><mo>←</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>g</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>L</mtext><mtext>S</mtext><mtext>Τ</mtext><mtext>Μ</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">c</mi><mo>←</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">其中, <i>g</i><sup> (LSTM) </sup>与式 (1) 中的相同, 但分别使用不同的参数, 在每个位置<i>t</i>, 隐含状态表示为<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mo stretchy="false">[</mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>→</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">]</mo></mrow></math></mathml>, 它是前向LSTM和后向LSTM隐含状态的联结。以这种方式, 可以同时考虑到向前和向后上下文信息, 从而更有效地从文本中提取特征。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">2.3 <b>基于卷积神经网络的文本情感分类</b></h4>
                <div class="p1">
                    <p id="109">卷积神经网络是一种多层神经网络, 由卷积层、池化层、全连接层等组成。由于能够提取空间或时间结构的局部相关性, <i>CNN</i>在计算机视觉、语音识别和自然语言处理等方面都取得了优异的表现。对于句子建模, <i>CNN</i>通过卷积核在句子不同位置提取<i>N</i>-<i>grams</i>特征, 并通过池化操作学习短、长程的上下文关系。其中, <i>Kim</i><citation id="327" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了经典的基于卷积神经网络的文本情感分析算法, 本文采用其中的<i>CNN</i>-<i>non</i>-<i>static</i>作为对照算法 (后文简称为<i>Kim</i>-<i>CNN</i>) , 其结构如图1所示。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907012_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 CNN-non-static网络结构" src="Detail/GetImg?filename=images/JSJY201907012_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 <i>CNN</i>-<i>non</i>-<i>static</i>网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907012_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 1 <i>CNN</i>-<i>non</i>-<i>static network architecture</i></p>

                </div>
                <div class="p1">
                    <p id="111"><i>Kim</i>-<i>CNN</i>共有4层, 如下所示。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">1) 输入层。</h4>
                <div class="p1">
                    <p id="113">文本<b><i>D</i></b>中每个词<b><i>w</i></b><sub><i>i</i></sub>查询词向量表得到对应的词向量<b><i>x</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>k</i></sup>, <i>k</i>为词向量的维度。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">2) 卷积层。</h4>
                <div class="p1">
                    <p id="115">一个卷积操作对应一个卷积核<b><i>w</i></b><sub><b><i>c</i></b></sub>∈<b>R</b><sup><i>hk</i></sup>, 每个卷积核通过能容纳<i>h</i>个词的滑动窗口进行卷积计算, 得到特征值:</p>
                </div>
                <div class="p1">
                    <p id="116"><i>c</i><sub><i>i</i></sub>=<i>f</i> (<b><i>w</i></b><sub><b><i>c</i></b></sub>·<b><i>x</i></b><sub><i>i</i>:<i>i</i>+<i>h</i>-1</sub>+<i>b</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="117">其中:<b><i>x</i></b><sub><i>i</i>:<i>i</i>+<i>h</i>-1</sub>∈<b>R</b><sup><i>kh</i></sup>表示从输入文本第<i>i</i>个词 (第<i>i</i>列) 开始邻接的<i>h</i>个词向量拼接而成的矩阵;<i>b</i>∈<b>R</b>表示偏置; <i>f</i>使用非线性激活函数ReLU (Rectified Linear Unit) ;<i>c</i><sub><i>i</i></sub>表示卷积核在文本第<i>i</i>个位置的特征值。</p>
                </div>
                <div class="p1">
                    <p id="118">卷积核<b><i>w</i></b><sub><b><i>c</i></b></sub>作用于句子中每一个可能的窗口{<b><i>x</i></b><sub>1:<i>h</i></sub>, <b><i>x</i></b><sub>2:<i>h</i>+1</sub>, …, <b><i>x</i></b><sub><i>n</i>-<i>h</i>+1:<i>n</i></sub>}, 获得特征图:</p>
                </div>
                <div class="p1">
                    <p id="119"><b><i>c</i></b>=[<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i>-<i>h</i>+1</sub>]∈<b>R</b><sup><i>n</i>-<i>h</i>+1</sup>      (5) </p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">3) 池化层。</h4>
                <div class="p1">
                    <p id="121">池化操作可改变句子的长度, 其主要目标是获取特征图中最重要的特征, 即最大的特征值<mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">c</mi><mo stretchy="false">}</mo></mrow></math></mathml>, 然后将得到的所有最大特征值拼接, 生成文本的高层特征向量<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">v</mi><mo>=</mo><mo stretchy="false">[</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">]</mo></mrow></math></mathml>, 其中<i>m</i>为卷积核数量。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">4) Softmax层。</h4>
                <div class="p1">
                    <p id="125">以特征向量<b><i>v</i></b>作为输入, 使用Softmax分类, 输出当前文本在各个类别上的概率:</p>
                </div>
                <div class="p1">
                    <p id="126"><b><i>y</i></b>=softmax (<b><i>w</i></b><sub><i>s</i></sub><b><i>v</i></b>+<b><i>b</i></b><sub><i>s</i></sub>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="127">其中:<b><i>y</i></b>∈<b>R</b><sup><i>s</i></sup>;<b><i>w</i></b><sub><i>s</i></sub>为权重;<b><i>b</i></b><sub><i>s</i></sub>为偏置, <i>s</i>为类别数目。预测出概率后, 使用交叉熵形式的损失函数并反向传播, 进行权值更新和词向量微调 (fine tuning) , 多次迭代后, 取最优的模型参数。</p>
                </div>
                <div class="p1">
                    <p id="128">目前, 基于CNN的情感分类方法大多使用二维卷积结构, 但通过这种方式获得的N-grams特征, 相比输入文本的词向量, 特征维度降低, 特征包含的语法和语义信息较少。Kim-CNN模型可以理解为使用二维卷积从文本中捕获对应情感标签下语义特征最显著的“关键词”, 通过全连接对多个“关键词”加权求和, 判断情感类别。然而, 使用二维卷积会压缩获取N-grams特征的维度, 损耗特征的大量语义信息, 无法实现多层神经网络来提取高层次的语义特征, 而且CNN只能获取文本的局部特征, 对时序信息和全局特征不能有效地识别与表示。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">2.4 <b>基于一维卷积的混合网络模型</b></h4>
                <div class="p1">
                    <p id="130">由于<i>Kim</i>-<i>CNN</i>模型存在二维卷积损失特征语义信息以及<i>CNN</i>无法有效地识别和表达时序信息的问题, 本文在该算法的基础上进行改进, 在网络结构中加入一维卷积结构和<i>BiLSTM</i>, 提出了一种应用于文本情感分类的神经网络结构:1<i>D</i>-<i>CLSTM</i>。具体改变为:将卷积层的二维卷积操作替换成词向量每个维度上的一维卷积操作, 并在池化层和<i>Softmax</i>层之间增加一个循环神经网络层, 其结构如图2所示。</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907012_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 1D-CLSTM网络结构" src="Detail/GetImg?filename=images/JSJY201907012_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 1<i>D</i>-<i>CLSTM</i>网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907012_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 1<i>D</i>-<i>CLSTM network architecture</i></p>

                </div>
                <div class="p1">
                    <p id="132">从图2可以看出, 该网络自底向上由5层结构组成, 分别如下。</p>
                </div>
                <h4 class="anchor-tag" id="133" name="133">1) 输入层。</h4>
                <div class="p1">
                    <p id="134">从文本<b><i>D</i></b>查找词<b><i>w</i></b><sub><i>i</i></sub>, 得到对应的词向量<b><i>x</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>k</i></sup>, <i>k</i>为词向量的维度。在文本开始和末尾分别添加全0的Padding, 以保证卷积后句子长度不变。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">2) 一维卷积层。</h4>
                <div class="p1">
                    <p id="136">词向量每一维 (每一行) 分别使用不同的一维卷积。一组一维卷积核表示为<b><i>w</i></b><sub><b><i>c</i></b></sub>∈<b>R</b><sup><i>hk</i></sup>, 可生成一个特征图<b><i>c</i></b>, <i>h</i>为滑动窗口大小。经过卷积, 文本第<i>i</i>个词的第<i>j</i>维得到的特征值为:</p>
                </div>
                <div class="p1">
                    <p id="137"><i>c</i><sub><i>ji</i></sub>=<i>f</i> (<b><i>w</i></b><sub><i>cj</i></sub>·<b><i>x</i></b><sub><i>j</i>, <i>i</i>:<i>i</i>+<i>h</i>-1</sub>+<i>b</i>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="138">其中, <b><i>w</i></b><sub><i>cj</i></sub>∈<b>R</b><sup><i>h</i></sup>表示词向量第<i>j</i>维上的一维卷积核。</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139">3) 池化层。</h4>
                <div class="p1">
                    <p id="140">池化操作的目标是选取文本各个位置最重要的特征, 对于每一个特征图<b><i>c</i></b>, 第<i>i</i>列的最大特征值<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></math></mathml>, 经过池化层得到其中一个特征向量<mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">v</mi><mo>=</mo><mo stretchy="false">[</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">]</mo></mrow></math></mathml>。其中, <b><i>c</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>k</i></sup>, <i>L</i>为文本长度。</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143">4) 循环神经网络层。</h4>
                <div class="p1">
                    <p id="144">经过池化层得到矩阵<b><i>V</i></b>=[<b><i>v</i></b><sub>1</sub>, <b><i>v</i></b><sub>2</sub>, …, <b><i>v</i></b><sub><i>N</i></sub>], 将其按列展开为[<i>α</i><sub>1</sub>, <i>α</i><sub>2</sub>, …, <i>α</i><sub><i>L</i></sub>], <i>N</i>表示特征向量<b><i>v</i></b>的数量。向量<i>α</i><sub><i>t</i></sub>作为BiLSTM网络<i>t</i>时刻的输入, 由式 (2) 和式 (3) 分别求得<i>t</i>时刻前向和后向的隐藏状态<mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>→</mo></mover></math></mathml><sub><i>t</i></sub>和<mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover></math></mathml><sub><i>t</i></sub>, 输出为:</p>
                </div>
                <div class="p1">
                    <p id="147" class="code-formula">
                        <mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>→</mo></mover><mi>y</mi></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>→</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover><mi>y</mi></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>y</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="148">其中: <i>β</i><sub><i>t</i></sub>∈<b>R</b><sup><i>d</i></sup>;<i>d</i>为隐藏状态<b><i>h</i></b><sub><i>t</i></sub>的维度。各个时刻的输出取平均值得到:</p>
                </div>
                <div class="p1">
                    <p id="149" class="code-formula">
                        <mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>t</mi><mi>L</mi></munderover><mi mathvariant="bold-italic">β</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="150" name="150">5) Softmax层。</h4>
                <div class="p1">
                    <p id="151">用Softmax作为分类器, 输出当前文本在各个情感类别下的概率。</p>
                </div>
                <div class="p1">
                    <p id="152">本文选用一维卷积替换二维卷积有两点考虑:其一, 通过一维卷积得到的特征图可以保持与输入文本相同的维度, 将语义信息的损失降到最小;其二, 词向量可以认为是神经网络训练语言模型的副产物<citation id="328" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>, 词向量各维的值可以看作隐含层各个神经元的输出, 词向量的各个维度之间相互独立, 卷积应该分别进行, 而不是将它们简单地进行加权求和。</p>
                </div>
                <div class="p1">
                    <p id="153">卷积操作虽然可以将相邻词或特征的语义进行组合, 但无法有效地识别和表示时序信息。为解决这一问题, 本文采用BiLSTM将一维卷积提取出的N-grams特征按时间顺序整合, 挖掘特征之间的时序关系以及全局语义信息。</p>
                </div>
                <div class="p1">
                    <p id="154">如上分析, 本文提出的1D-CLSTM模型相对Kim-CNN模型来说, 较好地避免了在二维卷积中损失语义信息, 无法识别和表达时序信息等问题, 第3章将用实验结果验证它在情感判别领域的应用效果。</p>
                </div>
                <h3 id="155" name="155" class="anchor-tag">3 实验及结果分析</h3>
                <div class="p1">
                    <p id="156">为了检验1<i>D</i>-<i>CLSTM</i>模型的性能以及证明一维卷积在情感分类任务上的优越性, 将其与近年来一些经典的文本情感分类算法进行实验对比。在基本模型之外, 还对一些其他的变体进行实验, 以分析各种网络结构对于分类效果的影响。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157">3.1 <b>情感分类数据集</b></h4>
                <div class="p1">
                    <p id="158">本文在多个标准英文数据集上测试了上述模型, 数据集的统计信息如表1所示。<i>SST</i>- 1和<i>SST</i>- 2<citation id="329" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>分别为五分类和二分类的影评情感分类数据集, 来自<i>Stanford Sentiment Treebank</i>。<i>MR</i>是二分类的影评数据集, 目标是判断评论的情感为正面还是负面。<i>IMDB</i>由100 000条二分类的影评数据组成, 每一条影评包含多个句子, 属于文档级的情感分类数据集<citation id="330" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表</b>1 <b>数据集统计信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Statistical information of datasets</i></p>
                    <p class="img_note"></p>
                    <table id="159" border="1"><tr><td>数据集</td><td>类别</td><td>句子平均长度</td><td>训练集大小</td><td colspan="2">验证集大小</td><td colspan="2">测试集大小</td></tr><tr><td><i>SST</i>- 1</td><td>5</td><td>18</td><td colspan="2">8 544</td><td colspan="2">1 101</td><td>2 210</td></tr><tr><td><br /><i>SST</i>- 2</td><td>2</td><td>19</td><td colspan="2">6 920</td><td colspan="2">872</td><td>1 821</td></tr><tr><td><br /><i>MR</i></td><td>2</td><td>20</td><td colspan="2">10 662</td><td colspan="2">—</td><td>—</td></tr><tr><td><br /><i>IMDB</i></td><td>2</td><td>294</td><td colspan="2">25 000</td><td colspan="2">—</td><td>25 000</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="160">由于<i>IMDB</i>数据集没有明确的训练集、验证集划分, 采用10折交叉验证 (10-<i>fold cross</i>-<i>validation</i>) , 将训练集随机分成10组, 每次使用训练数据的90%训练模型参数, 剩下10%作为验证集, 验证模型优劣, 选取验证结果最优的模型参数, 用于计算测试集上的情感分类准确度。<i>MR</i>数据集使用81%作为训练集, 9%作为验证集, 10%作为测试集。<i>SST</i>- 1和<i>SST</i>- 2数据集有固定的验证集划分, 因此每轮实验随机初始化参数, 重复训练10次, 取这10次实验结果的平均值作为最终的情感分类准确度。</p>
                </div>
                <h4 class="anchor-tag" id="161" name="161">3.2 <b>参数设置</b></h4>
                <div class="p1">
                    <p id="162">实验使用<i>GloVe</i>预训练的词嵌入向量作为初始的输入<citation id="331" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 维度为300。对于一维卷积层, 采用1、2、3这三种尺寸的卷积核, 每种尺寸的卷积核数量均设置为4, 确保卷积网络从不同维度不同位置提取多种文本特征。循环神经网络层使用<i>BiLSTM</i>, 记忆单元的维度为150, 词向量和<i>BiLSTM</i>后均设置0.5的<i>dropout</i>。训练批次大小 (<i>batch size</i>) 为50, 学习率为5<i>E</i>-4, 进行反向传播的参数增加0.001的<i>L</i>2正则约束, 选择<i>Adam</i>作为优化器 (<i>optimizer</i>) , 训练阶段最大迭代次数 (<i>epoch</i>) 为100。输入的词向量会根据反向传播微调, 在1 000个批次 (<i>batch</i>) 内验证集上的准确度没有提升会提前停止训练 (<i>early stop</i>) , 设置梯度截断 (<i>gradient clipping</i>) 为3。</p>
                </div>
                <h4 class="anchor-tag" id="163" name="163">3.3 <b>结果对比</b></h4>
                <div class="p1">
                    <p id="164">为测试模型性能, 本文将1<i>D</i>-<i>CLSTM</i>模型与主流的情感分类模型进行对比。实验采用分类准确度 (<i>Accuracy</i>) 作为情感分类算法的评价指标, 结果以百分比为单位。实验结果如表2所示。</p>
                </div>
                <div class="area_img" id="165">
                                            <p class="img_tit">
                                                <b>表</b>2 <b>不同算法的分类准确度对比</b>
                                                    <br />
                                                <i>Tab</i>. 2 <i>Classification accuracy comparison among</i><i>different algorithms</i>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907012_16500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201907012_16500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907012_16500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 不同算法的分类准确度对比" src="Detail/GetImg?filename=images/JSJY201907012_16500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>
                                <p class="img_note">注:算法后标注有*的实验结果直接引用于相关参考文献, 其余实验结果来自对应模型相同参数设置和开源代码的复现结果。</p>

                </div>
                <div class="p1">
                    <p id="166">其中, SST- 1和SST- 2数据集下的短语和句子分别代表训练时使用短语级或句子级的标注数据, 而验证和测试阶段均使用句子级的标注数据<citation id="332" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。前5个模型的结果引用自Socher<citation id="333" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。NB和BiNB均为朴素贝叶斯模型, 前者仅使用unigram特征, 后者同时使用unigram和bigram。SVM是使用unigram和bigram特征的支持向量机模型。RNN和RNTN (Recursive Neural Tensor Network) 均是基于文本句法解析树的递归神经网络模型。DCNN (Dynamic Convolutional Neural Network) <citation id="334" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>同样采用一维卷积, 但经过动态<i>k</i>-max池化后文本长度会减小, 仅保留了最显著的<i>k</i>个N-grams特征。BiLSTM是以词向量为输入的双向长短期记忆网络, 去掉了1D-CLSTM中的一维卷积层, 其余参数设置相同。Kim-CNN<citation id="335" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>是根据原论文的参数设置和开源代码复现, 将词向量由Word2vec换作GloVe, 由于文献<citation id="336" type="reference">[<a class="sup">4</a>]</citation>对于数据集划分、参数选择和优化的细节没有说明, 导致复现结果相比原文略差。文献<citation id="337" type="reference">[<a class="sup">4</a>]</citation>汇报了SST- 1短语、SST- 2短语和MR的结果, 分别为48.0%、87.2%和81.5%。</p>
                </div>
                <div class="p1">
                    <p id="167">从表2可以看出, 基于深度学习的情感分类方法性能明显优于传统机器学习方法, 1D-CLSTM在各个数据集上的结果均优于其他模型。对比BiLSTM, 1D-CLSTM增加了一维卷积, 使得各数据集的结果均有明显提升。这表明, 经过一维卷积提取的N-grams特征, 相比初始词向量, 包含了更有价值的语义信息, 适合后续的池化和循环神经网络处理。对比Kim-CNN的重现模型, 用一维卷积替换二维卷积并增加BiLSTM后, 在各数据集上的结果都有一定的提升, 最高达到2.9%。</p>
                </div>
                <div class="p1">
                    <p id="168">为了进一步验证一维卷积相对于二维卷积的优势, 本文在1D-CLSTM的基础上修改了部分结构, 提出了两种变体:1D-2D-CNN与1D-2D-CLSTM, 并通过实验对比了改进前和改进后分类准确度的变化。</p>
                </div>
                <div class="p1">
                    <p id="169">其中, 1D-2D-CNN是在Kim-CNN的基础上, 输入层和卷积层之间增加一个一维卷积层, 其他参数与Kim-CNN一致。1D-2D-CLSTM是在1D-2D-CNN的基础上, 在池化层前添加一个BiLSTM网络。</p>
                </div>
                <div class="p1">
                    <p id="170">从表3可以看出, 在SST- 1数据集中, 额外的一维卷积层为Kim-CNN模型带来了1至2个百分点的提升。相比原始输入的词向量, 经过一维卷积的特征包含了更多额外的语义信息, 说明一维卷积在提取N-grams特征方面具有一定的优势, 但是IMDB数据集上增加一维卷积反而让准确度有所下降, 可能原因是该数据集属于文档级的情感分类, 输入文本的平均长度为294, 仅使用卷积神经网络无法表征足够的全局语义信息。</p>
                </div>
                <div class="area_img" id="171">
                    <p class="img_tit"><b>表</b>3 <b>两种算法的分类准确度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Classification accuracy comparison of two algorithms </p>
                    <p class="img_note">%</p>
                    <table id="171" border="1"><tr><td rowspan="2">算法</td><td colspan="2"><br />SST- 1</td><td rowspan="2"></td><td colspan="2"><br />SST- 2</td><td rowspan="2">MR</td><td rowspan="2">IMDB</td></tr><tr><td><br />短语</td><td>句子</td><td><br />短语</td><td>句子</td></tr><tr><td>Kim-CNN</td><td>46.7</td><td>45.7</td><td></td><td>86.3</td><td>85.1</td><td>79.5</td><td>85.7</td></tr><tr><td><br />1D-2D-CNN</td><td>48.8</td><td>46.4</td><td></td><td>86.5</td><td>85.9</td><td>81.0</td><td>85.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="172">从表4可以看出, 将最大池化层改为BiLSTM后, 1D-2D-CLSTM模型相比1D-2D-CNN分类准确度有1个百分点左右的提升。这表明使用LSTM的循环神经网络相对简单的池化操作具有更强的表达力, 能够提取和表达更复杂的语法和语义特征, 适合情感分类任务。</p>
                </div>
                <div class="area_img" id="173">
                    <p class="img_tit"><b>表</b>4 3<b>种算法的分类准确度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Classification accuracy comparison of three algorithms </p>
                    <p class="img_note">%</p>
                    <table id="173" border="1"><tr><td rowspan="2">算法</td><td colspan="2"><br />SST- 1</td><td rowspan="2"></td><td colspan="2"><br />SST- 2</td><td rowspan="2">MR</td><td rowspan="2">IMDB</td></tr><tr><td><br />短语</td><td>句子</td><td><br />短语</td><td>句子</td></tr><tr><td>1D-CLSTM</td><td>49.6</td><td>46.1</td><td></td><td>87.4</td><td>85.5</td><td>81.1</td><td>85.9</td></tr><tr><td><br />1D-2D-CNN</td><td>48.8</td><td>46.4</td><td></td><td>86.5</td><td>85.9</td><td>81.0</td><td>85.1</td></tr><tr><td><br />1D-2D-CLSTM</td><td>49.8</td><td>46.6</td><td></td><td>87.5</td><td>86.1</td><td>81.8</td><td>86.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="174">1D-2D-CLSTM模型和1D-CLSTM模型的差别在, 将池化层换成了二维卷积层, 通过卷积来实现特征降维。从表4可以看到, 1D-2D-CLSTM模型仍有微弱的提升。这说明, 在情感分类任务上, 二维卷积层或许是比池化层更好的降维特征提取方式, 当然其代价是更大的网络参数和更高的训练成本。</p>
                </div>
                <h3 id="175" name="175" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="176">本文针对文本情感分类任务提出了一种基于一维卷积和循环神经网络的混合网络模型。该模型使用一维卷积替代二维卷积提取N-grams特征, 通过BiLSTM挖掘特征之间的时序关系, 解决了Kim-CNN模型中卷积损失特征语义以及无法表示时序信息的问题。实验结果表明, 该模型的情感分类性能相比同类主流算法有明显的提升, 并通过进一步的对比实验验证了引入一维卷积和循环神经网络对分类性能的正面影响。</p>
                </div>
                <div class="p1">
                    <p id="177">从现有实验结果来看, 在词嵌入向量的基础上使用卷积和循环神经网络进行情感分类, 对于最终分类性能的提升是有一定限度的, 其中尤其是原始词嵌入向量的质量对于最终模型的分类效果有很大影响。传统词嵌入向量是在较大的无标注文本集中通过计算各个关键词共同出现的统计指标得到的<citation id="341" type="reference"><link href="254" rel="bibliography" /><link href="256" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>, Kim-CNN<citation id="338" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的实验已经证明这些原始向量并不适合情感分析任务, 一个重要证据是具有相似统计关系的反义词会被表达为类似的嵌入向量, 因此, 最近的一些研究工作尝试在另一个有监督场景下训练上下文相关的嵌入向量<citation id="339" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>, 或者借用卷积神经网络逐层特征提取的思路来设计一个多层LSTM网络以表达语言不同粒度的特征<citation id="340" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>, 将这些额外的关键词表达与原始词嵌入向量结合带来了显著提升的分类精度。</p>
                </div>
                <div class="p1">
                    <p id="178">在计算机视觉领域使用迁移学习的方法可以最大限度地利用大规模数据集上预训练的模型参数, 使得在一个相似场景下进行学习的效率大幅度提高, 但是对于自然语言处理来说, 大多数场景下除了词嵌入向量可以重用以外, 都需要从头开始训练, 这对于训练效率和最终效果都造成了较大影响。Howard等<citation id="342" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>提出了一个ULMFiT (Universal Language Model Fine-Tuning) 模型, 建立了自然语言处理领域中迁移学习的完整应用框架。Radford等<citation id="343" type="reference"><link href="292" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>则将迁移学习过程建立在一个深层的Transformer网络<citation id="344" type="reference"><link href="294" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>上, 使用该框架预训练的模型通常在3次迭代左右就可以完成细调。同样基于Transformer网络, BERT<citation id="345" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>设计了一个填空 (Cloze) 任务, 同时利用前向和后向的上下文, 比之前的单向语言模型或者相互独立的双向LSTM都有明显优势, 该模型在11个不同的公开任务中, 都在之前最好的结果基础上明显提高。</p>
                </div>
                <div class="p1">
                    <p id="179">未来的工作将会集中在以下几个方面:首先是基于对现有词嵌入向量及其在情感分析任务中的表达能力分析, 尝试运用其他包含更丰富语义信息的特征表达方式;其次是进一步比较卷积神经网络、循环神经网络及Transformer网络等不同的网络结构在该场景不同任务中的性能, 选择适当的网络结构进行组合和优化;最后是充分利用以BERT为代表的预训练模型, 使用其网络结构和预训练参数, 结合其他网络模型, 该模型即使在不进行原始参数细调的前提下也能取得接近最优的效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="228">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200811000&amp;v=MTUyMzBac0Z5L25WTHZJTHo3QmQ3RzRIdG5Ocm85RlpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 周立柱, 贺宇凯, 王建勇.情感分析研究综述[J].计算机应用, 2008, 28 (11) :2725-2728. (ZHOU L Z, HE Y K, WANG J Y.Survey on research of sentiment analysis [J].Journal of Computer Applications, 2008, 28 (11) :2725-2728.) 
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201008009&amp;v=MjY0MzVxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25WTHZJTnlmVGJMRzRIOUhNcDQ5RmJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 赵妍妍, 秦兵, 刘挺.文本情感分析[J].软件学报, 2010, 21 (8) :1834-1848. (ZHAO Y Y, QIN B, LIU T.Sentiment analysis [J].Journal of Software, 2010, 21 (8) :1834-1848.) 
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A sensitivity analysis of (and practitioners&amp;#39;&amp;#39;guide to)convolutional neural networks for sentence classification">

                                <b>[3]</b> ZHANG Y, WALLACE B.A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification[EB/OL]. (2016- 04- 06) [2018- 06- 07].https://arxiv.org/abs/1510.03820.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Sentence Classification">

                                <b>[4]</b> KIM Y.Convolutional neural networks for sentence classification [EB/OL]. (2014- 09- 03) [2018- 06- 01].https://arxiv.org/abs/1408.5882.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD26FFFB5EBB77055235F755653217FCAF&amp;v=MjEwNzZhS2U2M1lvd0Zwa0lDM3c4eWhRUTcwbDZUWHJrcVJFM2VMWGlOc3ZwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxNHhLZz1OaWZjYXJHKw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> ZHANG L, WANG S, LIU B.Deep learning for sentiment analysis:a survey[J].Wiley Interdisciplinary Reviews:Data Mining and Knowledge Discovery, 2018, 8 (4) :e1253.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting opinions, opinion holders, and topics expressed in online newsmedia text">

                                <b>[6]</b> KIM S-M, HOVY E.Extracting opinions, opinion holders, and topics expressed in online news media text[C]// Proceedings of the 2006 Workshop on Sentiment and Subjectivity in Text.Stroudsburg, PA:Association for Computational Linguistics, 2006:1-8.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Thumbs up or thumbs down?Semantic orientation applied to unsupervised classification of reviews">

                                <b>[7]</b> TURNEY P D.Thumbs up or thumbs down?:semantic orientation applied to unsupervised classification of reviews[C]// Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2002:417-424.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mining and summarizing customer reviews">

                                <b>[8]</b> HU M, LIU B.Mining and summarizing customer reviews[C]// Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2004:168-177.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 PANG B, LEE L, VAITHYANATHAN S.Thumbs up?:sentiment classification using machine learning techniques[C]// Proceedings of the ACL- 02 Conference on Empirical Methods in Natural Language Processing-Volume 10.Stroudsburg, PA:Association for Computational Linguistics, 2002:79-86.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NRC-Canada:building the state-of-the-art in sentiment analysis of tweets">

                                <b>[10]</b> MOHAMMAD S M, KIRITCHENKO S, ZHU X.NRC-Canada:building the state-of-the-art in sentiment analysis of tweets[EB/OL]. (2013- 08- 28) [2018- 07- 02].https://arxiv.org/abs/1308.6242.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic identification of pro and con reasons in online reviews">

                                <b>[11]</b> KIM S-M, HOVY E.Automatic identification of pro and con reasons in online reviews[C]// Proceedings of the 2006 COLING/ACL on Main Conference Poster Sessions.Stroudsburg, PA:Association for Computational Linguistics, 2006:483-490.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700305623&amp;v=MDU4NzRNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VGJoST1OaWZPZmJLOEh0Zk5xSTlGWitzS0NuNDZvQg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> MEDHAT W, HASSAN A, KORASHY H.Sentiment analysis algorithms and applications:a survey [J].Ain Shams Engineering Journal, 2014, 5 (4) :1093-1113.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 BENGIO Y, DUCHARME R, VINCENT P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3:1137-1155.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">

                                <b>[14]</b> MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[C]// NIPS'13:Proceedings of the 26th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2013:3111-3119.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Glove:Global vectors for word representation">

                                <b>[15]</b> PENNINGTON J, SOCHER R, MANNING C.GloVe:global vectors for word representation[C]// Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2014:1532-1543.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recursive deep models for semantic compositionality over a sentiment treebank">

                                <b>[16]</b> SOCHER R, PERELYGIN A, WU J, et al.Recursive deep models for semantic compositionality over a sentiment treebank[C]// Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2013:1631-1642.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised recursive autoencoders for predicting sentiment distributions">

                                <b>[17]</b> SOCHER R, PENNINGTON J, HUANG E H, et al.Semi-supervised recursive autoencoders for predicting sentiment distributions[C]// Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2011:151-161.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning tag embeddings and tag-specific composition functions in recursive neural network">

                                <b>[18]</b> QIAN Q, TIAN B, HUANG M, et al.Learning tag embeddings and tag-specific composition functions in recursive neural network[C]// Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2015:1365-1374.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved semantic representations from tree-structured long short-term memory networks">

                                <b>[19]</b> TAI K S, SOCHER R, MANNING C D.Improved semantic representations from tree-structured long short-term memory networks[EB/OL]. (2015- 05- 30) [2018- 08- 10].https://arxiv.org/abs/1503.00075.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Opinion Mining with Deep Recurrent Neural Networks">

                                <b>[20]</b> IRSOY O, CARDIE C.Opinion mining with deep recurrent neural networks[C]// Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg, PA:Association for Computational Linguistics, 2014:720-728.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent Neural Network for Text Classification with Multi-Task Learning">

                                <b>[21]</b> LIU P, QIU X, HUANG X.Recurrent neural network for text classification with multi-task learning[EB/OL]. (2016- 05- 17) [2018- 08- 01].https://arxiv.org/abs/1605.05101.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Linguistically regularized LSTMs for sentiment classification">

                                <b>[22]</b> QIAN Q, HUANG M, LEI J, et al.Linguistically regularized LSTMs for sentiment classification [EB/OL]. (2017- 04- 25) [2018- 08- 15].https://arxiv.org/abs/1611.03949.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Convolutional Neural Network for Modelling Sentences">

                                <b>[23]</b> KALCHBRENNER N, GREFENSTETTE E, BLUNSOM P.A convolutional neural network for modelling sentences [EB/OL]. (2014- 04- 08) [2018- 07- 16].https://arxiv.org/abs/1404.2188.
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A C-LSTM neural network for text classification">

                                <b>[24]</b> ZHOU C, SUN C, LIU Z, et al.A C-LSTM neural network for text classification[EB/OL]. (2015- 11- 30) [2018- 08- 22].https://arxiv.org/abs/1511.08630.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Natural language processing (almost) from scratch">

                                <b>[25]</b> COLLOBERT R, WESTON J, BOTTOU L, et al.Natural lan-guage processing (almost) from scratch[J].Journal of Machine Learning Research, 2011, 12:2493-2537.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MDEyMzZxbzlGWk9vTERYVXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXdUYmhJPU5pZkpaYks5SHRqTQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hybrid speech recognition with deep bidirectional LSTM">

                                <b>[27]</b> GRAVES A, JAITLY N, MOHAMED A.Hybrid speech recognition with deep bidirectional LSTM[C]// Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2013:273-278.
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_28" >
                                    <b>[28]</b>
                                 MIKOLOV T, CHEN K, CORRADO G, et al.Efficient estimation of word representations in vector space[EB/OL]. (2013- 09- 07) [2018- 09- 02].https://arxiv.org/abs/1301.3781.
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sentiment Analysis and Opinion Mining">

                                <b>[29]</b> LIU B.Sentiment Analysis and Opinion Mining[M].San Rafael, CA:Morgan and Claypool Publishers, 2012:1-167.
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learned in translation:contextualized word vectors">

                                <b>[30]</b> McCANN B, BRADBURY J, XIONG C, et al.Learned in translation:contextualized word vectors[C]// NIPS 2017:Proceedings of the 31st Annual Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2017:6297-6308.
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep contextualized word representations">

                                <b>[31]</b> PETERS M E, NEUMANN M, IYYER M et al.Deep contextualized word representations[EB/OL]. (2018- 03- 22) [2018- 10- 21].https://arxiv.org/abs/1802.05365.
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Universal language model fine-tuning for text classification">

                                <b>[32]</b> HOWARD J, RUDER S.Universal language model fine-tuning for text classification[C]// Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:Association for Computational Linguistics, 2018:328-339.
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving language understanding by generative pre-training">

                                <b>[33]</b> RADFORD A, NARASIMHAN K, SALIMANS T, et al.Improving language understanding by generative pre-training[EB/OL]. (2018- 06- 11) [2018- 10- 22].https://blog.openai.com/language-unsupervised/.
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention is All You Need">

                                <b>[34]</b> VASWANI A, SHAZEER N, PARMAR N, et al.Attention is all you need[C]// NIPS 2017:Proceedings of the 31st Annual Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2017:5998-6008.
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding">

                                <b>[35]</b> DEVLIN J, CHANG M-W, LEE K, et al.BERT:pre-training of deep bidirectional transformers for language understanding[EB/OL]. (2018- 10- 11) [2018- 11- 13].https://arxiv.org/abs/1810.04805.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907012&amp;v=Mjg0OTVuVkx2SUx6N0JkN0c0SDlqTXFJOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
