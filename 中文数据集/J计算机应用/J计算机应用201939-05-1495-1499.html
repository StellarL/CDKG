<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136768373408750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905044%26RESULT%3d1%26SIGN%3dbmGEkCrmq12thUkrdQaI7JfMNVo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905044&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905044&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905044&amp;v=MDg5NDI3QmQ3RzRIOWpNcW85QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VYnZLTHo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="1 深度强化学习 ">1 深度强化学习</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="1.1 &lt;b&gt;强化学习&lt;/b&gt;">1.1 <b>强化学习</b></a></li>
                                                <li><a href="#76" data-title="1.2 &lt;b&gt;深度学习&lt;/b&gt;">1.2 <b>深度学习</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="2 基于深度强化学习的交通信号控制算法 ">2 基于深度强化学习的交通信号控制算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="2.1 &lt;b&gt;状态表示&lt;/b&gt;">2.1 <b>状态表示</b></a></li>
                                                <li><a href="#86" data-title="2.2 &lt;b&gt;动作集合&lt;/b&gt;">2.2 <b>动作集合</b></a></li>
                                                <li><a href="#89" data-title="2.3 &lt;b&gt;奖赏值函数&lt;/b&gt;">2.3 <b>奖赏值函数</b></a></li>
                                                <li><a href="#93" data-title="2.4 &lt;b&gt;交通信号控制智能体&lt;/b&gt;">2.4 <b>交通信号控制智能体</b></a></li>
                                                <li><a href="#96" data-title="2.5 &lt;b&gt;学习算法&lt;/b&gt;">2.5 <b>学习算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#117" data-title="3 实验 ">3 实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#119" data-title="3.1 &lt;b&gt;实验设置&lt;/b&gt;">3.1 <b>实验设置</b></a></li>
                                                <li><a href="#123" data-title="3.2 &lt;b&gt;超参数优化&lt;/b&gt;">3.2 <b>超参数优化</b></a></li>
                                                <li><a href="#130" data-title="3.3 &lt;b&gt;性能提升&lt;/b&gt;">3.3 <b>性能提升</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#132" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#85" data-title="图1 环境状态表示">图1 环境状态表示</a></li>
                                                <li><a href="#122" data-title="图2 2&#215;2交通网络">图2 2×2交通网络</a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;i&gt;DQN Agent&lt;/i&gt;&lt;b&gt;超参数设置&lt;/b&gt;"><b>表</b>1 <i>DQN Agent</i><b>超参数设置</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;i&gt;Agent&lt;/i&gt;3～ &lt;i&gt;Agent&lt;/i&gt;5&lt;b&gt;性能指标比较&lt;/b&gt;"><b>表</b>2 <i>Agent</i>3～ <i>Agent</i>5<b>性能指标比较</b></a></li>
                                                <li><a href="#129" data-title="图3 &lt;i&gt;Agent&lt;/i&gt;1～&lt;i&gt;Agent&lt;/i&gt;4性能评估">图3 <i>Agent</i>1～<i>Agent</i>4性能评估</a></li>
                                                <li><a href="#136" data-title="图4 智能体在不同流量环境中的性能">图4 智能体在不同流量环境中的性能</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="166">


                                    <a id="bibliography_1" title=" 李颖宏, 王力, 尹怡欣.区域交通信号系统节点分析及优化策略研究[J].计算机应用, 2010, 30 (4) :1107-1109. (LI Y H, WANG L, YIN Y X.Node analysis and optimization strategy for regional traffic network system [J].Journal of Computer Applications, 2010, 30 (4) :1107-1109.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201004074&amp;v=MTkyNjRkN0c0SDlITXE0OUNZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVWJ2S0x6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         李颖宏, 王力, 尹怡欣.区域交通信号系统节点分析及优化策略研究[J].计算机应用, 2010, 30 (4) :1107-1109. (LI Y H, WANG L, YIN Y X.Node analysis and optimization strategy for regional traffic network system [J].Journal of Computer Applications, 2010, 30 (4) :1107-1109.) 
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_2" title=" CHIU S, CHAND S.Self-organizing traffic control via fuzzy logic[C]// Proceedings of the 32nd IEEE Conference on Decision and Control.Piscataway, NJ:IEEE, 1994:1897-1902." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Na?ve Bayesian Classifier for Rand IEEE Conference on Decision and Control">
                                        <b>[2]</b>
                                         CHIU S, CHAND S.Self-organizing traffic control via fuzzy logic[C]// Proceedings of the 32nd IEEE Conference on Decision and Control.Piscataway, NJ:IEEE, 1994:1897-1902.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_3" title=" NAKAMITI G, GOMIDE F.Fuzzy sets in distributed traffic control[C]// Proceedings of IEEE 5th International Fuzzy Systems.Piscataway, NJ:IEEE, 1996:1617-1623." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy sets in distributed traffic control">
                                        <b>[3]</b>
                                         NAKAMITI G, GOMIDE F.Fuzzy sets in distributed traffic control[C]// Proceedings of IEEE 5th International Fuzzy Systems.Piscataway, NJ:IEEE, 1996:1617-1623.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_4" title=" MIKAMI S, KAKAZU Y.Genetic reinforcement learning for cooperative traffic signal control[C]// Proceedings of the 1st IEEE Conference on Evolutionary Computation.Piscataway, NJ:IEEE, 1994:223-228." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Genetic reinforcement learning for cooperative traffic signal control">
                                        <b>[4]</b>
                                         MIKAMI S, KAKAZU Y.Genetic reinforcement learning for cooperative traffic signal control[C]// Proceedings of the 1st IEEE Conference on Evolutionary Computation.Piscataway, NJ:IEEE, 1994:223-228.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_5" title=" MANIKONDA V, LEVY R, SATAPATHY G, et al.Autonomous Agents for traffic simulation and control[J].Transportation Research Record Journal of the Transportation Research Board, 2001, 1774 (1) :1-10." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJIX&amp;filename=SJIX1291BBCA9555132F34352F7E84D2B394&amp;v=MjI2NDhiZTRLQ1gwNnpXQVE3ang0U2dubDJSb3hEYkRtUnJPYkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMcTl4S289TmlmQ2RySzZGOUMrM2Z3MA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         MANIKONDA V, LEVY R, SATAPATHY G, et al.Autonomous Agents for traffic simulation and control[J].Transportation Research Record Journal of the Transportation Research Board, 2001, 1774 (1) :1-10.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_6" title=" LEE J H, LEE-KWANG H.Distributed and cooperative fuzzy controllers for traffic intersections group[J].IEEE Transactions on Systems, Man &amp;amp; Cybernetics Part C:Applications &amp;amp; Reviews, 1999, 29 (2) :263-271." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed and cooperative fuzzy controllers for traffic intersections group">
                                        <b>[6]</b>
                                         LEE J H, LEE-KWANG H.Distributed and cooperative fuzzy controllers for traffic intersections group[J].IEEE Transactions on Systems, Man &amp;amp; Cybernetics Part C:Applications &amp;amp; Reviews, 1999, 29 (2) :263-271.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_7" title=" SUTTON R S, BARTO A G.Reinforcement learning:an introduction[J].IEEE Transactions on Neural Networks, 1998, 9 (5) :1054-1054." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning: An Introduction">
                                        <b>[7]</b>
                                         SUTTON R S, BARTO A G.Reinforcement learning:an introduction[J].IEEE Transactions on Neural Networks, 1998, 9 (5) :1054-1054.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_8" title=" MEDINA J C, HAJBABAIE A, BENEKOHAL R F.Arterial traffic control using reinforcement learning Agents and information from adjacent intersections in the state and reward structure[C]// Proceedings of the 13th International IEEE Conference on Intelligent Transportation Systems.Piscataway, NJ:IEEE, 2010:525-530." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Arterial traffic control using reinforcement learning agents and information from adjacent intersections in the state and reward structure">
                                        <b>[8]</b>
                                         MEDINA J C, HAJBABAIE A, BENEKOHAL R F.Arterial traffic control using reinforcement learning Agents and information from adjacent intersections in the state and reward structure[C]// Proceedings of the 13th International IEEE Conference on Intelligent Transportation Systems.Piscataway, NJ:IEEE, 2010:525-530.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_9" title=" PRASHANTH L A, BHATNAGAR S.Reinforcement learning with function approximation for traffic signal control[J].IEEE Transactions on Intelligent Transportation Systems, 2011, 12 (2) :412-421." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning with function approximation for traffic signal control">
                                        <b>[9]</b>
                                         PRASHANTH L A, BHATNAGAR S.Reinforcement learning with function approximation for traffic signal control[J].IEEE Transactions on Intelligent Transportation Systems, 2011, 12 (2) :412-421.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_10" title=" ABDULHAI B, PRINGLE R, KARAKOULAS G J.Reinforcement learning for true adaptive traffic signal control[J].Journal of Transportation Engineering, 2003, 129 (3) :278-285." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCE&amp;filename=SJCEF267B2AA651A735003FE63622189ACC5&amp;v=Mjc0NDZrSVRuemtyaEEwY2J2bE5zbWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THE5eEtvPU5pZklhOFc2R05hK3JmNDBZdTRPZlhzNnloWVQ2VQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         ABDULHAI B, PRINGLE R, KARAKOULAS G J.Reinforcement learning for true adaptive traffic signal control[J].Journal of Transportation Engineering, 2003, 129 (3) :278-285.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_11" title=" BINGHAM E.Reinforcement learning in neurofuzzy traffic signal control[J].European Journal of Operational Research, 2001, 131 (2) :232-241." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100626986&amp;v=MjM4MDViSzdIdERPcm85Rll1a0pCWFEvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3V2JoQT1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         BINGHAM E.Reinforcement learning in neurofuzzy traffic signal control[J].European Journal of Operational Research, 2001, 131 (2) :232-241.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_12" title=" LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Learning,&amp;quot;">
                                        <b>[12]</b>
                                         LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_13" title=" LI L, LYU Y S, WANG F Y.Traffic signal timing via deep reinforcement learning[J].IEEE/CAA Journal of Automatica Sinica, 2016, 3 (3) :247-254." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201603003&amp;v=MTU4OTBac0Z5RG5VYnZLUHluRGJMRzRIOWZNckk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         LI L, LYU Y S, WANG F Y.Traffic signal timing via deep reinforcement learning[J].IEEE/CAA Journal of Automatica Sinica, 2016, 3 (3) :247-254.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_14" title=" MOUSAVI S S, SCHUKAT M, HOWLEY E.Traffic light control using deep policy-gradient and value-function-based reinforcement learning[J].IET Intelligent Transport Systems, 2017, 11 (7) :417-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Traffic light control using deep policygradient and value-function-based reinforcement learning">
                                        <b>[14]</b>
                                         MOUSAVI S S, SCHUKAT M, HOWLEY E.Traffic light control using deep policy-gradient and value-function-based reinforcement learning[J].IET Intelligent Transport Systems, 2017, 11 (7) :417-423.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_15" title=" van der POL E.Deep reinforcement learning for coordination in traffic light control[D].Amsterdam:University of Amsterdam, 2016:1-56." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning for coordination in traffic light control">
                                        <b>[15]</b>
                                         van der POL E.Deep reinforcement learning for coordination in traffic light control[D].Amsterdam:University of Amsterdam, 2016:1-56.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_16" title=" MNIH V, KAVUKCUOGLU K, SILVER D, et al.Playing atari with deep reinforcement learning[J/OL].arXiv Preprint, 2013, 2013:arXiv:1312.5602 [2013- 12- 09].https://arxiv.org/abs/1312.5602." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Playing Atari with deep reinforcement learning[OL]">
                                        <b>[16]</b>
                                         MNIH V, KAVUKCUOGLU K, SILVER D, et al.Playing atari with deep reinforcement learning[J/OL].arXiv Preprint, 2013, 2013:arXiv:1312.5602 [2013- 12- 09].https://arxiv.org/abs/1312.5602.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_17" title=" MNIH V, KAVUKCUOGLU K, SILVER D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human-Level control through deep reinforcement learning">
                                        <b>[17]</b>
                                         MNIH V, KAVUKCUOGLU K, SILVER D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_18" title=" LI Y X.Deep reinforcement learning:an overview[J/OL].arXiv Preprint, 2017, 2017:arXiv:1701.07274 [2017- 01- 25].https://arxiv.org/abs/1701.07274." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning:an overview">
                                        <b>[18]</b>
                                         LI Y X.Deep reinforcement learning:an overview[J/OL].arXiv Preprint, 2017, 2017:arXiv:1701.07274 [2017- 01- 25].https://arxiv.org/abs/1701.07274.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_19" title=" DULACARNOLD G, EVANS R, SUNEHAG P, et al.Reinforcement learning in large discrete action spaces[J/OL].arXiv Preprint, 2016, 2016:arXiv:1603.06861 [2016- 03- 22].https://arxiv.org/abs/1603.06861." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning in large discrete action spaces">
                                        <b>[19]</b>
                                         DULACARNOLD G, EVANS R, SUNEHAG P, et al.Reinforcement learning in large discrete action spaces[J/OL].arXiv Preprint, 2016, 2016:arXiv:1603.06861 [2016- 03- 22].https://arxiv.org/abs/1603.06861.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_20" title=" MNIH V, BADIA A P, MIRZA M, et al.Asynchronous methods for deep reinforcement learning[J/OL].arXiv Preprint, 2016, 2016:arXiv:1603.01783 [2016- 02- 04].https://arxiv.org/abs/1602.01783." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Asynchronous methods for deep reinforcement learning">
                                        <b>[20]</b>
                                         MNIH V, BADIA A P, MIRZA M, et al.Asynchronous methods for deep reinforcement learning[J/OL].arXiv Preprint, 2016, 2016:arXiv:1603.01783 [2016- 02- 04].https://arxiv.org/abs/1602.01783.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_21" title=" WANG Z, SCHAUL T, HESSEL M, et al.Dueling network architectures for deep reinforcement learning[C]// Proceedings of the 33rd International Conference on International Conference on Machine Learning.New York:JMLR.org, 2016:1995-2003." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dueling network architectures for deep reinforcement learning">
                                        <b>[21]</b>
                                         WANG Z, SCHAUL T, HESSEL M, et al.Dueling network architectures for deep reinforcement learning[C]// Proceedings of the 33rd International Conference on International Conference on Machine Learning.New York:JMLR.org, 2016:1995-2003.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_22" title=" DULAC-ARNOLD G, EVANS R, HASSELT H V.Deep reinforcement learning in large discrete action spaces[J/OL].arXiv Preprint, 2015, 2015:arXiv:1512.07679 [2015- 12- 24].https://arxiv.org/abs/1512.07679." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning in large discrete action spaces">
                                        <b>[22]</b>
                                         DULAC-ARNOLD G, EVANS R, HASSELT H V.Deep reinforcement learning in large discrete action spaces[J/OL].arXiv Preprint, 2015, 2015:arXiv:1512.07679 [2015- 12- 24].https://arxiv.org/abs/1512.07679.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-28 10:49</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1495-1499 DOI:10.11772/j.issn.1001-9081.2018092015            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度强化学习的城市交通信号控制算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%88%92%E5%87%8C%E6%B4%B2&amp;code=41746663&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">舒凌洲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E4%BD%B3&amp;code=25654413&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴佳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%A8&amp;code=11537561&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晨</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0178313&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">电子科技大学信息与软件工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对城市交通信号控制中如何有效利用相关信息优化交通控制并保证控制算法的适应性和鲁棒性的问题, 提出一种基于深度强化学习的交通信号控制算法, 利用深度学习网络构造一个智能体来控制整个区域交通。首先通过连续感知交通环境的状态来选择当前状态下可能的最优控制策略, 环境的状态由位置矩阵和速度矩阵抽象表示, 矩阵表示法有效地抽象出环境中的主要信息并减少了冗余信息;然后智能体以在有限时间内最大化车辆通行全局速度为目标, 根据所选策略对交通环境的影响, 利用强化学习算法不断修正其内部参数;最后, 通过多次迭代, 智能体学会如何有效地控制交通。在微观交通仿真软件Vissim中进行的实验表明, 对比其他基于深度强化学习的算法, 所提算法在全局平均速度、平均等待队长以及算法稳定性方面展现出更好的结果。其中, 与基线相比, 平均速度提高9%, 平均等待队长降低约13.4%。实验结果证明该方法能够适应动态变化的复杂的交通环境。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%A4%E9%80%9A%E4%BF%A1%E5%8F%B7%E6%8E%A7%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">交通信号控制;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    舒凌洲 (1995—) , 男, 四川彭州人, 硕士研究生, 主要研究方向:深度强化学习、智能交通系统;;
                                </span>
                                <span>
                                    *吴佳 (1980—) , 女, 四川成都人, 副教授, 博士, 主要研究方向:深度强化学习、智能交通系统;电子邮箱jiawu@uestc.edu.cn;
                                </span>
                                <span>
                                    王晨 (1995—) , 女, 陕西西安人, 硕士研究生, 主要研究方向:深度强化学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-08</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61503059);</span>
                    </p>
            </div>
                    <h1><b>Urban traffic signal control based on deep reinforcement learning</b></h1>
                    <h2>
                    <span>SHU Lingzhou</span>
                    <span>WU Jia</span>
                    <span>WANG Chen</span>
            </h2>
                    <h2>
                    <span>School of Information and Software Engineering, University of Electronic Science and Technology of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To meet the requirements for adaptivity, and robustness of the algorithm to optimize urban traffic signal control, a traffic signal control algorithm based on Deep Reinforcement Learning (DRL) was proposed to control the whole regional traffic with a control Agent contructed by a deep learning network. Firstly, the Agent predicted the best possible traffic control strategy for the current state by observing continously the state of the traffic environment with an abstract representation of a location matrix and a speed matrix, because the matrix representation method can effectively abstract vital information and reduce redundant information about the traffic environment. Then, based on the impact of the strategy selected on the traffic environment, a reinforcement learning algorithm was employed to correct the intrinsic parameters of the Agent constantly in order to maximize the global speed in a period of time. Finally, after several iterations, the Agent learned how to effectively control the traffic.The experiments in the traffic simulation software Vissim show that compared with other algorithms based on DRL, the proposed algorithm is superior in average global speed, average queue length and stability; the average global speed increases 9% and the average queue length decreases 13.4% compared to the baseline. The experimental results verify that the proposed algorithm can adapt to complex and dynamically changing traffic environment.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=traffic%20signal%20control&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">traffic signal control;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SHU Lingzhou, born in 1995, M. S. candidate. His research interests include deep reinforcement learning, intelligent traffic system. ;
                                </span>
                                <span>
                                    WU Jia, born in 1980, Ph. D. , associate professor. Her research interests include deep reinforcement learning, intelligent traffic system. ;
                                </span>
                                <span>
                                    WANG Chen, born in 1995, M. S. candidate. Her research interests include deep reinforcement learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-08</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61503059);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">城市交通信号控制一直以来是一个具有挑战性的研究课题。由于交通系统的复杂性和动态性, 随着控制范围的扩大, 交通状态信息数据量急剧增加, 控制的复杂度呈指数级增长<citation id="210" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。除此以外, 交通信号控制面临着鲁棒性、适应性等问题同样增加了控制的难度。对此, 部分研究者提出了分布式控制方案, 基于多智能体的方法被广泛用于解决城市交通分布式控制问题<citation id="213" type="reference"><link href="168" rel="bibliography" /><link href="170" rel="bibliography" /><link href="172" rel="bibliography" /><link href="174" rel="bibliography" /><link href="176" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>, 如文献<citation id="211" type="reference">[<a class="sup">4</a>]</citation>利用了遗传算法和强化学习算法来训练多智能体, 文献<citation id="212" type="reference">[<a class="sup">6</a>]</citation>采用基于模糊控制的多智能体算法。然而, 由于以下原因, 交通网络信号控制问题依旧没有得到有效地解决:1) 多智能体之间的协作通信基于预定义的规则, 导致智能体无法快速适应不断变化的交通状况, 因此, 智能体性能缺乏稳定性; 2) 随着控制区域范围增大, 交通状态信息和交通控制方式复杂度陡增, 传统的交通控制方式很难发现交通数据中隐藏的模式, 因此, 优化控制目标难度增加。</p>
                </div>
                <div class="p1">
                    <p id="49">为了解决上述问题, 本文提出一种基于深度强化学习的城市交通信号控制算法。强化学习<citation id="214" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>是一类重要的机器学习技术, 它通过与环境的交互来学习最优的控制决策。交通信号控制领域很早开始运用强化学习方法来解决交通控制问题。智能体以提升车辆平均速度、最小化车辆平均通行时间、减少车辆平均等待队长为目标, 通过观察当前交通状态, 选择最优的交通控制策略<citation id="216" type="reference"><link href="172" rel="bibliography" /><link href="180" rel="bibliography" /><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><link href="186" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。这些方法采用人工提取的特征表示交通状态, 极大地降低了交通状态表示的复杂度。例如, 文献<citation id="215" type="reference">[<a class="sup">9</a>]</citation>选择每个车道等待车辆队长和信号灯时间作为交通信息状态, 通过将数据离散化为不同级别, 达到压缩数据的目的。该方法虽然降低了信息的复杂度, 但丢失了交通状态潜在的重要信息。因此, 单纯基于强化学习的交通控制策略只能应对低维度数据, 一旦交通数据量和复杂度增加, 该方法无法满足城市区域交通信号的精确控制需求。</p>
                </div>
                <div class="p1">
                    <p id="50">深度学习可以很好地解决高维度数据抽象表征问题。受到人脑工作模式的启发, 深度学习将底层特征组合形成更加抽象的高层特征<citation id="217" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。通过与强化学习结合, 即深度强化学习 (Deep Reinforcement Learning, DRL) , DRL能够发掘交通状态信息中隐藏模式, 直接从高维数据中学习到有效的控制策略。近年来部分研究者开始在交通信号控制领域采用深度强化学习技术<citation id="219" type="reference"><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><link href="194" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>, 但大多数研究仅考虑单个交叉路口的交通控制<citation id="220" type="reference"><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。文献<citation id="218" type="reference">[<a class="sup">15</a>]</citation>虽能够实现小型交通网络的控制, 但文中仍是单个智能体控制单个路口交通, 然后利用传统多智能体协调机制来控制交通网络。由于 DRL 算法的训练非常耗时, 因此每个交叉路口由单个DRL智能体控制的交通网络的训练将消耗较长的时间;另外, 作者在文中指出了其智能体在训练过程中稳定性较差, 因此, 对于大型路网来说, 此方法可行性较差。</p>
                </div>
                <div class="p1">
                    <p id="51">本文旨在利用深度强化学习的优势高效地控制城市交通。智能体通过不断地与交通环境进行交互以最大限度提高交通通行效率。在此框架下智能体无需预知交通系统控制的内在规则, 而是通过不断探索新的策略, 根据该策略对环境的影响来学习到最优的控制策略。本文方法主要优点在于:</p>
                </div>
                <div class="p1">
                    <p id="52">1) 单个智能体对交通路网进行全局控制。</p>
                </div>
                <div class="p1">
                    <p id="53">2) 交通网络的状态由位置矩阵和速度矩阵联合表示, 这样能有效抽象出交通路网中的主要信息。与文献<citation id="221" type="reference">[<a class="sup">13</a>,<a class="sup">14</a>]</citation>相比, 矩阵表示法压缩了数据的维度并且缩短了计算时间。</p>
                </div>
                <div class="p1">
                    <p id="54">3) 通过对智能体超参数的调整, 信号控制的稳定性显著提高, 训练时间显著减少。</p>
                </div>
                <div class="p1">
                    <p id="55">4) 所有仿真实验均在著名的微观交通仿真软件Vissim上运行, 实验结果可信度高。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">1 深度强化学习</h3>
                <div class="p1">
                    <p id="57">深度强化学习是一种将具有决策能力的强化学习和具有感知能力的深度学习相结合的技术, 其最早兴起于2015年, 当时被用于教电脑玩视频游戏, 最终达到了与专业人类游戏玩家相媲美的水平<citation id="223" type="reference"><link href="196" rel="bibliography" /><link href="198" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。随后深度强化学习被用于处理一系列具有挑战性的任务, 如机器人控制、语音识别、自然语言处理、视频分析等<citation id="222" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。本章将简要介绍强化学习和深度学习的基本概念。</p>
                </div>
                <h4 class="anchor-tag" id="58" name="58">1.1 <b>强化学习</b></h4>
                <div class="p1">
                    <p id="59">强化学习是一种通过和环境交互, 学习实现某个确定目标所需策略的学习框架<citation id="224" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。它定义了一个学习智能体 (Agent) 与其所在环境的状态, 行为和行为对环境的影响 (奖赏值) , 以在与环境的交互过程中获得最大的累积奖赏值为目标。状态<i>s</i>是Agent对于环境状态的观察;奖赏值 (reward) 是在Agent执行动作后, 行为对环境影响的反馈信号。在每个时间步<i>t</i>, Agent观察到环境状态<i>s</i><sub><i>t</i></sub>, 执行动作<i>a</i><sub><i>t</i></sub>并接收环境的奖赏值<i>r</i><sub><i>t</i></sub>, 同时环境状态更新为新状态<i>s</i><sub><i>t</i>+1</sub>。经过长时间的交互后, Agent获得一系列状态、动作和奖赏值:</p>
                </div>
                <div class="p1">
                    <p id="60"><b><i>H</i></b><sub><i>t</i></sub>={<i>s</i><sub>1</sub>, <i>a</i><sub>1</sub>, <i>r</i><sub>1</sub>, <i>s</i><sub>2</sub>, <i>a</i><sub>2</sub>, <i>r</i><sub>2</sub>, …, <i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>, <i>r</i><sub><i>t</i></sub>}</p>
                </div>
                <div class="p1">
                    <p id="61">策略<i>π</i>是Agent的行为基础, 它是从环境状态到Agent动作的一个映射。Agent在<i>t</i>时刻开始的累积奖赏定义为:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>k</mi></msup><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中:<i>γ</i>是折扣因子, <i>γ</i>∈ (0, 1]; <i>G</i><sub><i>t</i></sub>称为回报 (return) ; Agent的目标是找到最优策略<i>π</i><sup>*</sup>以最大化其获得的奖赏数量, 更确切地说, Agent的目标旨在最大化累积奖赏值的期望。</p>
                </div>
                <div class="p1">
                    <p id="64">状态动作值函数<i>Q</i><sub><i>π</i></sub> (<i>s</i>, <i>a</i>) 是指在当前状态<i>s</i>下依据某个策略<i>π</i>执行动作<i>a</i>后累积回报的期望:</p>
                </div>
                <div class="p1">
                    <p id="65"><i>Q</i><sub><i>π</i></sub> (<i>s</i>, <i>a</i>) =E<sub><i>π</i></sub>[<i>G</i><sub><i>t</i></sub>|<i>S</i><sub><i>t</i></sub>=<i>s</i>, <i>A</i><sub><i>t</i></sub>=<i>a</i>]      (1) </p>
                </div>
                <div class="p1">
                    <p id="66">状态动作值函数<i>Q</i><sub><i>π</i></sub> (<i>s</i>, <i>a</i>) 是对Agent在当先状态<i>s</i>下采取某行为<i>a</i>好坏程度的衡量。最优状态动作值函数<i>Q</i><sup>*</sup> (<i>s</i>, <i>a</i>) 是所有策略中最大动作状态值函数:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>π</mi></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>π</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">由于所有的最优策略都实现了最优的状态动作值函数:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>Q</i><sub><i>π</i><sup>*</sup></sub> (<i>s</i>, <i>a</i>) =<i>Q</i><sup>*</sup> (<i>s</i>, <i>a</i>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="70">那么最优策略可以通过最优状态动作值函数<i>Q</i><sup>*</sup> (<i>s</i>, <i>a</i>) 找到:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>π</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>a</mi><mo stretchy="false">|</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo></mtd><mtd columnalign="left"><mi>a</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mspace width="0.25em" /><mi>Q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中<i>A</i>表示所有可执行动作的集合。最优状态动作值函数遵循贝尔曼最优方程 (Bellman optimality equation) :</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>E</mtext><msub><mrow></mrow><msup><mi>s</mi><mo>′</mo></msup></msub><mo stretchy="false">[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>a</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中, <i>Q</i><sup>*</sup> (<i>s</i>′, <i>a</i>′) 是下一个时间步处于状态<i>s</i>′, 执行动作<i>a</i>′的状态动作值函数最优值。</p>
                </div>
                <div class="p1">
                    <p id="75">在复杂应用场景中由于状态数量或者动作空间过大, 会面临维度灾难问题。因此, 通常采用神经网络等非线性函数逼近器去近似表示状态动作值函数<i>Q</i> (<i>s</i>, <i>a</i>) 。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">1.2 <b>深度学习</b></h4>
                <div class="p1">
                    <p id="77">传统的机器学习方法其性能往往依赖于从原始数据中抽取的特征, 而特征的提取需要设计者具有深厚的专业背景知识及实际操作经验。深度学习能够从原始数据中自动发掘特征, 不需要人工操作。深度学习模型是由多层的非线性单元组合构成, 较低层的输出作为更高一层的输入, 通过多层的抽象学习到数据的特征<citation id="225" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。根据深度学习模型结构不同, 分为卷积神经网络 (<i>Convolutional Neural Network</i>, <i>CNN</i>) 、循环神经网络 (<i>Recurrent Neural Network</i>, <i>RNN</i>) 、深度信念网络 (<i>Deep Belief Network</i>) 等, 其中, <i>CNN</i>是一种特殊结构的深度神经网络模型, 它能够学习到图像的细节特征, 并在更高层对抽象特征进行组合, 从而学习到图形的表征。</p>
                </div>
                <div class="p1">
                    <p id="78">通常为了学习神经网络的参数<i>θ</i>, 需要设定一个代价函数, 该函数衡量了网络的输出<i>y</i>与期望输出<i>t</i> (目标) 之间的误差, 如式 (6) 所示。通过反向传播算法可以求解出满足优化目标的神经网络参数。反向传播算法利用链式求导规则逐层计算代价函数对网络权重<i>θ</i>的梯度, 作为修改<i>θ</i>的依据<sup></sup><citation id="226" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="79"><i>L</i> (<i>θ</i>) =E[ (<i>y</i>-<i>t</i>) <sup>2</sup>]      (6) </p>
                </div>
                <div class="p1">
                    <p id="80">文献<citation id="227" type="reference">[<a class="sup">16</a>]</citation>将CNN与强化学习算法中的Q-learning算法<citation id="228" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>相结合提出了DQN (Deep Q-Network) 算法, 该算法利用经验回放机制打破了样本序列的相关性并提高学习效率。接下来将采用DQN来实现交通信号控制。</p>
                </div>
                <h3 id="81" name="81" class="anchor-tag">2 基于深度强化学习的交通信号控制算法</h3>
                <div class="p1">
                    <p id="82">本章描述如何将<i>DQN</i>算法应用于城市交通信号控制。交通信号控制<i>Agent</i>通过对环境状态的观察选择一系列控制交通信号的动作;交通信号控制的结果作为奖赏值反馈给<i>Agent</i>;<i>Agent</i>根据这些反馈值并利用学习算法来修正<i>Agent</i>的参数。下面首先详细描述环境状态的构造方法、动作集合的设定以及奖赏值函数的设置;接下来对<i>DQN</i>算法的架构进行简要介绍;最后, 对智能体的训练算法进行说明。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">2.1 <b>状态表示</b></h4>
                <div class="p1">
                    <p id="84">图1展示了交通网络状态的矩阵表示方法, 该方法通过两个矩阵分别表示交通网络中车辆的位置信息以及局部平均速度。具体地, 该方法将车道划分为具有定长的多个单元格;每一个单元格对应矩阵中的一个元素;矩阵中每个元素代表着对应单元格中车辆的数量和平均速度。该方法能够从交通网络状态中抽取出主要信息以便智能体作出有效决策。与其他方法相比, 如:文献<citation id="229" type="reference">[<a class="sup">14</a>]</citation>中将整个交通网络运行画面作为环境状态, 矩阵表示法能大幅压缩数据的维度, 去除冗余信息并加快训练速度。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905044_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 环境状态表示" src="Detail/GetImg?filename=images/JSJY201905044_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 环境状态表示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905044_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 1 <i>Representation of environment state</i></p>

                </div>
                <h4 class="anchor-tag" id="86" name="86">2.2 <b>动作集合</b></h4>
                <div class="p1">
                    <p id="87">本文考虑针对四岔交叉路口进行控制 (如图2所示) 。路口冲突交通流为直行南北方向和直行东西方向交通流 (为了描述方便, 不考虑转弯情况) 。根据交通流情况, 信号灯控制器为每个交通流分配一定的绿灯时间。值得注意的是, 本文方法可以对任意形态的交叉口进行控制。</p>
                </div>
                <div class="p1">
                    <p id="88">对于每个交叉口, <i>Agent</i>或者为南北向交通流赋予通行权或者为东西向交通流赋予通行权。假设控制区域包含<i>N</i>个交叉路口, Agent所有可能动作集合的数量为2<sup><i>N</i></sup>。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">2.3 <b>奖赏值函数</b></h4>
                <div class="p1">
                    <p id="90"><i>Agent</i>在时间步<i>t</i>执行动作<i>a</i><sub><i>t</i></sub>后获得环境对该动作的反馈, 即奖赏值<i>r</i><sub><i>t</i></sub>。针对交通控制问题, 定义<i>r</i><sub><i>t</i></sub>为相邻时间步的全局平均速度之差:</p>
                </div>
                <div class="p1">
                    <p id="91"><i>r</i><sub><i>t</i></sub>=<i>a</i> (<i>V</i><sub><i>t</i>+1</sub>-<i>V</i><sub><i>t</i></sub>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="92">其中: <i>V</i><sub><i>t</i></sub>表示时刻<i>t</i>交通路网中所有车辆的平均速度, 即全局平均速度; <i>V</i><sub><i>t</i>+1</sub>表示下一时刻的全局平均速度;<i>a</i>为常量, 确保奖赏值波动范围不会过大。这里通过多次实验设置<i>a</i>=0.1。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">2.4 <b>交通信号控制智能体</b></h4>
                <div class="p1">
                    <p id="94">交通信号控制智能体 (<i>Agent</i>) 由一个深度神经网络构成。该神经网络的输入为<i>Agent</i>对环境的观测值 (2.1节中交通环境状态) ;输出是<i>Agent</i>对于所有合法动作的状态动作值<i>Q</i>。<i>Agent</i>根据该价值函数选择当前状态下的最优动作。这种结构的优势在于给定任意环境状态仅需一次前向传播算法就能够得到所有动作的状态动作值。</p>
                </div>
                <div class="p1">
                    <p id="95"><i>Agent</i>结构由<i>CNN</i>构成。选择<i>CNN</i>构造<i>Agent</i>原因在于<i>CNN</i>能够根据原始输入数据提取出有效的空间特征, 而交通环境状态隐含大量空间特征, 借助<i>CNN</i>的特性, <i>Agent</i>能够挖掘到有效的交通状态特征, 从而学习到更优的策略。<i>Agent</i>结构包含3层卷积层、3层池化层和1层全连接层, 卷积核尺寸分别为4×4、3×3、3×3, 步长为1。隐藏层激活函数采用<i>ReLU</i> (<i>Rectified Linear Units</i>) 函数; 池化层采用尺寸为2×2的最大池化; 输出层为线性全连接层, 对应着有效动作的状态动作值。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">2.5 <b>学习算法</b></h4>
                <div class="p1">
                    <p id="97">本节将详细描述<i>Agent</i>网络参数的训练流程<citation id="230" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。假设<i>θ</i>表示Agent网络参数。在每次迭代中, 神经网络的输出是状态动作值<i>Q</i> (<i>s</i>, <i>a</i>;<i>θ</i>) 。为了训练Agent网络, 需要每次输出对应的目标值 (target value) :</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>a</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">但是该目标值往往难以获取, 因此通常用下式来近似目标值:</p>
                </div>
                <div class="p1">
                    <p id="100"><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>a</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup></mrow></math></mathml>;<i>θ</i><sup>-</sup>) </p>
                </div>
                <div class="p1">
                    <p id="102">其中<i>θ</i><sup>-</sup>表示计算目标值的网络参数。网络的损失函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="103"><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo>, </mo><mi>r</mi><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>a</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup></mrow></math></mathml>;<i>θ</i><sup>-</sup><sub><i>i</i></sub>) -<i>Q</i> (<i>s</i>, <i>a</i>;<i>θ</i><sub><i>i</i></sub>) ) <sup>2</sup>]      (8) </p>
                </div>
                <div class="p1">
                    <p id="106">其中:<i>θ</i><sub><i>i</i></sub>是第<i>i</i>次迭代时网络的参数;<i>θ</i><sup>-</sup><sub><i>i</i></sub>是第<i>i</i>次迭代时, 用于计算目标值的网络参数。在每次迭代优化第<i>i</i>个损失函数<i>L</i><sub><i>i</i></sub> (<i>θ</i><sub><i>i</i></sub>) 时保持之前迭代的参数<i>θ</i><sup>-</sup><sub><i>i</i></sub>不变。接下来, 采用随机梯度下降算法计算损失函数的梯度, 完成梯度反向传播后, <i>θ</i><sup>-</sup><sub><i>i</i></sub>更新为<i>θ</i><sub><i>i</i>-1</sub>。</p>
                </div>
                <div class="p1">
                    <p id="107"><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><msub><mrow></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo>, </mo><mi>r</mi><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>a</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup></mrow></math></mathml>;<i>θ</i><sup>-</sup><sub><i>i</i></sub>) -<i>Q</i> (<i>s</i>, <i>a</i>;<i>θ</i><sub><i>i</i></sub>) ) ᐁ<sub><i>θ</i><sub><i>i</i></sub></sub><i>Q</i> (<i>s</i>, <i>a</i>;<i>θ</i><sub><i>i</i></sub>) ]      (9) </p>
                </div>
                <div class="p1">
                    <p id="110">当使用非线性函数逼近器 (如神经网络) 来近似表示状态动作值函数时, 强化学习表现不稳定, 主要原因<citation id="231" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>在于:采样数据间存在相关性;状态动作函数值微小的改动影响策略的形成, 进而改变样本数据的分布以及状态动作函数与目标值之间的相关性。</p>
                </div>
                <div class="p1">
                    <p id="111">为了解决上述问题, 文献<citation id="232" type="reference">[<a class="sup">16</a>]</citation>提出了经验回放机制 (experience replay) 。将Agent与环境交互的数据存储在回放记忆单元<b><i>D</i></b><sub><i>t</i></sub> = {<i>e</i><sub>1</sub>, <i>e</i><sub>2</sub>, …, <i>e</i><sub><i>t</i></sub>}中, 其中<i>e</i><sub><i>t</i></sub>= (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>, <i>r</i><sub><i>t</i></sub>, <i>s</i><sub><i>t</i>+1</sub>) 。在训练时, 随机从<b><i>D</i></b><sub><i>t</i></sub>中随机抽取小批量的数据。采用这种处理方式与标准的Q-learning算法相比, 具有如下优点:首先, 每一次采样数据都可能反复用于网络权重更新, 进而提高数据利用效率;其次, 由于样本之间的强相关性, 直接从连续样本中学习效率低下, 对样本进行随机化打破了相关性, 因此降低了更新的方差。Agent网络的损失函数重新定义为:</p>
                </div>
                <div class="p1">
                    <p id="112"><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mtext>E</mtext><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo>, </mo><mi>r</mi><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>∼</mo><mi>U</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>a</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup></mrow></math></mathml>;<i>θ</i><sup>-</sup><sub><b><i>i</i></b></sub>) -<b><i>Q</i> (<i>s</i>, <i>a</i></b>;<i>θ</i><sub><b><i>i</i></b></sub>) ) <sup>2</sup>]      (10) </p>
                </div>
                <div class="p1">
                    <p id="115">为了提高学习算法的稳定性, 目标值由单独的网络来生成。每<i>C</i>次迭代后, 将Agent网络<i>Q</i>的参数赋给目标网络<i>Q</i>^ 并使用 <i>Q</i>^ 来生成目标值<citation id="233" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="116">DQN算法采用<i>ε</i>-贪婪算法来探索状态空间。该算法可在探索和利用之前取得平衡。算法原理如下:以概率<i>ε</i>在可选动作集合上随机选择一个动作;否则选择最大状态动作值对应的动作。</p>
                </div>
                <h3 id="117" name="117" class="anchor-tag">3 实验</h3>
                <div class="p1">
                    <p id="118">为了验证本文提出的交通控制算法的有效性, 该控制算法在<i>Vissim</i>平台运行仿真实验。<i>Vissim</i>是一种微观的、基于离散时间的驾驶行为仿真建模工具, 可以准确地模拟城市交通场景并用以城市交通的建模和分析。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119">3.1 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="120">仿真任务场景是一个2×2网格形城市道路网, 如图2所示。所有的交叉路口有着相同的结构且彼此间隔400 <i>m</i>。每个交叉口进入车流量相同, 初始车流量设定为600辆/时 (<i>veh</i>/<i>h</i>) 。假设信号控制器不存在全红灯的缓冲时间, 即在一个相位结束之后立即进入下一个相位。在<i>Agent</i>训练阶段, 每个场景包含150个时间步, 等同于25 <i>min</i>的仿真时间。</p>
                </div>
                <div class="p1">
                    <p id="121">在接下来的实验中, 将通过优化超参数提高<i>Agent</i>性能的稳定性, 并提出能够有效提升智能体性能和稳定性的方法。</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905044_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 2&#215;2交通网络" src="Detail/GetImg?filename=images/JSJY201905044_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 2×2交通网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905044_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 2×2 <i>traffic network</i></p>

                </div>
                <h4 class="anchor-tag" id="123" name="123">3.2 <b>超参数优化</b></h4>
                <div class="p1">
                    <p id="124">训练初始阶段<i>Agent</i>超参数采用常规设定:学习率为10<sup>-4</sup>, 缓存池尺寸为3 000, 折扣因子为0.9, 初始<i>epsilon</i>为0.6, 衰减率为0.99, 最小<i>epsilon</i>为0.01。训练结束后的测试阶段, <i>Agent</i>在重置后的<i>Vissim</i>环境中进行25 <i>min</i>的测试。本文采用全局平均速度作为评估指标, 因为该指标不仅能描述交通网络的状态, 而且和<i>Agent</i>的目标函数, 即累积奖赏值相关。此外, 还引入了平均等待队列长度作为补充评价指标。实验结果如图3所示:<i>Agent</i>1在第50个时间步之后, 连续作出错误决策将导致路网交通状况恶化。接下来, 将通过优化超参数来提升<i>Agent</i>的性能。</p>
                </div>
                <div class="p1">
                    <p id="125">表1展示了4个具有不同超参数的<i>Agent</i>, 其中<i>Agent</i>1～<i>Agent</i>3的训练阶段车流量固定为600 <i>veh</i>/<i>h</i>, 而<i>Agent</i>4的训练阶段车流量在450～550 <i>veh</i>/<i>h</i>间浮动。因为变化的车流量能够增加训练场景的多样性, 进而促使<i>Agent</i>学习到更多的策略。此外, 为了更好地评估本文方法, 引入基线方法 (如图中水平实线所示) 进行对比。基线代表的控制算法见参考文献<citation id="234" type="reference">[<a class="sup">13</a>]</citation>, 该方法使用了<i>DQN</i>算法来实现单路口的交通信号控制, 其中<i>Agent</i>结构为一个小型深度栈式自编码神经网络。交通网络的状态由进入交叉路口的等待队列长度表示;奖赏值函数定义为南北向和东西向等待队长差的绝对值。本文将此方法拓展到路网控制中, 每个路口由一个<i>Agent</i>进行控制, 此方法的平均性能指标作为基线。</p>
                </div>
                <div class="area_img" id="126">
                    <p class="img_tit"><b>表</b>1 <i>DQN Agent</i><b>超参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Hyperparameter settings of DQN Agents</i></p>
                    <p class="img_note"></p>
                    <table id="126" border="1"><tr><td><br /><i>Agent</i></td><td><i>Replay size</i></td><td><i>Epsilon</i></td><td><i>Train steps</i></td><td><i>Flow rate</i></td></tr><tr><td><br /><i>Agent</i>1</td><td>3 000</td><td>0.6</td><td>3 000</td><td>600</td></tr><tr><td><br /><i>Agent</i>2</td><td>1 000</td><td>0.6</td><td>3 000</td><td>600</td></tr><tr><td><br /><i>Agent</i>3</td><td>1 000</td><td>0.9</td><td>3 000</td><td>600</td></tr><tr><td><br /><i>Agent</i>4</td><td>1 000</td><td>0.9</td><td>4 500</td><td><i>mix</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:<i>mix</i>表示训练阶段浮动车流量。</p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="127">在训练阶段采用<i>Xavier</i>方法<citation id="235" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>对<i>Agent</i>的神经网络权重初始化。实验结果如图3所示, 从图中可以看出<i>Agent</i>3和<i>Agent</i>4能够有效控制交通信号且性能较稳定, 即减小经验回放缓存池大小以及时淘汰旧有的经验利于<i>Agent</i>的学习。此外, 注意到<i>Agent</i>4的性能略优于<i>Agent</i>3, 相较于基线方法其全局平均速度提升了约9%, 平均等待队长降低约13.4% (见表2) 。<i>Agent</i>3和<i>Agent</i>4的区别在于训练阶段<i>Agent</i>4在车流量波动范围更大的环境中训练。由此可见, 车流量对于<i>Agent</i>最终的表现和稳定性有着重要的影响。</p>
                </div>
                <div class="area_img" id="128">
                    <p class="img_tit"><b>表</b>2 <i>Agent</i>3～ <i>Agent</i>5<b>性能指标比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Performance index comparison of Agent</i>3～<i>Agent</i>5</p>
                    <p class="img_note"></p>
                    <table id="128" border="1"><tr><td><br /><i>Agent</i></td><td>全局平均速度/ (<i>km</i>·<i>h</i><sup>-1</sup>) </td><td>队列长度总和/<i>veh</i></td></tr><tr><td><br /><i>Agent</i>3</td><td>31.83</td><td>28.84</td></tr><tr><td><br /><i>Agent</i>4</td><td>33.44</td><td>29.07</td></tr><tr><td><br /><i>Agent</i>5</td><td>34.38</td><td>21.53</td></tr><tr><td><br />基线</td><td>30.68</td><td>33.60</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905044_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Agent1～Agent4性能评估" src="Detail/GetImg?filename=images/JSJY201905044_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>Agent</i>1～<i>Agent</i>4性能评估  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905044_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Performance evaluation of Agent</i>1～<i>Agent</i>4</p>

                </div>
                <h4 class="anchor-tag" id="130" name="130">3.3 <b>性能提升</b></h4>
                <div class="p1">
                    <p id="131">在本节中将在<i>Agent</i>4的基础上通过改变车流量范围进一步提升<i>Agent</i>性能。经过多次实验发现, <i>Agent</i>5在训练时缩小车流量范围至550～650 <i>veh</i>/<i>h</i>, 其在测试阶段性能优于<i>Agent</i>4。实验结果表明 (表2) , <i>Agent</i>5平均速度增长约2%。究其原因在于更大的车流量范围增加了交通状态的复杂度, 使得<i>Agent</i>训练难度增加, 最终导致了<i>Agent</i>性能不稳定, 增加神经网络的深度和每层节点数可解决该问题。在原有测试之上本文为<i>Agent</i>5增加了低车流量场景 (300 <i>veh</i>/<i>h</i>) 测试任务 (如图4所示) , 实验表明在高流量环境中训练仍能够在低流量环境中取得良好表现。</p>
                </div>
                <h3 id="132" name="132" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="133">随着城市交通状况的复杂性增长, 交通状态中的隐藏模式难以发现。深度学习提供了从高维数据中挖掘隐藏模式的有效方法。通过与强化学习算法结合, 为城市交通控制提供了解决方案。本文提出了一种基于深度强化学习的交通网络交通控制方法, 该方法通过与交通环境交互, 连续地感知交通环境的状态并挖掘其中隐藏模式, 进而找到当前状态下可能的最优控制策略。实验结果表明, 该方法能有效控制城市交通, 提升交通通行效率, 但是此方法依然存在一定的局限性, 如随着控制范围的扩大、交叉口数量的增加带来动作空间的陡增;由于深度学习与强化学习结合带来的训练困难等问题。在今后研究中, 将考虑从以下方面进一步优化算法:</p>
                </div>
                <div class="p1">
                    <p id="134">1) 利用文献<citation id="236" type="reference">[<a class="sup">19</a>]</citation>提出的<i>Wolpertinger</i> 框架解决随着控制范围扩大出现的动作空间指数增长的问题;</p>
                </div>
                <div class="p1">
                    <p id="135">2) 随着城市交通状况的复杂性增加, 训练时间将大幅增加。为了减少训练时间, 提升训练效果, 拟考虑更先进的深度强化学习技术, 如<i>Asynchronous Advantage Actor</i>-<i>Critic A</i>3<i>C</i><citation id="237" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>或<i>Dueling</i>-<i>DQN</i>算法<citation id="238" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905044_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 智能体在不同流量环境中的性能" src="Detail/GetImg?filename=images/JSJY201905044_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 智能体在不同流量环境中的性能  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905044_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Performance of Agent in different flowrates</i></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="166">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201004074&amp;v=MDY3Mjk5SE1xNDlDWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVidktMejdCZDdHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 李颖宏, 王力, 尹怡欣.区域交通信号系统节点分析及优化策略研究[J].计算机应用, 2010, 30 (4) :1107-1109. (LI Y H, WANG L, YIN Y X.Node analysis and optimization strategy for regional traffic network system [J].Journal of Computer Applications, 2010, 30 (4) :1107-1109.) 
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Na?ve Bayesian Classifier for Rand IEEE Conference on Decision and Control">

                                <b>[2]</b> CHIU S, CHAND S.Self-organizing traffic control via fuzzy logic[C]// Proceedings of the 32nd IEEE Conference on Decision and Control.Piscataway, NJ:IEEE, 1994:1897-1902.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy sets in distributed traffic control">

                                <b>[3]</b> NAKAMITI G, GOMIDE F.Fuzzy sets in distributed traffic control[C]// Proceedings of IEEE 5th International Fuzzy Systems.Piscataway, NJ:IEEE, 1996:1617-1623.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Genetic reinforcement learning for cooperative traffic signal control">

                                <b>[4]</b> MIKAMI S, KAKAZU Y.Genetic reinforcement learning for cooperative traffic signal control[C]// Proceedings of the 1st IEEE Conference on Evolutionary Computation.Piscataway, NJ:IEEE, 1994:223-228.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJIX&amp;filename=SJIX1291BBCA9555132F34352F7E84D2B394&amp;v=MjQzNzBwaHpMcTl4S289TmlmQ2RySzZGOUMrM2Z3MGJlNEtDWDA2eldBUTdqeDRTZ25sMlJveERiRG1Sck9iQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> MANIKONDA V, LEVY R, SATAPATHY G, et al.Autonomous Agents for traffic simulation and control[J].Transportation Research Record Journal of the Transportation Research Board, 2001, 1774 (1) :1-10.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed and cooperative fuzzy controllers for traffic intersections group">

                                <b>[6]</b> LEE J H, LEE-KWANG H.Distributed and cooperative fuzzy controllers for traffic intersections group[J].IEEE Transactions on Systems, Man &amp; Cybernetics Part C:Applications &amp; Reviews, 1999, 29 (2) :263-271.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning: An Introduction">

                                <b>[7]</b> SUTTON R S, BARTO A G.Reinforcement learning:an introduction[J].IEEE Transactions on Neural Networks, 1998, 9 (5) :1054-1054.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Arterial traffic control using reinforcement learning agents and information from adjacent intersections in the state and reward structure">

                                <b>[8]</b> MEDINA J C, HAJBABAIE A, BENEKOHAL R F.Arterial traffic control using reinforcement learning Agents and information from adjacent intersections in the state and reward structure[C]// Proceedings of the 13th International IEEE Conference on Intelligent Transportation Systems.Piscataway, NJ:IEEE, 2010:525-530.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning with function approximation for traffic signal control">

                                <b>[9]</b> PRASHANTH L A, BHATNAGAR S.Reinforcement learning with function approximation for traffic signal control[J].IEEE Transactions on Intelligent Transportation Systems, 2011, 12 (2) :412-421.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCE&amp;filename=SJCEF267B2AA651A735003FE63622189ACC5&amp;v=MTUzNTBVMDV0cGh6THE5eEtvPU5pZklhOFc2R05hK3JmNDBZdTRPZlhzNnloWVQ2VWtJVG56a3JoQTBjYnZsTnNtYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> ABDULHAI B, PRINGLE R, KARAKOULAS G J.Reinforcement learning for true adaptive traffic signal control[J].Journal of Transportation Engineering, 2003, 129 (3) :278-285.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100626986&amp;v=MTUwMDZud1plWnRGaW5sVXIzSUtGd1diaEE9TmlmT2ZiSzdIdERPcm85Rll1a0pCWFEvb0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> BINGHAM E.Reinforcement learning in neurofuzzy traffic signal control[J].European Journal of Operational Research, 2001, 131 (2) :232-241.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Learning,&amp;quot;">

                                <b>[12]</b> LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201603003&amp;v=MzExMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVidktQeW5EYkxHNEg5Zk1ySTlGWjRRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> LI L, LYU Y S, WANG F Y.Traffic signal timing via deep reinforcement learning[J].IEEE/CAA Journal of Automatica Sinica, 2016, 3 (3) :247-254.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Traffic light control using deep policygradient and value-function-based reinforcement learning">

                                <b>[14]</b> MOUSAVI S S, SCHUKAT M, HOWLEY E.Traffic light control using deep policy-gradient and value-function-based reinforcement learning[J].IET Intelligent Transport Systems, 2017, 11 (7) :417-423.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning for coordination in traffic light control">

                                <b>[15]</b> van der POL E.Deep reinforcement learning for coordination in traffic light control[D].Amsterdam:University of Amsterdam, 2016:1-56.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Playing Atari with deep reinforcement learning[OL]">

                                <b>[16]</b> MNIH V, KAVUKCUOGLU K, SILVER D, et al.Playing atari with deep reinforcement learning[J/OL].arXiv Preprint, 2013, 2013:arXiv:1312.5602 [2013- 12- 09].https://arxiv.org/abs/1312.5602.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human-Level control through deep reinforcement learning">

                                <b>[17]</b> MNIH V, KAVUKCUOGLU K, SILVER D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning:an overview">

                                <b>[18]</b> LI Y X.Deep reinforcement learning:an overview[J/OL].arXiv Preprint, 2017, 2017:arXiv:1701.07274 [2017- 01- 25].https://arxiv.org/abs/1701.07274.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning in large discrete action spaces">

                                <b>[19]</b> DULACARNOLD G, EVANS R, SUNEHAG P, et al.Reinforcement learning in large discrete action spaces[J/OL].arXiv Preprint, 2016, 2016:arXiv:1603.06861 [2016- 03- 22].https://arxiv.org/abs/1603.06861.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Asynchronous methods for deep reinforcement learning">

                                <b>[20]</b> MNIH V, BADIA A P, MIRZA M, et al.Asynchronous methods for deep reinforcement learning[J/OL].arXiv Preprint, 2016, 2016:arXiv:1603.01783 [2016- 02- 04].https://arxiv.org/abs/1602.01783.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dueling network architectures for deep reinforcement learning">

                                <b>[21]</b> WANG Z, SCHAUL T, HESSEL M, et al.Dueling network architectures for deep reinforcement learning[C]// Proceedings of the 33rd International Conference on International Conference on Machine Learning.New York:JMLR.org, 2016:1995-2003.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning in large discrete action spaces">

                                <b>[22]</b> DULAC-ARNOLD G, EVANS R, HASSELT H V.Deep reinforcement learning in large discrete action spaces[J/OL].arXiv Preprint, 2015, 2015:arXiv:1512.07679 [2015- 12- 24].https://arxiv.org/abs/1512.07679.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905044" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905044&amp;v=MDg5NDI3QmQ3RzRIOWpNcW85QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VYnZLTHo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
