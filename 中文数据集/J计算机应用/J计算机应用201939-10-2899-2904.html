<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136511448721250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910019%26RESULT%3d1%26SIGN%3dkvE6ZpUjl7ZTTUpxHu2qZN0C%252fJw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910019&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910019&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910019&amp;v=MDAwNDFyQ1VSN3FmWnVac0Z5dm1WN3JKTHo3QmQ3RzRIOWpOcjQ5RWJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#55" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="1.1 &lt;b&gt;传统插值算法&lt;/b&gt;">1.1 <b>传统插值算法</b></a></li>
                                                <li><a href="#68" data-title="1.2 &lt;b&gt;基于学习的去马赛克算法&lt;/b&gt;">1.2 <b>基于学习的去马赛克算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="2 本文算法 ">2 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="2.1 &lt;b&gt;算法理论&lt;/b&gt;">2.1 <b>算法理论</b></a></li>
                                                <li><a href="#83" data-title="2.2 &lt;b&gt;网络结构&lt;/b&gt;">2.2 <b>网络结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="3.1 &lt;b&gt;训练数据预处理&lt;/b&gt;">3.1 <b>训练数据预处理</b></a></li>
                                                <li><a href="#109" data-title="3.2 &lt;b&gt;评价指标&lt;/b&gt;">3.2 <b>评价指标</b></a></li>
                                                <li><a href="#114" data-title="3.3 &lt;b&gt;参数设定及训练分析&lt;/b&gt;">3.3 <b>参数设定及训练分析</b></a></li>
                                                <li><a href="#118" data-title="3.4 &lt;b&gt;实验结果及分析&lt;/b&gt;">3.4 <b>实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="图1 Bayer模式CFA">图1 Bayer模式CFA</a></li>
                                                <li><a href="#60" data-title="图2 RGB-NIR图像的CFA">图2 RGB-NIR图像的CFA</a></li>
                                                <li><a href="#85" data-title="图3 &lt;i&gt;VDRDN&lt;/i&gt;结构">图3 <i>VDRDN</i>结构</a></li>
                                                <li><a href="#90" data-title="图4 残差稠密块">图4 残差稠密块</a></li>
                                                <li><a href="#107" data-title="图5 训练集数据制作">图5 训练集数据制作</a></li>
                                                <li><a href="#117" data-title="图6 &lt;i&gt;VDRDN&lt;/i&gt;和&lt;i&gt;SRCNN&lt;/i&gt;的训练效率对比">图6 <i>VDRDN</i>和<i>SRCNN</i>的训练效率对比</a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;客观评价指标&lt;/b&gt;"><b>表</b>1 <b>客观评价指标</b></a></li>
                                                <li><a href="#128" data-title="图7 主观结果对比">图7 主观结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="164">


                                    <a id="bibliography_1" title="BAYER B E.Color imaging array:US3971065[P].197-07-20." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Color imaging array">
                                        <b>[1]</b>
                                        BAYER B E.Color imaging array:US3971065[P].197-07-20.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_2" title="RAMANATH R,SNYDER W E,BILBRO G L,et al.Demosaicking methods for Bayer color arrays[J].Journal of Electronic Imaging,2002,11(3):306-315." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Demosaicking methods for bayer color arrays">
                                        <b>[2]</b>
                                        RAMANATH R,SNYDER W E,BILBRO G L,et al.Demosaicking methods for Bayer color arrays[J].Journal of Electronic Imaging,2002,11(3):306-315.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_3" title="TERANAKA H,MONNO Y,TANAKA M,et al.Single-sensor RGB and NIR image acquisition:toward optimal performance by taking account of CFA pattern,demosaicking,and color correction[EB/OL].[2019-01-10].http://www.ok.sc.e.titech.ac.jp/res/MSI/RGB-NIR/ei2016.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-sensor RGB and NIR image acquisition:toward optimal performance by taking account of CFA pattern,demosaicking,and color correction">
                                        <b>[3]</b>
                                        TERANAKA H,MONNO Y,TANAKA M,et al.Single-sensor RGB and NIR image acquisition:toward optimal performance by taking account of CFA pattern,demosaicking,and color correction[EB/OL].[2019-01-10].http://www.ok.sc.e.titech.ac.jp/res/MSI/RGB-NIR/ei2016.pdf.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_4" title="MONNO Y,TANAKA M,OKUTOMI M.N-to-SRGB mapping for single-sensor multispectral imaging[C]//Proceedings of the 2015IEEE International Conference on Computer Vision Workshops.Piscataway:IEEE,2015:66-73." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=N-to-SRGB mapping for single-sensor multispectral imaging">
                                        <b>[4]</b>
                                        MONNO Y,TANAKA M,OKUTOMI M.N-to-SRGB mapping for single-sensor multispectral imaging[C]//Proceedings of the 2015IEEE International Conference on Computer Vision Workshops.Piscataway:IEEE,2015:66-73.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_5" title="GRIBBON K T,BAILEY D G.A novel approach to real-time bilinear interpolation[C]//Proceedings of the 2nd IEEE International Workshop on Electronic Design,Test and Applications.Piscataway:IEEE,2004:126-131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Novel Approach to Real-Time Bilinear Interpolation">
                                        <b>[5]</b>
                                        GRIBBON K T,BAILEY D G.A novel approach to real-time bilinear interpolation[C]//Proceedings of the 2nd IEEE International Workshop on Electronic Design,Test and Applications.Piscataway:IEEE,2004:126-131.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_6" title="MIAO L,QI H,RAMANATH R,et al.Binary tree-based generic demosaicking algorithm for multispectral filter arrays[J].IEEETransactions on Image Processing,2006,15(11):3550-3558." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Binary tree-based generic demosaicking algorithm for multispectral filter arrays">
                                        <b>[6]</b>
                                        MIAO L,QI H,RAMANATH R,et al.Binary tree-based generic demosaicking algorithm for multispectral filter arrays[J].IEEETransactions on Image Processing,2006,15(11):3550-3558.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_7" title="AGGARWAL H K,MAJUMDAR A.Single-sensor multi-spectral image demosaicing algorithm using learned interpolation weights[C]//Proceedings of the 2014 IEEE Geoscience and Remote Sensing Symposium.Piscataway:IEEE,2014:2011-2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-sensor multi-spectral image demosaicing algorithm using learned interpolation weights">
                                        <b>[7]</b>
                                        AGGARWAL H K,MAJUMDAR A.Single-sensor multi-spectral image demosaicing algorithm using learned interpolation weights[C]//Proceedings of the 2014 IEEE Geoscience and Remote Sensing Symposium.Piscataway:IEEE,2014:2011-2014.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_8" title="MARTINELLO M,WAJS A,QUAN S,et al.Dual aperture photography:Image and depth from a mobile camera[C]//Proceedings of the 2015 IEEE International Conference on Computational Photography.Piscataway:IEEE,2015:1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dual aperture photography:Image and depth from a mobile camera">
                                        <b>[8]</b>
                                        MARTINELLO M,WAJS A,QUAN S,et al.Dual aperture photography:Image and depth from a mobile camera[C]//Proceedings of the 2015 IEEE International Conference on Computational Photography.Piscataway:IEEE,2015:1-10.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_9" title="MONNO Y,KIKU D,TANAKA M,et al.Adaptive residual interpolation for color and multispectral image demosaicking[J].Sensors,2017,17(12):2787-2807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive residual interpolation for color and multispectral image demosaicking">
                                        <b>[9]</b>
                                        MONNO Y,KIKU D,TANAKA M,et al.Adaptive residual interpolation for color and multispectral image demosaicking[J].Sensors,2017,17(12):2787-2807.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_10" title="LIU W,ANGUELOV D,ERHAN D,et al.SSD:Single Shot multibox Detector[C]//Proceedings of the 2016 European Conference on Computer Vision,LNCS 9905.Cham:Springer,2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">
                                        <b>[10]</b>
                                        LIU W,ANGUELOV D,ERHAN D,et al.SSD:Single Shot multibox Detector[C]//Proceedings of the 2016 European Conference on Computer Vision,LNCS 9905.Cham:Springer,2016:21-37.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_11" title="MASI I,RAWLS S,MEDIONI G,et al.Pose-aware face recognition in the wild[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:4838-4846." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose-Aware Face Recognition in the Wild">
                                        <b>[11]</b>
                                        MASI I,RAWLS S,MEDIONI G,et al.Pose-aware face recognition in the wild[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:4838-4846.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_12" title="WANG L,XIONG Y,WANG Z,et al.Temporal segment networks:towards good practices for deep action recognition[C]//Proceedings of the European Conference on Computer Vision,LNCS 9912.Cham:Springer,2016:20-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Temporal segment networks:towards good practices for deep action recognition">
                                        <b>[12]</b>
                                        WANG L,XIONG Y,WANG Z,et al.Temporal segment networks:towards good practices for deep action recognition[C]//Proceedings of the European Conference on Computer Vision,LNCS 9912.Cham:Springer,2016:20-36.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_13" title="YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">
                                        <b>[13]</b>
                                        YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_14" title="GHARBI M,CHAURASIA G,PARIS S,et al.Deep joint demosaicking and denoising[J].ACM Transactions on Graphics,2016,35(6):191-202." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM51E2FC729BD6CE17D784F12A15993CF5&amp;v=MTg0NjdIWWZPR1FsZkJyTFUwNXRwaHg3dTd4YWs9TmlmSVk3YTVhOU82M0loSGJabDdDZzlNemhGbjdUZDVQbjdnM1JNd2NMdVhOc3lhQ09OdkZTaVdXcjdKSUZwbWFCdQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        GHARBI M,CHAURASIA G,PARIS S,et al.Deep joint demosaicking and denoising[J].ACM Transactions on Graphics,2016,35(6):191-202.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_15" title="TAN R,ZHANG K,ZUO W,et al.Color image demosaicking via deep residual learning[C]//Proceedings of the 2017 IEEE International Conference on Multimedia and Expo.Piscataway:IEEE,2017:793-798." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Color image demosaicking via deep residual learning">
                                        <b>[15]</b>
                                        TAN R,ZHANG K,ZUO W,et al.Color image demosaicking via deep residual learning[C]//Proceedings of the 2017 IEEE International Conference on Multimedia and Expo.Piscataway:IEEE,2017:793-798.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_16" title="HU X,HEIDE F,DAI Q,et al.Convolutional sparse coding for RGB+NIR imaging[J].IEEE Transactions on Image Processing,2018,27(4):1611-1625." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional sparse coding for RGB+NIR imaging">
                                        <b>[16]</b>
                                        HU X,HEIDE F,DAI Q,et al.Convolutional sparse coding for RGB+NIR imaging[J].IEEE Transactions on Image Processing,2018,27(4):1611-1625.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_17" title="SHOPOVSKA I,JOVANOV L,PHILIPS W.RGB-NIR demosaicing using deep residual U-net[C]//Proceedings of the 26th Telecommunications Forum.Piscataway:IEEE,2018:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RGB-NIR demosaicing using deep residual U-net">
                                        <b>[17]</b>
                                        SHOPOVSKA I,JOVANOV L,PHILIPS W.RGB-NIR demosaicing using deep residual U-net[C]//Proceedings of the 26th Telecommunications Forum.Piscataway:IEEE,2018:1-4.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_18" title="ZHANG Y,TIAN Y,KONG Y,et al.Residual dense network for image super-resolution[C]//Proceedings of the 2018 IEEE/CVFConference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2472-2481." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Residual dense network for image super-resolution">
                                        <b>[18]</b>
                                        ZHANG Y,TIAN Y,KONG Y,et al.Residual dense network for image super-resolution[C]//Proceedings of the 2018 IEEE/CVFConference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2472-2481.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_19" title="TANG H,ZHANG X,ZHUO S,et al.High resolution photography with an RGB-infrared camera[C]//Proceedings of the 2015IEEE International Conference on Computational Photography.Piscataway:IEEE,2015:1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High resolution photography with an RGB-infrared camera">
                                        <b>[19]</b>
                                        TANG H,ZHANG X,ZHUO S,et al.High resolution photography with an RGB-infrared camera[C]//Proceedings of the 2015IEEE International Conference on Computational Photography.Piscataway:IEEE,2015:1-10.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_20" title="BROWN M,S&lt;image id=&quot;248&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910019_24800.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;SSTRUNK S.Multi-spectral SIFT for scene category recognition[C]//Proceedings of the 2011 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2011:177-184." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Multi-Spectral SIFT for Scene Category Recognition,&amp;quot;">
                                        <b>[20]</b>
                                        BROWN M,S&lt;image id=&quot;248&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910019_24800.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;SSTRUNK S.Multi-spectral SIFT for scene category recognition[C]//Proceedings of the 2011 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2011:177-184.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_21" title="SORIA X,SAPPA A D,AKBARINIA A.Multispectral singlesensor RGB-NIR imaging:new challenges and opportunities[C]//Proceedings of the 7th International Conference on Image Processing Theory,Tools and Applications.Piscataway:IEEE,2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multispectral singlesensor RGB-NIR imaging:new challenges and opportunities">
                                        <b>[21]</b>
                                        SORIA X,SAPPA A D,AKBARINIA A.Multispectral singlesensor RGB-NIR imaging:new challenges and opportunities[C]//Proceedings of the 7th International Conference on Image Processing Theory,Tools and Applications.Piscataway:IEEE,2017:1-6.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_22" title="FREDEMBACH C,S&lt;image id=&quot;247&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910019_24700.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;SSTRUNK S.Colouring the near-infrared[EB/OL].[2019-01-10].https://infoscience.epfl.ch/record/129419/files/IR_colour.pdf?version=2." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Colouring the near-infrared">
                                        <b>[22]</b>
                                        FREDEMBACH C,S&lt;image id=&quot;247&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910019_24700.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;SSTRUNK S.Colouring the near-infrared[EB/OL].[2019-01-10].https://infoscience.epfl.ch/record/129419/files/IR_colour.pdf?version=2.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_23" title="VALADA A,OLIVEIRA G L,BROX T,et al.Deep multispectral semantic scene understanding of forested environments using multimodal fusion[C]//Proceedings of the 2016 International Symposium on Experimental Robotics.Cham:Springer,2016:465-477." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep multispectral semantic scene understanding of forested environments using multimodal fusion">
                                        <b>[23]</b>
                                        VALADA A,OLIVEIRA G L,BROX T,et al.Deep multispectral semantic scene understanding of forested environments using multimodal fusion[C]//Proceedings of the 2016 International Symposium on Experimental Robotics.Cham:Springer,2016:465-477.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_24" title="MONNO Y,TERANAKA H,YOSHIZAKI K,et al.Single-sensor RGB-NIR imaging:high-quality system design and prototype implementation[J].IEEE Sensors Journal,2019,19(2):497-507." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-sensor RGB-NIR imaging:high-quality system design and prototype implementation">
                                        <b>[24]</b>
                                        MONNO Y,TERANAKA H,YOSHIZAKI K,et al.Single-sensor RGB-NIR imaging:high-quality system design and prototype implementation[J].IEEE Sensors Journal,2019,19(2):497-507.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_25" title="WANG Z,BOVIK A C,SHEIKH H R,et al.Image quality assessment:from error visibility to structural similarity[J].IEEETransactions on Image Processing,2004,13(4):600-612." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">
                                        <b>[25]</b>
                                        WANG Z,BOVIK A C,SHEIKH H R,et al.Image quality assessment:from error visibility to structural similarity[J].IEEETransactions on Image Processing,2004,13(4):600-612.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_26" >
                                        <b>[26]</b>
                                    DONG C,LOY C C,HE K,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2899-2904 DOI:10.11772/j.issn.1001-9081.2019040614            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的彩色以及近红外图像去马赛克</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E9%95%BF%E6%B1%9F&amp;code=42897032&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢长江</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%99%93%E6%95%8F&amp;code=09800851&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨晓敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%A5%E6%96%8C%E5%AE%87&amp;code=08736216&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">严斌宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8A%A6%E7%92%90&amp;code=42052306&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">芦璐</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0054367&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川大学电子信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>单传感器捕获的彩色-近红外(RGB-NIR)图像存在光谱干扰,从而导致重建出的标准彩色图像(RGB)图像与近红外(NIR)图像存在色彩失真以及细节信息模糊。针对这个问题提出一种基于深度学习的去马赛克方法,通过引入跳远连接与稠密连接解决了梯度消失和梯度弥散问题,使得网络更容易训练,并且提升了网络的拟合能力。首先,用浅层特征提取层提取了马赛克图像的像素相关性以及通道相关性等低级特征;然后,将得到的浅层特征图输入到连续多个的残差稠密块以提取专门针对去马赛克的高级语义特征;其次,为充分利用低级特征与高级特征,将多个残差稠密块提取到的特征进行组合;最后,通过全局跳远连接恢复最终的RGB-NIR图像。在深度学习框架Tensorflow上使用公共的图像与视觉表示组(IVRG)数据集、有植被的户外多光谱图像(OMSIV)数据集和森林(Forest)三个公开数据集进行实验。实验结果表明,所提方法优于基于多级自适应残差插值、基于卷积卷积和神经神经网络以及基于深度残差U型网络的主流的RGB-NIR图像去马赛克方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BD%A9%E8%89%B2-%E8%BF%91%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彩色-近红外图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8E%BB%E9%A9%AC%E8%B5%9B%E5%85%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">去马赛克;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE%E7%A8%A0%E5%AF%86%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差稠密网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%B3%E8%BF%9C%E8%BF%9E%E6%8E%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跳远连接;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%A0%E5%AF%86%E8%BF%9E%E6%8E%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稠密连接;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    谢长江(1994—),男,四川巴中人,硕士研究生,主要研究方向:图像处理、机器学习;;
                                </span>
                                <span>
                                    杨晓敏(1980—),女,四川成都人,教授,博士,主要研究方向:图像处理、机器学习;;
                                </span>
                                <span>
                                    *严斌宇(1975—),男,四川成都人,副教授,博士,主要研究方向:移动传感器网络、网络安全;电子邮箱yby@scu.edu.cn;
                                </span>
                                <span>
                                    芦璐(1990—),男,四川成都人,博士,主要研究方向:自适应信号处理、核方法、分布式估计。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-15</p>

            </div>
                    <h1><b>RGB-NIR image demosaicing based on deep learning</b></h1>
                    <h2>
                    <span>XIE Changjiang</span>
                    <span>YANG Xiaomin</span>
                    <span>YAN Binyu</span>
                    <span>LU Lu</span>
            </h2>
                    <h2>
                    <span>College of Electronics and Information Engineering, Sichuan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Spectral interference in Red Green Blue-Near InfRared(RGB-NIR) images captured by single sensor results in colour distortion and detail information ambiguity of the reconstructed standard Red Green Blue(RBG) and Near InfRared(NIR) images. To resolve this problem, a demosaicing method based on deep learning was proposed. In this method, the grandient dppearance and dispersion problems were solved by introducing long jump connection and dense connection, the network was easier to be trained, and the fitting ability of the network was improved. Firstly, the low-level features such as pixel correlation and channel correlation of the mosaic image were extracted by the shallow feature extraction layer. Secondly, the obtained shallow feature graph was input into successive and multiple residual dense blocks to extract the high-level semantic features aiming at the demosaicing. Thirdly, to make full use of the low-level features and high-level features, the features extracted by multiple residual dense blocks were combined. Finally, the RGB-NIR image was reconstructed by the global long jump connection. Experiments were performed on the deep learning framework Tensorflow using three public data sets, the Common Image and Visual Representation Group(IVRG) dataset, the Outdoor Multi-Spectral Images with Vegetation(OMSIV) dataset, and the Forest dataset. The experimental results show that the proposed method is superior to the RGB-NIR image demosaicing methods based on multi-level adaptive residual interpolation, convolutional neural network and deep residual U-shaped network.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Red%20Green%20Blue-Near%20InfRared(RGB-NIR)%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Red Green Blue-Near InfRared(RGB-NIR) image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=demosaicing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">demosaicing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=residual%20dense%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">residual dense network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=long%20jump%20connection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">long jump connection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dense%20connection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dense connection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XIE Changjiang,born in 1994,M.S.candidate.His research interests include image processing,machine learning.;
                                </span>
                                <span>
                                    YANG Xiaomin,born in 1980,Ph.D.,professor.Her research interests include image processing,machine learning.;
                                </span>
                                <span>
                                    YAN Binyu,born in 1975,Ph.D.,associate professor,His research interests include mobile sensor network,network security.;
                                </span>
                                <span>
                                    LU Lu,born in 1990,Ph.D.His research interests include adaptive signal processing,kernel methods,distributed estimation.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-15</p>
                            </div>


        <!--brief start-->
                        <h3 id="55" name="55" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="56">近些年来,在图像处理领域对于彩色(Red Green Blue, RGB)图像及其对应的近红外(Near Infrared, NIR)图像的应用变得越来越多,比如:图像融合、行人检测以及人脸识别。同时捕获彩色图像及其红外图像,通常需要两个传感器,然后对捕获到的图像进行配准。虽然得到了成对的彩色图像及红外图像,但是额外引入了配准这一难题。</p>
                </div>
                <div class="p1">
                    <p id="57">随着传感器技术的发展,使得用单传感器同时捕获彩色图像与红外图像成为了可能。这不仅解决了两个传感器的成本问题,同时也解决了双传感器带来的配准问题。单传感器在每一个像素位置,根据相应的彩色滤波阵列(Color Filter Array, CFA)捕获单一像素,得到的图像为彩色-近红外(Red Green Blue-Near InfRared, RGB-NIR)图像。</p>
                </div>
                <div class="p1">
                    <p id="58">目前大部分的相机通常采用拜尔(Bayer)模式的CFA<citation id="216" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,如图1所示。对红色(Red, R)、绿色(Green, G)、蓝色(Blue, B)三个颜色通道中的一个进行捕获,然后根据插值算法推断出未捕获到的颜色通道的值,这个过程被称为去马赛克<citation id="217" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。类似于拜尔模式的CFA,专门针对RGB-NIR图像的CFA<citation id="218" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,也相应出现,具体结构如图2所示。</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910019_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Bayer模式CFA" src="Detail/GetImg?filename=images/JSJY201910019_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Bayer模式CFA  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910019_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Bayer pattern color filter array</p>

                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910019_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 RGB-NIR图像的CFA" src="Detail/GetImg?filename=images/JSJY201910019_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 RGB-NIR图像的CFA  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910019_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Color filter array of RGB-NIR image</p>

                </div>
                <div class="p1">
                    <p id="61">与拜尔模式的CFA相比,RGB-NIR的CFA当中的G分量保持不变,而NIR分量代替了R、B分量总和的一半,R、G分量各自改变为对应的CFA中R、G分量的一半,在本文中,将这种RGB-NIR采样的方式称为稠密RGB-NIR模式<citation id="219" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。用单传感器捕获RGB以及NIR分量时,传感器输出的为马赛克数据,其中每一像素位置只有R、G、B、NIR当中的一个分量被保存。因此,完整的RGB图像以及NIR图像,需要对马赛克图像进行相应的插值,这与传统的拜尔CFA去马赛克的思路相同。</p>
                </div>
                <div class="p1">
                    <p id="62">由于NIR分量在R、G、B分量的波长范围内是完全折射的,因此在传统相机的CFA前面通常都有一个近红外滤光片,以消除NIR分量对R、G、B分量的影响。而对于需要同时捕获RGB图像以及NIR图像来说,需要去除近红外滤波片,这将引入一个光谱干扰的问题。由于NIR分量对其余三个颜色分量的干扰,从而导致得到的图像存在颜色失真问题。</p>
                </div>
                <div class="p1">
                    <p id="63">针对同时捕获RGB-NIR,然后重建当中存在的光谱干扰问题,本文将它转换为去马赛克问题,并且提出一种有效的基于卷积神经网络的去马赛克算法。在传统算法中,在对马赛克图像进行插值之后,需要色彩校正等一系列工作,而本文算法可实现马赛克图像到标准彩色图像以及红外图像的端到端映射,不需要再进行额外的操作。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="65">由于成本与体积的限制,目前大多数用于成像的产品都是采用单传感器获得CFA图像,然后利用一定的手段重建出需要的图像。目前用于RGB-NIR图像去马赛克的算法可以分为两类:一类为基于传统插值算法,另一类为基于学习的去马赛克算法。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.1 <b>传统插值算法</b></h4>
                <div class="p1">
                    <p id="67">传统的插值算法通常是根据像素之间的相关性进行相应的重建。最简单的插值算法为双线性插值(Bilinear)<citation id="220" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,它通过相邻像素对未捕获到的像素进行估计,这种方法重建出的图像存在很多伪影以及色彩存在较大的失真。首次针对RGB-NIR的CFA模式,Miao等<citation id="221" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了相应的去马赛克算法。在Miao工作的基础上,Aggarwal等<citation id="222" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>同时考虑空间以及频谱相关来插值,这个插值方式被称为多频谱去马赛克。文献<citation id="223" type="reference">[<a class="sup">8</a>]</citation>中首先对NIR分量进行双三次插值,接下来通过捕获到的G分量与NIR分量的色差对未捕获的G分量进行估计。文献<citation id="224" type="reference">[<a class="sup">9</a>]</citation>中提出残差插值算法,首先对G分量进行插值,然后将G分量作为引导图,分别对其余分量与G分量的色差进行插值,最后将所有分量插值过后的值与原本捕获到的各个分量进行相加,得到标准的彩色图像与红外图像。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">1.2 <b>基于学习的去马赛克算法</b></h4>
                <div class="p1">
                    <p id="69">基于学习的思想主要是根据大量的训练样本学习到马赛克图像到标准图像之间的映射关系。深度学习目前在图像处理的各个领域取得了巨大的成功,比如:目标检测<citation id="225" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、人脸识别<citation id="226" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、行为检测<citation id="227" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>以及超分辨率<citation id="228" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。在去马赛克领域当中,2016年,Gharbi等<citation id="229" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>首次将深度学习应用于对CFA图像联合去马赛克去噪。他们将捕获到的各个通道的像素重新组合;然后,输入到卷积神经网络当中,提取特征;最后,通过重新整理得到标准彩色图像。2017年Tan等<citation id="230" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>通过引入残差模块,使得深度学习用于CFA图像去马赛克取得重大突破。2018年Hu等<citation id="231" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>将卷积稀疏表示应用于RGB-NIR去马赛克,使用字典学习的理论从马赛克图像当中恢复标准的彩色图像以及红外图像。同年,Shopovska等<citation id="232" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出深度残差U型网络,将马赛克图像输入网络,然后,通过一连串的下采样与残差模块,最后通过与下采样相反的反卷积操作输出标准的彩色图像以及红外图像。</p>
                </div>
                <div class="p1">
                    <p id="70">超分辨率问题与去马赛克问题相似,只是下采样的方式不同。文献<citation id="233" type="reference">[<a class="sup">18</a>]</citation>中提出了一个对单幅图像进行超分辨率的残差稠密网络结构,本文提出相似的网络结构用于RGB-NIR图像的去马赛克。相对于传统超分辨率卷积神经网络(Super-Resolution Conventional Neural Network, SRCNN)算法的3层结构,本文设计了一个很深的残差稠密网络(Very Deep Residual Dense Network,VDRDN),层数达到了36。</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag">2 本文算法</h3>
                <h4 class="anchor-tag" id="72" name="72">2.1 <b>算法理论</b></h4>
                <div class="p1">
                    <p id="73">RGB-NIR图像去马赛克的模型<citation id="234" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>定义如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="74"><i><b>y</b></i>=<i><b>Fx</b></i>+<i><b>n</b></i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="75">其中: <i><b>y</b></i>为传感器捕获到的马赛克图像的向量;<i><b>x</b></i>=<i><b>x</b></i><sub><i>R</i></sub>,<i><b>x</b></i><sub><i>G</i></sub>,<i><b>x</b></i><sub><i>B</i></sub>,<i><b>x</b></i><sub><i>N</i></sub>表示需要重建出的理想的彩色图像以及红外图像;<i><b>n</b></i>表示噪声;操作符<i><b>F</b></i>建模了单传感器捕获图像的操作。</p>
                </div>
                <div class="p1">
                    <p id="76">本文主要是对RGB-NIR图像单纯去马赛克,可以将式(1)改写为:</p>
                </div>
                <div class="p1">
                    <p id="77"><i><b>y</b></i>=<i><b>Sx</b></i>      (2)</p>
                </div>
                <div class="p1">
                    <p id="78">其中<i><b>S</b></i>表示为下采样操作。由于<i><b>S</b></i>是奇异的,不存在唯一解,因此需要重建的理想图像可以近似为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mo>=</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">y</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中:<i><b>D</b></i>是一个去马赛克的操作。本文对<i><b>D</b></i>建模为一个很深的残差稠密卷积神经网络如式(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">D</mi></munder><mspace width="0.25em" /><mfrac><mn>1</mn><mi>Μ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mo stretchy="false">∥</mo></mstyle><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<i>M</i>表示训练集样本的个数;<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></math></mathml>为真实的RGB-NIR图像。网络参数通过不断地优化重建出的RGB-NIR图像与真实RGB-NIR图像之间的最小均方误差进行更新。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">2.2 <b>网络结构</b></h4>
                <div class="p1">
                    <p id="84">本文提出一种用于<i>RGB</i>-<i>NIR</i>图像去马赛克的网络模型,它由浅层特征提取层、残差稠密块(<i>Residual Dense Block</i>, <i>RDB</i>)以及全局跳远连接构成,具体网络结构如图3所示。网络的输入大小为64×64的4通道马赛克图像,即<i>R</i>、<i>G</i>、<i>B</i>以及<i>NIR</i>通道,每一个通道保留对应<i>RGB</i>-<i>NIR</i>模式的<i>CFA</i>捕获到的像素值,其余未捕获到的像素为0。然后将输入依次经过卷积操作→卷积操作→第一个残差稠密块→第二个残差稠密块→…→第d个残差稠密块→拼接层→1×1卷积→输入的跳远连接→输出。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910019_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 VDRDN结构" src="Detail/GetImg?filename=images/JSJY201910019_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>VDRDN</i>结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910019_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.3 <i>Structure of very deep residual dense network</i></p>

                </div>
                <div class="p1">
                    <p id="86">为保证输入与输出具有同样大小的维度,所有的卷积核大小设置为3×3,步长为1,填充为1,使得网络从输入到输出的维度保持一致。浅层特征提取层包含两次卷积的操作,用于提取一些低级特征,如边缘、颜色、纹理等。提取浅层特征可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="87"><i><b>F</b></i><sub>0</sub>=<i>ω</i><sub>2</sub>*(<i>ω</i><sub>1</sub>*<i>I</i><sup>input</sup>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="88">其中:<i>I</i><sup>input</sup>表示输入的四通道的马赛克图像;<i>ω</i><sub>1</sub>表示第一层卷积层(Conv1)的参数;<i>ω</i><sub>2</sub>表示第二层卷积层(Conv2)的参数;“*”表示卷积操作;<i><b>F</b></i><sub>0</sub>表示得到的特征图。</p>
                </div>
                <div class="p1">
                    <p id="89">通过浅层特征提取之后,将输出的低级特征送入残差稠密块(RDB)。如图4所示,残差稠密块中主要操作为跳远连接与稠密连接。通过引入这样的连接很好地解决了梯度消失以及梯度弥散的问题,使得网络更容易训练。同时,由于稠密连接的作用,使得信息在前向传播中不会丢失,充分地利用了低级特征与高级特征,使得网络的学习能力更强。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910019_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 残差稠密块" src="Detail/GetImg?filename=images/JSJY201910019_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 残差稠密块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910019_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Residual dense blocks</p>

                </div>
                <div class="p1">
                    <p id="91">浅层特征经过残差稠密块的具体过程可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="92"><i><b>F</b></i><sub><i>d</i></sub>=<i>H</i><sub>RDB,</sub><sub><i>d</i></sub>(<i><b>F</b></i><sub><i>d</i></sub><sub>-1</sub>)=</p>
                </div>
                <div class="p1">
                    <p id="93"><i>H</i><sub>RDB,</sub><sub><i>d</i></sub>(<i>H</i><sub>RDB,</sub><sub><i>d</i></sub><sub>-1</sub>(…(<i>H</i><sub>RDB,1</sub>(<i><b>F</b></i><sub>0</sub>))…))      (6)</p>
                </div>
                <div class="p1">
                    <p id="94">其中:<i>H</i><sub><i>RDB</i></sub><sub>,</sub><sub><i>d</i></sub>表示第<i>d</i>个残差稠密块的相应卷积、修正线性单元以及拼接操作,更多的细节参见文献<citation id="235" type="reference">[<a class="sup">18</a>]</citation>;<i><b>F</b></i><sub><i>d</i></sub>表示通过残差稠密块提取得到的特征图;<i><b>F</b></i><sub>0</sub>表示输入的马赛克图像经过浅层卷积之后得到的特征图。</p>
                </div>
                <div class="p1">
                    <p id="95">通过一系列的残差稠密块提取特征之后,接下来对所有残差稠密块提取到的特征进行拼接,然后将通过1×1卷积改变特征图的通道数为4,最后经过3×3卷积得到最终的特征图。具体过程可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="96"><i><b>F</b></i><sub><i>f</i></sub>=<i>ω</i><sub>4</sub>*<i>ω</i><sub>3</sub>*(<i><b>F</b></i><sub>1</sub>,<i><b>F</b></i><sub>2</sub>,…,<i><b>F</b></i><sub><i>d</i></sub>)      (7)</p>
                </div>
                <div class="p1">
                    <p id="97">其中:<i><b>F</b></i><sub><i>f</i></sub>表示最终提取到的特征图;<i>ω</i><sub>3</sub>表示倒数第二层卷积层(Conv3)的参数;<i>ω</i><sub>4</sub>表示最后一层卷积层(Conv4)的参数;<i><b>F</b></i><sub><i>i</i></sub>表示第<i>i</i>(1,2,…,<i>d</i>)个残差稠密块输出的特征图;<i><b>F</b></i><sub>1</sub>,<i><b>F</b></i><sub>2</sub>,…,<i><b>F</b></i><sub><i>d</i></sub>表示所有残差稠密块输出特征的拼接操作。</p>
                </div>
                <div class="p1">
                    <p id="98">由于RGB-NIR去马赛克算法只是需要估计出传感器未捕获到的其余三通道的值,因此本文采用一个输入到最终特征图的跳远连接,使得对应像素相加,让网络模型的学习未捕获到的其余通道的像素值。具体过程可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="99"><i>I</i><sup>ouput</sup>=<i>I</i><sup>input</sup>+<i><b>F</b></i><sub><i>f</i></sub>      (8)</p>
                </div>
                <div class="p1">
                    <p id="100">其中:<i>I</i><sup>ouput</sup>表示重建出的标准的RGB-NIR图像。</p>
                </div>
                <div class="p1">
                    <p id="101">综上所述,输入经过前向传播输出重建出的RGB-NIR图像,然后与对应输入的真实RGB-NIR图像求差值,然后进行反向传播更新网络当中的参数。本文设计网络的损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mfrac><mn>1</mn><mi>Μ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>Ι</mi><mtext> </mtext><msup><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext><mtext>p</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msup><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">其中:<i>M</i>表示训练样本的个数;<i>I</i><sub>gt</sub>表示训练集对应的标签,即标准的RGB-NIR图像;gt指基准(ground truth)。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">3 实验与分析</h3>
                <h4 class="anchor-tag" id="105" name="105">3.1 <b>训练数据预处理</b></h4>
                <div class="p1">
                    <p id="106">本文选用文献<citation id="236" type="reference">[<a class="sup">20</a>]</citation>中的数据集作为训练网络模型的训练集,其中包含477张已经配准的同一场景同一时刻的彩色图像与红外图像。将数据集随机裁剪为64×64大小的图像块,一共得到7万张训练图像,其中每个训练图像为4通道。为模拟<i>RGB</i>-<i>NIR</i>传感器捕获各个通道的像素值,本文对训练集进行相应的下采样得到输入网络的马赛克图像,标签为真实的未采样的图像。相应的图像下采样过程如图5所示。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910019_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 训练集数据制作" src="Detail/GetImg?filename=images/JSJY201910019_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 训练集数据制作  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910019_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.5 <i>Steps of generating training set</i></p>

                </div>
                <div class="p1">
                    <p id="108">图5为制作样本集的过程。首先,对完整配对的<i>RGB</i>图像与<i>NIR</i>图像进行拼接,组成四通道的<i>RGB</i>-<i>NIR</i>图像。然后,通过前面提到的稠密<i>RGB</i>-<i>NIR</i>图像<i>CFA</i>对每一个像素点位置的四个分量(<i>R</i>、<i>G</i>、<i>B</i>、<i>NIR</i>)中的一个分量进行保留,其余三个分量值设置为0。最后,得到输入到网络当中的马赛克图像。图5的左上角为完整的<i>RGB</i>图像,左下角为对应的<i>NIR</i>图像,右上角为得到的<i>RGB</i>-<i>NIR</i>图像的<i>R</i>、<i>G</i>、<i>B</i>三个分量的图像,由于<i>CFA</i>对于<i>G</i>分量采样密度最大,可见整体图像偏绿。右下角为采样得到的<i>NIR</i>图像,可见当中缺失了很多的信息。其中右边的图像的右下角都是选取图像的局部区域放大,为方便可视化,在展示中未拼接,如果拼接则是完全对应于本文的<i>RGB</i>-<i>NIR</i>图像的<i>CFA</i>模式。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">3.2 <b>评价指标</b></h4>
                <div class="p1">
                    <p id="110">本文采用的测试集为公共的图像与视觉表示组(<i>Images and Visual Representation Group</i>, <i>IVRG</i>)数据集<citation id="237" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、有植被的户外多光谱图像(<i>Outdoor Multi</i>-<i>Spectral Images with Vegetation</i>, <i>OMSIV</i>)数据集<citation id="238" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和森林(<i>Forest</i>)数据集<citation id="239" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>,其中<i>OMSIV</i>包含533张彩色图像及其对应的红外图像,<i>IVRG</i>包含25张高质量的彩色图像及其对应的红外图像,<i>Forest</i>数据集包含136张彩色图像及其对应的红外图像。客观指标选用复合峰值信噪比(<i>Composition Peak Signal</i>-<i>to</i>-<i>Noise Ratio</i>, <i>CPSNR</i>)<citation id="240" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>以及结构相似性(<i>Structural SIMilarity</i>, <i>SSIM</i>)。</p>
                </div>
                <div class="p1">
                    <p id="111">复合峰值信噪比是基于像素点之间的误差,是基于误差敏感的图像质量评价。计算公式如下所示:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>Ρ</mi><mi>S</mi><mi>Ν</mi><mi>R</mi><mo>=</mo><mn>1</mn><mn>0</mn><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mfrac><mrow><mn>2</mn><mn>5</mn><mn>5</mn><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>s</mi><mi>R</mi><mo>,</mo><mi>s</mi><mi>G</mi><mo>,</mo><mi>s</mi><mi>B</mi><mo>,</mo><mi>Ν</mi><mi>Ι</mi><mi>R</mi></mrow></munder><mo stretchy="false">∥</mo></mstyle><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">其中:<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></math></mathml>的表示重建得出的RGB-NIR图像;<i><b>x</b></i>表示真实的RGB-NIR图像。结构相似性由文献<citation id="241" type="reference">[<a class="sup">25</a>]</citation>提出,为测量图像质量的一种重要指标,值的范围为0～1,越大表示结构越相似。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">3.3 <b>参数设定及训练分析</b></h4>
                <div class="p1">
                    <p id="115">关于网络训练过程当中的一些超参数设定为:学习率为0.000 1,<i>Batch</i>_<i>size</i>设为64,梯度下降算法选择<i>Adam</i>。输入图像的大小为64×64,通道数为4,除去倒数第二层的卷积核大小为1×1,其余所有卷积核大小为3×3, <i>padding</i>为1,<i>stride</i>为1。<i>RDB</i>个数为8,每一<i>RDB</i>块当中的卷积+修正线性单元个数为4,因此网络的深度为36层,网络参数总量为3 190 104。本文采用的实验平台是使用<i>NVIDIA</i>显卡<i>GeForce GTX</i> 1080-<i>TI</i>、3.20 <i>GHz Intel i</i>5 <i>CPU</i>、32 <i>GB RAM</i>,编译软件采用<i>PyCharm</i> 2019,并使用<i>Tensorflow</i>深度学习框架进行训练。训练200轮(<i>epoch</i>为200),一共训练2 <i>d</i>。主观结果采用<i>Matlab R</i>2014<i>a</i>进行绘制。</p>
                </div>
                <div class="p1">
                    <p id="116">图6展示了本文提出的<i>VDRDN</i>与经典的<i>SRCNN</i>之间的训练效率对比。由图6可知,不论在收敛的速度还是最优的CPSNR值上,<i>VDRDN</i>都全面领先于<i>SRCNN</i>。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910019_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 VDRDN和SRCNN的训练效率对比" src="Detail/GetImg?filename=images/JSJY201910019_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 <i>VDRDN</i>和<i>SRCNN</i>的训练效率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910019_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.6 <i>Training efficiency comparison of VDRDN and SRCNN</i></p>

                </div>
                <h4 class="anchor-tag" id="118" name="118">3.4 <b>实验结果及分析</b></h4>
                <div class="p1">
                    <p id="119">在与现有方法进行对比时,选择已经公开的实现方法。分别选择<i>Bilinear</i>、基于二叉树的边缘检测(<i>Binary Tree</i>-<i>based Edge</i>-<i>Sensing</i>,<i>BTES</i>)的去马赛克算法<citation id="242" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、基于最小二乘的多光谱去马赛克(<i>Least</i>-<i>square based Multi</i>-<i>Spectral Demosaicing</i>, <i>LMSD</i>)算法<citation id="243" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、基于多级自适应残差插值(<i>MultiStage</i>-<i>Adaptive Residual Interpolation</i>,<i>MS</i>-<i>ARI</i>)的去马赛克算法<citation id="244" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、<i>SRCNN</i><citation id="245" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>以及基于深度残差<i>U</i>型网络去马赛克(<i>RGB</i>-<i>NIR Demosaicing Using Deep Residual U</i>-<i>Net</i>, <i>DRU</i>-<i>Net</i>)算法<citation id="246" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。其中一些算法需要进行相应的改进以符合<i>RGB</i>-<i>NIR</i>去马赛克的需求。超分辨率问题与去马赛克问题类似,<i>SRCNN</i>作为解决超分辨率问题的经典深度学习方法,更改输入与输出的形式得到去马赛克的功能。</p>
                </div>
                <div class="p1">
                    <p id="120">对<i>OMSIV</i>、<i>IVRG</i>以及<i>Forest</i>三个公开的数据进行测试,得到相应的客观指标与现存主流方法进行对比,结果如表1所示。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表</b>1 <b>客观评价指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.1 <i>Objective evaluation indexes</i></p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td rowspan="2">算法</td><td colspan="2"><br /><i>OMSIV</i></td><td rowspan="2"></td><td colspan="2"><br /><i>Forest</i></td><td rowspan="2"></td><td colspan="2"><br /><i>IVRG</i></td></tr><tr><td><br />CPSNR/<i>dB</i></td><td>SSIM</td><td><br />CPSNR/<i>dB</i></td><td>SSIM</td><td><br />CPSNR/<i>dB</i></td><td>SSIM</td></tr><tr><td><i>Bilinear</i></td><td>30.57</td><td>0.828 7</td><td></td><td>41.26</td><td>0.901 3</td><td></td><td>30.42</td><td>0.741 7</td></tr><tr><td><br /><i>BTES</i></td><td>30.48</td><td>0.815 3</td><td></td><td>41.00</td><td>0.890 1</td><td></td><td>30.42</td><td>0.734 9</td></tr><tr><td><br /><i>LMSD</i></td><td>30.66</td><td>0.776 2</td><td></td><td>40.55</td><td>0.850 4</td><td></td><td>31.41</td><td>0.681 8</td></tr><tr><td><br /><i>MS</i>-<i>ARI</i></td><td>32.58</td><td>0.868 1</td><td></td><td>41.91</td><td>0.913 9</td><td></td><td>33.02</td><td>0.794 9</td></tr><tr><td><br /><i>SRCNN</i></td><td>25.23</td><td>0.695 1</td><td></td><td>37.17</td><td>0.849 6</td><td></td><td>25.89</td><td>0.605 4</td></tr><tr><td><br /><i>DRU</i>-<i>Net</i></td><td>33.52</td><td>0.873 8</td><td></td><td>43.43</td><td>0.891 7</td><td></td><td>35.14</td><td>0.807 0</td></tr><tr><td><br />本文算法</td><td>34.32</td><td>0.951 4</td><td></td><td>44.09</td><td>0.984 9</td><td></td><td>36.80</td><td>0.967 5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="122">由表1可知,本文算法在<i>OMSIV</i>、<i>IVRG</i>以及<i>Forest</i>数据集上的CPSNR以及SSIM完全优于目前的主流的<i>RGB</i>-<i>NIR</i>去马赛克算法。</p>
                </div>
                <div class="p1">
                    <p id="123">在测试集当中选取清晰度高、色彩明亮、饱和度较大的两张图像用于主观显示不同算法之间的结果对比。图7为扇子和蝴蝶图像不同算法的主观结果比较。其中:大图为真实的<i>RGB</i>和对应的<i>NIR</i>图像,图中方框为选取的区域,其他为应用不同算法处理后获得的方框区域放大图。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">1)扇子图像。</h4>
                <div class="p1">
                    <p id="125"><i>Bilinear</i>算法重建出<i>RGB</i>图像存在较大的伪影,图像质量很差。<i>BTES</i>算法未能重建出真实图像当中的纹路细节,也存在伪影。<i>LMSD</i>算法重建出的<i>RGB</i>图像偏亮,颜色存在较大失真。<i>MS</i>-<i>ARI</i>算法重建出<i>RGB</i>图像也存在较大的颜色失真问题。<i>SRCNN</i>是利用深度学习来解决<i>RGB</i>-<i>NIR</i>图像去马赛克的方法,重建的<i>RGB</i>图像相对于上述传统算法取得了巨大的成功;但是,重建的<i>RGB</i>图像的颜色也存在失真,在图像的条纹处的颜色尤其明显。基于<i>DRU</i>-<i>Net</i>算法,相对于<i>SRCNN</i>由于采取更优的网络设计,因此重建出的图像取得了更佳的主观效果,颜色失真问题基本消失,只是图像的一些细节未能完美修复。本文提出的<i>VDRDN</i>算法重建出的<i>RGB</i>图像消除了颜色失真问题,同时恢复出了最准确的细节,在所有对比算法中取得了最佳的视觉质量。对于重建出的<i>NIR</i>图像,<i>Bilinear</i>、<i>BTES</i>、<i>LMSD</i>、<i>MS</i>-<i>ARI</i>这些传统的插值算法重建出的图像存在细节丢失、额外引入噪声点以及模糊的问题;基于<i>SRCNN</i>算法重建出的<i>NIR</i>图像存在模糊的问题;基于<i>DRU</i>-<i>Net</i>算法以及本文算法均重建出较准确的细节,恢复出了较好的细节。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126">2)蝴蝶图像。</h4>
                <div class="p1">
                    <p id="127">对于重建出的<i>RGB</i>图像,<i>Bilinear</i>、<i>BTES</i>、<i>LMSD</i>、<i>MS</i>-<i>ARI</i>这些传统的插值算法重建出来的图像存在较大的颜色失真问题,在图像当中的字母的边缘处存在较大的锯齿效应;基于<i>SRCNN</i>算法较好地消除了锯齿效应,而且恢复的细节较好,但是存在一定的颜色失真问题;基于<i>DRU</i>-<i>Net</i>以及本文算法均取得了较好的主观效果。对于重建出<i>NIR</i>图像,基于<i>Bilinear</i>以及基于<i>BTES</i>算法重建出的图像未能重建出细节,图像过于平滑。基于<i>LMSD</i>算法重建出的图像当中存在严重的阴影,恢复出的细节不正确。基于<i>MS</i>-<i>ARI</i>算法恢复出的细节与真实图像不对应。基于<i>SRCNN</i>重建的图像较好地恢复出了细节,但存在模糊问题。基于<i>DRU</i>-<i>Net</i>以及本文算法较好地恢复出了细节,取得了很好的视觉效果。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910019_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 主观结果对比" src="Detail/GetImg?filename=images/JSJY201910019_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 主观结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910019_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.7 <i>Subjective result comparison</i></p>

                </div>
                <h3 id="129" name="129" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="130">针对目前使用单传感器捕获彩色图像以及近红外图像当中存在的去马赛克问题,本文提出一种基于深度残差稠密网络的算法。该算法由浅层特征提取、残差稠密块以及全局跳远连接组成。其中:浅层特征提取为两层卷积,用于提取图像的低级特征;其次,残差稠密块一共有8个,每个残差稠密块当中包含4个相应的卷积与非线性映射操作。最终的全局跳远连接模块将输入与最后一个残差稠密块的输出进行相加进行输出。对于<i>OMSIV</i>、<i>IVRG</i>以及<i>Forest</i>数据集,本文算法优于<i>Bilinear</i>、<i>BTES</i>、<i>LMSD</i>、<i>MS</i>-<i>ARI</i>、<i>SRCNN</i>以及<i>DRU</i>-<i>Net</i>,充分说明本文算法的适用性。下一步的工作主要是寻找更优的网络结构,使得<i>RGB</i>-<i>NIR</i>图像去马赛克问题能得到很好的解决。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="164">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Color imaging array">

                                <b>[1]</b>BAYER B E.Color imaging array:US3971065[P].197-07-20.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Demosaicking methods for bayer color arrays">

                                <b>[2]</b>RAMANATH R,SNYDER W E,BILBRO G L,et al.Demosaicking methods for Bayer color arrays[J].Journal of Electronic Imaging,2002,11(3):306-315.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-sensor RGB and NIR image acquisition:toward optimal performance by taking account of CFA pattern,demosaicking,and color correction">

                                <b>[3]</b>TERANAKA H,MONNO Y,TANAKA M,et al.Single-sensor RGB and NIR image acquisition:toward optimal performance by taking account of CFA pattern,demosaicking,and color correction[EB/OL].[2019-01-10].http://www.ok.sc.e.titech.ac.jp/res/MSI/RGB-NIR/ei2016.pdf.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=N-to-SRGB mapping for single-sensor multispectral imaging">

                                <b>[4]</b>MONNO Y,TANAKA M,OKUTOMI M.N-to-SRGB mapping for single-sensor multispectral imaging[C]//Proceedings of the 2015IEEE International Conference on Computer Vision Workshops.Piscataway:IEEE,2015:66-73.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Novel Approach to Real-Time Bilinear Interpolation">

                                <b>[5]</b>GRIBBON K T,BAILEY D G.A novel approach to real-time bilinear interpolation[C]//Proceedings of the 2nd IEEE International Workshop on Electronic Design,Test and Applications.Piscataway:IEEE,2004:126-131.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Binary tree-based generic demosaicking algorithm for multispectral filter arrays">

                                <b>[6]</b>MIAO L,QI H,RAMANATH R,et al.Binary tree-based generic demosaicking algorithm for multispectral filter arrays[J].IEEETransactions on Image Processing,2006,15(11):3550-3558.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-sensor multi-spectral image demosaicing algorithm using learned interpolation weights">

                                <b>[7]</b>AGGARWAL H K,MAJUMDAR A.Single-sensor multi-spectral image demosaicing algorithm using learned interpolation weights[C]//Proceedings of the 2014 IEEE Geoscience and Remote Sensing Symposium.Piscataway:IEEE,2014:2011-2014.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dual aperture photography:Image and depth from a mobile camera">

                                <b>[8]</b>MARTINELLO M,WAJS A,QUAN S,et al.Dual aperture photography:Image and depth from a mobile camera[C]//Proceedings of the 2015 IEEE International Conference on Computational Photography.Piscataway:IEEE,2015:1-10.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive residual interpolation for color and multispectral image demosaicking">

                                <b>[9]</b>MONNO Y,KIKU D,TANAKA M,et al.Adaptive residual interpolation for color and multispectral image demosaicking[J].Sensors,2017,17(12):2787-2807.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">

                                <b>[10]</b>LIU W,ANGUELOV D,ERHAN D,et al.SSD:Single Shot multibox Detector[C]//Proceedings of the 2016 European Conference on Computer Vision,LNCS 9905.Cham:Springer,2016:21-37.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose-Aware Face Recognition in the Wild">

                                <b>[11]</b>MASI I,RAWLS S,MEDIONI G,et al.Pose-aware face recognition in the wild[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:4838-4846.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Temporal segment networks:towards good practices for deep action recognition">

                                <b>[12]</b>WANG L,XIONG Y,WANG Z,et al.Temporal segment networks:towards good practices for deep action recognition[C]//Proceedings of the European Conference on Computer Vision,LNCS 9912.Cham:Springer,2016:20-36.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">

                                <b>[13]</b>YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM51E2FC729BD6CE17D784F12A15993CF5&amp;v=MDYzMzRVMDV0cGh4N3U3eGFrPU5pZklZN2E1YTlPNjNJaEhiWmw3Q2c5TXpoRm43VGQ1UG43ZzNSTXdjTHVYTnN5YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>GHARBI M,CHAURASIA G,PARIS S,et al.Deep joint demosaicking and denoising[J].ACM Transactions on Graphics,2016,35(6):191-202.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Color image demosaicking via deep residual learning">

                                <b>[15]</b>TAN R,ZHANG K,ZUO W,et al.Color image demosaicking via deep residual learning[C]//Proceedings of the 2017 IEEE International Conference on Multimedia and Expo.Piscataway:IEEE,2017:793-798.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional sparse coding for RGB+NIR imaging">

                                <b>[16]</b>HU X,HEIDE F,DAI Q,et al.Convolutional sparse coding for RGB+NIR imaging[J].IEEE Transactions on Image Processing,2018,27(4):1611-1625.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RGB-NIR demosaicing using deep residual U-net">

                                <b>[17]</b>SHOPOVSKA I,JOVANOV L,PHILIPS W.RGB-NIR demosaicing using deep residual U-net[C]//Proceedings of the 26th Telecommunications Forum.Piscataway:IEEE,2018:1-4.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Residual dense network for image super-resolution">

                                <b>[18]</b>ZHANG Y,TIAN Y,KONG Y,et al.Residual dense network for image super-resolution[C]//Proceedings of the 2018 IEEE/CVFConference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2472-2481.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High resolution photography with an RGB-infrared camera">

                                <b>[19]</b>TANG H,ZHANG X,ZHUO S,et al.High resolution photography with an RGB-infrared camera[C]//Proceedings of the 2015IEEE International Conference on Computational Photography.Piscataway:IEEE,2015:1-10.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Multi-Spectral SIFT for Scene Category Recognition,&amp;quot;">

                                <b>[20]</b>BROWN M,S<image id="248" type="formula" href="images/JSJY201910019_24800.jpg" display="inline" placement="inline"><alt></alt></image>SSTRUNK S.Multi-spectral SIFT for scene category recognition[C]//Proceedings of the 2011 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2011:177-184.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multispectral singlesensor RGB-NIR imaging:new challenges and opportunities">

                                <b>[21]</b>SORIA X,SAPPA A D,AKBARINIA A.Multispectral singlesensor RGB-NIR imaging:new challenges and opportunities[C]//Proceedings of the 7th International Conference on Image Processing Theory,Tools and Applications.Piscataway:IEEE,2017:1-6.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Colouring the near-infrared">

                                <b>[22]</b>FREDEMBACH C,S<image id="247" type="formula" href="images/JSJY201910019_24700.jpg" display="inline" placement="inline"><alt></alt></image>SSTRUNK S.Colouring the near-infrared[EB/OL].[2019-01-10].https://infoscience.epfl.ch/record/129419/files/IR_colour.pdf?version=2.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep multispectral semantic scene understanding of forested environments using multimodal fusion">

                                <b>[23]</b>VALADA A,OLIVEIRA G L,BROX T,et al.Deep multispectral semantic scene understanding of forested environments using multimodal fusion[C]//Proceedings of the 2016 International Symposium on Experimental Robotics.Cham:Springer,2016:465-477.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-sensor RGB-NIR imaging:high-quality system design and prototype implementation">

                                <b>[24]</b>MONNO Y,TERANAKA H,YOSHIZAKI K,et al.Single-sensor RGB-NIR imaging:high-quality system design and prototype implementation[J].IEEE Sensors Journal,2019,19(2):497-507.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">

                                <b>[25]</b>WANG Z,BOVIK A C,SHEIKH H R,et al.Image quality assessment:from error visibility to structural similarity[J].IEEETransactions on Image Processing,2004,13(4):600-612.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_26" >
                                    <b>[26]</b>
                                DONG C,LOY C C,HE K,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910019" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910019&amp;v=MDAwNDFyQ1VSN3FmWnVac0Z5dm1WN3JKTHo3QmQ3RzRIOWpOcjQ5RWJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dMUmdYZnFWQS91MHJJSHg2Zz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
