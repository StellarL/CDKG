<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136675237033750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906020%26RESULT%3d1%26SIGN%3dMBOuAdmhKtTYXU1wumwnwX0ssJU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906020&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906020&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906020&amp;v=MDEzMTQvaFdyckFMejdCZDdHNEg5ak1xWTlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="1 Faster R-CNN检测原理 ">1 Faster R-CNN检测原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="1.1 &lt;b&gt;特征提取网络&lt;/b&gt;">1.1 <b>特征提取网络</b></a></li>
                                                <li><a href="#63" data-title="1.2 &lt;b&gt;区域建议网络&lt;/b&gt;">1.2 <b>区域建议网络</b></a></li>
                                                <li><a href="#69" data-title="1.3 Fast R-CNN&lt;b&gt;检测网络&lt;/b&gt;">1.3 Fast R-CNN<b>检测网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="2 实验过程与结果分析 ">2 实验过程与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="2.1 &lt;b&gt;数据产生与训练&lt;/b&gt;">2.1 <b>数据产生与训练</b></a></li>
                                                <li><a href="#77" data-title="2.2 &lt;b&gt;结果分析&lt;/b&gt;">2.2 <b>结果分析</b></a></li>
                                                <li><a href="#103" data-title="2.3 IoU&lt;b&gt;优化&lt;/b&gt;">2.3 IoU<b>优化</b></a></li>
                                                <li><a href="#113" data-title="2.4 &lt;b&gt;人员定位&lt;/b&gt;">2.4 <b>人员定位</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="3 系统实现与展示 ">3 系统实现与展示</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="图1 Faster R-CNN的网络结构">图1 Faster R-CNN的网络结构</a></li>
                                                <li><a href="#62" data-title="图2 残差网络的单元结构">图2 残差网络的单元结构</a></li>
                                                <li><a href="#67" data-title="图3 4种尺度与3种长宽比的锚点">图3 4种尺度与3种长宽比的锚点</a></li>
                                                <li><a href="#68" data-title="图4 RPN网络结构">图4 RPN网络结构</a></li>
                                                <li><a href="#75" data-title="图5 数据集图像示例">图5 数据集图像示例</a></li>
                                                <li><a href="#79" data-title="图6 训练过程中的损失值">图6 训练过程中的损失值</a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;模型检测的四种情况&lt;/b&gt;"><b>表</b>1 <b>模型检测的四种情况</b></a></li>
                                                <li><a href="#101" data-title="图7 不同置信度下的&lt;i&gt;F&lt;/i&gt;1-&lt;i&gt;Score&lt;/i&gt;">图7 不同置信度下的<i>F</i>1-<i>Score</i></a></li>
                                                <li><a href="#105" data-title="图8 检测结果示例1">图8 检测结果示例1</a></li>
                                                <li><a href="#106" data-title="图9 检测结果示例2">图9 检测结果示例2</a></li>
                                                <li><a href="#110" data-title="图10 IoU 示意图">图10 IoU 示意图</a></li>
                                                <li><a href="#112" data-title="图11 IoU 算法流程">图11 IoU 算法流程</a></li>
                                                <li><a href="#115" data-title="图12 区域划分">图12 区域划分</a></li>
                                                <li><a href="#119" data-title="图13 实验室人数变化">图13 实验室人数变化</a></li>
                                                <li><a href="#120" data-title="图14 实验室历史监控画面">图14 实验室历史监控画面</a></li>
                                                <li><a href="#121" data-title="图15 工作台详细使用情况查询页面 (6号工作台) ">图15 工作台详细使用情况查询页面 (6号工作台) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="149">


                                    <a id="bibliography_1" title="何扬名, 杜建强, 肖贤波.基于区域深度特征的人头检测方法[J].微电子学与计算机, 2013, 30 (11) :39-42. (HE Y M, DU J Q, XIAO XB.Detecting human head by depth characteristics of regions[J].Microelectronics&amp;amp;Computer, 2013, 30 (11) :39-42.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201311010&amp;v=MTQ1MzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnJBTWpYU1pMRzRIOUxOcm85RVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        何扬名, 杜建强, 肖贤波.基于区域深度特征的人头检测方法[J].微电子学与计算机, 2013, 30 (11) :39-42. (HE Y M, DU J Q, XIAO XB.Detecting human head by depth characteristics of regions[J].Microelectronics&amp;amp;Computer, 2013, 30 (11) :39-42.) 
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_2" title="夏菁菁, 高琳, 范勇, 等.基于骨架特征的人数统计[J].计算机应用, 2014, 34 (2) :585-588. (XIA J J, GAO L, FAN Y, et al.People counting based on skeleton feature[J].Journal of Computer Applications, 2014, 34 (2) :585-588.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201402065&amp;v=MDM0MDdmWnVac0Z5L2hXcnJBTHo3QmQ3RzRIOVhNclk5RFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        夏菁菁, 高琳, 范勇, 等.基于骨架特征的人数统计[J].计算机应用, 2014, 34 (2) :585-588. (XIA J J, GAO L, FAN Y, et al.People counting based on skeleton feature[J].Journal of Computer Applications, 2014, 34 (2) :585-588.) 
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_3" title="叶锋, 洪斯婷, 郑德城, 等.基于Adaboost与背景差分级联的室内人数统计方法[J].福建师范大学学报 (自然科学版) , 2017, 33 (1) :7-13. (YE F, HONG S T, ZHENG D C, et al.A people counting method based on Adaboost and background subtraction in indoor environment[J].Journal of Fujian Normal University (Natural Science Edition) , 2017, 33 (1) :7-13.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FJSZ201701002&amp;v=MDk5MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyckFJeWZZZExHNEg5Yk1ybzlGWm8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        叶锋, 洪斯婷, 郑德城, 等.基于Adaboost与背景差分级联的室内人数统计方法[J].福建师范大学学报 (自然科学版) , 2017, 33 (1) :7-13. (YE F, HONG S T, ZHENG D C, et al.A people counting method based on Adaboost and background subtraction in indoor environment[J].Journal of Fujian Normal University (Natural Science Edition) , 2017, 33 (1) :7-13.) 
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_4" title="张晓琪, 宋钢.基于多特征协同的人头检测新方法[J].西南师范大学学报 (自然科学版) , 2018, 43 (7) :46-52. (ZHANG XQ, SONG G.A new head detection method oriented for vertical monocular camera way[J].Journal of Southwest China Normal University (Natural Science Edition) , 2018, 43 (7) :46-52.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XNZK201807010&amp;v=MDk1MDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnJBUFNQUlpiRzRIOW5NcUk5RVpJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        张晓琪, 宋钢.基于多特征协同的人头检测新方法[J].西南师范大学学报 (自然科学版) , 2018, 43 (7) :46-52. (ZHANG XQ, SONG G.A new head detection method oriented for vertical monocular camera way[J].Journal of Southwest China Normal University (Natural Science Edition) , 2018, 43 (7) :46-52.) 
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_5" title="GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[5]</b>
                                        GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:580-587.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                    GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:1440-1448.</a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_7" title="REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//NIPS 2015:Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MITPress, 2015:91-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[7]</b>
                                        REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//NIPS 2015:Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MITPress, 2015:91-99.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_8" title="REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified,Real-Time Object Detection">
                                        <b>[8]</b>
                                        REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:779-788.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_9" title="LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//Proceedings of the 2016 European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">
                                        <b>[9]</b>
                                        LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//Proceedings of the 2016 European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_10" title="王黎, 陆慧娟, 叶敏超, 等.Faster R-CNN的癌症影像检测方法[J].中国计量大学学报, 2018, 29 (2) :136-141. (WANG L, LU H J, YE M C, et al.A cancer image detection method based on Faster R-CNN[J].Journal of China University of Metrology, 2018, 29 (2) :136-141.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGJL201802004&amp;v=MjMyNjJDVVI3cWZadVpzRnkvaFdyckFQeXJCWXJHNEg5bk1yWTlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        王黎, 陆慧娟, 叶敏超, 等.Faster R-CNN的癌症影像检测方法[J].中国计量大学学报, 2018, 29 (2) :136-141. (WANG L, LU H J, YE M C, et al.A cancer image detection method based on Faster R-CNN[J].Journal of China University of Metrology, 2018, 29 (2) :136-141.) 
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_11" title="HUANG W Q, HUANG M Z, ZHANG Y T.Detection of traffic signs based on combination of GAN and faster R-CNN[J].Journal of Physics:Conference Series, 2018, 1069 (1) :012159." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SIPD&amp;filename=SIPD6F8F25FC2394A5034215D8F7E3CE6557&amp;v=MTMzNTZadWdHQ0EwOHp4VVg2RDU0UEhlVXEyYzJDc2VTUUwrWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3eTJ4YUE9TmlUYmFyWE9GcWZPcXZrMg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        HUANG W Q, HUANG M Z, ZHANG Y T.Detection of traffic signs based on combination of GAN and faster R-CNN[J].Journal of Physics:Conference Series, 2018, 1069 (1) :012159.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_12" title="戴陈卡, 李毅.基于Faster R-CNN以及多部件结合的机场场面静态飞机检测[J].计算机应用, 2017, 37 (z2) :85-88. (DAI CK, LI Y.Aeroplane detection in static aerodrome based on faster R-CNN and multi-part model[J].Journal of Computer Applications, 2017, 37 (z2) :85-88.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2017S2021&amp;v=MjA3NTJGeS9oV3JyQUx6N0JkN0c0SDlhdnJZOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        戴陈卡, 李毅.基于Faster R-CNN以及多部件结合的机场场面静态飞机检测[J].计算机应用, 2017, 37 (z2) :85-88. (DAI CK, LI Y.Aeroplane detection in static aerodrome based on faster R-CNN and multi-part model[J].Journal of Computer Applications, 2017, 37 (z2) :85-88.) 
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_13" title="胡炎, 单子力, 高峰.基于Faster R-CNN和多分辨率SAR的海上舰船目标检测[J].无线电工程, 2018, 48 (2) :96-100. (HUY, SHAN Z L, GAO F.Ship detection based on Faster R-CNNand multiresolution SAR[J].Radio Engineering, 2018, 48 (2) :96-100.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXDG201802005&amp;v=MzAzNzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3JyQU1qWFBhYkc0SDluTXJZOUZZWVE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        胡炎, 单子力, 高峰.基于Faster R-CNN和多分辨率SAR的海上舰船目标检测[J].无线电工程, 2018, 48 (2) :96-100. (HUY, SHAN Z L, GAO F.Ship detection based on Faster R-CNNand multiresolution SAR[J].Radio Engineering, 2018, 48 (2) :96-100.) 
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_14" title="UIJLINGS J R R, van de SANDE K E A, GEVERS T, et al.Selective search for object recognition[J].International Journal of Computer Vision, 2013, 104 (2) :154-171." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=Mjc4OTQ5RlpPb01Dbjg5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjFvZGJ4bz1OajdCYXJLN0h0bk1yWQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        UIJLINGS J R R, van de SANDE K E A, GEVERS T, et al.Selective search for object recognition[J].International Journal of Computer Vision, 2013, 104 (2) :154-171.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                    KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.</a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_16" title="SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].ar Xiv Preprint, 2014, 2014:ar Xiv.1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[16]</b>
                                        SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].ar Xiv Preprint, 2014, 2014:ar Xiv.1409.1556.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_17" >
                                        <b>[17]</b>
                                    SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.</a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_18" title="HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[18]</b>
                                        HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_19" title="彭刚, 杨诗琪, 黄心汉, 等.改进的基于区域卷积神经网络的微操作系统目标检测方法[J].模式识别与人工智能, 2018, 31 (2) :142-149. (PENG G, YANG S Q, HUANG X H, et al.Improved object detection method of micro-operating system based on region convolutional neural network[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (2) :142-149.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201802006&amp;v=MDAyMzlBS0Q3WWJMRzRIOW5Nclk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        彭刚, 杨诗琪, 黄心汉, 等.改进的基于区域卷积神经网络的微操作系统目标检测方法[J].模式识别与人工智能, 2018, 31 (2) :142-149. (PENG G, YANG S Q, HUANG X H, et al.Improved object detection method of micro-operating system based on region convolutional neural network[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (2) :142-149.) 
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_20" title="LIENHART R, MAYDT J.An extended set of Haar-like features for rapid object detection[C]//Proceedings of the 2002 International Conference on Image Processing.Piscataway, NJ:IEEE, 2002:900-903." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An extended set of haar-like features for rapid object detection">
                                        <b>[20]</b>
                                        LIENHART R, MAYDT J.An extended set of Haar-like features for rapid object detection[C]//Proceedings of the 2002 International Conference on Image Processing.Piscataway, NJ:IEEE, 2002:900-903.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_21" title="NEUBECK A, van GOOL L.Efficient non-maximum suppression[C]//ICPR 2006:Proceedings of the 18th International Conference on Pattern Recognition.Washington, DC:IEEE Computer Society, 2006:850-855." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Non-Maximum Suppression">
                                        <b>[21]</b>
                                        NEUBECK A, van GOOL L.Efficient non-maximum suppression[C]//ICPR 2006:Proceedings of the 18th International Conference on Pattern Recognition.Washington, DC:IEEE Computer Society, 2006:850-855.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_22" title="李嘉璇.Tensor Flow技术解析与实战[M].北京:人民邮电出版社, 2017:12-16. (LI J X.Tensor Flow Technology Analysis and Practice[M].Beijing:Posts and Telecom Press, 2017:12-16.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787115456137000&amp;v=MDcxNDduS3JpZlplWnZGeW5uVTdqTUtWc2NYRnF6R2JLNUc5WEpxWTVHWStzUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        李嘉璇.Tensor Flow技术解析与实战[M].北京:人民邮电出版社, 2017:12-16. (LI J X.Tensor Flow Technology Analysis and Practice[M].Beijing:Posts and Telecom Press, 2017:12-16.) 
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_23" title="EVERINGHAM M, van GOOL L, WILLIAMS C K I, et al.The pascal Visual Object Classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MjE0ODkrWnVGaXJsVkx2QkpWWT1OajdCYXJPNEh0SFBxWWRIWStJTFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRa&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                        EVERINGHAM M, van GOOL L, WILLIAMS C K I, et al.The pascal Visual Object Classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_24" title="刘长龙.Python高效开发实战:Django、Tornado、Flask、Twisted[M].北京:电子工业出版社, 2016:175-177. (LIU C L.Efficient Python Development Practices:Django, Tornado, Flask, Twisted[M].Beijing:Publishing House of Electronics Industry, 2016:175-177.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121300103000&amp;v=MTAyMTFuM3hFOWZidm5LcmlmWmVadkZ5bm5VN2pNS1ZzY1hGcXpHYks2SDlMTXI0NUZaK3NQREJNOHp4VVNtRGQ5U0g3&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                        刘长龙.Python高效开发实战:Django、Tornado、Flask、Twisted[M].北京:电子工业出版社, 2016:175-177. (LIU C L.Efficient Python Development Practices:Django, Tornado, Flask, Twisted[M].Beijing:Publishing House of Electronics Industry, 2016:175-177.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-29 10:10</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1669-1674 DOI:10.11772/j.issn.1001-9081.2018102182            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于Faster R-CNN和IoU优化的实验室人数统计与管理系统</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%9B%9B%E6%81%92&amp;code=41987898&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">盛恒</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E9%93%AD&amp;code=09281416&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄铭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%99%B6%E6%99%B6&amp;code=09337604&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨晶晶</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BA%91%E5%8D%97%E5%A4%A7%E5%AD%A6%E6%97%A0%E7%BA%BF%E5%88%9B%E6%96%B0%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0233984&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">云南大学无线创新实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对人员位置相对固定的场景中实时人数统计的管理需求, 以普通高校实验室为例, 设计并实现了一套基于更快速的区域卷积神经网络 (Faster R-CNN) 和交并比 (IoU) 优化的实验室人数统计与管理系统。首先, 使用Faster R-CNN模型对实验室内人员头部进行检测;然后, 根据模型检测的输出结果, 利用IoU算法滤去重复检测的目标;最后, 采用基于坐标定位的方法确定实验室内各个工作台是否有人, 并将相对应的数据存入数据库。该系统主要功能有:①实验室实时视频监控及远程管理;②定时自动拍照检测采集数据, 为实验室的量化管理提供数据支撑;③实验室人员变化数据查询与可视化展示。实验结果表明, 所提基于Faster R-CNN和IoU优化的实验室人数统计与管理系统可用于办公场景中实时人数统计和远程管理。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9B%B4%E5%BF%AB%E9%80%9F%E7%9A%84%E5%8C%BA%E5%9F%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">更快速的区域卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E6%95%B0%E7%BB%9F%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人数统计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%A4%E5%B9%B6%E6%AF%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">交并比;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    盛恒 (1991—) , 男, 湖北武汉人, 硕士研究生, 主要研究方向:深度学习、目标检测;;
                                </span>
                                <span>
                                    *黄铭 (1963—) , 男, 云南文山人, 教授, 博士, 主要研究方向:谱传感、网络通信、无线电监测;huangming@ ynu. edu. cn;
                                </span>
                                <span>
                                    杨晶晶 (1983—) , 女, 云南河口人, 副教授, 博士, 主要研究方向:无线通信、无线电监测。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-30</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61461052, 11564044, 61863035);</span>
                    </p>
            </div>
                    <h1><b>Laboratory personnel statistics and management system based on Faster R-CNN and IoU optimization</b></h1>
                    <h2>
                    <span>SHENG Heng</span>
                    <span>HUANG Ming</span>
                    <span>YANG Jingjing</span>
            </h2>
                    <h2>
                    <span>Wireless Innovation Lab, Yunnan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the management requirement of real-time personnel statistics in office scenes with relatively fixed personnel positions, a laboratory personnel statistics and management system based on Faster Region-based Convolutional Neural Network (Faster R-CNN) and Intersection over Union (IoU) optimization was designed and implemented with an ordinary university laboratory as the example. Firstly, Faster R-CNN model was used to detect the heads of the people in the laboratory. Then, according to the output results of the model detection, the repeatedly detected targets were filtered by using IoU algorithm. Finally, a coordinate-based method was used to determine whether there were people at each workbench in the laboratory and store the corresponding data in the database. The main functions of the system are as follows: ① real-time video surveillance and remote management of the laboratory; ② timed automatic photo, detection and acquisition of data to provide data support for the quantitative management of the laboratory; ③ laboratory personnel change data query and visualization. The experimental results show that the proposed laboratory personnel statistics and management system based on Faster R-CNN and IoU optimization can be used for real-time personnel statistics and remote management in office scenes.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Faster%20Region-based%20Convolutional%20Neural%20Network%20(Faster%20R-CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Faster Region-based Convolutional Neural Network (Faster R-CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=personnel%20statistics&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">personnel statistics;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Intersection%20over%20Union%20(IoU)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Intersection over Union (IoU) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SHENG Heng, born in 1991, M. S. candidate. His research interests include deep learning, object detection. ;
                                </span>
                                <span>
                                    HUANG Ming, born in 1963, Ph. D. , professor. His research interests include spectrum sensing, network communication, radio monitoring. ;
                                </span>
                                <span>
                                    YANG Jingjing, born in 1983, Ph. D. , associate professor. Her research interests include wireless communication, radio monitoring.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-30</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61461052, 11564044, 61863035);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="52">较之传统的室内人数统计技术, 利用目标检测技术实现的室内人数统计技术能够实现对目标的实时检测与统计。因此, 该类室内人数统计技术越来越广泛地被运用在智能监控领域, 该技术的运用能够有效地提升对室内相关人员的监督管理作用。如何利用目标检测技术改进和提高室内实时人数统计技术是许多研究工作者关注的热点。</p>
                </div>
                <div class="p1">
                    <p id="53">文献<citation id="197" type="reference">[<a class="sup">1</a>]</citation>提出基于区域深度特征的人头检测方法;文献<citation id="198" type="reference">[<a class="sup">2</a>]</citation>提出基于骨架特征的人数统计;文献<citation id="199" type="reference">[<a class="sup">3</a>]</citation>提出自适应增强 (Adaptive boosting, Adaboost) 与背景差分级联的室内人数统计方法;文献<citation id="200" type="reference">[<a class="sup">4</a>]</citation>提出基于多特征协同的人头检测方法。这些方法都是基于人工提取图像特征的目标检测方法, 无论是检测准确率, 还是适用范围, 均有较大的局限性。近年来, 随着深度学习的发展, 利用深层神经网络提取图像特征越来越成为目标检测领域主流的研究方向。自Girshick等<citation id="201" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了基于区域的卷积神经网络 (Region-based Convolutional Neural Network, R-CNN) 的目标检测方法, Fast R-CNN<citation id="202" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、Faster R-CNN<citation id="203" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、YOLO (You Only Look Once) <citation id="204" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、单发多盒检测器 (Single Shot multibox Detector, SSD) <citation id="205" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等一系列具有较高准确率的基于深度学习的目标检测算法被相继提出。其中, Faster R-CNN在检测速度较快的同时达到了较高的检测准确率, 因此, 在实际场景中运用较多<citation id="206" type="reference"><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><link href="171" rel="bibliography" /><link href="173" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="54">针对室内人员固定、工位固定这一常见办公场景, 本文以普通高校实验室为例, 提出了一种基于Faster R-CNN和交并比 (Intersection over Union, IoU) 优化的室内人数统计方法。该方法利用深度神经网络提取图像中的人头特征, 具有更高的检测准确率;此外, 提出了基于坐标定位的方法, 能够较为精确地确定实验室每个工作台上是否有人。最后, 利用训练好的检测模型设计并开发了一套实验室人数统计与管理系统, 较好地实现了实验室远程化、自动化、智能化管理。实验结果表明, 该系统可应用于常见的室内办公场景。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">1 Faster R-CNN检测原理</h3>
                <div class="p1">
                    <p id="56">Fast R-CNN作为R-CNN模型的改进, 虽然提升了检测速度, 但它和R-CNN一样, 采用选择性搜索 (Selective Search, SS) 方法<citation id="207" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提取图像的候选目标区域 (Proposal) , 因此依然存在检测步骤繁琐、时间和内存消耗较大等问题。而Faster R-CNN则在模型中引入区域建议网络 (Region Proposal Network, RPN) 提取候选目标区域, 实现了卷积层特征共享, 极大地提升了候选目标区域的生成速度。</p>
                </div>
                <div class="p1">
                    <p id="57">Faster R-CNN网络结构如图1所示, 主要由RPN和Fast R-CNN检测器构成, 其中RPN的输入是经过一系列卷积所提取的图像特征。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Faster R-CNN的网络结构" src="Detail/GetImg?filename=images/JSJY201906020_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Faster R-CNN的网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Network structure of Faster R-CNN</p>

                </div>
                <h4 class="anchor-tag" id="59" name="59">1.1 <b>特征提取网络</b></h4>
                <div class="p1">
                    <p id="60">诸如Alex神经网络 (Alex Neural Network, AlexNet) <citation id="208" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、极深卷积神经网络 (Visual Geometry Group Neural Network, VGGNet) <citation id="209" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、谷歌神经网络 (GoogLe Neural Network, GoogLeNet) <citation id="210" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等传统的深度神经网络可以通过增加网络层数来增加提取到的图像特征数量。但是在深层网络能够收敛的情况下, 随着网络层数的增加, 网络的检测准确率会出现饱和甚至下降的现象, 即网络的“退化”现象<citation id="211" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。He等<citation id="212" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出的残差网络 (Residual Neural Network, ResNet) 能够有效解决网络“退化”现象, 并且具有更为优秀的图像特征学习能力。因此本文选取残差网络作为Faster R-CNN的特征提取网络。</p>
                </div>
                <div class="p1">
                    <p id="61">残差网络的单元结构如图2所示, 假设网络单元的原始映射输出为<i>H</i> (<i>x</i>) , 即<i>H</i> (<i>x</i>) =<i>F</i> (<i>x</i>) +<i>x</i>, 则<i>F</i> (<i>x</i>) = <i>H</i> (<i>x</i>) -<i>x</i>。因此, 深层网络的每一层卷积输出将变为拟合残差。可以简单理解为, 残差网络在传统的深层卷积网络中增加了一些“跨层连接” (图2中的<i>x</i>) <sup></sup><citation id="213" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 当训练误差随着网络的深入而增大时, 残差网络将会跳过某些卷积层, 直接将原始数据输入到之后的卷积层, 既保证了数据传递的完整性, 又相对降低了训练误差, 减少了深层网络的训练难度。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 残差网络的单元结构" src="Detail/GetImg?filename=images/JSJY201906020_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 残差网络的单元结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Unit structure of ResNet</p>

                </div>
                <h4 class="anchor-tag" id="63" name="63">1.2 <b>区域建议网络</b></h4>
                <div class="p1">
                    <p id="64">传统的候选目标区域提取方法都存在着非常耗时的问题, 如Adaboost<citation id="214" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>中使用的滑动窗口和图像金字塔、 R-CNN和Fast R-CNN中使用的SS。而Faster R-CNN所使用的RPN将候选目标区域的提取嵌入到网络内部, 并通过共享卷积层特征参数的方式提升了候选目标区域的生成速度。</p>
                </div>
                <div class="p1">
                    <p id="65">本文中, 结合目标区域的实际像素大小, 为了获得多尺度的检测框, RPN使用一个3×3的卷积核, 在特征提取网络输出的特征图上滑动, 并将卷积核中心对应的区域映射回原始输入图像, 生成4种尺度{16<sup>0.5</sup>, 16, 16<sup>1.5</sup>, 16<sup>2</sup>}和3种长宽比{0.5, 1, 2}共12个的锚点 (anchor) , 如图3所示。因此, 在每一个滑动窗口的卷积核中心对应有12个建议区域。</p>
                </div>
                <div class="p1">
                    <p id="66">RPN是一种全卷积网络, 其输入的是特征提取网络输出的原始图像卷积特征图, 主要结构如图4所示。每个锚点所对应的建议区域经过中间层的卷积计算输出512维的特征向量, 然后分别被送入分类层和位置回归层。其中, 分类层输出对应锚点中目标的分类信息, 包括背景的置信度和目标类别的置信度;位置回归层输出锚点中目标的位置信息, 包括目标区域的中心点坐标、长度和高度。最后, 采用非极大值抑制 (Non-Maximum Suppression, NMS) 算法<sup></sup><citation id="215" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 根据所有锚点的分类信息和位置信息, 对候选目标区域进行筛选, 从而得到2 000个质量较高的目标候选区域。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 4种尺度与3种长宽比的锚点" src="Detail/GetImg?filename=images/JSJY201906020_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 4种尺度与3种长宽比的锚点  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Anchors of 4 scales and 3 aspect ratios</p>

                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 RPN网络结构" src="Detail/GetImg?filename=images/JSJY201906020_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 RPN网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Network structure of RPN</p>

                </div>
                <h4 class="anchor-tag" id="69" name="69">1.3 Fast R-CNN<b>检测网络</b></h4>
                <div class="p1">
                    <p id="70">对于RPN中生成的目标候选区域, 需要送到Fast R-CNN检测器中作进一步的精确分类和坐标回归。由于这些目标候选区域的尺寸大小不一, 它们首先将被送入到感兴趣区域 (Region of Interest, RoI) 池化层进行处理, 从而得到统一大小的目标候选区域。</p>
                </div>
                <div class="p1">
                    <p id="71">从图1中可以看出, RoI池化层结合特征图和目标候选区域进行坐标映射, 输出固定大小的目标候选区域。接着, 这些目标候选区域被送到Fast R-CNN检测器中进行训练, 得到包括分类信息和坐标信息在内的最终检测结果。</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag">2 实验过程与结果分析</h3>
                <h4 class="anchor-tag" id="73" name="73">2.1 <b>数据产生与训练</b></h4>
                <div class="p1">
                    <p id="74">本文的实验数据利用实验室内顶部的单目摄像头采集。顶部摄像头所拍摄到的画面, 人体各部位存在着较多的遮挡, 因此选定人体头部作为检测目标, 进而确定实验内人员数量和位置分布。共采集到约6 000张原始图片, 经过翻转、对称, 数据集扩展到约24 000张。图片尺寸统一为1 510×860像素, 每张图片中人数从1到10不等。按照10∶1的比例将数据集随机分为训练集和测试集。数据集图像示例如图5所示。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 数据集图像示例" src="Detail/GetImg?filename=images/JSJY201906020_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 数据集图像示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Dataset image examples</p>

                </div>
                <div class="p1">
                    <p id="76">本文的实验环境为Windows10, GeForce GTX 1080Ti, 并利用主流的深度学习框架TensorFlow<citation id="216" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>实现网络模型, 采用ResNet101<citation id="217" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>作为整个模型的特征提取网络。批文件大小为4, 初始学习率为0.000 3, 迭代到40 000次后学习率降到0.000 03, 迭代到80 000次后学习率降到0.000 003, 共迭代200 000次。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">2.2 <b>结果分析</b></h4>
                <div class="p1">
                    <p id="78">本文中, 经过训练, Faster R-CNN模型在测试集上的mAP (mean Average Precision) <citation id="218" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>达到了98.49%。训练过程中的损失值 (loss) <citation id="219" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>曲线如图6所示。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 训练过程中的损失值" src="Detail/GetImg?filename=images/JSJY201906020_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 训练过程中的损失值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Loss during training process</p>

                </div>
                <div class="p1">
                    <p id="80">由图6可以看出, 模型损失值在经过18万次迭代后达到了收敛, 稳定在0.15左右。模型检测性能比较突出, 究其原因可以归于:</p>
                </div>
                <div class="p1">
                    <p id="81">1) 实验室内场景背景单一, 人员流动性小, 人员和背景变化都较少, 图像特征更为突出;</p>
                </div>
                <div class="p1">
                    <p id="82">2) 训练数据样本较多, 训练集包含有2万多张图片, 共约7万个标记的人体头部样本;</p>
                </div>
                <div class="p1">
                    <p id="83">3) 针对不同尺度的目标, 采用4种尺度、3种长宽比, 共计12种的锚点, 能够较好地检测不同尺度的目标;</p>
                </div>
                <div class="p1">
                    <p id="84">4) 模型利用RPN生成高质量的目标候选区域, 为后续Fast R-CNN网络提供了质量较高训练数据。</p>
                </div>
                <div class="p1">
                    <p id="85">为了进一步研究模型的泛化能力, 即在实际场景中的检测性能, 从摄像头拍摄的画面中采集105张图片作为增量测试集, 测试不同置信度下模型的检测性能。检测模型最常用的评价指标是精确率和召回率。调用模型进行检测时存在着以下四种情况:1) 实际上是目标, 检测认为是目标;2) 实际上是目标, 检测认为不是目标;3) 实际上不是目标, 检测认为是目标;4) 实际上不是目标, 检测认为不是目标。</p>
                </div>
                <div class="p1">
                    <p id="86">模型对目标进行检测的四种可能情况如表1所示。</p>
                </div>
                <div class="area_img" id="87">
                    <p class="img_tit"><b>表</b>1 <b>模型检测的四种情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.1 Four cases of model detection</p>
                    <p class="img_note"></p>
                    <table id="87" border="1"><tr><td><br />情况</td><td>检测认为是目标</td><td>检测认为不是目标</td></tr><tr><td><br />实际上是目标</td><td>TP (真是) </td><td>FN (假否) </td></tr><tr><td><br />实际上不是目标</td><td>FP (假是) </td><td>TN (真否) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="88">因此, 可以给出精确率和召回率的定义:</p>
                </div>
                <div class="p1">
                    <p id="89"><i>P</i>=<i>TP</i>/ (<i>TP</i>+<i>FP</i>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="90"><i>R</i>=<i>TP</i>/ (<i>TP</i>+<i>FN</i>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="91">其中, <i>P</i>为模型精确率, <i>R</i>为模型召回率。精确率指的是模型给出的检测结果中有多少是正确的, 而召回率指的是实际上正确的目标有多少被检测出来了。这两者通常是此消彼长的, 为了综合考虑这两个指标, 引入一个新的评价指标, 精确率和召回率的加权调和平均值<i>F</i>-<i>Score</i>。</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>-</mo><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>*</mo><mfrac><mrow><mi>Ρ</mi><mo>*</mo><mi>R</mi></mrow><mrow><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo>*</mo><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">其中<i>β</i>为调和参数, 当<i>β</i>&lt;1时, 则认为精确率更重要些, 当<i>β</i>&gt;1时, 则认为召回率更重要些。本文中, 认为召回率和准确率同样重要, 即取<i>β</i>=1, 因此, 本文的加权调和平均值为<i>F</i>1-<i>Score</i>:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mn>1</mn><mo>-</mo><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mo>*</mo><mi>Ρ</mi><mo>*</mo><mi>R</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">增量测试集有105张图片, 共445实际目标。通过增量测试集对模型进行测试, 统计不同检测置信度下的<i>F</i>1-<i>Score</i>, 如图7所示。</p>
                </div>
                <div class="p1">
                    <p id="96">从图7可以看出, 模型在检测置信度为50%～99%均有较高的<i>F</i>1-<i>Score</i>, <i>F</i>1-<i>Score</i>均大于91%, 说明模型具有较强的泛化能力。同时, 当检测置信度为96%时, <i>F</i>1-<i>Score</i>达到最高, 为95.7%, 说明模型此时具有最为良好的检测效果, 即当检测置信度为96%时模型泛化能力最好。</p>
                </div>
                <div class="p1">
                    <p id="97">在增量测试过程中, 检测时间共28.89 s, 平均检测速度为每张275.1 ms, 远远低于文献<citation id="220" type="reference">[<a class="sup">7</a>]</citation>中的检测速度, 表明了本文实现的模型虽然具有较高的检测精度, 但检测速度难以达到视频流实时检测的目的。在目标检测领域中, 能够提高检测速度的方法通常有三种:</p>
                </div>
                <div class="p1">
                    <p id="98">1) 减小输入图像的尺寸。该方法适用于监控区域较小的场景, 本文的输入图像大小为1 510×860像素, 仅正好覆盖实验监控区域, 因此不适用该方法。</p>
                </div>
                <div class="p1">
                    <p id="99">2) 采用计算性能更强大的硬件设备。该方法需要耗费大量的资金, 因此更适用于工业领域。</p>
                </div>
                <div class="p1">
                    <p id="100">3) 利用网络结构更加精简的检测模型。该方法通常会在一定程度上牺牲检测精度, 如SSD, 因此更适用于对检测精度要求不高、对检测速度要求高的应用领域。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同置信度下的F1-Score" src="Detail/GetImg?filename=images/JSJY201906020_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同置信度下的<i>F</i>1-<i>Score</i>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 <i>F</i>1-<i>Score</i> with different confidence level</p>

                </div>
                <div class="p1">
                    <p id="102">实验室内, 人员绝大部分时候都是坐着工作的状态, 流动性很小。在这种场景中一味的追求检测速度是没有必要的, 因此本文没有采用上述的三种方法, 而是将系统设计为每分钟自动检测一次, 因此模型的检测速度是满足系统的设计要求的。对于常见的办公场景, 每分钟检测一次的设计足以为管理者的科学管理提供可靠的数据。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">2.3 IoU<b>优化</b></h4>
                <div class="p1">
                    <p id="104">本文实现的模型准确率和泛化能力都比较突出, 检测结果较为良好, 如图8所示。但也会出现如图9所示的误检情况, 图9中, 检测模型将图中左下角的一个人体头部检测成了两个。针对这种情况, 本文利用IoU算法作进一步的优化。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 检测结果示例1" src="Detail/GetImg?filename=images/JSJY201906020_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 检测结果示例1  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Detection result example 1</p>

                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 检测结果示例2" src="Detail/GetImg?filename=images/JSJY201906020_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 检测结果示例2  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Detection result example 2</p>

                </div>
                <div class="p1">
                    <p id="107">IoU指的是两个有重叠区域的检测框的重叠率, 即这两个检测框之间交集与并集的比值。</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mrow><mi>S</mi><mo stretchy="false"> (</mo><mi>C</mi><mo stretchy="false">) </mo></mrow><mrow><mi>S</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∪</mo><mi>S</mi></mstyle><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>S</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>S</mi></mstyle><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo></mrow><mrow><mi>S</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∪</mo><mi>S</mi></mstyle><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">如图10所示, <i>A</i>框与<i>B</i>框有重叠区域。其中, <i>S</i> (<i>A</i>) 、<i>S</i> (<i>B</i>) 、<i>S</i> (<i>C</i>) 分别代表<i>A</i>、<i>B</i>和<i>C</i>框的面积。其算法实现流程如图11所示。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 IoU 示意图" src="Detail/GetImg?filename=images/JSJY201906020_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 IoU 示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Schematic diagram of IoU</p>

                </div>
                <div class="p1">
                    <p id="111">利用Faster R-CNN目标检测模型对目标图像进行检测后, 将检测输出的所有包含位置信息的检测框输入到IoU算法中。其中<i>number</i>为模型检测出来的目标数量, 即模型检测输出的实验室内人员数量。最后输出的<i>N</i>则是经过IoU过滤后的实验室内实际人数。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 IoU 算法流程" src="Detail/GetImg?filename=images/JSJY201906020_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 IoU 算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.11 Flow chart of IoU algorithm</p>

                </div>
                <h4 class="anchor-tag" id="113" name="113">2.4 <b>人员定位</b></h4>
                <div class="p1">
                    <p id="114">针对实验室内人员流动性小、环境单一, 以及人员位置相对固定的特点, 本文提出基于坐标定位的方法来确定每个工作台上是否有人, 并将相应数据存入数据库, 为实验室的科学管理提供可靠的数据支撑。如图12所示, 预先将监控区域划分为12个矩形区域, 分别代表其区域内的工作台。首先利用模型检测输出的人员目标位置信息确定人体头部的质心, 然后逐一进行判别, 质心落在在哪个区域则认为哪个区域的工作台上有人。如图12所示, 可认为2、3、4、6、8号区域的工作台上有人。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 区域划分" src="Detail/GetImg?filename=images/JSJY201906020_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 区域划分  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.12 Region division</p>

                </div>
                <h3 id="116" name="116" class="anchor-tag">3 系统实现与展示</h3>
                <div class="p1">
                    <p id="117">系统利用开源Web开发框架Django<citation id="221" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>开发而成, 系统主要有两大功能模块:系统历史数据查询与展示模块和实时视频监控与检测模块。系统基于B/S模式开发而成, 具备远程管理功能。授权用户在校园网范围内, 通过PC端浏览器登录系统均可进行访问。</p>
                </div>
                <div class="p1">
                    <p id="118">在每天6:30—23:30, 系统服务器每分钟自动拍摄一张实验室监控画面, 并调用检测模型对其进行检测;然后对检测结果进行人员定位, 确定每个工作台是否有人;最后将相应数据存入数据库, 以便实验室管理员查询。图13为实验室内单日人数变化与一段日期内人数变化查询页面;图14为实验室内历史监控画面查看页面与查看结果;图15为实验室各个工作台的详细使用情况查询页面 (以6号工作台为例) 。本系统已在实验室稳定运行半年, 其推广应用价值被验证。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_119.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 实验室人数变化" src="Detail/GetImg?filename=images/JSJY201906020_119.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 实验室人数变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_119.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Changes of number of people in laboratory</p>

                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_120.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 实验室历史监控画面" src="Detail/GetImg?filename=images/JSJY201906020_120.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 实验室历史监控画面  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_120.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.14 Laboratory history monitoring screen</p>

                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906020_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 工作台详细使用情况查询页面 (6号工作台)" src="Detail/GetImg?filename=images/JSJY201906020_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图15 工作台详细使用情况查询页面 (6号工作台)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906020_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.15 Workbench detailed usage query page (workbench 6) </p>

                </div>
                <h3 id="122" name="122" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="123">针对常见办公场景中人员固定、工位固定的特点, 以普通高校实验室为例, 提出了基于Faster R-CNN和IoU优化的室内人数统计方法。实验结果表明, 所提方法具有良好的检测准确效果。然后, 结合检测结果, 采用基于坐标定位的方法确定室内每个工作台是否有人。最后, 利用Django框架, 开发了一套实验室人数统计与管理系统, 实现了实验室的远程管理、自动管理和智能管理。但本文所研发的系统存在系统人员定位依赖于人员位置相对固定这一前提条件的问题, 当人员位置发生移动时系统无法作出准确的判断。因此, 接下来将在保证检测精度的前提下进一步开展视频帧之间的目标跟踪算法研究, 通过绘制人员的移动轨迹进而实现人员的动态定位。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="149">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201311010&amp;v=MTMxMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyckFNalhTWkxHNEg5TE5ybzlFWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>何扬名, 杜建强, 肖贤波.基于区域深度特征的人头检测方法[J].微电子学与计算机, 2013, 30 (11) :39-42. (HE Y M, DU J Q, XIAO XB.Detecting human head by depth characteristics of regions[J].Microelectronics&amp;Computer, 2013, 30 (11) :39-42.) 
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201402065&amp;v=MTkwMzZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3JyQUx6N0JkN0c0SDlYTXJZOUQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>夏菁菁, 高琳, 范勇, 等.基于骨架特征的人数统计[J].计算机应用, 2014, 34 (2) :585-588. (XIA J J, GAO L, FAN Y, et al.People counting based on skeleton feature[J].Journal of Computer Applications, 2014, 34 (2) :585-588.) 
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FJSZ201701002&amp;v=MTE4NjNVUjdxZlp1WnNGeS9oV3JyQUl5ZllkTEc0SDliTXJvOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>叶锋, 洪斯婷, 郑德城, 等.基于Adaboost与背景差分级联的室内人数统计方法[J].福建师范大学学报 (自然科学版) , 2017, 33 (1) :7-13. (YE F, HONG S T, ZHENG D C, et al.A people counting method based on Adaboost and background subtraction in indoor environment[J].Journal of Fujian Normal University (Natural Science Edition) , 2017, 33 (1) :7-13.) 
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XNZK201807010&amp;v=MTgzNjc0SDluTXFJOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3JyQVBTUFJaYkc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>张晓琪, 宋钢.基于多特征协同的人头检测新方法[J].西南师范大学学报 (自然科学版) , 2018, 43 (7) :46-52. (ZHANG XQ, SONG G.A new head detection method oriented for vertical monocular camera way[J].Journal of Southwest China Normal University (Natural Science Edition) , 2018, 43 (7) :46-52.) 
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[5]</b>GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:580-587.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[7]</b>REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//NIPS 2015:Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MITPress, 2015:91-99.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified,Real-Time Object Detection">

                                <b>[8]</b>REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:779-788.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">

                                <b>[9]</b>LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//Proceedings of the 2016 European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGJL201802004&amp;v=MDk2NDU3cWZadVpzRnkvaFdyckFQeXJCWXJHNEg5bk1yWTlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>王黎, 陆慧娟, 叶敏超, 等.Faster R-CNN的癌症影像检测方法[J].中国计量大学学报, 2018, 29 (2) :136-141. (WANG L, LU H J, YE M C, et al.A cancer image detection method based on Faster R-CNN[J].Journal of China University of Metrology, 2018, 29 (2) :136-141.) 
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SIPD&amp;filename=SIPD6F8F25FC2394A5034215D8F7E3CE6557&amp;v=MjQyODlpVGJhclhPRnFmT3F2azJadWdHQ0EwOHp4VVg2RDU0UEhlVXEyYzJDc2VTUUwrWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3eTJ4YUE9Tg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>HUANG W Q, HUANG M Z, ZHANG Y T.Detection of traffic signs based on combination of GAN and faster R-CNN[J].Journal of Physics:Conference Series, 2018, 1069 (1) :012159.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2017S2021&amp;v=MDE1Mjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3JyQUx6N0JkN0c0SDlhdnJZOUhaWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>戴陈卡, 李毅.基于Faster R-CNN以及多部件结合的机场场面静态飞机检测[J].计算机应用, 2017, 37 (z2) :85-88. (DAI CK, LI Y.Aeroplane detection in static aerodrome based on faster R-CNN and multi-part model[J].Journal of Computer Applications, 2017, 37 (z2) :85-88.) 
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXDG201802005&amp;v=MzAwOTE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3JyQU1qWFBhYkc0SDluTXJZOUZZWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>胡炎, 单子力, 高峰.基于Faster R-CNN和多分辨率SAR的海上舰船目标检测[J].无线电工程, 2018, 48 (2) :96-100. (HUY, SHAN Z L, GAO F.Ship detection based on Faster R-CNNand multiresolution SAR[J].Radio Engineering, 2018, 48 (2) :96-100.) 
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MDg1NzA3QmFySzdIdG5Nclk5RlpPb01Dbjg5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjFvZGJ4bz1Oag==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>UIJLINGS J R R, van de SANDE K E A, GEVERS T, et al.Selective search for object recognition[J].International Journal of Computer Vision, 2013, 104 (2) :154-171.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[16]</b>SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].ar Xiv Preprint, 2014, 2014:ar Xiv.1409.1556.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_17" >
                                    <b>[17]</b>
                                SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[18]</b>HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201802006&amp;v=MjkyMjVMRzRIOW5Nclk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnJBS0Q3WWI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>彭刚, 杨诗琪, 黄心汉, 等.改进的基于区域卷积神经网络的微操作系统目标检测方法[J].模式识别与人工智能, 2018, 31 (2) :142-149. (PENG G, YANG S Q, HUANG X H, et al.Improved object detection method of micro-operating system based on region convolutional neural network[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (2) :142-149.) 
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An extended set of haar-like features for rapid object detection">

                                <b>[20]</b>LIENHART R, MAYDT J.An extended set of Haar-like features for rapid object detection[C]//Proceedings of the 2002 International Conference on Image Processing.Piscataway, NJ:IEEE, 2002:900-903.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Non-Maximum Suppression">

                                <b>[21]</b>NEUBECK A, van GOOL L.Efficient non-maximum suppression[C]//ICPR 2006:Proceedings of the 18th International Conference on Pattern Recognition.Washington, DC:IEEE Computer Society, 2006:850-855.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787115456137000&amp;v=MzAwMzdCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3ak1LVnNjWEZxekdiSzVHOVhKcVk1R1krc1BE&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>李嘉璇.Tensor Flow技术解析与实战[M].北京:人民邮电出版社, 2017:12-16. (LI J X.Tensor Flow Technology Analysis and Practice[M].Beijing:Posts and Telecom Press, 2017:12-16.) 
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=Mjk3MThrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJsVkx2QkpWWT1OajdCYXJPNEh0SFBxWWRIWStJTFkz&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b>EVERINGHAM M, van GOOL L, WILLIAMS C K I, et al.The pascal Visual Object Classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121300103000&amp;v=MjY0MjJublU3ak1LVnNjWEZxekdiSzZIOUxNcjQ1Rlorc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b>刘长龙.Python高效开发实战:Django、Tornado、Flask、Twisted[M].北京:电子工业出版社, 2016:175-177. (LIU C L.Efficient Python Development Practices:Django, Tornado, Flask, Twisted[M].Beijing:Publishing House of Electronics Industry, 2016:175-177.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906020" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906020&amp;v=MDEzMTQvaFdyckFMejdCZDdHNEg5ak1xWTlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="3" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
