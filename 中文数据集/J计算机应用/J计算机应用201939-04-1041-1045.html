<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775751846250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904018%26RESULT%3d1%26SIGN%3dHuQdXGWuRHTubDJuufESiHqbBLQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904018&amp;v=MjY5OTFzRnlEZ1ZML0lMejdCZDdHNEg5ak1xNDlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 相关理论基础 ">1 相关理论基础</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="1.1 SNN&lt;b&gt;网络一般结构&lt;/b&gt;">1.1 SNN<b>网络一般结构</b></a></li>
                                                <li><a href="#57" data-title="1.2 &lt;b&gt;损失函数&lt;/b&gt;">1.2 <b>损失函数</b></a></li>
                                                <li><a href="#72" data-title="1.3 &lt;b&gt;度量函数&lt;/b&gt;">1.3 <b>度量函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="2 相似性度量实验及分析 ">2 相似性度量实验及分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="3 分类验证实验及比较 ">3 分类验证实验及比较</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="3.1 &lt;b&gt;分类实验流程&lt;/b&gt;">3.1 <b>分类实验流程</b></a></li>
                                                <li><a href="#89" data-title="3.2 &lt;b&gt;超参数选择&lt;/b&gt;">3.2 <b>超参数选择</b></a></li>
                                                <li><a href="#97" data-title="3.3 &lt;b&gt;数值实验&lt;/b&gt;">3.3 <b>数值实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="图1 孪生网络结构">图1 孪生网络结构</a></li>
                                                <li><a href="#77" data-title="图2 Beef数据集中5类时序数据">图2 Beef数据集中5类时序数据</a></li>
                                                <li><a href="#79" data-title="图3 实验数据t-SNE可视化">图3 实验数据t-SNE可视化</a></li>
                                                <li><a href="#96" data-title="图4 隐层神经元个数对错误率的影响">图4 隐层神经元个数对错误率的影响</a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;三种分类器的实验结果对比&lt;/b&gt;"><b>表</b>1 <b>三种分类器的实验结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="163">


                                    <a id="bibliography_1" title="崔婧, 赵秀娟, 宋吟秋.中日股价序列相似性的比较分析[J].系统工程理论与实践, 2009, 29 (12) :125-133. (CUI J, ZHAOX J, SONG Y Q.Similarity analysis on China&#39;s and Japan&#39;s security price series[J].Systems Engineering-Theory and Practice, 2009, 29 (12) :125-133.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTLL200912020&amp;v=MDg4NTMzenFxQnRHRnJDVVI3cWZadVpzRnlEZ1ZML0lQVG5IWXJHNEh0ak5yWTlIWklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        崔婧, 赵秀娟, 宋吟秋.中日股价序列相似性的比较分析[J].系统工程理论与实践, 2009, 29 (12) :125-133. (CUI J, ZHAOX J, SONG Y Q.Similarity analysis on China&#39;s and Japan&#39;s security price series[J].Systems Engineering-Theory and Practice, 2009, 29 (12) :125-133.) 
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_2" title="SIVARAKS H, RATANAMAHATANA C A.Robust and accurate anomaly detection in ECG artifacts using time series motif discovery[J].Computational and Mathematical Methods in Medicine, 2015, 2015:453214." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust and Accurate Anomaly Detection in ECG Artifacts Using Time Series Motif Discovery">
                                        <b>[2]</b>
                                        SIVARAKS H, RATANAMAHATANA C A.Robust and accurate anomaly detection in ECG artifacts using time series motif discovery[J].Computational and Mathematical Methods in Medicine, 2015, 2015:453214.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_3" title="陈海燕, 刘晨晖, 孙博.时间序列数据挖掘的相似性度量综述[J].控制与决策, 2017, 32 (1) :1-11. (CHEN H Y, LIU C H, SUN B.Survey on similarity measurement of time series data mining[J].Control and Decision, 2017, 32 (1) :1-11.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201701001&amp;v=MjIxMTlHRnJDVVI3cWZadVpzRnlEZ1ZML0lMamZTYmJHNEg5Yk1ybzlGWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        陈海燕, 刘晨晖, 孙博.时间序列数据挖掘的相似性度量综述[J].控制与决策, 2017, 32 (1) :1-11. (CHEN H Y, LIU C H, SUN B.Survey on similarity measurement of time series data mining[J].Control and Decision, 2017, 32 (1) :1-11.) 
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                    BERNDT D J, CLIFFORD J.Using dynamic time warping to find patterns in time series[C]//AAAIWS 1994:Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining.Menlo Park, CA:AAAI Press, 1994, 10 (16) :359-370.</a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_5" title="FALOUTSOS C, RANGANATHAN M, MANOLOPOULOS Y.Fast subsequence matching in time-series databases[C]//SIGMOD1994:Proceedings of the 1994 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1994:419-429." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast subsequence matching in time-series databases">
                                        <b>[5]</b>
                                        FALOUTSOS C, RANGANATHAN M, MANOLOPOULOS Y.Fast subsequence matching in time-series databases[C]//SIGMOD1994:Proceedings of the 1994 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1994:419-429.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_6" title="李海林, 梁叶, 王少春.时间序列数据挖掘中的动态时间弯曲研究综述[J].控制与决策, 2018, 33 (8) :1345-1353. (LI HL, LIANG Y, WANG S C.Review on dynamic time warping in time series data mining[J].Control and Decision, 2018, 33 (8) :1345-1353.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201808001&amp;v=MDYyMjRSN3FmWnVac0Z5RGdWTC9JTGpmU2JiRzRIOW5NcDQ5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        李海林, 梁叶, 王少春.时间序列数据挖掘中的动态时间弯曲研究综述[J].控制与决策, 2018, 33 (8) :1345-1353. (LI HL, LIANG Y, WANG S C.Review on dynamic time warping in time series data mining[J].Control and Decision, 2018, 33 (8) :1345-1353.) 
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_7" title="沈媛媛, 严严, 王菡子.有监督的距离度量学习算法研究进展[J].自动化学报, 2014, 40 (12) :2673-2686. (SHEN Y Y, YAN Y, WANG H Z.Recent advances on supervised distance metric learning algorithms[J].Acta Automatica Sinica, 2014, 40 (12) :2673-2686.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201412002&amp;v=MDIyNzZWTC9JS0NMZlliRzRIOVhOclk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        沈媛媛, 严严, 王菡子.有监督的距离度量学习算法研究进展[J].自动化学报, 2014, 40 (12) :2673-2686. (SHEN Y Y, YAN Y, WANG H Z.Recent advances on supervised distance metric learning algorithms[J].Acta Automatica Sinica, 2014, 40 (12) :2673-2686.) 
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_8" title="BROMLEY J, GUYON I, LECUN Y, et al.Signature verification using a&quot;siamese&quot;time delay neural network[C]//NIPS 1993:Proceedings of the 6th International Conference on Neural Information Processing Systems.San Francisco, CA:Morgan Kaufmann Publishers, 1994:737-744." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Signature Verification using a&amp;quot;Siamese&amp;quot;Time Delay Neural Network">
                                        <b>[8]</b>
                                        BROMLEY J, GUYON I, LECUN Y, et al.Signature verification using a&quot;siamese&quot;time delay neural network[C]//NIPS 1993:Proceedings of the 6th International Conference on Neural Information Processing Systems.San Francisco, CA:Morgan Kaufmann Publishers, 1994:737-744.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_9" title="CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with application to face verification[C]//CVPR2005:Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2005:539-546." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a similarity met-ric discriminatively,with application to face verification">
                                        <b>[9]</b>
                                        CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with application to face verification[C]//CVPR2005:Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2005:539-546.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_10" title="WANG F Q, ZUO W M, LIN L, et al.Joint learning of single-image and cross-image representations for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:1288-1296." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint learning of single-image and cross-image representations for person re-identification">
                                        <b>[10]</b>
                                        WANG F Q, ZUO W M, LIN L, et al.Joint learning of single-image and cross-image representations for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:1288-1296.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_11" title="DONG Y, ZHEN L, SHENG L, et al.Deep metric learning for person re-identification[C]//Proceedings of the 2014 22nd International Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2014:34-39." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep metric learning for person re-identification">
                                        <b>[11]</b>
                                        DONG Y, ZHEN L, SHENG L, et al.Deep metric learning for person re-identification[C]//Proceedings of the 2014 22nd International Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2014:34-39.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_12" title="HUANG P S, HE X D, GAO J F, et al.Learning deep structured semantic models for Web search using clickthrough data[C]//Proceedings of the 22nd ACM International Conference on Conference on Information&amp;amp;Knowledge Management.New York:ACM, 2013:2333-2338." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep structured semantic models for web search using clickthrough data">
                                        <b>[12]</b>
                                        HUANG P S, HE X D, GAO J F, et al.Learning deep structured semantic models for Web search using clickthrough data[C]//Proceedings of the 22nd ACM International Conference on Conference on Information&amp;amp;Knowledge Management.New York:ACM, 2013:2333-2338.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_13" title="COVER T, HART P.Nearest neighbor pattern classification[J].IEEE Transactions on Information Theory, 1967, 13 (1) :21-27." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nearest neighbor pattern classification">
                                        <b>[13]</b>
                                        COVER T, HART P.Nearest neighbor pattern classification[J].IEEE Transactions on Information Theory, 1967, 13 (1) :21-27.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_14" title="BATISTA G E, WANG X, KEOGH E J.A complexity-invariant distance measure for time series[EB/OL].[2018-05-10].https://epubs.siam.org/doi/pdf/10.1137/1.9781611972818.60." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A complexity-invariant distance measure for time series">
                                        <b>[14]</b>
                                        BATISTA G E, WANG X, KEOGH E J.A complexity-invariant distance measure for time series[EB/OL].[2018-05-10].https://epubs.siam.org/doi/pdf/10.1137/1.9781611972818.60.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_15" title="MAATEN L, HINTON G.Visualizing data using t-SNE[J].Journal of Machine Learning Research, 2008, 9:2579-2605." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing data using t-SNE">
                                        <b>[15]</b>
                                        MAATEN L, HINTON G.Visualizing data using t-SNE[J].Journal of Machine Learning Research, 2008, 9:2579-2605.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_16" title="KINGMA D P, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-05-10].https://arxiv.org/pdf/1412.6980." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">
                                        <b>[16]</b>
                                        KINGMA D P, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-05-10].https://arxiv.org/pdf/1412.6980.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_17" title="CHEN Y, KEOGH E, HU B, et al.The UCR time series classification archive[DB/OL].[2018-05-10].http://www.cs.ucr.edu/~eamonn/time_series_data/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The UCR time series classification archive">
                                        <b>[17]</b>
                                        CHEN Y, KEOGH E, HU B, et al.The UCR time series classification archive[DB/OL].[2018-05-10].http://www.cs.ucr.edu/~eamonn/time_series_data/.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-11-16 14:11</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),1041-1045 DOI:10.11772/j.issn.1001-9081.2018081837            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于孪生神经网络的时间序列相似性度量</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E9%80%B8%E5%87%A1&amp;code=41473327&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜逸凡</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E9%9D%92&amp;code=06475235&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶青</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%95%BF%E6%B2%99%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0175884&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长沙理工大学电气与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在时间序列分类等数据挖掘工作中, 不同数据集基于类别的相似性表现有明显不同, 因此一个合理有效的相似性度量对数据挖掘非常关键。传统的欧氏距离、余弦距离和动态时间弯曲等方法仅针对数据自身进行相似度公式计算, 忽略了不同数据集所包含的知识标注对于相似性度量的影响。为了解决这一问题, 提出基于孪生神经网络 (SNN) 的时间序列相似性度量学习方法。该方法从样例标签的监督信息中学习数据之间的邻域关系, 建立时间序列之间的高效距离度量。在UCR提供的时间序列数据集上进行的相似性度量和验证性分类实验的结果表明, 与ED/DTW-1NN相比SNN在分类质量总体上有明显的提升。虽然基于动态时间弯曲 (DTW) 的1近邻 (1NN) 分类方法在部分数据上表现优于基于SNN的1NN分类方法, 但在分类过程的相似度计算复杂度和速度上SNN优于DTW。可见所提方法能明显提高分类数据集相似性的度量效率, 在高维、复杂的时间序列的数据分类上有不错的表现。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时间序列;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相似性度量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%AA%E7%94%9F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孪生神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    姜逸凡 (1993—) , 男, 湖南娄底人, 硕士研究生, 主要研究方向:机器学习、数据挖掘;;
                                </span>
                                <span>
                                    *叶青 (1963—) , 女, 湖南长沙人, 教授, 硕士, 主要研究方向:模式识别、道路交通智能检测。电子邮箱594143395@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-05</p>

            </div>
                    <h1><b>Time series similarity measure based on Siamese neural network</b></h1>
                    <h2>
                    <span>JIANG Yifan</span>
                    <span>YE Qing</span>
            </h2>
                    <h2>
                    <span>School of of Electrical & Information Engineering, Changsha University of Science & Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In data mining such as time series classification, the similarity performance based on category of different datasets are significantly different from each other. Therefore, a reasonable and effective similarity measure is crucial to data mining. The traditional methods such as Euclidean Distance (ED) , cosine distance and Dynamic Time Warping (DTW) only focus on the similarity formula of the data themselves, but ignore the influence of the knowledge annotation contained in different datasets on the similarity measure. To solve this problem, a learning method of time series similarity measure based on Siamese Neural Network (SNN) was proposed. In the method, the neighborhood relationship between the data was learnt from the supervision information of sample tags, and an efficient distance measure between time series was established. The similarity measurement and confirmatory classification experiments were performed on UCR-provided time series datasets. Experimental results show that compared with ED/DTW-1 NN (one Nearest Neighbors) , the overall classification quality of SNN is improved significantly. The Dynamic Time Warping (DTW) -based 1 NN calssification method outperforms the SNN-based 1 NN classification method on some data, but SNN outperforms DTW in complexity and speed of similarity calculation during the classification. The results show that the proposed method can significantly improve the measurement efficiency of the classification of dataset similarity, and has good performance for high-dimensional and complex time-series data classification.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=time%20serie&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">time serie;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=similarity%20measure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">similarity measure;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Siamese%20Neural%20Network%20(SNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Siamese Neural Network (SNN) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JIANG Yifan, born in 1993, M. S. candidate. His research interests include machine learning, data mining.;
                                </span>
                                <span>
                                    YE Qing, born in 1963, M. S. , professor. Her research interests include pattern recognition, Intelligent detection of road traffic.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-05</p>
                            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="38">时间序列是某一事件随着时间的推移产生的一系列数据, 由一定的时间间隔提取采集得到, 对于时间序列的数据挖掘在工业、农业、经济、医疗等领域时间序列都有广泛的应用, 例如证券市场的基金、股票数据分析研究<citation id="197" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、辅助心电图疾病诊断<citation id="198" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等。时间序列的分类 (Time Series Classification, TSC) 等数据挖掘研究关键之一是时间序列之间的相似性度量, 不同的相似性度量方法对时间序列的挖掘性能有很大的影响。目前主流的时间序列相似性度量使用L-P范数距离 (如欧氏距离 (European Distance, ED) ) 和动态时间弯曲 (Dynamic Time Warping, DTW) <citation id="199" type="reference"><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><link href="171" rel="bibliography" /><link href="173" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。这两种距离度量方法都局限于时间序列之间特征向量的固定公式数值计算, 不能有效利用标注好的类别标签。实际上, 不同数据集的相似性描述并不能一概而论, 数据样本中包含的关于类别的先验知识蕴含了相似性的统计规律, 这些统计规律可以从标注好的训练集中进行学习, 进而构成更有效的数据相似性度量的表达方式。</p>
                </div>
                <div class="p1">
                    <p id="39">近年来深度学习在很多领域得到了广泛的应用, 一些学者结合距离度量学习<citation id="200" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>与孪生神经网络 (Siamese Neural Network, SNN) 进行了各种应用研究。SNN最早用于手写签名验证<citation id="201" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 验证平板电脑上书写的签名真伪。文献<citation id="203" type="reference">[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</citation>将图像识别中常用的卷积神经网络组成孪生卷积网络 (Siamese Convolutional Neural Network, SCNN) 模型应用于人脸识别 (Face Verification) 和行人重识别 (Person Re-Identification) 达到了不错的识别率。文献<citation id="202" type="reference">[<a class="sup">12</a>]</citation>将SNN用于文本匹配, 每个文本对象分别由子网络单独向量化, 计算两个向量的余弦相似度来衡量这两段文本的相似程度。这些方法的共同目标是学习一个好的距离度量, 以便同类数据对之间的距离缩小, 而异类数据对之间的距离尽可能地扩大。不同于一般基于特征向量的分类问题, 时间序列数据往往具有高维度、属性之间先后次序不可变等特点。</p>
                </div>
                <div class="p1">
                    <p id="40">针对欧氏距离 (ED) 和DTW相似性度量方法的不足, 借鉴SNN在模式识别等领域的良好表现, 提出一种基于SNN的时间序列相似性度量学习方法, 从标签信息和序列样本中学习其数据之间相似性关系的知识并形成度量模型, 时间序列的时间顺序则在输入变量的排序中体现, SNN网络隐含层输出每个时间序列新的矢量表示, 计算新矢量之间的距离度量作为网络的输出, 该输出作为原时间序列之间的相似度<i>S</i>=<i>SNN</i> (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) 。本文依据该相似度结合近邻分类器 (<i>K</i>-Nearest Neighbor, KNN) <citation id="204" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>对时间序列进行分类, 验证了该方法的优势。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">1 相关理论基础</h3>
                <div class="p1">
                    <p id="42">有时间序列<i>X</i><sub><i>i</i></sub>= (<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>d</i></sub>) 和<i>X</i><sub><i>j</i></sub>= (<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>d</i></sub>) , 要衡量两序列间相似度ED是常用的度量方法, 计算两数据点在<i>d</i>维空间中的直线距离来描述两序列的相似度。计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">基于ED的相似性度量方法实现简单, 但在计算不同时间相位的时间序列相似度时ED的表现较差<citation id="205" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。而DTW搜索两个给定序列之间的最小匹配能解决序列不等长、时间相位延迟等问题。针对长度为<i>m</i>的序列<i>P</i>=<i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>, …, <i>p</i><sub><i>m</i></sub>和长度为<i>n</i>的序列<i>Q</i>=<i>q</i><sub>1</sub>, <i>q</i><sub>2</sub>, …, <i>q</i><sub><i>n</i></sub>, 计算任意两数据点<i>p</i><sub><i>i</i></sub>和<i>q</i><sub><i>j</i></sub>的距离<i>d</i> (<i>i</i>, <i>j</i>) 构建一个<i>n</i>×<i>m</i>的距离矩阵<b><i>D</i></b><sub><i>n</i>×<i>m</i></sub>, 通常<i>d</i> (<i>i</i>, <i>j</i>) = (<i>p</i><sub><i>i</i></sub>-<i>q</i><sub><i>j</i></sub>) <sup>2</sup>。DTW从距离矩阵中寻找一组相邻的元素集合作为弯曲路径, 记为<b><i>S</i></b>=[<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>L</i></sub>], max (<i>m</i>, <i>n</i>) ≤<i>L</i>≤<i>m</i>+<i>n</i>-1, 其中<i>s</i><sub><i>k</i></sub>=<i>d</i> (<i>i</i>, <i>j</i>) 。使用动态规划从矩阵中找到一条使<i>P</i>、<i>Q</i>间累计距离最小的路径。</p>
                </div>
                <div class="p1">
                    <p id="45" class="code-formula">
                        <mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>Τ</mi><mi>W</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mo>, </mo><mi>Q</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>min</mi></mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>s</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="46">最小路径通过递归的计算矩阵起始点 (1, 1) 到终止点 (<i>m</i>, <i>n</i>) 之间的局部最优解求得:</p>
                </div>
                <div class="p1">
                    <p id="47">起始点<i>D</i> (1, 1) =<i>d</i> (<i>p</i><sub>1</sub>, <i>q</i><sub>1</sub>) 。</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>q</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mrow><mi>min</mi></mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中:<i>D</i> (<i>i</i>, <i>j</i>) 为当前元素距离值与邻近的3个元素累积距离最小值之和;<i>D</i> (<i>n</i>, <i>m</i>) 就是DTW度量下序列<i>P</i>和<i>Q</i>的最小距离, 可知<i>DTW</i> (<i>P</i>, <i>Q</i>) =<i>D</i> (<i>n</i>, <i>m</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="50">区别于ED、DTW依赖于输入空间中有意义且可计算的距离度量, SNN模型将度量学习的思想与神经网络的非线性表示嵌入结合起来, 以监督的方式从数据中学习数据之间的相似度的特定表达, 经过SNN子网络映射到新的度量空间中。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">1.1 SNN<b>网络一般结构</b></h4>
                <div class="p1">
                    <p id="52"><i>SNN</i>的一般结构如图1所示, 图中包含了两个结构相同参数共享的子网络和一个相似性度量模块。<i>X</i><sub><i>i</i></sub>和<i>X</i><sub><i>j</i></sub>为具有类别标签的时间序列, 分别输入两个子网络, 原序列<i>X</i><sub><i>i</i></sub>和<i>X</i><sub><i>j</i></sub>通过子网络非线性映射到新的向量空间输出<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) 和<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>j</i></sub>) , 其中<b><i>w</i></b>为子网络共享的参数矩阵。假设该神经网络有<i>K</i>层, 第<i>k</i>层有<i>p</i><sup> (<i>k</i>) </sup>个神经元 (<i>k</i>=1, 2, …, <i>N</i>) , 输入<i>X</i><sub><i>i</i></sub>= (<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>d</i></sub>) , 第<i>k</i>层的输出<b><i>z</i></b><sup> (<i>k</i>) </sup>=<i>s</i> (<b><i>w</i></b><sup> (<i>k</i>) </sup><b><i>z</i></b><sup> (<i>k</i>-1) </sup>+<b><i>b</i></b><sup> (<i>k</i>) </sup>) , 其中:<b><i>w</i></b><sup> (<i>k</i>) </sup>是一个<i>p</i><sup> (<i>k</i>) </sup>×<i>d</i>的矩阵;<b><i>b</i></b><sup> (<i>k</i>) </sup>是一个长度为<i>p</i><sup> (<i>k</i>) </sup>的偏置向量;<i>s</i>是一个非线性激活函数, 例如tanh或sigmoid函数。则网络顶层的输出为:</p>
                </div>
                <div class="p1">
                    <p id="53"><i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) =<b><i>z</i></b><sup> (<i>K</i>) </sup>=<i>s</i> (<b><i>w</i></b><sup> (<i>K</i>) </sup><b><i>z</i></b><sup> (<i>K</i>-1) </sup>+<b><i>b</i></b><sup> (<i>K</i>) </sup>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="54">输出层作为输入向量映射输出的特征向量, 用于度量模块计算两个子网络输出之间的相似度<i>E</i><sub><i>W</i></sub>。<i>E</i><sub><i>W</i></sub>的定义见式 (12) 。</p>
                </div>
                <div class="p1">
                    <p id="55">在图1中将一对同类或异类时间序列 (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) 分别输入两个权值共享结构相同的子网络, 利用反向传播算法调节网络权值<b><i>w</i></b>, 在训练阶段如果一对时序数据 (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) 属于同一类别, 则使得相似性度量<i>E</i><sub><i>W</i></sub>数值小化, 如果 (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) 属于不同类别, 则<i>E</i><sub><i>W</i></sub>数值大化。通过最小化损失函数值来学习优化模型的所有参数。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904018_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 孪生网络结构" src="Detail/GetImg?filename=images/JSJY201904018_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 孪生网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904018_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Architecture of Siamese neural network</p>

                </div>
                <h4 class="anchor-tag" id="57" name="57">1.2 <b>损失函数</b></h4>
                <div class="p1">
                    <p id="58">为了改善类内相似性度量并扩大类间距离, 损失函数定义如下所示:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mspace width="0.25em" /><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>Y</mi><mo stretchy="false">) </mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><mo>∑</mo><mi>E</mi></mstyle><msub><mrow></mrow><mi>W</mi></msub><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>Y</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false">{</mo></mstyle><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>m</mi><mo>-</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">其中<i>Y</i>为类别因子:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext>类</mtext><mtext>别</mtext><mtext>相</mtext><mtext>同</mtext></mtd></mtr><mtr><mtd><mn>1</mn><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext>类</mtext><mtext>别</mtext><mtext>不</mtext><mtext>同</mtext></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">为了使损失函数最小化使用随机梯度下降算法优化参数{<b><i>w</i></b><sup> (<i>k</i>) </sup>, <b><i>b</i></b><sup> (<i>k</i>) </sup>}。当SNN输入为同类数据对时<i>Y</i>=0, 此时损失函数:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>S</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false"> (</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">参数{<b><i>w</i></b><sup> (<i>k</i>) </sup>, <b><i>b</i></b><sup> (<i>k</i>) </sup>}更新过程:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>-</mo><mi>μ</mi><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>-</mo><mi>μ</mi><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">当SNN输入为不同类数据对时<i>Y</i>=1, 此时损失函数:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">{</mo><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>m</mi><mo>-</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">}</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">式 (5) 中超参数<i>m</i>表示一个间距 (margin) , 定义了<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) 周围的半径。只有当<i>E</i><sub><i>W</i></sub> (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) 的距离在该半径内时, 不同类别的时序数据输入才会对损失函数<i>L</i> (<b><i>w</i></b>, (<i>Y</i>, <i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) ) 作用, 才需要对权值参数<b><i>w</i></b>进行调整。</p>
                </div>
                <div class="p1">
                    <p id="69">式 (9) 中:当<i>E</i><sub><i>W</i></sub>&gt;<i>m</i>时, max (0, <i>m</i>-<i>E</i><sub><i>W</i></sub>) =0, <i>L</i><sub><i>D</i></sub>的梯度为0, 参数{<b><i>w</i></b><sup> (<i>k</i>) </sup>, <b><i>b</i></b><sup> (<i>k</i>) </sup>}不需要调整;当<i>E</i><sub><i>W</i></sub>&lt;<i>m</i>时, </p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>μ</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>-</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub><mo stretchy="false">) </mo><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>μ</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>-</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub><mo stretchy="false">) </mo><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>W</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">网络的整体目标是最小化损失函数<i>L</i> (<b><i>w</i></b>, (<i>Y</i>, <i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) ) , <i>L</i><sub><i>S</i></sub>和<i>L</i><sub><i>D</i></sub>设计成使得对于参数<b><i>w</i></b>最小化<i>L</i>将导致同类时序对的<i>E</i><sub><i>W</i></sub>数值单调递减, 不同类时序对的<i>E</i><sub><i>W</i></sub>值单调递增。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">1.3 <b>度量函数</b></h4>
                <div class="p1">
                    <p id="73">时间序列数据对 (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) 输入子网络, 经过隐含层映射得到两个时序数据的矢量表示<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) 、<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>j</i></sub>) , 定义在<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) 和<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>j</i></sub>) 之间的相似性度量函数<i>E</i><sub><i>W</i></sub> (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) 作为时间序列 (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) 之间的相似度如式 (9) , 等价为SNN的输出<i>E</i><sub><i>W</i></sub>。度量函数<i>E</i><sub><i>W</i></sub>可以是<i>L</i><sub>1</sub>, <i>L</i><sub>2</sub>范数距离或余弦距离等, 本文选择<i>L</i><sub>2</sub>范数作为度量函数。</p>
                </div>
                <div class="p1">
                    <p id="74"><i>E</i><sub><i>W</i></sub> (<i>X</i><sub><i>i</i></sub>, <i>X</i><sub><i>j</i></sub>) =‖<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) -<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>j</i></sub>) ‖<sub>2</sub>      (12) </p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">2 相似性度量实验及分析</h3>
                <div class="p1">
                    <p id="76">图2分别展示了<i>Beef</i>数据集中5类时序数据的其中一个样本。该数据集包含5个类别共60条数据, 划分训练集和测试集各30条, 每个类别有6条时间序列数据。为了更直观地了解<i>SNN</i>度量学习方法的优势, 在<i>Beef</i>测试集上应用基于<i>t</i>分布的随机近邻嵌入 (<i>t</i>-<i>distributed Stochastic Neighbor Embedding</i>, <i>t</i>-<i>SNE</i>) 算法<citation id="206" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>对<i>SNN</i>的中间层嵌入输出<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) 进行了分析。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904018_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Beef数据集中5类时序数据" src="Detail/GetImg?filename=images/JSJY201904018_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Beef数据集中5类时序数据  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904018_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Five class of time series data in Beef dataset</p>

                </div>
                <div class="p1">
                    <p id="78">图3中不同的数字代表相应的序列类别。t-SNE可视化表明SNN能够很好地将相似的序列聚集在一起。为了比较, 还使用相同数据原始序列的t-SNE图。观察图2可以发现类别为第2类、第3类、第4类和第5类的时间序列样本在形态变化上非常相似, 反映在图3 (a) 中相应的类别数据点之间相互混淆, 不同类别序列的区分度不明显。观察图3 (b) , 通过学习有监督的成对约束信息, 原序列经过子网络映射到新的向量空间后, 得到了一个能更有效地表达序列数据间邻域关系的向量表示。这两幅图反映了SNN的潜在优势:SNN在衡量时间序列样本相似度时能更好地描述序列间相似度。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 实验数据t-SNE可视化" src="Detail/GetImg?filename=images/JSJY201904018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 实验数据t-SNE可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 t-SNE visualization of experimental data</p>

                </div>
                <h3 id="80" name="80" class="anchor-tag">3 分类验证实验及比较</h3>
                <h4 class="anchor-tag" id="81" name="81">3.1 <b>分类实验流程</b></h4>
                <div class="p1">
                    <p id="82">本文将<i>SNN</i>所学度量应用于最近邻分类器<citation id="207" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 并对比<i>ED</i>/<i>DTW</i>-1<i>NN</i>分类器以验证学习到的相似性度量的有效性。基于<i>SNN</i>的最近邻分类过程如下:</p>
                </div>
                <div class="p1">
                    <p id="83">设训练集包含了<i>n</i>个样本{<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>n</i></sub>}, 分别属于类别{<i>C</i><sub>1</sub>, <i>C</i><sub>2</sub>, …, <i>C</i><sub><i>m</i></sub>}。</p>
                </div>
                <div class="p1">
                    <p id="84">输入 训练集{<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>n</i></sub>}, 要预测的时间序列<i>X</i><sup><i>k</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="85">输出 <i>X</i><sup><i>k</i></sup>的类别。</p>
                </div>
                <div class="p1">
                    <p id="86">步骤1 所有训练样本<i>X</i>={<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>n</i></sub>}输入训练好的SNN模型, 得到模型的输出<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) 。</p>
                </div>
                <div class="p1">
                    <p id="87">步骤2 将要预测的时间序列<i>X</i><sup><i>k</i></sup>输入SNN模型得到对应的输出<i>Net</i> (<b><i>w</i></b>, <i>X</i><sup><i>k</i></sup>) 。</p>
                </div>
                <div class="p1">
                    <p id="88">步骤3 由式 (12) 计算所有<i>Net</i> (<b><i>w</i></b>, <i>X</i><sub><i>i</i></sub>) 和<i>Net</i> (<b><i>w</i></b>, <i>X</i><sup><i>k</i></sup>) 之间的相似度<i>E</i><sub><i>W</i></sub>=<i>E</i><sub><i>W</i></sub>{<i>E</i><sub><i>W</i><sub>1</sub></sub>, <i>E</i><sub><i>W</i><sub>2</sub></sub>, …, <i>E</i><sub><i>W</i><sub><i>n</i></sub></sub>}, 根据1-NN分类器, 有<i>E</i><sub><i>W</i><sub><i>i</i></sub></sub>=<i>E</i><sub><i>W</i></sub> (<i>X</i><sub><i>i</i></sub>, <i>X</i><sup><i>k</i></sup>) =min ({<i>E</i><sub><i>W</i><sub>1</sub></sub>, <i>E</i><sub><i>W</i><sub>2</sub></sub>, …, <i>E</i><sub><i>W</i><sub><i>n</i></sub></sub>}) , <i>X</i><sub><i>i</i></sub>∈<i>C</i><sub><i>m</i></sub>。输出最近邻分类器的分类决策<i>X</i><sup><i>k</i></sup>∈<i>C</i><sub><i>m</i></sub>。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">3.2 <b>超参数选择</b></h4>
                <div class="p1">
                    <p id="90">本文采用一层隐含层的前馈神经网络构建<i>SNN</i>模型子网络。为了选择合适的网络结构, 从<i>UCR</i>数据集中随机选取了两个时序数据集进行数值实验, 通过分析实验结果探讨<i>SNN</i>模型结构参数对错误率的影响, 实验结果如图4所示。实验中模型权值参数通过在区间[-0.05, 0.05]内的均匀分布采样来初始化, 选择<i>Adam</i>优化算法<citation id="208" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>训练模型, 多次实验后选择使用<i>sigmoid</i>函数作为神经元激活函数, 实验程序基于<i>Python</i>和<i>Tensorflow</i>平台。</p>
                </div>
                <div class="p1">
                    <p id="91">采用错误率作为分类质量的评价标准, 并对比以<i>ED</i>、<i>DTW</i>作为相似性度量的分类错误率。图4展示了随着神经元数量增加时错误率的变化。</p>
                </div>
                <div class="p1">
                    <p id="92">错误率的定义:分类错误的样本数占样本总数的比例。</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><mo>∑</mo><mo stretchy="false">{</mo></mstyle><mn>1</mn><mo stretchy="false">|</mo><msup><mi>Y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo>≠</mo><mi>Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">其中:<i>N</i>为预测样本总数;<i>Y</i>′<sub><i>i</i></sub>为样本<i>X</i><sub><i>i</i></sub>的预测类别;<i>Y</i><sub><i>i</i></sub>为样本<i>X</i><sub><i>i</i></sub>的真实标签。</p>
                </div>
                <div class="p1">
                    <p id="95">在相同参数下进行五次实验取平均值为实验结果。图4的实验结果显示, 随着隐层神经元个数的增加分类错误率随之明显下降, 较多的神经元有利于学习数据的特征表示, 但存在一定的饱和性。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904018_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 隐层神经元个数对错误率的影响" src="Detail/GetImg?filename=images/JSJY201904018_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 隐层神经元个数对错误率的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904018_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Influence of neurons in hidden layer on the error rate</p>

                </div>
                <h4 class="anchor-tag" id="97" name="97">3.3 <b>数值实验</b></h4>
                <div class="p1">
                    <p id="98">将<i>SNN</i>模型结合1-<i>NN</i>分类器与主流的<i>ED</i>/<i>DTW</i>-1<i>NN</i>分类器相比较, 部分实验结果引用自文献<citation id="209" type="reference">[<a class="sup">17</a>]</citation>。在<i>UCR</i>公共数据集上选取22个数据集完成对比实验, 数据在获取时已划分好训练集与测试集, 所有错误率都在测试集上计算。表1展示了所对比的三种相似度度量方法在22个公共数据集上的分类错误率, 加粗的数值表示所有对比方法中最优的结果。其中<i>SNN</i>-1<i>NN</i>的结果是五次实验平均值。</p>
                </div>
                <div class="p1">
                    <p id="99">从表1展示的实验结果来看, 总体上分类错误率<i>ED</i>-1<i>NN</i>&gt;<i>DTW</i>-1<i>NN</i>&gt;<i>SNN</i>-1<i>NN</i>, 三种分类器在22个数据集上的平均分类错误率分别是0.251、0.236、0.191, <i>SNN</i>-1<i>NN</i>具有明显的优势, 其分类错误率在其中12个数据集上显著低于主流的<i>DTW</i>-1<i>NN</i>分类器, 相对于<i>ED</i>-1<i>NN</i>在其中19个数据集上表现明显占优, 在其余数据集上分类错误率相近。若对于不同数据集测试选择更合适的调整网络结构则可进一步降低<i>SNN</i>-1<i>NN</i>的分类错误率。</p>
                </div>
                <div class="area_img" id="100">
                                            <p class="img_tit">
                                                <b>表</b>1 <b>三种分类器的实验结果对比</b>
                                                    <br />
                                                <i>Tab</i>. 1 <i>Comparison of experimental results of three classifiers</i>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904018_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201904018_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904018_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 三种分类器的实验结果对比" src="Detail/GetImg?filename=images/JSJY201904018_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="101" name="101" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="102">本文针对目前时间序列相似性度量方法的优点和不足, 借鉴度量学习和神经网络模型相结合的思想, 提出了一种基于SNN模型的度量学习方法, 可提高对时间序列数据挖掘的性能。将该方法结合最近邻算法在UCR公共数据集上进行分类实验, 结果表明, 与ED/DTW-1NN相比在分类质量总体上有明显的提升。基于DTW-1NN的模型在部分数据上表现仍然优于SNN-1NN分类方法, 但在分类过程的相似度计算复杂度和速度上, SNN优于DTW。在之后的工作中, 可对本文方法作进一步的研究, 针对数据集特点探讨更加合理的网络结构, 改进模型的学习算法扩展到时间序列数据挖掘聚类等任务中的应用。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="163">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTLL200912020&amp;v=MjY4NjRSN3FmWnVac0Z5RGdWTC9JUFRuSFlyRzRIdGpOclk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>崔婧, 赵秀娟, 宋吟秋.中日股价序列相似性的比较分析[J].系统工程理论与实践, 2009, 29 (12) :125-133. (CUI J, ZHAOX J, SONG Y Q.Similarity analysis on China's and Japan's security price series[J].Systems Engineering-Theory and Practice, 2009, 29 (12) :125-133.) 
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust and Accurate Anomaly Detection in ECG Artifacts Using Time Series Motif Discovery">

                                <b>[2]</b>SIVARAKS H, RATANAMAHATANA C A.Robust and accurate anomaly detection in ECG artifacts using time series motif discovery[J].Computational and Mathematical Methods in Medicine, 2015, 2015:453214.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201701001&amp;v=MzAwNTN5RGdWTC9JTGpmU2JiRzRIOWJNcm85RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>陈海燕, 刘晨晖, 孙博.时间序列数据挖掘的相似性度量综述[J].控制与决策, 2017, 32 (1) :1-11. (CHEN H Y, LIU C H, SUN B.Survey on similarity measurement of time series data mining[J].Control and Decision, 2017, 32 (1) :1-11.) 
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                BERNDT D J, CLIFFORD J.Using dynamic time warping to find patterns in time series[C]//AAAIWS 1994:Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining.Menlo Park, CA:AAAI Press, 1994, 10 (16) :359-370.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast subsequence matching in time-series databases">

                                <b>[5]</b>FALOUTSOS C, RANGANATHAN M, MANOLOPOULOS Y.Fast subsequence matching in time-series databases[C]//SIGMOD1994:Proceedings of the 1994 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1994:419-429.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201808001&amp;v=MTc1MTVnVkwvSUxqZlNiYkc0SDluTXA0OUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeUQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>李海林, 梁叶, 王少春.时间序列数据挖掘中的动态时间弯曲研究综述[J].控制与决策, 2018, 33 (8) :1345-1353. (LI HL, LIANG Y, WANG S C.Review on dynamic time warping in time series data mining[J].Control and Decision, 2018, 33 (8) :1345-1353.) 
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201412002&amp;v=MTkzNTMzenFxQnRHRnJDVVI3cWZadVpzRnlEZ1ZML0lLQ0xmWWJHNEg5WE5yWTlGWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>沈媛媛, 严严, 王菡子.有监督的距离度量学习算法研究进展[J].自动化学报, 2014, 40 (12) :2673-2686. (SHEN Y Y, YAN Y, WANG H Z.Recent advances on supervised distance metric learning algorithms[J].Acta Automatica Sinica, 2014, 40 (12) :2673-2686.) 
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Signature Verification using a&amp;quot;Siamese&amp;quot;Time Delay Neural Network">

                                <b>[8]</b>BROMLEY J, GUYON I, LECUN Y, et al.Signature verification using a"siamese"time delay neural network[C]//NIPS 1993:Proceedings of the 6th International Conference on Neural Information Processing Systems.San Francisco, CA:Morgan Kaufmann Publishers, 1994:737-744.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a similarity met-ric discriminatively,with application to face verification">

                                <b>[9]</b>CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with application to face verification[C]//CVPR2005:Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2005:539-546.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint learning of single-image and cross-image representations for person re-identification">

                                <b>[10]</b>WANG F Q, ZUO W M, LIN L, et al.Joint learning of single-image and cross-image representations for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:1288-1296.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep metric learning for person re-identification">

                                <b>[11]</b>DONG Y, ZHEN L, SHENG L, et al.Deep metric learning for person re-identification[C]//Proceedings of the 2014 22nd International Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2014:34-39.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep structured semantic models for web search using clickthrough data">

                                <b>[12]</b>HUANG P S, HE X D, GAO J F, et al.Learning deep structured semantic models for Web search using clickthrough data[C]//Proceedings of the 22nd ACM International Conference on Conference on Information&amp;Knowledge Management.New York:ACM, 2013:2333-2338.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nearest neighbor pattern classification">

                                <b>[13]</b>COVER T, HART P.Nearest neighbor pattern classification[J].IEEE Transactions on Information Theory, 1967, 13 (1) :21-27.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A complexity-invariant distance measure for time series">

                                <b>[14]</b>BATISTA G E, WANG X, KEOGH E J.A complexity-invariant distance measure for time series[EB/OL].[2018-05-10].https://epubs.siam.org/doi/pdf/10.1137/1.9781611972818.60.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing data using t-SNE">

                                <b>[15]</b>MAATEN L, HINTON G.Visualizing data using t-SNE[J].Journal of Machine Learning Research, 2008, 9:2579-2605.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">

                                <b>[16]</b>KINGMA D P, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-05-10].https://arxiv.org/pdf/1412.6980.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The UCR time series classification archive">

                                <b>[17]</b>CHEN Y, KEOGH E, HU B, et al.The UCR time series classification archive[DB/OL].[2018-05-10].http://www.cs.ucr.edu/~eamonn/time_series_data/.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904018&amp;v=MjY5OTFzRnlEZ1ZML0lMejdCZDdHNEg5ak1xNDlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
