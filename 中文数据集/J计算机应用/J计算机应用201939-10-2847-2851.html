<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136455705440000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910010%26RESULT%3d1%26SIGN%3dJaa7ODOPvytN1lTed7uLiQSW7GQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910010&amp;v=Mjc5MzlHRnJDVVI3cWZadVpzRnluaFVMclBMejdCZDdHNEg5ak5yNDlFWklRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="1 本文系统 ">1 本文系统</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="1.1 SLAM&lt;b&gt;系统框架&lt;/b&gt;">1.1 SLAM<b>系统框架</b></a></li>
                                                <li><a href="#44" data-title="1.2 &lt;b&gt;语义分割&lt;/b&gt;">1.2 <b>语义分割</b></a></li>
                                                <li><a href="#49" data-title="1.3 &lt;b&gt;移动一致性检查和外点剔除&lt;/b&gt;">1.3 <b>移动一致性检查和外点剔除</b></a></li>
                                                <li><a href="#68" data-title="1.4 &lt;b&gt;语义点云地图和语义八叉树地图&lt;/b&gt;">1.4 <b>语义点云地图和语义八叉树地图</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="2 实验与分析 ">2 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="2.1 &lt;b&gt;数据集&lt;/b&gt;">2.1 <b>数据集</b></a></li>
                                                <li><a href="#83" data-title="2.2 &lt;b&gt;实验结果&lt;/b&gt;">2.2 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="图1 本文SLAM系统框架">图1 本文SLAM系统框架</a></li>
                                                <li><a href="#48" data-title="图2 PSPNet的结构">图2 PSPNet的结构</a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;绝对轨迹误差结果&lt;/b&gt;"><b>表</b>1 <b>绝对轨迹误差结果</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;平移漂移结果&lt;/b&gt;"><b>表</b>2 <b>平移漂移结果</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;旋转漂移结果&lt;/b&gt;"><b>表</b>3 <b>旋转漂移结果</b></a></li>
                                                <li><a href="#93" data-title="图3 freiburg3_walking_rpy序列中ORB-SLAM2、DS-SLAM和本文系统的绝对轨迹误差">图3 freiburg3_walking_rpy序列中ORB-SLAM2、DS-SLAM和本文系统的......</a></li>
                                                <li><a href="#94" data-title="图4 freiburg3_walking_rpy序列中ORB-SLAM2 DS-SLAM和本文系统的相对位姿误差">图4 freiburg3_walking_rpy序列中ORB-SLAM2 DS-SLAM和本文系统的......</a></li>
                                                <li><a href="#95" data-title="图5 ORB-SLAM2、DS-SLAM和本文系统在 freiburg3_walking_rpy上的绝对轨迹误差">图5 ORB-SLAM2、DS-SLAM和本文系统在 freiburg3_walking_rpy上的......</a></li>
                                                <li><a href="#98" data-title="图6 语义点云图和语义八叉树地图">图6 语义点云图和语义八叉树地图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="123">


                                    <a id="bibliography_1" title="CADENA C,CARLONE L,CARRILLO H,et al.Past,present,and future of simultaneous localization and mapping:toward the robust-perception age[J].IEEE Transactions on Robotics,2016,32(6):1309-1332." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Past,Present,and Future of Simultaneous Localization and Mapping:Toward the Robust-Perception Age">
                                        <b>[1]</b>
                                        CADENA C,CARLONE L,CARRILLO H,et al.Past,present,and future of simultaneous localization and mapping:toward the robust-perception age[J].IEEE Transactions on Robotics,2016,32(6):1309-1332.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_2" title="LI X,AO H,BELAROUSSI R,et al.Fast semi-dense 3D semantic mapping with monocular visual SLAM[C]//Proceedings of the IEEE 20th International Conference on Intelligent Transportation Systems.Piscataway:IEEE,2017:385-390." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast semi-dense 3D semantic mapping with monocular visual SLAM">
                                        <b>[2]</b>
                                        LI X,AO H,BELAROUSSI R,et al.Fast semi-dense 3D semantic mapping with monocular visual SLAM[C]//Proceedings of the IEEE 20th International Conference on Intelligent Transportation Systems.Piscataway:IEEE,2017:385-390.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_3" title="Mc CORMAC J,HANDA A,DAVISON A,et al.Semantic Fusion:dense 3D semantic mapping with convolutional neural networks[C]//Proceedings of the 2017 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2017:4628-4635." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic fusion:Dense3D semantic mapping with convolutional neural networks">
                                        <b>[3]</b>
                                        Mc CORMAC J,HANDA A,DAVISON A,et al.Semantic Fusion:dense 3D semantic mapping with convolutional neural networks[C]//Proceedings of the 2017 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2017:4628-4635.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_4" title="KIM D H,KIM J H.Effective background model-based RGB-Ddense visual odometry in a dynamic environment[J].IEEE Transactions on Robotics,2016,32(6):1565-1573." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Effective background model-based RGB-D dense visual odometry in a synamic environment&amp;quot;">
                                        <b>[4]</b>
                                        KIM D H,KIM J H.Effective background model-based RGB-Ddense visual odometry in a dynamic environment[J].IEEE Transactions on Robotics,2016,32(6):1565-1573.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_5" title="SUN Y,LIU M,MENG M Q.Improving RGB-D SLAM in dynamic environments:a motion removal approach[J].Robotics&amp;amp;Autonomous Systems,2017,89:110-122." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES958B89FF619B482CFB687A34EC293ED6&amp;v=MTkxNzlYTU02WkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieTh4YTg9TmlmT2ZicTlGcVBFcHZrell1b0dmbmd4eldWbG1EbDFUdzdocUdkR2U3dQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        SUN Y,LIU M,MENG M Q.Improving RGB-D SLAM in dynamic environments:a motion removal approach[J].Robotics&amp;amp;Autonomous Systems,2017,89:110-122.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_6" title="YU C,LIU Z,LIU X,et al.DS-SLAM:a semantic visual SLAMtowards dynamic environments[C]//Proceedings of the 2018IEEE/RSJ International Conference on Intelligent Robots and Systems.Piscataway:IEEE,2018:1168-1174." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ds-slam:a semantic visual slam towards dynamic environments">
                                        <b>[6]</b>
                                        YU C,LIU Z,LIU X,et al.DS-SLAM:a semantic visual SLAMtowards dynamic environments[C]//Proceedings of the 2018IEEE/RSJ International Conference on Intelligent Robots and Systems.Piscataway:IEEE,2018:1168-1174.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_7" title="BADRINARAYANAN V,KENDALL A,CIPOLLA R.Seg Net:a deep convolutional encoder-decoder architecture for scene Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">
                                        <b>[7]</b>
                                        BADRINARAYANAN V,KENDALL A,CIPOLLA R.Seg Net:a deep convolutional encoder-decoder architecture for scene Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_8" title="LI S,LEE D.RGB-D SLAM in dynamic environments using static point weighting[J].IEEE Robotics and Automation Letters,2017,2(4):2263-2270." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RGB-D SLAM in dynamic environments usingstatic point weighting">
                                        <b>[8]</b>
                                        LI S,LEE D.RGB-D SLAM in dynamic environments using static point weighting[J].IEEE Robotics and Automation Letters,2017,2(4):2263-2270.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_9" title="BESCOS B,F&lt;image id=&quot;169&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910010_16900.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;CIL J M,CIVERA J,et al.Dyna SLAM:tracking,mapping,and inpainting in dynamic scenes[J].IEEE Robotics and Automation Letters,2018,3(4):4076-4083." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dyna SLAM:tracking,mapping,and inpainting in dynamic scenes">
                                        <b>[9]</b>
                                        BESCOS B,F&lt;image id=&quot;169&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910010_16900.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;CIL J M,CIVERA J,et al.Dyna SLAM:tracking,mapping,and inpainting in dynamic scenes[J].IEEE Robotics and Automation Letters,2018,3(4):4076-4083.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_10" title="MUR-ARTAL R,TARD&lt;image id=&quot;170&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910010_17000.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;S J D.ORB-SLAM 2:an open-source SLAM system for monocular,stereo,and RGB-D cameras[J].IEEETransactions on Robotics,2017,33(5):1255-1262." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras">
                                        <b>[10]</b>
                                        MUR-ARTAL R,TARD&lt;image id=&quot;170&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910010_17000.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;S J D.ORB-SLAM 2:an open-source SLAM system for monocular,stereo,and RGB-D cameras[J].IEEETransactions on Robotics,2017,33(5):1255-1262.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_11" title="ZHAO H,SHI J,QI X,et al.Pyramid scene parsing network[C]//Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6230-6239." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pyramid Scene Parsing Network">
                                        <b>[11]</b>
                                        ZHAO H,SHI J,QI X,et al.Pyramid scene parsing network[C]//Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6230-6239.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_12" title="STURM J,ENGELHARD N,ENDRES F,et al.A benchmark for the evaluation of RGB-D SLAM systems[C]//Proceedings of the2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.Piscataway:IEEE,2012:573-580." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A benchmark for the evaluation of RGB-D SLAM systems">
                                        <b>[12]</b>
                                        STURM J,ENGELHARD N,ENDRES F,et al.A benchmark for the evaluation of RGB-D SLAM systems[C]//Proceedings of the2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.Piscataway:IEEE,2012:573-580.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_13" title="SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis,2017,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[13]</b>
                                        SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis,2017,39(4):640-651.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_14" title="JIA Y,SHELHAMER E,DONAHUE J,et al.Caffe:convolutional architecture for fast feature embedding[EB/OL].[2019-02-10].https://arxiv.org/pdf/1408.5093.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">
                                        <b>[14]</b>
                                        JIA Y,SHELHAMER E,DONAHUE J,et al.Caffe:convolutional architecture for fast feature embedding[EB/OL].[2019-02-10].https://arxiv.org/pdf/1408.5093.pdf.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_15" title="EVERINGHAM M,van GOOL L,WILLIAMS C K I,et al.The PASCAL Visual Object Classes Challenge 2012(VOC2012)Resultst[EB/OL].[2019-01-10].http://host.robots.ox.ac.uk/pascal/VOC/voc2012/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The PASCAL Visual Object Classes Challenge 2012 (VOC2012)Resultst">
                                        <b>[15]</b>
                                        EVERINGHAM M,van GOOL L,WILLIAMS C K I,et al.The PASCAL Visual Object Classes Challenge 2012(VOC2012)Resultst[EB/OL].[2019-01-10].http://host.robots.ox.ac.uk/pascal/VOC/voc2012/.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-19 09:49</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2847-2851 DOI:10.11772/j.issn.1001-9081.2019040711            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于语义分割的室内动态场景同步定位与语义建图</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B8%AD%E5%BF%97%E7%BA%A2&amp;code=05981488&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">席志红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A9%E5%8F%8C%E5%85%A8&amp;code=42897021&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩双全</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B4%AA%E6%97%AD&amp;code=42897022&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王洪旭</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0119964&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈尔滨工程大学信息与通信工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对动态物体在室内同步定位与地图构建(SLAM)系统中影响位姿估计的问题,提出一种动态场景下基于语义分割的SLAM系统。在相机捕获图像后,首先用PSPNet(Pyramid Scene Parsing Network)对图像进行语义分割;之后提取图像特征点,剔除分布在动态物体内的特征点,并用静态的特征点进行相机位姿估计;最后完成语义点云图和语义八叉树地图的构建。在公开数据集上的五个动态序列进行多次对比测试的结果表明,相对于使用SegNet网络的SLAM系统,所提系统的绝对轨迹误差的标准偏差有6.9%～89.8%的下降,平移和旋转漂移的标准偏差在高动态场景中的最佳效果也能分别提升73.61%和72.90%。结果表明,改进的系统能够显著减小动态场景下位姿估计的误差,准确地在动态场景中进行相机位姿估计。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A8%E6%80%81%E5%9C%BA%E6%99%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">动态场景;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%A4%E5%86%85%E5%9C%BA%E6%99%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">室内场景;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">位姿估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E5%90%8C%E6%AD%A5%E5%AE%9A%E4%BD%8D%E4%B8%8E%E5%9C%B0%E5%9B%BE%E6%9E%84%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉同步定位与地图构建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%90%8C%E6%AD%A5%E5%AE%9A%E4%BD%8D%E4%B8%8E%E5%9C%B0%E5%9B%BE%E6%9E%84%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义同步定位与地图构建;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    席志红(1965—),女,黑龙江哈尔滨人,教授,博士,主要研究方向:图像处理、室内定位;;
                                </span>
                                <span>
                                    *韩双全(1993—),男,山东潍坊人,硕士研究生,主要研究方向:视觉SLAM、图像理解;电子邮箱hanshuangquan@hrbeu.edu.cn;
                                </span>
                                <span>
                                    王洪旭(1994—),男,吉林松原人,硕士研究生,主要研究方向:视觉SLAM、图像分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-26</p>

            </div>
                    <h1><b>Simultaneous localization and semantic mapping of indoor dynamic scene based on semantic segmentation</b></h1>
                    <h2>
                    <span>XI Zhihong</span>
                    <span>HAN Shuangquan</span>
                    <span>WANG Hongxu</span>
            </h2>
                    <h2>
                    <span>School of Information and Communication Engineering, Harbin Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the problem that dynamic objects affect pose estimation in indoor Simultaneous Localization And Mapping(SLAM) systems, a semantic segmentation based SLAM system in dynamic scenes was proposed. Firstly, an image was semantically segmented by the Pyramid Scene Parsing Network(PSPNet) after being captured by the camera. Then image feature points were extracted, feature points distributed in the dynamic object were removed, and camera pose was estimated by using static feature points. Finally, the semantic point cloud map and semantic octree map were constructed. Results of multiple comparison tests on five dynamic sequences of public datasets show that compared with the SLAM system using SegNet network, the proposed system has the standard deviation of absolute trajectory error improved by 6.9%-89.8%, and has the standard deviation of translation and rotation drift improved by 73.61% and 72.90% respectively in the best case in high dynamic scenes. The results show that the improved method can significantly reduce the error of pose estimation in dynamic scenes, and can correctly estimate the camera pose in dynamic scenes.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dynamic%20scene&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dynamic scene;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=indoor%20scene&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">indoor scene;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pose%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pose estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Visual%20Simultaneous%20Localization%20And%20Mapping(VSLAM)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Visual Simultaneous Localization And Mapping(VSLAM);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20Simultaneous%20Localization%20And%20Mapping(SLAM)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic Simultaneous Localization And Mapping(SLAM);</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XI Zhihong,born in 1965,Ph.D.,professor.Her research interests include image processing,indoor positioning.;
                                </span>
                                <span>
                                    HAN Shuangquan,born in 1993,M.S.candidate.His research interests include visual SLAM,image understanding.;
                                </span>
                                <span>
                                    WANG Hongxu,born in 1994,M.S.candidate.His research interests include visual SLAM,image analysis.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-26</p>
                            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="34">同步定位与地图构建(Simultaneous Localization And Mapping, SLAM)是移动机器人、无人机、无人驾驶等应用的基础技术。场景中存在运动物体时,动态物体的不稳定特征点被提取后会严重影响相机位姿估计,造成轨迹误差偏大,甚至系统崩溃。基于视觉的SLAM在动态场景中仍然具有很大的挑战性,而且目前视觉SLAM通常基于几何信息建图,缺少对地图信息的抽象理解,不能为移动载体的感知和导航提供环境的语义信息,制约了感知和导航效果。因此语义信息与几何信息结合构建语义地图成为了一个研究热点。</p>
                </div>
                <div class="p1">
                    <p id="35">深度学习与SLAM结合可以从几何和语义两个层次上感知场景,从而对环境内容进行抽象理解,缓解对环境特征的依赖,获得高层次的感知<citation id="153" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,提高移动机器人对周围环境的理解。Li等<citation id="154" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>将SLAM与卷积神经网络(Convolutional Neural Network, CNN)结合,选择关键帧进行语义分割,利用二维语义信息和相邻关键帧之间的对应关系进行三维建图。McCormac等<citation id="155" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>将ElasticFusion和CNN结合,利用稠密的SLAM系统ElasticFusion计算位姿并建出稠密的图,卷积神经网络预测每个像素的物体类别,通过贝叶斯更新来把识别的结果和SLAM生成的关联信息整合到稠密语义地图中。在动态场景中,Kim等<citation id="156" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出通过计算连续的深度图像在同一平面上的投影的差异获得场景中的静态物体。Sun等<citation id="157" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>通过计算连续RGB图像的强度差异、量化深度图像的分割完成像素分类,区分动态静态物体。Yu等<citation id="158" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出的DS-SLAM(Semantic visual SLAM towards Dynamic environments)在动态场景下将SLAM与SegNet<citation id="159" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>网络结合,利用语义信息和运动特征点检测滤除每一帧的动态物体,从而提高位姿估计准确性,同时建立语义八叉树地图。Li等<citation id="160" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了一种关键帧边缘点的静态加权方法用以表示一个点是静态环境一部分的可能性,减少动态对象对位姿估计的影响。Bescos等<citation id="161" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>将多视几何和深度学习结合,实现没有先验动态标记而具有移动性的物体的检测和分割,并且通过对动态物体遮挡的背景帧进行修复,生成静态场景地图。</p>
                </div>
                <div class="p1">
                    <p id="36">本文针对室内动态场景下SLAM位姿估计和建立语义地图问题展开研究,改进了动态场景中的语义SLAM系统,主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="37">1)将ORB-SLAM2<citation id="162" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>与语义分割网络PSPNet(Pyramid Scene Parsing Network)<citation id="163" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>相结合,减小了动态物体内的特征点对相机位姿估计的影响,减小了轨迹误差。</p>
                </div>
                <div class="p1">
                    <p id="38">2)建立带有语义信息的点云图和语义八叉树地图,为导航定位等应用提供语义地图信息。</p>
                </div>
                <div class="p1">
                    <p id="39">3)在相同公开数据集慕尼黑工业大学TUM(Technische Universität München) RGB-D<citation id="164" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>上与DS-SLAM进行对比,评估本文系统的有效性。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag">1 本文系统</h3>
                <h4 class="anchor-tag" id="41" name="41">1.1 SLAM<b>系统框架</b></h4>
                <div class="p1">
                    <p id="42">ORB-SLAM2被认为是目前最完整、最稳定的SLAM系统之一,可以运行单目、双目和RGB-D相机。本文SLAM采用ORB-SLAM2来提供SLAM方案,并行运行五个线程:语义分割、跟踪、局部建图、闭环检测和稠密地图构建。本文SLAM系统框架如图1所示,RGB-D相机捕获的原始RGB图像先在语义分割线程中处理,得到每个像素语义标签;然后由跟踪线程提取ORB特征点,由移动一致性检查检测潜在的外点,若ORB特征点落在语义分割预测的动态物体中,则将这些特征点作为外点剔除,用剔除外点后稳定的静态点估计相机位姿。</p>
                </div>
                <div class="area_img" id="43">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文SLAM系统框架" src="Detail/GetImg?filename=images/JSJY201910010_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文SLAM系统框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Framework of SLAM system in this paper</p>

                </div>
                <h4 class="anchor-tag" id="44" name="44">1.2 <b>语义分割</b></h4>
                <div class="p1">
                    <p id="45">DS-SLAM采用的语义分割网络是基于全卷积神经网络(Fully Convolutional Network, FCN)<citation id="165" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的SegNet,由VGG-16网络修改得到,但FCN存在几个问题:缺乏依据上下文推断的能力,不能通过类别之间的关系弥补标签之间的关联;模型可能会忽略小的东西,而大的东西可能会超过FCN接受范围,从而导致不连续的预测。总之,FCN不能很好地处理场景之间的关系和全局信息。而PSPNet提出了一个金字塔场景解析网络,能够将难解析的场景信息特征嵌入基于FCN预测框架中,将局部信息和全局特征融合到一起,并提出了适度监督损失的优化策略,既能获取全局场景信息,还可以有效地处理场景之间的关系。</p>
                </div>
                <div class="p1">
                    <p id="46">因此,本文SLAM系统采用基于Caffe<citation id="166" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的逐像素语义分割网络PSPNet。在PASCAL VOC2012<citation id="167" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>数据集上训练的PSPNet总共划分20个类别。</p>
                </div>
                <div class="p1">
                    <p id="47">PSPNet的结构如图2所示。输入的图片经过卷积神经网络提取特征图,提取后的特征图经过金字塔池化模块,得到不同尺度下的带有整体信息的特征,在上采样后将金字塔生成的不同层次的特征图连接,最后经过卷积层获得每个像素的分类。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 PSPNet的结构" src="Detail/GetImg?filename=images/JSJY201910010_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 PSPNet的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 PSPNet structure</p>

                </div>
                <h4 class="anchor-tag" id="49" name="49">1.3 <b>移动一致性检查和外点剔除</b></h4>
                <div class="p1">
                    <p id="50">动态点的检查方法采用DS-SLAM的移动一致性检查。如果移动一致性检查得到的动态点落在分割的对象内,并且动态点数量大于一定阈值,则该对象可以被视为动态对象。动态点检测方法如下:</p>
                </div>
                <div class="p1">
                    <p id="51">第1步 计算光流金字塔以获得当前帧中匹配的特征点。</p>
                </div>
                <div class="p1">
                    <p id="52">第2步 如果匹配点对太靠近图像边缘或者匹配点对中心的3×3图像块的像素差超过设定阈值,则匹配点对将被丢弃。</p>
                </div>
                <div class="p1">
                    <p id="53">第3步 通过随机采样一致性方法和匹配点对计算基础矩阵,对于每一对匹配点,根据参考帧的像素坐标和基础矩阵计算极线,确定从匹配点到其对应的极线的距离是否小于等于某个阈值:如果距离大于阈值,则认为它是一个动态点<citation id="168" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="54">其中:光流金字塔的计算采用LK(Lucas-Kanade)光流金字塔算法,假设亮度不变、运动幅度小、空间一致。画面移动过程中,图像上每个像素的偏移量为(<i>x</i>, <i>y</i>),若第<i>t</i>帧<i><b>A</b></i>点的位置是(<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>),则第<i>t</i>+1帧时,假如<i><b>A</b></i>点的位置是(<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>),可以确定<i><b>A</b></i>点的运动为(<i><b>u</b></i>, <i><b>v</b></i>)=(<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>)-(<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>)。假设原图是<i>I</i>(<i>x</i>, <i>y</i>,<i>z</i>,<i>t</i>),移动后的图像是<i>I</i>(<i>x</i>+<i>δx</i>, <i>y</i>+<i>δy</i>,<i>z</i>+<i>δz</i>,<i>t</i>+<i>δt</i>),两者满足图像约束方程:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mtext>d</mtext><mi>x</mi><mo>,</mo><mspace width="0.25em" /><mi>y</mi><mo>+</mo><mtext>d</mtext><mi>y</mi><mo>,</mo><mi>t</mi><mo>+</mo><mtext>d</mtext><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mspace width="0.25em" /><mi>y</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac><mtext>d</mtext><mi>x</mi><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac><mtext>d</mtext><mi>y</mi><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>t</mi></mrow></mfrac><mtext>d</mtext><mi>t</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">在运动足够小的情况下,二阶和高阶泰勒级数展开式可以忽略,因此有</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac><mi>δ</mi><mi>x</mi><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac><mi>δ</mi><mi>y</mi><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>z</mi></mrow></mfrac><mi>δ</mi><mi>z</mi><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>t</mi></mrow></mfrac><mi>δ</mi><mi>t</mi><mo>=</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">由式(2)等式两边同除d<i>t</i>可得:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac><mi>V</mi><msub><mrow></mrow><mi>x</mi></msub><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac><mi>V</mi><msub><mrow></mrow><mi>y</mi></msub><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>z</mi></mrow></mfrac><mi>V</mi><msub><mrow></mrow><mi>z</mi></msub><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">其中:<i>V</i><sub><i>x</i></sub>=<i><b>u</b></i>;<i>V</i><sub><i>y</i></sub>=<i><b>v</b></i>;<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>z</mi></mrow></mfrac></mrow></math></mathml>是图像在(<i>x</i>, <i>y</i>,<i>z</i>,<i>t</i>)这一点的梯度;<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ι</mi></mrow><mrow><mo>∂</mo><mi>t</mi></mrow></mfrac></mrow></math></mathml>是两帧图像间差值。</p>
                </div>
                <div class="p1">
                    <p id="61">假设光流(<i>V</i><sub><i>x</i></sub>,<i>V</i><sub><i>y</i></sub>,<i>V</i><sub><i>z</i></sub>)在一个大小为<i>m</i>×<i>m</i>×<i>m</i>(<i>m</i>&gt;1)的小窗中是一个常数,那么从像素1到<i>n</i>(<i>n</i>=<i>m</i>*<i>m</i>*<i>m</i>)中可以得到简化的一组方程:</p>
                </div>
                <div class="area_img" id="62">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201910010_06200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="63">即<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">A</mi><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>→</mo></mover><mo>=</mo><mo>-</mo><mi mathvariant="bold-italic">b</mi></mrow></math></mathml>,采用最小二乘法即得:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>→</mo></mover><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">A</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">(</mo><mo>-</mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">由于LK算法假设是小位移,需要在多层图像缩放金字塔上求解,每一层的求解结果乘以2后加到下一层即可解决位移大的问题。</p>
                </div>
                <div class="p1">
                    <p id="66">基于几何的方法提取动态物体轮廓时间开销大,而用语义分割的方法去提取轮廓耗时少,因此,在本文SLAM系统中采用了语义分割网络,可以快速地获得对象的完整轮廓。如果通过移动一致性检查产生一定数量的动态点落在分割对象的轮廓中,则确定该对象正在移动。如果确定分割的对象正在移动,则移除位于对象轮廓中的所有特征点。通过这种方式,可以精确地消除外点。此外,错误分割的影响也可以在一定程度上降低。</p>
                </div>
                <div class="p1">
                    <p id="67">在实际场景中,人是动态场景中的主要动态物体,因此将人作为实验的动态物体。在语义分割结果出来之后,如果没有检测到人,则所有ORB特征将直接与最后一帧匹配估计位姿;否则,使用移动一致性检查结果确定人员是否在移动。如果人被确定为静态,则直接估计位姿;否则在匹配之前删除属于人物轮廓的所有ORB特征点。这样,可以显著降低动态对象的影响。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">1.4 <b>语义点云地图和语义八叉树地图</b></h4>
                <div class="p1">
                    <p id="69">系统构建的语义点云地图和语义八叉树地图,能够较好地呈现出室内的场景,语义分割识别出的物体被标注了不同的颜色信息,并且场景中的动态物体(在数据集中即为人)得到了很好的剔除。相对于语义点云地图,语义八叉树地图所占的空间约是语义点云地图的1%,能够节省大量的硬盘空间,为机器人提供导航地图,并且能够提供语义信息。</p>
                </div>
                <div class="p1">
                    <p id="70">在八叉树中,用概率表达一个叶子节点是否被占据。假设<i>t</i>=1,2,…,<i>T</i>时刻,观测的数据为<i>Z</i><sub>1</sub>,<i>Z</i><sub>2</sub>,…,<i>Z</i><sub><i>T</i></sub>,则第<i>n</i>个叶子节点记录的信息为:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false">(</mo><mi>n</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>:</mo><mi>Τ</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mo>〖</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>n</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>n</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mfrac><mo>×</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>n</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>:</mo><mi>Τ</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>n</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>:</mo><mi>Τ</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mfrac><mo>×</mo><mfrac><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>-</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">采用logit变换把概率<i>p</i>变换到全实数空间<b>R</b>上:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>α</mi><mo>=</mo><mtext>l</mtext><mtext>o</mtext><mtext>g</mtext><mtext>i</mtext><mtext>t</mtext><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>log</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mi>p</mi><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>p</mi><mo>=</mo><mtext>l</mtext><mtext>o</mtext><mtext>g</mtext><mtext>i</mtext><mtext>t</mtext></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula"><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false">(</mo><mo>-</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="75">其中:α称为概率对数值(<i>log</i>-<i>odds</i>)。若用叶子节点概率对数形式描述节点占据情况,式(6)可以写成:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>n</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>:</mo><mi>Τ</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>L</mi><mo stretchy="false">(</mo><mi>n</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>:</mo><mi>Τ</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy="false">)</mo><mo>+</mo><mi>L</mi><mo stretchy="false">(</mo><mi>n</mi><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>:</mo><mi>Τ</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">只要将概率对数值的逆变换转换为概率,即可查询节点是否被占据。</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag">2 实验与分析</h3>
                <h4 class="anchor-tag" id="79" name="79">2.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="80">实验所需的数据集<i>TUM RGB</i>-<i>D</i>是<i>TUM</i>(德国慕尼黑工业大学)开源的大型数据集,包含<i>RGB</i>-<i>D</i>数据和地面实况数据,目的是为视觉测距和视觉<i>SLAM</i>系统的评估建立新的基准。</p>
                </div>
                <div class="p1">
                    <p id="81">本文的数据集主要采用数据集中<i>Asus Xtion</i>采集的5个序列,分别为<i>freiburg</i>3_<i>sitting</i>_<i>static</i>(<i>f</i>_<i>s</i>_<i>static</i>)、 <i>freiburg</i>3_<i>walking</i>_<i>static</i>(<i>f</i>_<i>w</i>_<i>static</i>)、 <i>freiburg</i>3_<i>walking</i>_<i>xyz</i>(<i>f</i>_<i>w</i>_<i>xyz</i>)、<i>reiburg</i>3_<i>walking</i>_<i>halfsphere</i>(<i>f</i>_<i>w</i>_<i>halfhere</i>)和<i>freiburg</i>3_<i>walking</i>_<i>rpy</i>(<i>f</i>_<i>w</i>_<i>rpy</i>)。 <i>freiburg</i>3_<i>sitting</i>_<i>static</i>序列中两个人坐在办公桌前,相机保持在适当位置,视为是低动态序列,其他四个序列均为两个人走过办公室,视为高动态序列。 <i>freiburg</i>3_<i>walking</i>_<i>static</i>序列相机保持在适当位置, <i>freiburg</i>3_<i>walking</i>_<i>xyz</i>序列相机沿三个方向(x,  y,z)移动,<i>reiburg</i>3_<i>walking</i>_<i>halfsphere</i>序列中相机在大约一米直径的小半球上移动, <i>freiburg</i>3_<i>walking</i>_<i>rpy</i>序列中相机沿主轴(滚转-俯仰-偏航)在相同位置旋转。</p>
                </div>
                <div class="p1">
                    <p id="82">另外,数据集还提供了用于系统评估的方法——绝对轨迹误差(<i>Absolute Trajectory Error</i>, <i>ATE</i>)和相对位姿误差(<i>Relative Pose Error</i>, <i>RPE</i>)。其中:<i>ATE</i>代表轨迹的全局一致性,而<i>RPE</i>测量平移和旋转漂移。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">2.2 <b>实验结果</b></h4>
                <div class="p1">
                    <p id="84">在本节中,将展示本文<i>SLAM</i>系统实验结果以说明系统在公共数据集<i>TUM RGB</i>-<i>D</i>的动态场景数据集中的性能。所有实验均在配备<i>Intel i</i>7 <i>CPU</i>,<i>GTX</i>1070 <i>GPU</i>和16 <i>GB</i>内存的计算机上进行。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">2.2.1 定量结果</h4>
                <div class="p1">
                    <p id="86">定量比较结果显示在表1～3中。实验对比了本文系统与<i>DS</i>-<i>SLAM</i>在数据集五个序列中的实验结果,评价指标是均方根误差(<i>Root Mean Squared Error</i>, <i>RMSE</i>)、平均误差(<i>Mean</i>)、中值误差(<i>Median</i>)和标准偏差(<i>Standard Deviation</i>, <i>S</i>.<i>D</i>.)。其中:均方根误差(<i>RMSE</i>)描述估计值与真实值之间的偏差,因此值越小,代表的系统估计的轨迹越接近于真实值;平均误差反映所有估计误差的平均水平;中值误差代表所有误差的中等水平;标准偏差(<i>S</i>.<i>D</i>.)反映系统轨迹估计的离散程度。这几种客观评价算法表现出系统估计的轨迹与真实值之间的差距,反映了系统的稳定性和可靠性。</p>
                </div>
                <div class="p1">
                    <p id="87">从表1～3可看出,相对于<i>DS</i>-<i>SLAM</i>,本文<i>SLAM</i>系统可以使大多数高动态序列的性能得到提高,在动态程度较高的序列<i>freiburg</i>3_<i>walking</i>_<i>xyz</i>和<i>freiburg</i>3_<i>walking</i>_<i>rpy</i>中,提升明显。结果表明,本文<i>SLAM</i>系统可以提高<i>SLAM</i>系统在高动态场景中的鲁棒性和稳定性。但是,在低动态序列中,例如<i>fr</i>3_<i>sitting</i>_<i>static</i>序列,性能的提高并不明显,原因是<i>DS</i>-<i>SLAM</i>可以处理低动态场景并获得良好性能,因此可以改进的空间有限。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">2.2.2 定量结果</h4>
                <div class="p1">
                    <p id="89">图3～4显示了在高动态<i>freiburg</i>3_<i>walking</i>_<i>rpy</i>序列中,<i>ORB</i>-<i>SLAM</i>2、<i>DS</i>-<i>SLAM</i>和本文<i>SLAM</i>系统的<i>ATE</i>和<i>RPE</i>图。图5显示了<i>ORB</i>-<i>SLAM</i>2、<i>DS</i>-<i>SLAM</i>和本文系统在<i>freiburg</i>3_<i>walking</i>_<i>rpy</i>上的<i>ATE</i>对比。可看出,本文<i>SLAM</i>系统的绝对轨迹误差和相对位姿误差均有不同程度的减少。</p>
                </div>
                <div class="area_img" id="90">
                                            <p class="img_tit">
                                                <b>表</b>1 <b>绝对轨迹误差结果</b>
                                                    <br />
                                                <i>Tab</i>.1 <i>Results of absolute trajectory error</i>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_09000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201910010_09000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_09000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 绝对轨迹误差结果" src="Detail/GetImg?filename=images/JSJY201910010_09000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="91">
                                            <p class="img_tit">
                                                <b>表</b>2 <b>平移漂移结果</b>
                                                    <br />
                                                Tab.2 Results of translation drift
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201910010_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 平移漂移结果" src="Detail/GetImg?filename=images/JSJY201910010_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="92">
                                            <p class="img_tit">
                                                <b>表</b>3 <b>旋转漂移结果</b>
                                                    <br />
                                                Tab.3 Result of rotation drift
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201910010_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 旋转漂移结果" src="Detail/GetImg?filename=images/JSJY201910010_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 freiburg3_walking_rpy序列中ORB-SLAM2、DS-SLAM和本文系统的绝对轨迹误差" src="Detail/GetImg?filename=images/JSJY201910010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 freiburg3_walking_rpy序列中ORB-SLAM2、DS-SLAM和本文系统的绝对轨迹误差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Absolute trajectory error of DS-SLAM and the proposed system in freiburg 3_walking_rpy sequence</p>

                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 freiburg3_walking_rpy序列中ORB-SLAM2 DS-SLAM和本文系统的相对位姿误差" src="Detail/GetImg?filename=images/JSJY201910010_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 freiburg3_walking_rpy序列中ORB-SLAM2 DS-SLAM和本文系统的相对位姿误差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Relative pose error of DS-SLAM and the proposed system in freiburg 3_walking_rpy sequence</p>

                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 ORB-SLAM2、DS-SLAM和本文系统在 freiburg3_walking_rpy上的绝对轨迹误差" src="Detail/GetImg?filename=images/JSJY201910010_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 ORB-SLAM2、DS-SLAM和本文系统在 freiburg3_walking_rpy上的绝对轨迹误差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Absolute trajectory error of ORB-SLAM2, DS-SLAM and the proposed system in freiburg3_walking_rpy</p>

                </div>
                <h4 class="anchor-tag" id="96" name="96">2.2.3 语义点云图和语义八叉树地图</h4>
                <div class="p1">
                    <p id="97">实验生成的语义点云图和语义八叉树地图如图6所示,点云图和八叉树地图上的显示器和椅子均被上色。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 语义点云图和语义八叉树地图" src="Detail/GetImg?filename=images/JSJY201910010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 语义点云图和语义八叉树地图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Semantic point cloud map and semantic octree map</p>

                </div>
                <h3 id="99" name="99" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="100">动态物体的存在对于轨迹和位姿的估计影响较大,对动态物体剔除以减小轨迹和位姿误差很有必要。为此,本文改进系统使用了分割准确率高的PSPNet作为分割网络,用以分类场景中的物体,对于场景中的动态物体上的动态特征点予以剔除,利用稳定的静态特征点进行动态场景下的运动估计,继而完成语义地图的构建,并通过实验对比验证了本文系统在减小轨迹和位姿误差上的优势。在接下来的工作中,将研究运动模糊对动态特征点的影响并优化。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="123">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Past,Present,and Future of Simultaneous Localization and Mapping:Toward the Robust-Perception Age">

                                <b>[1]</b>CADENA C,CARLONE L,CARRILLO H,et al.Past,present,and future of simultaneous localization and mapping:toward the robust-perception age[J].IEEE Transactions on Robotics,2016,32(6):1309-1332.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast semi-dense 3D semantic mapping with monocular visual SLAM">

                                <b>[2]</b>LI X,AO H,BELAROUSSI R,et al.Fast semi-dense 3D semantic mapping with monocular visual SLAM[C]//Proceedings of the IEEE 20th International Conference on Intelligent Transportation Systems.Piscataway:IEEE,2017:385-390.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic fusion:Dense3D semantic mapping with convolutional neural networks">

                                <b>[3]</b>Mc CORMAC J,HANDA A,DAVISON A,et al.Semantic Fusion:dense 3D semantic mapping with convolutional neural networks[C]//Proceedings of the 2017 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2017:4628-4635.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Effective background model-based RGB-D dense visual odometry in a synamic environment&amp;quot;">

                                <b>[4]</b>KIM D H,KIM J H.Effective background model-based RGB-Ddense visual odometry in a dynamic environment[J].IEEE Transactions on Robotics,2016,32(6):1565-1573.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES958B89FF619B482CFB687A34EC293ED6&amp;v=MjA5MjFHZEdlN3VYTU02WkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieTh4YTg9TmlmT2ZicTlGcVBFcHZrell1b0dmbmd4eldWbG1EbDFUdzdocQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>SUN Y,LIU M,MENG M Q.Improving RGB-D SLAM in dynamic environments:a motion removal approach[J].Robotics&amp;Autonomous Systems,2017,89:110-122.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ds-slam:a semantic visual slam towards dynamic environments">

                                <b>[6]</b>YU C,LIU Z,LIU X,et al.DS-SLAM:a semantic visual SLAMtowards dynamic environments[C]//Proceedings of the 2018IEEE/RSJ International Conference on Intelligent Robots and Systems.Piscataway:IEEE,2018:1168-1174.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">

                                <b>[7]</b>BADRINARAYANAN V,KENDALL A,CIPOLLA R.Seg Net:a deep convolutional encoder-decoder architecture for scene Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RGB-D SLAM in dynamic environments usingstatic point weighting">

                                <b>[8]</b>LI S,LEE D.RGB-D SLAM in dynamic environments using static point weighting[J].IEEE Robotics and Automation Letters,2017,2(4):2263-2270.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dyna SLAM:tracking,mapping,and inpainting in dynamic scenes">

                                <b>[9]</b>BESCOS B,F<image id="169" type="formula" href="images/JSJY201910010_16900.jpg" display="inline" placement="inline"><alt></alt></image>CIL J M,CIVERA J,et al.Dyna SLAM:tracking,mapping,and inpainting in dynamic scenes[J].IEEE Robotics and Automation Letters,2018,3(4):4076-4083.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras">

                                <b>[10]</b>MUR-ARTAL R,TARD<image id="170" type="formula" href="images/JSJY201910010_17000.jpg" display="inline" placement="inline"><alt></alt></image>S J D.ORB-SLAM 2:an open-source SLAM system for monocular,stereo,and RGB-D cameras[J].IEEETransactions on Robotics,2017,33(5):1255-1262.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pyramid Scene Parsing Network">

                                <b>[11]</b>ZHAO H,SHI J,QI X,et al.Pyramid scene parsing network[C]//Proceedings of the 30th IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6230-6239.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A benchmark for the evaluation of RGB-D SLAM systems">

                                <b>[12]</b>STURM J,ENGELHARD N,ENDRES F,et al.A benchmark for the evaluation of RGB-D SLAM systems[C]//Proceedings of the2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.Piscataway:IEEE,2012:573-580.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[13]</b>SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis,2017,39(4):640-651.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">

                                <b>[14]</b>JIA Y,SHELHAMER E,DONAHUE J,et al.Caffe:convolutional architecture for fast feature embedding[EB/OL].[2019-02-10].https://arxiv.org/pdf/1408.5093.pdf.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The PASCAL Visual Object Classes Challenge 2012 (VOC2012)Resultst">

                                <b>[15]</b>EVERINGHAM M,van GOOL L,WILLIAMS C K I,et al.The PASCAL Visual Object Classes Challenge 2012(VOC2012)Resultst[EB/OL].[2019-01-10].http://host.robots.ox.ac.uk/pascal/VOC/voc2012/.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910010" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910010&amp;v=Mjc5MzlHRnJDVVI3cWZadVpzRnluaFVMclBMejdCZDdHNEg5ak5yNDlFWklRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
