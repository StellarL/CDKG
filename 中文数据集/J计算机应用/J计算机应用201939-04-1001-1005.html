<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775550752500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904012%26RESULT%3d1%26SIGN%3dHjJcNWVfV%252fLVo6fWtEYWvM6dkGM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904012&amp;v=MjgxMTh0R0ZyQ1VSN3FmWnVac0Z5RGdWN3ZKTHo3QmQ3RzRIOWpNcTQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#55" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 基于深度学习的目标检测 ">1 基于深度学习的目标检测</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="2 四旋翼无人机避障 ">2 四旋翼无人机避障</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#119" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#122" data-title="3.1 &lt;b&gt;目标检测&lt;/b&gt;">3.1 <b>目标检测</b></a></li>
                                                <li><a href="#125" data-title="3.2 &lt;b&gt;距离估计&lt;/b&gt;">3.2 <b>距离估计</b></a></li>
                                                <li><a href="#127" data-title="3.3 &lt;b&gt;四旋翼无人机避障&lt;/b&gt;">3.3 <b>四旋翼无人机避障</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#131" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="图1 系统结构及单目测距算法流程">图1 系统结构及单目测距算法流程</a></li>
                                                <li><a href="#68" data-title="图2 Faster R-CNN模型框架">图2 Faster R-CNN模型框架</a></li>
                                                <li><a href="#69" data-title="图3 RPN模型">图3 RPN模型</a></li>
                                                <li><a href="#118" data-title="图4 相似三角形估计距离原理示意图">图4 相似三角形估计距离原理示意图</a></li>
                                                <li><a href="#121" data-title="图5 基于&lt;i&gt;Pixhawk&lt;/i&gt;的飞行实验平台">图5 基于<i>Pixhawk</i>的飞行实验平台</a></li>
                                                <li><a href="#124" data-title="图6 基于深度学习的目标检测实验结果">图6 基于深度学习的目标检测实验结果</a></li>
                                                <li><a href="#129" data-title="图7 基于深度学习的单目视觉距离检测结果">图7 基于深度学习的单目视觉距离检测结果</a></li>
                                                <li><a href="#130" data-title="图8 四旋翼无人机避障实验结果">图8 四旋翼无人机避障实验结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="159">


                                    <a id="bibliography_1" title="杨秀霞, 刘小伟, 张毅.基于时间约束的无人机避障研究[J].飞行力学, 2015, 33 (2) :125-129. (YANG X X, LIU X W, ZHANG Y.Research on obstacle avoidance of UAV based on time constraint[J].Flight Mechanics, 2015, 33 (2) :125-129.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FHLX201502007&amp;v=MjQzOTA1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3dklJeVhIZHJHNEg5VE1yWTlGWTRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        杨秀霞, 刘小伟, 张毅.基于时间约束的无人机避障研究[J].飞行力学, 2015, 33 (2) :125-129. (YANG X X, LIU X W, ZHANG Y.Research on obstacle avoidance of UAV based on time constraint[J].Flight Mechanics, 2015, 33 (2) :125-129.) 
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_2" title="高迪.无人机避障雷达目标探测与跟踪算法研究[D].哈尔滨:哈尔滨工业大学, 2017:10-17. (GAO D.Research on target detection and tracking algorithm for UAV obstacle avoidance radar[D].Harbin:Harbin Institute of Technology, 2017:10-17.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017863963.nh&amp;v=MTUxOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd2SVZGMjZHYnUrSGRqS3JKRWJQSVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        高迪.无人机避障雷达目标探测与跟踪算法研究[D].哈尔滨:哈尔滨工业大学, 2017:10-17. (GAO D.Research on target detection and tracking algorithm for UAV obstacle avoidance radar[D].Harbin:Harbin Institute of Technology, 2017:10-17.) 
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_3" title="朱平, 甄子洋, 覃海群, 等.基于立体视觉和光流的无人机避障算法研究[J].电光与控制, 2017, 24 (12) :31-35. (ZHU P, ZHEN Z Y, QIN H Q, el al.Research on UAV obstacle avoidance algorithm based on stereo vision and optical flow[J].Electronics Optics&amp;amp;Control, 2017, 24 (12) :31-35.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DGKQ201712008&amp;v=MTE3ODVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd2SUlTckFmN0c0SDliTnJZOUZiSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        朱平, 甄子洋, 覃海群, 等.基于立体视觉和光流的无人机避障算法研究[J].电光与控制, 2017, 24 (12) :31-35. (ZHU P, ZHEN Z Y, QIN H Q, el al.Research on UAV obstacle avoidance algorithm based on stereo vision and optical flow[J].Electronics Optics&amp;amp;Control, 2017, 24 (12) :31-35.) 
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_4" title="YANG Y, WANG T T, CEHN L, et al.Stereo vision based obstacle avoidance strategy for quadcopter UAV[C]//Proceedings of the2018 Chinese Control and Decision Conference.Piscataway, NJ:IEEE, 2018:490-494." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo vision based obstacle avoidance strategy for quadcopter UAV">
                                        <b>[4]</b>
                                        YANG Y, WANG T T, CEHN L, et al.Stereo vision based obstacle avoidance strategy for quadcopter UAV[C]//Proceedings of the2018 Chinese Control and Decision Conference.Piscataway, NJ:IEEE, 2018:490-494.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_5" title="HU Y Y, WANG Y X.Stereo vision-based fast obstacles avoidance without obstacles discrimination for indoor UAVs[C]//Proceedings of the 2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce.Piscataway, NJ:IEEE, 2011:4332-4337." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo Vision-Based Fast Obstacles Avoidance Without Obstacles Discrimination for Indoor UAVs">
                                        <b>[5]</b>
                                        HU Y Y, WANG Y X.Stereo vision-based fast obstacles avoidance without obstacles discrimination for indoor UAVs[C]//Proceedings of the 2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce.Piscataway, NJ:IEEE, 2011:4332-4337.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_6" title="HU J, NIU Y, WANG Z.Obstacle avoidance methods for rotor UAVs using Real Sense camera[C]//Proceedings of the 2017 Chinese Automation Congress.Piscataway, NJ:IEEE, 2017:7151-7155." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Obstacle avoidance methods for rotor UAVs using Real Sense camera">
                                        <b>[6]</b>
                                        HU J, NIU Y, WANG Z.Obstacle avoidance methods for rotor UAVs using Real Sense camera[C]//Proceedings of the 2017 Chinese Automation Congress.Piscataway, NJ:IEEE, 2017:7151-7155.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_7" title="XU Z, WER R, ZHANG Q, et al.Obstacle avoidance algorithm for UAVs in unknown environment based on distributional perception and decision making[C]//Proceedings of the 2016 IEEE Chinese Guidance, Navigation and Control Conference.Piscataway, NJ:IEEE, 2016:1072-1075." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Obstacle avoidance algorithm for UAVs in unknown environment based on distributional perception and decision making">
                                        <b>[7]</b>
                                        XU Z, WER R, ZHANG Q, et al.Obstacle avoidance algorithm for UAVs in unknown environment based on distributional perception and decision making[C]//Proceedings of the 2016 IEEE Chinese Guidance, Navigation and Control Conference.Piscataway, NJ:IEEE, 2016:1072-1075.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_8" title="GAGEIK N, BENZ P, MONTENEGRO S.Obstacle detection and collision avoidance for a UAV with complementary low-cost sensors[J].IEEE Access, 2015, 3:599-609." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Obstacle detection and collision avoidance for a UAV with complementary lowcost sensors">
                                        <b>[8]</b>
                                        GAGEIK N, BENZ P, MONTENEGRO S.Obstacle detection and collision avoidance for a UAV with complementary low-cost sensors[J].IEEE Access, 2015, 3:599-609.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_9" title="RIISGAARD S, BLAS M R.SLAM for dummies:a tutorial approach to simultaneous localization and mapping[EB/OL].[2018-05-10].http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.208.6289&amp;amp;rep=rep1&amp;amp;type=pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLAM for dummies:a tutorial approach to simultaneous localization and mapping">
                                        <b>[9]</b>
                                        RIISGAARD S, BLAS M R.SLAM for dummies:a tutorial approach to simultaneous localization and mapping[EB/OL].[2018-05-10].http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.208.6289&amp;amp;rep=rep1&amp;amp;type=pdf.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_10" title="陈炜楠, 刘冠峰, 李俊良, 等.室内环境的元胞自动机SLAM算法[J].机器人, 2016, 38 (2) :169-177. (CHEN W N, LIUG F, LI J L, et al.Cellular automaton SLAM algorithm for indoor environment[J].Robot, 2016, 38 (2) :169-177.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201602006&amp;v=MTQ0ODY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWN3ZJTHp6WmZMRzRIOWZNclk5RllvUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        陈炜楠, 刘冠峰, 李俊良, 等.室内环境的元胞自动机SLAM算法[J].机器人, 2016, 38 (2) :169-177. (CHEN W N, LIUG F, LI J L, et al.Cellular automaton SLAM algorithm for indoor environment[J].Robot, 2016, 38 (2) :169-177.) 
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_11" title="徐伟杰, 李平, 韩波.基于2点RANSAC的无人直升机单目视觉SLAM[J].机器人, 2012, 34 (1) :65-71. (XU W J, LI P, HAN B.Unmanned helicopter monocular vision SLAM based on 2points RANSAC[J].Robot, 2012, 34 (1) :65-71.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201201011&amp;v=MTkxOTlJTHp6WmZMRzRIOVBNcm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWN3Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        徐伟杰, 李平, 韩波.基于2点RANSAC的无人直升机单目视觉SLAM[J].机器人, 2012, 34 (1) :65-71. (XU W J, LI P, HAN B.Unmanned helicopter monocular vision SLAM based on 2points RANSAC[J].Robot, 2012, 34 (1) :65-71.) 
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_12" title="BAI G, XIANG X, ZHU H, et al.Research on obstacles avoidance technology for UAV based on improved PTAM algorithm[C]//Proceedings of the 2015 IEEE International Conference on Progress in Informatics and Computing.Piscataway, NJ:IEEE, 2015:543-550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on obstacles avoidance technology for UAV based on improved PTAM algorithm">
                                        <b>[12]</b>
                                        BAI G, XIANG X, ZHU H, et al.Research on obstacles avoidance technology for UAV based on improved PTAM algorithm[C]//Proceedings of the 2015 IEEE International Conference on Progress in Informatics and Computing.Piscataway, NJ:IEEE, 2015:543-550.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_13" title="CHEN Z, LUO X, DAI B.Design of obstacle avoidance system for micro-UAV based on binocular vision[C]//Proceedings of the2017 International Conference on Industrial Informatics-Computing Technology, Intelligent Technology, Industrial Information Integration.Piscataway, NJ:IEEE, 2017:67-70." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design of obstacle avoidance system for micro-UAV based on binocular vision">
                                        <b>[13]</b>
                                        CHEN Z, LUO X, DAI B.Design of obstacle avoidance system for micro-UAV based on binocular vision[C]//Proceedings of the2017 International Conference on Industrial Informatics-Computing Technology, Intelligent Technology, Industrial Information Integration.Piscataway, NJ:IEEE, 2017:67-70.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                    HINTON G E, SALAKHUTDINOV R R.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :504-507.</a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_15" title="ABBAS M A.Improving deep learning performance using random forest HTM cortical learning algorithm[C]//Proceedings of the2018 First International Workshop on Deep and Representation Learning.Piscataway, NJ:IEEE, 2018:13-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving deep learning performance using random forest HTM cortical learning algorithm">
                                        <b>[15]</b>
                                        ABBAS M A.Improving deep learning performance using random forest HTM cortical learning algorithm[C]//Proceedings of the2018 First International Workshop on Deep and Representation Learning.Piscataway, NJ:IEEE, 2018:13-18.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_16" title="WANG Y.Cognitive foundations of knowledge science and deep knowledge learning by cognitive robots[C]//Proceedings of the2017 IEEE 16th International Conference on Cognitive Informatics&amp;amp;Cognitive Computing.Piscataway, NJ:IEEE, 2017:5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cognitive foundations of knowledge science and deep knowledge learning by cognitive robots">
                                        <b>[16]</b>
                                        WANG Y.Cognitive foundations of knowledge science and deep knowledge learning by cognitive robots[C]//Proceedings of the2017 IEEE 16th International Conference on Cognitive Informatics&amp;amp;Cognitive Computing.Piscataway, NJ:IEEE, 2017:5.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_17" title="GOODFELLOW I, BENGIO Y, COURVILLE A, et al.Deep learning[M].Cambridge, MA:MIT Press, 2016:11-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[17]</b>
                                        GOODFELLOW I, BENGIO Y, COURVILLE A, et al.Deep learning[M].Cambridge, MA:MIT Press, 2016:11-12.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_18" title="杜学丹, 蔡莹皓, 鲁涛, 等.一种基于深度学习的机械臂抓取方法[J].机器人, 2017, 39 (6) :820-828, 837. (DU X D, CAI Y H, LU T, et al.A mechanical arm grabbing method based on deep learning[J].Robot, 2017, 39 (6) :820-828, 837.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201706007&amp;v=MTI0MjBMenpaZkxHNEg5Yk1xWTlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3dkk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        杜学丹, 蔡莹皓, 鲁涛, 等.一种基于深度学习的机械臂抓取方法[J].机器人, 2017, 39 (6) :820-828, 837. (DU X D, CAI Y H, LU T, et al.A mechanical arm grabbing method based on deep learning[J].Robot, 2017, 39 (6) :820-828, 837.) 
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_19" title="ZOU J, SONG R.Microarray camera image segmentation with faster-RCNN[C]//Proceedings of the 2018 IEEE International Conference on Applied System Invention.Piscataway, NJ:IEEE, 2018:86-89." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Microarray camera image segmentation with faster-RCNN">
                                        <b>[19]</b>
                                        ZOU J, SONG R.Microarray camera image segmentation with faster-RCNN[C]//Proceedings of the 2018 IEEE International Conference on Applied System Invention.Piscataway, NJ:IEEE, 2018:86-89.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_20" title="LECUN Y, KAVUKCUOGLU K, FARABET C.Convolutional networks and applications in vision[C]//Proceedings of the 2010IEEE International Symposium on Circuits and Systems.Piscataway, NJ:IEEE, 2010:253-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional networks and applications in vision">
                                        <b>[20]</b>
                                        LECUN Y, KAVUKCUOGLU K, FARABET C.Convolutional networks and applications in vision[C]//Proceedings of the 2010IEEE International Symposium on Circuits and Systems.Piscataway, NJ:IEEE, 2010:253-256.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_21" title="GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation]">
                                        <b>[21]</b>
                                        GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_22" title="GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[22]</b>
                                        GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_23" title="REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[23]</b>
                                        REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_24" title="ZHENG W, XIAO J, XIN T.Integrated navigation system with monocular vision and LIDAR for indoor UAVs[C]//Proceedings of the 2017 12th IEEE Conference on Industrial Electronics and Applications.Piscataway, NJ:IEEE, 2017:924-929." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Integrated navigation system with monocular vision and LIDAR for indoor UAVs">
                                        <b>[24]</b>
                                        ZHENG W, XIAO J, XIN T.Integrated navigation system with monocular vision and LIDAR for indoor UAVs[C]//Proceedings of the 2017 12th IEEE Conference on Industrial Electronics and Applications.Piscataway, NJ:IEEE, 2017:924-929.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_25" title="GUANGLEI M, HAIBING P.The application of ultrasonic sensor in the obstacle avoidance of quad-rotor UAV[C]//Proceedings of the 2016 IEEE Chinese Guidance, Navigation and Control Conference.Piscataway, NJ:IEEE, 2016:976-981." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The application of ultrasonic sensor in the obstacle avoidance of quad-rotor UAV">
                                        <b>[25]</b>
                                        GUANGLEI M, HAIBING P.The application of ultrasonic sensor in the obstacle avoidance of quad-rotor UAV[C]//Proceedings of the 2016 IEEE Chinese Guidance, Navigation and Control Conference.Piscataway, NJ:IEEE, 2016:976-981.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_26" title="DIXIT K R, KRISHNA P P, ANTONY R.Design and development of H frame quadcopter for control system with obstacle detection using ultrasound sensors[C]//Proceedings of the 2017 International Conference on Circuits, Controls, and Communications.Piscataway, NJ:IEEE, 2017:100-104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design and development of H frame quadcopter for control system with obstacle detection using ultrasound sensors">
                                        <b>[26]</b>
                                        DIXIT K R, KRISHNA P P, ANTONY R.Design and development of H frame quadcopter for control system with obstacle detection using ultrasound sensors[C]//Proceedings of the 2017 International Conference on Circuits, Controls, and Communications.Piscataway, NJ:IEEE, 2017:100-104.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-05 11:32</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),1001-1005 DOI:10.11772/j.issn.1001-9081.2018091952            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的四旋翼无人机单目视觉避障方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%8D%88%E9%98%B3&amp;code=39095977&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张午阳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%AB%A0%E4%BC%9F&amp;code=25884926&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">章伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%8B%E8%8A%B3&amp;code=28243818&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宋芳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BE%99%E6%9E%97&amp;code=28372422&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">龙林</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%99%BA%E8%83%BD%E6%8E%A7%E5%88%B6%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0202052&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海工程技术大学机器人智能控制实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E4%B8%8E%E6%B1%BD%E8%BD%A6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海工程技术大学机械与汽车工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对无人机避障问题, 提出一种基于深度学习的四旋翼无人机单目视觉避障方法。首先通过目标检测框选出目标在图像中的位置, 并通过计算目标选框上下边距的长度, 以此来估量出障碍物到无人机之间的距离;然后通过协同计算机判断是否执行避障动作;最后使用基于Pixhawk搭建的飞行实验平台进行实验。实验结果表明, 该方法可用于无人机低速飞行条件下避障。该方法所用到的传感器只有一块单目摄像头, 而且相对于传统的主动式传感器避障方法, 所占用无人机的体积大幅减小。该方法鲁棒性较好, 能够准确识别不同姿态的人, 实现对人避障。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%95%E7%9B%AE%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单目视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E4%BA%BA%E6%9C%BA%E9%81%BF%E9%9A%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无人机避障;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张午阳 (1994—) , 男, 安徽淮北人, 硕士研究生, 主要研究方向:无人机避障、计算机视觉;;
                                </span>
                                <span>
                                    章伟 (1977—) , 男, 安徽桐城人, 副教授, 博士, 主要研究方向:非线性控制与观测器、鲁棒控制、无人机控制;;
                                </span>
                                <span>
                                    *宋芳 (1980—) , 女, 黑龙江大庆人, 副教授, 博士, 主要研究方向:机器人、运动控制;电子邮箱songfang@sues.edu.cn;
                                </span>
                                <span>
                                    龙林 (1976—) , 男, 安徽桐城人, 硕士, 主要研究方向:智能化控制、物联网。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (51505273);</span>
                    </p>
            </div>
                    <h1><b>Monocular vision obstacle avoidance method for quadcopter based on deep learning</b></h1>
                    <h2>
                    <span>ZHANG Wuyang</span>
                    <span>ZHANG Wei</span>
                    <span>SONG Fang</span>
                    <span>LONG Lin</span>
            </h2>
                    <h2>
                    <span>Laboratory of Intelligent Control and Robotics, Shanghai University of Engineering Science</span>
                    <span>College of Mechanical and Automobile Engineering, Shanghai University of Engineering Science</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A monocular vision obstacle avoidance method for quadrotor based on deep learning was proposed to help quadrotors to avoid obstacles. Firstly, the position of object in the image was obtained by object detection, and by calculating the height of the object box in the image, the distance between quadcopter and obstacle was estimated. Then, whether performing obstacle avoidance was determined by synergetic computer. Finally, experiments were conducted on a flight test platform based on Pixhawk flight control board. The results show that the proposed method can be applied to quadcoptor obstacle avoidance with low speed. Compared with traditional active sensor methods, the proposed method greatly reduces the occupied volume with only one monocular camera as sensor. This method is robust and can identify people with different postures as obstacles.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=monocular%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">monocular vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=quadcopter%20obstacle%20avoidance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">quadcopter obstacle avoidance;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Wuyang, born in 1994, M. S. candidate. His research interests include quadcopter obstacle avoidance, computer vision.;
                                </span>
                                <span>
                                    ZHANG Wei, born in 1977, Ph. D. associate professor. His research interests include nonlinear control and observer, robust control, unmanned aerial vehicle control.;
                                </span>
                                <span>
                                    SONG Fang, born in 1980, Ph. D. associate professor. Her research interests include robot, motion control.;
                                </span>
                                <span>
                                    LONG Lin, born in 1976, M. S. His research interests include intelligent control, Internet of things.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-20</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (51505273);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="55" name="55" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="56">随着机器人技术以及微机电系统 (Micro-Electro-Mechanical System, MEMS) 的发展, 无人机在各领域中的应用越来越广泛。围绕着无人机相关的研究也日趋增多, 其中, 无人机的自主飞行技术一直是研究热点之一。四旋翼无人机具有体积小、结构简单、机动性强、可垂直起降等固定翼无人机所不具备的优点, 因此具有很强的适应性和广泛的应用场景, 可用在航拍、巡检、物流等低空域任务当中, 很多科研机构和高校均对此展开了不同层次的深入研究。</p>
                </div>
                <div class="p1">
                    <p id="57">对于固定翼无人机, 因其飞行高度较高, 在其任务高度上基本没有障碍物阻拦, 可以人为地为其划定航线, 让其沿着预定航线在预定高度飞行。然而, 四旋翼无人机执行任务时, 受限于通信距离以及续航时间, 其飞行高度一般较低, 飞行路径上可能会分布着各式各样的障碍物, 比如楼房、树木、人类等, 若四旋翼无人机不具有避障功能, 则会沿着预定航线与障碍物相撞, 造成财产损失甚至造成人员伤亡事故。这是一个客观存在而且棘手的问题, 本文致力于提出此类问题的解决方案。</p>
                </div>
                <div class="p1">
                    <p id="58">目前, 有很多科研机构和高校都对小型四旋翼无人机的避障问题展开了深入研究, 并提出了很多行之有效的解决方案<citation id="215" type="reference"><link href="159" rel="bibliography" /><link href="161" rel="bibliography" /><link href="163" rel="bibliography" /><link href="165" rel="bibliography" /><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。若要实现避障, 首先要使无人机感知到障碍物的方位, 文献<citation id="211" type="reference">[<a class="sup">6</a>]</citation>提出了一种基于RealSense相机的避障方式, 通过RealSense相机获得当前帧的景深图像, 通过对景深图像的分层和分割, 获得障碍物的方向和距离信息, 以此达到感知障碍物的目的。在感知到障碍物的空间相对位置后, 无人机需要对任务路径进行重新规划, 针对这一问题, 文献<citation id="212" type="reference">[<a class="sup">7</a>]</citation>提出了一种基于分布感知与决策的避障算法, 该算法将避障问题分为了局部决策和全局决策两部分, 这样做不仅避免了无人机与障碍物碰撞, 同时也能够在任务区域内规划出一条最优路径。为了降低无人机避障系统的成本, Gageik等<citation id="213" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了一种成本较低的避障方案, 该方案在尽量减少冗余模块的基础上, 作了一定的整合优化, 让无人机实现障碍物检测和避障。除此之外, 也可以通过同步定位与地图构建 (Simultaneous Localization and Mapping, SLAM) <citation id="216" type="reference"><link href="175" rel="bibliography" /><link href="177" rel="bibliography" /><link href="179" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>技术、并行跟踪与地图构建 (Parallel Tracking and Mapping, PTAM) <citation id="214" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>技术等实现无人机避障功能。</p>
                </div>
                <div class="p1">
                    <p id="59">传统的避障方式是通过高精度传感器感知障碍物的空间相对位置, 然后据此执行避障动作<citation id="217" type="reference"><link href="171" rel="bibliography" /><link href="173" rel="bibliography" /><link href="175" rel="bibliography" /><link href="181" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">12</a>]</sup></citation>, 此种避障方式精度高, 但是由于所依赖的传感器体积较大, 导致无人机载荷明显增大, 其续航能力将会明显下降;并且其无法对识别出的障碍进行分类, 即其并不知道前方障碍物到底为何物, 这样, 无人机就无法根据不同障碍物类型作出最优的规避动作。近年来, 随着图像处理及计算机视觉技术的发展, 基于机器视觉的无人机避障逐渐成为当前无人机领域的一个研究热点<citation id="218" type="reference"><link href="165" rel="bibliography" /><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><link href="183" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">13</a>]</sup></citation>。图像采集和处理设备体积小, 非常适合无人机在任务中携带。传统的视觉识别是通过特征提取方法来处理图像信息, 然而, 它会受到光照、物体摆放位置等外部因素影响, 鲁棒性较差, 故这种方法适合于特定条件下对特定物体进行识别。</p>
                </div>
                <div class="p1">
                    <p id="60">近年来, 深度学习发展迅速<citation id="220" type="reference"><link href="185" rel="bibliography" /><link href="187" rel="bibliography" /><link href="189" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>, 其优势在于不需要人为地指定需要提取的特征值, 而是使用大数据去训练得到模型, 使其自己学习总结目标所具备的特征<citation id="219" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="61">文献<citation id="221" type="reference">[<a class="sup">18</a>]</citation>使用基于深度学习的目标检测算法对机械臂将要抓取之物进行检测分类, 并通过实验验证了其有效性, 其泛化性和鲁棒性较好。受其启发, 本文提出一种基于深度神经网络的四旋翼无人机避障方法, 通过训练集对模型进行训练, 使其能够辨认出一定数量的障碍物, 并能识别前方障碍的类型, 即具有障碍物分类的功能。通过机载协同计算机将目标障碍物识别并框选出来, 得到目标障碍物在图像中的位置, 进而计算出目标障碍物的图上尺寸, 再利用相似三角形原理估量出目标障碍物到无人机之间的间隔, 再由机载协同计算机判断是否执行避障动作。该方法能应对各类复杂场景中的目标检测任务, 例如目标被遮挡、亮度变化、目标姿态变化等。系统结构及算法流程如图1所示。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904012_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 系统结构及单目测距算法流程" src="Detail/GetImg?filename=images/JSJY201904012_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 系统结构及单目测距算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904012_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 System structure and flow chart of monocular ranging algorithm</p>

                </div>
                <h3 id="63" name="63" class="anchor-tag">1 基于深度学习的目标检测</h3>
                <div class="p1">
                    <p id="64">在目标检测领域, 卷积神经网络 (Convolutional Neural Network, CNN) 具有优异的特性, 其权重共享网络结构能够降低网络模型的复杂性和权重数量<citation id="226" type="reference"><link href="195" rel="bibliography" /><link href="197" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>。CNN的优势是识别准确率高, 然而大量的卷积运算使其运行效率偏低, 实时性较差。为了克服这一缺陷, 区域卷积神经网络 (Regional-CNN, R-CNN) 应运而生<citation id="222" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。R-CNN算法分为四个步骤:首先将一张图像划分为2 000～3 000个候选区域 (Region Proposals) , 然后利用CNN提取每个候选区域的特征, 再训练SVM分类器对这些特征进行分类, 最后通过边界框回归算法重新定位目标边界框。虽然R-CNN克服了传统方法的缺点, 但是其运算效率依然不高, 因为其对于每个候选区域都要重新计算整个网络。快速R-CNN (fast R-CNN) 则可以较好地克服以上缺点<citation id="223" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。相对于R-CNN, fast R-CNN主要优势在于提出了ROI (Region Of Interests) 策略, 将候选区域映射到CNN模型的特征层上, 直接在特征层上提取对应区域的深层特征, 避免了不断输入不同区域图像的部分, 克服了R-CNN提取卷积特征时冗余操作的缺点<citation id="224" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>;但其实时性并未得到很大的提升。因此Ren等<citation id="225" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>在fast R-CNN的基础上提出了Faster R-CNN, 其模型框架如图2所示。考虑到无人机在飞行过程中需要对障碍物进行实时检测并作出相应的规避动作, 本文采用Faster R-CNN作为目标检测模型。</p>
                </div>
                <div class="p1">
                    <p id="65">作为fast R-CNN的改进, Faster R-CNN引入了区域生成网络 (Region Proposal Network, RPN) 的概念, RPN模型如图3所示。在RPN中, 损失函数为分类损失 (Classify Loss) 和回归损失 (Regression Loss) 的联合损失<citation id="227" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mo stretchy="false">{</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mo>+</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>L</mi></mstyle><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mi>λ</mi><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>⋅</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中:<i>i</i>表示第<i>i</i>个锚点; <i>p</i><sub><i>i</i></sub>表示锚点<i>i</i>为目标的概率; <i>p</i><sup>*</sup><sub><i>i</i></sub>=1时表示第<i>i</i>个锚点为正样本;<b><i>t</i></b><sub><i>i</i></sub>表示预测的边界框的4个参数化坐标向量;<b><i>t</i></b><sup>*</sup><sub><i>i</i></sub>表示候选区域边框和真实目标边框之间的偏差;<i>L</i><sub>cls</sub>表示分类损失;<i>L</i><sub>reg</sub>表示回归损失;<i>N</i><sub>cls</sub>和<i>N</i><sub>reg</sub>分别表示分类损失和回归损失的归一化参数;<i>λ</i>表示平衡权重。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904012_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Faster R-CNN模型框架" src="Detail/GetImg?filename=images/JSJY201904012_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Faster R-CNN模型框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904012_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Framework of Faster R-CNN model</p>

                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904012_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 RPN模型" src="Detail/GetImg?filename=images/JSJY201904012_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 RPN模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904012_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 RPN model</p>

                </div>
                <div class="p1">
                    <p id="70">Faster R-CNN的数据分为训练集和测试集两部分, 分别记为:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo stretchy="false">{</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mi>Τ</mi><mi>r</mi></mrow></msubsup><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mi>Τ</mi><mi>r</mi></mrow></msubsup><mo stretchy="false">}</mo><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>r</mi></mrow></msub></mrow></msubsup></mtd></mtr><mtr><mtd><mo stretchy="false">{</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mi>Τ</mi><mi>e</mi></mrow></msubsup><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mi>Τ</mi><mi>e</mi></mrow></msubsup><mo stretchy="false">}</mo><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>e</mi></mrow></msub></mrow></msubsup></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中:<i>Tr</i>表示训练集;<i>Te</i>表示测试集;<b><i>x</i></b><sub><i>n</i></sub>∈<b>R</b><sup><i>n</i>×<i>m</i></sup>为输入; <b><i>y</i></b><sub><i>n</i></sub>为输出。<b><i>y</i></b><sub><i>n</i></sub>包括两部分:一是场景中所有目标的位置, 二是目标区域所对应的物体类别。可写为:</p>
                </div>
                <div class="area_img" id="153">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904012_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="77">其中:<i>S</i>为场景<b><i>x</i></b><sub><i>n</i></sub>中目标的个数;<i>C</i>为场景中的目标种类;<i>P</i><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mi>t</mi><mi>a</mi><mi>r</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>和<i>L</i><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mi>t</mi><mi>a</mi><mi>r</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>分别表示第<i>s</i>个目标对应的坐标和类标。</p>
                </div>
                <div class="p1">
                    <p id="80">对于区域生成网络, 输入与输出之间存在以下关系:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mi>Ν</mi><mi>e</mi><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mi>u</mi><mo>×</mo><mi>v</mi><mo>@</mo><mi>r</mi></mrow></msup></mtd></mtr><mtr><mtd><mi>R</mi><mi>Ρ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">x</mi></msub><mo>=</mo><mo stretchy="false">[</mo><mi>R</mi><mi>Ρ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">x</mi><mn>1</mn></msubsup><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mn>2</mn><mo>@</mo><mover accent="true"><mi>S</mi><mo>˜</mo></mover></mrow></msup><mo>, </mo><mi>R</mi><mi>Ρ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">x</mi><mn>2</mn></msubsup><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mn>4</mn><mo>@</mo><mover accent="true"><mi>S</mi><mo>˜</mo></mover></mrow></msup><mo stretchy="false">]</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>R</mi><mi>Ρ</mi><mi>Ν</mi><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>R</mi><mi>Ρ</mi></mrow></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<b><i>x</i></b>∈<b>R</b><sup><i>n</i>×<i>m</i></sup>为输入场景;<i>X</i><sub>1</sub>∈<b>R</b><sup><i>u</i>×<i>v</i>@<i>r</i></sup>为经过CNN提取特征值之后的输出, 即RPN的输入, 有<i>r</i>个特征图, 每个尺寸为<i>u</i>×<i>v</i>;<i>θ</i><sub>1</sub>和<i>θ</i><sub><i>RP</i></sub>为待学习的参数;<b><i>x</i></b>为输入图像;<i>RP</i><sub><b><i>x</i></b></sub>为输出;<mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>S</mi><mo>˜</mo></mover></math></mathml>为候选目标区域的个数;<i>RP</i><sup>1</sup><sub><b><i>x</i></b></sub>为每个候选区域判断为目标区域的得分;<i>RP</i><sup>2</sup><sub><b><i>x</i></b></sub>为候选目标区域的位置。</p>
                </div>
                <div class="p1">
                    <p id="84">结合RPN的输出, Faster R-CNN网络的输入和输出之间存在如下关系:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mi>Ν</mi><mi>e</mi><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mi>u</mi><mo>×</mo><mi>v</mi><mo>@</mo><mi>r</mi></mrow></msup></mtd></mtr><mtr><mtd><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mi>Ν</mi><mi>e</mi><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>X</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mrow><mi>R</mi><mi>Ο</mi><mi>Ι</mi><mtext> </mtext></mrow><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi>R</mi><mi>Ρ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">x</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">y</mi><mo>=</mo><mo stretchy="false">[</mo><mi>R</mi><mi>Ρ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">x</mi><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>S</mi><mo>˜</mo></mover><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi>L</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">x</mi><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi>S</mi><mo>˜</mo></mover><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo>;</mo><mover accent="true"><mi>S</mi><mo>˜</mo></mover><mo>=</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>S</mi><mo>˜</mo></mover></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中:<i>X</i><sub>2</sub>为经过特有卷积层处理后的特征图, 由<i>X</i><sub>1</sub>得到;<i>θ</i><sub>2</sub>为待学习参数;<i>X</i><sub>3</sub>为ROI池化层输出; <b><i>y</i></b>为预测输出, 包括目标区域的位置和类别两部分。</p>
                </div>
                <div class="p1">
                    <p id="87">Faster R-CNN网络优化目标函数有两个:</p>
                </div>
                <div class="p1">
                    <p id="88">1) RPN优化通路。优化目标函数为:</p>
                </div>
                <div class="area_img" id="154">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904012_15400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="92">其中:<i>L</i> (<b><i>x</i></b><sub><i>n</i></sub>, <i>τ</i><sub><i>n</i></sub>;<i>θ</i><sub>1</sub>, <i>θ</i><sub><i>RP</i></sub>) 为风险评估损失项;余下部分为正则项。风险评估损失项表达式形式为:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>τ</mi><msub><mrow></mrow><mi>n</mi></msub><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>R</mi><mi>Ρ</mi></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>κ</mi></munderover><mi>L</mi></mstyle><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi>Ρ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mspace width="0.25em" /><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>κ</mi></munderover><mi>p</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mi>L</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi>Ρ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mspace width="0.25em" /><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mi>log</mi><mspace width="0.25em" /><mo stretchy="false">[</mo><mover accent="true"><mi>Ρ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo><mo>⋅</mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>Ρ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>=</mo><mi>R</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></munderover><mi>R</mi></mstyle><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">t</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">其中:<i>L</i><sub>cls</sub> (<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ρ</mi><mo>^</mo></mover></math></mathml><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, <i>p</i><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) 为区域得分损失;<i>L</i><sub>reg</sub> (<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>^</mo></mover></math></mathml><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, <b><i>t</i></b><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) 为候选区域位置; <i>p</i><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>      (1) 和<b><i>t</i></b><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml> (<i>p</i>) 分别为回归器<i>L</i><sub>reg</sub> (<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>^</mo></mover></math></mathml><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, <b><i>t</i></b><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) 和分类器<i>L</i><sub>cls</sub> (<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ρ</mi><mo>^</mo></mover></math></mathml><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, <i>p</i><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) 中的权重因子。式 (7) 中的回归器的非线性函数为:</p>
                </div>
                <div class="area_img" id="156">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904012_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="157">2) fast R-CNN优化通路:</p>
                </div>
                <div class="area_img" id="158">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904012_15800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="111">其中:<i>θ</i><sub>1</sub>、<i>θ</i><sub>2</sub>、<i>θ</i><sub>3</sub>均为待学习参数;<i>L</i><sub>clc</sub>为区域分类损失;<i>L</i><sub>loc</sub>为目标区域的位置精修损失;Re (<i>RP</i><sub><b><i>x</i></b><sub><i>n</i></sub></sub>) 为每一类所对应的目标区域进行精修的参数。</p>
                </div>
                <div class="p1">
                    <p id="112">采用随机梯度下降的方法进行端到端的训练, 求解优化目标函数式。</p>
                </div>
                <h3 id="113" name="113" class="anchor-tag">2 四旋翼无人机避障</h3>
                <div class="p1">
                    <p id="114">通常的无人机避障方案是通过距离传感器测得障碍物到无人机之间的距离, 比如激光雷达 (<i>Lidar</i>) <citation id="228" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、超声波<citation id="229" type="reference"><link href="207" rel="bibliography" /><link href="209" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>等, 然后根据测得的距离判断无人机与障碍物之间的位置关系, 从而实现避开障碍物。这类主动传感器需要时刻对无人机周围的环境进行扫描, 并且其体积相对较大, 造成了大量的能量损耗, 缩短了无人机的续航时间。本文利用单目视觉方案进行目标与摄像头之间距离的估计, 属于一种软测量方案, 体积相对于传统传感器测量大幅减小, 成本也大幅缩减。</p>
                </div>
                <div class="p1">
                    <p id="115">利用上文提到的<i>Faster R</i>-<i>CNN</i>, 被测目标能够有效并完整地被框选出来, 并计算选择框上下边距之间的像素尺寸。利用相似三角形的原理, 通过计算得到的目标选择框上下边距之间的像素尺寸, 以此来估计出障碍物距无人机当前位置的距离, 原理如图4所示。由相似三角性原理, 物体到摄像头之间的距离满足以下关系:</p>
                </div>
                <div class="p1">
                    <p id="116"><i>d</i>= (<i>f</i>·<i>h</i>) /<i>l</i>      (10) </p>
                </div>
                <div class="p1">
                    <p id="117">其中:<i>d</i>表示物体到摄像头之间的实际距离; <i>f</i>表示摄像头的焦距, 本文采用的摄像头为定焦摄像头;<i>h</i>代表被检测目标的实际尺寸;<i>l</i>表示物体经过摄像头成像后的尺寸。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904012_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 相似三角形估计距离原理示意图" src="Detail/GetImg?filename=images/JSJY201904012_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 相似三角形估计距离原理示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904012_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Distance estimation using similar triangle principle</p>

                </div>
                <h3 id="119" name="119" class="anchor-tag">3 实验与结果分析</h3>
                <div class="p1">
                    <p id="120">实验平台为一架四旋翼无人机, 基于<i>Pixhawk</i>飞控板搭建, 并通过<i>Pixhawk</i>的<i>Telem</i> 2接口将<i>Raspberry</i> 3<i>B</i>+与其连接, 作为协同电脑。<i>RaspberryPi</i> 3<i>B</i>+操作系统为<i>Raspbian stretch</i>系统, 搭载一分辨率为640×480的单目摄像头, 负责图像采集以及目标检测, 并通过<i>DroneKit</i>-<i>Python</i>控制无人机的动作 (升降、俯仰、横滚以及偏航) 。在对目标检测网络进行训练时, 采用的是<i>Windows</i> 10操作系统, 并使用<i>Nvidia GeForce GTX</i> 1060显卡对训练过程进行加速。<i>Faster R</i>-<i>CNN</i>模型所依赖的深度学习框架为<i>Tensor flow</i>。图5为本次实验所采用的飞行实验平台。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904012_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于Pixhawk的飞行实验平台" src="Detail/GetImg?filename=images/JSJY201904012_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基于<i>Pixhawk</i>的飞行实验平台  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904012_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Flight test platform based on Pixhawk</i></p>

                </div>
                <h4 class="anchor-tag" id="122" name="122">3.1 <b>目标检测</b></h4>
                <div class="p1">
                    <p id="123">因为无人机避障要优先考虑避开人体, 因此本文先训练模型识别人体。将“<i>people</i>”作为标签, 训练模型识别人体的能力。本文从互联网上随机选取了6张带有人物的照片, 以此来检验模型对于人体识别的准确率。为了实现障碍物分类的功能, 分别将“<i>screen</i>”“<i>trash can</i>”“<i>laptop</i>”“<i>chair</i>”等作为标签, 训练模型对于障碍物分类的能力。经实验验证, 本文所训练的模型识别人体的准确率均在99%以上, 并且无论人体是以何种角度出现在图片中 (正面、侧面或背面) , 模型均能将其识别出来, 鲁棒性较好。对于其他类型的障碍物, 比如椅子、垃圾桶等地面小型目标, 也有一定的识别能力。目标检测结果如图6所示。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904012_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 基于深度学习的目标检测实验结果" src="Detail/GetImg?filename=images/JSJY201904012_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 基于深度学习的目标检测实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904012_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Detection results based on deep learning</i></p>

                </div>
                <h4 class="anchor-tag" id="125" name="125">3.2 <b>距离估计</b></h4>
                <div class="p1">
                    <p id="126">实验选取人体的参考身高为1.75 <i>m</i>, 以此为参考, 通过计算图像中目标选择框上下边框之间的长度便可估计出人体距离无人机之间的距离。考虑到无人机的反应时间以及图像传输延迟, 本文将安全距离设置为1.5 <i>m</i>。当检测到无人机与人体之间的距离小于或等于1.5 <i>m</i>时, <i>RaspberryPi</i> 3<i>B</i>+将产生报警信号, 并向<i>Pixhawk</i>发送避障指令。经对比, 估计出的距离和测量得到的实际距离误差在±0.5 <i>m</i>。图像处理结果如图7所示, 图中所示距离信息为摄像头到人体之间的距离估计值。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.3 <b>四旋翼无人机避障</b></h4>
                <div class="p1">
                    <p id="128">是否执行避障动作由协同计算机<i>RaspberryPi</i> 3<i>B</i>+作出判断, 再通过<i>DroneKit</i>-<i>Python</i>向<i>Pixhawk</i>发送<i>Mavlink</i>指令。通过实际飞行, 无人机确实可以在距离人1.5 <i>m</i>之外避让行人, 该实验验证了本文方法的有效性。避障实验结果如图8所示, 其中, 图8 (<i>a</i>) 为从机载协同计算机处理得到的距离估计结果, 图8 (<i>b</i>) 为在两种不同的距离下进行实验的实拍图。</p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904012_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 基于深度学习的单目视觉距离检测结果" src="Detail/GetImg?filename=images/JSJY201904012_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 基于深度学习的单目视觉距离检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904012_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Detection result of monocular visual distance</i><i>estimation based on deep learning</i></p>

                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904012_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 四旋翼无人机避障实验结果" src="Detail/GetImg?filename=images/JSJY201904012_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 四旋翼无人机避障实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904012_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Experiment results of quadcopter obstacle avoidance</i></p>

                </div>
                <h3 id="131" name="131" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="132">本文利用单目摄像头, 基于深度学习实现了对人体的目标检测, 并将人体在图像中的位置通过框选的方式标注出来, 再利用相似三角形原理估算出人体到无人机之间的近似距离。与传统避障方法相比, 此方法具有占用体积小、鲁棒性好、成本低等优点。通过仿真实验验证了该方法能对人体进行高准确率识别, 并且能估算出人体到无人机之间的距离, 误差为±0.5 <i>m</i>。在飞行实验平台上进行了实际飞行避障实验, 实验结果验证了该方法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="133">由于受到<i>RaspberryPi</i> 3<i>B</i>+机能的限制, 在进行飞行实验时, 它对人体的检测和距离估计往往会出现延迟现象。为保证实验人员的安全, 目前无人机只能进行低速飞行以抵消延迟带来的影响;并且本文只训练了模型用于识别人体, 并不能做到行为预测, 并且在实际应用中还有许多种障碍要闪避, 这些都是未来需要研究的内容。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="159">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FHLX201502007&amp;v=MjA3MTBUTXJZOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd2SUl5WEhkckc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>杨秀霞, 刘小伟, 张毅.基于时间约束的无人机避障研究[J].飞行力学, 2015, 33 (2) :125-129. (YANG X X, LIU X W, ZHANG Y.Research on obstacle avoidance of UAV based on time constraint[J].Flight Mechanics, 2015, 33 (2) :125-129.) 
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017863963.nh&amp;v=MTgyNzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd2SVZGMjZHYnUrSGRqS3JKRWJQSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>高迪.无人机避障雷达目标探测与跟踪算法研究[D].哈尔滨:哈尔滨工业大学, 2017:10-17. (GAO D.Research on target detection and tracking algorithm for UAV obstacle avoidance radar[D].Harbin:Harbin Institute of Technology, 2017:10-17.) 
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DGKQ201712008&amp;v=MTU4MTMzenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3dklJU3JBZjdHNEg5Yk5yWTlGYklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>朱平, 甄子洋, 覃海群, 等.基于立体视觉和光流的无人机避障算法研究[J].电光与控制, 2017, 24 (12) :31-35. (ZHU P, ZHEN Z Y, QIN H Q, el al.Research on UAV obstacle avoidance algorithm based on stereo vision and optical flow[J].Electronics Optics&amp;Control, 2017, 24 (12) :31-35.) 
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo vision based obstacle avoidance strategy for quadcopter UAV">

                                <b>[4]</b>YANG Y, WANG T T, CEHN L, et al.Stereo vision based obstacle avoidance strategy for quadcopter UAV[C]//Proceedings of the2018 Chinese Control and Decision Conference.Piscataway, NJ:IEEE, 2018:490-494.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo Vision-Based Fast Obstacles Avoidance Without Obstacles Discrimination for Indoor UAVs">

                                <b>[5]</b>HU Y Y, WANG Y X.Stereo vision-based fast obstacles avoidance without obstacles discrimination for indoor UAVs[C]//Proceedings of the 2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce.Piscataway, NJ:IEEE, 2011:4332-4337.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Obstacle avoidance methods for rotor UAVs using Real Sense camera">

                                <b>[6]</b>HU J, NIU Y, WANG Z.Obstacle avoidance methods for rotor UAVs using Real Sense camera[C]//Proceedings of the 2017 Chinese Automation Congress.Piscataway, NJ:IEEE, 2017:7151-7155.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Obstacle avoidance algorithm for UAVs in unknown environment based on distributional perception and decision making">

                                <b>[7]</b>XU Z, WER R, ZHANG Q, et al.Obstacle avoidance algorithm for UAVs in unknown environment based on distributional perception and decision making[C]//Proceedings of the 2016 IEEE Chinese Guidance, Navigation and Control Conference.Piscataway, NJ:IEEE, 2016:1072-1075.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Obstacle detection and collision avoidance for a UAV with complementary lowcost sensors">

                                <b>[8]</b>GAGEIK N, BENZ P, MONTENEGRO S.Obstacle detection and collision avoidance for a UAV with complementary low-cost sensors[J].IEEE Access, 2015, 3:599-609.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLAM for dummies:a tutorial approach to simultaneous localization and mapping">

                                <b>[9]</b>RIISGAARD S, BLAS M R.SLAM for dummies:a tutorial approach to simultaneous localization and mapping[EB/OL].[2018-05-10].http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.208.6289&amp;rep=rep1&amp;type=pdf.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201602006&amp;v=Mjc0MTVnVjd2SUx6elpmTEc0SDlmTXJZOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeUQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>陈炜楠, 刘冠峰, 李俊良, 等.室内环境的元胞自动机SLAM算法[J].机器人, 2016, 38 (2) :169-177. (CHEN W N, LIUG F, LI J L, et al.Cellular automaton SLAM algorithm for indoor environment[J].Robot, 2016, 38 (2) :169-177.) 
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201201011&amp;v=MDYwODVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd2SUx6elpmTEc0SDlQTXJvOUVaWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>徐伟杰, 李平, 韩波.基于2点RANSAC的无人直升机单目视觉SLAM[J].机器人, 2012, 34 (1) :65-71. (XU W J, LI P, HAN B.Unmanned helicopter monocular vision SLAM based on 2points RANSAC[J].Robot, 2012, 34 (1) :65-71.) 
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on obstacles avoidance technology for UAV based on improved PTAM algorithm">

                                <b>[12]</b>BAI G, XIANG X, ZHU H, et al.Research on obstacles avoidance technology for UAV based on improved PTAM algorithm[C]//Proceedings of the 2015 IEEE International Conference on Progress in Informatics and Computing.Piscataway, NJ:IEEE, 2015:543-550.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design of obstacle avoidance system for micro-UAV based on binocular vision">

                                <b>[13]</b>CHEN Z, LUO X, DAI B.Design of obstacle avoidance system for micro-UAV based on binocular vision[C]//Proceedings of the2017 International Conference on Industrial Informatics-Computing Technology, Intelligent Technology, Industrial Information Integration.Piscataway, NJ:IEEE, 2017:67-70.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                HINTON G E, SALAKHUTDINOV R R.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :504-507.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving deep learning performance using random forest HTM cortical learning algorithm">

                                <b>[15]</b>ABBAS M A.Improving deep learning performance using random forest HTM cortical learning algorithm[C]//Proceedings of the2018 First International Workshop on Deep and Representation Learning.Piscataway, NJ:IEEE, 2018:13-18.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cognitive foundations of knowledge science and deep knowledge learning by cognitive robots">

                                <b>[16]</b>WANG Y.Cognitive foundations of knowledge science and deep knowledge learning by cognitive robots[C]//Proceedings of the2017 IEEE 16th International Conference on Cognitive Informatics&amp;Cognitive Computing.Piscataway, NJ:IEEE, 2017:5.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[17]</b>GOODFELLOW I, BENGIO Y, COURVILLE A, et al.Deep learning[M].Cambridge, MA:MIT Press, 2016:11-12.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201706007&amp;v=MDAzMThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3dklMenpaZkxHNEg5Yk1xWTlGWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>杜学丹, 蔡莹皓, 鲁涛, 等.一种基于深度学习的机械臂抓取方法[J].机器人, 2017, 39 (6) :820-828, 837. (DU X D, CAI Y H, LU T, et al.A mechanical arm grabbing method based on deep learning[J].Robot, 2017, 39 (6) :820-828, 837.) 
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Microarray camera image segmentation with faster-RCNN">

                                <b>[19]</b>ZOU J, SONG R.Microarray camera image segmentation with faster-RCNN[C]//Proceedings of the 2018 IEEE International Conference on Applied System Invention.Piscataway, NJ:IEEE, 2018:86-89.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional networks and applications in vision">

                                <b>[20]</b>LECUN Y, KAVUKCUOGLU K, FARABET C.Convolutional networks and applications in vision[C]//Proceedings of the 2010IEEE International Symposium on Circuits and Systems.Piscataway, NJ:IEEE, 2010:253-256.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation]">

                                <b>[21]</b>GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[22]</b>GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[23]</b>REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Integrated navigation system with monocular vision and LIDAR for indoor UAVs">

                                <b>[24]</b>ZHENG W, XIAO J, XIN T.Integrated navigation system with monocular vision and LIDAR for indoor UAVs[C]//Proceedings of the 2017 12th IEEE Conference on Industrial Electronics and Applications.Piscataway, NJ:IEEE, 2017:924-929.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The application of ultrasonic sensor in the obstacle avoidance of quad-rotor UAV">

                                <b>[25]</b>GUANGLEI M, HAIBING P.The application of ultrasonic sensor in the obstacle avoidance of quad-rotor UAV[C]//Proceedings of the 2016 IEEE Chinese Guidance, Navigation and Control Conference.Piscataway, NJ:IEEE, 2016:976-981.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design and development of H frame quadcopter for control system with obstacle detection using ultrasound sensors">

                                <b>[26]</b>DIXIT K R, KRISHNA P P, ANTONY R.Design and development of H frame quadcopter for control system with obstacle detection using ultrasound sensors[C]//Proceedings of the 2017 International Conference on Circuits, Controls, and Communications.Piscataway, NJ:IEEE, 2017:100-104.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904012&amp;v=MjgxMTh0R0ZyQ1VSN3FmWnVac0Z5RGdWN3ZKTHo3QmQ3RzRIOWpNcTQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
