<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136482459815000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909042%26RESULT%3d1%26SIGN%3dnCRz5ZevVKkOOmohwsxuWgaLOkI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909042&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909042&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909042&amp;v=MzIxNzBqTXBvOUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmJ6SUx6N0JkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="1 相关理论 ">1 相关理论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="1.1 &lt;b&gt;宽残差的高效超分辨率网络算法&lt;/b&gt;">1.1 <b>宽残差的高效超分辨率网络算法</b></a></li>
                                                <li><a href="#55" data-title="1.2 &lt;b&gt;深度可分离卷积&lt;/b&gt;">1.2 <b>深度可分离卷积</b></a></li>
                                                <li><a href="#60" data-title="1.3 &lt;b&gt;归一化层&lt;/b&gt;">1.3 <b>归一化层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#65" data-title="2 深度可分离卷积宽残差超分辨率神经网络 ">2 深度可分离卷积宽残差超分辨率神经网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="2.1 &lt;b&gt;改进思想&lt;/b&gt;">2.1 <b>改进思想</b></a></li>
                                                <li><a href="#68" data-title="2.2 &lt;b&gt;网络结构&lt;/b&gt;">2.2 <b>网络结构</b></a></li>
                                                <li><a href="#90" data-title="2.3 &lt;b&gt;实验参数及训练过程&lt;/b&gt;">2.3 <b>实验参数及训练过程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="3 实验结果 ">3 实验结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#95" data-title="3.1 &lt;b&gt;改进残差块对比实验&lt;/b&gt;">3.1 <b>改进残差块对比实验</b></a></li>
                                                <li><a href="#102" data-title="3.2 &lt;i&gt;GN&lt;/i&gt;&lt;b&gt;对比实验&lt;/b&gt;">3.2 <i>GN</i><b>对比实验</b></a></li>
                                                <li><a href="#110" data-title="3.3 &lt;b&gt;改进算法的对比实验&lt;/b&gt;">3.3 <b>改进算法的对比实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="图1 WDSR网络结构">图1 WDSR网络结构</a></li>
                                                <li><a href="#53" data-title="图2 线性低秩卷积">图2 线性低秩卷积</a></li>
                                                <li><a href="#58" data-title="图3 深度可分离卷积原理">图3 深度可分离卷积原理</a></li>
                                                <li><a href="#63" data-title="图4 BN、LN、WN和GN方法">图4 BN、LN、WN和GN方法</a></li>
                                                <li><a href="#71" data-title="图5 &lt;i&gt;WR&lt;/i&gt;-&lt;i&gt;SR&lt;/i&gt;网络结构">图5 <i>WR</i>-<i>SR</i>网络结构</a></li>
                                                <li><a href="#76" data-title="图6 WR-SR残差块">图6 WR-SR残差块</a></li>
                                                <li><a href="#97" data-title="图7 两种方法的&lt;i&gt;L&lt;/i&gt;1损失">图7 两种方法的<i>L</i>1损失</a></li>
                                                <li><a href="#98" data-title="图8 两种方法的&lt;i&gt;PSNR&lt;/i&gt;">图8 两种方法的<i>PSNR</i></a></li>
                                                <li><a href="#107" data-title="图9 &lt;i&gt;WN&lt;/i&gt;和&lt;i&gt;GN&lt;/i&gt;归一化的&lt;i&gt;L&lt;/i&gt;1损失">图9 <i>WN</i>和<i>GN</i>归一化的<i>L</i>1损失</a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;是否修改残差块和添加&lt;/b&gt;&lt;i&gt;GN&lt;/i&gt;&lt;b&gt;或&lt;/b&gt;&lt;i&gt;WN&lt;/i&gt;&lt;b&gt;时的&lt;/b&gt;&lt;i&gt;PSNR&lt;/i&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;SSIM&lt;/i&gt;"><b>表</b>1 <b>是否修改残差块和添加</b><i>GN</i><b>或</b><i>WN</i><b>时的</b><i>PSNR</i><b>和</b><i>SSIM</i></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;各方法的&lt;/b&gt;&lt;i&gt;PSNR&lt;/i&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;SSIM&lt;/i&gt;"><b>表</b>2 <b>各方法的</b><i>PSNR</i><b>和</b><i>SSIM</i></a></li>
                                                <li><a href="#115" data-title="图10 各个算法的实验结果">图10 各个算法的实验结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="139">


                                    <a id="bibliography_1" title=" RIVIERE P J L,VARGAS P,FU G.et al.Accelerating X-ray fluorescence computed tomography [C]// Proceedings of the 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway,NJ:IEEE,2009:1000-1003." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating X-ray fluorescence computed tomography">
                                        <b>[1]</b>
                                         RIVIERE P J L,VARGAS P,FU G.et al.Accelerating X-ray fluorescence computed tomography [C]// Proceedings of the 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway,NJ:IEEE,2009:1000-1003.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_2" title=" PELEDS,YESHURUNY.Superresolution in MRI:application to human white matter fiber tract visualization by diffusion tensor imaging[J].Magnetic Resonance in Medicine,2001,45(1):29-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Superresolution in MRI: application to human white matter fiber tract visualization by diffusion tensor imaging">
                                        <b>[2]</b>
                                         PELEDS,YESHURUNY.Superresolution in MRI:application to human white matter fiber tract visualization by diffusion tensor imaging[J].Magnetic Resonance in Medicine,2001,45(1):29-35.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_3" title=" SUBBARAO M.High-sensitivity Single-Photon Emission Computed Tomography (SPECT):safer,faster,and more accurate SPECT [C]// Proceedings of the 8th International Conference and Expo on Emerging Technologies for a Smarter World.Piscataway,NJ:IEEE,2011:1-2." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-sensitivity Single-Photon Emission Computed Tomography (SPECT):safer,faster,and more accurate SPECT">
                                        <b>[3]</b>
                                         SUBBARAO M.High-sensitivity Single-Photon Emission Computed Tomography (SPECT):safer,faster,and more accurate SPECT [C]// Proceedings of the 8th International Conference and Expo on Emerging Technologies for a Smarter World.Piscataway,NJ:IEEE,2011:1-2.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_4" title=" PARK S C,PARK M K,KANG M G.Super-resolution image reconstruction:a technical overview[J].IEEE Signal Processing Magazine,2003,20(3):21-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution image reconstruction: a technical overview">
                                        <b>[4]</b>
                                         PARK S C,PARK M K,KANG M G.Super-resolution image reconstruction:a technical overview[J].IEEE Signal Processing Magazine,2003,20(3):21-36.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_5" title=" LEDIG C,THEIS L,HUSZAR F,et al.Photo-realistic single image super-resolution using a generative adversarial network [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:105-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution Using a Genera-tive Adversarial Network">
                                        <b>[5]</b>
                                         LEDIG C,THEIS L,HUSZAR F,et al.Photo-realistic single image super-resolution using a generative adversarial network [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:105-114.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_6" title=" JOHNSON J,ALAHI A,LI F.Perceptual losses for real-time style transfer and super-resolution [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:694-711." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual losses for real-time style transfer and superresolution">
                                        <b>[6]</b>
                                         JOHNSON J,ALAHI A,LI F.Perceptual losses for real-time style transfer and super-resolution [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:694-711.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_7" title=" 苏衡,周杰,张志浩.超分辨率图像重建方法综述[J].自动化学报,2013,39(8):1202-1213.(SU H,ZHOU J,ZHANG Z H.Survey of super-resolution image reconstruction methods [J].Acta Automatica Sinica,2013,39(8):1202-1213.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201308005&amp;v=Mjc3NTJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WYnpJS0NMZlliRzRIOUxNcDQ5RllZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         苏衡,周杰,张志浩.超分辨率图像重建方法综述[J].自动化学报,2013,39(8):1202-1213.(SU H,ZHOU J,ZHANG Z H.Survey of super-resolution image reconstruction methods [J].Acta Automatica Sinica,2013,39(8):1202-1213.)
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_8" title=" KEYS R.Cubic convolution interpolation for digital image processing[J].IEEE Transactions on Acoustics,Speech and Signal Processing,1981,29(6):1153-1160." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cubic convolution interpolation for digital image processing">
                                        <b>[8]</b>
                                         KEYS R.Cubic convolution interpolation for digital image processing[J].IEEE Transactions on Acoustics,Speech and Signal Processing,1981,29(6):1153-1160.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_9" title=" YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation [J].IEEE Transactions on Image Processing,2010,19(11):2861-2873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">
                                        <b>[9]</b>
                                         YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation [J].IEEE Transactions on Image Processing,2010,19(11):2861-2873.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_10" title=" DONG C,LOY C C,HE K.et al.Image super-resolution using deep convolutional networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">
                                        <b>[10]</b>
                                         DONG C,LOY C C,HE K.et al.Image super-resolution using deep convolutional networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_11" title=" LIM B,SON S,KIM H,et al.Enhanced deep residual networks for single image super-resolution [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway,NJ:IEEE,2017:1132-1440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">
                                        <b>[11]</b>
                                         LIM B,SON S,KIM H,et al.Enhanced deep residual networks for single image super-resolution [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway,NJ:IEEE,2017:1132-1440.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_12" title=" YU J,FAN Y,YANG J,et al.Wide activation for efficient and accurate image super-resolution [EB/OL].[2019- 01- 23].https://arxiv.org/pdf/1808.08718v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wide activation for efficient and accurate image super-resolution">
                                        <b>[12]</b>
                                         YU J,FAN Y,YANG J,et al.Wide activation for efficient and accurate image super-resolution [EB/OL].[2019- 01- 23].https://arxiv.org/pdf/1808.08718v1.pdf.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_13" title=" CHOLLET F.Xception:deep learning with depthwise separable convolutions [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:1800-1807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">
                                        <b>[13]</b>
                                         CHOLLET F.Xception:deep learning with depthwise separable convolutions [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:1800-1807.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_14" title=" WU Y,HE K.Group normalization [EB/OL].[2018- 12- 12].https://arxiv.org/pdf/1803.08494.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Group normalization">
                                        <b>[14]</b>
                                         WU Y,HE K.Group normalization [EB/OL].[2018- 12- 12].https://arxiv.org/pdf/1803.08494.pdf.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_15" title=" SIFRE L,MALLAT S.Rotation,scaling and deformation invariant scattering for texture discrimination [C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ :IEEE,2013:1233-1240." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rotation,scaling and deformation invariant scattering for texture discrimination">
                                        <b>[15]</b>
                                         SIFRE L,MALLAT S.Rotation,scaling and deformation invariant scattering for texture discrimination [C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ :IEEE,2013:1233-1240.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_16" title=" IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1502.03167.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">
                                        <b>[16]</b>
                                         IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1502.03167.pdf.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_17" title=" NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:257-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep multi-scale convolutional neural network for dynamic scene deblurring">
                                        <b>[17]</b>
                                         NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:257-265.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_18" title=" BA J L,KIROS J R,HINTON G E.Layer normalization [EB/OL].[2019- 01- 20].https://arxiv.org/pdf/1607.06450.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Layer normalization">
                                        <b>[18]</b>
                                         BA J L,KIROS J R,HINTON G E.Layer normalization [EB/OL].[2019- 01- 20].https://arxiv.org/pdf/1607.06450.pdf.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_19" title=" SALIMANS T,KIMGMA D P.Weight normalization:a simple reparameterization to accelerate training of deep neural networks [C]// Proceedings of the 30th Neural Information Processing Systems.New York:NIPS,2016:901-909." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weight normalization:a simple reparameterization to accelerate training of deep neural networks">
                                        <b>[19]</b>
                                         SALIMANS T,KIMGMA D P.Weight normalization:a simple reparameterization to accelerate training of deep neural networks [C]// Proceedings of the 30th Neural Information Processing Systems.New York:NIPS,2016:901-909.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_20" title=" RAZAVIAN A S,AZIZPOUR H,SULLIVAN J,et al.CNN features off-the-shelf:an astounding baseline for recognition [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway,NJ:IEEE,2014:512-519." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CNN features off-the-shelf: an astounding base-line for recognition">
                                        <b>[20]</b>
                                         RAZAVIAN A S,AZIZPOUR H,SULLIVAN J,et al.CNN features off-the-shelf:an astounding baseline for recognition [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway,NJ:IEEE,2014:512-519.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-09 15:49</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2731-2737 DOI:10.11772/j.issn.1001-9081.2019030413            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度可分离卷积和宽残差网络的医学影像超分辨率重建</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E5%AA%9B&amp;code=10314023&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高媛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%93%E6%99%A8&amp;code=42779660&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晓晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A7%A6%E5%93%81%E4%B9%90&amp;code=25670970&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">秦品乐</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%B8%BD%E8%8A%B3&amp;code=10684243&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王丽芳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%8C%97%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E9%99%A2&amp;code=0036109&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中北大学大数据学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为提高医学影像超分辨率的重建质量,提出了一种基于深度可分离卷积的宽残差超分辨率神经网络算法。首先,利用深度可分离卷积改进网络的残差块,扩宽残差块中卷积层的通道,将更多的特征信息传入了激活函数,使得网络中浅层低级图像特征更容易地传播到高层,提高了医学影像超分辨率的重建质量;然后,采用组归一化的方法训练网络,将卷积层的通道维度划分为组,在每个组内计算归一化的均值和方差,使得网络训练过程更快地收敛,解决了深度可分离卷积扩宽通道数导致网络训练难度增加的问题,同时网络表现出更好的性能。实验结果表明,对比传统的最近邻插值、双三次插值超分辨率算法,以及基于稀疏表达的超分辨率算法,所提算法重建出的医学影像纹理细节更加丰富、视觉效果更加逼真。对比基于卷积神经网络的超分辨率算法,基于宽残差超分辨率神经网络算法和生成对抗网络超分辨率算法,所提算法在峰值信噪比(PSNR)和结构相似性(SSIM)上有显著的提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%BD%E6%AE%8B%E5%B7%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宽残差;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度可分离卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%84%E5%BD%92%E4%B8%80%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">组归一化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE%E5%9D%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差块;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *高媛(1972—),女,山西太原人,副教授,硕士,主要研究方向:图像处理、人工智能;电子邮箱15536917179@163.com;
                                </span>
                                <span>
                                    王晓晨(1995—),男,山西高平人,硕士研究生,主要研究方向:深度学习、计算机视觉;;
                                </span>
                                <span>
                                    秦品乐(1978—),男,山西长治人,副教授,博士,主要研究方向:机器视觉、大数据处理;;
                                </span>
                                <span>
                                    王丽芳(1977—),女,山西长治人,副教授,博士,主要研究方向:机器视觉、大数据处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-12</p>

            </div>
                    <h1><b>Medical image super-resolution reconstruction based on depthwise separable convolution and wide residual network</b></h1>
                    <h2>
                    <span>GAO Yuan</span>
                    <span>WANG Xiaochen</span>
                    <span>QIN Pinle</span>
                    <span>WANG Lifang</span>
            </h2>
                    <h2>
                    <span>School of Data Science, North University of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to improve the quality of medical image super-resolution reconstruction, a wide residual super-resolution neural network algorithm based on depthwise separable convolution was proposed. Firstly, the depthwise separable convolution was used to improve the residual block of the network, widen the channel of the convolution layer in the residual block, and pass more feature information into the activation function, making the shallow low-level image features in the network easier transmitted to the upper level, so that the quality of medical image super-resolution reconstruction was enhanced. Then, the network was trained by group normalization, the channel dimension of the convolutional layer was divided into groups, and the normalized mean and variance were calculated in each group, which made the network training process converge faster, and solved the difficulty of network training because the depthwise separable convolution widens the number of channels. Meanwhile, the network showed better performance. The experimental results show that compared with the traditional nearest neighbor interpolation, bicubic interpolation super-resolution algorithm and the super-resolution algorithm based on sparse expression, the medical image reconstructed by the proposed algorithm has richer texture detail and more realistic visual effects. Compared with the super-resolution algorithm based on convolutional neural network, the super-resolution neural network algorithm based on wide residual and the generative adversarial-network super-resolution algorithm, the proposed algorithm has a significant improvement in PSNR(Peak Signal-to-Noise Ratio) and SSIM(Structural SIMilarity index).</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=super%20resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">super resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=wide%20residual&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">wide residual;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depthwise%20separable%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depthwise separable convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=group%20normalization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">group normalization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=residual%20block&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">residual block;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    GAO Yuan, born in 1972, M. S. , associate professor. Her Research interests include image processing, artificial intelligence. ;
                                </span>
                                <span>
                                    WANG Xiaochen, born in 1995, M. S. candidate. His research interests include deep learning, computer vision. ;
                                </span>
                                <span>
                                    QIN Pinle, born in 1978, Ph. D. , associate professor. His research interests include machine vision, big data processing. ;
                                </span>
                                <span>
                                    WANG Lifang, born in 1977, Ph. D. , associate professor. Her research interest include machine vision, big data processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-12</p>
                            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="44">随着医学影像在医疗实践中的不断发展和广泛使用,计算机断层成像(Computed Tomography, CT)<citation id="179" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、磁共振成像(Magnetic Resonance Imaging, MRI)<citation id="180" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、正电子发射计算机断层扫描(Positron Emission Tomography, PET)<citation id="181" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等技术成为临床诊断不可或缺的工具,而清晰的医学影像可以提供丰富的病灶信息,辅助医生进行更加精确的诊断。但由于成像技术和成像原理的不同,获得的医学影像的分辨率也就不同,低分辨率的医学影像中,噪声和伪影会损害诊断信息,在临床上很难进行全面综合的诊断,因此,对医学影像进行超分辨率重建,可以在不增加高分辨率成像技术成本的基础上,降低对成像环境的要求,通过复原出的清晰医学影像,实现对病变部位的精准探测,有助于医生对患者病情作出更好的诊断<citation id="182" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="45">目前,图像超分辨率(Super Resolution, SR)技术的研究方向分为两类:1)基于整体视觉效果的超分辨率重建。经典算法如基于生成对抗的超分辨率网络(photo-realistic single image Super-Resolution using a Generative Adversarial Network, SRGAN)<citation id="183" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,以及具有感知损耗的实时传输超分辨率网络(perceptual losses for Real-time style Transfer and Super-Resolution, RTSR)<citation id="184" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等。该类方法追求符合人类认知视觉的整体效果,但对细节部分的重建要求不高。应用场景如低分辨率电视视频的恢复、相机模糊图像的恢复等。2)基于细节的超分辨率重建。传统算法如最近邻插值(bilinear)<citation id="185" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和双三次插值(bicubic)<citation id="186" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,以及基于稀疏表达的图像超分辨率(image Super-Resolution Via Sparse Representation, SRVSR)<citation id="187" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>;基于深度学习的算法如使用深度卷积网络的图像超分辨率(image Super-Resolution using Deep Convolutional Networks, SRCNN)<citation id="188" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,单图像超分辨率的增强型深度残差网络(Enhanced Deep residual networks for single image Super-Resolution, EDSR)<citation id="189" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>,以及宽残差的高效超分辨率网络(Wide activation for efficient and accurate image Super-Resolution, WDSR)<citation id="190" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等。该类方法对细节要求苛刻,力求恢复出图像真实可靠的细节及纹理信息,应用场景如医学影像上的超分辨率重建、低分辨率摄像头人脸或者外形的恢复等。</p>
                </div>
                <div class="p1">
                    <p id="46">传统的超分辨率算法,如基于稀疏表达的图像超分辨率,通过强化高分辨率图像和低分辨率图像的字典之间稀疏表示的相似性,学习低分辨率图像和高分辨率图像之间的稀疏系数,使用学习到的稀疏系数和高分辨率字典重建出了较为清晰的高分辨率图片。而基于深度学习的超分辨率神经网络算法,通过加深网络的深度,学习图像的高级特征信息,重建出的图像更加清晰,在PSNR和SSIM上均取得比传统方法更好的效果。医学影像的超分辨率重建要求细节明显、纹理清晰,同时减弱噪声的影响。因此,在对医学影像进行超分辨率重建时,定位在对超分辨率神经网络的优化上:即如何在图像尺寸放大的同时获得更好的细节收益,且不放大噪声,减弱噪声对重建的影响。</p>
                </div>
                <div class="p1">
                    <p id="47">Yu等<citation id="191" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出的WDSR网络认为,在单图像超分辨率(Single Image Super-Resolution, SISR)网络的问题中,具有更广泛特征信息激活的网络具有更好的性能,因此,基于WDSR算法,本文提出了基于深度可分离卷积的宽残差超分辨率神经网络算法(Wide Residual Super-Resolution neural network based on depthwise separable convolution, WR-SR)。该算法利用深度可分离卷积(Depthwise separable convolution)<citation id="192" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>进一步扩宽了卷积层的通道数,使得浅层更多的特征信息传入到深层,增大了医学影像在上采样4倍时的细节收益,同时利用组归一化(Group Normalization, GN)<citation id="193" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>方法在卷积层的通道维度内计算均值和方差,在提升网络训练速度的同时提高了网络的性能。实验结果显示,使用深度可分离卷积扩宽卷积层通道这一方法效果显著,与SRGAN、WDSR等超分辨率算法相比,获得了更高的峰值信噪比(Peak Signal-to-Noise Ratio, PSNR)和结构相似性(Structural SIMilarity index, SSIM)。</p>
                </div>
                <h3 id="48" name="48" class="anchor-tag">1 相关理论</h3>
                <h4 class="anchor-tag" id="49" name="49">1.1 <b>宽残差的高效超分辨率网络算法</b></h4>
                <div class="p1">
                    <p id="50">宽残差的高效超分辨率(WDSR)网络算法在NTIRE 2018数据集DIV2K(一个大型的自然图像数据集,具有大量的RGB图像)的超分辨率挑战赛上获得了3个比赛方向上的冠军。该算法的网络结构如图1所示,包括全局残差学习、递归学习和上采样处理3个模块。网络使用低分辨率图片作为输入,在低分辨率阶段使用3×3卷积核(3×3 Conv)提取图像所有特征;之后在递归残差块(Residual Body)内学习图像的高级特征,再使用3×3卷积核(3×3 Conv)提取学习到的高级特征;最后在上采样层中使用像素重组(Pixel Shuffle)的方式对图像进行放大。同时使用全局残差学习(Global Residual Learning)将原始低分辨率图像进行上采样处理,与学习的高级特征叠加后输出超分辨率图片。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 WDSR网络结构" src="Detail/GetImg?filename=images/JSJY201909042_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 WDSR网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Network of WDSR</p>

                </div>
                <div class="p1">
                    <p id="52">网络深度的增加给表示能力带来了好处,但同时未充分使用来自浅层的特征信息。WDSR中证明非线性激活函数(Rectified Linear Unit, ReLU)阻碍了从浅层到深层的信息流,不同于在其他网络中添加各种跳过连接(Skip Connection)的方式,只需在激活函数ReLU之前扩展特征图(Feature map)的大小,即可显著改善图像的重建质量。为此,该算法提出了线性低秩卷积(如图2所示)构建超分辨率网络的残差块,它将大卷积核分解为两个低秩卷积核,其中一个1×1的卷积核用于减少特征图的通道数,另一个3×3的卷积核用于提取特征。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 线性低秩卷积" src="Detail/GetImg?filename=images/JSJY201909042_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 线性低秩卷积  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Linear low-rank convolution</p>

                </div>
                <div class="p1">
                    <p id="54">WDSR使用残差块扩展特征图大小的同时,又使用了1×1卷积核减少了特征图的通道数,解决了网络过宽导致难以训练的难题。但是,该方法限制了网络宽度的增加,使得之后的3×3卷积层提取的特征信息不足,导致网络学习的浅层特征较少,从而影响了网络的重建效果。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">1.2 <b>深度可分离卷积</b></h4>
                <div class="p1">
                    <p id="56">线性低秩卷积是实现空间相关性和通道相关性的联合映射,即同时考虑输入的空间特征和通道特征。Sifre等<citation id="194" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>认为,卷积层通道间的相关性和空间相关性是可以解耦合的,将它们分开映射,能达到更好的效果。提出深度可分离卷积模块将空间和通道的计算分开,在输入的每个通道上单独地执行通道卷积,之后再进行不添加非线性激活函数的空间卷积。</p>
                </div>
                <div class="p1">
                    <p id="57">如图3所示,Input表示卷积核的输入通道数,Output表示卷积核的输出通道数,该模块将线性低秩卷积的3×3×Input×Output卷积核分解为一个3×3×1×Input的卷积核和一个1×1×Input×Output的卷积核。线性低秩卷积核所需参数为3×3×Input×Output,而深度可分离卷积核所需的参数为3×3×1×Input+1×1×Input×Output。当网络的宽度增大时,即输入与输出通道数增大时,深度可分离卷积操作所需的参数将远小于线性低秩卷积所需的参数。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 深度可分离卷积原理" src="Detail/GetImg?filename=images/JSJY201909042_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 深度可分离卷积原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Principle of depthwise separable convolution</p>

                </div>
                <div class="p1">
                    <p id="59">深度可分离卷积首先使用3×3的卷积核将特征图的各个通道映射到一个新的空间,在这一过程中学习通道的相关性,再通过1×1的卷积核进行卷积,以同时学习空间上的相关性和通道间的相关性。与同时学习通道和空间相关性的线性低秩卷积操作相比,深度可分离卷积实现了通道和空间的分离,不仅比线性低秩卷积减少了所需要的参数,使得网络的训练速度变快,而且进一步提升了网络的宽度,使得更多的特征信息能够在网络中进行传播,提高了网络的重建质量。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.3 <b>归一化层</b></h4>
                <div class="p1">
                    <p id="61">神经网络中的每一层都面临输入的分布随训练而变化的情况,所以每个中间层需要不断适应其输入的变化,导致网络变得难以训练。归一化层通过标准化输入的均值和方差,使得每一层都能够学习到更稳定的输入分布,确保网络能够使用更大的学习率,从而提高了网络的训练速度,提高了网络表达能力。</p>
                </div>
                <div class="p1">
                    <p id="62">在深度学习中,常见的归一化层有以下四种。如图4所示,其中每个图表示特征图张量,<i><b>H</b></i>和<i><b>W</b></i>表示特征图的高和宽,<i><b>C</b></i>表示通道维度,<i><b>N</b></i>表示批量维度,深色区域的内容表示使用的是相同的均值和方差,通过聚合该区域的像素值来计算该归一化。1)批量归一化(Batch Normalization, BN)<citation id="195" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。BN沿样本的批量维度<i><b>N</b></i>执行全局归一化,当输入分布发生变化时,之前计算的数据分布也可能改变,这导致网络每层的权重训练不一致。Nah等<citation id="196" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>在图像去模糊工作中认为,由于批量标准化层对特征进行了规范化,导致网络变得不灵活,并且消耗了计算资源,去除BN层后使网络的重建性能获得了提升。2)层归一化(Layer Normalization, LN)<citation id="197" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。LN沿通道维度<i><b>C</b></i>进行归一化,仅针对每个样本操作,解决了批量归一化中网络过度依赖批量大小的问题,但在大多数卷积神经网络中,LN训练的效果不如BN。3)权重归一化(Weight Normalization, WN)<citation id="198" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。WN对卷积核的权重进行归一化,而不是对特征进行操作。WN层同样不会受到批量维度的影响,但WN仅针对卷积核的参数归一化,扩宽卷积核的通道后将导致网络的训练速度降低。4)组归一化,简称GN层,将通道维度<i><b>C</b></i>划分为不同的组,在组内进行归一化。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 BN、LN、WN和GN方法" src="Detail/GetImg?filename=images/JSJY201909042_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 BN、LN、WN和GN方法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Methods of BN, LN, WN and GN</p>

                </div>
                <div class="p1">
                    <p id="64">由于图像的形状、照明、纹理等特征的系数相互依赖,它们的依赖关系在网络中表现为前后不同的几组通道。因此,将网络通道进行分组归一化来处理,可使超分辨率网络学习到这些特征之间的依赖关系,有效提升网络的性能。本文将在扩宽网络宽度的基础上,使用GN层在卷积核的通道维度内进行归一化,从而加快网络在训练时的收敛速度并提高网络的性能。</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag">2 深度可分离卷积宽残差超分辨率神经网络</h3>
                <h4 class="anchor-tag" id="66" name="66">2.1 <b>改进思想</b></h4>
                <div class="p1">
                    <p id="67">影响超分辨率网络重建性能的两个重要因素是网络的宽度和深度。基于更多的特征信息有益于提高超分辨率网络的重建性能这一思想,<i>WDSR</i>算法提出线性低秩卷积,用以扩展输入通道的特征维度,但随着网络宽度的增加,网络变得难以训练,限制了其宽度的继续增长。本文运用深度可分离卷积,改进<i>WDSR</i>算法的残差块,进一步提升网络的宽度,提出了基于深度可分离卷积的宽残差超分辨率神经网络(<i>WR</i>-<i>SR</i>)。同时,运用组归一化的方法,解决了由于网络变宽导致难以训练的难题,将输入特征的维度进行分组归一化处理,不仅可以加快网络的训练速度,而且可以提升网络的性能。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">2.2 <b>网络结构</b></h4>
                <h4 class="anchor-tag" id="69" name="69">2.2.1 基于深度可分离卷积的宽残差超分辨率神经网络</h4>
                <div class="p1">
                    <p id="70"><i>WR</i>-<i>SR</i>的网络深度为34层,包括32层的残差块(<i>Residual Body</i>)以及残差块前后2个提取特征的卷积层。通过改进残差块与使用组归一化方法,设计的整个<i>WR</i>-<i>SR</i>网络结构如图5所示。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 WR-SR网络结构" src="Detail/GetImg?filename=images/JSJY201909042_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>WR</i>-<i>SR</i>网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Network of WR</i>-<i>SR</i></p>

                </div>
                <div class="p1">
                    <p id="72"><i>WR</i>-<i>SR</i>算法直接使用随机裁剪的3×24×24大小的补丁作为输入,在残差块前后使用大小为3×3的卷积核(<i>Conv</i>)提取特征,使用<i>GN</i>层进行归一化;之后在残差块内学习图像之间的高级特征,输出3×4<sup>2</sup>×24×24的补丁,其中4是上采样的比例;最后通过像素重组(<i>Pixel Shuffle</i>)的方式将补丁上采样为3×96×96的大小。同时,使用全局残差学习(<i>Global Residual Learning</i>)将输入图像的原始特征信息与深层学习的高级特征信息叠加,输出重建的超分辨率图像。最终将原始的低分辨率图像放大4倍为超分辨率图像,且重建的图像细节信息丰富,同时减弱了噪声的影响。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">2.2.2 构建残差块</h4>
                <div class="p1">
                    <p id="74">为扩展卷积层的特征维度,即通道数,基于<i>Laurent Sifre</i>提出的深度可分离卷积模块对<i>WDSR</i>算法的残差块作了改进:通过分别计算通道和空间上的特征,使用3×3的卷积核和1×1的卷积核线性组合的方式重新设计了维度更高的残差块,将网络的特征维度分别扩展至了1 536维和2 048维。同时使用<i>GN</i>层对卷积层的通道数进行归一化,以加快网络在训练时的收敛速度。</p>
                </div>
                <div class="p1">
                    <p id="75">每个残差块内输入的特征图大小是<i>x</i>×<i>x</i>×256,其中<i>x</i>表示特征图的高或宽,256表示其通道数。首先,在通道相关性上,使用3×3大小的卷积核提取通道特征,得到<i>x</i>×<i>x</i>×256大小的特征图,再使用1×1大小的卷积核对这些特征图提取空间特征,即联合映射所有维度的相关性,得到<i>x</i>×<i>x</i>×1 536大小的特征图;其次,继续使用3×3卷积核和1×1卷积核以同样的方式将特征图扩宽至<i>x</i>×<i>x</i>×2 048;之后,使用ReLU作为激活函数,以增加输入的非线性;最后,使用3×3和1×1的卷积核,将卷积层的通道数由2 048维降到256维,以实现网络的快速训练。最终输出<i>x</i>×<i>x</i>×256大小的特征图。残差块的深度为32层,前一层的输出为后一层的输入。每层残差块的定义如图6所示。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 WR-SR残差块" src="Detail/GetImg?filename=images/JSJY201909042_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 WR-SR残差块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Residual body of WR-SR</p>

                </div>
                <div class="p1">
                    <p id="77">通过该残差块结构以及扩宽特征维度的方式,网络中来自浅层的低级图像特征可以更容易地传播到深层。改进的残差块获得了更清晰的细节及纹理信息,同时减弱了图像的噪声及伪影的影响。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">2.2.3 组归一化</h4>
                <div class="p1">
                    <p id="79">针对医学影像超分辨率,小批量的医学影像其均值和方差差异较大,因此不适用使用<i>BN</i>层进行归一化。由于<i>WR</i>-<i>SR</i>扩宽了卷积层的通道,<i>LN</i>层和<i>WN</i>层归一化也不适用于该网络的训练。因此,本文采用了<i>GN</i>层对<i>WR</i>-<i>SR</i>网络进行归一化。</p>
                </div>
                <div class="p1">
                    <p id="80">描述特征归一化的一般公式:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>/</mo><mi>σ</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<b><i>x</i></b><sub><i>i</i></sub>是由某一层计算的特征,<b><i>i</i></b>是索引,在二维图像的情况下,<b><i>i</i></b>=(<b><i>i</i></b><sub><b><i>N</i></b></sub>,<b><i>i</i></b><sub><b><i>C</i></b></sub>,<b><i>i</i></b><sub><b><i>H</i></b></sub>,<b><i>i</i></b><sub><b><i>W</i></b></sub>)是以(<b><i>N</i></b>,<b><i>C</i></b>,<b><i>H</i></b>,<b><i>W</i></b>)顺序索引的4维向量,其中<b><i>N</i></b>是批量向量,<b><i>C</i></b>是通道向量,<b><i>H</i></b>是高度向量,<b><i>W</i></b>是宽度向量。 <i>μ</i>和<i>σ</i>是由式(2)和式(3)计算的均值和标准差:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>x</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mo stretchy="false">(</mo></mstyle><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>ε</mi></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中:<i>m</i>是该集合的大小,<i>ε</i>是一个常数,<i>k</i>是索引。如式(4)所示,<i>S</i><sub><i>i</i></sub>为<i>μ</i>和<i>σ</i>的约束条件,它是计算均值和方差的像素集合。</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><mrow><mi>k</mi><mo>|</mo></mrow><mi mathvariant="bold-italic">k</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ν</mi></msub><mo>=</mo><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ν</mi></msub><mo>,</mo><mfrac><mrow><mi mathvariant="bold-italic">k</mi><msub><mrow></mrow><mi mathvariant="bold-italic">C</mi></msub></mrow><mrow><mi mathvariant="bold-italic">C</mi><mo>/</mo><mi>G</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi mathvariant="bold-italic">C</mi></msub></mrow><mrow><mi mathvariant="bold-italic">C</mi><mo>/</mo><mi>G</mi></mrow></mfrac></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中:<i><b>i</b></i><sub><i><b>N</b></i></sub>和<i><b>k</b></i><sub><i><b>N</b></i></sub>表示沿批量向量<i><b>N</b></i>的<i><b>i</b></i>和<i><b>k</b></i>的索引,这表示相同批量索引的像素被一起归一化。<i><b>i</b></i><sub><i><b>C</b></i></sub>和<i><b>k</b></i><sub><i><b>C</b></i></sub>表示索引<i><b>i</b></i>和<i><b>k</b></i>在同一组通道中。<i>G</i>是组的数量,它是预定义的超参数。<i><b>C</b></i>/<i>G</i>是每组的通道数。 GN沿(<i><b>H</b></i>,<i><b>W</b></i>)向量在一组通道内计算<i>μ</i>和<i>σ</i>。如Razavian等<citation id="199" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>在通过减少内部协变量偏移来加速深度网络训练中所述相同,组归一化也学习了一个通道线性变换,以补偿表示能力的可能丢失:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>γ</mi><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>β</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">其中:<i>γ</i>和<i>β</i>是可训练的比例和位移。</p>
                </div>
                <div class="p1">
                    <p id="89">综上所述,GN层由等式(1)、(2)、(3)、(4)、(5)定义。具体而言,同一组中的像素通过计算相同的<i>μ</i>和<i>σ</i>被一起归一化。除此之外,GN还学习每个通道的<i>γ</i>和<i>β</i>。通过运用组归一化的方法,WR-SR算法在训练期间加快L1损失收敛速度的同时获得了更高的PSNR值。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">2.3 <b>实验参数及训练过程</b></h4>
                <div class="p1">
                    <p id="91">实验所采用的数据集来源于开源的美国国家医学图书馆<i>MedPix</i>数据库中的肺部淋巴结数据,并从中选取了不同病人的900张512×512高分辨率医学影像用作训练与测试。实验使用800张图像作为训练集,100张图像作为测试集。为减少测试所需的时间,在训练期间取测试集中的10张图像作为验证集,以验证训练期间的<i>L</i>1损失和<i>PSNR</i>值。在训练期间,使用随机水平和旋转两种方式增强训练集。在训练之前将512×512的医学影像转为<i>png</i>格式的<i>RGB</i>图片,之后使用随机裁剪的方式将512×512的图像裁剪为96×96大小的补丁,并将其作为高分辨率(<i>High Resolution</i>, <i>HR</i>)图像,接着将96×96的<i>HR</i>图像下采样为24×24的低分辨率(<i>Low Resolution</i>, <i>LR</i>)图像并作为网络的输入。网络通过端到端的映射将<i>LR</i>图像上采样4倍为96×96大小的超分辨率(<i>Super Resolution</i>, <i>SR</i>)图像,通过计算并不断迭代<i>SR</i>和<i>HR</i>之间的<i>L</i>1损失以优化网络中的参数。</p>
                </div>
                <div class="p1">
                    <p id="92">实验采用<i>PSNR</i>和<i>SSIM</i>作为医学影像超分辨率的评价指标,同时以<i>L</i>1损失验证网络训练期间的收敛速度。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">3 实验结果</h3>
                <div class="p1">
                    <p id="94">本文的实验采用<i>mini</i>-<i>batch</i>的训练方式,<i>mini</i>-<i>batch</i>大小设置为16。初始学习率设置为10<sup>-4</sup>,每迭代2×105次,学习率减半,以加快网络的训练。本文的实验环境为<i>Ubuntu</i> 14.01.5 <i>LTS</i>操作系统,<i>Pytorch V</i>0.4.1,<i>CUDA Toolkit</i> 9.0,<i>Python</i>3.6。整个网络在两块<i>NVIDIA Tesla M</i>40上运行了2天。基于残差块的改进与<i>GN</i>层的替换,本文在相同的实验环境下做了两组对比实验,以验证改进算法<i>WR</i>-<i>SR</i>在医学影像超分辨率上的<i>PSNR</i>值和<i>SSIM</i>指标,该评价指标以验证集作为参考图像,通过计算测试结果与参考图像之间的差异评判网络的准确性。此外,在相同的实验环境和数据集下,与传统的超分辨率方法(包括<i>Biliner</i>和<i>Bicubic</i>)以及基于深度学习的超分辨率算法(包括<i>SRGAN</i>、<i>WDSR</i>)作了对比,以显示<i>WR</i>-<i>SR</i>算法的优越性。本文共做了三组对比实验,实验证明<i>WR</i>-<i>SR</i>算法在性能和评价指标上与其他算法相比,确实有明显提高。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">3.1 <b>改进残差块对比实验</b></h4>
                <div class="p1">
                    <p id="96">为了验证进一步扩宽特征维度的改进残差块与未改进的残差块的效果,在使用<i>GN</i>归一化层的基础上,以及相同的实验环境和数据集条件下做了两种残差块的对比实验。实验运行了250个时期(<i>epoch</i>),每个<i>epoch</i>内迭代了1 000次,共迭代了2.5×10<sup>5</sup>次。同时在10张图像的验证集上计算了网络的<i>L</i>1损失和<i>PSNR</i>值以验证网络的收敛性和准确性,同时对测试结果计算了<i>SSIM</i>,以评价各残差块在医学影像超分辨率上的优劣。两种方法的<i>L</i>1损失及<i>PSNR</i>值如图7和图8所示。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 两种方法的L1损失" src="Detail/GetImg?filename=images/JSJY201909042_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 两种方法的<i>L</i>1损失  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>L</i>1 <i>loss of two methods</i></p>

                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 两种方法的PSNR" src="Detail/GetImg?filename=images/JSJY201909042_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 两种方法的<i>PSNR</i>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>PSNR of two methods</i></p>

                </div>
                <div class="p1">
                    <p id="99">图7中,上下两条线分别代表<i>WDSR</i>网络和<i>WR</i>-<i>SR</i>网络的训练结果。从图7中可以看出,在迭代相同的250个<i>Epoch</i>的情况下,与<i>WDSR</i>网络相比,<i>WR</i>-<i>SR</i>网络在训练过程中,其<i>L</i>1损失收敛得更快,且获得了更低的损失值。证明<i>WR</i>-<i>SR</i>算法在改进残差块后,加快了网络的训练速度,同时提升了网络性能。</p>
                </div>
                <div class="p1">
                    <p id="100">图8中,上下两条线分别代表<i>WR</i>-<i>SR</i>网络和<i>WDSR</i>网络的训练结果。从图8中可以看出:两种方法均在200至250个<i>Epoch</i>期间趋于稳定。同时,与<i>WDSR</i>网络对比,<i>WR</i>-<i>SR</i>在改进残差块后收敛得更快,且获得更高了<i>PSNR</i>值。</p>
                </div>
                <div class="p1">
                    <p id="101">该组对比实验证明,通过使用深度可分离卷积扩展残差块的特征维度,改进的<i>WR</i>-<i>SR</i>网络获得了更快的收敛性和更高的准确性。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">3.2 <i>GN</i><b>对比实验</b></h4>
                <div class="p1">
                    <p id="103">该组实验在相同的数据集以及实验环境下分别使用<i>GN</i>归一化层和<i>WN</i>归一化层进行了实验。实验在10张图片的验证集上以<i>L</i>1损失评估两种方法的训练过程的准确性及收敛速度,其结果如图9所示。</p>
                </div>
                <div class="p1">
                    <p id="104">从图9(<i>a</i>)中可以看出,使用<i>WN</i>层归一化<i>WR</i>-<i>SR</i>网络时,其开始的<i>L</i>1损失较高。在网络训练至20个<i>Epoch</i>后,<i>L</i>1损失才出现明显收敛。</p>
                </div>
                <div class="p1">
                    <p id="105">从图9(<i>b</i>)中可以看出,使用<i>GN</i>层归一化<i>WR</i>-<i>SR</i>的网络其<i>L</i>1损失开始值远小于使用<i>WN</i>层归一化的网络。网络在训练至10个<i>Epoch</i>后出现明显收敛,且其<i>L</i>1损失为3.97,远小于使用<i>WN</i>层训练的网络,表示网络的准确性更高。</p>
                </div>
                <div class="p1">
                    <p id="106">该组对比实验证明,通过使用组归一化的方法,<i>WR</i>-<i>SR</i>网络获得了更快的收敛性,显著提升了网络的性能。同时,实验对比了各个方法的<i>PSNR</i>值和<i>SSIM</i>指标,其结果如表1所示。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 WN和GN归一化的L1损失" src="Detail/GetImg?filename=images/JSJY201909042_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 <i>WN</i>和<i>GN</i>归一化的<i>L</i>1损失  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 9 <i>L</i>1 <i>loss of WN and GN</i></p>

                </div>
                <div class="area_img" id="108">
                    <p class="img_tit"><b>表</b>1 <b>是否修改残差块和添加</b><i>GN</i><b>或</b><i>WN</i><b>时的</b><i>PSNR</i><b>和</b><i>SSIM</i> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>PSNR and SSIM whether</i><i>modifying residual block and adding GN or WN</i></p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td><br />算法</td><td><i>PSNR</i>/<i>dB</i></td><td><i>SSIM</i></td></tr><tr><td><br />修改残差块+<i>GN</i></td><td>39.684</td><td>0.983 75</td></tr><tr><td><br />未修改残差块+<i>GN</i></td><td>35.499</td><td>0.880 57</td></tr><tr><td><br />修改残差块+<i>WN</i></td><td>39.386</td><td>0.980 39</td></tr><tr><td><br />未修改残差块+<i>WN</i></td><td>33.386</td><td>0.874 81</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="109">从表1中可以看出,同时修改残差块和添加<i>GN</i>层归一化的方法,其<i>PSNR</i>值和<i>SSIM</i>指标均高于其余任何方法。且单独修改残差块的方法,其<i>PSNR</i>值和<i>SSIM</i>指标均显著高于单独未修改残差块的方法。同时,添加<i>GN</i>层的方法其<i>PSNR</i>值和<i>SSIM</i>指标均高于使用<i>WN</i>层的方法。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">3.3 <b>改进算法的对比实验</b></h4>
                <div class="p1">
                    <p id="111">本文将修改的残差块和使用<i>GN</i>层训练的网络作为最终模型,在测试集中选取了4幅图像,与传统的超分辨率算法(包括<i>Bilinear</i>、<i>Bicubic</i>算法和<i>SRVSR</i>算法)以及基于深度学习的<i>SRCNN</i>算法、<i>SRGAN</i>算法和<i>WDSR</i>算法进行了对比,并以<i>PSNR</i>与<i>SSIM</i>作为评价指标计算原图像与各算法生成的图像之间的差异。各算法的评价指标如表2所示,实验结果如图10所示。</p>
                </div>
                <div class="p1">
                    <p id="112">由表2可以看出,<i>WR</i>-<i>SR</i>算法在<i>PSNR</i>和<i>SSIM</i>上均显著高于传统方法与基于深度学习的算法,此外,<i>WR</i>-<i>SR</i>算法与<i>WDSR</i>原始算法对比,亦提升了1～2 <i>dB</i>的<i>PSNR</i>值与0.2～0.3的<i>SSIM</i>指标。</p>
                </div>
                <div class="p1">
                    <p id="113">图10(<i>a</i>)为原始高清图像(<i>Ground Truth</i>),图10(<i>b</i>)～(<i>h</i>)为表2中提到的各个算法训练出的实验结果。由图10显示的结果可以看出,使用<i>WR</i>-<i>SR</i>网络训练的实验结果其图像更加接近于原始图像,且均明显优于其他方法,得到了较好的超分辨率图像。该组对比实验证明,<i>WR</i>-<i>SR</i>算法在医学影像超分辨率上显示出了更好的细节收益以及更少的噪声以及伪影信息。</p>
                </div>
                <div class="area_img" id="114">
                                            <p class="img_tit">
                                                <b>表</b>2 <b>各方法的</b><i>PSNR</i><b>和</b><i>SSIM</i>
                                                    <br />
                                                <i>Tab</i>. 2 <i>PSNR and SSIM of each super</i>-<i>resolution algorithm</i>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909042_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 各方法的PSNR和SSIM" src="Detail/GetImg?filename=images/JSJY201909042_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909042_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 各个算法的实验结果" src="Detail/GetImg?filename=images/JSJY201909042_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 各个算法的实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909042_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 10 <i>Experimental results of each algorithm</i></p>

                </div>
                <h3 id="116" name="116" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="117">本文提出了一种基于深度可分离卷积的宽残差超分辨率神经网络算法,该算法使用深度可分离卷积构建残差块,以扩宽卷积层的特征维度,即通道数;使用组归一化方法对卷积层的通道分组,在组内进行归一化。三组对比实验显示,对于肺部淋巴结的4倍超分辨率图像,<i>WR</i>-<i>SR</i>算法在测试集上的<i>PSNR</i>和<i>SSIM</i>指标上均显著高于传统的超分辨率方法和其他基于深度学习的超分辨率算法。同时生成的图像细节丰富,减弱了噪声的影响,充分证明了改进算法在医学影像超分辨率上高效与准确性。在下一步的工作中,将使<i>WR</i>-<i>SR</i>算法结合生成式对抗网络(<i>Generative Adversarial Network</i>, <i>GAN</i>)框架,探讨感知质量良好但<i>PSNR</i>值较低的<i>GAN</i>网络对于医学影像,其超分辨率的结果是否也具有较高的准确性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="139">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating X-ray fluorescence computed tomography">

                                <b>[1]</b> RIVIERE P J L,VARGAS P,FU G.et al.Accelerating X-ray fluorescence computed tomography [C]// Proceedings of the 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway,NJ:IEEE,2009:1000-1003.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Superresolution in MRI: application to human white matter fiber tract visualization by diffusion tensor imaging">

                                <b>[2]</b> PELEDS,YESHURUNY.Superresolution in MRI:application to human white matter fiber tract visualization by diffusion tensor imaging[J].Magnetic Resonance in Medicine,2001,45(1):29-35.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-sensitivity Single-Photon Emission Computed Tomography (SPECT):safer,faster,and more accurate SPECT">

                                <b>[3]</b> SUBBARAO M.High-sensitivity Single-Photon Emission Computed Tomography (SPECT):safer,faster,and more accurate SPECT [C]// Proceedings of the 8th International Conference and Expo on Emerging Technologies for a Smarter World.Piscataway,NJ:IEEE,2011:1-2.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution image reconstruction: a technical overview">

                                <b>[4]</b> PARK S C,PARK M K,KANG M G.Super-resolution image reconstruction:a technical overview[J].IEEE Signal Processing Magazine,2003,20(3):21-36.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution Using a Genera-tive Adversarial Network">

                                <b>[5]</b> LEDIG C,THEIS L,HUSZAR F,et al.Photo-realistic single image super-resolution using a generative adversarial network [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:105-114.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual losses for real-time style transfer and superresolution">

                                <b>[6]</b> JOHNSON J,ALAHI A,LI F.Perceptual losses for real-time style transfer and super-resolution [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:694-711.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201308005&amp;v=Mjg3NDhIOUxNcDQ5RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WYnpJS0NMZlliRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 苏衡,周杰,张志浩.超分辨率图像重建方法综述[J].自动化学报,2013,39(8):1202-1213.(SU H,ZHOU J,ZHANG Z H.Survey of super-resolution image reconstruction methods [J].Acta Automatica Sinica,2013,39(8):1202-1213.)
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cubic convolution interpolation for digital image processing">

                                <b>[8]</b> KEYS R.Cubic convolution interpolation for digital image processing[J].IEEE Transactions on Acoustics,Speech and Signal Processing,1981,29(6):1153-1160.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">

                                <b>[9]</b> YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation [J].IEEE Transactions on Image Processing,2010,19(11):2861-2873.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">

                                <b>[10]</b> DONG C,LOY C C,HE K.et al.Image super-resolution using deep convolutional networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">

                                <b>[11]</b> LIM B,SON S,KIM H,et al.Enhanced deep residual networks for single image super-resolution [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway,NJ:IEEE,2017:1132-1440.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wide activation for efficient and accurate image super-resolution">

                                <b>[12]</b> YU J,FAN Y,YANG J,et al.Wide activation for efficient and accurate image super-resolution [EB/OL].[2019- 01- 23].https://arxiv.org/pdf/1808.08718v1.pdf.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">

                                <b>[13]</b> CHOLLET F.Xception:deep learning with depthwise separable convolutions [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:1800-1807.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Group normalization">

                                <b>[14]</b> WU Y,HE K.Group normalization [EB/OL].[2018- 12- 12].https://arxiv.org/pdf/1803.08494.pdf.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rotation,scaling and deformation invariant scattering for texture discrimination">

                                <b>[15]</b> SIFRE L,MALLAT S.Rotation,scaling and deformation invariant scattering for texture discrimination [C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ :IEEE,2013:1233-1240.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">

                                <b>[16]</b> IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1502.03167.pdf.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep multi-scale convolutional neural network for dynamic scene deblurring">

                                <b>[17]</b> NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:257-265.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Layer normalization">

                                <b>[18]</b> BA J L,KIROS J R,HINTON G E.Layer normalization [EB/OL].[2019- 01- 20].https://arxiv.org/pdf/1607.06450.pdf.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weight normalization:a simple reparameterization to accelerate training of deep neural networks">

                                <b>[19]</b> SALIMANS T,KIMGMA D P.Weight normalization:a simple reparameterization to accelerate training of deep neural networks [C]// Proceedings of the 30th Neural Information Processing Systems.New York:NIPS,2016:901-909.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CNN features off-the-shelf: an astounding base-line for recognition">

                                <b>[20]</b> RAZAVIAN A S,AZIZPOUR H,SULLIVAN J,et al.CNN features off-the-shelf:an astounding baseline for recognition [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway,NJ:IEEE,2014:512-519.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909042" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909042&amp;v=MzIxNzBqTXBvOUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmJ6SUx6N0JkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
