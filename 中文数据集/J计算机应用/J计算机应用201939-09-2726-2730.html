<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136482422471250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909041%26RESULT%3d1%26SIGN%3dDnysPVMBfYUmXOOnvfJAyHW%252b1Ss%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909041&amp;v=MTc3NzNvOUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmIvT0x6N0JkN0c0SDlqTXA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="1 网络模型 ">1 网络模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="1.1 &lt;b&gt;基于卷积的视频帧语义特征编码器&lt;/b&gt;">1.1 <b>基于卷积的视频帧语义特征编码器</b></a></li>
                                                <li><a href="#45" data-title="1.2 &lt;b&gt;基于&lt;/b&gt;LSTM&lt;b&gt;的时序特征融合器&lt;/b&gt;">1.2 <b>基于</b>LSTM<b>的时序特征融合器</b></a></li>
                                                <li><a href="#74" data-title="1.3 &lt;b&gt;基于反卷积的视频着色解码器&lt;/b&gt;">1.3 <b>基于反卷积的视频着色解码器</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#83" data-title="2 实验结果与对比分析 ">2 实验结果与对比分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="2.1 &lt;b&gt;视频帧图像着色结果&lt;/b&gt;">2.1 <b>视频帧图像着色结果</b></a></li>
                                                <li><a href="#88" data-title="2.2 &lt;b&gt;视频帧图像着色对比&lt;/b&gt;">2.2 <b>视频帧图像着色对比</b></a></li>
                                                <li><a href="#91" data-title="2.3 &lt;b&gt;视频序列时空一致性保持&lt;/b&gt;">2.3 <b>视频序列时空一致性保持</b></a></li>
                                                <li><a href="#101" data-title="2.4 &lt;b&gt;实验结果评估&lt;/b&gt;">2.4 <b>实验结果评估</b></a></li>
                                                <li><a href="#105" data-title="2.5 &lt;b&gt;用户研究&lt;/b&gt;">2.5 <b>用户研究</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="3 方法推广—动画片段彩色化 ">3 方法推广—动画片段彩色化</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#114" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="图1 视频着色网络模型">图1 视频着色网络模型</a></li>
                                                <li><a href="#48" data-title="图2 基于VGG-19的局部语义特征提取过程">图2 基于VGG-19的局部语义特征提取过程</a></li>
                                                <li><a href="#50" data-title="图3 LSTM神经元结构">图3 LSTM神经元结构</a></li>
                                                <li><a href="#52" data-title="图4 本文采用的双向RNN结构">图4 本文采用的双向RNN结构</a></li>
                                                <li><a href="#86" data-title="图5 本文算法视频帧着色结果">图5 本文算法视频帧着色结果</a></li>
                                                <li><a href="#90" data-title="图6 不同算法视频帧着色结果对比">图6 不同算法视频帧着色结果对比</a></li>
                                                <li><a href="#98" data-title="图7 本文方法与Gupta等方法视频帧局部着色对比">图7 本文方法与Gupta等方法视频帧局部着色对比</a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不用方法得到的&lt;/b&gt;NCD&lt;b&gt;值对比&lt;/b&gt;"><b>表</b>1 <b>不用方法得到的</b>NCD<b>值对比</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同方法视频帧彩色化效果定量评估&lt;/b&gt;"><b>表</b>2 <b>不同方法视频帧彩色化效果定量评估</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;用户研究指标&lt;/b&gt;"><b>表</b>3 <b>用户研究指标</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;用户研究平均分&lt;/b&gt;"><b>表</b>4 <b>用户研究平均分</b></a></li>
                                                <li><a href="#113" data-title="图8 《熊出没》片段彩色化效果">图8 《熊出没》片段彩色化效果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="148">


                                    <a id="bibliography_1" title=" CHENG Z,YANG Q,SHENG B .Colorization using neural network ensemble [J].IEEE Transactions on Image Processing,2017,26(11):5491-5505." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Colorization using neural network ensemble">
                                        <b>[1]</b>
                                         CHENG Z,YANG Q,SHENG B .Colorization using neural network ensemble [J].IEEE Transactions on Image Processing,2017,26(11):5491-5505.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_2" title=" DESHPANDE A,ROCK J,FORSYTH D.Learning large-scale automatic image colorization [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2015:567-575." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Large-Scale Automatic Image Colorization">
                                        <b>[2]</b>
                                         DESHPANDE A,ROCK J,FORSYTH D.Learning large-scale automatic image colorization [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2015:567-575.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_3" title=" IIZUKA S,SIMO-SERRA E,ISHIKAWA H.Let there be color!:joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification [J].ACM Transactions on Graphics,2016,35(4):Article No.110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMAFFF2D121F4EA546037EFF6E3BD86EAE&amp;v=MzIyNzBPR1FsZkJyTFUwNXRwaHhMcTV3SzQ9TmlmSVk4TE9hS2ZPMjQ1SFpaMExlUTA4eXhBVDZUZ0lQZ25rMlJGSERicVNNTXZxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         IIZUKA S,SIMO-SERRA E,ISHIKAWA H.Let there be color!:joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification [J].ACM Transactions on Graphics,2016,35(4):Article No.110.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_4" title=" CHENG Z,YANG Q,SHENG B.Deep colorization [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2015:415-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep colorization">
                                        <b>[4]</b>
                                         CHENG Z,YANG Q,SHENG B.Deep colorization [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2015:415-423.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_5" title=" LARSSON G,MAIRE M,SHAKHNAROVICH G.Learning representations for automatic colorization [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9908.Berlin:Springer,2016:577-593." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning representations for automatic colorization">
                                        <b>[5]</b>
                                         LARSSON G,MAIRE M,SHAKHNAROVICH G.Learning representations for automatic colorization [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9908.Berlin:Springer,2016:577-593.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_6" title=" ZHANG R,ISOLA P,EFROS A A.Colorful image colorization [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9907.Berlin:Springer,2016:649-666." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Colorful Image Colorization">
                                        <b>[6]</b>
                                         ZHANG R,ISOLA P,EFROS A A.Colorful image colorization [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9907.Berlin:Springer,2016:649-666.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     HOCHREITER S,SCHMIDHUBER J.LSTM can solve hard long time lag problems [C]// Proceedings of the 9th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,1996:473-479.</a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_8" title=" SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2019- 01- 03].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[8]</b>
                                         SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2019- 01- 03].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_9" title=" KARPATHY A,TODERICI G,SHETTY S,et al.Large-scale video classification with convolutional neural networks [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2014:1725-1732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale video classification with convolutional neural networks">
                                        <b>[9]</b>
                                         KARPATHY A,TODERICI G,SHETTY S,et al.Large-scale video classification with convolutional neural networks [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2014:1725-1732.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_10" title=" ULLAH A,AHMAD J,MUHAMMAD K,et al.Action recognition in video sequences using deep bi-directional LSTM with CNN features [J].IEEE Access,2018,6:1155-1166." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition in video sequences using deep bi-directional LSTM with CNN features">
                                        <b>[10]</b>
                                         ULLAH A,AHMAD J,MUHAMMAD K,et al.Action recognition in video sequences using deep bi-directional LSTM with CNN features [J].IEEE Access,2018,6:1155-1166.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_11" title=" HOCHREITER S,SCHMIDHUBER J.LSTM can solve hard long time lag problems [C]// Proceedings of the 9th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,1996:473-479." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSTM can solve hard long time lag problems">
                                        <b>[11]</b>
                                         HOCHREITER S,SCHMIDHUBER J.LSTM can solve hard long time lag problems [C]// Proceedings of the 9th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,1996:473-479.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_12" title=" SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[12]</b>
                                         SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_13" title=" PERAZZI F,PONT-TUSET J,McWILLIAMS B,et al.A benchmark dataset and evaluation methodology for video object segmentation [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:724-732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation">
                                        <b>[13]</b>
                                         PERAZZI F,PONT-TUSET J,McWILLIAMS B,et al.A benchmark dataset and evaluation methodology for video object segmentation [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:724-732.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_14" title=" GUPTA R K,CHIA A Y-S,RAJAN D,et al.A learning-based approach for automatic image and video colorization [EB/OL].[2019- 01- 20].https://arxiv.org/pdf/1704.04610.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A learning-based approach for automatic image and video colorization">
                                        <b>[14]</b>
                                         GUPTA R K,CHIA A Y-S,RAJAN D,et al.A learning-based approach for automatic image and video colorization [EB/OL].[2019- 01- 20].https://arxiv.org/pdf/1704.04610.pdf.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_15" title=" RUSSO F.Performance evaluation of noise reduction filters for color images through Normalized Color Difference (NCD) decomposition [J].ISRN Machine Vision,2014,2014:Article No.579658." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance evaluation of noise reduction filters for color images through Normalized Color Difference (NCD) decomposition">
                                        <b>[15]</b>
                                         RUSSO F.Performance evaluation of noise reduction filters for color images through Normalized Color Difference (NCD) decomposition [J].ISRN Machine Vision,2014,2014:Article No.579658.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2726-2730 DOI:10.11772/j.issn.1001-9081.2019020264            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于长短时记忆单元和卷积神经网络混合神经网络模型的视频着色方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%94%BF&amp;code=29830033&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张政</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E5%B1%B1&amp;code=10624347&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何山</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BA%E9%9D%96%E6%B7%87&amp;code=36387764&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贺靖淇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E7%9F%B3%E6%B2%B9%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0184187&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南石油大学计算机科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>视频可以看作是连续的视频帧图像组成的序列,视频彩色化的实质是对图像进行彩色化处理,但由于视频的长期序列性,若直接将现有的图像着色方法应用到视频彩色化上极易产生抖动或闪烁现象。针对这个问题,提出一种结合长短时记忆(LSTM)和卷积神经网络(CNN)的混合神经网络模型用于视频的着色。该方法用CNN提取视频帧的语义特征,同时使用LSTM单元学习灰度视频的时序信息,保证视频的时空一致性,然后融合局部语义特征和时序特征,生成最终的彩色视频帧序列。通过对实验结果的定量分析和用户研究表明,该方法在视频彩色化上实现了较好的效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E5%BD%A9%E8%89%B2%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频彩色化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短时记忆;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E7%A9%BA%E4%B8%80%E8%87%B4%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时空一致性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *张政(1994—),男,四川成都人,硕士研究生,主要研究方向:深度学习、图像处理;电子邮箱381092776@qq.com;
                                </span>
                                <span>
                                    何山(1972—),男,四川成都人,副教授,硕士,主要研究方向:数据挖掘、机器学习;;
                                </span>
                                <span>
                                    贺靖淇(1993—),男,四川成都人,硕士研究生,主要研究方向:嵌入式系统。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-19</p>

            </div>
                    <h1><b>Video colorization method based on hybrid neural network model of long short term memory and convolutional neural network</b></h1>
                    <h2>
                    <span>ZHANG Zheng</span>
                    <span>HE Shan</span>
                    <span>HE Jingqi</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Southwest Petroleum University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A video can be seen as a sequence formed by continuous video frames of images, and the colorization process of video actually is the colorization of images. If the existing image colorization method is directly applied to video colorization, it tends to cause flutter or twinkle because of long-term sequentiality of videos. For this problem, a method based on Long Short Term Memory(LSTM) cells and Convolutional Neural Network(CNN) was proposed to colorize the grayscale video. In the method, the semantic features of video frames were extracted with CNN and the time sequence information of video was learned by LSTM cells to keep the time-space consistency of video, then local semantic features and time sequence features were fused to generate the final colorized video frames. The quantitative assessment and user study of the experimental results show that this method achieves good performance in video colorization.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20colorization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video colorization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Long%20Short%20Term%20Memory(LSTM)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Long Short Term Memory(LSTM);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=time-space%20consistency&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">time-space consistency;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Zheng, born in 1994, M. S. candidate. His research interests include deep learning, image processing. ;
                                </span>
                                <span>
                                    HE Shan, born in 1972, M. S. , associate professor. His research interests include data mining, machine learning. ;
                                </span>
                                <span>
                                    HE Jingqi, born in 1993, M. S. candidate. His research interests include embedded system.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-02-19</p>
                            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="34">图像的彩色化是根据灰度图像将矢量RGB(Red Green Blue)分配给图像的每个像素,该技术在历史相片处理、视频处理、艺术品修复等方面有着广泛的应用前景。目前,着色算法主要分为三种类型:基于用户涂鸦的方法、基于参考图的方法和基于深度学习的方法。</p>
                </div>
                <div class="p1">
                    <p id="35">由于传统方法需要用户的干预,增加了实现的复杂度。随着卷积神经网络(Convolutional Neural Network, CNN)在物体识别和检测任务上取得的巨大成功,它对图像深层语义特征的表达能力引起了研究人员的广泛关注,为彩色化提供了巨大的帮助。因为图像的语义特征可以提供与颜色相关的附加信息,所以近几年提出的着色算法<citation id="178" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>都使用卷积神经网络来提取灰色图像的语义特征,并参考图像的语义特征进行着色。如Despande等<citation id="179" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出一种针对大规模图像的自动着色算法;Iizuka等<citation id="180" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出利用卷积神经网络获取图像的全局特征和局部特征,对于户外景观照片取得了许多令人满意的结果;Cheng等<citation id="181" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出一种通过卷积神经网络提取图像深层特征进行自动着色的算法,并结合双边滤波来改善着色结果;Larsson等<citation id="182" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出利用深度网络提取图像的底层细节特征和高层语义特征,实现图像自动着色,生成的彩色图像能够让大多数人感到真实;Zhang等<citation id="183" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>利用VGG(Visual Geometry Group)卷积神经网络模型,对灰度图像提取特征,并通过预测每个像素的颜色分布直方图来为灰度图像着色。</p>
                </div>
                <div class="p1">
                    <p id="36">然而,在Iizuka等<citation id="184" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>的着色算法中,该方法采用两路神经网络模型来得到图片的颜色信息和对象类别,并结合两个损失函数来进行训练。由于网络模型中包含大量训练参数,所以训练时间相当长;另外尽管作者已经使用了共享权值的技巧来降低复杂度,双路卷积网络仍然面临权值平衡问题;并且目标函数只是简单的线性结合,如果将该方法直接扩展到视频的着色,容易产生抖动和闪烁现象。本文针对这些问题,提出了结合长短时记忆(Long Short Term Memory, LSTM)单元<citation id="185" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和卷积神经网络(CNN)的混合神经网络模型用于视频的彩色化,并结合视频的时序信息来指导着色,使得模型的训练时间减少并且模型准确度相对提高。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag">1 网络模型</h3>
                <div class="p1">
                    <p id="38">本文方法采用的视频着色网络模型如图1所示。首先,使用经典卷积神经网络模型VGG-19<citation id="186" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>作为编码器来提取视频帧的局部语义特征和全局特征。然后,将在编码器中提取到的特征图进行规模调整,再输入循环神经网络中。本文采用LSTM结构作为循环神经网络的记忆单元来学习视频序列的时序信息,同时进一步细化提取到的特征。最后,将LSTM的结果输入到视频的着色网络中进行基于反卷积的解码过程<citation id="187" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,通过解码操作合成为所需要的彩色视频序列{<i>C</i><sub><i>t</i></sub>|<i>t</i>=1,2,…,<i>n</i>}。整个网络的训练过程将VGG-19网络提取特征图的过程看作一个黑箱操作,学习时序特征的LSTM结构的输入数据是VGG-19输出的高维抽象特征数据。通过训练这些特征数据,就能得到本文的着色模型。</p>
                </div>
                <div class="area_img" id="39">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909041_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 视频着色网络模型" src="Detail/GetImg?filename=images/JSJY201909041_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 视频着色网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909041_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Video colorization network structure</p>

                </div>
                <h4 class="anchor-tag" id="40" name="40">1.1 <b>基于卷积的视频帧语义特征编码器</b></h4>
                <div class="p1">
                    <p id="41">基于卷积神经网络实现对视频帧语义特征的提取,也叫作编码操作,本文采用的是基于预训练的VGG-19模型来实现对视频序列的编码。</p>
                </div>
                <div class="p1">
                    <p id="42">在本文中的视频着色模型中,首先通过VGG-19对输入的视频序列进行编码,提取视频帧图像的局部语义特征。该模型将灰度视频序列作为输入,由一系列卷积操作对灰度视频帧进行编码,并输出对应的特征表示<i>f</i>=(<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>,…, <i>f</i><sub><i>t</i></sub>),对灰度视频帧的操作可以抽象为如下的编码过程:</p>
                </div>
                <div class="p1">
                    <p id="43"><i>f</i><sub><i>i</i></sub>=<i>encoder</i>([<i>x</i><sub><i>i</i></sub>,<i>C</i><sub><i>i</i></sub>]); <i>i</i>∈{1,2,…,<i>t</i>}      (1)</p>
                </div>
                <div class="p1">
                    <p id="44">然后将经过编码操作得到的特征表示<i>f</i>=(<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>,…, <i>f</i><sub><i>t</i></sub>)送入LSTM结构中,用于学习视频序列的时序特征。</p>
                </div>
                <h4 class="anchor-tag" id="45" name="45">1.2 <b>基于</b>LSTM<b>的时序特征融合器</b></h4>
                <div class="p1">
                    <p id="46">本文方法采用将LSTM结构作为记忆单元的双向循环神经网络来提取视频的时序特征,学习时序特征的主要目的是提取连续视频帧之间的时序信息,并且保证特征表示的时序一致性。同时将时序特征和由基于VGG-19的编码器得到的局部语义特征进行融合<citation id="188" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,进一步细化时序特征。</p>
                </div>
                <div class="p1">
                    <p id="47">图2展示了本文基于VGG-19预训练模型的语义特征提取过程,VGG-19使用了级联结构,网络模型的卷积层之间将多个3×3的卷积核和1×1的卷积核相结合,网络级联使得上层提取的所有彩色图像特征融合在一起,传递到下一层进一步提取高维特征。VGG-19对本文使用的数据集具有很好的泛化能力,能够提取到视频帧图像的深度特征。这里灰度视频的第一帧是作为单独的图像进行着色的,其后的视频帧则需要共享前面视频帧着色过程中的时序信息和着色网络中的参数,来实现保持视频的时空一致性。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909041_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于VGG-19的局部语义特征提取过程" src="Detail/GetImg?filename=images/JSJY201909041_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于VGG-19的局部语义特征提取过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909041_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Local semantic feature extracting process based on VGG-19</p>

                </div>
                <div class="p1">
                    <p id="49">LSTM是门限循环神经网络(Recurrent Neural Network, RNN)中最著名的一种<citation id="189" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>,门限RNN允许在不同时刻改变连接间的权重系数,且允许网络忘记当前已经累积的信息。LSTM结构的关键在于引入了一个判断信息是否有用的处理单元,这个处理单元称为“cell”,单个cell的结构如图3所示。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909041_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 LSTM神经元结构" src="Detail/GetImg?filename=images/JSJY201909041_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 LSTM神经元结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909041_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Single cell structure of LSTM</p>

                </div>
                <div class="p1">
                    <p id="51">LSTM实现添加或者删除信息是通过一种叫作门的结构来实现的,通过增加门限使得自循环的权重是变化的,这样一来在模型参数固定的情况下,不同时刻的积分尺度可以动态改变,从而避免了梯度消失或者梯度膨胀的问题。本文采用的视频着色模型的工作原理可以简化为如图4所示,使用双向循环神经网络学习视频的时序信息,其中的记忆单元采用了LSTM神经单元,其结构在图3中进行了详细的描述,采用LSTM结构的原因是LSTM神经元中的门限结构可以更好地学习连续视频的长期一致性。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909041_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 本文采用的双向RNN结构" src="Detail/GetImg?filename=images/JSJY201909041_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 本文采用的双向RNN结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909041_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Bi-directional recurrent neural network</p>

                </div>
                <div class="p1">
                    <p id="53">LSTM的遗忘门决定了上一时刻的“cell”单元状态<i>P</i><sub><i>t</i>-1</sub>有多少保留到当前时刻<i>P</i><sub><i>t</i></sub>。本文方法中,遗忘门会读取上一视频帧的隐含层特征<i>h</i><sub><i>t</i>-1</sub>和<i>x</i><sub><i>t</i></sub>,然后由Sigmoid层处理输出一个在0到1之间的数,其中0表示全部舍弃,1表示全部保留。在视频着色过程中,对于从VGG-19中获取的视频帧局部特征和语义特征,如果相邻视频帧中出现了相同的特征,那么这个特征将被保留,并且在着色后该特征在相邻视频帧中会得到相同的色度值;否则该特征将在遗忘门被舍弃。遗忘门的表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="54"><i>f</i><sub><i>t</i></sub>=<i>Sigmoid</i>(<i>W</i><sub><i>f</i></sub>·[<i>h</i><sub><i>t</i>-1</sub>,<i>x</i><sub><i>t</i></sub>]+<i>b</i><sub><i>f</i></sub>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="55">其中:<i>h</i><sub><i>t</i>-1</sub>是上一视频帧cell的输出,<i>x</i><sub><i>t</i></sub>是当前cell的输入,<i>W</i><sub><i>f</i></sub>和<i>b</i><sub><i>f</i></sub>是网络训练得到的遗忘门的权重和偏置。</p>
                </div>
                <div class="p1">
                    <p id="56">LSTM的输入门决定让多少新的信息加入到cell状态中来。实现这个操作需要两个步骤:首先,Sigmoid层决定哪些信息需要更新,也就是需要提取的新的特征;其次,tanh层生成一个向量,也就是备选的用来更新的内容。然后通过结合这两部分信息,更新cell的状态。本文视频着色方法过程中,相邻视频帧之间新的特征便是通过输入门输入,并在cell状态中被记录更新。输入门的操作如下:</p>
                </div>
                <div class="p1">
                    <p id="57"><i>i</i><sub><i>t</i></sub>=<i>Sigmoid</i>(<i>W</i><sub><i>i</i></sub>·[<i>h</i><sub><i>t</i>-1</sub>,<i>x</i><sub><i>t</i></sub>]+<i>b</i><sub><i>i</i></sub>)      (3)</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>C</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>C</mi></msub><mo>⋅</mo><mo stretchy="false">[</mo><mi>h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">]</mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中:<i>i</i><sub><i>t</i></sub>表示输入的需要更新的信息,<mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>C</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>是用来更新的备选内容,<i>C</i><sub><i>t</i></sub>是更新之后的cell状态,<i>W</i><sub><i>i</i></sub>,<i>W</i><sub><i>C</i></sub>,<i>b</i><sub><i>i</i></sub>,<i>b</i><sub><i>C</i></sub>分别是输入门中的权重和偏置。当前状态<i>C</i><sub><i>t</i></sub>是由上一次单元状态<i>C</i><sub><i>t</i>-1</sub>与<i>f</i><sub><i>t</i></sub>相乘,舍弃需要舍弃的特征信息,再加上新的候选值<mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>*</mo><mover accent="true"><mi>C</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>,如下式:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>*</mo><mi>C</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>*</mo><mover accent="true"><mi>C</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">这样就把LSTM关于当前的记忆<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>C</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>和长期的记忆<i>C</i><sub><i>t</i>-1</sub>组合到了一起,形成了新的cell状态。由于遗忘门的控制,它可以保存很久之前的信息,同时由于输入门的控制,它又可以避免当前无关紧要的内容进入记忆。</p>
                </div>
                <div class="p1">
                    <p id="65">最后,LSTM的输出门确定要输出哪些信息。首先运行一个Sigmoid层来确定细胞状态的哪个部分将输出;其次,把细胞状态通过tanh进行处理,得到一个在-1～1的值,并将它和输入门的输出相乘。最终仅会输出LSTM在输出门中确定输出的那部分特征信息,输出门的过程如下:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>o</i><sub><i>t</i></sub>=<i>Sigmoid</i>(<i>W</i><sub><i>o</i></sub>·[<i>h</i><sub><i>t</i>-1</sub>,<i>x</i><sub><i>t</i></sub>]+<i>b</i><sub><i>o</i></sub>)      (6)</p>
                </div>
                <div class="p1">
                    <p id="67"><i>h</i><sub><i>t</i></sub>=<i>o</i><sub><i>t</i></sub>*tanh(<i>C</i><sub><i>t</i></sub>)      (7)</p>
                </div>
                <div class="p1">
                    <p id="68">特征提取时,在前向传播阶段,网络对于输入的视频帧序列从时间节点1到时间节点<i>t</i>,正向计算一遍,并保存每个时间节点向前隐藏层的输出。正向计算完成后,再由时间节点<i>t</i>到时间节点1反向计算一遍,同时保存每个时间节点的向后隐藏层的输出。当正向传播阶段和反向传播阶段都完成计算之后,融合各时间节点向前和向后隐藏层的输出作为最后的输出。反向传播过程中,模型通过基于时间的反向传播(Back Propagation Through Time, BPTT)算法更新所有输出层的信息。另外,所有传播过程中隐藏层参数的更新都是通过LSTM门限结构进行控制,用<i>h</i><sup>L</sup><sub><i>i</i></sub>和<i>h</i><sup>R</sup><sub><i>i</i></sub>分别表示前向传播阶段向前和向后的隐藏层,基于LSTM的特征融合过程如下:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>h</i><sup>L</sup><sub><i>i</i></sub>=<i>h</i><sup>L</sup><sub><i>i</i>+1</sub>+<i>g</i>(<i>W</i><sup>L</sup><sub><i>f</i></sub>·<i>f</i><sub><i>i</i></sub>+<i>W</i><sup>L</sup><sub><i>h</i></sub>·<i>h</i><sup>L</sup><sub><i>i</i>+1</sub>+<i>b</i><sup>L</sup>)      (8)</p>
                </div>
                <div class="p1">
                    <p id="70"><i>h</i><sup>R</sup><sub><i>i</i></sub>=<i>h</i><sup>R</sup><sub><i>i</i>-1</sub>+<i>g</i>(<i>W</i><sup>R</sup><sub><i>f</i></sub>·<i>f</i><sub><i>i</i></sub>+<i>W</i><sup>R</sup><sub><i>h</i></sub>·<i>h</i><sup>R</sup><sub><i>i</i>-1</sub>+<i>b</i><sup>R</sup>)      (9)</p>
                </div>
                <div class="p1">
                    <p id="71">式中:<i>h</i><sup>L</sup><sub><i>i</i></sub>和<i>h</i><sup>R</sup><sub><i>i</i></sub>分别是前向和后向的隐藏层状态,<i>g</i>(·)是用于跟记忆单元传输信息的函数,这里指LSTM(),<i>h</i>(·)表示激活函数,本文使用的是tanh()。为了提取视频的时序特征并保持时空一致性,采用了结合LSTM结构的双向循环神经网络,用<i>e</i>=(<i>e</i><sub>1</sub>,<i>e</i><sub>2</sub>,…,<i>e</i><sub><i>t</i></sub>)来表示融合之后的特征,并作为融合后的结果输出。</p>
                </div>
                <div class="p1">
                    <p id="72"><i>e</i><sub><i>i</i></sub>=<i>h</i>(<i>W</i>[<i>h</i><sup>L</sup><sub><i>i</i></sub>;<i>h</i><sup>R</sup><sub><i>i</i></sub>]+<i>b</i>),<i>i</i>∈{1,2,…,<i>t</i>}      (10)</p>
                </div>
                <div class="p1">
                    <p id="73">通过将融合之后的视频帧序列<i>e</i>=(<i>e</i><sub>1</sub>,<i>e</i><sub>2</sub>,…,<i>e</i><sub><i>t</i></sub>)输入基于反卷积的解码器中进行着色操作,得到最后的彩色视频帧序列。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74">1.3 <b>基于反卷积的视频着色解码器</b></h4>
                <div class="p1">
                    <p id="75">在视频时序特征融合阶段,已经得到了融合之后的优化特征序列<i>e</i>=(<i>e</i><sub>1</sub>,<i>e</i><sub>2</sub>,…,<i>e</i><sub><i>t</i></sub>),而我们的最终目标是要通过基于反卷积的解码器<citation id="190" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>来生成彩色视频序列<i>C</i>=(<i>C</i><sub>1</sub>,<i>C</i><sub>2</sub>,…,<i>C</i><sub><i>t</i></sub>)。前面已经提到,视频的处理需要考虑保持连续视频序列之间的时空一致性以及连续性,在解码器阶段,其输入是优化之后的融合特征表示<i>e</i>=(<i>e</i><sub>1</sub>,<i>e</i><sub>2</sub>,…,<i>e</i><sub><i>t</i></sub>),这便很好地保证了视频的时空一致性,将该特征序列送入解码器中基于下式进行解码即可得到彩色化的视频帧序列。</p>
                </div>
                <div class="p1">
                    <p id="76"><i>C</i><sub><i>i</i></sub>=<i>decoder</i>(<i>e</i><sub><i>i</i></sub>); <i>i</i>∈{1,2,…,<i>t</i>}      (11)</p>
                </div>
                <div class="p1">
                    <p id="77">本文参考Shelhamer等<citation id="191" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的思想,采用的方法是将反卷积层作为解码器,反卷积和卷积互为逆过程,也叫卷积转置,利用反卷积可以实现特征序列的可视化。本文选用反卷积操作是因为反卷积跟卷积操作类似,反卷积常被用于对CNN的输出进行上采样回到原始图像分辨率,这样能够更好地保留图像的原始结构,另外反卷积操作在图像分类、图像分割、图像生成、边缘检测等领域有广泛应用。</p>
                </div>
                <div class="p1">
                    <p id="78">在解码器过程中,通过一系列反卷积操作来实现将融合的后时序特征<i>e</i>=(<i>e</i><sub>1</sub>,<i>e</i><sub>2</sub>,…,<i>e</i><sub><i>t</i></sub>)进行解码。在基于反卷积的解码器中,本文使用6个反卷积层作为解码器,设置卷积核大小为3×3,并在第4个池化层后插入了一个1×1的卷积层以获得额外的输出,这将被添加到第二个反卷积层的输出。最终视频帧序列经过解码后转换成彩色的视频帧序列输出。</p>
                </div>
                <div class="p1">
                    <p id="79">本文方法选择交叉熵损失函数作为网络的训练目标函数,采用交叉熵的原因是交叉熵作为损失函数时在使用Sigmoid函数在梯度下降时其学习速率可以被输出的误差所控制,而且收敛速度更快,能够有效地避免均方误差损失函数学习速率降低的问题,也更不容易陷入局部最优解。</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mo>-</mo><mrow><mi>lg</mi></mrow><mspace width="0.25em" /><mi>p</mi><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>Ν</mi></munderover><mrow><mi>exp</mi></mrow></mstyle><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中:<i>C</i>表示的是网络模型数据的实际输出值;<i>p</i>表示概率,它与训练集上的负对数相同。</p>
                </div>
                <div class="p1">
                    <p id="82">本文采用的是基于预训练的VGG-19模型用于提取局部语义特征,训练数据采用的是DAVIS数据集<citation id="192" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>中的视频序列作为训练集,数据集中包含50段10 s左右的视频序列,色彩丰富,类别多样,常用于视频的处理;训练过程中设置学习速率为0.000 1,批处理数据量大小设置为10,同样本文选取ReLU作为激活函数,优化方法采用随机梯度下降法,目标函数采用交叉熵损失函数,LSTM节点设置为64个。</p>
                </div>
                <h3 id="83" name="83" class="anchor-tag">2 实验结果与对比分析</h3>
                <h4 class="anchor-tag" id="84" name="84">2.1 <b>视频帧图像着色结果</b></h4>
                <div class="p1">
                    <p id="85">实验表明,用本文模型对单帧图像的着色时间达到了秒级,对验证集里的单帧视频帧图像的着色时间约为3 s,而Iizuka等<citation id="193" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>的方法,单帧图像的处理时间均在5 s以上,说明本文方法在效率上已经有较大提升。图5展示了从数据集中随机选取的3段视频的第5、15、25、35、45帧视频图像及其彩色化过后的视频图像。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909041_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 本文算法视频帧着色结果" src="Detail/GetImg?filename=images/JSJY201909041_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 本文算法视频帧着色结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909041_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Colorized results of video frames by the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="87">关于视频帧的对比度保留,从图5可以看到,奶牛和草原的背景色彩存在明显的对比度,着色后的视频帧也能够很好地区分;同样,灰熊和石头的颜色对比度也被很好地保持;第6行行人和背景以及长椅的色彩对比度也被较好地保持。可以看出,本文方法得到的视频图像很好地保持了视频序列的对比度。所以本文基于VGG-19的编码器能够很好地提取图片帧的深层特征,并用于指导视频帧着色。综上,本文结合LSTM和CNN的混合神经网络模型能够很好地实现对视频序列的彩色化,验证了方法的有效性。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">2.2 <b>视频帧图像着色对比</b></h4>
                <div class="p1">
                    <p id="89">除了展示本文方法对视频帧的彩色化效果,还跟Iizuka等<citation id="194" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Cheng等<citation id="195" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>以及Larsson等<citation id="196" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的经典着色算法对视频帧图像进行着色的结果进行对比。选取了5段视频中的第20帧图像进行着色,对比实验结果如图6所示。可以看到本文方法得到的单帧视频帧图像的彩色化结果对包含不同场景的多个实体都有很好的图像着色效果,生成的彩色视频帧看起来更真实、更自然。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909041_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同算法视频帧着色结果对比" src="Detail/GetImg?filename=images/JSJY201909041_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同算法视频帧着色结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909041_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of colorized results of video frames by different algorithms</p>

                </div>
                <h4 class="anchor-tag" id="91" name="91">2.3 <b>视频序列时空一致性保持</b></h4>
                <div class="p1">
                    <p id="92">除了对单独的视频帧进行着色展示以外,还对彩色化后的视频序列的视频时空一致性进行了评估。视频的时空一致性含义主要考虑视频序列在相邻视频帧的相同局部特征在彩色化后的色度值是否相同。如果在整个视频序列中,相同局部特征的色度值越相似,表明时空一致性保持越好,保持时空一致性能有效避免视频出现闪烁或卡顿等现象。</p>
                </div>
                <div class="p1">
                    <p id="93">追踪了一段视频的4幅视频帧图像,同Gupta等<citation id="197" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的方法进行对比,并将着色图像的局部放大以观察其效果,图7展示了本文方法和Gupta等<citation id="198" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的方法对视频图像的着色效果。矩形方框内是放大的局部,可以更好地看到本文方法得到的汽车颜色更准确,并且颜色饱和度优于Gupta等<citation id="199" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的结果。</p>
                </div>
                <div class="p1">
                    <p id="94">使用归一化色差(Normalized Color Difference, NCD)<citation id="200" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>来评估视频序列的连续性和时空一致性, NCD的值越小,说明连续视频帧之间的归一化色差越小,即彩色视频序列的颜色值越平滑,从而更好地保持视频序列的连续性和时空一致性。NCD的定义如下:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><mi>C</mi><mi>D</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mrow><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mo stretchy="false">[</mo></mstyle><mi>Ζ</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy="false">)</mo><mo>-</mo><msup><mi>Ζ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mstyle></mrow></mstyle></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mrow><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mo stretchy="false">[</mo></mstyle><mi>Ζ</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mstyle></mrow></mstyle></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中:<i>Q</i>代表颜色通道,<i>H</i>和<i>W</i>代表图像大小,<i>Z</i>和<i>Z</i>′分别代表相邻的视频帧图像,<i>q</i><sub>1</sub>、<i>q</i><sub>2</sub>、<i>q</i><sub>3</sub>分别代表L、a、b三个颜色通道。</p>
                </div>
                <div class="p1">
                    <p id="97">选取了两段视频中的连续五帧图像并分别计算它们之间的NCD值然后求出其平均值,将本文方法的结果和Iizuka等<citation id="201" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Zhang 等<citation id="202" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>以及Gupta等<citation id="203" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的方法得到的结果进行对比,结果记录于表1中。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909041_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 本文方法与Gupta等方法[14]视频帧局部着色对比" src="Detail/GetImg?filename=images/JSJY201909041_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 本文方法与Gupta等方法<citation id="204" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>视频帧局部着色对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909041_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Local video frame colorization comparison between the proposed method and the method in <citation id="205" type="reference"><link href="174" rel="bibliography" />[14]</citation></p>

                </div>
                <div class="area_img" id="99">
                    <p class="img_tit"><b>表</b>1 <b>不用方法得到的</b>NCD<b>值对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Comparison of NCD in different methods</p>
                    <p class="img_note"></p>
                    <table id="99" border="1"><tr><td><br />方法</td><td>视频1-NCD</td><td>视频2-NCD</td></tr><tr><td><br />Iizuka等方法<sup>[3]</sup></td><td>0.013 9</td><td>0.011 4</td></tr><tr><td><br />Zhang等方法<sup>[6]</sup></td><td>0.019 6</td><td>0.015 8</td></tr><tr><td><br />Gupta等方法<sup>[14]</sup></td><td>0.015 5</td><td>0.012 9</td></tr><tr><td><br />本文方法</td><td>0.012 8</td><td>0.010 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="100">从实验结果可以看到,相对于Iizuka等<citation id="206" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation> 和Zhang等<citation id="207" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>以及Gupta等<citation id="208" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的结果,本文方法得到的NCD值较小,表明连续视频帧图像之间的归一化色差越小,也说明了本文方法较好地保持了视频的连续性和时空一致性。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">2.4 <b>实验结果评估</b></h4>
                <div class="p1">
                    <p id="102">为了验证本文方法的有效性,本文选取了定量评估指标峰值信噪比PSNR、均方根误差RMSE以及结构相似性SSIM对视频帧图像的彩色化效果进行评估,定量评估的目标主要是对本文方法得到的彩色视频序列和原始彩色视频序列进行比较,主要考虑本文方法是否引入更多噪声以及两幅图像的误差大小。随机选取了一段视频中的第10、20、30、40帧图像进行评估,表2是评估结果。</p>
                </div>
                <div class="p1">
                    <p id="103">从表2中可以看出,本文方法对视频帧的着色后定量评价指标PSNR和RMSE以及SSIM在一定程度上都优于现有方法,验证了本文方法的可行性。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表</b>2 <b>不同方法视频帧彩色化效果定量评估</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Quantitative assessment of video frame colorization by different methods</p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td rowspan="2">方法</td><td colspan="3"><br />视频帧号=10</td><td rowspan="2"></td><td colspan="3"><br />视频帧号=20</td><td rowspan="2"></td><td colspan="3"><br />视频帧号=30</td><td rowspan="2"></td><td colspan="3"><br />视频帧号=40</td></tr><tr><td><br />PSNR/dB</td><td>RMSE</td><td>SSIM</td><td><br />PSNR/dB</td><td>RMSE</td><td>SSIM</td><td><br />PSNR/dB</td><td>RMSE</td><td>SSIM</td><td><br />PSNR/dB</td><td>RMSE</td><td>SSIM</td></tr><tr><td>Iizuka等方法<sup>[3]</sup></td><td>36.011 2</td><td>0.006 9</td><td>0.935 2</td><td></td><td>35.837 2</td><td>0.007 5</td><td>0.935 3</td><td></td><td>36.424 7</td><td>0.008 9</td><td>0.946 1</td><td></td><td>38.209 1</td><td>0.009 0</td><td>0.957 6</td></tr><tr><td><br />Cheng等方法<sup>[4]</sup></td><td>36.274 3</td><td>0.015 7</td><td>0.944 7</td><td></td><td>36.106 6</td><td>0.016 1</td><td>0.943 5</td><td></td><td>36.765 2</td><td>0.0177</td><td>0.954 4</td><td></td><td>38.985 7</td><td>0.008 4</td><td>0.964 7</td></tr><tr><td><br />Larsson等方法<sup>[5]</sup></td><td>35.995 4</td><td>0.012 8</td><td>0.936 5</td><td></td><td>35.834 4</td><td>0.012 0</td><td>0.938 7</td><td></td><td>36.428 7</td><td>0.009 6</td><td>0.946 9</td><td></td><td>38.174 6</td><td>0.006 3</td><td>0.957 8</td></tr><tr><td><br />本文方法</td><td>36.413 5</td><td>0.007 1</td><td>0.950 0</td><td></td><td>36.067 4</td><td>0.006 9</td><td>0.941 2</td><td></td><td>36.936 5</td><td>0.007 7</td><td>0.954 1</td><td></td><td>39.023 6</td><td>0.005 1</td><td>0.966 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">2.5 <b>用户研究</b></h4>
                <div class="p1">
                    <p id="106">为更进一步测试本文方法的结果,本文进行了一组用户研究来评估视频彩色化效果,这里用户研究主要是视频的彩色化质量测试。邀请了50名年龄在20～30岁的用户参与用户研究,主要考虑视频帧图像的颜色饱和度、自然度,以及视频序列的时空一致性和人眼视觉感官的效果。用户研究评分指标如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="107">让用户观看5组视频序列的彩色化效果然后统计出结果如表4所示,结果表明本文方法得到的彩色视频序列无论是在颜色自然度还是饱和度上都优于Gupta等<citation id="209" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的方法。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit"><b>表</b>3 <b>用户研究指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 User study standard</p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td><br />分数</td><td>视频帧视觉感知质量</td></tr><tr><td><br />1</td><td>未保留灰度视频帧的信息</td></tr><tr><td><br />2</td><td>色彩低饱和且不自然</td></tr><tr><td><br />3</td><td>色彩饱和但不自然</td></tr><tr><td><br />4</td><td>色彩饱和但边缘色彩混合</td></tr><tr><td><br />5</td><td>色彩饱和无边缘色彩混合</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="109">
                    <p class="img_tit"><b>表</b>4 <b>用户研究平均分</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Average scores of user study</p>
                    <p class="img_note"></p>
                    <table id="109" border="1"><tr><td><br />方法</td><td>色彩自然度</td><td>色彩饱和度</td><td>色彩丰富度</td></tr><tr><td><br />Gupta等方法<sup>[14]</sup></td><td>3.81</td><td>4.05</td><td>4.24</td></tr><tr><td><br />本文方法</td><td>4.56</td><td>4.52</td><td>4.41</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="110">从用户研究结果可以看出,本文方法得到的彩色视频帧和视频序列的效果优于现有方法,在色彩自然度和饱和度方面都展现了更好的效果,充分验证了本文方法的有效性。</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag">3 方法推广—动画片段彩色化</h3>
                <div class="p1">
                    <p id="112">为了对比实验效果,将本文方法应用到动画类片段的彩色化上,并取得了较好的效果。图8是对动画题材片段《熊出没》(版权归属:华强方特(深圳)动漫有限公司)的彩色化效果,截取了某一段视频的8幅视频帧,并用本文方法对其进行彩色化,可以看到整体彩色化效果较好地保留了颜色信息,并且实体和背景之间的区分度被很好地展现出来。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909041_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 《熊出没》片段彩色化效果" src="Detail/GetImg?filename=images/JSJY201909041_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 《熊出没》片段彩色化效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909041_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Colorized result of some clip of <i>Boonie Bears</i></p>

                </div>
                <h3 id="114" name="114" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="115">本文将图像着色方法扩展到视频的彩色化上来,并针对现有方法存在的问题,提出了一种结合CNN和LSTM混合神经网络模型的方法对视频序列进行彩色化处理,在保持视频序列帧着色饱和度和着色自然度的前提下,同时采用LSTM结构用作双向循环神经网络的记忆单元,以保证视频序列的长期一致性。实验结果表明,本文方法无论是在单帧视频帧着色还是在视频序列的着色上都优于现有方法,达到了先进的性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="148">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Colorization using neural network ensemble">

                                <b>[1]</b> CHENG Z,YANG Q,SHENG B .Colorization using neural network ensemble [J].IEEE Transactions on Image Processing,2017,26(11):5491-5505.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Large-Scale Automatic Image Colorization">

                                <b>[2]</b> DESHPANDE A,ROCK J,FORSYTH D.Learning large-scale automatic image colorization [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2015:567-575.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMAFFF2D121F4EA546037EFF6E3BD86EAE&amp;v=MDE3NjNIWlowTGVRMDh5eEFUNlRnSVBnbmsyUkZIRGJxU01NdnFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4THE1d0s0PU5pZklZOExPYUtmTzI0NQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> IIZUKA S,SIMO-SERRA E,ISHIKAWA H.Let there be color!:joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification [J].ACM Transactions on Graphics,2016,35(4):Article No.110.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep colorization">

                                <b>[4]</b> CHENG Z,YANG Q,SHENG B.Deep colorization [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2015:415-423.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning representations for automatic colorization">

                                <b>[5]</b> LARSSON G,MAIRE M,SHAKHNAROVICH G.Learning representations for automatic colorization [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9908.Berlin:Springer,2016:577-593.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Colorful Image Colorization">

                                <b>[6]</b> ZHANG R,ISOLA P,EFROS A A.Colorful image colorization [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9907.Berlin:Springer,2016:649-666.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 HOCHREITER S,SCHMIDHUBER J.LSTM can solve hard long time lag problems [C]// Proceedings of the 9th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,1996:473-479.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[8]</b> SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2019- 01- 03].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale video classification with convolutional neural networks">

                                <b>[9]</b> KARPATHY A,TODERICI G,SHETTY S,et al.Large-scale video classification with convolutional neural networks [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2014:1725-1732.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition in video sequences using deep bi-directional LSTM with CNN features">

                                <b>[10]</b> ULLAH A,AHMAD J,MUHAMMAD K,et al.Action recognition in video sequences using deep bi-directional LSTM with CNN features [J].IEEE Access,2018,6:1155-1166.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSTM can solve hard long time lag problems">

                                <b>[11]</b> HOCHREITER S,SCHMIDHUBER J.LSTM can solve hard long time lag problems [C]// Proceedings of the 9th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,1996:473-479.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[12]</b> SHELHAMER E,LONG J,DARRELL T.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation">

                                <b>[13]</b> PERAZZI F,PONT-TUSET J,McWILLIAMS B,et al.A benchmark dataset and evaluation methodology for video object segmentation [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:724-732.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A learning-based approach for automatic image and video colorization">

                                <b>[14]</b> GUPTA R K,CHIA A Y-S,RAJAN D,et al.A learning-based approach for automatic image and video colorization [EB/OL].[2019- 01- 20].https://arxiv.org/pdf/1704.04610.pdf.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance evaluation of noise reduction filters for color images through Normalized Color Difference (NCD) decomposition">

                                <b>[15]</b> RUSSO F.Performance evaluation of noise reduction filters for color images through Normalized Color Difference (NCD) decomposition [J].ISRN Machine Vision,2014,2014:Article No.579658.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909041" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909041&amp;v=MTc3NzNvOUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmIvT0x6N0JkN0c0SDlqTXA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
