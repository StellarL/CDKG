<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136675051721250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906022%26RESULT%3d1%26SIGN%3dxQzqaFPUfn9XTS%252fj4Sxdx2T7s94%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906022&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906022&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906022&amp;v=MjE1MDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyM0pMejdCZDdHNEg5ak1xWTlIWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="1 肺炎图像识别系统架构 ">1 肺炎图像识别系统架构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="1.1 &lt;b&gt;卷积神经网络模型&lt;/b&gt;">1.1 <b>卷积神经网络模型</b></a></li>
                                                <li><a href="#56" data-title="1.2 &lt;b&gt;网络结构设计&lt;/b&gt;">1.2 <b>网络结构设计</b></a></li>
                                                <li><a href="#60" data-title="1.3 GIV3&lt;b&gt;网络&lt;/b&gt;">1.3 GIV3<b>网络</b></a></li>
                                                <li><a href="#78" data-title="1.4 &lt;b&gt;分类识别&lt;/b&gt;">1.4 <b>分类识别</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="2 实验结果及分析 ">2 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#95" data-title="2.1 &lt;b&gt;数据集&lt;/b&gt;">2.1 <b>数据集</b></a></li>
                                                <li><a href="#100" data-title="2.2 &lt;b&gt;实验过程&lt;/b&gt;">2.2 <b>实验过程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="3 实验结果分析 ">3 实验结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同深度神经网络相关参数对比&lt;/b&gt;"><b>表</b>1 <b>不同深度神经网络相关参数对比</b></a></li>
                                                <li><a href="#65" data-title="图1 GIV3网络结构">图1 GIV3网络结构</a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同节点组合以及节点隐藏率实验结果&lt;/b&gt;"><b>表</b>2 <b>不同节点组合以及节点隐藏率实验结果</b></a></li>
                                                <li><a href="#98" data-title="图2 图像实例">图2 图像实例</a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;i&gt;Chest X&lt;/i&gt;-&lt;i&gt;Ray Images&lt;/i&gt;&lt;b&gt;数据集数据分布情况&lt;/b&gt;"><b>表</b>3 <i>Chest X</i>-<i>Ray Images</i><b>数据集数据分布情况</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;特征融合层增加后模型识别准确率的对比&lt;/b&gt;"><b>表</b>4 <b>特征融合层增加后模型识别准确率的对比</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同模型识别效果对比&lt;/b&gt;"><b>表</b>5 <b>不同模型识别效果对比</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;本文模型与现有研究成果对比&lt;/b&gt;"><b>表</b>6 <b>本文模型与现有研究成果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="147">


                                    <a id="bibliography_1" title="RAJPURKAR P, IRVIN J, ZHU K, et al.Che XNet:radiologistlevel pneumonia detection on chest X-rays with deep learning[EB/OL].https://arxiv.org/pdf/1711.05225.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Che XNet:radiologistlevel pneumonia detection on chest X-rays with deep learning">
                                        <b>[1]</b>
                                        RAJPURKAR P, IRVIN J, ZHU K, et al.Che XNet:radiologistlevel pneumonia detection on chest X-rays with deep learning[EB/OL].https://arxiv.org/pdf/1711.05225.pdf.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_2" title="RINALDI P, MENCHINI L, MARTINELLI M, et al.Computer-aided diagnosis[J].Rays, 2003, 28 (1) :103-108." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computer-aided diagnosis">
                                        <b>[2]</b>
                                        RINALDI P, MENCHINI L, MARTINELLI M, et al.Computer-aided diagnosis[J].Rays, 2003, 28 (1) :103-108.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_3" title="ZHAO G, AHONEN T, MATAS J, et al.Rotation-invariant image and video description with local binary pattern features[J].IEEETransactions on Image Processing, 2011, 21 (4) :1465-1477." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rotation-invariant image and video description with local binary pattern features">
                                        <b>[3]</b>
                                        ZHAO G, AHONEN T, MATAS J, et al.Rotation-invariant image and video description with local binary pattern features[J].IEEETransactions on Image Processing, 2011, 21 (4) :1465-1477.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_4" title="吕洪艳, 刘芳.组合核函数SVM在特定领域文本分类中的应用[J].计算机系统应用, 2016, 25 (5) :124-128. (LYU H Y, LIUF.Application of text classification for specific domains based on combination kernel function SVM[J].Computer Systems&amp;amp;Applications, 2016, 25 (5) :124-128.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYY201605022&amp;v=MTQ1MjNTZDdHNEg5Zk1xbzlIWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyM0pQVG4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        吕洪艳, 刘芳.组合核函数SVM在特定领域文本分类中的应用[J].计算机系统应用, 2016, 25 (5) :124-128. (LYU H Y, LIUF.Application of text classification for specific domains based on combination kernel function SVM[J].Computer Systems&amp;amp;Applications, 2016, 25 (5) :124-128.) 
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_5" title="HERSHEY S, CHAUDHURI S, ELLIS D P W, et al.CNN architectures for largescale audio classification[C]//Proceedings of the2017 IEEE International Conference on Acoustic, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:131-135." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cnn architectures for large-scale audio classification">
                                        <b>[5]</b>
                                        HERSHEY S, CHAUDHURI S, ELLIS D P W, et al.CNN architectures for largescale audio classification[C]//Proceedings of the2017 IEEE International Conference on Acoustic, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:131-135.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_6" title="刘长征, 相文波.基于改进卷积神经网络的肺炎影像判别[J].计算机测量与控制, 2017, 25 (4) :185-188. (LIU C Z, XIANGW B.Recognition of pneumonia type based on improved convolution neural network[J].Computer Measurement&amp;amp;Control, 2017, 25 (4) :185-188.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZCK201704051&amp;v=MjY4NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcjNKTHpmSVpiRzRIOWJNcTQ5QVpZUUs=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        刘长征, 相文波.基于改进卷积神经网络的肺炎影像判别[J].计算机测量与控制, 2017, 25 (4) :185-188. (LIU C Z, XIANGW B.Recognition of pneumonia type based on improved convolution neural network[J].Computer Measurement&amp;amp;Control, 2017, 25 (4) :185-188.) 
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_7" title="KERMANY D S, GOLDBAUM M, CAI W J, et al.Identifying medical diagnoses and treatable diseases by image-based deep learning[J].Cell, 2018, 172 (5) :1122-1131." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES50930390119CE9E5141BCCC0599EF667&amp;v=MDc4NDZNUzdqNFBPd3lSckJjOGNNZmlRN3lZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzd5Mndxaz1OaWZPZmJhNEY5TE1ySVpGWmVvR2Z3a3d1aA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        KERMANY D S, GOLDBAUM M, CAI W J, et al.Identifying medical diagnoses and treatable diseases by image-based deep learning[J].Cell, 2018, 172 (5) :1122-1131.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_8" title="VIANNA V P.Study and development of a computer-aided diagnosis system for classification of chest X-ray images using convolutional neural networks pre-trained for Image Net and data augmentation[EB/OL].[2018-09-16].https://arxiv.org/pdf/1806.00839v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Study and development of a computer-aided diagnosis system for classification of chest X-ray images using convolutional neural networks pre-trained for Image Net and data augmentation">
                                        <b>[8]</b>
                                        VIANNA V P.Study and development of a computer-aided diagnosis system for classification of chest X-ray images using convolutional neural networks pre-trained for Image Net and data augmentation[EB/OL].[2018-09-16].https://arxiv.org/pdf/1806.00839v1.pdf.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_9" title="BALLESTER P, ARAUJO R M.On the performance of Goog Le Net and Alex Net applied to sketches[C]//AAAI 2016:Proceedings of the 2016 Thirtieth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:1124-1128." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the performance of Goog Le Net and Alex Net applied to sketches">
                                        <b>[9]</b>
                                        BALLESTER P, ARAUJO R M.On the performance of Goog Le Net and Alex Net applied to sketches[C]//AAAI 2016:Proceedings of the 2016 Thirtieth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:1124-1128.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_10" title="MCDONNELL M D, VLADUSICH T.Enhanced image classification with a fast-learning shallow convolutional neural network[C]//Proceedings of the 2015 International Joint Conference on Neural Networks.Piscataway, NJ:IEEE, 2015:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhanced image classification with a fast-learning shallow convolutional neural network">
                                        <b>[10]</b>
                                        MCDONNELL M D, VLADUSICH T.Enhanced image classification with a fast-learning shallow convolutional neural network[C]//Proceedings of the 2015 International Joint Conference on Neural Networks.Piscataway, NJ:IEEE, 2015:1-7.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_11" title="GAUDART J, GIUSIANO B, HUIART L.Comparison of the performance of multi-layer perceptron and linear regression for epidemiological data[J].Computational Statistics&amp;amp;Data Analysis, 2004, 44 (4) :547-570." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300391817&amp;v=MTg0MjhOaWZPZmJLN0h0RE9ySTlGWitJT0JIMCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMW9kYUJNPQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        GAUDART J, GIUSIANO B, HUIART L.Comparison of the performance of multi-layer perceptron and linear regression for epidemiological data[J].Computational Statistics&amp;amp;Data Analysis, 2004, 44 (4) :547-570.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_12" title="SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[12]</b>
                                        SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_13" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">
                                        <b>[13]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_14" title="HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[14]</b>
                                        HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_15" title="ZHANG C L, LUO J H, WEI X S, et al.In defense of fully connected layers in visual representation transfer[C]//PCM 2017:Proceedings of the 2017 18th Pacific Rim Conference on Multimedia, LNCS 10736.Cham:Springer, 2017:807-817." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In Defense of Fully Connected Layers in Visual Representation Transfer">
                                        <b>[15]</b>
                                        ZHANG C L, LUO J H, WEI X S, et al.In defense of fully connected layers in visual representation transfer[C]//PCM 2017:Proceedings of the 2017 18th Pacific Rim Conference on Multimedia, LNCS 10736.Cham:Springer, 2017:807-817.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_16" title="YUAN J W, YU S C.Privacy preserving back-propagation neural network learning made practical with cloud computing[J].IEEETransactions on Parallel and Distributed Systems, 2014, 25 (1) :212-221." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy preserving back-propagation neural network learning made practical with cloud computing">
                                        <b>[16]</b>
                                        YUAN J W, YU S C.Privacy preserving back-propagation neural network learning made practical with cloud computing[J].IEEETransactions on Parallel and Distributed Systems, 2014, 25 (1) :212-221.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_17" title="MORSE G, STANLEY K O.Simple evolutionary optimization can rival stochastic gradient descent in neural networks[C]//GECCO2016:Proceedings of the 2016 Genetic and Evolutionary Computation Conference.New York:ACM, 2016:477-484." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simple evolutionary optimization can rival stochastic gradient descent in neural networks">
                                        <b>[17]</b>
                                        MORSE G, STANLEY K O.Simple evolutionary optimization can rival stochastic gradient descent in neural networks[C]//GECCO2016:Proceedings of the 2016 Genetic and Evolutionary Computation Conference.New York:ACM, 2016:477-484.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_18" title="ARTSTEIN-AVIDAN S, KNIG H, MILMAN V.The chain rule as a functional equation[J].Journal of Functional Analysis, 2010, 259 (11) :2999-3024." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601201340&amp;v=MjE1NDYzSUoxb2RhQk09TmlmT2ZiSzdIdEROcVk5RVp1c09EM2c1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        ARTSTEIN-AVIDAN S, KNIG H, MILMAN V.The chain rule as a functional equation[J].Journal of Functional Analysis, 2010, 259 (11) :2999-3024.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_19" title="BREIMAN L, LAST M, RICE J.Random forests:finding quasars[M]//FEIGELSON E D, BABU G J.Statistical Challenges in Astronomy.New York:Springer, 2003:243-254." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Random forests:finding quasars">
                                        <b>[19]</b>
                                        BREIMAN L, LAST M, RICE J.Random forests:finding quasars[M]//FEIGELSON E D, BABU G J.Statistical Challenges in Astronomy.New York:Springer, 2003:243-254.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_20" title="PRUSA J D, KHOSHGOFTAAR T M, NAPOLITANO A.Using feature selection in combination with ensemble learning techniques to improve tweet sentiment classification performance[C]//ICTAI2015:Proceedings of the 2015 IEEE 27th International Conference on Tools with Artificial Intelligence.Washington, DC:IEEE Computer Society, 2015:186-193." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using feature selection in combination with ensemble learning techniques to improve tweet sentiment classification performance">
                                        <b>[20]</b>
                                        PRUSA J D, KHOSHGOFTAAR T M, NAPOLITANO A.Using feature selection in combination with ensemble learning techniques to improve tweet sentiment classification performance[C]//ICTAI2015:Proceedings of the 2015 IEEE 27th International Conference on Tools with Artificial Intelligence.Washington, DC:IEEE Computer Society, 2015:186-193.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_21" title="BREIMAN L I, FRIEDMAN J H, OLSHEN R A, et al.Classification and Regression Trees (CART) [J].Encyclopedia of Ecology, 2015, 57 (1) :582-588." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification and Regression Trees">
                                        <b>[21]</b>
                                        BREIMAN L I, FRIEDMAN J H, OLSHEN R A, et al.Classification and Regression Trees (CART) [J].Encyclopedia of Ecology, 2015, 57 (1) :582-588.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_22" title="REN X D, GUO H N, LI S H, et al.A novel image classification method with CNN-XGBoost model[C]//IWDW 2017:Proceedings of the 2017 International Workshop on Digital Watermarking, LNCS 10431.Cham:Springer, 2017:378-390." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel image classification method with CNN-XGBoost model">
                                        <b>[22]</b>
                                        REN X D, GUO H N, LI S H, et al.A novel image classification method with CNN-XGBoost model[C]//IWDW 2017:Proceedings of the 2017 International Workshop on Digital Watermarking, LNCS 10431.Cham:Springer, 2017:378-390.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 09:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1680-1684 DOI:10.11772/j.issn.1001-9081.2018102112            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度神经网络的肺炎图像识别模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E6%96%B0%E5%AE%87&amp;code=41987900&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何新宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%99%93%E9%BE%99&amp;code=09029011&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张晓龙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=1045150&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉科技大学计算机科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=1045150&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉科技大学大数据科学与工程研究院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E4%B8%8E%E5%AE%9E%E6%97%B6%E5%B7%A5%E4%B8%9A%E7%B3%BB%E7%BB%9F%E6%B9%96%E5%8C%97%E7%9C%81%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1045150&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">智能信息处理与实时工业系统湖北省重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>当前的肺炎图像识别算法面临两个问题:一是肺炎特征提取器使用的迁移学习模型在源数据集与肺炎数据集上图像差异较大, 所提取的特征不能很好地契合肺炎图像;二是算法使用的softmax分类器对高维特征处理能力不够强, 在识别准确率上仍有提升的空间。针对这两个问题, 提出了一种基于深度卷积神经网络的肺炎图像识别模型。首先使用ImageNet数据集训练好的GoogLeNet Inception V3网络模型进行特征提取;其次, 增加了特征融合层, 使用随机森林分类器进行分类预测。实验在Chest X-Ray Images肺炎标准数据集上进行。实验结果表明, 该模型的识别准确率、敏感度、特异度的值分别达到96.77%、97.56%、94.26%。在识别准确率以及敏感度指标上, 与经典的GoogLeNet Inception V3+Data Augmentation (GIV+DA) 算法相比, 所提模型分别提高了1.26、1.46个百分点, 在特异度指标上已接近GIV+DA算法的最优结果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%82%BA%E7%82%8E%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">肺炎图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">随机森林;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%8F%E6%84%9F%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">敏感度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BC%82%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特异度;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    何新宇 (1993—) , 男, 湖北咸宁人, 硕士研究生, 主要研究方向:计算机视觉、深度学习;;
                                </span>
                                <span>
                                    *张晓龙 (1963—) , 男, 江西吉安人, 教授, 博士, 主要研究方向:人工智能和机器学习、数据挖掘、生物信息处理。xiaolong. zhang@ wust. edu. cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61273225, 61702381);</span>
                    </p>
            </div>
                    <h1><b>Pneumonia image recognition model based on deep neural network</b></h1>
                    <h2>
                    <span>HE Xinyu</span>
                    <span>ZHANG Xiaolong</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Wuhan University of Science and Technology</span>
                    <span>Institute of Big Data Science and Engineering, Wuhan University of Science and Technology</span>
                    <span>Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Current recognition algorithm of pneumonia image faces two problems. First, the extracted features can not fit the pneumonia image well because the transfer learning model used by the pneumonia feature extractor has large image difference between the source dataset and the pneumonia dataset. Second, the softmax classifier used by the algorithm can not well process high-dimensional features, and there is still room for improvement in recognition accuracy. Aiming at these two problems, a pneumonia image recognition algorithm based on Deep Convolution Neural Network (DCNN) was proposed. Firstly, the GoogLeNet Inception V3 network model trained by ImageNet dataset was used to extract the features. Then, a feature fusion layer was added and random forest classifier was used to classify and forecast. Experiments were implemented on Chest X-Ray Images pneumonia standard dataset. The experimental results show that the recognition accuracy, sensitivity and specificity of the proposed model reach 96.77%, 97.56% and 94.26% respectively. The proposed model is 1.26 percentage points and 1.46 percentage points higher than the classic GoogLeNet Inception V3+Data Augmentation (GIV+DA) algorithm in the index of recognition accuracy and sensitivity, and is close to the optimal result of GIV+DA in the index of specificity.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pneumonia%20image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pneumonia image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=transfer%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">transfer learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Convolution%20Neural%20Network%20(DCNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Convolution Neural Network (DCNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=random%20forest&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">random forest;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sensitivity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sensitivity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=specificity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">specificity;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    HE Xinyu, born in 1993, M. S. candidate. His research interests include computer vision, deep learning. ;
                                </span>
                                <span>
                                    ZHANG Xiaolong, born in 1963, Ph. D. , professor. His research interests include artificial intelligence, machine learning, data mining, biological information processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-19</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61273225, 61702381);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">肺炎是一种常见的肺部疾病。当前, 肺部 X 光检查是肺炎诊断的主要途径, 这种方法在临床护理以及流行病学研究中发挥着重要作用<citation id="191" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。利用X光进行肺炎诊断是一项具有挑战性的工作, 需要医生具备较高的医疗影像判断能力。近年来, 计算机辅助诊断 (Computer-Aided Diagnosis, CAD) 技术<citation id="192" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>作为一种辅助医疗工具, 被广泛应用于很多发达国家的医疗相关领域, 特别是在涉及医疗影像学的领域。借助计算机辅助诊断技术, 实现肺炎图像的识别分类在提高医生识别准确率、减少漏诊误诊、提高工作效率等方面发挥着积极的促进作用。</p>
                </div>
                <div class="p1">
                    <p id="49">随着图像处理、模式识别、以及科学计算可视化技术的不断发展, 在肺炎图像领域国内外学者纷纷展开了研究。上海交通大学李阳等利用图像处理技术将肺炎图像的局部二值模式 (Local Binary Pattern, LBP) <citation id="193" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>特征信息作为识别特征, 然后用支持向量机 (Support Vector Machine, SVM) 算法进行肺炎图像识别。虽然SVM算法对于处理高维度、非线性问题有较大优势, 但是当训练样本较大时, 计算量会呈指数增长, 导致模型最终的效率偏低<citation id="194" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。近年来, 以卷积神经网络 (Convolutional Neural Network, CNN) <citation id="195" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>为代表的深度学习 (Deep Learning) 算法在计算机视觉领域的成功应用, 为CNN在医疗图像处理领域的应用奠定了基础。刘长征等<citation id="196" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>利用卷积神经网络实现了肺炎影像的判别。该方法得到的识别准确率明显高于使用人工提取的特征进行预测分类的SVM算法得到的识别准确率, 这说明卷积神经网络自动提取的肺炎图像特征比人工提取的特征更完善、更有表现力。但是他们所用的卷积神经网络结构相对简单, 学习特征的能力有限, 识别准确率还有提升的空间。在现阶段的图像识别任务中, 往往将大规模图像数据集 (ImageNet) 上训练好的深度卷积神经网络作为其他图像识别模型的特征提取器, 然后在此基础上进行识别分类。Kermany等<citation id="197" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>利用上述方法对肺炎图像进行识别, 识别准确率达到92.80%。相比人工进行特征提取的方法以及刘长征等<citation id="198" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>使用的方法, 该方法得到的识别准确率有5～10个百分点的提升。Vianna等<citation id="199" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用预训练的GoogLeNet Inception V3 网络<citation id="200" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>与Data Augmentation方法相结合的方式对肺炎图像进行识别, 得到的识别准确率比Kermany等<citation id="201" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>得到的识别准确率提高2.71个百分点。</p>
                </div>
                <div class="p1">
                    <p id="50">虽然Kermany等<citation id="202" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>以及Vianna等<citation id="203" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用迁移学习的方法在肺炎图像识别上取得了比已有研究更好的识别效果, 但是, 由于ImageNet数据集与肺炎数据集图像差异较大, 文献<citation id="204" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>]</citation>并没有对已有的迁移学习模型进行相应的改进, 使它更加契合肺炎图像数据集以便得到更高的识别准确率, 因此本文提出了一种改进的基于GoogLeNet Inception V3网络结构的深度神经网络模型, 并将其应用于肺炎图像的识别。</p>
                </div>
                <div class="p1">
                    <p id="51">本文研究的问题是肺炎图像识别, 主要的工作如下:1) 构建了特征融合层, 优选了特征融合层相关参数, 并且将GoogLeNet Inception V3网络与特征融合层进行组合获得了更加多样化的非线性特征表示, 缓解了ImageNet数据集与肺炎数据集图像差异较大的问题;2) 使用随机森林作为分类器, 提升了模型抗干扰以及处理高维特征数据的能力。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">1 肺炎图像识别系统架构</h3>
                <h4 class="anchor-tag" id="53" name="53">1.1 <b>卷积神经网络模型</b></h4>
                <div class="p1">
                    <p id="54">传统的图像识别算法在对图像进行分类时, 需要人工参与进行特征处理, 这样得到的特征模型的泛化能力较差, 识别准确率存在较大的提升空间。</p>
                </div>
                <div class="p1">
                    <p id="55">深度学习出现的目的在于获得比浅层学习<citation id="205" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>更好的识别效果。CNN作为一种经典的深度学习模型, 可以捕捉输入与输出之间复杂的非线性映射关系。CNN是由多层感知机 (Multi-Layer Perceptron, MLP) <citation id="206" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>发展而来, 它以二维或者三维图像直接作为模型的输入, 通过对多个卷积层以及池化层进行堆叠形成具有自主学习能力的层次化结构模型。同时权值共享、局部感知技术的应用使CNN的网络结构与生物神经网络结构更加接近。其中:卷积层的主要作用是对输入图像进行多层次的特征抽取;池化层的作用主要是增加模型的平移、旋转、以及尺度不变性, 在保留主要特征的同时有效地减少参数量, 防止过拟合, 提高模型泛化能力。</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56">1.2 <b>网络结构设计</b></h4>
                <div class="p1">
                    <p id="57">GoogLeNet网络是Szegedy等<citation id="207" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>于2014年提出的, 由一种名为Inception的深度卷积神经网络架构构成。相比AlexNet<citation id="208" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、VGG (Visual Geometry Group) <citation id="209" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等网络, GoogLeNet凭借更大的网络深度 (层数) 和宽度、更高效的资源利用率, 获得了更好的识别效果。</p>
                </div>
                <div class="p1">
                    <p id="58">表1为Resnet50、Inception V3等四种常用深度神经网络Top-1/ Top-5准确率、网络参数量以及网络深度对比情况 (https://keras-cn.readthedocs.io/en/latest/other/application) 。由表1可以看到, Inception V3的网络深度约为VGG网络深度的7倍, 但其参数量仅为VGG网络的1/6, 这得益于Inception V3在网络结构优化上所作的改进。Inception V3模型将每个<i>n</i>×<i>n</i>的卷积层拆成1×<i>n</i>以及<i>n</i>×1的两个卷积层, 这样做使网络深度进一步增加, 同时网络的非线性表达能力也得到增强。在模型的构建过程中, 凭借这些改进可以将更多的计算资源应用于网络结构的调整。本文选取 Inception V3网络作为特征提取器的关键因素在于:1) 通过表1可以发现Inception V3网络训练得到的Top- 1/Top- 5准确率明显高于其他深度神经网络模型;2) 在实际应用中, 参数少的模型在内存受限的场景下具有突出的优势。</p>
                </div>
                <div class="area_img" id="59">
                    <p class="img_tit"><b>表</b>1 <b>不同深度神经网络相关参数对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Comparison of different deep neural networks with relevant parameters</p>
                    <p class="img_note"></p>
                    <table id="59" border="1"><tr><td><br />模型名称</td><td>Top- 1<br />准确率/%</td><td>Top- 5<br />准确率/%</td><td>网络参数量</td><td>网络深度</td></tr><tr><td>VGG16</td><td>71.5</td><td>90.1</td><td>138 357 544</td><td>23</td></tr><tr><td><br />VGG19</td><td>72.7</td><td>91.0</td><td>143 667 240</td><td>26</td></tr><tr><td><br />Resnet50</td><td>75.9</td><td>92.9</td><td>25 636 712</td><td>168</td></tr><tr><td><br />Inception V3</td><td>78.8</td><td>94.4</td><td>23 851 784</td><td>159</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.3 GIV3<b>网络</b></h4>
                <div class="p1">
                    <p id="61">本文将GoogLeNet Inception V3网络和特征融合层组合得到的新网络简称为GIV3网络, 原始的GoogLeNet Inception V3网络简称为GIV网络。</p>
                </div>
                <div class="p1">
                    <p id="62">实验过程中, 由于ImageNet数据集与实验的目标数据集图像差异较大, 如果直接使用GIV网络提取的特征数据作分类, 得到的实验结果会较差<citation id="210" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。为了弱化源数据集与目标数据集图像差异的影响, 本文设计了一个特征融合层。该特征融合层可以对GIV网络学习到的特征信息进行非线性的组合, 得到更加多样化的非线性特征表示。</p>
                </div>
                <div class="p1">
                    <p id="63">图1是本文设计的GIV3网络结构, 它与传统的GIV网络不同之处在于特征融合层的增加。特征融合层包括两个全连接层以及一个Dropout层。</p>
                </div>
                <div class="p1">
                    <p id="64">神经网络的构建过程中, 全连接层节点数相对固定, 一般在1 024、512、256、128、64中进行选择, 而Dropout层节点隐藏率一般是在0.3、0.4、0.5、0.6、0.7中进行选择。本文模型为了得到最优的组合参数, 在实验中利用网格寻优算法对节点数以及节点隐藏率进行随机组合, 然后分别进行数据集测试, 根据最终测试结果选取的节点数组合为1 024与512、Dropout层节点隐藏率为0.5 (不同节点组合以及节点隐藏率的识别准确率对比如表2所示) 。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906022_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 GIV3网络结构" src="Detail/GetImg?filename=images/JSJY201906022_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 GIV3网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906022_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 GIV3 network structure</p>

                </div>
                <div class="area_img" id="66">
                    <p class="img_tit"><b>表</b>2 <b>不同节点组合以及节点隐藏率实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Experimental results of different node number combinations and node hiding rates</p>
                    <p class="img_note"></p>
                    <table id="66" border="1"><tr><td rowspan="2"><br />节点数</td><td colspan="5"><br />节点隐藏率</td></tr><tr><td><br />0.3</td><td>0.4</td><td>0.5</td><td>0.6</td><td>0.7</td></tr><tr><td><br />1 024, 1 024</td><td>94.35</td><td>95.66</td><td>96.64</td><td>94.12</td><td>94.56</td></tr><tr><td><br />1 024, 512</td><td>95.71</td><td>96.46</td><td>96.77</td><td>95.64</td><td>96.58</td></tr><tr><td><br />1 024, 256</td><td>94.56</td><td>95.62</td><td>96.58</td><td>95.84</td><td>95.19</td></tr><tr><td><br />1 024, 128</td><td>95.28</td><td>94.82</td><td>96.34</td><td>96.13</td><td>95.24</td></tr><tr><td><br />1 024, 64</td><td>95.29</td><td>95.19</td><td>96.28</td><td>96.10</td><td>95.82</td></tr><tr><td><br />512, 512</td><td>94.28</td><td>92.43</td><td>95.34</td><td>94.28</td><td>92.17</td></tr><tr><td><br />512, 256</td><td>94.28</td><td>95.18</td><td>96.24</td><td>95.10</td><td>93.56</td></tr><tr><td><br />512, 128</td><td>94.78</td><td>95.12</td><td>94.85</td><td>95.18</td><td>93.55</td></tr><tr><td><br />512, 64</td><td>94.78</td><td>95.18</td><td>95.83</td><td>95.10</td><td>93.45</td></tr><tr><td><br />256, 256</td><td>89.43</td><td>95.48</td><td>96.23</td><td>94.15</td><td>92.46</td></tr><tr><td><br />256, 128</td><td>91.50</td><td>92.53</td><td>95.28</td><td>94.18</td><td>94.18</td></tr><tr><td><br />256, 64</td><td>92.20</td><td>92.49</td><td>94.76</td><td>92.56</td><td>94.48</td></tr><tr><td><br />128, 128</td><td>94.28</td><td>92.46</td><td>95.74</td><td>94.89</td><td>92.10</td></tr><tr><td><br />128, 64</td><td>93.17</td><td>94.15</td><td>95.38</td><td>94.79</td><td>91.38</td></tr><tr><td><br />64, 64</td><td>92.14</td><td>93.20</td><td>94.72</td><td>95.18</td><td>92.57</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="67">在模型的训练过程中, 每一轮迭代结束时使用反向传播 (Back Propagation, BP) 算法<citation id="211" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>对特征融合层的相关参数进行调整。特征融合层中使用的全连接层与传统卷积神经网络里面使用的全连接层一样, 层与层之间采用密集连接的方式。全连接层的计算式如 (1) 式所示:</p>
                </div>
                <div class="p1">
                    <p id="68"><b><i>X</i></b><sub>out</sub>=<i>f</i> (<b><i>W</i></b><b><i>X</i></b><sub>in</sub>+<b><i>b</i></b>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="69">式中:<b><i>X</i></b><sub>out</sub>为全连接层的输出;<b><i>X</i></b><sub>in</sub>为全连接层的输入;<b><i>W</i></b>为训练权值;<b><i>b</i></b>为偏置; <i>f</i> (·) 为ReLU激活函数。相比Sigmoid函数、tanh函数、径向基函数等传统激活函数, ReLU函数处理过的模型能够更好地拟合训练数据, 挖掘相关特征, 同时它的非线性表达能力更强, 这在一定程度上能够减少梯度消失的现象, 并使模型的收敛速度维持在一个稳定的状态。</p>
                </div>
                <div class="p1">
                    <p id="70">GIV3网络使用的损失函数如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>n</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>J</i>为均方误差 (Mean Squared Error, MSE) 损失函数;<i>y</i><sub><i>i</i></sub>实际标签值;<b><i>X</i></b><sub><i>i</i></sub>为节点值; <i>f</i> (·) 为softmax分类函数;<i>n</i>为样本个数。损失函数的作用是通过计算预测值与实际标签值之间的差距程度来衡量模型预测的好坏。本文模型选用MSE作为损失函数的原因在于MSE的梯度会随着损失值的变化而进行同向变化, 这一特性会使模型在训练后期得到更精确的实验结果。特征融合层参数更新公式如式 (3) ～ (4) 所示:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>W</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mi>W</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>-</mo><mi>α</mi><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><mi>W</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>b</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo>=</mo><mi>b</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo>-</mo><mi>α</mi><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><mi>b</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">式中:<i>α</i>为学习率;<i>W</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>为节点间的权值;<i>b</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>为偏置。在模型的训练过程中, 式 (3) ～ (4) 利用梯度下降<citation id="212" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>以及链式求导法则<citation id="213" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>对特征融合层相关参数进行更新。</p>
                </div>
                <div class="p1">
                    <p id="77">通过不断的参数更新以及更加多样化的非线性特征表示, GIV3网络提取的特征能够更好地契合肺炎数据集, 逐渐弱化图像的差异对整个网络结构带来的影响, 以此来提高模型的识别精度。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">1.4 <b>分类识别</b></h4>
                <div class="p1">
                    <p id="79">随机森林 (<i>Random Forest</i>, <i>RF</i>) <citation id="214" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>是利用多棵决策树对样本进行分类预测的集成学习 (<i>Ensemble Learning</i>) 算法<citation id="215" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。<i>RF</i>的基本组成单元是<i>CART</i> (<i>Classification And Regression Tree</i>) <citation id="216" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation> 。首先, 在总样本中有放回地随机抽取<i>m</i>个样本作为CART树的训练集 (<i>m</i> &lt; <i>M</i>, <i>M</i>为总样本数) 。然后, 在特征的选取上也采用有放回的随机取样的方式从总特征中选取<i>k</i>个特征 (<i>k</i> &lt; <i>K</i>, <i>K</i>为总特征数) 。按照上述数据集以及特征的选取规则训练多棵CART。最终, 以投票或者赋予权重的方式对多棵CART的结果进行综合处理得到分类结果。</p>
                </div>
                <div class="p1">
                    <p id="80">本文选取RF作为分类器的原因有:1) 实验提取出的特征维度较高, 而RF在处理高维特征信息时表现出较高的识别准确率。2) 本文使用的GIV3网络结构复杂, 实验数据较少, 容易产生过拟合。凭借随机性的特点, 使用RF作为分类器的模型往往不容易发生过拟合。3) 相比其他集成学习算法, RF得到的实验结果更稳定。</p>
                </div>
                <div class="p1">
                    <p id="81">本文模型整体算法流程如下所示。</p>
                </div>
                <div class="p1">
                    <p id="82">输入 合并后的数据集<i>S</i>;</p>
                </div>
                <div class="p1">
                    <p id="83">输出 RF分类器预测结果。</p>
                </div>
                <div class="p1">
                    <p id="84">1) 按一定的比例随机将<i>S</i>分为相互独立的训练集<i>S</i><sub>1</sub>、验证集<i>V</i><sub>1</sub>、测试集<i>T</i><sub>1</sub>。</p>
                </div>
                <div class="p1">
                    <p id="85">2) 利用训练集<i>S</i><sub>1</sub>对GIV3网络进行参数更新, 并且利用验证集<i>V</i><sub>1</sub>实时评测GIV3网络特征提取能力。</p>
                </div>
                <div class="p1">
                    <p id="86">3) 利用GIV3网络对训练集<i>S</i><sub>1</sub>、验证集<i>V</i><sub>1</sub>以及测试集<i>T</i><sub>1</sub>图像进行特征提取, 得到的数据集图像特征信息分别为<i>X</i><sub>GIV3</sub>、<i>X</i><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>G</mtext><mtext>Ι</mtext><mtext>V</mtext><mn>3</mn></mrow><mn>1</mn></msubsup></mrow></math></mathml>、<i>X</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>G</mtext><mtext>Ι</mtext><mtext>V</mtext><mn>3</mn></mrow><mn>2</mn></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="89">4) 将训练集和验证集图像特征信息<i>X</i><sub>GIV3</sub>、<i>X</i><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>G</mtext><mtext>Ι</mtext><mtext>V</mtext><mn>3</mn></mrow><mn>1</mn></msubsup></mrow></math></mathml>及其标签值作为输入训练RF分类器。</p>
                </div>
                <div class="p1">
                    <p id="91">5) 利用训练好的RF分类器对测试集图像特征信息<i>X</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>G</mtext><mtext>Ι</mtext><mtext>V</mtext><mn>3</mn></mrow><mn>2</mn></msubsup></mrow></math></mathml>进行分类预测。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">2 实验结果及分析</h3>
                <div class="p1">
                    <p id="94">实验在<i>Windows</i> 10系统下进行, <i>CPU</i>为<i>Inter Core i</i>5- 7500<i>CPU</i>@3.41 <i>GHz</i>, 实验环境是基于<i>Windows</i>的<i>TensorFlow</i>机器学习框架, 实验在<i>CPU</i>模式下运行。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">2.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="96">本文采用加州大学圣迭戈分校<i>Kermany</i>等<citation id="217" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>于2018年公开的数据集<i>Chest X</i>-<i>Ray Images</i>。该数据集包括两个独立的肺炎图像数据集:训练集 (5 232幅) 、测试集 (624幅) , 每个数据集包含两种类别的图像:<i>PNEUMONIA</i> (肺炎图片) 和<i>NORMAL</i> (正常肺部图片) 。图2是肺炎图像数据集的相关数据实例展示。</p>
                </div>
                <div class="p1">
                    <p id="97">两种类型的肺部图像在两个数据集中的具体分布情况如表3所示。为保证实验过程中训练集与测试集图像的独立性, 提高模型的泛化能力, 在实验开始前将原始的训练集以及测试集进行合并, 然后按8∶1∶1的比例将合并后的数据集随机分为训练集、测试集和验证集。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906022_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 图像实例" src="Detail/GetImg?filename=images/JSJY201906022_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 图像实例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906022_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Image instances</i></p>

                </div>
                <div class="area_img" id="99">
                    <p class="img_tit"><b>表</b>3 <i>Chest X</i>-<i>Ray Images</i><b>数据集数据分布情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Data distribution of dataset of Chest</i><i>X</i>-<i>Ray Images</i></p>
                    <p class="img_note"></p>
                    <table id="99" border="1"><tr><td><br />数据集</td><td>肺炎图数</td><td>非肺炎图数</td><td>合计</td></tr><tr><td><br />训练集</td><td>3 883</td><td>1 349</td><td>5 232</td></tr><tr><td><br />测试集</td><td>390</td><td>234</td><td>624</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">2.2 <b>实验过程</b></h4>
                <div class="p1">
                    <p id="101">首先, 利用预训练得到的<i>GIV</i>3网络对训练集、验证集、测试集图像进行特征提取;然后, 将训练集和验证集的特征数据及其对应的标签作为输入训练<i>RF</i>分类器;最后, 利用训练好的<i>RF</i>分类器对测试集图像特征数据作分类预测。</p>
                </div>
                <div class="p1">
                    <p id="102">在实验过程中, 为了避免实验结果出现局部最优而不是全局最优的情况, 对学习率采取逐步递减的方式。模型每迭代100次或者连续多次迭代的模型损失函数之间的差值小于某个阈值时, 利用已训练的肺炎图像识别模型对验证集图像作分类预测、查看分类结果, 然后将学习率变为原来的0.9倍。这样做能使模型的实验结果在训练后期不会有太大的波动, 并且更加接近全局最优。</p>
                </div>
                <h3 id="103" name="103" class="anchor-tag">3 实验结果分析</h3>
                <div class="p1">
                    <p id="104">为全面衡量模型的分类性能, 实验使用了3个模型评价指标:准确率 (<i>Accuracy</i>, <i>Acc</i>) 、敏感度 (<i>Sensitivity</i>, <i>Sen</i>) 、特异度 (<i>Specificity</i>, <i>Spe</i>) , 计算式分别如式 (5) 、 (6) 、 (7) 所示。</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>A</mi><mi>c</mi><mi>c</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>S</mi><mi>e</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>S</mi><mi>p</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">其中<i>TP</i>、<i>TN</i>、<i>FP</i>、<i>FN</i>分别表示真阳性、真阴性、假阳性和假阴性。准确率体现的是模型的预测结果与真实检测结果的符合程度, 敏感度体现的是模型识别肺炎图像的能力, 而特异度体现的是模型识别非肺炎图像的能力。</p>
                </div>
                <div class="p1">
                    <p id="107">表4是特征融合层增加后相关模型识别准确率的对比情况。通过对比实验1与2、实验3与4的识别准确率, 可以发现, 特征融合层的增加使模型的识别准确率分别提高了1.03和1.62个百分点, 表明改进之后的特征提取器获得的特征更好地拟合了肺炎图像数据集。同时分别对比实验1与实验3、实验2与实验4, 可以发现, 使用RF作为分类器的模型得到的识别准确率比使用softmax作为分类器的模型得到的识别准确率更高, 表明了RF分类器对高维肺炎特征的处理能力更强。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit"><b>表</b>4 <b>特征融合层增加后模型识别准确率的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Model recognition accuracy comparison after increasing feature fusion layers</p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td>实验序号</td><td>识别方法</td><td>准确率/%</td><td></td><td>实验序号</td><td>识别方法</td><td>准确率/%</td></tr><tr><td>1</td><td>GIV +softmax</td><td>94.31</td><td></td><td>3</td><td>GIV+RF</td><td>95.15</td></tr><tr><td><br />2</td><td>GIV3+softmax</td><td>95.34</td><td></td><td>4</td><td>GIV3+RF</td><td>96.77</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="109">为验证模型的有效性, 选择了Alexnet<citation id="218" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、ResNet18<citation id="219" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、GIV3+softmax、GIV3+SVM、GIV3+Xgboost<citation id="220" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation> 五种模型与本文的GIV3+RF模型作对比, 实验结果如表5所示。从表5可以看出, 在与其他深度神经网络模型的对比中, 本文模型在识别准确率以及特异度两个评价指标上均取得最优, 在敏感度指标上也取得不错的结果, 表明了本文模型的有效性。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit"><b>表</b>5 <b>不同模型识别效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Comparison of recognition effect by different models </p>
                    <p class="img_note"> %</p>
                    <table id="110" border="1"><tr><td><br />识别模型</td><td>准确率</td><td>敏感度</td><td>特异度</td></tr><tr><td><br />Alexnet</td><td>85.37</td><td>98.70</td><td>72.00</td></tr><tr><td><br />ResNet18</td><td>86.38</td><td>98.70</td><td>65.00</td></tr><tr><td><br />GIV3+softmax</td><td>95.34</td><td>99.10</td><td>85.68</td></tr><tr><td><br />GIV3+SVM</td><td>95.96</td><td>97.33</td><td>87.04</td></tr><tr><td><br />GIV3+Xgboost</td><td>96.53</td><td>96.67</td><td>90.61</td></tr><tr><td><br />GIV3+RF</td><td>96.77</td><td>97.56</td><td>94.26</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="111">在Chest X-Ray Images数据集上, 如表6所示, Vianna等<citation id="221" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>采用GIV+DA方法得到的识别准确率以及敏感度分别为95.51%、96.10%, 为目前Chest X-Ray Images数据集最优的识别准确率与敏感度。本文模型在Chest X-Ray Images数据集上的识别准确率以及敏感度为96.77%、97.56%, 相比Vianna<citation id="222" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>得到的结果分别提升1.26、1.46个百分点, 不仅如此, 在特异度指标上也超过或者接近现有较优的研究成果。</p>
                </div>
                <div class="area_img" id="112">
                    <p class="img_tit"><b>表</b>6 <b>本文模型与现有研究成果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 6 Comparisons between the proposed model and existing research results </p>
                    <p class="img_note">%</p>
                    <table id="112" border="1"><tr><td>方法</td><td>准确率</td><td>敏感度</td><td>特异度</td><td></td><td>方法</td><td>准确率</td><td>敏感度</td><td>特异度</td></tr><tr><td>迁移学习<sup>[7]</sup></td><td>92.80</td><td>93.20</td><td>90.10</td><td></td><td>GIV3+RF</td><td>96.77</td><td>97.56</td><td>94.26</td></tr><tr><td><br />GIV+DA<sup>[8] </sup></td><td>95.51</td><td>96.10</td><td>94.50</td><td></td><td></td><td></td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="113" name="113" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="114">本文提出了一种改进的深度卷积神经网络模型, 实现了肺炎图像的识别分类。本文模型在<i>GoogLeNet Inception V</i>3网络的基础上通过构建特征融合层, 使得模型获得了更加多样化的非线性特征表示, 缓解了迁移学习过程中源数据集与目标数据集图像差异大的问题;同时使用<i>Random forest</i>作为模型的分类器, 用于处理高维肺炎图像特征数据。在对比实验中, 本文模型在训练集和测试集上均表现出比其他已有模型更好的识别效果和泛化能力。在后续工作中, 考虑到医疗图像的特殊性, 可利用语义分割技术对图像进行预处理, 再利用深度神经网络对精确分割得到的图像进行分类预测。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="147">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Che XNet:radiologistlevel pneumonia detection on chest X-rays with deep learning">

                                <b>[1]</b>RAJPURKAR P, IRVIN J, ZHU K, et al.Che XNet:radiologistlevel pneumonia detection on chest X-rays with deep learning[EB/OL].https://arxiv.org/pdf/1711.05225.pdf.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computer-aided diagnosis">

                                <b>[2]</b>RINALDI P, MENCHINI L, MARTINELLI M, et al.Computer-aided diagnosis[J].Rays, 2003, 28 (1) :103-108.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rotation-invariant image and video description with local binary pattern features">

                                <b>[3]</b>ZHAO G, AHONEN T, MATAS J, et al.Rotation-invariant image and video description with local binary pattern features[J].IEEETransactions on Image Processing, 2011, 21 (4) :1465-1477.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYY201605022&amp;v=MDc2MTFzRnkvaFdyM0pQVG5TZDdHNEg5Zk1xbzlIWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>吕洪艳, 刘芳.组合核函数SVM在特定领域文本分类中的应用[J].计算机系统应用, 2016, 25 (5) :124-128. (LYU H Y, LIUF.Application of text classification for specific domains based on combination kernel function SVM[J].Computer Systems&amp;Applications, 2016, 25 (5) :124-128.) 
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cnn architectures for large-scale audio classification">

                                <b>[5]</b>HERSHEY S, CHAUDHURI S, ELLIS D P W, et al.CNN architectures for largescale audio classification[C]//Proceedings of the2017 IEEE International Conference on Acoustic, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:131-135.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZCK201704051&amp;v=MDg4NzVoV3IzSkx6ZklaYkc0SDliTXE0OUFaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>刘长征, 相文波.基于改进卷积神经网络的肺炎影像判别[J].计算机测量与控制, 2017, 25 (4) :185-188. (LIU C Z, XIANGW B.Recognition of pneumonia type based on improved convolution neural network[J].Computer Measurement&amp;Control, 2017, 25 (4) :185-188.) 
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES50930390119CE9E5141BCCC0599EF667&amp;v=MTEwNDc0RjlMTXJJWkZaZW9HZndrd3VoTVM3ajRQT3d5UnJCYzhjTWZpUTd5WUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3eTJ3cWs9TmlmT2ZiYQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>KERMANY D S, GOLDBAUM M, CAI W J, et al.Identifying medical diagnoses and treatable diseases by image-based deep learning[J].Cell, 2018, 172 (5) :1122-1131.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Study and development of a computer-aided diagnosis system for classification of chest X-ray images using convolutional neural networks pre-trained for Image Net and data augmentation">

                                <b>[8]</b>VIANNA V P.Study and development of a computer-aided diagnosis system for classification of chest X-ray images using convolutional neural networks pre-trained for Image Net and data augmentation[EB/OL].[2018-09-16].https://arxiv.org/pdf/1806.00839v1.pdf.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the performance of Goog Le Net and Alex Net applied to sketches">

                                <b>[9]</b>BALLESTER P, ARAUJO R M.On the performance of Goog Le Net and Alex Net applied to sketches[C]//AAAI 2016:Proceedings of the 2016 Thirtieth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:1124-1128.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhanced image classification with a fast-learning shallow convolutional neural network">

                                <b>[10]</b>MCDONNELL M D, VLADUSICH T.Enhanced image classification with a fast-learning shallow convolutional neural network[C]//Proceedings of the 2015 International Joint Conference on Neural Networks.Piscataway, NJ:IEEE, 2015:1-7.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300391817&amp;v=MjIzMzNUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjFvZGFCTT1OaWZPZmJLN0h0RE9ySTlGWitJT0JIMCtvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>GAUDART J, GIUSIANO B, HUIART L.Comparison of the performance of multi-layer perceptron and linear regression for epidemiological data[J].Computational Statistics&amp;Data Analysis, 2004, 44 (4) :547-570.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[12]</b>SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">

                                <b>[13]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[14]</b>HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In Defense of Fully Connected Layers in Visual Representation Transfer">

                                <b>[15]</b>ZHANG C L, LUO J H, WEI X S, et al.In defense of fully connected layers in visual representation transfer[C]//PCM 2017:Proceedings of the 2017 18th Pacific Rim Conference on Multimedia, LNCS 10736.Cham:Springer, 2017:807-817.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy preserving back-propagation neural network learning made practical with cloud computing">

                                <b>[16]</b>YUAN J W, YU S C.Privacy preserving back-propagation neural network learning made practical with cloud computing[J].IEEETransactions on Parallel and Distributed Systems, 2014, 25 (1) :212-221.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simple evolutionary optimization can rival stochastic gradient descent in neural networks">

                                <b>[17]</b>MORSE G, STANLEY K O.Simple evolutionary optimization can rival stochastic gradient descent in neural networks[C]//GECCO2016:Proceedings of the 2016 Genetic and Evolutionary Computation Conference.New York:ACM, 2016:477-484.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601201340&amp;v=MDgyMDFycVFUTW53WmVadEZpbmxVcjNJSjFvZGFCTT1OaWZPZmJLN0h0RE5xWTlFWnVzT0QzZzVvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>ARTSTEIN-AVIDAN S, KNIG H, MILMAN V.The chain rule as a functional equation[J].Journal of Functional Analysis, 2010, 259 (11) :2999-3024.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Random forests:finding quasars">

                                <b>[19]</b>BREIMAN L, LAST M, RICE J.Random forests:finding quasars[M]//FEIGELSON E D, BABU G J.Statistical Challenges in Astronomy.New York:Springer, 2003:243-254.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using feature selection in combination with ensemble learning techniques to improve tweet sentiment classification performance">

                                <b>[20]</b>PRUSA J D, KHOSHGOFTAAR T M, NAPOLITANO A.Using feature selection in combination with ensemble learning techniques to improve tweet sentiment classification performance[C]//ICTAI2015:Proceedings of the 2015 IEEE 27th International Conference on Tools with Artificial Intelligence.Washington, DC:IEEE Computer Society, 2015:186-193.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification and Regression Trees">

                                <b>[21]</b>BREIMAN L I, FRIEDMAN J H, OLSHEN R A, et al.Classification and Regression Trees (CART) [J].Encyclopedia of Ecology, 2015, 57 (1) :582-588.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel image classification method with CNN-XGBoost model">

                                <b>[22]</b>REN X D, GUO H N, LI S H, et al.A novel image classification method with CNN-XGBoost model[C]//IWDW 2017:Proceedings of the 2017 International Workshop on Digital Watermarking, LNCS 10431.Cham:Springer, 2017:378-390.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906022" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906022&amp;v=MjE1MDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyM0pMejdCZDdHNEg5ak1xWTlIWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
