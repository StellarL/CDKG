<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136674917502500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906018%26RESULT%3d1%26SIGN%3dIo1YiCZdCZ%252bfR62FrEFBKcj7%252bfY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906018&amp;v=MjY0MjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3IvUEx6N0JkN0c0SDlqTXFZOUViSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#45" data-title="1 卷积LSTM ">1 卷积LSTM</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="1.1 &lt;b&gt;循环神经网络&lt;/b&gt;">1.1 <b>循环神经网络</b></a></li>
                                                <li><a href="#50" data-title="1.2 &lt;b&gt;卷积&lt;/b&gt;LSTM">1.2 <b>卷积</b>LSTM</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="2 深度卷积LSTM ">2 深度卷积LSTM</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="3 编码-解码网络 ">3 编码-解码网络</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="4 仿真实验及分析 ">4 仿真实验及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="4.1 &lt;b&gt;对比实验&lt;/b&gt;">4.1 <b>对比实验</b></a></li>
                                                <li><a href="#89" data-title="4.2 &lt;b&gt;算法通用性实验&lt;/b&gt;">4.2 <b>算法通用性实验</b></a></li>
                                                <li><a href="#96" data-title="4.3 &lt;b&gt;任务通用性实验&lt;/b&gt;">4.3 <b>任务通用性实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#100" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="图1 循环神经网络">图1 循环神经网络</a></li>
                                                <li><a href="#52" data-title="图2 卷积LSTM结构">图2 卷积LSTM结构</a></li>
                                                <li><a href="#59" data-title="图3 深度卷积&lt;i&gt;LSMT&lt;/i&gt;结构">图3 深度卷积<i>LSMT</i>结构</a></li>
                                                <li><a href="#63" data-title="图4 激活函数">图4 激活函数</a></li>
                                                <li><a href="#65" data-title="图5 本文算法流程">图5 本文算法流程</a></li>
                                                <li><a href="#69" data-title="图6 经典网络结构">图6 经典网络结构</a></li>
                                                <li><a href="#71" data-title="图7 编码-解码结构">图7 编码-解码结构</a></li>
                                                <li><a href="#79" data-title="图8 数字运动预测结果">图8 数字运动预测结果</a></li>
                                                <li><a href="#81" data-title="图9 不同算法训练损失下降情况对比">图9 不同算法训练损失下降情况对比</a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;训练损失和结构相似度对比&lt;/b&gt;"><b>表</b>1 <b>训练损失和结构相似度对比</b></a></li>
                                                <li><a href="#91" data-title="图10 &lt;i&gt;Relu&lt;/i&gt;激活函数">图10 <i>Relu</i>激活函数</a></li>
                                                <li><a href="#93" data-title="图11 深度卷积GRU">图11 深度卷积GRU</a></li>
                                                <li><a href="#95" data-title="图12 卷积GRU结果对比">图12 卷积GRU结果对比</a></li>
                                                <li><a href="#98" data-title="图13 降水预测结果对比">图13 降水预测结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="120">


                                    <a id="bibliography_1" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[1]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="122">


                                    <a id="bibliography_2" title="SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-10-15].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[2]</b>
                                        SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-10-15].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="124">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                    SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.</a>
                                </li>
                                <li id="126">


                                    <a id="bibliography_4" title="RYOO M S.Human activity prediction:early recognition of ongoing activities from streaming videos[C]//Proceedings of the 2011IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2011:1036-1043." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human activity prediction:early recognition of ongoing activities from streaming videos">
                                        <b>[4]</b>
                                        RYOO M S.Human activity prediction:early recognition of ongoing activities from streaming videos[C]//Proceedings of the 2011IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2011:1036-1043.
                                    </a>
                                </li>
                                <li id="128">


                                    <a id="bibliography_5" title="ZHU S, JIA Y, PEI M.Parsing video events with goal inference and intent prediction[C]//Proceedings of the 2011 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2011:487-494." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parsing Video Events with Goal Inference and Intent Prediction">
                                        <b>[5]</b>
                                        ZHU S, JIA Y, PEI M.Parsing video events with goal inference and intent prediction[C]//Proceedings of the 2011 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2011:487-494.
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_6" title="VONDRICK C, PIRSIAVASH H, TORRALBA A.Anticipating visual representations from unlabeled video[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:98-106." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anticipating visual representations from unlabeled video">
                                        <b>[6]</b>
                                        VONDRICK C, PIRSIAVASH H, TORRALBA A.Anticipating visual representations from unlabeled video[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:98-106.
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_7" title="KOOIJ J F P, SCHNEIDER N, FLOHR F, et al.Context-based pedestrian path prediction[C]//Proceedings of the 2014 European Conference on Computer Vision, LNCS 8694.Berlin:Springer, 2014:618-633." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contextbased pedestrian path prediction">
                                        <b>[7]</b>
                                        KOOIJ J F P, SCHNEIDER N, FLOHR F, et al.Context-based pedestrian path prediction[C]//Proceedings of the 2014 European Conference on Computer Vision, LNCS 8694.Berlin:Springer, 2014:618-633.
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_8" title="WALKER J, GUPTA A, HEBERT M.Dense optical flow prediction from a static image[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:2443-2451." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dense Optical Flow Prediction from a Static Image">
                                        <b>[8]</b>
                                        WALKER J, GUPTA A, HEBERT M.Dense optical flow prediction from a static image[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:2443-2451.
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_9" title="MOTTAGHI R, RASTEGARI M, GUPTA A, et al.“What happens if…”learning to predict the effect of forces in images[C]//Proceedings of the 2016 European Conference on Computer Vision, LNCS 9908.Berlin:Springer, 2016:269-285." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;What happens if..&amp;quot;learning to predict the effect of forces in images">
                                        <b>[9]</b>
                                        MOTTAGHI R, RASTEGARI M, GUPTA A, et al.“What happens if…”learning to predict the effect of forces in images[C]//Proceedings of the 2016 European Conference on Computer Vision, LNCS 9908.Berlin:Springer, 2016:269-285.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_10" title="HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTQxODVycVFUTW53WmVadEZpbmxVcjNJSjFvZGFoUT1OaWZKWmJLOUh0ak1xbzlGWk9vTERYVXhvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_11" title="ELMAN J L.Distributed representations, simple recurrent networks, and grammatical structure[J].Machine Learning, 1991, 7 (2/3) :195-225." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001338364&amp;v=MDQ4NDJIN1I3cWRaK1p1RmlybFZMdkJJRmc9Tmo3QmFyTzRIdEhOckl4TlorMExZM2s1ekJkaDRqOTlTWHFScnhveGNN&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        ELMAN J L.Distributed representations, simple recurrent networks, and grammatical structure[J].Machine Learning, 1991, 7 (2/3) :195-225.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_12" title="李洋, 董红斌.基于CNN和Bi LSTM网络特征融合的文本情感分析[J].计算机应用, 2018, 38 (11) :3075-3080. (LI Y, DONG H B.Text sentiment analysis based on feature fusion of convolution neural network and bidirectional long short-term memory network[J].Journal of Computer Applications, 2018, 38 (11) :3075-3080.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811006&amp;v=MDAzMzZxQnRHRnJDVVI3cWZadVpzRnkvaFdyL09MejdCZDdHNEg5bk5ybzlGWW9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        李洋, 董红斌.基于CNN和Bi LSTM网络特征融合的文本情感分析[J].计算机应用, 2018, 38 (11) :3075-3080. (LI Y, DONG H B.Text sentiment analysis based on feature fusion of convolution neural network and bidirectional long short-term memory network[J].Journal of Computer Applications, 2018, 38 (11) :3075-3080.) 
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_13" title="姚煜, RYAD C.基于双向长短时记忆联结时序分类和加权有限状态转换器的端到端中文语音识别系统[J].计算机应用, 2018, 38 (9) :2495-2499. (YAO W, RYAD C.End-to-end Chinese speech recognition system based on bidirectional long-term memory-timed timing classification and weighted finite state converter[J].Journal of Computer Applications, 2018, 38 (9) :2495-2499.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201809010&amp;v=MTczNjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyL09MejdCZDdHNEg5bk1wbzlFWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        姚煜, RYAD C.基于双向长短时记忆联结时序分类和加权有限状态转换器的端到端中文语音识别系统[J].计算机应用, 2018, 38 (9) :2495-2499. (YAO W, RYAD C.End-to-end Chinese speech recognition system based on bidirectional long-term memory-timed timing classification and weighted finite state converter[J].Journal of Computer Applications, 2018, 38 (9) :2495-2499.) 
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_14" title="SUTSKEVER I, VINYALS O, LE Q V.Sequence to sequence learning with neural networks[C]//Proceedings of the 2014 Neural Information Processing Systems Conference.Cambridge, MA:MIT Press, 2014:3104-3112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sequence to sequence learning with neural networks">
                                        <b>[14]</b>
                                        SUTSKEVER I, VINYALS O, LE Q V.Sequence to sequence learning with neural networks[C]//Proceedings of the 2014 Neural Information Processing Systems Conference.Cambridge, MA:MIT Press, 2014:3104-3112.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_15" title="BENGIO Y, SIMARD P, FRASCONI P.Learning long-term dependencies with gradient descent is difficult[J].IEEE Transactions on Neural Networks, 1994, 5 (2) :157-166." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning long-term dependencies with gradient descent is difficult">
                                        <b>[15]</b>
                                        BENGIO Y, SIMARD P, FRASCONI P.Learning long-term dependencies with gradient descent is difficult[J].IEEE Transactions on Neural Networks, 1994, 5 (2) :157-166.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_16" title="SHI X J, CHEN Z R, WANG H, et al.Convolutional LSTM network:a machine learning approach for precipitation nowcasting[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:802-810." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional LSTM network:a machine learning approach for precipitation nowcasting">
                                        <b>[16]</b>
                                        SHI X J, CHEN Z R, WANG H, et al.Convolutional LSTM network:a machine learning approach for precipitation nowcasting[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:802-810.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_17" title="MOLLAHOSSEINI A, CHAN D, MAHOOR M H.Going deeper in facial expression recognition using deep neural networks[C]//Proceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision.Piscataway, NJ:IEEE, 2016:1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Going deeper in facial expression recognition using deep neural networks.&amp;quot;">
                                        <b>[17]</b>
                                        MOLLAHOSSEINI A, CHAN D, MAHOOR M H.Going deeper in facial expression recognition using deep neural networks[C]//Proceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision.Piscataway, NJ:IEEE, 2016:1-10.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_18" title="IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning.Cambridge, MA:MIT Press, 2015:448-486." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift">
                                        <b>[18]</b>
                                        IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning.Cambridge, MA:MIT Press, 2015:448-486.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_19" title="LESHNO M, LIN V Y, PINKUS A, et al.Original contribution:multilayer feedforward networks with a nonpolynomial activation function can approximate any function[J].Neural Networks, 1991, 6 (6) :861-867." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Original Contribution: Multilayer feedforward networks with a nonpolynomial activation function can approximate any function">
                                        <b>[19]</b>
                                        LESHNO M, LIN V Y, PINKUS A, et al.Original contribution:multilayer feedforward networks with a nonpolynomial activation function can approximate any function[J].Neural Networks, 1991, 6 (6) :861-867.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-10 07:00</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1657-1662 DOI:10.11772/j.issn.1001-9081.2018122551            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度卷积长短时神经网络的视频帧预测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%BE%B7%E6%AD%A3&amp;code=41987896&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张德正</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BF%81%E7%90%86%E5%9B%BD&amp;code=26558882&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">翁理国</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A4%8F%E6%97%BB&amp;code=26558881&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">夏旻</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E8%BE%89&amp;code=36036143&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹辉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%8B%8F%E7%9C%81%E5%A4%A7%E6%B0%94%E7%8E%AF%E5%A2%83%E4%B8%8E%E8%A3%85%E5%A4%87%E6%8A%80%E6%9C%AF%E5%8D%8F%E5%90%8C%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83(%E5%8D%97%E4%BA%AC%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6)&amp;code=0151773&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江苏省大气环境与装备技术协同创新中心(南京信息工程大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对视频帧预测中难以准确预测空间结构信息细节的问题, 通过对卷积长短时记忆 (LSTM) 神经网络的改进, 提出了一种深度卷积长短时神经网络的方法。首先, 将输入序列图像输入到两个不同通道的深度卷积LSTM网络组成的编码网络中, 由编码网络学习输入序列图像的位置信息变化特征和空间结构信息变化特征;然后, 将学习到的变化特征输入到与编码网络通道数对应的解码网络中, 由解码网络输出预测的下一张图;最后, 将这张图输入回解码网络中, 预测接下来的一张图, 循环预先设定的次后输出全部的预测图。与卷积LSTM神经网络相比, 在Moving-MNIST数据集上的实验中, 相同训练步数下所提方法不仅保留了位置信息预测准确的特点, 而且空间结构信息细节表征能力更强。同时, 将卷积门控循环单元 (GRU) 神经网络的卷积层加深后, 该方法在空间结构信息细节表征上也取得了提升, 检验了该方法思想的通用性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E5%B8%A7%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频帧预测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短时记忆神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BC%96%E7%A0%81%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">编码预测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积门控循环单元;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张德正 (1995—) , 男, 江苏泗阳人, 硕士研究生, 主要研究方向:机器学习、大数据分析;;
                                </span>
                                <span>
                                    翁理国 (1981—) , 男, 江苏南京人, 副教授, 博士, 主要研究方向:机器学习、大数据分析;;
                                </span>
                                <span>
                                    *夏旻 (1983—) , 男, 江苏东台人, 副教授, 博士, 主要研究方向:机器学习、大数据分析;xiamin@ nuist. edu. cn;
                                </span>
                                <span>
                                    曹辉 (1993—) , 男, 江苏淮安人, 硕士研究生, 主要研究方向:机器学习、大数据分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61503192, 61773219);</span>
                                <span>江苏省自然科学基金资助项目 (BK20161533);</span>
                                <span>江苏省青蓝工程;</span>
                    </p>
            </div>
                    <h1><b>Video frame prediction based on deep convolutional long short-term memory neural network</b></h1>
                    <h2>
                    <span>ZHANG Dezheng</span>
                    <span>WENG Liguo</span>
                    <span>XIA Min</span>
                    <span>CAO Hui</span>
            </h2>
                    <h2>
                    <span>Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (Nanjing University of Information Science & Technology)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Concerning the difficulty in accurately predicting the spatial structure information details in video frame prediction, a method of deep convolutional Long Short Term Memory (LSTM) neural network was proposed by the improvement of the convolutional LSTM neural network. Firstly, the input sequence images were input into the coding network composed of two deep convolutional LSTM of different channels, and the position information change features and the spatial structure information change features of the input sequence images were learned by the coding network. Then, the learned change features were input into the decoding network corresponding to the coding network channel, and the next predicted picture was output by the decoding network. Finally, the picture was input back to the decoding network, and the next picture was predicted, and all the predicted pictures were output after the pre-set loop times. In the experiments on Moving-MNIST dataset, compared with the convolutional LSTM neural network, the proposed method preserved the accuracy of position information prediction, and had stronger spatial structure information detail representation ability with the same training steps. With the convolutional layer of the convolutional Gated Recurrent Unit (GRU) deepened, the method improved the details of the spatial structure information, verifying the versatility of the idea of the proposed method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20frame%20prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video frame prediction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Long%20and%20Short-Term%20Memory%20(LSTM)%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Long and Short-Term Memory (LSTM) neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=encoding%20prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">encoding prediction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20Gated%20Recurrent%20Unit%20(GRU)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional Gated Recurrent Unit (GRU) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Dezheng, born in 1995, M. S. candidate. His research interests include machine learning , big data analysis. ;
                                </span>
                                <span>
                                    WENG Liguo, born in 1981, Ph. D. , associate professor. His research interests include machine learning, big data analysis. ;
                                </span>
                                <span>
                                    XIA Min, born in 1983, Ph. D. , associate professor. His research interests include machine learning, big data analysis. ;
                                </span>
                                <span>
                                    CAO Hui, born in 1993, M. S. candidate. His research interests include machine learning, big data analysis.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-26</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61503192, 61773219);</span>
                                <span>the Natural Science Foundation of Jiangsu Province (BK20161533);</span>
                                <span>the Qing Lan Project of Jiangsu Province;</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">在近些年深度学习快速发展的背景下, 多种以前难以很好解决的计算机视觉问题有了解决方案。视频帧预测作为计算机视觉的难点问题, 长期以来得不到很好的解决, 也难以引起人们的重视, 但是随着无人驾驶技术的爆发式发展, 对其的需求越来越迫切。原因是:研究人员看重视频帧预测在无人驾驶过程中预测路面行人和车辆移动轨迹的能力。传统的方法停留在车载雷达实时检测车辆前方是否有障碍物, 但不能预测<i>t</i>时刻后将与另一个移动物体路径交汇发生碰撞, 也就无法作出需要提前减速或者加速的判断。这一能力对于危险情况有提前预防的作用, 能够显著增强无人驾驶的安全性能。此外, 视频帧预测在夏季短时强降水的预测任务中也可以应用。通过前一段时间雷达回波图预测下一段时间雷达回波图的可能情况, 从而判断接下来那些地方可能有强降水发生。视频帧预测也为夏季多发的台风轨迹预测提供了一种方法。由于我国沿海大部分是季风气候, 所以夏季东南沿海地区经常受到台风的侵扰, 良好地预测台风轨迹, 对防灾减灾有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="43">目前, 已经被很好解决的识别、分类和目标检测等问题的数据集都是静态的图片, 即便是同一类别的两张图也不存在时空序列相关性。而视频帧预测问题的前后两帧甚至是间隔数帧依旧存在着很强的时空序列相关性。对于这类时空序列问题, 近年来火热的基于卷积神经网络 (Convolutional Neural Network, CNN) 的AlexNet<citation id="158" type="reference"><link href="120" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、VGG (Visual Geometry Group) <citation id="159" type="reference"><link href="122" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、GoogLeNet<citation id="160" type="reference"><link href="124" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等算法都无法解决。主要原因是以上三种算法代表的卷积神经网络的优点是对结构表征能力很强, 缺点是局限于静态图, 无法建模动态图的时空序列问题。如何建模时空序列问题, 是一个比较基础也比较重要的任务。起初研究人员注意力主要集中在人类动作预测方向, 使用的方法大多基于统计学习和传统的机器学习。Ryoo<citation id="161" type="reference"><link href="126" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>把动作预测问题概率化, 使用时空特征积分直方图来建模特征分布与时间的变化关系。Zhu等<citation id="162" type="reference"><link href="128" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>应用条件随机场提出了感知语法事件解析、推断事件目标和预测可信动作的算法。Vondrick等<citation id="163" type="reference"><link href="130" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>利用一种深度回归网络的方法来学习视频表征, 结合动作识别模型, 能够很好地根据静态图像来推测未来动作。除此之外, 研究人员还进行了物体轨迹预测方向的研究。Kooij等<citation id="164" type="reference"><link href="132" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>基于贝叶斯算法提出了动态贝叶斯网络, 应用于行人路径预测。Walker等<citation id="165" type="reference"><link href="134" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的光流预测模型对静态图中的所有像素点进行光流标记, 可以预测每一个像素的运动。Mottaghi等<citation id="166" type="reference"><link href="136" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>使用两个CNN和一个循环神经网络 (Recurrent Neural Network, RNN) 来建模物体移动动态, 其结果表明可以从单个图像中预测出物体的长期运动作为外力的反映。以上的运动预测模型多是从建模移动物体的运动轨迹出发, 能较好地预测物体的瞬时运动轨迹, 但具有两个缺点:一是不能预测多帧后物体准确位置;二是不能表征多帧后物体的结构信息。</p>
                </div>
                <div class="p1">
                    <p id="44">针对这两个难解决的问题, 本文在卷积长短时神经网络 (Long and Short-Term Memory neural network, LSTM) <sup></sup><citation id="167" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>基础上提出了深度卷积长短时神经网络模型, 基于Moving-MNIST数据集的实验表明, 在多帧后的物体位置预测和物体结构信息保留两方面取得了更优的效果。</p>
                </div>
                <h3 id="45" name="45" class="anchor-tag">1 卷积LSTM</h3>
                <h4 class="anchor-tag" id="46" name="46">1.1 <b>循环神经网络</b></h4>
                <div class="p1">
                    <p id="47">循环神经网络<citation id="168" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的提出主要是为了解决自然语言处理领域的词语长期依赖问题。该问题将一条语句甚至是段落转化成词向量, 而语句中不同位置上的词语存在高度相关性, 体现在词向量上就是不同元素间存在相关性, 这就是序列问题。循环神经网络结构如图1所示。在左边部分中, <b><i>x</i></b>是神经网络的输入;<b><i>U</i></b>是输入层到隐藏层之间的权重矩阵;<b><i>w</i></b>是记忆单元到隐藏层之间的权重矩阵;<b><i>V</i></b>是隐藏层到输出层之间的权重矩阵;<b><i>s</i></b>是隐藏层的输出, 同时也是要保存到记忆单元中, 并与下一时刻的<b><i>x</i></b>一起作为输入;<b><i>O</i></b>是神经网络的输出。图的右边是展开结构。循环神经网络接受多个独立的输入, 并且最终输出多个结果。将循环神经网络的结构与一般的全连接神经网络比较, 会发现循环神经网络只是多了一个记忆单元<b><i>s</i></b>, 而这个记忆单元就是循环神经网络的关键所在。<i>t</i>时刻的神经元接受<i>t</i>时刻的输入<b><i>x</i></b><sub><i>t</i></sub>, 并且在给出输出之前参考上一时刻的记忆单元<b><i>s</i></b><sub><i>t</i>-1</sub>, 同样的<i>t</i>时刻的神经元也会留下该时刻的记忆单元<b><i>s</i></b><sub><i>t</i></sub>, 用于下一个时刻的输出参考。实际的效果就是将不同时刻状态联系到一起, 建模了时间的相关性。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 循环神经网络" src="Detail/GetImg?filename=images/JSJY201906018_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 循环神经网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Recurrent neural network</p>

                </div>
                <div class="p1">
                    <p id="49">在后来的神经网络发展中, 为了解决一些实际的问题, 例如:文本情感分析<citation id="169" type="reference"><link href="142" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、语音识别等<citation id="170" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 研究人员提出了基于原始循环神经网络的变体, 例如为了解决多输入单分类的问题提出多输入单输出的循环神经网络。原始的循环神经网络还要求输入与输出长度对应, 但在机器翻译中源语言与目标语言的句子往往并没有相同的长度, 这时可采用输入与输出不等长 (<i>N</i> vs <i>M</i>) 模型, 这种结构又称为“编码-解码” (Encoder-Decoder) 模型, 也称序列到序列 (Seq2Seq) 模型<citation id="171" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 是从自编码器发展而来的。在诸多变种中, 长短时神经网络 (LSTM) 模型由于能更好地处理长期依赖和训练时的梯度爆炸问题<citation id="172" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 所以经常被使用。</p>
                </div>
                <h4 class="anchor-tag" id="50" name="50">1.2 <b>卷积</b>LSTM</h4>
                <div class="p1">
                    <p id="51">卷积<i>LSTM</i>的提出是为了解决临近降水预测问题, 该问题的数据是雷达回波序列图, 有很强的时空相关特性。<i>Shi</i>等<citation id="173" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>在结合了<i>LSTM</i>的序列处理能力和<i>CNN</i>的空间特征表达能力后, 提出了能够解决时空序列预测问题的卷积<i>LSTM</i>模型。与各种循环神经网络不同的是:通过对输入序列图像使用卷积操作获取图像特征, 在循环神经网络应用于翻译等任务时获取的都是一维的词向量输入;而这里获取的是二维的图像输入, 也可以根据任务不同输入三通道的彩色图像, 这时就变成了三维输入。在视频帧预测任务中, 将单通道的64×64的数字序列图像作为输入。如图2所示, 卷积<i>LSTM</i>模型与<i>LSTM</i>模型具有同样的三个门控制单元和一个隐藏层, 分别是:输入门<b><i>i</i></b><sub><i>t</i></sub>、遗忘门<b><i>f</i></b><sub><i>t</i></sub>、输出门<b><i>o</i></b><sub><i>t</i></sub>和隐藏层<b><i>h</i></b><sub><i>t</i></sub>。最大的不同是在当前时刻的输入与隐层结合后进行了单层卷积计算, 这个不同点是提取空间结构信息的关键。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 卷积LSTM结构" src="Detail/GetImg?filename=images/JSJY201906018_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 卷积LSTM结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Convolutional LSTM structure</p>

                </div>
                <div class="p1">
                    <p id="53">该算法在雷达降水回波图上的预测结果明显优于全连接LSTM网络。在继承全连接LSTM算法强大的时间相关性处理能力的基础上, 通过增加一层卷积结构解决了原算法的空间数据冗余的问题, 实现了空间结构的表征能力。ConvLSTM通过其本地邻居的输入和过去状态来确定网格中某个单元的未来状态。这可以通过在状态到状态和输入到状态转换中使用卷积运算符来轻松实现。ConvLSTM的关键方程如下面的式 (1) 所示:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>δ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>i</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>i</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>δ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>f</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>f</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>˚</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>˚</mo><mi>tanh</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>c</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>c</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">o</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>δ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>o</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>o</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>c</mi><mi>o</mi></mrow></msub><mo>˚</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>o</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">o</mi><msub><mrow></mrow><mi>t</mi></msub><mo>˚</mo><mi>tanh</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">其中:“*”表示卷积运算符;“。”表示点积;<i>δ</i>表示sigmoid激活函数。</p>
                </div>
                <div class="p1">
                    <p id="56">该算法实现了预定的时空序列预测问题, 但对于空间结构信息的表征上还存在明显的模糊问题。根据其算法给出的预测结果来看, 在Moving-MNIST数据集上仅前两张图像的结构信息保存完整, 细节清晰, 能够辨认出具体数字;五张到十张仅能预测到位置信息, 结构信息完全模糊。为了增强算法对空间结构信息的表征能力, 本文提出了深度卷积LSTM模型。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">2 深度卷积LSTM</h3>
                <div class="p1">
                    <p id="58">在深层卷积可以提取更抽象特征的思想启发下, 本文提出了深度卷积<i>LSTM</i>模型, 卷积层的具体分布上不是简单地加深而是参考了<i>GoogLeNet</i>中让网络变宽的方法<citation id="174" type="reference"><link href="152" rel="bibliography" /><link href="154" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>, 目的是解决普通卷积<i>LSTM</i>在预测多帧时结构信息模糊的问题。具体操作是将输入与隐层结合后的单层卷积加深, 并且将不同层的卷积结果传递给不同的门控单元, 卷积层之间用<i>relu</i>函数激活, 结构如图3所示。</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 深度卷积LSMT结构" src="Detail/GetImg?filename=images/JSJY201906018_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 深度卷积<i>LSMT</i>结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Deep convolutional LSTM structure</i></p>

                </div>
                <div class="p1">
                    <p id="60">上一时刻的输出与这一时刻的输入拼接后送入卷积网络中, 这里的拼接操作体现了前一时刻与下一时刻的相关性, 为了实现这个操作要求上一时刻的输出图像长和宽保持与输入图像一致, 在<i>Moving</i>-<i>MNIST</i>数据集中长和宽始终保持64×64不变, 但通道数随着卷积核数量的变化而变化, 将第一层的结果传递给遗忘门由遗忘门的特征矩阵决定哪些特征是可以保留在记忆细胞中的;再经过<i>relu</i>激活后, 进入第二和第三层的卷积得到<i>t</i>时刻的输入与<i>t</i>-1时刻输出结合后的深度特征作为<i>t</i>时刻的输出, 同样卷积输出结果需要与下一时刻的维度保持一致, 将结果传递给输出层;最后的输入层决定了向记忆细胞中写入当前时刻的输入的图像特征信息, 为了学习到更深层的特征再经过两层卷积得到。深度卷积LSTM在门选择上继承了LSTM的门控设置, 保证了算法在长期依赖方面的有效性。</p>
                </div>
                <div class="p1">
                    <p id="61">在实验过程中发现, 遗忘门进行的操作是二值问题, 仅需要判断是否需要遗忘图像的某部分信息, 这是一个判断0还是1的二值问题, 所以只需要单层卷积来学习特征, 并且选用能将输出限制在0～1的sigmoid函数激活;与此不同的是输出门要求输出图像的具体空间结构信息, 为了保证输出的图像能够辨别就需要模型学习到更深层的细节, 所以将第三层的卷积结果用来输出;最难的部分是输入门的特征学习, 输入门根据当前时刻输入和上一时刻状态将这时刻状态更新, 需要用更多卷积层学习到更深层特征。最终将本层的细胞状态送入tanh函数激活后与经过sigmoid函数激活的输出门状态点积得到当前时刻的输出。</p>
                </div>
                <div class="p1">
                    <p id="62">深度卷积LSTM在门控激活函数的选择上与LSTM算法一致。遗忘门、输入门和输出门都使用sigmoid函数激活, 其性质如图4 (a) 所示:sigmoid函数的输出在0～1;当前细胞状态是用tanh函数激活, 如图4 (b) 所示:tanh函数的输出值为-1～1。激活函数的使用是为了增加非线性映射, 如果不使用激活函数, 那么无论有多少隐藏层, 它们之间都是线性关系, 网络的逼近能力有限<citation id="175" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 激活函数" src="Detail/GetImg?filename=images/JSJY201906018_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 激活函数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Activation function</p>

                </div>
                <div class="p1">
                    <p id="64">所提算法实现的整体流程如图5所示。首先, 将前10张序列图像输入深度卷积LSTM块中学习每张图像特征和前后图像的相关性, 记忆细胞负责跨图片传递特征;其次, 将最后的输出结合记忆细胞预测下一张图像的特征信息, 循环输出10次获得全部的预测特征图像;然后, 通过额外的卷积操作将特征图像还原到一通道的灰度图;最后, 预测图和真实图对比计算损失值, 利用梯度下降更新特征权重值。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 本文算法流程" src="Detail/GetImg?filename=images/JSJY201906018_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 本文算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Flow chart of the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="66">在训练完成后的测试中, 只需要加载已经训练好的模型。预测程序结构与训练结构的不同在于:预测时不需要进行损失的计算和权重的更新, 到“预测输出”这一步停止。所提算法从训练到预测的网络结构参考了经典的“编码-解码”网络结构。</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag">3 编码-解码网络</h3>
                <div class="p1">
                    <p id="68">采用与卷积<i>LSTM</i>一样的网络结构, 这种先编码后解码的网络结构在循环神经网络中经常被使用, 尤其是文本处理和机器翻译等领域。经典的结构如图6所示:<b><i>x</i></b><sub>1</sub>～<b><i>x</i></b><sub>3</sub>的输入先经过编码结构的编码, 将编码结果存储在记忆模块中;然后解码模块依据记忆模块中的内容, 解码出<b><i>y</i></b><sub>1</sub>～<b><i>y</i></b><sub>3</sub>的输出结果。</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_069.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 经典网络结构" src="Detail/GetImg?filename=images/JSJY201906018_069.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 经典网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_069.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Classical network structure</p>

                </div>
                <div class="p1">
                    <p id="70">在时空序列预测问题中, 采用了改进的如图7所示的编码-解码网络结构, 图的上边两个深度卷积LSTM块组成编码网络, 下边两个深度卷积LSTM块组成解码网络。编码网络由两个通道数不同的深度卷积LSTM堆叠成, 解码网络同样由两个通道数不同的卷积LSTM堆叠成。为了保证维度匹配, 对应上下两个编码解码网络通道必须一致, 目的是能够将编码网络学到的特征传递给解码网络。首先, 将前10张序列图依次输入串联两个编码器的网络来学习序列特征, 这里主要是数字运动特征和数字结构特点;然后由串联两个解码器的网络使用编码网络学到的特征作出下一帧的预测, 此时预测出的是经过解码器加工的多通道的图像矩阵, 并不是和输入一样单通道灰度图像, 所以要在这里加上三层卷积, 将预测结果还原为和输入一样格式的图像, 需要预测再下一帧时只需循环输入上一帧的图像矩阵, 经过10次循环输出全部的10张预测的序列图。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 编码-解码结构" src="Detail/GetImg?filename=images/JSJY201906018_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 编码-解码结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Encode-decode structure</p>

                </div>
                <h3 id="72" name="72" class="anchor-tag">4 仿真实验及分析</h3>
                <h4 class="anchor-tag" id="73" name="73">4.1 <b>对比实验</b></h4>
                <div class="p1">
                    <p id="74">为了进一步对深度卷积<i>LSTM</i>模型有直观了解, 在<i>Moving</i>-<i>MNIST</i>数据集上比较改进的深度卷积<i>LSTM</i>网络与普通卷积<i>LSTM</i>网络。在测试了添加不同卷积层后, 得到如下结论:</p>
                </div>
                <div class="p1">
                    <p id="75">1) 深度卷积<i>LSTM</i>在处理时空相关方面优于卷积<i>LSTM</i>。</p>
                </div>
                <div class="p1">
                    <p id="76">2) 深度卷积<i>LSTM</i>对图片结构信息表征能力更强, 细节信息可以传递到更深层预测。</p>
                </div>
                <div class="p1">
                    <p id="77">对比用的两个网络都借用<i>tensorflow</i>框架, 使用<i>python</i>语言编写, 运行在单块<i>NVIDIA TITAN X</i>显卡上。具体操作:将前10帧图像作为输入, 通过使用反向传播和最小化均方差损失来训练模型, 学习率设置为10<i>E</i>-3。首先, 使用普通卷积<i>LSMT</i>算法在训练后预测数字9和8的运动情况, 结果如图8 (<i>a</i>) 所示;再与原算法对比使用改进的深度卷积<i>LSTM</i>网络预测构造比较复杂的9和8两个数字的运动图, 结果如图8 (<i>b</i>) 所示。</p>
                </div>
                <div class="p1">
                    <p id="78">通过观察预测的结果图8可以发现, 单层卷积<i>LSTM</i>网络在预测时的表现很差, 只有前边两帧图像还能分辨出是数字9和8, 到最后一帧图片结构信息完全丢失只剩模糊的像素点;反观深度卷积<i>LSMT</i>网络的预测结果, 前边的七帧都能较好地保持图像的结构信息, 能分辨出是数字9和8, 最后一帧的结构信息也很模糊但还是有一些轮廓信息。也可以发现, 虽然原算法预测出的图像结构信息较差, 但在位置信息预测上比较准确, 这个优点在改进的深度卷积<i>LSTM</i>上依然得到了保留, 对于位置的预测还是一样地准确。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 数字运动预测结果" src="Detail/GetImg?filename=images/JSJY201906018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 数字运动预测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Digital motion prediction results</i></p>

                </div>
                <div class="p1">
                    <p id="80">从训练过程中的损失下降来看, 深度卷积<i>LSTM</i>也是优于普通卷积<i>LSTM</i>的, 结果对比如图9所示。图9中, 横坐标是训练步数, 从训练的10万步中每500步采样一次得出。从图9中可以很明显地看出, 两个算法在1万步训练之前相差不大, 但之后随着训练次数的增加, 深度卷积<i>LSTM</i>算法在预测损失值明显低于原算法。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同算法训练损失下降情况对比" src="Detail/GetImg?filename=images/JSJY201906018_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同算法训练损失下降情况对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 9 <i>Comparison of training loss reduction of different algorithms</i></p>

                </div>
                <div class="p1">
                    <p id="82">为了更准确地看出深度卷积<i>LSTM</i>相较于原算法的提升, 于是对这10万步采样的损失值求平均, 结果如表1所示, 卷积<i>LSTM</i>算法10万步的平均损失值约为2 030, 深度卷积<i>LSTM</i>算法10万步的平均损失值约为1 694, 训练损失值降低约15%, 对比看出, 深度卷积<i>LSTM</i>的在训练损失方面提升较为明显。此外还通过结构相似性 (<i>Structural SIMilarity index</i>, <i>SSIM</i>) 指标对比改进的深度卷积<i>LSTM</i>算法的表现。<i>SSIM</i>是一个衡量两张图片相似程度的指标, 主要由三个部分组成:亮度、对比度和结构。定义两个图像<i>x</i>和<i>y</i>, 两张图的结构相似度可以由式 (2) 求出:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>S</mi><mi>Ι</mi><mi>Μ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>2</mn><mi>μ</mi><msub><mrow></mrow><mi>x</mi></msub><mspace width="0.25em" /><mi>μ</mi><msub><mrow></mrow><mi>y</mi></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mn>2</mn><mi>σ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false"> (</mo><mi>μ</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>σ</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中:<i>μ</i><sub><i>x</i></sub>是图像<i>x</i>的平均值;<i>μ</i><sub><i>y</i></sub>是图像<i>y</i>的平均值;<i>σ</i><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup></mrow></math></mathml>是图像<i>x</i>的方差;<i>σ</i><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup></mrow></math></mathml>是图像<i>y</i>的方差;<i>σ</i><sub><i>xy</i></sub>是图像<i>x</i>和<i>y</i>的协方差;<i>c</i><sub>1</sub>= (<i>k</i><sub>1</sub><i>L</i>) <sup>2</sup>, <i>c</i><sub>2</sub>= (<i>k</i><sub>2</sub><i>L</i>) <sup>2</sup>是用来维持稳定的常数, <i>L</i>是像素值的动态范围, 一般取255, <i>k</i><sub>1</sub>=0.01, <i>k</i><sub>2</sub>=0.03。结构相似度的数值范围是0～1, 当数值为0代表两张图相似度几乎没有, 数值接近1时说明两张图片的结构接近相同。</p>
                </div>
                <div class="p1">
                    <p id="87">在卷积LSTM算法实验中, 利用训练好的模型检测了测试集中前100个序列的图像, 结果如表1所示, 预测出的序列图与真实序列图之间的SSIM平均值为0.48;在改进的深度卷积LSTM算法实验中, 同样的测试集得到的SSIM平均值为0.55, 相较于没有改进前提升了14.58%。取得这样的结果主要是因为采用了更深的卷积层, 并且让不同的卷积层承担不一样的任务:第一层卷积用来协助遗忘门判断是否遗忘上一时刻的某些东西;第三层卷积后的特征图, 协助输出门输出当前时刻信息;第五层卷积得到的特征图信息更抽象, 对于需要判断哪些信息存储到记忆中很必要。</p>
                </div>
                <div class="area_img" id="88">
                    <p class="img_tit"><b>表</b>1 <b>训练损失和结构相似度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Comparison of training loss and structural similarity</p>
                    <p class="img_note"></p>
                    <table id="88" border="1"><tr><td><br />算法</td><td>平均损失</td><td>结构相似度</td></tr><tr><td><br />卷积LSTM</td><td>2 030.27</td><td>0.48</td></tr><tr><td><br />深度卷积LSTM</td><td>1 694.59</td><td>0.55</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">4.2 <b>算法通用性实验</b></h4>
                <div class="p1">
                    <p id="90">进一步讨论加深卷积层的算法思想是否可以拓展到其他相关的序列预测算法上, 实现性能的提升。为此, 选择同样是由循环神经网络演变来的卷积门控循环单元 (<i>Gated Recurrent Unit</i>, <i>GRU</i>) 算法进行验证。<i>GRU</i>是新一代的循环神经网络, 结构上去掉了细胞状态, 使用隐藏状态来进行信息的传递, 整体和<i>LSTM</i>非常相似。改进思路与深度卷积<i>LSTM</i>一样, 将卷积<i>GRU</i>的一层卷积加深为五层, 卷积层之间用如图10所示的<i>relu</i>激活函数, 有三点好处:1) 防止梯度爆炸。2) 计算速度快, 只需要判断是否大于0。3) 收敛速度大于<i>sigmoid</i>和<i>tanh</i>。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 Relu激活函数" src="Detail/GetImg?filename=images/JSJY201906018_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 <i>Relu</i>激活函数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.10 <i>Relu activation function</i></p>

                </div>
                <div class="p1">
                    <p id="92">深度卷积<i>GRU</i>结构如图11所示, <i>GRU</i>只有两个门控单元:一个是更新门 (<b><i>z</i></b><sub><i>t</i></sub>) 用于控制前一时刻的状态信息被代入到当前状态中的程度, 更新门的值越大说明前一时刻的状态信息代入越多;另一个是重置门 (<b><i>r</i></b><sub><i>t</i></sub>) 用于控制忽略前一时刻的状态信息的程度, 重置门的值越小说明忽略得越多。相对于卷积LSTM, 卷积GRU由于只有两个门控单元, 所以参数较少且训练速度明显加快;但这个优点在视频帧预测这个任务上被缺点覆盖, 原因在于图像的特征较为复杂, 少量的参数难以准确表征, 从它的实验结果图与深度卷积LSTM对比来看, 图像细节特征模糊得相对严重。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 深度卷积GRU" src="Detail/GetImg?filename=images/JSJY201906018_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 深度卷积GRU  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Deep conventional GRU</p>

                </div>
                <div class="p1">
                    <p id="94">图12展示了效果的对比情况。从图12结果不难看出, 一层卷积GRU的输出明显模糊, 数字9仅可以保留到第3帧, 数字8仅可以保留到第2帧;加深到五层的卷积GRU网络明显有更加好地保留结构细节, 数字9可以保留到第5帧。但是与深度卷积LSTM结果相比, 加深的卷积GRU网络输出的图像清晰度较低, 主要原因应该是缺少遗忘门, 模糊的图像中如果能忘记更多不必要的像素点, 那么结构信息细节将会更清楚。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 卷积GRU结果对比" src="Detail/GetImg?filename=images/JSJY201906018_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 卷积GRU结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.12 Comparison of conventional GRU results</p>

                </div>
                <h4 class="anchor-tag" id="96" name="96">4.3 <b>任务通用性实验</b></h4>
                <div class="p1">
                    <p id="97">为了测试算法在不同类型图像上的表现, 选取了阿里巴巴天池大数据比赛上的部分珠三角地区降水雷达回波数据集。结合深度卷积<i>LSMT</i>网络的要求, 对数据集中的61张降水回波图按顺序间隔两张挑出一张, 做出了输入10张、输出10张的序列图。经过训练后测试结果如图13所示。从预测结果图13来看, 深度卷积<i>LSMT</i>对深黑色的降水区域预测更准确, 重合更大, 尤其从前边的三张预测图对比看出:原来的卷积<i>LSMT</i>网络预测图的左下角黑色降水区域偏差较大。原因还是因为卷积<i>LSMT</i>的单层卷积在训练时对图像的深层特征学习不够, 相反深度卷积<i>LSMT</i>的五层卷积更能挖掘到图像变化的深层特征。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906018_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 降水预测结果对比" src="Detail/GetImg?filename=images/JSJY201906018_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 降水预测结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906018_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>
                                <p class="img_note"><i>Fig</i>, 13 <i>Comparison of precipitation prediction results</i></p>

                </div>
                <div class="p1">
                    <p id="99">同样地在测试集上对比深度卷积<i>LSMT</i>网络和卷积<i>LSMT</i>网络的结构相似度。深度卷积<i>LSTM</i>网络在100个序列的测试集上取得了0.68的平均结构相似度, 卷积<i>LSMT</i>网络的结构相似度为0.62, 结构相似度提升了约9.7%。对比两个数据集结果发现, 不论是否改进, 算法整体的结构相似度都比<i>Moving</i>-<i>MNIST</i>数据集表现好;区别在于雷达回波数据集中的图像中雨云的结构不是固定的, 而<i>Moving</i>-<i>MNIST</i>数据集中的移动数字结构是固定的, 所以固定的结构预测难度更大。此外, 在<i>Moving</i>-<i>MNIST</i>数据集上深度卷积<i>LSMT</i>网络在结构相似度上提升了14.58%, 而雷达回波图上才提升了9.7%。这和雨云的运动高度相关, 数字图仅仅是有方向性的运动, 而雨云不仅运动还可能降水消失, 具有突变性, 所以提升难度更大。综合表现来看, 深度卷积<i>LSMT</i>对图像的空间结构信息预测更准确, 但在图片信息出现突变的情况中提升效果会下降。</p>
                </div>
                <h3 id="100" name="100" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="101">通过对卷积<i>LSTM</i>网络的加深和不同卷积层与不同门控单元的结合, 深度卷积<i>LSTM</i>模型成功增强了模型在时空序列预测问题中的空间结构信息表征能力, 在图像细节的表达能力上明显优于原模型;在位置预测上, 依然继承了卷积<i>LSTM</i>算法的精准度;并且加深的算法思想在同样可以作序列预测的卷积<i>GRU</i>模型上验证了, 加深卷积层来提升空间结构信息表征能力的想法是有效的、可行的。但是从序列预测的结果图来看, 经过深度卷积加深的网络依然很难将清晰的结构信息保留到7帧之后。分析原因, 首先最小化均方差作为损失函数, 会使得在反向传播后修正像素点值时过分追求均值最小, 将误差均值化从而导致了模糊;其次, 序列预测的过程中是利用前一张的预测结果预测下一张, 误差会累计。这两方面还有很大的优化空间, 未来将作进一步的研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="120">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[1]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="122">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[2]</b>SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-10-15].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="124">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.
                            </a>
                        </p>
                        <p id="126">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human activity prediction:early recognition of ongoing activities from streaming videos">

                                <b>[4]</b>RYOO M S.Human activity prediction:early recognition of ongoing activities from streaming videos[C]//Proceedings of the 2011IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2011:1036-1043.
                            </a>
                        </p>
                        <p id="128">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parsing Video Events with Goal Inference and Intent Prediction">

                                <b>[5]</b>ZHU S, JIA Y, PEI M.Parsing video events with goal inference and intent prediction[C]//Proceedings of the 2011 International Conference on Computer Vision.Piscataway, NJ:IEEE, 2011:487-494.
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anticipating visual representations from unlabeled video">

                                <b>[6]</b>VONDRICK C, PIRSIAVASH H, TORRALBA A.Anticipating visual representations from unlabeled video[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:98-106.
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contextbased pedestrian path prediction">

                                <b>[7]</b>KOOIJ J F P, SCHNEIDER N, FLOHR F, et al.Context-based pedestrian path prediction[C]//Proceedings of the 2014 European Conference on Computer Vision, LNCS 8694.Berlin:Springer, 2014:618-633.
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dense Optical Flow Prediction from a Static Image">

                                <b>[8]</b>WALKER J, GUPTA A, HEBERT M.Dense optical flow prediction from a static image[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:2443-2451.
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;What happens if..&amp;quot;learning to predict the effect of forces in images">

                                <b>[9]</b>MOTTAGHI R, RASTEGARI M, GUPTA A, et al.“What happens if…”learning to predict the effect of forces in images[C]//Proceedings of the 2016 European Conference on Computer Vision, LNCS 9908.Berlin:Springer, 2016:269-285.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjUwNTdIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMW9kYWhRPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001338364&amp;v=MjczNjV4b3hjTUg3UjdxZForWnVGaXJsVkx2QklGZz1OajdCYXJPNEh0SE5ySXhOWiswTFkzazV6QmRoNGo5OVNYcVJy&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>ELMAN J L.Distributed representations, simple recurrent networks, and grammatical structure[J].Machine Learning, 1991, 7 (2/3) :195-225.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811006&amp;v=MTA4NjNCZDdHNEg5bk5ybzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyL09Mejc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>李洋, 董红斌.基于CNN和Bi LSTM网络特征融合的文本情感分析[J].计算机应用, 2018, 38 (11) :3075-3080. (LI Y, DONG H B.Text sentiment analysis based on feature fusion of convolution neural network and bidirectional long short-term memory network[J].Journal of Computer Applications, 2018, 38 (11) :3075-3080.) 
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201809010&amp;v=MDM5Njc0SDluTXBvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3IvT0x6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>姚煜, RYAD C.基于双向长短时记忆联结时序分类和加权有限状态转换器的端到端中文语音识别系统[J].计算机应用, 2018, 38 (9) :2495-2499. (YAO W, RYAD C.End-to-end Chinese speech recognition system based on bidirectional long-term memory-timed timing classification and weighted finite state converter[J].Journal of Computer Applications, 2018, 38 (9) :2495-2499.) 
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sequence to sequence learning with neural networks">

                                <b>[14]</b>SUTSKEVER I, VINYALS O, LE Q V.Sequence to sequence learning with neural networks[C]//Proceedings of the 2014 Neural Information Processing Systems Conference.Cambridge, MA:MIT Press, 2014:3104-3112.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning long-term dependencies with gradient descent is difficult">

                                <b>[15]</b>BENGIO Y, SIMARD P, FRASCONI P.Learning long-term dependencies with gradient descent is difficult[J].IEEE Transactions on Neural Networks, 1994, 5 (2) :157-166.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional LSTM network:a machine learning approach for precipitation nowcasting">

                                <b>[16]</b>SHI X J, CHEN Z R, WANG H, et al.Convolutional LSTM network:a machine learning approach for precipitation nowcasting[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:802-810.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Going deeper in facial expression recognition using deep neural networks.&amp;quot;">

                                <b>[17]</b>MOLLAHOSSEINI A, CHAN D, MAHOOR M H.Going deeper in facial expression recognition using deep neural networks[C]//Proceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision.Piscataway, NJ:IEEE, 2016:1-10.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift">

                                <b>[18]</b>IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning.Cambridge, MA:MIT Press, 2015:448-486.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Original Contribution: Multilayer feedforward networks with a nonpolynomial activation function can approximate any function">

                                <b>[19]</b>LESHNO M, LIN V Y, PINKUS A, et al.Original contribution:multilayer feedforward networks with a nonpolynomial activation function can approximate any function[J].Neural Networks, 1991, 6 (6) :861-867.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906018" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906018&amp;v=MjY0MjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3IvUEx6N0JkN0c0SDlqTXFZOUViSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
