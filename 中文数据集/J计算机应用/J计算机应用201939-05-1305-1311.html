<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136763227627500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905011%26RESULT%3d1%26SIGN%3dnEP4Wh6s%252fOdh0P%252bdwcXKieGY4M0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905011&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905011&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905011&amp;v=MTE4NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1WNy9CTHo3QmQ3RzRIOWpNcW85RVpZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#57" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 理论介绍 ">1 理论介绍</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="1.1 &lt;b&gt;多标签学习&lt;/b&gt;">1.1 <b>多标签学习</b></a></li>
                                                <li><a href="#66" data-title="1.2 IMLLA">1.2 IMLLA</a></li>
                                                <li><a href="#87" data-title="1.3 &lt;b&gt;极限学习机求解权重&lt;/b&gt;">1.3 <b>极限学习机求解权重</b></a></li>
                                                <li><a href="#105" data-title="1.4 &lt;b&gt;萤火虫方法&lt;/b&gt;">1.4 <b>萤火虫方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="2 融合萤火虫方法的多标签懒惰学习算法 ">2 融合萤火虫方法的多标签懒惰学习算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#172" data-title="3 实验及其结果分析 ">3 实验及其结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#173" data-title="3.1 &lt;b&gt;实验数据集描述&lt;/b&gt;">3.1 <b>实验数据集描述</b></a></li>
                                                <li><a href="#176" data-title="3.2 &lt;b&gt;实验环境及评价指标&lt;/b&gt;">3.2 <b>实验环境及评价指标</b></a></li>
                                                <li><a href="#189" data-title="3.3 &lt;b&gt;算法选择与相关参数设置&lt;/b&gt;">3.3 <b>算法选择与相关参数设置</b></a></li>
                                                <li><a href="#191" data-title="3.4 &lt;b&gt;实验结果&lt;/b&gt;">3.4 <b>实验结果</b></a></li>
                                                <li><a href="#202" data-title="3.5 &lt;b&gt;统计假设检验及稳定性分析&lt;/b&gt;">3.5 <b>统计假设检验及稳定性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#211" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#118" data-title="图1 样本&lt;b&gt;&lt;i&gt;x&lt;/i&gt;&lt;/b&gt;与其近邻">图1 样本<b><i>x</i></b>与其近邻</a></li>
                                                <li><a href="#175" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;多标签数据集的详细描述&lt;/b&gt;"><b>表</b>1 <b>多标签数据集的详细描述</b></a></li>
                                                <li><a href="#193" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;各算法在&lt;/b&gt;8&lt;b&gt;个数据集上的平均精度测试结果&lt;/b&gt; (↑) "><b>表</b>2 <b>各算法在</b>8<b>个数据集上的平均精度测试结果</b> (↑) </a></li>
                                                <li><a href="#194" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各算法在&lt;/b&gt;8&lt;b&gt;个数据集上的覆盖率测试结果&lt;/b&gt; (↓) "><b>表</b>3 <b>各算法在</b>8<b>个数据集上的覆盖率测试结果</b> (↓) </a></li>
                                                <li><a href="#195" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;各算法在&lt;/b&gt;8&lt;b&gt;个数据集上的海明损失测试结果&lt;/b&gt; (↓) "><b>表</b>4 <b>各算法在</b>8<b>个数据集上的海明损失测试结果</b> (↓) </a></li>
                                                <li><a href="#196" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;各算法在&lt;/b&gt;8&lt;b&gt;个数据集上的&lt;/b&gt;1-&lt;b&gt;错误率测试结果&lt;/b&gt; (↓) "><b>表</b>5 <b>各算法在</b>8<b>个数据集上的</b>1-<b>错误率测试结果</b> (↓) </a></li>
                                                <li><a href="#198" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;各算法在&lt;/b&gt;8&lt;b&gt;个数据集上的排序损失测试结果&lt;/b&gt; (↓) "><b>表</b>6 <b>各算法在</b>8<b>个数据集上的排序损失测试结果</b> (↓) </a></li>
                                                <li><a href="#200" data-title="图2 &lt;i&gt;k&lt;/i&gt;值对5个指标的影响">图2 <i>k</i>值对5个指标的影响</a></li>
                                                <li><a href="#208" data-title="图3 算法综合性能比较">图3 算法综合性能比较</a></li>
                                                <li><a href="#209" data-title="图4 具有不同评估指标的8个基准多标签数据集测试获得的稳定性指数值">图4 具有不同评估指标的8个基准多标签数据集测试获得的稳定性指数值</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" GIBAJA E, VENTURA S.A tutorial on multilabel learning[J].ACM Computing Surveys, 2015, 47 (3) :1-38." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM857AEC4E4ECC23C23EF3102C8F0C8A4E&amp;v=Mjc0MTJ3S0U9TmlmSVk3dTlHYUM1M0lzd1lKNThmMzQ2dkJRUW4wbCtTWC9nM3hwRGVjR2NOTDdxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekx1Nw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         GIBAJA E, VENTURA S.A tutorial on multilabel learning[J].ACM Computing Surveys, 2015, 47 (3) :1-38.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 何志芬, 杨明, 刘会东.多标记分类和标记相关性的联合学习[J].软件学报, 2014, 25 (9) :1967-1981. (HE Z F, YANG M, LIU H D.Joint learning of multi-label classification and label correlations[J].Journal of Software, 2014, 25 (9) :1967-1981.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201409006&amp;v=MjkxMTVGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVY3L0JOeWZUYkxHNEg5WE1wbzk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         何志芬, 杨明, 刘会东.多标记分类和标记相关性的联合学习[J].软件学报, 2014, 25 (9) :1967-1981. (HE Z F, YANG M, LIU H D.Joint learning of multi-label classification and label correlations[J].Journal of Software, 2014, 25 (9) :1967-1981.) 
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" LIU J, CHANG W, WU Y, et al.Deep learning for extreme multi-label text classification[C]// Proceedings of the 40th International ACM SIGIR Conference on Research &amp;amp; Development in Information Retrieval.New York:ACM, 2017:115-124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for extreme multi-label text classification">
                                        <b>[3]</b>
                                         LIU J, CHANG W, WU Y, et al.Deep learning for extreme multi-label text classification[C]// Proceedings of the 40th International ACM SIGIR Conference on Research &amp;amp; Development in Information Retrieval.New York:ACM, 2017:115-124.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" KORDMAHALLEH M M, HOMAIFAR A, DUKKA B K C.Hierarchical multi-label gene function prediction using adaptive mutation in crowding niching[C]// Proceedings of the 13th IEEE International Conference on BioInformatics and BioEngineering.Piscataway, NJ:IEEE, 2013:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical multi-label gene function prediction using adaptive mutation in crowding niching">
                                        <b>[4]</b>
                                         KORDMAHALLEH M M, HOMAIFAR A, DUKKA B K C.Hierarchical multi-label gene function prediction using adaptive mutation in crowding niching[C]// Proceedings of the 13th IEEE International Conference on BioInformatics and BioEngineering.Piscataway, NJ:IEEE, 2013:1-6.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" ZHU X, LI X, ZHANG S.Block-row sparse multiview multilabel learning for image classification[J].IEEE Transactions on Cybernetics, 2016, 46 (2) :450." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Block-Row Sparse Multiview Multilabel Learning for Image Classification">
                                        <b>[5]</b>
                                         ZHU X, LI X, ZHANG S.Block-row sparse multiview multilabel learning for image classification[J].IEEE Transactions on Cybernetics, 2016, 46 (2) :450.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" WANG Z, CHEN T, LI G, et al.Multi-label image recognition by recurrently discovering attentional regions[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2017:464-472." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-label image recognition by recurrently discovering attentional regions">
                                        <b>[6]</b>
                                         WANG Z, CHEN T, LI G, et al.Multi-label image recognition by recurrently discovering attentional regions[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2017:464-472.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" OZONAT K M, YOUNG D E.Towards a universal marketplace over the Web:statistical multi-label classification of service provider forms with simulated annealing[C]// Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2009:1295-1304." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards a universal marketplace over the web:Statistical multi-label classification of service provider forms with simulated annealing">
                                        <b>[7]</b>
                                         OZONAT K M, YOUNG D E.Towards a universal marketplace over the Web:statistical multi-label classification of service provider forms with simulated annealing[C]// Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2009:1295-1304.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" HOU S, ZHOU S, CHEN L, et al.Multi-label learning with label relevance in advertising video[J].Neurocomputing, 2016, 171 (C) :932-948." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES8BB9B4C24E43506B1BD620A56E385822&amp;v=MTQ5MTNOaStxL3hIWUo0TEQzazV5V1FTbUV0N1NuK1RxUlJBZXJxUlRiaWRDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THU3d0tFPU5pZk9mYnZLYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         HOU S, ZHOU S, CHEN L, et al.Multi-label learning with label relevance in advertising video[J].Neurocomputing, 2016, 171 (C) :932-948.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" BOUTELL M R, LUO J, SHEN X, et al.Learning multi-label scene classification [J].Pattern Recognition, 2004, 37 (9) :1757-1771." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740311&amp;v=MjA1MjNsVXIzSUtGMFFhaHM9TmlmT2ZiSzdIdEROcVk5RlkrOFBEMzA0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         BOUTELL M R, LUO J, SHEN X, et al.Learning multi-label scene classification [J].Pattern Recognition, 2004, 37 (9) :1757-1771.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" ZHANG M, ZHOU Z.ML-KNN:a lazy learning approach to multi-label learning[J].Pattern Recognition, 2007, 40 (7) :2038-2048." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739502&amp;v=Mjg5NzRLN0h0RE5xWTlGWStnR0NYdzdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRjBRYWhzPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         ZHANG M, ZHOU Z.ML-KNN:a lazy learning approach to multi-label learning[J].Pattern Recognition, 2007, 40 (7) :2038-2048.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" LEE J, KIM H, KIM N R, et al.An approach for multi-label classification by directed acyclic graph with label correlation maximization[J].Information Sciences, 2016, 351 (C) :101-114." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB695B771550A53356F0493031209498E&amp;v=MDQ2OTE9TmlmT2ZjRytGOVMrcUloRVllNFBmWGs2ekJNVm5EOTVRWHppcnhNM2VidVFUTExxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekx1N3dLRQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         LEE J, KIM H, KIM N R, et al.An approach for multi-label classification by directed acyclic graph with label correlation maximization[J].Information Sciences, 2016, 351 (C) :101-114.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" ELISSEEFF A E, WESTON J.A kernel method for multi-labelled classification[C]// Proceedings of the 14th International Conference on Neural Information Processing Systems:Natural and Synthetic.Cambridge, MA:MIT Press, 2002:681-687." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A kernel method for multi-labeled classification">
                                        <b>[12]</b>
                                         ELISSEEFF A E, WESTON J.A kernel method for multi-labelled classification[C]// Proceedings of the 14th International Conference on Neural Information Processing Systems:Natural and Synthetic.Cambridge, MA:MIT Press, 2002:681-687.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" HUANG G, ZHU Q, SIEW C K.Extreme learning machine:theory and applications[J].Neurocomputing, 2006, 70 (1/2/3) :489-501." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913101&amp;v=MjM4MTdlb01EWHc0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0YwUWFocz1OaWZPZmJLN0h0RE5xbzlFYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         HUANG G, ZHU Q, SIEW C K.Extreme learning machine:theory and applications[J].Neurocomputing, 2006, 70 (1/2/3) :489-501.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 王一宾, 程玉胜, 何月, 等.回归核极限学习机的多标记学习算法[J].模式识别与人工智能, 2018, 31 (5) :419-430. (WANG Y B, CHENG Y S, HE Y, et al.Multi-label learning algorithm of regression kernel extreme learning machine[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (5) :419-430.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201805004&amp;v=MDA2MjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1WNy9CS0Q3WWJMRzRIOW5NcW85RllJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         王一宾, 程玉胜, 何月, 等.回归核极限学习机的多标记学习算法[J].模式识别与人工智能, 2018, 31 (5) :419-430. (WANG Y B, CHENG Y S, HE Y, et al.Multi-label learning algorithm of regression kernel extreme learning machine[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (5) :419-430.) 
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 张敏灵.一种新型多标记懒惰学习算法[J].计算机研究与发展, 2012, 49 (11) :2271-2282. (ZHANG M L.An improved multi-label lazy learning approach[J].Journal of Computer Research and Development, 2012, 49 (11) :2271-2282.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201211002&amp;v=MDQ1NzQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1WNy9CTHl2U2RMRzRIOVBOcm8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         张敏灵.一种新型多标记懒惰学习算法[J].计算机研究与发展, 2012, 49 (11) :2271-2282. (ZHANG M L.An improved multi-label lazy learning approach[J].Journal of Computer Research and Development, 2012, 49 (11) :2271-2282.) 
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" YANG X, HE X.Firefly algorithm:recent advances and applications[J].International Journal of Swarm Intelligence, 2013, 1 (1) :36-50." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCW&amp;filename=SJCW90964E44BB6AC7F55663F1EEB1FA7BBD&amp;v=Mjk3MjlvdEJGcGtKZlE4K3VSTVc3RGwrUG42WDJXQTBEOE9UTjhqckNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMdTd3S0U9TmlmSWVicTRGOWZJMg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         YANG X, HE X.Firefly algorithm:recent advances and applications[J].International Journal of Swarm Intelligence, 2013, 1 (1) :36-50.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" HIDALGOPANIAGUA A, MIDUEL A V, JOAQUIN F, et al.Solving the multi-objective path planning problem in mobile robotics with a firefly-based approach[J].Soft Computing, 2017, 21 (4) :1-16." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Solving the multi-objective path planning problem in mobile robotics with a firefly-based approach">
                                        <b>[17]</b>
                                         HIDALGOPANIAGUA A, MIDUEL A V, JOAQUIN F, et al.Solving the multi-objective path planning problem in mobile robotics with a firefly-based approach[J].Soft Computing, 2017, 21 (4) :1-16.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" LEI Y, ZHAO D, CAI H B.Prediction of length-of-day using extreme learning machine[J].Geodesy and Geodynamics, 2015, 6 (2) :151-159." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF29F0E0ADCFC5D7AE320D6BD3D1C6A70&amp;v=MTE1ODR6THU3d0tFPU5pZk9mY1c2RjZmTTJvODBFSmg1ZjNsTnlHZG02VDE5UEhtUTJCRkJlTUdTTkwyZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         LEI Y, ZHAO D, CAI H B.Prediction of length-of-day using extreme learning machine[J].Geodesy and Geodynamics, 2015, 6 (2) :151-159.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" WANG Z, XIN J, TIAN S, et al.Distributed and weighted extreme learning machine for imbalanced big data learning[J].Tsinghua Science and Technology, 2017, 22 (2) :160-173." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QHDY201702004&amp;v=Mjc5MzdCdEdGckNVUjdxZlp1WnNGeURtVjcvQk5DWFBkN0c0SDliTXJZOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         WANG Z, XIN J, TIAN S, et al.Distributed and weighted extreme learning machine for imbalanced big data learning[J].Tsinghua Science and Technology, 2017, 22 (2) :160-173.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" LUO F F, GUO W Z, YU Y L, et al.A multi-label classification algorithm based on kernel extreme learning machine[J].Neurocomputing, 2017, 260:313-320." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAF1AF2BD1806F907B59AD1CFBDD2C2BD&amp;v=MDI4MDJEblI4anJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THU3d0tFPU5pZk9mY0xPSDZDNnJmMHhaZU1QQ2dvd3p4Rmg3ellNUEg2UjJtQkJEYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         LUO F F, GUO W Z, YU Y L, et al.A multi-label classification algorithm based on kernel extreme learning machine[J].Neurocomputing, 2017, 260:313-320.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" 杨明极, 马池, 王娅, 等.一种改进&lt;i&gt;K&lt;/i&gt;-means 聚类的FCMM 算法[J/OL].计算机应用研究, 2019, 36 (7) [2018- 04- 12].http://www.arocmag.com/article/02-2019-07-006.html. (YANG M J, MA C, WANG Y, et al.Algorithm named FCMM to improve K-means clustering algorithm[J/OL].Application Research of Computers, 2019, 36 (7) [2018- 04- 12].http://www.arocmag.com/article/02-2019-07-006.html.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201907020&amp;v=MzE2MzFNcUk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1WNy9CTHo3U1pMRzRIOWo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         杨明极, 马池, 王娅, 等.一种改进&lt;i&gt;K&lt;/i&gt;-means 聚类的FCMM 算法[J/OL].计算机应用研究, 2019, 36 (7) [2018- 04- 12].http://www.arocmag.com/article/02-2019-07-006.html. (YANG M J, MA C, WANG Y, et al.Algorithm named FCMM to improve K-means clustering algorithm[J/OL].Application Research of Computers, 2019, 36 (7) [2018- 04- 12].http://www.arocmag.com/article/02-2019-07-006.html.) 
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" WANG H, WANG W, ZHOU X, et al.Firefly algorithm with neighborhood attraction[J].Information Sciences, 2017, 382/383:374-387." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES135235F971946B0F45C78B7899ADCE20&amp;v=MjU2NjFMdTd3S0U9TmlmT2ZiSzdHOVBQcXZsTVkrb0dDSHBMejJBWDcweDZRQTNscEJzOENNYm5NTGlmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         WANG H, WANG W, ZHOU X, et al.Firefly algorithm with neighborhood attraction[J].Information Sciences, 2017, 382/383:374-387.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" 程美英, 倪志伟, 朱旭辉.萤火虫优化算法理论研究综述[J].计算机科学, 2015, 42 (4) :19-24. (CHENG M Y, NI Z W, ZHU X H.Overview on glowworm swarm optimization or firefly algorithm[J].Computer Science, 2015, 42 (4) :19-24.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201504002&amp;v=Mjg4MjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVY3L0JMejdCYjdHNEg5VE1xNDlGWm9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         程美英, 倪志伟, 朱旭辉.萤火虫优化算法理论研究综述[J].计算机科学, 2015, 42 (4) :19-24. (CHENG M Y, NI Z W, ZHU X H.Overview on glowworm swarm optimization or firefly algorithm[J].Computer Science, 2015, 42 (4) :19-24.) 
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" ZHANG M L, ZHOU Z H.A Review on multi-label learning algorithms[J].IEEE Transactions on Knowledge &amp;amp; Data Engineering, 2014, 26 (8) :1819-1837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A review on multi-label learning algorithms">
                                        <b>[24]</b>
                                         ZHANG M L, ZHOU Z H.A Review on multi-label learning algorithms[J].IEEE Transactions on Knowledge &amp;amp; Data Engineering, 2014, 26 (8) :1819-1837.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" DEMSAR J.Statistical comparisons of classifiers over multiple data sets[J].Journal of Machine Learning Research, 2006, 7 (1) :1-30." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Statistical comparisons of classifiers over multiple data sets">
                                        <b>[25]</b>
                                         DEMSAR J.Statistical comparisons of classifiers over multiple data sets[J].Journal of Machine Learning Research, 2006, 7 (1) :1-30.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" ZHANG M, WU L.Lift:Multi-label learning with label-specific features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (1) :107-120." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LIFT:Multi-label learning with label-specific features">
                                        <b>[26]</b>
                                         ZHANG M, WU L.Lift:Multi-label learning with label-specific features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (1) :107-120.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" LIN Y, LI Y, WANG C, et al.Attribute reduction for multi-label learning with fuzzy rough set[J].Knowledge-Based Systems, 2018, 152:51-56." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES22A4CF21D5805EE3B00914C749BDE4F4&amp;v=MjIxNjZ1N3dLRT1OaWZPZmJHNmI5Vy8yWTFFRU80SERIbE11aFZoNmo5MFNYdVJxeFk4QzhiaFFjeWJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6TA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         LIN Y, LI Y, WANG C, et al.Attribute reduction for multi-label learning with fuzzy rough set[J].Knowledge-Based Systems, 2018, 152:51-56.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 10:13</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1305-1311 DOI:10.11772/j.issn.1001-9081.2018109182            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>融合萤火虫方法的多标签懒惰学习算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E7%8E%89%E8%83%9C&amp;code=35637663&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程玉胜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%B1%E5%9D%A4&amp;code=40031843&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钱坤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%B8%80%E5%AE%BE&amp;code=35637665&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王一宾</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%A4%A7%E5%8D%AB&amp;code=40031842&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵大卫</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%89%E5%BA%86%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=1699539&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安庆师范大学计算机与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%89%E5%BE%BD%E7%9C%81%E9%AB%98%E6%A0%A1%E6%99%BA%E8%83%BD%E6%84%9F%E7%9F%A5%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%AE%89%E5%BA%86%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安徽省高校智能感知与计算重点实验室(安庆师范大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>已有的多标签懒惰学习算法 (IMLLA) 在利用近邻标签时因仅考虑了近邻标签相关性信息, 而忽略相似度的影响, 这可能会使算法的鲁棒性有所降低。针对这个问题, 引入萤火虫方法, 将相似度信息与标签信息相结合, 提出一种融合萤火虫方法的多标签懒惰学习算法 (FF-MLLA) 。首先, 利用Minkowski距离来度量样本间相似度, 从而找到近邻点;然后, 结合标签近邻点和萤火虫方法对标签计数向量进行改进;最后, 使用奇异值分解 (SVD) 与核极限学习机 (ELM) 进行线性分类。该算法同时考虑了标签信息与相似度信息从而提高了鲁棒性。实验结果表明, 所提算法较其他的多标签学习算法有一定优势, 并使用统计假设检验与稳定性分析进一步说明所提出算法的合理性与有效性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多标签学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%90%A4%E7%81%AB%E8%99%AB%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">萤火虫方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E7%AD%BE%E7%9B%B8%E5%85%B3%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标签相关性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%87%E7%AD%BE%E6%87%92%E6%83%B0%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多标签懒惰学习算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9E%81%E9%99%90%E5%AD%A6%E4%B9%A0%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">极限学习机;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *程玉胜 (1969—) , 男, 安徽安庆人, 教授, 博士, 主要研究方向:大数据、粗糙集、特征选择的机器学习;电子邮箱chengyshaq@163.com;
                                </span>
                                <span>
                                    钱坤 (1995—) , 男, 安徽滁州人, 硕士研究生, CCF会员, 主要研究方向:多标签学习、机器学习、数据统计;;
                                </span>
                                <span>
                                    王一宾 (1970—) , 男, 安徽安庆人, 教授, 硕士, CCF会员, 主要研究方向:多标签学习、机器学习、软件安全;;
                                </span>
                                <span>
                                    赵大卫 (1993—) , 男, 安徽芜湖人, 硕士研究生, 主要研究方向:机器学习、大数据、数据统计。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>安徽省高校重点科研项目 (KJ2017A352);</span>
                                <span>安庆师范大学科研创新团队建设计划;</span>
                    </p>
            </div>
                    <h1><b>Multi-label lazy learning approach based on firefly method</b></h1>
                    <h2>
                    <span>CHENG Yusheng</span>
                    <span>QIAN Kun</span>
                    <span>WANG Yibing</span>
                    <span>ZHAO Dawei</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information, Anqing Normal University</span>
                    <span>University Key Laboratory of Intelligent Perception and Computing of Anhui Province (Anqing Normal University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The existing Improved Multi-label Lazy Learning Approach (IMLLA) has the problem that the influence of similarity information is ignored with only the neighbor label correlation information considered when the neighbor labels were used, which may reduce the robustness of the approach. To solve this problem, with firefly method introduced and the combination of similarity information with label information, a Multi-label Lazy Learning Approach based on FireFly method (FF-MLLA) was proposed. Firstly, Minkowski distance was used to measure the similarity between samples to find the neighbor point. Secondly, the label count vector was improved by combining the neighbor point and firefly method. Finally, Singular Value Decomposition (SVD) and kernel Extreme Learning Machine (ELM) were used to realize linear classification. The robustness of the approach was improved due to considering both label information and similarity information. The experimental results demonstrate that the proposed approach improves the classification performance to a great extent compared to other multi-label learning approaches. And the statistical hypothesis testing and stability analysis are used to further illustrate the rationality and effectiveness of the proposed approach.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-label%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-label learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=firefly%20method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">firefly method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=label%20correlation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">label correlation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Improved%20Multi-label%20Lazy%20Learning%20Approach%20(IMLLA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Improved Multi-label Lazy Learning Approach (IMLLA) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Extreme%20Learning%20Machine%20(ELM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Extreme Learning Machine (ELM) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHENG Yusheng, born in 1969, Ph. D. , professor. His research interests include big data, rough sets, machine learning for feature selection.;
                                </span>
                                <span>
                                     QIAN Kun, born in 1995, M. S. candidate. His research interests include multi-label learning, machine learning, data statistics. ;
                                </span>
                                <span>
                                    WANG Yibing, born in 1970. M. S. , professor. His research interests include multi-label learning, machine learning, software security. ;
                                </span>
                                <span>
                                    ZHAO Dawei, born in 1993. M. S. candidate. His research interests include machine learning, big data, data statistics.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-13</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Key Scientific Research Project for Universities in Anhui Province (KJ2017A352);</span>
                                <span>the Research and Innovation Team Building Plan of Anqing Normal University;</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="57" name="57" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="58">多标签学习<citation id="214" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是一种应用非常广泛的学习范式, 是机器学习研究的重要热点之一。传统的单标签学习, 每个对象只与单个标签相关联;然而, 真实世界中的对象往往具有多义性, 比如一篇文章可能属于军事、体育、运动等多个主题<citation id="215" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="59">多标签学习作为处理具有丰富语义真实世界对象的学习框架之一, 且其研究成果已经广泛应用到文本分类<citation id="216" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、基因工程<citation id="217" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、图像识别<citation id="222" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>、Web数据挖掘<citation id="218" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和视频自动标注<citation id="219" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等多个领域。对此许多学者提出了针对多标签分类的学习算法, 例如BR (Binary Relevance) 算法、LP (Label Power) 算法<citation id="220" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等, 它们通过增加分类器个数或者标签的种类来解决多标签问题, 但在一定程度上影响了分类器效率。经典的ML<i>K</i>NN (Multi-Label <i>K</i> Nearest Neighbors) 算法<citation id="221" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>利用最大化后验概率 (Maximum A Posteriori) 来解决多标签学习预测问题, 虽提升了分类器的性能, 却增加了其计算的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="60">此外, 现实世界中各样本所含标签并不相互独立, 存在相关关系, 然而目前绝大多数多标签学习算法未充分考虑其标签相关性。因此, 充分利用标签之间的相关性信息, 对构建强泛化性能的多标签分类学习算法具有重要意义<citation id="223" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="61">而针对标签间的相关性, 许多学者提出了相关算法, 取得了不错的效果。例如, RankSVM (Ranking Support Vector Machine) 算法<citation id="224" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>采用最大间隔策略以适应多标签学习, 采用类似BR策略构建SVM (Support Vector Machine) 多标签分类器, 但其时间消耗较大。由于极限学习机 (Extreme Learning Machine, ELM) <citation id="225" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>训练速度快, MLRKELM (Multi-Label algorithm of Regression Kernel Extreme Learning Machine) 算法<citation id="226" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>使用回归模式的核ELM, 缩短了算法的运行时间。MLASRKELM (MLRKELM with Association Rules) 算法<citation id="227" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在MLRKELM算法的基础上引入了关联规则, 保留了标签之间的信息。针对标签之间的相关性, 张敏灵<citation id="228" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>在ML<i>K</i>NN算法基础上提出一种新型的多标签懒惰学习算法 (Improved Multi-label Lazy Learning Approach, IMLLA) 。IMLLA利用近邻的标签信息构建一个标记计数向量来进行分类, 此算法在构建标签计数向量时使用了近邻标签信息, 认为近邻的标签具有相同的重要性。然而, 近邻与样本间的相似度越大, 此近邻的标签越重要, IMLLA因未考虑近邻相似度信息所以其泛化性有所降低。</p>
                </div>
                <div class="p1">
                    <p id="62">在上述研究成果上, 对于样本分布问题, 本文在IMLLA的基础上引入萤火虫方法<citation id="230" type="reference"><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。萤火虫方法作为模仿自然界中萤火虫发光行为而构造出的元启发式算法, 具有操作简单、易于并行处理、鲁棒性强等特点。故利用萤火虫方法将近邻的标签信息与近邻的相似度信息相融合, 以提高算法的鲁棒性, 而提出一种融合萤火虫方法的多标签懒惰学习算法 (Multi-label Lazy Learning Approach based on FireFly method, FF-MLLA) 。本文通过萤火虫方法根据相似度来计算样本与近邻间的吸引度, 吸引度越大则该近邻的标签越重要。然后将吸引度作为权重与标签信息相结合, 对IMLLA中的标签计数向量进行重构。由于Huang等提出的极限学习机算法<citation id="229" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>具有训练速度快、泛化能力强等优点, 所以在使用线性分类器进行分类时, 引入ELM进行权重求解。此外, 还使用了奇异值分解 (Singular Value Decomposition, SVD) 求解权重。为了验证本文算法的有效性, 本文将FF-MLLA与标准IMLLA, 以及其他经典的多标签算法在多个公开数据集上进行实验对比。实验结果表明, 本文算法较其他对比算法具有一定优势。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">1 理论介绍</h3>
                <h4 class="anchor-tag" id="64" name="64">1.1 <b>多标签学习</b></h4>
                <div class="p1">
                    <p id="65">多标签学习是针对现实生活中普遍存在的多义性对象而提出的一种学习框架。在这个框架之下, 样本由多个特征和多个标签构成, 学习的目标是将未知的实例对应更多正确的标签。在单实例多标签学习中, 假设<b><i>X</i></b>={<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>}<sup>T</sup>∈<b>R</b><sup><i>n</i>*<i>d</i></sup>表示有<i>n</i>个样本且每个样本的特征数为<i>d</i>, <b><i>Y</i></b>={1, 2, …, <i>Q</i>}表示可能的概念构成的集合。<b><i>T</i></b>={ (<b><i>x</i></b><sub>1</sub>, <b><i>Y</i></b><sub>1</sub>) , (<b><i>x</i></b><sub>2</sub>, <b><i>Y</i></b><sub>2</sub>) , …, (<b><i>x</i></b><sub><i>m</i></sub>, <b><i>Y</i></b><sub><i>m</i></sub>) } (<b><i>x</i></b><sub><i>i</i></sub>∈<b><i>X</i></b>, <b><i>Y</i></b><sub><i>i</i></sub>∈<b><i>Y</i></b>) 表示训练集, 多标签学习的目标就是得到映射关系<i>f</i>:<b><i>X</i></b>→{-1, 1}<sup><i>Q</i></sup>, 并对标签未知而特征已知的样本进行标签预测。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.2 IMLLA</h4>
                <div class="p1">
                    <p id="67"><i>ML</i>K<i>NN</i>是一种经典的多标签分类算法, 它先获取近邻样本的标签信息, 再通过“最大化后验概率”的方式推理未见实例的标签集合, 但它未充分考察标签之间的相关性。基于此问题, 张敏灵提出一种新型的多标签懒惰学习算法<i>IMLLA</i>。<i>IMLLA</i>首先将测试样本在训练集中找出<i>k</i>个近邻及其<i>k</i>个近邻的标记信息, 然后根据<i>k</i>个近邻的标记信息生成各标签计数向量, 并提交给已训练的分类器进行标签预测。</p>
                </div>
                <div class="p1">
                    <p id="68">设样本<b><i>x</i></b>∈<b><i>X</i></b>, 标签集合<b><i>y</i></b>∈<b><i>Y</i></b>, 设<b><i>y</i></b><sub><i>x</i></sub>为样本<b><i>x</i></b>对应的<i>Q</i>维标签类别向量。其中, 当<i>l</i>∈<b><i>Y</i></b>成立时该向量第<i>l</i>维<i>y</i><sub><i>x</i></sub> (<i>l</i>) 取值为+1, 否则<i>y</i><sub><i>x</i></sub> (<i>l</i>) 取值为-1。此外, 设<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mi>l</mi></msub><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mo>|</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mrow><mo>∈</mo><mi mathvariant="bold-italic">Τ</mi><mo>, </mo><mi>l</mi><mo>∈</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></math></mathml>为训练集<b><i>T</i></b>中具有标签<i>l</i>构成的集合。记<b><i>N</i></b><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>l</mi></msubsup></mrow></math></mathml>为样本<b><i>x</i></b>在训练集中的<i>k</i>个近邻构成的集合, 即:</p>
                </div>
                <div class="p1">
                    <p id="71"><b><i>N</i></b><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>l</mi></msubsup></mrow></math></mathml>={<b><i>z</i></b>|<b><i>z</i></b>是样本<b><i>x</i></b>在<b><i>T</i></b><sub><i>l</i></sub>中的<i>k</i>近邻点}      (1) </p>
                </div>
                <div class="p1">
                    <p id="73">在寻找样本<i>k</i>个近邻点集合时, IMLLA用<i>r</i>阶Minkowski距离度量两个样本<b><i>x</i></b><sub><i>i</i></sub>与<b><i>x</i></b><sub><i>j</i></sub>的相似度:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>h</mi></msubsup><mo>-</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>j</mi><mi>h</mi></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mi>r</mi></msup></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mi>r</mi></mfrac></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">基于与每一个标签对应的<b><i>N</i></b><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>l</mi></msubsup></mrow></math></mathml>, 定义<i>Q</i>维的标签计数向量<b><i>C</i></b><sub><i>x</i></sub>如下:</p>
                </div>
                <div class="p1">
                    <p id="77"><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi mathvariant="bold-italic">Y</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">z</mi><mo>∈</mo><mi mathvariant="bold-italic">Ν</mi><msubsup><mrow></mrow><mi>x</mi><mi>q</mi></msubsup></mrow></munder><mo stretchy="false">[</mo></mstyle></mrow></mstyle><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo><mrow><mo>=</mo><mo>=</mo></mrow><mn>1</mn><mo stretchy="false">|</mo><mo stretchy="false">]</mo></mrow></math></mathml>; <i>l</i>∈<b><i>Y</i></b>      (3) </p>
                </div>
                <div class="p1">
                    <p id="79">向量<b><i>C</i></b><sub><i>x</i></sub>的分量<b><i>C</i></b><sub><i>x</i></sub> (<i>l</i>) 表示<b><i>x</i></b>与所有类别对应的近邻样本集合中隶属于第<i>l</i>类的近邻个数。接着采用如下线性分类器来确定IMLLA在第<i>l</i>类上的输出:</p>
                </div>
                <div class="p1">
                    <p id="80"><i>f</i> (<b><i>t</i></b>, <i>l</i>) =<b><i>w</i></b><sub><i>l</i></sub><sup>T</sup>*<b><i>C</i></b><sub><i>t</i></sub>; <i>l</i>∈<b><i>Y</i></b>      (4) </p>
                </div>
                <div class="p1">
                    <p id="81">其中:<b><i>w</i></b><sub><i>l</i></sub>为与第<i>l</i>类对应的<i>Q</i>维列向量, T代表转置操作。若<i>f</i> (<b><i>t</i></b>, <i>l</i>) &gt;0, 类别<i>l</i>将隶属于测试样本<b><i>t</i></b>, 即<i>y</i><sub><i>t</i></sub> (<i>l</i>) 取值为+1;否则, 类别<i>l</i>将不隶属于<b><i>t</i></b>, 即<i>y</i><sub><i>t</i></sub> (<i>l</i>) 取值为-1。</p>
                </div>
                <div class="p1">
                    <p id="82">IMLLA对公式<i>f</i> (<b><i>t</i></b>, <i>l</i>) =<b><i>w</i></b><sub><i>l</i></sub><sup>T</sup>*<b><i>C</i></b><sub><i>t</i></sub>, <i>l</i>∈<b><i>Y</i></b>中所需的列向量<b><i>w</i></b><sub><i>l</i></sub>采用最小化误差平方和函数求得:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>l</mi><mo>∈</mo><mi mathvariant="bold-italic">Y</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mover accent="true"><mi>Y</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">由式 (5) 可推导出如下方程组:</p>
                </div>
                <div class="p1">
                    <p id="85"> (<i>Φ</i><sup>T</sup><i>Φ</i>) *<b><i>W</i></b>=<i>Φ</i><sup>T</sup>*<b><i>T</i></b>; <i>φ</i><sub><i>il</i></sub>=<i>C</i><sub><i>x</i><sub><i>i</i></sub></sub> (<i>l</i>) , <i>t</i><sub><i>il</i></sub>=<i>y</i><sub><i>x</i><sub><i>i</i></sub></sub> (<i>l</i>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="86">其中: <b><i>W</i></b>=[<b><i>w</i></b><sub>1</sub>, <b><i>w</i></b><sub>2</sub>, …, <b><i>w</i></b><sub><i>Q</i></sub>]由列向量<b><i>w</i></b><sub><i>l</i></sub> (<i>l</i>∈<b><i>Y</i></b>) 组成, 矩阵<b><i>T</i></b>=[<i>t</i><sub><i>il</i></sub>]<sub><i>m</i>×<i>Q</i></sub>, 然后使用SVD技术对矩阵<b><i>W</i></b>进行快速求解。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">1.3 <b>极限学习机求解权重</b></h4>
                <div class="p1">
                    <p id="88"><i>ELM</i>算法是一种快速的前馈单隐藏层神经网络学习算法, 只需设置隐藏层网络神经元的个数<citation id="231" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><link href="41" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>。当隐藏层的参数与神经元个数选择完毕后, 对隐藏层的输出权重进行矩阵的广义逆的操作, 便可得到算法的输出权重。</p>
                </div>
                <div class="p1">
                    <p id="89">设有<i>N</i>个随机样本的数据表示为<b><i>D</i></b>={ (<b><i>X</i></b><sub><i>i</i></sub>, <b><i>Y</i></b><sub><i>i</i></sub>) <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>}, <b><i>X</i></b><sub><i>i</i></sub>={<i>x</i><sub><i>i</i>1</sub>, <i>x</i><sub><i>i</i>2</sub>, …, <i>x</i><sub><i>in</i></sub>}<sup>T</sup>, <b><i>Y</i></b><sub><i>i</i></sub>={<i>y</i><sub><i>i</i>1</sub>, <i>y</i><sub><i>i</i>2</sub>, …, <i>y</i><sub><i>im</i></sub>}<sup>T</sup>, 隐藏层网络神经元的个数为<i>L</i>, 激活函数为<i>g</i> (<b><i>x</i></b>) , 则可得:</p>
                </div>
                <div class="area_img" id="91">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201905011_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="93">式中, <i>ω</i><sub><i>i</i></sub>={<i>ω</i><sub><i>i</i>1</sub>, <i>ω</i><sub><i>i</i>2</sub>, …, <i>ω</i><sub><i>im</i></sub>}表示第<i>i</i>层的输入权重, <i>b</i><sub><i>i</i></sub>表示为第<i>i</i>层的偏置, <i>β</i><sub><i>i</i></sub>={<i>β</i><sub><i>i</i>1</sub>, <i>β</i><sub><i>i</i>2</sub>, …, <i>β</i><sub><i>im</i></sub>}<sup>T</sup>表示第<i>i</i>层的输出权重, ·表示为点积。若<i>f</i><sub><i>L</i></sub> (<b><i>x</i></b><sub><i>j</i></sub>) -<b><i>y</i></b><sub><i>j</i></sub>=0, 即单隐藏层神经网络的输出与真实标签间不存在误差, 那么<i>f</i><sub><i>L</i></sub> (<b><i>x</i></b><sub><i>j</i></sub>) =<b><i>y</i></b><sub><i>j</i></sub>, 即:</p>
                </div>
                <div class="p1">
                    <p id="94"><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi mathvariant="bold-italic">β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>; <i>j</i>=1, 2, …, <i>N</i>      (8) </p>
                </div>
                <div class="p1">
                    <p id="96">使用矩阵形式的表达可将式 (8) 简化为:</p>
                </div>
                <div class="p1">
                    <p id="97"><b><i>H</i></b><i>β</i>=<b><i>Y</i></b>      (9) </p>
                </div>
                <div class="p1">
                    <p id="98">由式 (9) 可得:</p>
                </div>
                <div class="p1">
                    <p id="99"><i>β</i>=<b><i>H</i></b><sup>+</sup><b><i>Y</i></b>      (10) </p>
                </div>
                <div class="p1">
                    <p id="100">其中:<b><i>H</i></b><sup>+</sup>是<b><i>H</i></b>的Moore-Penrose广义逆矩阵, 根据KKT (Karush-Kuhn-Tucker) 最佳条件, 式 (10) 可写为:</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">β</mi><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mrow><mo> (</mo><mrow><mfrac><mi mathvariant="bold-italic">Ι</mi><mi>λ</mi></mfrac><mo>+</mo><mi mathvariant="bold-italic">Η</mi><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">Y</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">其中:<b><i>I</i></b>表示单位矩阵, <i>λ</i>分母表示正则项。</p>
                </div>
                <div class="p1">
                    <p id="103">在传统的ELM算法中权重和偏置是随机设定的, 因此算法的输出不稳定。本文采用径向基函数 (Radial Basis Function, RBF) 核ELM来解决这个问题, 即对<b><i>H</i></b>使用核函数进行一次映射, 如式 (12) 所示:</p>
                </div>
                <div class="p1">
                    <p id="104"><i>Ω</i><sub>ELM</sub>=<b><i>HH</i></b><sup>T</sup>:<i>Ω</i><sub><i>ELM</i> (<i>i</i>, <i>j</i>) </sub>=<i>h</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>)      (12) </p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">1.4 <b>萤火虫方法</b></h4>
                <div class="p1">
                    <p id="106">萤火虫方法模拟萤火虫的移动过程, 根据萤火虫的移动位置和荧光亮度赋予其目标函数值并计算相对吸引度<citation id="232" type="reference"><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><link href="47" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>。荧光较强的个体吸引荧光较弱的个体按照位置更新公式向其移动, 移动距离由吸引度的大小决定。优化过程基于以下3个原则:</p>
                </div>
                <div class="p1">
                    <p id="107">1) 忽略萤火虫的性别因素, 任何两只萤火虫个体均可以互相吸引。</p>
                </div>
                <div class="p1">
                    <p id="108">2) 萤火虫个体的吸引度与距离成反比, 与亮度成正比, 亮度强的萤火虫吸引亮度弱的萤火虫向其移动, 亮度最强的个体随机移动。</p>
                </div>
                <div class="p1">
                    <p id="109">3) 萤火虫个体的亮度由其所在位置的目标函数值决定。</p>
                </div>
                <div class="p1">
                    <p id="110">萤火虫的相对荧光亮度为:</p>
                </div>
                <div class="p1">
                    <p id="111"><i>I</i>=<i>I</i><sub>0</sub>exp (-<i>γr</i><sub><i>ij</i></sub>)      (13) </p>
                </div>
                <div class="p1">
                    <p id="112">萤火虫的吸引度为:</p>
                </div>
                <div class="p1">
                    <p id="113"><i>β</i>=<i>β</i><sub>0</sub>exp (-<i>γr</i><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>)      (14) </p>
                </div>
                <div class="p1">
                    <p id="115">其中:<i>I</i><sub>0</sub>表示萤火虫的最大荧光亮度, 为常量; <i>γ</i>表示光强吸收因子, 为常量; <i>r</i><sub><i>ij</i></sub>表示两只萤火虫之间的距离; <i>β</i><sub>0</sub>表示最大吸引度, 为常量。</p>
                </div>
                <h3 id="116" name="116" class="anchor-tag">2 融合萤火虫方法的多标签懒惰学习算法</h3>
                <div class="p1">
                    <p id="117">在传统的近邻中, 近邻点对样本有着同样的影响, 而真实世界中, 样本与近邻间的相似度越高, 近邻的标签越重要。在单标签情况下, 如图1所示, 给出样本的7个近邻。容易看出4个近邻有标签而3个近邻没有标签, 从个数而言, 样本有标签, 但能够看出不具有标签的近邻距离样本较近, 直观地认为样本也不具有标签。因此, 距离的远近也影响分类的结果。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905011_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 样本x与其近邻" src="Detail/GetImg?filename=images/JSJY201905011_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 样本<b><i>x</i></b>与其近邻  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905011_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Sample <b><i>x</i></b> and its neighbors</p>

                </div>
                <div class="p1">
                    <p id="119">受萤火虫方法思想的启发, 本文构造了一个<i>k</i>×1的吸引度向量对样本<b><i>x</i></b>在训练集中的<i>k</i>个近邻构成的标签集合进行加权, 从而达到改进IMLLA中的标签计数向量的目的。默认样本<b><i>x</i></b>为最亮的萤火虫, 计算样本<b><i>x</i></b>对<i>k</i>个近邻的吸引度。</p>
                </div>
                <div class="p1">
                    <p id="120">1) <b><i>x</i></b>对<i>k</i>个近邻均有吸引度。</p>
                </div>
                <div class="p1">
                    <p id="121">2) <b><i>x</i></b>对<i>k</i>个近邻的吸引度与距离成反比。</p>
                </div>
                <div class="p1">
                    <p id="122">3) 吸引度由<i>k</i>值及距离的排序决定。</p>
                </div>
                <div class="p1">
                    <p id="123">设样本<b><i>x</i></b>的<i>k</i>个近邻标签集合为<i>N</i><sub><i>x</i></sub>, <i>Dist</i> (<b><i>x</i></b>, <b><i>x</i></b><sub><i>j</i></sub>) 表示样本<b><i>x</i></b>与近邻间的距离, 对距离按升序排列得到<i>i</i> (<i>i</i>=1, 2, …, <i>k</i>) 。计算样本<b><i>x</i></b>对<i>k</i>个近邻的吸引度<i>θ</i><sub><i>i</i></sub>并构成列向量<i>θ</i>, 并且<i>θ</i><sub><i>i</i></sub>满足以下关系:</p>
                </div>
                <div class="p1">
                    <p id="124">1≥<i>θ</i><sub><i>i</i></sub>&gt;0      (15) </p>
                </div>
                <div class="p1">
                    <p id="125"><i>θ</i><sub><i>i</i></sub>&gt;<i>θ</i><sub><i>i</i>+1</sub>      (16) </p>
                </div>
                <div class="p1">
                    <p id="126"><i>θ</i><sub><i>i</i></sub>= (<i>k</i>-<i>i</i>-0.5) /<i>k</i>      (17) </p>
                </div>
                <div class="p1">
                    <p id="127">当近邻与样本之间的<i>Dist</i> (<b><i>x</i></b>, <b><i>x</i></b><sub><i>j</i></sub>) 越小, <i>i</i>越小, 则<i>θ</i><sub><i>i</i></sub>越大, 即吸引度越大。根据萤火虫方法的思想, 将相似度信息转化为列向量<i>θ</i>。结合式 (15) ～ (17) , 重新构建出标签计数向量<b><i>C</i></b><sub><i>x</i></sub>, 如下所示:</p>
                </div>
                <div class="p1">
                    <p id="128"><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo><mo>*</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>; <i>l</i>∈<b><i>Y</i></b>      (18) </p>
                </div>
                <div class="p1">
                    <p id="130">其中, <b><i>y</i></b> (<i>l</i>) 为<i>k</i>×1的列向量, 若<i>y</i><sub><i>i</i></sub> (<i>l</i>) =1, 表示样本具有标签<i>l</i>, 反之<i>y</i><sub><i>i</i></sub> (<i>l</i>) =-1, 表示不具有标签。由式 (18) 可以看出, 当样本<b><i>x</i></b>的近邻中具有标签<i>l</i>或者离样本较近时, <b><i>C</i></b><sub><i>x</i></sub> (<i>l</i>) 越大, 反之越小。在此结合罚函数的思想, 将不属于类别的信息融入<b><i>C</i></b><sub><i>x</i></sub>中。在计算<b><i>C</i></b><sub><i>x</i></sub> (<i>l</i>) 时, 若<i>y</i><sub><i>i</i></sub> (<i>l</i>) =-1, 那么<i>y</i><sub><i>i</i></sub> (<i>l</i>) *<i>θ</i><sub><i>i</i></sub>&lt;0, 此时对<b><i>C</i></b><sub><i>x</i></sub> (<i>l</i>) 施加惩罚, <i>y</i><sub><i>i</i></sub> (<i>l</i>) *<i>θ</i><sub><i>i</i></sub>作为惩罚加入<b><i>C</i></b><sub><i>x</i></sub> (<i>l</i>) 。加入惩罚后<b><i>C</i></b><sub><i>x</i></sub>携带的信息将增多, 在一定程度上, 加大了两个标签计数向量相应位上的差值, 减小了与真标签之间差值, 从而影响式 (4) 中<b><i>W</i></b>的计算, 进而影响分类效果。改进后的算法伪代码如下:</p>
                </div>
                <div class="p1">
                    <p id="131">算法1 FF-MLLA算法。</p>
                </div>
                <div class="area_img" id="238">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201905011_23800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="171">本文算法对式 (4) 所需的列向量<b><i>w</i></b><sub><i>l</i></sub> (<i>l</i>∈<b><i>Y</i></b>) 使用最小误差平方和函数的方式求解, 如式 (5) 所示。算法1给出了FF-MLLA算法的详细描述。算法首先通过多标签的训练集学习线性分类器所需的参数<b><i>W</i></b>, 在计算参数<b><i>W</i></b>时分别使用ELM与SVD来求解;然后在训练集中找到测试样本<b><i>t</i></b>的<i>k</i>个近邻并计算出标签计数向量<b><i>C</i></b><sub><i>x</i></sub>;最后将学习所得的<b><i>W</i></b>提交给分类器, 预测出测试样本的标签。</p>
                </div>
                <h3 id="172" name="172" class="anchor-tag">3 实验及其结果分析</h3>
                <h4 class="anchor-tag" id="173" name="173">3.1 <b>实验数据集描述</b></h4>
                <div class="p1">
                    <p id="174">为了说明本文算法的有效性, 选取了<i>Computer</i>、<i>Scene</i>等8个包含多个领域的多标签数据集, 数据集来自<i>http</i>://<i>mulan</i>.<i>sourceforge</i>.<i>net</i>/<i>datasets</i>-<i>mlc</i>.<i>html</i>。具体描述见表1。</p>
                </div>
                <div class="area_img" id="175">
                    <p class="img_tit"><b>表</b>1 <b>多标签数据集的详细描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Detailed description of multi</i>-<i>label datasets</i></p>
                    <p class="img_note"></p>
                    <table id="175" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="2"><br />样本数</td><td rowspan="2">标签数</td><td rowspan="2">属性数</td><td rowspan="2">所属领域</td></tr><tr><td><br />训练集</td><td>测试集</td></tr><tr><td><i>Birds</i></td><td>322</td><td>323</td><td>20</td><td>260</td><td><i>Audio</i></td></tr><tr><td><br /><i>Business</i></td><td>2 000</td><td>3 000</td><td>30</td><td>438</td><td><i>Text</i></td></tr><tr><td><br /><i>Emotion</i></td><td>391</td><td>202</td><td>6</td><td>72</td><td><i>Music</i></td></tr><tr><td><br /><i>Flags</i></td><td>129</td><td>65</td><td>7</td><td>19</td><td><i>Images</i></td></tr><tr><td><br /><i>Natural</i></td><td>1 000</td><td>1 000</td><td>5</td><td>294</td><td><i>Images</i></td></tr><tr><td><br /><i>Scene</i></td><td>1 211</td><td>1 196</td><td>6</td><td>294</td><td><i>Text</i></td></tr><tr><td><br /><i>Society</i></td><td>2 000</td><td>3 000</td><td>27</td><td>636</td><td><i>Text</i></td></tr><tr><td><br /><i>Social</i></td><td>2 000</td><td>3 000</td><td>26</td><td>462</td><td><i>Text</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="176" name="176">3.2 <b>实验环境及评价指标</b></h4>
                <div class="p1">
                    <p id="177">实验代码均在<i>Matlab</i>2016<i>a</i>中运行, 硬件环境为<i>Intel Core i</i>5-2525<i>M</i> 2.50 <i>GHz CPU</i>, 8 <i>GB</i>内存;操作系统为<i>Windows</i> 7。本文选取了常用的5种评价准则, 即平均精度 (<i>Average Precision</i>, <i>AP</i>) , 覆盖率 (<i>Coverage</i>, <i>CV</i>) , 海明损失 (<i>Hamming Loss</i>, <i>HL</i>) , 1-错误率 (<i>One Error</i>, <i>OE</i>) 和排序损失 (<i>Ranking Loss</i>, <i>RL</i>) <citation id="233" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>来评价多标签算法性能。将5类评价函数分别简写为:<i>AP</i>↑、<i>CV</i>↓、<i>HL</i>↓、<i>OE</i>↓和<i>RL</i>↓。其中↑为指标数值越高越好, ↓为指标数值越低越好。设多标签分类器<i>h</i> (·) , 预测函数<i>f</i> (·, ·) , 排序函数<i>rank</i><sub><i>f</i></sub>, 多标签数据集<mathml id="178"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mo>|</mo><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>n</mi></mrow></mrow><mo stretchy="false">) </mo><mo stretchy="false">}</mo></mrow></math></mathml>。上述5种评价指标AP、CV、HL、OE和RL形式化定义如下:</p>
                </div>
                <div class="p1">
                    <p id="179">1) Average Precision:评估在特定标签<b><i>y</i></b>∈<b><i>Y</i></b><sub><i>i</i></sub>排列的正确标签的平均分数:</p>
                </div>
                <div class="p1">
                    <p id="180" class="code-formula">
                        <mathml id="180"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>A</mi><mi>Ρ</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">y</mi><mo>∈</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mo stretchy="false">{</mo><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>≤</mo><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>, </mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><mo>∈</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mo>|</mo></mrow></mrow><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="181">2) Coverage:用于度量平均需要多少步才能遍历样本所有的相关标签:</p>
                </div>
                <div class="p1">
                    <p id="182" class="code-formula">
                        <mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>V</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munderover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">y</mi><mo>∈</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mspace width="0.25em" /></munderover></mrow></mstyle><mspace width="0.25em" /><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="183">3) Hamming Loss:用于度量样本在单个标签的真实标签和预测标签的错误匹配情况:</p>
                </div>
                <div class="p1">
                    <p id="184" class="code-formula">
                        <mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mi>L</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false"> (</mo><mi>h</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mi>Y</mi><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle><mrow><mo>|</mo><mrow><mi>h</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>≠</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="185">4) One Error:评估对象最高排位标签并未正确的标签的次数情况:</p>
                </div>
                <div class="p1">
                    <p id="186" class="code-formula">
                        <mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mi>E</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo stretchy="false">[</mo><mo stretchy="false">[</mo></mrow></mstyle><munderover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">y</mi><mo>∈</mo><mi mathvariant="bold-italic">Y</mi></mrow><mspace width="0.25em" /></munderover><mspace width="0.25em" /><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>∉</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="187">5) Ranking Loss:用来考察样本的不相关标签的排序低于相关标签的排序的情况:</p>
                </div>
                <div class="p1">
                    <p id="188" class="code-formula">
                        <mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mi>L</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow></mstyle><mo stretchy="false">|</mo><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>≤</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>, </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>∈</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>×</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">|</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="189" name="189">3.3 <b>算法选择与相关参数设置</b></h4>
                <div class="p1">
                    <p id="190">为了验证所提算法性能, 将<i>FF</i>-<i>MLLA</i>算法与4个经典多标签分类算法做对比实验, 分别是<i>MLKELM</i> (<i>Multi</i>-<i>Label Kernel Extreme Learning Machine</i>) 、<i>ML</i>K<i>NN</i>、<i>IMMLA</i>和<i>RankSVM</i>。在<i>FF</i>-<i>MLLA</i>算法中近邻个数<i>k</i>设为15, 正则化系数设为1, 核函数选择RBF, 核参数设为100, 训练方式选择线性回归拟合。在RankSVM中, 代价参数设为1, 同时选择RBF为核函数。根据文献<citation id="234" type="reference">[<a class="sup">9</a>,<a class="sup">13</a>]</citation>, ML<i>K</i>NN算法中近邻个数<i>k</i>和平滑参数<i>s</i>分别设为15 和1; IMMLA算法中近邻个数<i>k</i>设为15, 且选择一阶Minkowski距离来度量相似度。在使用ELM求解权重时, 将标签计数向量作为输入, 通过十折交叉验证来确定ELM的参数, 得到正则化系数为1, 核参数为100。</p>
                </div>
                <h4 class="anchor-tag" id="191" name="191">3.4 <b>实验结果</b></h4>
                <div class="p1">
                    <p id="192">表2～6给出了本文算法和其他4种算法在8个多标签数据集上实验结果, 最好的结果加粗表示。同时, 每种方法在所有数据集上的平均排位结果列在最后一行, 其中平均排位越小, 算法性能越优。</p>
                </div>
                <div class="area_img" id="193">
                    <p class="img_tit"><b>表</b>2 <b>各算法在</b>8<b>个数据集上的平均精度测试结果</b> (↑)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>AP</i> (↑) <i>results of each approach on</i> 8 <i>datasets</i></p>
                    <p class="img_note"></p>
                    <table id="193" border="1"><tr><td rowspan="2">数据集</td><td colspan="2"><br /><i>FF</i>-<i>MLLA</i></td><td rowspan="2"><i>MLKELM</i></td><td rowspan="2"><i>ML</i>K<i>NN</i></td><td rowspan="2"><i>IMLLA</i></td><td rowspan="2"><i>RankSVM</i></td></tr><tr><td><br /><i>ELM</i></td><td><i>SVD</i></td></tr><tr><td><i>Birds</i></td><td>0.701 3</td><td>0.690 6</td><td>0.684 7</td><td>0.662 5</td><td>0.689 2</td><td>0.611 2</td></tr><tr><td><br /><i>Business</i></td><td>0.872 4</td><td>0.856 2</td><td>0.872 7</td><td>0.871 6</td><td>0.866 2</td><td>0.871 1</td></tr><tr><td><br /><i>Emotion</i></td><td>0.807 7</td><td>0.791 3</td><td>0.770 1</td><td>0.784 7</td><td>0.787 8</td><td>0.750 3</td></tr><tr><td><br /><i>Flags</i></td><td>0.814 6</td><td>0.791 0</td><td>0.740 9</td><td>0.796 5</td><td>0.789 4</td><td>0.800 7</td></tr><tr><td><br /><i>Natural</i></td><td>0.798 6</td><td>0.792 7</td><td>0.764 3</td><td>0.799 1</td><td>0.798 5</td><td>0.768 9</td></tr><tr><td><br /><i>Scene</i></td><td>0.877 2</td><td>0.862 5</td><td>0.859 0</td><td>0.858 2</td><td>0.853 3</td><td>0.537 2</td></tr><tr><td><br /><i>Society</i></td><td>0.586 2</td><td>0.580 8</td><td>0.583 3</td><td>0.398 4</td><td>0.406 4</td><td>0.591 7</td></tr><tr><td><br /><i>Social</i></td><td>0.692 6</td><td>0.669 5</td><td>0.706 3</td><td>0.688 4</td><td>0.682 6</td><td>0.690 1</td></tr><tr><td><br />平均排位</td><td>1.500 0</td><td>3.750 0</td><td>3.625 0</td><td>3.750 0</td><td>4.250 0</td><td>4.125 0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="194">
                    <p class="img_tit"><b>表</b>3 <b>各算法在</b>8<b>个数据集上的覆盖率测试结果</b> (↓)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>CV</i> (↓) <i>results of each approach on</i> 8 <i>datasets</i></p>
                    <p class="img_note"></p>
                    <table id="194" border="1"><tr><td rowspan="2">数据集</td><td colspan="2"><br /><i>FF</i>-<i>MLLA</i></td><td rowspan="2"><i>MLKELM</i></td><td rowspan="2"><i>ML</i>K<i>NN</i></td><td rowspan="2"><i>IMLLA</i></td><td rowspan="2"><i>RankSVM</i></td></tr><tr><td><br /><i>ELM</i></td><td><i>SVD</i></td></tr><tr><td><i>Birds</i></td><td>3.111 5</td><td>3.764 7</td><td>3.318 8</td><td>3.439 6</td><td>3.774 0</td><td>4.244 6</td></tr><tr><td><br /><i>Business</i></td><td>2.593 0</td><td>3.475 7</td><td>2.133 0</td><td>2.378 7</td><td>2.908 6</td><td>2.287 7</td></tr><tr><td><br /><i>Emotion</i></td><td>1.831 6</td><td>1.811 8</td><td>2.029 7</td><td>1.871 3</td><td>1.866 3</td><td>2.242 6</td></tr><tr><td><br /><i>Flags</i></td><td>3.676 9</td><td>3.892 3</td><td>4.292 3</td><td>3.846 2</td><td>3.923 1</td><td>3.692 3</td></tr><tr><td><br /><i>Natural</i></td><td>0.951 0</td><td>0.964 0</td><td>1.024 0</td><td>0.937 0</td><td>0.943 0</td><td>1.055 0</td></tr><tr><td><br /><i>Scene</i></td><td>0.479 9</td><td>0.519 2</td><td>0.502 5</td><td>0.527 6</td><td>0.543 5</td><td>1.927 3</td></tr><tr><td><br /><i>Society</i></td><td>6.150 0</td><td>6.190 0</td><td>5.306 3</td><td>5.835 7</td><td>6.237 7</td><td>5.316 0</td></tr><tr><td><br /><i>Social</i></td><td>4.329 0</td><td>5.427 0</td><td>3.046 3</td><td>3.724 0</td><td>4.685 7</td><td>3.336 0</td></tr><tr><td><br />平均排位</td><td>2.500 0</td><td>4.250 0</td><td>2.875 0</td><td>3.000 0</td><td>4.375 0</td><td>4.000 0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="195">
                    <p class="img_tit"><b>表</b>4 <b>各算法在</b>8<b>个数据集上的海明损失测试结果</b> (↓)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 4 <i>HL</i> (↓) <i>results of each approach on</i> 8 <i>datasets</i></p>
                    <p class="img_note"></p>
                    <table id="195" border="1"><tr><td rowspan="2">数据集</td><td colspan="2"><br /><i>FF</i>-<i>MLLA</i></td><td rowspan="2"><i>MLKELM</i></td><td rowspan="2"><i>ML</i>K<i>NN</i></td><td rowspan="2"><i>IMLLA</i></td><td rowspan="2"><i>RankSVM</i></td></tr><tr><td><br /><i>ELM</i></td><td><i>SVD</i></td></tr><tr><td><i>Birds</i></td><td>0.052 7</td><td>0.056 0</td><td>0.054 5</td><td>0.057 7</td><td>0.055 8</td><td>0.089 3</td></tr><tr><td><br /><i>Business</i></td><td>0.027 1</td><td>0.027 6</td><td>0.028 7</td><td>0.028 0</td><td>0.027 8</td><td>0.029 1</td></tr><tr><td><br /><i>Emotion</i></td><td>0.207 0</td><td>0.201 3</td><td>0.254 1</td><td>0.217 8</td><td>0.203 0</td><td>0.294 5</td></tr><tr><td><br /><i>Flags</i></td><td>0.523 0</td><td>0.525 2</td><td>0.540 7</td><td>0.727 4</td><td>0.523 1</td><td>0.800 7</td></tr><tr><td><br /><i>Natural</i></td><td>0.163 2</td><td>0.167 8</td><td>0.204 6</td><td>0.174 6</td><td>0.166 8</td><td>0.185 4</td></tr><tr><td><br /><i>Scene</i></td><td>0.078 5</td><td>0.085 5</td><td>0.107 0</td><td>0.091 4</td><td>0.089 6</td><td>0.176 6</td></tr><tr><td><br /><i>Society</i></td><td>0.055 9</td><td>0.056 9</td><td>0.055 0</td><td>0.058 2</td><td>0.057 2</td><td>0.060 3</td></tr><tr><td><br /><i>Social</i></td><td>0.024 8</td><td>0.025 8</td><td>0.024 7</td><td>0.025 6</td><td>0.025 8</td><td>0.029 0</td></tr><tr><td><br />平均排位</td><td>1.500 0</td><td>2.812 5</td><td>3.625 0</td><td>4.250 0</td><td>2.937 5</td><td>5.875 0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="196">
                    <p class="img_tit"><b>表</b>5 <b>各算法在</b>8<b>个数据集上的</b>1-<b>错误率测试结果</b> (↓)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 5 <i>OE</i> (↓) <i>results of each approach on</i> 8 <i>datasets</i></p>
                    <p class="img_note"></p>
                    <table id="196" border="1"><tr><td rowspan="2">数据集</td><td colspan="2"><br /><i>FF</i>-<i>MLLA</i></td><td rowspan="2"><i>MLKELM</i></td><td rowspan="2"><i>ML</i>K<i>NN</i></td><td rowspan="2"><i>IMLLA</i></td><td rowspan="2"><i>RankSVM</i></td></tr><tr><td><br /><i>ELM</i></td><td><i>SVD</i></td></tr><tr><td><i>Birds</i></td><td>0.374 6</td><td>0.402 4</td><td>0.396 3</td><td>0.452 0</td><td>0.408 6</td><td>0.532 5</td></tr><tr><td><br /><i>Business</i></td><td>0.123 6</td><td>0.127 6</td><td>0.136 7</td><td>0.130 0</td><td>0.129 6</td><td>0.136 7</td></tr><tr><td><br /><i>Emotion</i></td><td>0.262 3</td><td>0.336 6</td><td>0.326 7</td><td>0.321 8</td><td>0.321 8</td><td>0.316 8</td></tr><tr><td><br /><i>Flags</i></td><td>0.246 1</td><td>0.261 5</td><td>0.276 9</td><td>0.230 8</td><td>0.261 5</td><td>0.246 2</td></tr><tr><td><br /><i>Natural</i></td><td>0.305 0</td><td>0.317 0</td><td>0.371 0</td><td>0.307 0</td><td>0.307 0</td><td>0.350 0</td></tr><tr><td><br /><i>Scene</i></td><td>0.200 6</td><td>0.225 7</td><td>0.236 6</td><td>0.233 3</td><td>0.240 8</td><td>0.681 4</td></tr><tr><td><br /><i>Society</i></td><td>0.458 6</td><td>0.468 0</td><td>0.494 7</td><td>0.474 6</td><td>0.475 7</td><td>0.483 0</td></tr><tr><td><br /><i>Social</i></td><td>0.398 0</td><td>0.409 7</td><td>0.398 3</td><td>0.408 7</td><td>0.407 3</td><td>0.433 3</td></tr><tr><td><br />平均排位</td><td>1.125 0</td><td>3.437 5</td><td>4.562 5</td><td>3.375 0</td><td>3.437 5</td><td>5.062 5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="239">实验结果说明:如表2所示, FF-MLLA (ELM) 在Birds、Emotion、Flags、Scene等数据集上平均精度指标最优。在表3中, FF-MLLA (SVD) 在Emotion数据集上有着较好的覆盖率, FF-MLLA (ELM) 在Birds、Flags、Scene等数据集上也取得了不错的覆盖率。由表4可得, 在海明损失指标上, FF-MLLA在Birds、Emotion、Flags、Society最优。表5表明FF-MLLA (ELM) 在1-错误率指标上获得了良好的效果, 在Birds、Emotion、Scene、Society上最优。如表6所示:FF-MLLA (ELM) 算法在平均评价指标中, 除Computer、Reference、Society数据集外, 均为最优。</p>
                </div>
                <div class="p1">
                    <p id="197">综合表2～表6的平均排位易知, <i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 在海明损失指标上排第二, 在平均精度与覆盖率指标上排第三, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 在5个指标的平均排位上均为最优。</p>
                </div>
                <div class="area_img" id="198">
                    <p class="img_tit"><b>表</b>6 <b>各算法在</b>8<b>个数据集上的排序损失测试结果</b> (↓)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 6 <i>RL</i> (↓) <i>results of each approach on</i> 8 <i>datasets</i></p>
                    <p class="img_note"></p>
                    <table id="198" border="1"><tr><td rowspan="2">数据集</td><td colspan="2"><br /><i>FF</i>-<i>MLLA</i></td><td rowspan="2"><i>MLKELM</i></td><td rowspan="2"><i>ML</i>K<i>NN</i></td><td rowspan="2"><i>IMLLA</i></td><td rowspan="2"><i>RankSVM</i></td></tr><tr><td><br /><i>ELM</i></td><td><i>SVD</i></td></tr><tr><td><i>Birds</i></td><td>0.114 0</td><td>0.137 1</td><td>0.119 5</td><td>0.128 9</td><td>0.137 2</td><td>0.162 9</td></tr><tr><td><br /><i>Business</i></td><td>0.047 3</td><td>0.064 8</td><td>0.037 2</td><td>0.043 1</td><td>0.056 2</td><td>0.040 2</td></tr><tr><td><br /><i>Emotion</i></td><td>0.160 2</td><td>0.164 5</td><td>0.191 4</td><td>0.168 2</td><td>0.169 3</td><td>0.224 4</td></tr><tr><td><br /><i>Flags</i></td><td>0.202 0</td><td>0.240 2</td><td>0.323 1</td><td>0.239 4</td><td>0.242 3</td><td>0.226 2</td></tr><tr><td><br /><i>Natural</i></td><td>0.168 9</td><td>0.171 1</td><td>0.188 4</td><td>0.165 6</td><td>0.166 7</td><td>0.196 5</td></tr><tr><td><br /><i>Scene</i></td><td>0.074 9</td><td>0.082 7</td><td>0.079 6</td><td>0.084 9</td><td>0.087 8</td><td>0.365 1</td></tr><tr><td><br /><i>Society</i></td><td>0.154 2</td><td>0.156 8</td><td>0.130 0</td><td>0.149 3</td><td>0.158 7</td><td>0.127 8</td></tr><tr><td><br /><i>Social</i></td><td>0.082 4</td><td>0.106 9</td><td>0.058 0</td><td>0.070 6</td><td>0.089 7</td><td>0.061 9</td></tr><tr><td><br />平均排位</td><td>2.375 0</td><td>4.250 0</td><td>3.000 0</td><td>2.875 0</td><td>4.625 0</td><td>3.875 0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="199">由于<i>k</i>值的取值不同对本文算法有一定影响, 若取太小, 影响分类精度, 若取太大噪声增多, 本文的<i>k</i>在区间<citation id="235" type="reference">[<a class="sup">6</a>,<a class="sup">17</a>]</citation>中选取。图2展示了在Birds数据集上<i>k</i>值对5个指标的影响。</p>
                </div>
                <div class="area_img" id="200">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905011_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 k值对5个指标的影响" src="Detail/GetImg?filename=images/JSJY201905011_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>k</i>值对5个指标的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905011_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Influence of <i>k</i> on five indicators</p>

                </div>
                <div class="p1">
                    <p id="201">从图2中可看出ML<i>K</i>NN与IMLLA在平均精度、海明损失指标上的波动较大, 即不同的<i>k</i>值对其影响较大, 在其余的3个指标上比较稳定。如图2 (d) 所示, 4个算法在1-错误率指标上都比较稳定。而本文算法在5个指标上相对于ML<i>K</i>NN与IMLLA都相对稳定且结果较优。</p>
                </div>
                <h4 class="anchor-tag" id="202" name="202">3.5 <b>统计假设检验及稳定性分析</b></h4>
                <div class="p1">
                    <p id="203">通过3.4节的分析不难发现, 本文算法获得了不错的效果。下面将通过统计假设检验与稳定性分析进一步说明本文算法的合理性。</p>
                </div>
                <div class="p1">
                    <p id="204">统计假设检验 在上述8个数据集上采用显著性水平为5%的<i>Nemenyi</i>检验<citation id="236" type="reference"><link href="51" rel="bibliography" /><link href="53" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>来对比<i>FF</i>-<i>MLLA</i>算法与其他对比算法。如果两个算法在所有数据集上的平均排位的差值小于或者等于临界差值 (<i>Critical Difference</i>, <i>CD</i>) , 那么这两个算法之间没有显著性差异, 反之存在显著性差异。如图3所示, 在最上行为临界值<i>CD</i>=2.665 9时, 若两个算法之间没有显著性差异则用实线相连, 反之不连线。在图3中从左到右, 算法性能依次降低。</p>
                </div>
                <div class="p1">
                    <p id="205">在平均精度评价指标上, 如图3 (<i>a</i>) 所示, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 、<i>MLKELM</i>、<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 以及<i>ML</i>K<i>NN</i>算法没有显著差异, <i>MLKELM</i>、<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 、<i>ML</i>K<i>NN</i>、<i>RankSVM</i>以及<i>IMLLA</i>算法没有显著性差异。在覆盖率评价指标上, 如图3 (<i>b</i>) 所示, 存在显著差异, 且<i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 最优, <i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 优于<i>IMLLA</i>。在海明损失评价指标上, 如图3 (<i>c</i>) 所示, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 、<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 、<i>IMLLA</i>与<i>MLKELM</i>算法没有显著差异, <i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 、<i>IMLLA</i>、<i>MLKELM</i>与<i>ML</i>K<i>NN</i>没有显著差异, <i>MLKELM</i>、<i>ML</i>K<i>NN</i>以及<i>RankSVM</i>算法无显著性差异。在1-错误率评价指标上, 如图3 (<i>d</i>) 所示, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 、<i>ML</i>K<i>NN</i>、<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 与<i>IMLLA</i>算法没有显著差异, <i>ML</i>K<i>NN</i>、<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 、<i>IMLLA</i>、<i>MLKELM</i>与<i>RankSVM</i>算法也没有显著差异。在排序损失评价指标上, 如图3 (<i>e</i>) 所示, 存在显著差异, 且<i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 最优。</p>
                </div>
                <div class="p1">
                    <p id="206">虽然通过实验结果与统计假设检验说明了本文算法的有效性与合理性, 但并未说明其稳定性。下面将通过稳定性分析来说明本文算法的稳定性。</p>
                </div>
                <div class="p1">
                    <p id="207">稳定性分析 为验证各多标签算法稳定性, 采用蜘蛛网图进行算法稳定性表示<citation id="237" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>。由于预测分类各评价指标在不同数据集中结果差异较大, 故均将结果标准化为[0.1, 0.5]区间。最终采用标准化后数值来衡量算法稳定指数。图4显示了8个数据集上6个算法的稳定性。</p>
                </div>
                <div class="area_img" id="208">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905011_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 算法综合性能比较" src="Detail/GetImg?filename=images/JSJY201905011_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 算法综合性能比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905011_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Comprehensive performance comparison of algorithms</i></p>

                </div>
                <div class="area_img" id="209">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905011_209.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 具有不同评估指标的8个基准多标签数据集测试获得的稳定性指数值" src="Detail/GetImg?filename=images/JSJY201905011_209.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 具有不同评估指标的8个基准多标签数据集测试获得的稳定性指数值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905011_209.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Stability index values obtained on</i> 8 <i>benchmark multi</i>-<i>label datasets with different evaluation metrics</i></p>

                </div>
                <div class="p1">
                    <p id="210">从图4中, 可以观察到:在图4 (<i>a</i>) 中, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 在8个数据集上的平均精度的稳定指数值在[0.42, 0.5]间获得了相当稳定的效果。而<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 不是非常稳定, 但多数数据集上其稳定性指标比其他算法略优。在图4 (<i>b</i>) 中覆盖率指标下, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 在3个数据集上的稳定指数值在[0.42, 0.5], 稳定性不错, <i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 部分数据集上较稳定, 都相对于<i>IMLLA</i>较优。在图4 (<i>c</i>) 中海明损失指标下, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 可以在7个数据集上得到更稳定的结果, 并且稳定指数值在[0.42, 0.5]区间。剩余的一个数据集稳定指数值也比<i>ML</i>K<i>NN</i>、<i>RankSVM</i>和<i>MLKELM</i>都要稳定。<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 在<i>Emotion</i>数据集上最优, 在其他数据集上也比较稳定。在图4 (<i>d</i>) 中1-错误率指标下, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 可以在8个数据集上提供更稳定的解决方案, 其稳定指数值也在[0.42, 0.5]。<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 在<i>Scene</i>、<i>Society</i>与<i>Business</i>数据集上次优。在图4 (<i>e</i>) 中排序损失指标下, <i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 在4个数据集上实现了稳定的解决方案。<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 虽不是非常稳定, 但多数数据集上其稳定性指标比其他算法略优。对于图4所示的结果表明<i>FF</i>-<i>MLLA</i> (<i>ELM</i>) 有更好的稳定性, 其稳定指数值波动不大且较优。<i>FF</i>-<i>MLLA</i> (<i>SVD</i>) 的结果在部分数据集上不稳定, 但多数数据集上较稳定且结果略优。</p>
                </div>
                <h3 id="211" name="211" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="212">本文针对基于<i>k</i>近邻的多标签相关性算法未考虑样本分布问题进行了研究, 运用萤火虫方法的思想将相似度信息与近邻标签信息进行融合。萤火虫方法是源于模拟自然界萤火虫在晚上的群聚活动的自然现象而提出的, 其计算效率高、鲁棒性强, 能够很好地将相似度信息与标签信息融合。本文算法在重构标签计数向量后分别使用了奇异值分解与核极限学习机进行权重求解, 再进行线性分类。实验结果表明了本文提出的FF-MLLA算法具有不错的效果和较好的稳定性。</p>
                </div>
                <div class="p1">
                    <p id="213">虽然将相似度信息与近邻标签信息相结合的方法一定程度上提升了模型的分类精度, 但与预期效果之间还存在一定差距, 因此如何从近邻空间提取出比相似度信息更为有效的信息来辅助分类器进行分类是今后研究的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM857AEC4E4ECC23C23EF3102C8F0C8A4E&amp;v=MTA0NjhRbGZCckxVMDV0cGh6THU3d0tFPU5pZklZN3U5R2FDNTNJc3dZSjU4ZjM0NnZCUVFuMGwrU1gvZzN4cERlY0djTkw3cUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPRw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> GIBAJA E, VENTURA S.A tutorial on multilabel learning[J].ACM Computing Surveys, 2015, 47 (3) :1-38.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201409006&amp;v=Mjk4ODF5ZlRiTEc0SDlYTXBvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURtVjcvQk4=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 何志芬, 杨明, 刘会东.多标记分类和标记相关性的联合学习[J].软件学报, 2014, 25 (9) :1967-1981. (HE Z F, YANG M, LIU H D.Joint learning of multi-label classification and label correlations[J].Journal of Software, 2014, 25 (9) :1967-1981.) 
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for extreme multi-label text classification">

                                <b>[3]</b> LIU J, CHANG W, WU Y, et al.Deep learning for extreme multi-label text classification[C]// Proceedings of the 40th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval.New York:ACM, 2017:115-124.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical multi-label gene function prediction using adaptive mutation in crowding niching">

                                <b>[4]</b> KORDMAHALLEH M M, HOMAIFAR A, DUKKA B K C.Hierarchical multi-label gene function prediction using adaptive mutation in crowding niching[C]// Proceedings of the 13th IEEE International Conference on BioInformatics and BioEngineering.Piscataway, NJ:IEEE, 2013:1-6.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Block-Row Sparse Multiview Multilabel Learning for Image Classification">

                                <b>[5]</b> ZHU X, LI X, ZHANG S.Block-row sparse multiview multilabel learning for image classification[J].IEEE Transactions on Cybernetics, 2016, 46 (2) :450.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-label image recognition by recurrently discovering attentional regions">

                                <b>[6]</b> WANG Z, CHEN T, LI G, et al.Multi-label image recognition by recurrently discovering attentional regions[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2017:464-472.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards a universal marketplace over the web:Statistical multi-label classification of service provider forms with simulated annealing">

                                <b>[7]</b> OZONAT K M, YOUNG D E.Towards a universal marketplace over the Web:statistical multi-label classification of service provider forms with simulated annealing[C]// Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2009:1295-1304.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES8BB9B4C24E43506B1BD620A56E385822&amp;v=MDY2MzcveEhZSjRMRDNrNXlXUVNtRXQ3U24rVHFSUkFlcnFSVGJpZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMdTd3S0U9TmlmT2ZidktiTmkrcQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> HOU S, ZHOU S, CHEN L, et al.Multi-label learning with label relevance in advertising video[J].Neurocomputing, 2016, 171 (C) :932-948.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740311&amp;v=MjM5MTZaZVp0RmlubFVyM0lLRjBRYWhzPU5pZk9mYks3SHRETnFZOUZZKzhQRDMwNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> BOUTELL M R, LUO J, SHEN X, et al.Learning multi-label scene classification [J].Pattern Recognition, 2004, 37 (9) :1757-1771.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739502&amp;v=MjEwMDRLRjBRYWhzPU5pZk9mYks3SHRETnFZOUZZK2dHQ1h3N29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> ZHANG M, ZHOU Z.ML-KNN:a lazy learning approach to multi-label learning[J].Pattern Recognition, 2007, 40 (7) :2038-2048.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB695B771550A53356F0493031209498E&amp;v=MjgwMzVIWWZPR1FsZkJyTFUwNXRwaHpMdTd3S0U9TmlmT2ZjRytGOVMrcUloRVllNFBmWGs2ekJNVm5EOTVRWHppcnhNM2VidVFUTExxQ09OdkZTaVdXcjdKSUZwbWFCdQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> LEE J, KIM H, KIM N R, et al.An approach for multi-label classification by directed acyclic graph with label correlation maximization[J].Information Sciences, 2016, 351 (C) :101-114.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A kernel method for multi-labeled classification">

                                <b>[12]</b> ELISSEEFF A E, WESTON J.A kernel method for multi-labelled classification[C]// Proceedings of the 14th International Conference on Neural Information Processing Systems:Natural and Synthetic.Cambridge, MA:MIT Press, 2002:681-687.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913101&amp;v=MTkyMTAvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGMFFhaHM9TmlmT2ZiSzdIdEROcW85RWJlb01EWHc0b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> HUANG G, ZHU Q, SIEW C K.Extreme learning machine:theory and applications[J].Neurocomputing, 2006, 70 (1/2/3) :489-501.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201805004&amp;v=MDM4MDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVY3L0JLRDdZYkxHNEg5bk1xbzlGWUlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 王一宾, 程玉胜, 何月, 等.回归核极限学习机的多标记学习算法[J].模式识别与人工智能, 2018, 31 (5) :419-430. (WANG Y B, CHENG Y S, HE Y, et al.Multi-label learning algorithm of regression kernel extreme learning machine[J].Pattern Recognition and Artificial Intelligence, 2018, 31 (5) :419-430.) 
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201211002&amp;v=MTA2NDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURtVjcvQkx5dlNkTEc0SDlQTnJvOUZab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 张敏灵.一种新型多标记懒惰学习算法[J].计算机研究与发展, 2012, 49 (11) :2271-2282. (ZHANG M L.An improved multi-label lazy learning approach[J].Journal of Computer Research and Development, 2012, 49 (11) :2271-2282.) 
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCW&amp;filename=SJCW90964E44BB6AC7F55663F1EEB1FA7BBD&amp;v=Mjk4MTRxNEY5Zkkyb3RCRnBrSmZROCt1Uk1XN0RsK1BuNlgyV0EwRDhPVE44anJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THU3d0tFPU5pZkllYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> YANG X, HE X.Firefly algorithm:recent advances and applications[J].International Journal of Swarm Intelligence, 2013, 1 (1) :36-50.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Solving the multi-objective path planning problem in mobile robotics with a firefly-based approach">

                                <b>[17]</b> HIDALGOPANIAGUA A, MIDUEL A V, JOAQUIN F, et al.Solving the multi-objective path planning problem in mobile robotics with a firefly-based approach[J].Soft Computing, 2017, 21 (4) :1-16.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF29F0E0ADCFC5D7AE320D6BD3D1C6A70&amp;v=MDY1NzZOTDJmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekx1N3dLRT1OaWZPZmNXNkY2Zk0ybzgwRUpoNWYzbE55R2RtNlQxOVBIbVEyQkZCZU1HUw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> LEI Y, ZHAO D, CAI H B.Prediction of length-of-day using extreme learning machine[J].Geodesy and Geodynamics, 2015, 6 (2) :151-159.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QHDY201702004&amp;v=MjE0MjhIOWJNclk5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1WNy9CTkNYUGQ3RzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> WANG Z, XIN J, TIAN S, et al.Distributed and weighted extreme learning machine for imbalanced big data learning[J].Tsinghua Science and Technology, 2017, 22 (2) :160-173.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAF1AF2BD1806F907B59AD1CFBDD2C2BD&amp;v=MTQwODU2QzZyZjB4WmVNUENnb3d6eEZoN3pZTVBINlIybUJCRGJEblI4anJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THU3d0tFPU5pZk9mY0xPSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> LUO F F, GUO W Z, YU Y L, et al.A multi-label classification algorithm based on kernel extreme learning machine[J].Neurocomputing, 2017, 260:313-320.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201907020&amp;v=MjgwNzN5RG1WNy9CTHo3U1pMRzRIOWpNcUk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 杨明极, 马池, 王娅, 等.一种改进<i>K</i>-means 聚类的FCMM 算法[J/OL].计算机应用研究, 2019, 36 (7) [2018- 04- 12].http://www.arocmag.com/article/02-2019-07-006.html. (YANG M J, MA C, WANG Y, et al.Algorithm named FCMM to improve K-means clustering algorithm[J/OL].Application Research of Computers, 2019, 36 (7) [2018- 04- 12].http://www.arocmag.com/article/02-2019-07-006.html.) 
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES135235F971946B0F45C78B7899ADCE20&amp;v=MTA3MjdXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMdTd3S0U9TmlmT2ZiSzdHOVBQcXZsTVkrb0dDSHBMejJBWDcweDZRQTNscEJzOENNYm5NTGlmQ09OdkZTaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> WANG H, WANG W, ZHOU X, et al.Firefly algorithm with neighborhood attraction[J].Information Sciences, 2017, 382/383:374-387.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201504002&amp;v=MDM4MDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1WNy9CTHo3QmI3RzRIOVRNcTQ5RlpvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 程美英, 倪志伟, 朱旭辉.萤火虫优化算法理论研究综述[J].计算机科学, 2015, 42 (4) :19-24. (CHENG M Y, NI Z W, ZHU X H.Overview on glowworm swarm optimization or firefly algorithm[J].Computer Science, 2015, 42 (4) :19-24.) 
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A review on multi-label learning algorithms">

                                <b>[24]</b> ZHANG M L, ZHOU Z H.A Review on multi-label learning algorithms[J].IEEE Transactions on Knowledge &amp; Data Engineering, 2014, 26 (8) :1819-1837.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Statistical comparisons of classifiers over multiple data sets">

                                <b>[25]</b> DEMSAR J.Statistical comparisons of classifiers over multiple data sets[J].Journal of Machine Learning Research, 2006, 7 (1) :1-30.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LIFT:Multi-label learning with label-specific features">

                                <b>[26]</b> ZHANG M, WU L.Lift:Multi-label learning with label-specific features[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (1) :107-120.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES22A4CF21D5805EE3B00914C749BDE4F4&amp;v=MDkyODF0cGh6THU3d0tFPU5pZk9mYkc2YjlXLzJZMUVFTzRIREhsTXVoVmg2ajkwU1h1UnF4WThDOGJoUWN5YkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> LIN Y, LI Y, WANG C, et al.Attribute reduction for multi-label learning with fuzzy rough set[J].Knowledge-Based Systems, 2018, 152:51-56.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905011" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905011&amp;v=MTE4NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG1WNy9CTHo3QmQ3RzRIOWpNcW85RVpZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
