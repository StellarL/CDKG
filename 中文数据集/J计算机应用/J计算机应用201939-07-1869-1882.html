<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136658971096250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907002%26RESULT%3d1%26SIGN%3dpXDhMLZyTtkGQ9qI5FJTAy9spXE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907002&amp;v=MTMyOTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMTHo3QmQ3RzRIOWpNcUk5RlpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#279" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#288" data-title="1 聚类分析过程 ">1 聚类分析过程</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#291" data-title="1.1 &lt;b&gt;特征选择或变换&lt;/b&gt;">1.1 <b>特征选择或变换</b></a></li>
                                                <li><a href="#293" data-title="1.2 &lt;b&gt;聚类算法选择或设计&lt;/b&gt;">1.2 <b>聚类算法选择或设计</b></a></li>
                                                <li><a href="#295" data-title="1.3 &lt;b&gt;聚类结果评价与物理解析&lt;/b&gt;">1.3 <b>聚类结果评价与物理解析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#298" data-title="2 相似性度量 ">2 相似性度量</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#300" data-title="2.1 &lt;b&gt;连续性变量的相似性度量&lt;/b&gt;">2.1 <b>连续性变量的相似性度量</b></a></li>
                                                <li><a href="#339" data-title="2.2 &lt;b&gt;离散变量的相似性度量&lt;/b&gt;">2.2 <b>离散变量的相似性度量</b></a></li>
                                                <li><a href="#352" data-title="2.3 &lt;b&gt;混合变量的相似性度量&lt;/b&gt;">2.3 <b>混合变量的相似性度量</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#359" data-title="3 聚类算法分类 ">3 聚类算法分类</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#362" data-title="4 小数据聚类算法 ">4 小数据聚类算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#364" data-title="4.1 &lt;b&gt;传统聚类&lt;/b&gt;">4.1 <b>传统聚类</b></a></li>
                                                <li><a href="#390" data-title="4.2 &lt;b&gt;智能聚类&lt;/b&gt;">4.2 <b>智能聚类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#415" data-title="5 大数据聚类算法 ">5 大数据聚类算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#420" data-title="5.1 &lt;b&gt;分布式聚类&lt;/b&gt;">5.1 <b>分布式聚类</b></a></li>
                                                <li><a href="#424" data-title="5.2 &lt;b&gt;并行聚类&lt;/b&gt;">5.2 <b>并行聚类</b></a></li>
                                                <li><a href="#426" data-title="5.3 &lt;b&gt;高维聚类&lt;/b&gt;">5.3 <b>高维聚类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#429" data-title="6 算法对比分析 ">6 算法对比分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#438" data-title="7 聚类结果评价 ">7 聚类结果评价</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#445" data-title="8 结语 ">8 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#290" data-title="图1 聚类分析过程">图1 聚类分析过程</a></li>
                                                <li><a href="#338" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;连续性变量相似性度量及其应用&lt;/b&gt;"><b>表</b>1 <b>连续性变量相似性度量及其应用</b></a></li>
                                                <li><a href="#343" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;样本之间二值型变量相似性度量数值&lt;/b&gt;"><b>表</b>2 <b>样本之间二值型变量相似性度量数值</b></a></li>
                                                <li><a href="#361" data-title="图2 聚类算法分类">图2 聚类算法分类</a></li>
                                                <li><a href="#385" data-title="图3 层次聚类构建示意图">图3 层次聚类构建示意图</a></li>
                                                <li><a href="#405" data-title="图4 传统聚类与序列聚类的比较">图4 传统聚类与序列聚类的比较</a></li>
                                                <li><a href="#417" data-title="图5 多机聚类的硬件架构">图5 多机聚类的硬件架构</a></li>
                                                <li><a href="#419" data-title="图6 多机聚类算法的执行过程">图6 多机聚类算法的执行过程</a></li>
                                                <li><a href="#422" data-title="图7 &lt;i&gt;MapReduce&lt;/i&gt;的工作流程">图7 <i>MapReduce</i>的工作流程</a></li>
                                                <li><a href="#437" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;典型聚类算法的性能对比&lt;/b&gt;"><b>表</b>3 <b>典型聚类算法的性能对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="559">


                                    <a id="bibliography_1" title=" ALDENDERFER M S, BLASHFIELD R K.Cluster Analysis [M].Los Angeles:Sage Publications, 1984:2-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cluster Analysis">
                                        <b>[1]</b>
                                         ALDENDERFER M S, BLASHFIELD R K.Cluster Analysis [M].Los Angeles:Sage Publications, 1984:2-12.
                                    </a>
                                </li>
                                <li id="561">


                                    <a id="bibliography_2" title=" AGGARWAL C C, REDDY C K.Data Clustering:Algorithms and Applications [M].London:Taylor and Francis Group, 2014:4-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data Clustering:Algorithms and Applications">
                                        <b>[2]</b>
                                         AGGARWAL C C, REDDY C K.Data Clustering:Algorithms and Applications [M].London:Taylor and Francis Group, 2014:4-7.
                                    </a>
                                </li>
                                <li id="563">


                                    <a id="bibliography_3" title=" EVERITT B, LANDAU S, LEESE M.Cluster Analysis [M].4th ed.London:Arnold, 2001:144-201." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cluster Analysis">
                                        <b>[3]</b>
                                         EVERITT B, LANDAU S, LEESE M.Cluster Analysis [M].4th ed.London:Arnold, 2001:144-201.
                                    </a>
                                </li>
                                <li id="565">


                                    <a id="bibliography_4" title=" BARALDI A, ALPAYDIN E.Constructive feedforward ART clustering networks—Part I and II [J].IEEE Transactions on Neural Networks, 2002, 13 (3) :645-677." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Constructive feedforward ART clustering networks. II">
                                        <b>[4]</b>
                                         BARALDI A, ALPAYDIN E.Constructive feedforward ART clustering networks—Part I and II [J].IEEE Transactions on Neural Networks, 2002, 13 (3) :645-677.
                                    </a>
                                </li>
                                <li id="567">


                                    <a id="bibliography_5" title=" JAIN A K, MURTY M N, FLYNN P J.Data clustering:a review[J].ACM Computing Surveys, 1999, 31 (3) :264-323." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000017469&amp;v=Mjc2MzBvd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmlmSVk3SzdIdGpOcjQ5RlpPb0lDSA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         JAIN A K, MURTY M N, FLYNN P J.Data clustering:a review[J].ACM Computing Surveys, 1999, 31 (3) :264-323.
                                    </a>
                                </li>
                                <li id="569">


                                    <a id="bibliography_6" title=" HANSEN P, JAUMARD B.Cluster analysis and mathematical programming[J].Mathematical Programming, 1997, 79:191-215." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001140461&amp;v=MDUxODFWMD1OajdCYXJPNEh0SE5yb3RGWU8wT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFZMM0lK&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         HANSEN P, JAUMARD B.Cluster analysis and mathematical programming[J].Mathematical Programming, 1997, 79:191-215.
                                    </a>
                                </li>
                                <li id="571">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     章永来.基于聚类的社区居民健康指数预测模型研究[D].北京:中国科学院大学, 2015. (ZHANG Y L.Research on prediction model of health index for community residents based on clustering[D].Beijing:University of Chinese Academy of Sciences, 2015.) </a>
                                </li>
                                <li id="573">


                                    <a id="bibliography_8" title=" ZHOU X, ZHANG Y, SHI M, et al.Early detection of liver disease using data visualisation and classification method[J].Biomedical Signal Processing and Control, 2014, 11:27-35." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300158296&amp;v=MDUwODZubFVyM0lKMXdVYnhFPU5pZk9mYks4SHRMT3JJOUZaZTRIRG5VL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         ZHOU X, ZHANG Y, SHI M, et al.Early detection of liver disease using data visualisation and classification method[J].Biomedical Signal Processing and Control, 2014, 11:27-35.
                                    </a>
                                </li>
                                <li id="575">


                                    <a id="bibliography_9" title=" ZHANG Y, ZHOU X, SHI H, et al.Corrosion pitting damage detection of rolling bearings using data mining techniques[J].International Journal of Modeling, Identification and Control, 2015, 24 (3) :235-243." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCW&amp;filename=SJCWD825BE71F4AB87CB9BBD401663677943&amp;v=MTIwMzRmQnJMVTA1dHBodzdxL3hhcz1OaWZJZWNld0hOUysyb2hFRXU5K2ZuUSt2R1FhbUUwSlRIL2pxaFEyZjdXVFRMNmNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         ZHANG Y, ZHOU X, SHI H, et al.Corrosion pitting damage detection of rolling bearings using data mining techniques[J].International Journal of Modeling, Identification and Control, 2015, 24 (3) :235-243.
                                    </a>
                                </li>
                                <li id="577">


                                    <a id="bibliography_10" title=" ZHOU Y, YU J, WANG X.Time series prediction methods for depth-averaged current velocities of underwater gliders [J].IEEE Access, 2017, 5:5773-5784." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Time series prediction methods for depth-averaged current velocities of underwater gliders">
                                        <b>[10]</b>
                                         ZHOU Y, YU J, WANG X.Time series prediction methods for depth-averaged current velocities of underwater gliders [J].IEEE Access, 2017, 5:5773-5784.
                                    </a>
                                </li>
                                <li id="579">


                                    <a id="bibliography_11" title=" 章永来, 史海波, 尚文利, 等.面向乳腺癌辅助诊断的改进支持向量机方法[J].计算机应用研究, 2013, 30 (8) :2373-2376. (ZHANG Y L, SHI H B, SHANG W L, et al.Improved method for computer-aided diagnosis of breast cancer based on support vector machines [J].Application Research of Computers, 2013, 30 (8) :2373-2376.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201308034&amp;v=MTc1MTc3ckxMejdTWkxHNEg5TE1wNDlHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         章永来, 史海波, 尚文利, 等.面向乳腺癌辅助诊断的改进支持向量机方法[J].计算机应用研究, 2013, 30 (8) :2373-2376. (ZHANG Y L, SHI H B, SHANG W L, et al.Improved method for computer-aided diagnosis of breast cancer based on support vector machines [J].Application Research of Computers, 2013, 30 (8) :2373-2376.) 
                                    </a>
                                </li>
                                <li id="581">


                                    <a id="bibliography_12" title=" KLEINBERG J.An impossibility theorem for clustering [C]// Proceedings of the 15th International Conference on Neural Information Processing Systems.Cambridge:MIT Press, 2002:463-470." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Impossibility Theorem for Clustering">
                                        <b>[12]</b>
                                         KLEINBERG J.An impossibility theorem for clustering [C]// Proceedings of the 15th International Conference on Neural Information Processing Systems.Cambridge:MIT Press, 2002:463-470.
                                    </a>
                                </li>
                                <li id="583">


                                    <a id="bibliography_13" title=" DUDA R O, HART P E, STORK D G.Pattern Classification [M].2nd ed.New York:John Wiley and Sons, 2001:47-56." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pattern Classification">
                                        <b>[13]</b>
                                         DUDA R O, HART P E, STORK D G.Pattern Classification [M].2nd ed.New York:John Wiley and Sons, 2001:47-56.
                                    </a>
                                </li>
                                <li id="585">


                                    <a id="bibliography_14" title=" GAO J, WANG Y, LI J.Bounds on covering radius of linear codes with Chinese Euclidean distance over the finite non chain ring F-2+vF (2) [J].Information Processing Letters, 2018, 138:22-26." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE0BC821EBA1C25412EB510831D62A216&amp;v=Mjg4ODhOaWZPZmNhNGJLTEVyWTR3RnBvT2YzNDh5eGNSbjAxNFNYL3FyeE5CZjdEbFI3dVpDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         GAO J, WANG Y, LI J.Bounds on covering radius of linear codes with Chinese Euclidean distance over the finite non chain ring F-2+vF (2) [J].Information Processing Letters, 2018, 138:22-26.
                                    </a>
                                </li>
                                <li id="587">


                                    <a id="bibliography_15" title=" HOGG R, TANIS E.Probability and Statistical Inference [M].7th ed.Upper Saddle River :Prentice Hall, 2005:120-145." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probability and Statistical Inference">
                                        <b>[15]</b>
                                         HOGG R, TANIS E.Probability and Statistical Inference [M].7th ed.Upper Saddle River :Prentice Hall, 2005:120-145.
                                    </a>
                                </li>
                                <li id="589">


                                    <a id="bibliography_16" title=" BOBROWSKIL, BEZDEK J C.C-means clustering with the L&lt;sub&gt;1&lt;/sub&gt; and L∞ norms [J].IEEE Transactions on Systems, Man, and Cybernetics, 1991, 21 (3) :545-554." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=c-means clustering with the ll and l&amp;amp;infin; norms">
                                        <b>[16]</b>
                                         BOBROWSKIL, BEZDEK J C.C-means clustering with the L&lt;sub&gt;1&lt;/sub&gt; and L∞ norms [J].IEEE Transactions on Systems, Man, and Cybernetics, 1991, 21 (3) :545-554.
                                    </a>
                                </li>
                                <li id="591">


                                    <a id="bibliography_17" title=" ANTER A, HASSENIAN A E, OLIVA D.An improved fast fuzzy c-means using crow search optimization algorithm for crop identification in agricultural[J].Expert Systems with Applications, 2019, 118:340-354." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES777F239568D78AEFA9BDBF9114051838&amp;v=MDA1MTRpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mYlMvR2FmT3JJWkFZdU43QzNSSXVtQmk0MDBKT2ducnJSTXhlYmVWVGJtWENPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         ANTER A, HASSENIAN A E, OLIVA D.An improved fast fuzzy c-means using crow search optimization algorithm for crop identification in agricultural[J].Expert Systems with Applications, 2019, 118:340-354.
                                    </a>
                                </li>
                                <li id="593">


                                    <a id="bibliography_18" title=" MAO J, JAIN A K.A self-organizing network for HyperEllipsoidal Clustering (HEC) [J].IEEE Transactions on Neural Networks, 1996, 7 (1) :16-29." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A self-organizing network for hyperellipsoidal clustering">
                                        <b>[18]</b>
                                         MAO J, JAIN A K.A self-organizing network for HyperEllipsoidal Clustering (HEC) [J].IEEE Transactions on Neural Networks, 1996, 7 (1) :16-29.
                                    </a>
                                </li>
                                <li id="595">


                                    <a id="bibliography_19" title=" ZHAN J, WANG R, YI L.Health assessment methods for wind turbines based on power prediction and Mahalanobis distance [J].International Journal of Pattern Recognition and Artificial Intelligence, 2019, 33 (2) :1951001." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Health assessment methods for wind turbines based on power prediction and Mahalanobis distance">
                                        <b>[19]</b>
                                         ZHAN J, WANG R, YI L.Health assessment methods for wind turbines based on power prediction and Mahalanobis distance [J].International Journal of Pattern Recognition and Artificial Intelligence, 2019, 33 (2) :1951001.
                                    </a>
                                </li>
                                <li id="597">


                                    <a id="bibliography_20" title=" KAUFMAN L, ROUSSEEUW P J.Finding Groups in Data:An Introduction to Cluster Analysis[M].New York:John Wiley and Sons, 2009:82-85." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Finding Groups in Data:An Introduction to Cluster Analysis">
                                        <b>[20]</b>
                                         KAUFMAN L, ROUSSEEUW P J.Finding Groups in Data:An Introduction to Cluster Analysis[M].New York:John Wiley and Sons, 2009:82-85.
                                    </a>
                                </li>
                                <li id="599">


                                    <a id="bibliography_21" title=" XU R, DONALD C W.Clustering[M].New York:John Wiley and Sons, 2009:12-95." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering">
                                        <b>[21]</b>
                                         XU R, DONALD C W.Clustering[M].New York:John Wiley and Sons, 2009:12-95.
                                    </a>
                                </li>
                                <li id="601">


                                    <a id="bibliography_22" title=" FORGY E W.Cluster analysis of multivariate data:efficiency vs.interpretability of classification[J].International Journal of Environmental Studies, 1965, 21 (3) :41-52." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cluster analysis of multivariate data: efficiency vs. interpretability of classifications">
                                        <b>[22]</b>
                                         FORGY E W.Cluster analysis of multivariate data:efficiency vs.interpretability of classification[J].International Journal of Environmental Studies, 1965, 21 (3) :41-52.
                                    </a>
                                </li>
                                <li id="603">


                                    <a id="bibliography_23" title=" ANTOINE G B, CATHY M R, ANDREA R.Clustering transformed compositional data using K-means, with applications in gene expression and bicycle sharing system data [J].Journal of Applied Statistics, 2019, 46 (1) :47-65." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD34656CB852467C00E1E9C4D890AF25C0&amp;v=MTM4MTFLM1AxTllla0xDbnRLenhabTYwcDBPM3VXcEJzMUNNU1dRTW1mQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1Oam5CYXJDOEdOVA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         ANTOINE G B, CATHY M R, ANDREA R.Clustering transformed compositional data using K-means, with applications in gene expression and bicycle sharing system data [J].Journal of Applied Statistics, 2019, 46 (1) :47-65.
                                    </a>
                                </li>
                                <li id="605">


                                    <a id="bibliography_24" title=" HATHAWAY R J, BEZDEK J C, HU Y.Generalized fuzzy c-means clustering strategies using L&lt;sub&gt;P&lt;/sub&gt; norm distances [J].IEEE Transactions on Fuzzy Systems, 2000, 8 (5) :576-582." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized fuzzy c-means clustering strategies using Lp norm distances">
                                        <b>[24]</b>
                                         HATHAWAY R J, BEZDEK J C, HU Y.Generalized fuzzy c-means clustering strategies using L&lt;sub&gt;P&lt;/sub&gt; norm distances [J].IEEE Transactions on Fuzzy Systems, 2000, 8 (5) :576-582.
                                    </a>
                                </li>
                                <li id="607">


                                    <a id="bibliography_25" title=" 耿宗科, 王长宾, 张振国.基于模糊c-means与自适应粒子群优化的模糊聚类算法[J].计算机科学, 2016, 43 (8) :267-272. (GENG Z K, WANG C B, ZHANG Z G.Fuzzy c-means and adaptive PSO based fuzzy clustering algorithm[J].Computer Science, 2016, 43 (8) :267-272.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201608054&amp;v=MDkxNTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMTHo3QmI3RzRIOWZNcDQ5QVlJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         耿宗科, 王长宾, 张振国.基于模糊c-means与自适应粒子群优化的模糊聚类算法[J].计算机科学, 2016, 43 (8) :267-272. (GENG Z K, WANG C B, ZHANG Z G.Fuzzy c-means and adaptive PSO based fuzzy clustering algorithm[J].Computer Science, 2016, 43 (8) :267-272.) 
                                    </a>
                                </li>
                                <li id="609">


                                    <a id="bibliography_26" title=" CARPENTER G A, GROSSBERG S, ROSEN D B.Fuzzy ART:fast stable learning and categorization of analog patterns by an adaptive resonance system[J].Neural Networks, 1991, 4 (6) :759-771." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system">
                                        <b>[26]</b>
                                         CARPENTER G A, GROSSBERG S, ROSEN D B.Fuzzy ART:fast stable learning and categorization of analog patterns by an adaptive resonance system[J].Neural Networks, 1991, 4 (6) :759-771.
                                    </a>
                                </li>
                                <li id="611">


                                    <a id="bibliography_27" title=" CHANDRAPRABHA K, GEETHA B G.Wireless network confidence level improvement via fusion adaptive resonance theory[J].Cluster Computing, 2018 (2) :1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wireless network confidence level improvement via fusion adaptive resonance theory">
                                        <b>[27]</b>
                                         CHANDRAPRABHA K, GEETHA B G.Wireless network confidence level improvement via fusion adaptive resonance theory[J].Cluster Computing, 2018 (2) :1-11.
                                    </a>
                                </li>
                                <li id="613">


                                    <a id="bibliography_28" title=" ANAGNOSTOPOULOS G C, GEORGIOPOULOS M.Ellipsoid ART and ARTMAP for incremental unsupervised and supervised learning[C]// Proceedings of the 2001 International Society of Optical Engineering.Bellingham:SPIE Publications, 2001:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ellipsoid ART and ARTMAP for incremental unsupervised and supervised learning">
                                        <b>[28]</b>
                                         ANAGNOSTOPOULOS G C, GEORGIOPOULOS M.Ellipsoid ART and ARTMAP for incremental unsupervised and supervised learning[C]// Proceedings of the 2001 International Society of Optical Engineering.Bellingham:SPIE Publications, 2001:1-6.
                                    </a>
                                </li>
                                <li id="615">


                                    <a id="bibliography_29" title=" MOSHTAGHI M, RAJASEGARAR S, LECKIE C, et al.An efficient hyperellipsoidal clustering algorithm for resource-constrained environments[J].Pattern Recognition, 2011, 44 (9) :2197-2209." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738142&amp;v=MTU3NzFVYnhFPU5pZk9mYks3SHRETnFZOUZZK2dIRFhnN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxdw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                         MOSHTAGHI M, RAJASEGARAR S, LECKIE C, et al.An efficient hyperellipsoidal clustering algorithm for resource-constrained environments[J].Pattern Recognition, 2011, 44 (9) :2197-2209.
                                    </a>
                                </li>
                                <li id="617">


                                    <a id="bibliography_30" title=" SU M C, CHOU C H.A modified version of the K-Means algorithm with a distance based on cluster symmetry [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :674-680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Modified Version of the K-Means Algorithm with a Distance Based on Cluster Symmetry">
                                        <b>[30]</b>
                                         SU M C, CHOU C H.A modified version of the K-Means algorithm with a distance based on cluster symmetry [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :674-680.
                                    </a>
                                </li>
                                <li id="619">


                                    <a id="bibliography_31" title=" EISEN M B, SPELLMAN P T, BROWN P O, et al.Cluster analysis and display of genome-wide expression patterns [J].Proceedings of the National Academy of Sciences of the United States of America, 1998, 95 (25) :14863-14868." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cluster analysis and display of genome-wide expression patterns">
                                        <b>[31]</b>
                                         EISEN M B, SPELLMAN P T, BROWN P O, et al.Cluster analysis and display of genome-wide expression patterns [J].Proceedings of the National Academy of Sciences of the United States of America, 1998, 95 (25) :14863-14868.
                                    </a>
                                </li>
                                <li id="621">


                                    <a id="bibliography_32" title=" STEINBACH M, KARYPIS G, KUMAR V.A Comparison of Document Clustering Techniques [M].New York:John Wiley and Sons, 2000:50-56." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Comparison of Document Clustering Techniques">
                                        <b>[32]</b>
                                         STEINBACH M, KARYPIS G, KUMAR V.A Comparison of Document Clustering Techniques [M].New York:John Wiley and Sons, 2000:50-56.
                                    </a>
                                </li>
                                <li id="623">


                                    <a id="bibliography_33" title=" GERSHO A, GRAY R M.Vector quantization and signal compression[J].Springer International, 1992, 159 (1) :407-485." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vector Quantization and Signal Compression">
                                        <b>[33]</b>
                                         GERSHO A, GRAY R M.Vector quantization and signal compression[J].Springer International, 1992, 159 (1) :407-485.
                                    </a>
                                </li>
                                <li id="625">


                                    <a id="bibliography_34" title=" ABDEL-GHAFFAR K A S.Sets of binary sequences with small total Hamming distances[J].Information Processing Letters, 2019, 142:27-29." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD7CFDA2A82A546CB26F43265B8640959&amp;v=MDE0NTBPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZjZS9iYWU0M28wMGJPbCtDWGcvdkdRUjdFbDVTMzNrcVdBOWY3YVVUTCtXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[34]</b>
                                         ABDEL-GHAFFAR K A S.Sets of binary sequences with small total Hamming distances[J].Information Processing Letters, 2019, 142:27-29.
                                    </a>
                                </li>
                                <li id="627">


                                    <a id="bibliography_35" title=" GETZ G, LEVINE E, DOMANY E.Coupled two-way clustering analysis of gene microarray data[J].Proceedings of the National Academy of Sciences, 2000, 97 (22) :12079-12084." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled two-way clustering analysis of gene microarray data">
                                        <b>[35]</b>
                                         GETZ G, LEVINE E, DOMANY E.Coupled two-way clustering analysis of gene microarray data[J].Proceedings of the National Academy of Sciences, 2000, 97 (22) :12079-12084.
                                    </a>
                                </li>
                                <li id="629">


                                    <a id="bibliography_36" title=" EVERITT B, HOTHORN T.Cluster Analysis[M].New York:John Wiley and Sons, 2011:111-134." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cluster Analysis">
                                        <b>[36]</b>
                                         EVERITT B, HOTHORN T.Cluster Analysis[M].New York:John Wiley and Sons, 2011:111-134.
                                    </a>
                                </li>
                                <li id="631">


                                    <a id="bibliography_37" title=" CHEN J Y, HE H H.A fast density-based data stream clustering algorithm with cluster centers self-determined for mixed data[J].Information Sciences, 2016, 345:271-293." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7907A53B16E9FCCD1DE8A3860EF73FFB&amp;v=MjgyNzBJU25rcDFPWHpxcWhKQUQ3V1hNOHp0Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaWZPZmJTeEh0YTlxb3czWmUxNkJRcEt2Rw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[37]</b>
                                         CHEN J Y, HE H H.A fast density-based data stream clustering algorithm with cluster centers self-determined for mixed data[J].Information Sciences, 2016, 345:271-293.
                                    </a>
                                </li>
                                <li id="633">


                                    <a id="bibliography_38" title=" SHIRKHORSHIDI A S, AGHABOZORGI S, WAH T Y, et al.Big data clustering:a review[C]// Proceedings of the 14th International Conference on Computational Science and Its Applications.Berlin:Springer, 2014:707-720." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Big Data Clustering:A Review">
                                        <b>[38]</b>
                                         SHIRKHORSHIDI A S, AGHABOZORGI S, WAH T Y, et al.Big data clustering:a review[C]// Proceedings of the 14th International Conference on Computational Science and Its Applications.Berlin:Springer, 2014:707-720.
                                    </a>
                                </li>
                                <li id="635">


                                    <a id="bibliography_39" title=" KALYANI P.Approaches to partition medical data using clustering algorithms[J].International Journal of Computer Applications, 2013, 49 (23) :7-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Approaches to partition medical data using clustering algorithms">
                                        <b>[39]</b>
                                         KALYANI P.Approaches to partition medical data using clustering algorithms[J].International Journal of Computer Applications, 2013, 49 (23) :7-10.
                                    </a>
                                </li>
                                <li id="637">


                                    <a id="bibliography_40" title=" LIU G.Introduction to Combinatorial Mathematics[M].New York:McGraw Hill, 1968:12-16." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Introduction to Combinatorial Mathematics">
                                        <b>[40]</b>
                                         LIU G.Introduction to Combinatorial Mathematics[M].New York:McGraw Hill, 1968:12-16.
                                    </a>
                                </li>
                                <li id="639">


                                    <a id="bibliography_41" title=" KRISHNA K, MURTY M N.Genetic K-means algorithm[J].IEEE Transactions on Systems, Man, and Cybernetics:Part B, 1999, 29 (3) :433-439." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Genetic K-means algorithm">
                                        <b>[41]</b>
                                         KRISHNA K, MURTY M N.Genetic K-means algorithm[J].IEEE Transactions on Systems, Man, and Cybernetics:Part B, 1999, 29 (3) :433-439.
                                    </a>
                                </li>
                                <li id="641">


                                    <a id="bibliography_42" title=" LU Y, LU S, FOTOUHI F.FGKA:a fast genetic K-means clustering algorithm[C]// Proceedings of the 2004 ACM Symposium on Applied Computing.New York:ACM, 2004:622-623." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FGKA:A fast genetic K-means clustering algorithm">
                                        <b>[42]</b>
                                         LU Y, LU S, FOTOUHI F.FGKA:a fast genetic K-means clustering algorithm[C]// Proceedings of the 2004 ACM Symposium on Applied Computing.New York:ACM, 2004:622-623.
                                    </a>
                                </li>
                                <li id="643">


                                    <a id="bibliography_43" title=" BRADLEY P, FAYYAD U.Refining initial points for K-means clustering[C]// Proceedings of the 15th International Conference on Machine Learning.New York:ACM, 1998:91-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Refining Initial Points for K-Means Clustering">
                                        <b>[43]</b>
                                         BRADLEY P, FAYYAD U.Refining initial points for K-means clustering[C]// Proceedings of the 15th International Conference on Machine Learning.New York:ACM, 1998:91-99.
                                    </a>
                                </li>
                                <li id="645">


                                    <a id="bibliography_44" title=" PELLEG D, MOORE A.X-means:extending K-means with efficient estimation of the number of the clusters[C]// Proceedings of the 17th International Conference on Machine Learning.New York:ACM, 2000:111-117." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=X-Means:extending k-Means with efficient estimation of the number of clusters">
                                        <b>[44]</b>
                                         PELLEG D, MOORE A.X-means:extending K-means with efficient estimation of the number of the clusters[C]// Proceedings of the 17th International Conference on Machine Learning.New York:ACM, 2000:111-117.
                                    </a>
                                </li>
                                <li id="647">


                                    <a id="bibliography_45" title=" BERKHIN P, BECHER J.Learning simple relations:theory and applications[C]// Proceedings of the 2nd International Conference on Data Mining, Washington, DC:IEEE Computer Society, 2002:333-349." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning simple relations:theory and applications">
                                        <b>[45]</b>
                                         BERKHIN P, BECHER J.Learning simple relations:theory and applications[C]// Proceedings of the 2nd International Conference on Data Mining, Washington, DC:IEEE Computer Society, 2002:333-349.
                                    </a>
                                </li>
                                <li id="649">


                                    <a id="bibliography_46" title=" NGUYEN H H.Privacy-preserving mechanisms for K-modes clustering[J].Computers and Security, 2018, 78:60-75." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA41EE26395A2B0E167852DAB46E56028&amp;v=MTEzMjM4SDZTNXJZbEdiZTUrRGc0NXVoY1Y3VGQ0U2d1VDNoWXpETGVTUmJpWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZjSw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[46]</b>
                                         NGUYEN H H.Privacy-preserving mechanisms for K-modes clustering[J].Computers and Security, 2018, 78:60-75.
                                    </a>
                                </li>
                                <li id="651">


                                    <a id="bibliography_47" title=" LACKO D, HUYSMANST, VLEUGELS J, et al.Product sizing with 3D anthropometry and k-medoids clustering[J].Computer-Aided Design, 2017, 91:60-74." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES28D8954DBDBEBEB50AB598FA7B6CA052&amp;v=MDY1ODFPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZiR3dhdG5GcW9zeEZwOTllUTVNdlJNVG0wMTRRWGVVM1JWSGY4SGxSYitkQw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[47]</b>
                                         LACKO D, HUYSMANST, VLEUGELS J, et al.Product sizing with 3D anthropometry and k-medoids clustering[J].Computer-Aided Design, 2017, 91:60-74.
                                    </a>
                                </li>
                                <li id="653">


                                    <a id="bibliography_48" title=" NAKAGAWA K, IMAMURA M, YOSHIDA K.Stock price prediction using k-medoids clustering with indexing dynamic time warping[J].Electronics and Communications in Japan, 2019, 102 (2) :3-8." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD132B2EA93EDBF0565B6A7BB295B67633&amp;v=Mjg0MzNySzdIS1BPMnY1TVo1NTdmZ281eWhBV21Ea01UdzJRcmhzd0M3U1RRN21jQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaWZjYQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[48]</b>
                                         NAKAGAWA K, IMAMURA M, YOSHIDA K.Stock price prediction using k-medoids clustering with indexing dynamic time warping[J].Electronics and Communications in Japan, 2019, 102 (2) :3-8.
                                    </a>
                                </li>
                                <li id="655">


                                    <a id="bibliography_49" title=" FRALEY C, RAFTERY A.Model-based clustering, discriminant analysis, and density estimation [J].Journal of the American Statistical Association, 2002, 97:611-631." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800007182&amp;v=MDc3NTk3SHRmT3A0OUZaT3NJRFhRN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmpuQmFySw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[49]</b>
                                         FRALEY C, RAFTERY A.Model-based clustering, discriminant analysis, and density estimation [J].Journal of the American Statistical Association, 2002, 97:611-631.
                                    </a>
                                </li>
                                <li id="657">


                                    <a id="bibliography_50" title=" McLACHLAN G, KRISHNAN T.The EM Algorithm and Extensions[M].New York:John Wiley and Sons, 1997:6-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The EM Algorithm and Extensions">
                                        <b>[50]</b>
                                         McLACHLAN G, KRISHNAN T.The EM Algorithm and Extensions[M].New York:John Wiley and Sons, 1997:6-9.
                                    </a>
                                </li>
                                <li id="659">


                                    <a id="bibliography_51" title=" ZHOU Y, XU S, JIN C, et al.Multiple point sets registration based on expectation maximization algorithm [J].Computers and Electrical Engineering, 2018, 70:1-11." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES4A6188FE98C4652F9A8871301EF78AAE&amp;v=MTAzODh3N3EveGFzPU5pZk9mYmZKR05ERXAva3diZU44Q0hvOHpXQWFtemQxVDM3aHJCTkFEN1djTk12cUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[51]</b>
                                         ZHOU Y, XU S, JIN C, et al.Multiple point sets registration based on expectation maximization algorithm [J].Computers and Electrical Engineering, 2018, 70:1-11.
                                    </a>
                                </li>
                                <li id="661">


                                    <a id="bibliography_52" title=" BEN-DOR A, SHAMIR R, YAKHINI Z.Clustering gene expression patterns[J].Journal of Computational Biology, 1999, 6:281-297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering gene expression patterns">
                                        <b>[52]</b>
                                         BEN-DOR A, SHAMIR R, YAKHINI Z.Clustering gene expression patterns[J].Journal of Computational Biology, 1999, 6:281-297.
                                    </a>
                                </li>
                                <li id="663">


                                    <a id="bibliography_53" title=" SHARAN R, SHAMIR R.CLICK:A clustering algorithm with applications to gene expression analysis[C]// Proceedings of the 8th International Conference on Intelligent Systems for Molecular Biology.San Diego:[s.n.], 2000:307-316." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CLICK: a clustering algorithm with applications to gene expression analysis">
                                        <b>[53]</b>
                                         SHARAN R, SHAMIR R.CLICK:A clustering algorithm with applications to gene expression analysis[C]// Proceedings of the 8th International Conference on Intelligent Systems for Molecular Biology.San Diego:[s.n.], 2000:307-316.
                                    </a>
                                </li>
                                <li id="665">


                                    <a id="bibliography_54" title=" NGUYEN M N, SIM A Y L, WAN Y, et al.Topology independent comparison of RNA 3D structures using the CLICK algorithm[J].Nucleic Acids Research, 2017, 45 (1) :e5." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJRS&amp;filename=SJRSBD83030EC908205125BCB459C4E26103&amp;v=MDQwOTBDU1JMcWNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZlpmY0hNRnRMTXJJOHdGK0lQQkg0NXloY1I3MDBPT252bnBXRXhETA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[54]</b>
                                         NGUYEN M N, SIM A Y L, WAN Y, et al.Topology independent comparison of RNA 3D structures using the CLICK algorithm[J].Nucleic Acids Research, 2017, 45 (1) :e5.
                                    </a>
                                </li>
                                <li id="667">


                                    <a id="bibliography_55" title=" DING C F, LI K.Centrality ranking in multiplex networks using topologically biased random walks[J].Neuro-computing, 2018, 312:263-275." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6E4857C69F4F1F934899A4715565823E&amp;v=MDczMTZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mYlhOR3RuSnFQeERiWjBMZW4xUHhoVVg0alowT1h2bHJSY3dmN2VjUjducQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[55]</b>
                                         DING C F, LI K.Centrality ranking in multiplex networks using topologically biased random walks[J].Neuro-computing, 2018, 312:263-275.
                                    </a>
                                </li>
                                <li id="669">


                                    <a id="bibliography_56" title=" SHANG R, ZHANG Z, JIAO L, et al.Global discriminative-based nonnegative spectral clustering[J].Pattern Recognition, 2016, 55:172-182." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES24A84D79395484ED5F25AD179A0AAA90&amp;v=MjgxNzhJV25EMTRPUXZqcXh0RWVjUGxOTE9mQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaWZPZmJHOGI5bkkyNGhNWitJS0NIUTl1bQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[56]</b>
                                         SHANG R, ZHANG Z, JIAO L, et al.Global discriminative-based nonnegative spectral clustering[J].Pattern Recognition, 2016, 55:172-182.
                                    </a>
                                </li>
                                <li id="671">


                                    <a id="bibliography_57" title=" 宋健, 许国艳, 夭荣朋.基于差分隐私的数据匿名化隐私保护方法[J].计算机应用, 2016, 36 (10) :2753-2757. (SONG J, XU G Y, YAO R P.Spectral clustering algorithm based on differential privacy protection[J].Journal of Computer Applications, 2016, 36 (10) :2753-2757.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201610021&amp;v=MDUyODc0SDlmTnI0OUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVTdyTEx6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[57]</b>
                                         宋健, 许国艳, 夭荣朋.基于差分隐私的数据匿名化隐私保护方法[J].计算机应用, 2016, 36 (10) :2753-2757. (SONG J, XU G Y, YAO R P.Spectral clustering algorithm based on differential privacy protection[J].Journal of Computer Applications, 2016, 36 (10) :2753-2757.) 
                                    </a>
                                </li>
                                <li id="673">


                                    <a id="bibliography_58" title=" LEE C-H, ZAIANE O R, PARK H-H, et al.Clustering high dimensional data:a graph-based relaxed optimization approach[J].Information Sciences, 2008, 178 (23) :4501-4511." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601417758&amp;v=MDMzMTF3WmVadEZpbmxVcjNJSjF3VWJ4RT1OaWZPZmJLN0h0RE5xWTlFWU9vSUMza3hvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[58]</b>
                                         LEE C-H, ZAIANE O R, PARK H-H, et al.Clustering high dimensional data:a graph-based relaxed optimization approach[J].Information Sciences, 2008, 178 (23) :4501-4511.
                                    </a>
                                </li>
                                <li id="675">


                                    <a id="bibliography_59" title=" SHI D, WANG J, CHENG D, et al.A global-local affinity matrix model via EigenGap for graph-based subspace clustering [J].Pattern Recognition Letters, 2017, 89:67-72." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES20E8B149712152668B810F0763205918&amp;v=MjY3Mzg3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZiRzRhOW0rcm90TVkrb05EWGs3eVJBYm1EZDhTQW5pcXhRMmU3S1JUTHVYQ09OdkZTaVdXcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[59]</b>
                                         SHI D, WANG J, CHENG D, et al.A global-local affinity matrix model via EigenGap for graph-based subspace clustering [J].Pattern Recognition Letters, 2017, 89:67-72.
                                    </a>
                                </li>
                                <li id="677">


                                    <a id="bibliography_60" title=" BEZDEK J.Pattern Recognition with Fuzzy Objective Function Algorithms[M].New York:Plenum Press, 1981:37-89." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pattern recognition with fuzzy objective function algorithms">
                                        <b>[60]</b>
                                         BEZDEK J.Pattern Recognition with Fuzzy Objective Function Algorithms[M].New York:Plenum Press, 1981:37-89.
                                    </a>
                                </li>
                                <li id="679">


                                    <a id="bibliography_61" title=" HATHAWAY R, BEZDEK J.Fuzzy c-means clustering of incomplete data[J].IEEE Transactions on Systems, Man, and Cybernetics, 2001, 31 (5) :735-744." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy c-means clustering of incomplete data">
                                        <b>[61]</b>
                                         HATHAWAY R, BEZDEK J.Fuzzy c-means clustering of incomplete data[J].IEEE Transactions on Systems, Man, and Cybernetics, 2001, 31 (5) :735-744.
                                    </a>
                                </li>
                                <li id="681">


                                    <a id="bibliography_62" title=" ESTER M, KRIEGEL H, SANDER J, et al.A density-based algorithm for discovering clusters in large spatial databases with noise[C]// Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.New York:AAAI Press, 1996:56-69." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise">
                                        <b>[62]</b>
                                         ESTER M, KRIEGEL H, SANDER J, et al.A density-based algorithm for discovering clusters in large spatial databases with noise[C]// Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.New York:AAAI Press, 1996:56-69.
                                    </a>
                                </li>
                                <li id="683">


                                    <a id="bibliography_63" title=" BIRANT D, KUT A.ST-DBSCAN:an algorithm for clustering spatial-temporal data[J].Data and Knowledge Engineering, 2007, 60 (1) :208-221." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300248374&amp;v=MTIxNzBFPU5pZk9mYks3SHRET3JJOUZadThIRDNzOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[63]</b>
                                         BIRANT D, KUT A.ST-DBSCAN:an algorithm for clustering spatial-temporal data[J].Data and Knowledge Engineering, 2007, 60 (1) :208-221.
                                    </a>
                                </li>
                                <li id="685">


                                    <a id="bibliography_64" title=" TRISMININGSIH R, SHAZTIKA S S.ST-DBSCAN clustering module in SpagoBI for hotspots distribution in Indonesia[C]// Proceedings of the 3rd International Conference on Information Technology.New York:ACM, 2017:60-67." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ST-DBSCAN clustering module in SpagoBI for hotspots distribution in Indonesia">
                                        <b>[64]</b>
                                         TRISMININGSIH R, SHAZTIKA S S.ST-DBSCAN clustering module in SpagoBI for hotspots distribution in Indonesia[C]// Proceedings of the 3rd International Conference on Information Technology.New York:ACM, 2017:60-67.
                                    </a>
                                </li>
                                <li id="687">


                                    <a id="bibliography_65" title=" ANKERST M, BREUNIG M, KRIEGEL H, et al.OPTICS:ordering points to identify the clustering structure[C]// Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1999:49-60." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OPTICS: Ordering points to identify the clustering structure">
                                        <b>[65]</b>
                                         ANKERST M, BREUNIG M, KRIEGEL H, et al.OPTICS:ordering points to identify the clustering structure[C]// Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1999:49-60.
                                    </a>
                                </li>
                                <li id="689">


                                    <a id="bibliography_66" title=" HINNEBURG A, KEIM D.An efficient approach to clustering in large multimedia databases with noise[C]// Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1998:58-65." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An efficient approach to clustering in large multimedia databases with noise">
                                        <b>[66]</b>
                                         HINNEBURG A, KEIM D.An efficient approach to clustering in large multimedia databases with noise[C]// Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1998:58-65.
                                    </a>
                                </li>
                                <li id="691">


                                    <a id="bibliography_67" title=" WANG W, YANG J, MUNTZ R.STING:a statistical information grid approach to spatial data mining[C]// Proceedings of the 23rd Conference on Very Large Data Bases.New York:ACM, 1997:186-195." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=STING: a statistical information grid approach to spatial data mining">
                                        <b>[67]</b>
                                         WANG W, YANG J, MUNTZ R.STING:a statistical information grid approach to spatial data mining[C]// Proceedings of the 23rd Conference on Very Large Data Bases.New York:ACM, 1997:186-195.
                                    </a>
                                </li>
                                <li id="693">


                                    <a id="bibliography_68" title=" SHEIKHOIESIAMI G, CHATTERJEE S, ZHANG A.WaveCluster:a multi-resolution clustering approach for very large spatial databases[C]// Proceedings of the 24th Conference on Very Large Data Bases.New York:ACM, 1998:428-439." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=WaveCluster:A Multi-Resolution Clustering Approach for Very Large Spatial Databases">
                                        <b>[68]</b>
                                         SHEIKHOIESIAMI G, CHATTERJEE S, ZHANG A.WaveCluster:a multi-resolution clustering approach for very large spatial databases[C]// Proceedings of the 24th Conference on Very Large Data Bases.New York:ACM, 1998:428-439.
                                    </a>
                                </li>
                                <li id="695">


                                    <a id="bibliography_69" title=" YILDIRIM A A, WATSON D.A comparative study of the parallel wavelet-based clustering algorithm on three-dimensional dataset[J].Journal of Supercomputing, 2015, 71 (7) :2365-2380." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A comparative study of the parallel wavelet-based clustering algorithm on three-dimensional dataset">
                                        <b>[69]</b>
                                         YILDIRIM A A, WATSON D.A comparative study of the parallel wavelet-based clustering algorithm on three-dimensional dataset[J].Journal of Supercomputing, 2015, 71 (7) :2365-2380.
                                    </a>
                                </li>
                                <li id="697">


                                    <a id="bibliography_70" title=" SHAO J, HE X, BOHM C, et al.Synchronization-inspired partitioning and hierarchical clustering[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (4) :893-905." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Synchronization inspired partitioning and hierarchical clustering">
                                        <b>[70]</b>
                                         SHAO J, HE X, BOHM C, et al.Synchronization-inspired partitioning and hierarchical clustering[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (4) :893-905.
                                    </a>
                                </li>
                                <li id="699">


                                    <a id="bibliography_71" title=" FREY B J, DUECK D.Clustering by passing messages between data points[J].Science, 2007, 315 (5814) :972-976." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering by passing messages between data points">
                                        <b>[71]</b>
                                         FREY B J, DUECK D.Clustering by passing messages between data points[J].Science, 2007, 315 (5814) :972-976.
                                    </a>
                                </li>
                                <li id="701">


                                    <a id="bibliography_72" title=" 刘晓楠, 尹美娟, 李明涛, 等.面向大规模数据的分层近邻传播聚类算法[J].计算机科学, 2014, 41 (3) :185-188. (LIU X N, YIN M J, LI M T, et al.Hierarchical affinity propagation clustering for large-scale data set[J].Computer Science, 2014, 41 (3) :185-188.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201403042&amp;v=MDY0NzQvblU3ckxMejdCYjdHNEg5WE1ySTlCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[72]</b>
                                         刘晓楠, 尹美娟, 李明涛, 等.面向大规模数据的分层近邻传播聚类算法[J].计算机科学, 2014, 41 (3) :185-188. (LIU X N, YIN M J, LI M T, et al.Hierarchical affinity propagation clustering for large-scale data set[J].Computer Science, 2014, 41 (3) :185-188.) 
                                    </a>
                                </li>
                                <li id="703">


                                    <a id="bibliography_73" title=" AKASH O M, AZMI M S B.A new similarity measure based affinity propagation for data clustering [J].Advanced Science Letters, 2018, 24 (2) :1130-1133." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJAH&amp;filename=SJAHA71DC551782EDD163ED017A9E23848A3&amp;v=MzE1MjBXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaWZLWnNLL0g2Vy9xb3BFWStNTmVRaE56aEFRbjB0OVNYaVRwV2MzZXJxUVRjdWNDT052RlNpVw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[73]</b>
                                         AKASH O M, AZMI M S B.A new similarity measure based affinity propagation for data clustering [J].Advanced Science Letters, 2018, 24 (2) :1130-1133.
                                    </a>
                                </li>
                                <li id="705">


                                    <a id="bibliography_74" title=" ZHAO J L, QU H, ZHAO J H.Towards controller placement problem for software-defined network using affinity propagation[J].Electronics Letters, 2017, 53 (14) :928-929." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards controller placement problem for software-defined network using affinity propagation">
                                        <b>[74]</b>
                                         ZHAO J L, QU H, ZHAO J H.Towards controller placement problem for software-defined network using affinity propagation[J].Electronics Letters, 2017, 53 (14) :928-929.
                                    </a>
                                </li>
                                <li id="707">


                                    <a id="bibliography_75" title=" RODRIGUEZ A, LAIO A.Clustering by fast search and find of density peaks[J].Science, 2014, 344 (6191) :1492-1496." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering by fast search and find of densitypeaks">
                                        <b>[75]</b>
                                         RODRIGUEZ A, LAIO A.Clustering by fast search and find of density peaks[J].Science, 2014, 344 (6191) :1492-1496.
                                    </a>
                                </li>
                                <li id="709">


                                    <a id="bibliography_76" title=" XIAO X, DING S, SUN T.A fast density peaks clustering algorithm based on pre-screening[C]// Proceedings of the 2018 IEEE International Conference on Big Data and Smart Computing.Piscataway, NJ:IEEE, 2018:456-462." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fast density peaks clustering algorithm based on pre-screening">
                                        <b>[76]</b>
                                         XIAO X, DING S, SUN T.A fast density peaks clustering algorithm based on pre-screening[C]// Proceedings of the 2018 IEEE International Conference on Big Data and Smart Computing.Piscataway, NJ:IEEE, 2018:456-462.
                                    </a>
                                </li>
                                <li id="711">


                                    <a id="bibliography_77" title=" GHOSH S, MITRA S.Clustering large data with uncertainty[J].Applied Soft Computing, 2013, 13 (4) :1639-1645." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900004958&amp;v=MDkxNjhVcjNJSjF3VWJ4RT1OaWZPZmJLN0h0VE1wbzlGWk9zTEJYa3hvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[77]</b>
                                         GHOSH S, MITRA S.Clustering large data with uncertainty[J].Applied Soft Computing, 2013, 13 (4) :1639-1645.
                                    </a>
                                </li>
                                <li id="713">


                                    <a id="bibliography_78" title=" LE H S, NGUYEN D T.Tune up fuzzy c-means for big data:some novel hybrid clustering algorithms based on initial selection and incremental clustering[J].International Journal of Fuzzy Systems, 2017, 19 (5) :1585-1602." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDC67B9C49390E287DAE65A467915D0232&amp;v=MzE5NTh2a3F4czBmTWFVUjdtZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9Tmo3QmFzQytHYVBGM0l0TVorSVBlWDR4eUdKaW56bDRPWA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[78]</b>
                                         LE H S, NGUYEN D T.Tune up fuzzy c-means for big data:some novel hybrid clustering algorithms based on initial selection and incremental clustering[J].International Journal of Fuzzy Systems, 2017, 19 (5) :1585-1602.
                                    </a>
                                </li>
                                <li id="715">


                                    <a id="bibliography_79" title=" LIU A, SU Y, NIE W, et al.Hierarchical clustering multi-task learning for joint human action grouping and recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (1) :102-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical clustering multi-task learning for joint human action grouping and recognition">
                                        <b>[79]</b>
                                         LIU A, SU Y, NIE W, et al.Hierarchical clustering multi-task learning for joint human action grouping and recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (1) :102-114.
                                    </a>
                                </li>
                                <li id="717">


                                    <a id="bibliography_80" title=" MADAN S, DANA K J.Modified balanced iterative reducing and clustering using hierarchies (m-BIRCH) for visual clustering[J].Pattern Analysis and Applications, 2016, 19 (4) :1023-1040." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modified balanced iterative reducing and clustering using hierarchies (m-BIRCH)for visual clustering">
                                        <b>[80]</b>
                                         MADAN S, DANA K J.Modified balanced iterative reducing and clustering using hierarchies (m-BIRCH) for visual clustering[J].Pattern Analysis and Applications, 2016, 19 (4) :1023-1040.
                                    </a>
                                </li>
                                <li id="719">


                                    <a id="bibliography_81" title=" GUHA S, RASTOGI R, SHIM K, et al.CURE:an efficient clustering algorithm for large databases[J].Information Systems, 1998, 26 (1) :35-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100892160&amp;v=MzI2MjdsVXIzSUoxd1VieEU9TmlmT2ZiSzdIdERPcm85RmJPSU5EWG81b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[81]</b>
                                         GUHA S, RASTOGI R, SHIM K, et al.CURE:an efficient clustering algorithm for large databases[J].Information Systems, 1998, 26 (1) :35-58.
                                    </a>
                                </li>
                                <li id="721">


                                    <a id="bibliography_82" title=" GUHA S, RASTOGI R, SHIM K.ROCK:a robust clustering algorithm for categorical attributes[J].Information Systems, 1999, 25 (5) :345-366." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100892195&amp;v=MzExNjJJTkRYVThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXdVYnhFPU5pZk9mYks3SHRET3JvOUZiTw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[82]</b>
                                         GUHA S, RASTOGI R, SHIM K.ROCK:a robust clustering algorithm for categorical attributes[J].Information Systems, 1999, 25 (5) :345-366.
                                    </a>
                                </li>
                                <li id="723">


                                    <a id="bibliography_83" title=" GELBARD R, GOLDMAN O, SPIEGLER I.Investigating diversity of clustering methods:an empirical comparison[J].Data and Knowledge Engineering, 2007, 63 (1) :155-166." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300248277&amp;v=MTcxNTJadThIRG5zK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmlmT2ZiSzdIdERPckk5Rg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[83]</b>
                                         GELBARD R, GOLDMAN O, SPIEGLER I.Investigating diversity of clustering methods:an empirical comparison[J].Data and Knowledge Engineering, 2007, 63 (1) :155-166.
                                    </a>
                                </li>
                                <li id="725">


                                    <a id="bibliography_84" title=" BARALDI A, BLONDA P.A survey of fuzzy clustering algorithms for pattern recognition—Part I and II[J].IEEE Transactions on Systems, Man, and Cybernetics—Part B:Cybernetics, 1999, 29 (6) :778-801." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of fuzzy clustering algorithms for pattern recognition. I">
                                        <b>[84]</b>
                                         BARALDI A, BLONDA P.A survey of fuzzy clustering algorithms for pattern recognition—Part I and II[J].IEEE Transactions on Systems, Man, and Cybernetics—Part B:Cybernetics, 1999, 29 (6) :778-801.
                                    </a>
                                </li>
                                <li id="727">


                                    <a id="bibliography_85" title=" ALAHAKOON D, HALGAMUGE S K, SRINIVASAN B.Dynamic self-organizing maps with controlled growth for knowledge discovery[J].IEEE Transactions on Neural Networks, 2000, 11 (3) :601-614." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic Self-Organizing Maps with Controlled Growth for Knowledge Discovery">
                                        <b>[85]</b>
                                         ALAHAKOON D, HALGAMUGE S K, SRINIVASAN B.Dynamic self-organizing maps with controlled growth for knowledge discovery[J].IEEE Transactions on Neural Networks, 2000, 11 (3) :601-614.
                                    </a>
                                </li>
                                <li id="729">


                                    <a id="bibliography_86" title=" YIN H.VISOM:a novel method for multivariate data projection and structure[J].IEEE Transactions on Neural Networks, 2002, 13 (1) :237-243." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ViSOM-a novel method for multivariate data projection and structure visualization">
                                        <b>[86]</b>
                                         YIN H.VISOM:a novel method for multivariate data projection and structure[J].IEEE Transactions on Neural Networks, 2002, 13 (1) :237-243.
                                    </a>
                                </li>
                                <li id="731">


                                    <a id="bibliography_87" title=" CAO Y, WU J.Dynamics of projective adaptive resonance theory model:the foundation of PART algorithm[J].IEEE Transactions on Neural Network, 2004, 15 (2) :245-260." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamics of projective adaptive resonance theory model: the foundation of PART algorithm">
                                        <b>[87]</b>
                                         CAO Y, WU J.Dynamics of projective adaptive resonance theory model:the foundation of PART algorithm[J].IEEE Transactions on Neural Network, 2004, 15 (2) :245-260.
                                    </a>
                                </li>
                                <li id="733">


                                    <a id="bibliography_88" title=" ZHANG Y, LU J, LIU F.Does deep learning help topic extraction?a kernel k-means clustering method with word embedding [J].Journal of Informetrics, 2018, 12 (4) :1099-1117." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES06858764C5555FCEE730F682E7892A6A&amp;v=MTcyOThmT2ZiTytGdFRFcUlsQkYrNEtDWGxQdkdObTdUeDlQbm5xcm1jeWNidVdOTHp1Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[88]</b>
                                         ZHANG Y, LU J, LIU F.Does deep learning help topic extraction?a kernel k-means clustering method with word embedding [J].Journal of Informetrics, 2018, 12 (4) :1099-1117.
                                    </a>
                                </li>
                                <li id="735">


                                    <a id="bibliography_89" title=" HAN J, TAO J, WANG C.FlowNet:a deep learning framework for clustering and selection of streamlines and stream surfaces [J].IEEE Transactions on Visualization and Computer Graphics, 2018, 11:678-689." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FlowNet:a deep learning framework for clustering and selection of streamlines and stream surfaces">
                                        <b>[89]</b>
                                         HAN J, TAO J, WANG C.FlowNet:a deep learning framework for clustering and selection of streamlines and stream surfaces [J].IEEE Transactions on Visualization and Computer Graphics, 2018, 11:678-689.
                                    </a>
                                </li>
                                <li id="737">


                                    <a id="bibliography_90" title=" ZHAO Z, BARIJOUGH K, GERSTLAUSER A.DeepThings:distributed adaptive deep learning inference on resource-constrained IoT edge clusters [J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (11) :2348-2359." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepThings:distributed adaptive deep learning inference on resource-constrained IoT edge clusters">
                                        <b>[90]</b>
                                         ZHAO Z, BARIJOUGH K, GERSTLAUSER A.DeepThings:distributed adaptive deep learning inference on resource-constrained IoT edge clusters [J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (11) :2348-2359.
                                    </a>
                                </li>
                                <li id="739">


                                    <a id="bibliography_91" title=" CORTES C, VAPNIK V.Support vector networks[J].Machine Learning, 1995, 20:273-297." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339251&amp;v=MjQyMjVYcVJyeG94Y01IN1I3cWRaK1p1RmlybFZMM0lKVjA9Tmo3QmFyTzRIdEhOckl4TVp1NE9ZM2s1ekJkaDRqOTlT&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[91]</b>
                                         CORTES C, VAPNIK V.Support vector networks[J].Machine Learning, 1995, 20:273-297.
                                    </a>
                                </li>
                                <li id="741">


                                    <a id="bibliography_92" title=" SCH&#214;LKOPF B, BURGES C, VAPNIK V.Incorporating invariances in support vector learning machines[C]// Proceedings of the 1996 International Conference on Artificial Neural Networks.Berlin:Springer, 1996:47-52." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incorporating Invariances in Support Vector Learning Machines">
                                        <b>[92]</b>
                                         SCH&#214;LKOPF B, BURGES C, VAPNIK V.Incorporating invariances in support vector learning machines[C]// Proceedings of the 1996 International Conference on Artificial Neural Networks.Berlin:Springer, 1996:47-52.
                                    </a>
                                </li>
                                <li id="743">


                                    <a id="bibliography_93" title=" TAX D M J, DUIN R P W.Support vector domain description[J].Pattern Recognition Letters, 1999, 20 (11) :1191-1199." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300415525&amp;v=MjQ5MTdUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VWJ4RT1OaWZPZmJLN0h0RE9ySTlGWU9vS0NYNDhvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[93]</b>
                                         TAX D M J, DUIN R P W.Support vector domain description[J].Pattern Recognition Letters, 1999, 20 (11) :1191-1199.
                                    </a>
                                </li>
                                <li id="745">


                                    <a id="bibliography_94" title=" BEN-HUR A, HORN D, SIEGELMANN H T, et al.A support vector clustering method[C]// Proceedings of the 2000 International Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2000:2724-2727." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A support vector clustering method">
                                        <b>[94]</b>
                                         BEN-HUR A, HORN D, SIEGELMANN H T, et al.A support vector clustering method[C]// Proceedings of the 2000 International Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2000:2724-2727.
                                    </a>
                                </li>
                                <li id="747">


                                    <a id="bibliography_95" title=" BEN-HUR A, HORN D, SIEGELMANN H T, et al.Support vector clustering[J].Journal Machine Learning Research, 2001, 2:125-137." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support vector clustering">
                                        <b>[95]</b>
                                         BEN-HUR A, HORN D, SIEGELMANN H T, et al.Support vector clustering[J].Journal Machine Learning Research, 2001, 2:125-137.
                                    </a>
                                </li>
                                <li id="749">


                                    <a id="bibliography_96" title=" WANG D, YEUNG D S, TSANG T C C.Structured one-class classification[J].IEEE Transactions on Systems, Man, and Cybernetics, 2006, 36 (6) :1283-1295." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structured One-Class Classification">
                                        <b>[96]</b>
                                         WANG D, YEUNG D S, TSANG T C C.Structured one-class classification[J].IEEE Transactions on Systems, Man, and Cybernetics, 2006, 36 (6) :1283-1295.
                                    </a>
                                </li>
                                <li id="751">


                                    <a id="bibliography_97" title=" RAJASEGARAR S, LECKIE C, BEZDEK J C.Centered hyperspherical and hyperellipsoidal one-class support vector machines for anomaly detection in sensor networks[J].IEEE Transactions on Information Forensics and Security, 2010, 5 (3) :518-533." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Centered hyperspherical and hyperellipsoidal one-class support vector machines for anomaly detection in sensor networks">
                                        <b>[97]</b>
                                         RAJASEGARAR S, LECKIE C, BEZDEK J C.Centered hyperspherical and hyperellipsoidal one-class support vector machines for anomaly detection in sensor networks[J].IEEE Transactions on Information Forensics and Security, 2010, 5 (3) :518-533.
                                    </a>
                                </li>
                                <li id="753">


                                    <a id="bibliography_98" title=" AMAMI R, SMITI A.An incremental method combining density clustering and support vector machines for voice pathology detection[J].Computers and Electrical Engineering, 2017, 57:257-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An incremental method combining density clustering and support vector machines for voice pathology detection">
                                        <b>[98]</b>
                                         AMAMI R, SMITI A.An incremental method combining density clustering and support vector machines for voice pathology detection[J].Computers and Electrical Engineering, 2017, 57:257-265.
                                    </a>
                                </li>
                                <li id="755">


                                    <a id="bibliography_99" title=" 李海林, 梁叶.基于中心度的标签传播时间序列聚类方法[J].控制与决策, 2018, 33 (11) :33-41. (LI H L, LIANG Y.Time series clustering method with label propagation based on centrality [J].Control and Decision, 2018, 33 (11) :33-41.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201811004&amp;v=MDc5MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblU3ckxMamZTYmJHNEg5bk5ybzlGWUk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[99]</b>
                                         李海林, 梁叶.基于中心度的标签传播时间序列聚类方法[J].控制与决策, 2018, 33 (11) :33-41. (LI H L, LIANG Y.Time series clustering method with label propagation based on centrality [J].Control and Decision, 2018, 33 (11) :33-41.) 
                                    </a>
                                </li>
                                <li id="757">


                                    <a id="bibliography_100" >
                                        <b>[100]</b>
                                     熊英志.时间序列的特征表示与聚类方法研究[D].镇江:江苏大学, 2018:36-61. (XIONG Y Z.Research on feature representation and clustering algorithm for time series[D].Zhengjiang:Jiangsu University, 2018:36-61.) </a>
                                </li>
                                <li id="759">


                                    <a id="bibliography_101" title=" JONES R H.Longitudinal data with serial correlation:a state space approach[J].Journal of the Royal Statistical Society, 1994, 36 (2) :231-239." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Longitudinal data with serial correlation:a state space approach">
                                        <b>[101]</b>
                                         JONES R H.Longitudinal data with serial correlation:a state space approach[J].Journal of the Royal Statistical Society, 1994, 36 (2) :231-239.
                                    </a>
                                </li>
                                <li id="761">


                                    <a id="bibliography_102" title=" GAFFNEY S, SMYTH P.Trajectory clustering with mixtures of regression models[C]// Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:63-72." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Trajectory clustering with mixtures of regression models">
                                        <b>[102]</b>
                                         GAFFNEY S, SMYTH P.Trajectory clustering with mixtures of regression models[C]// Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:63-72.
                                    </a>
                                </li>
                                <li id="763">


                                    <a id="bibliography_103" title=" DU F, ZHU A, QI F.Interactive visual cluster detection in large geospatial datasets based on dynamic density volume visualization[J].Geocarto International, 2016, 31 (6) :597-611." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDDB8B307A3B081731EF5C3620F4D81CEB&amp;v=MDQxNzBvT1MzbmdyR1F4RGJxVk5zL3RDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5qbkJhc2ZLRnFQUHI0ZzBaNWtQQkgwK3pCZG1uRA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[103]</b>
                                         DU F, ZHU A, QI F.Interactive visual cluster detection in large geospatial datasets based on dynamic density volume visualization[J].Geocarto International, 2016, 31 (6) :597-611.
                                    </a>
                                </li>
                                <li id="765">


                                    <a id="bibliography_104" title=" HU A, CAO J, HU M, et al.Distributed control of cluster synchronisation in networks with randomly occurring non-linearities[J].International Journal of Systems Science, 2015, 65 (4) :1-10." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD62ECD9B7DCFE697D315432292B8D9D95&amp;v=MDI2Nzk0cHYxQ0VKaDVlWG93eUdJUTZ6cDVTMzNncFJCSGNjYWRNYk9hQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1Oam5CYXJXNmE2Sw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[104]</b>
                                         HU A, CAO J, HU M, et al.Distributed control of cluster synchronisation in networks with randomly occurring non-linearities[J].International Journal of Systems Science, 2015, 65 (4) :1-10.
                                    </a>
                                </li>
                                <li id="767">


                                    <a id="bibliography_105" title=" GONG M, CAI Q, CHEN X, et al.Complex network clustering by multiobjective discrete particle swarm optimization based on decomposition [J].IEEE Transactions on Evolutionary Computation, 2014, 18 (1) :82-97." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Complex network clustering by multiobjective discrete particle swarm optimization based on decomposition">
                                        <b>[105]</b>
                                         GONG M, CAI Q, CHEN X, et al.Complex network clustering by multiobjective discrete particle swarm optimization based on decomposition [J].IEEE Transactions on Evolutionary Computation, 2014, 18 (1) :82-97.
                                    </a>
                                </li>
                                <li id="769">


                                    <a id="bibliography_106" title=" GUIMERA R, AMARAL L A N.Functional cartography of complex metabolic networks[J].Nature, 2005, 433 (7028) :895-900." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Functional cartography of complex metabolic networks">
                                        <b>[106]</b>
                                         GUIMERA R, AMARAL L A N.Functional cartography of complex metabolic networks[J].Nature, 2005, 433 (7028) :895-900.
                                    </a>
                                </li>
                                <li id="771">


                                    <a id="bibliography_107" title=" 杨博, 刘大有, LIU J-M, 等.复杂网络聚类方法[J].软件学报, 2009, 20 (1) :54-66. (YANG B, LIU D Y, LIU J-M, et al.Complex network clustering algorithms [J].Journal of Software, 2009, 20 (1) :54-66.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200901008&amp;v=MDI4MzQ5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMTnlmVGJMRzRIdGpNcm8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[107]</b>
                                         杨博, 刘大有, LIU J-M, 等.复杂网络聚类方法[J].软件学报, 2009, 20 (1) :54-66. (YANG B, LIU D Y, LIU J-M, et al.Complex network clustering algorithms [J].Journal of Software, 2009, 20 (1) :54-66.) 
                                    </a>
                                </li>
                                <li id="773">


                                    <a id="bibliography_108" title=" GIRVAN M, NEWMAN M E J.Community structure in social and biological networks[J].Proceedings of the National Academy of Science, 2002, 99 (12) :7821-7826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Community structure in social and biological networks">
                                        <b>[108]</b>
                                         GIRVAN M, NEWMAN M E J.Community structure in social and biological networks[J].Proceedings of the National Academy of Science, 2002, 99 (12) :7821-7826.
                                    </a>
                                </li>
                                <li id="775">


                                    <a id="bibliography_109" title=" 郭玉泉, 李雄飞.复杂网络社区的分形聚类检测方法[J].吉林大学学报 (工学版) , 2016, 46 (5) :1633-1638. (GUO Y Q, LI X F.Fractal clustering method for uncovering community of complex network [J].Journal of Jilin University (Engineering and Technology Edition) , 2016, 46 (5) :1633-1638.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201605037&amp;v=Mjg3NDc0SDlmTXFvOUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVTdyTEx5SE1kN0c=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[109]</b>
                                         郭玉泉, 李雄飞.复杂网络社区的分形聚类检测方法[J].吉林大学学报 (工学版) , 2016, 46 (5) :1633-1638. (GUO Y Q, LI X F.Fractal clustering method for uncovering community of complex network [J].Journal of Jilin University (Engineering and Technology Edition) , 2016, 46 (5) :1633-1638.) 
                                    </a>
                                </li>
                                <li id="777">


                                    <a id="bibliography_110" title=" 赵凤, 刘汉强, 范九伦.基于互补空间信息的多目标进化聚类图像分割[J].电子与信息学报, 2015, 37 (3) :672-678. (ZHAO F, LIU H Q, FAN J L.Multi-objective evolutionary clustering with complementary spatial information for image segmentation [J].Journal of Electronics and Information Technology, 2015, 37 (3) :672-678.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201503025&amp;v=MTE4MjhIOVRNckk5SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMSVRmU2RyRzQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[110]</b>
                                         赵凤, 刘汉强, 范九伦.基于互补空间信息的多目标进化聚类图像分割[J].电子与信息学报, 2015, 37 (3) :672-678. (ZHAO F, LIU H Q, FAN J L.Multi-objective evolutionary clustering with complementary spatial information for image segmentation [J].Journal of Electronics and Information Technology, 2015, 37 (3) :672-678.) 
                                    </a>
                                </li>
                                <li id="779">


                                    <a id="bibliography_111" title=" 张引, 潘云鹤.基于模拟退火的最大似然聚类图像分割算法[J].软件学报, 2001, 12 (2) :212-218. (ZHANG Y, PAN Y H.Simulated annealing based maximum likelihood clustering algorithm for image segmentation [J].Journal of Software, 2001, 12 (2) :212-218.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200102007&amp;v=MDA5Mzh0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMTnlmVGJMRzRIdERNclk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[111]</b>
                                         张引, 潘云鹤.基于模拟退火的最大似然聚类图像分割算法[J].软件学报, 2001, 12 (2) :212-218. (ZHANG Y, PAN Y H.Simulated annealing based maximum likelihood clustering algorithm for image segmentation [J].Journal of Software, 2001, 12 (2) :212-218.) 
                                    </a>
                                </li>
                                <li id="781">


                                    <a id="bibliography_112" title=" GLOVER F.Tabu search, Part I[J].ORSA Journal of Computing, 1989, 1 (3) :190-206." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJIF&amp;filename=SJIF13022100009273&amp;v=MDM1NThzNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmlmQ2FMSzdIdFBPcm85RlpPc0dEbg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[112]</b>
                                         GLOVER F.Tabu search, Part I[J].ORSA Journal of Computing, 1989, 1 (3) :190-206.
                                    </a>
                                </li>
                                <li id="783">


                                    <a id="bibliography_113" title=" BOUYER A, HATAMLOU A.An efficient hybrid clustering method based on improved cuckoo optimization and modified particle swarm optimization algorithms[J].Applied Soft Computing, 2018, 67:172-182." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC5DED3C8ED4FBA96111B92C86EF1B4F2&amp;v=MjYzMTBzPU5pZk9mY0M5YXFTNHJQeE5FWjhMZWc1SXhoQVM2ejRQUVgyUnBCUkFEN1BtUWN5ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[113]</b>
                                         BOUYER A, HATAMLOU A.An efficient hybrid clustering method based on improved cuckoo optimization and modified particle swarm optimization algorithms[J].Applied Soft Computing, 2018, 67:172-182.
                                    </a>
                                </li>
                                <li id="785">


                                    <a id="bibliography_114" title=" MIRJALILI S, MIRJALILI S M, LEWIS A.Grey wolf optimizer[J].Advances in Engineering Software, 2014, 69 (3) :46-61." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14021300038273&amp;v=Mjg0NDZzNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmlmT2ZiSzhIdFBOckk5RlpPZ0hEbg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[114]</b>
                                         MIRJALILI S, MIRJALILI S M, LEWIS A.Grey wolf optimizer[J].Advances in Engineering Software, 2014, 69 (3) :46-61.
                                    </a>
                                </li>
                                <li id="787">


                                    <a id="bibliography_115" title=" 李麟玮, 吴益平, 苗发盛.基于灰狼支持向量机的非等时距滑坡位移预测[J].浙江大学学报 (工学版) , 2018, 52 (10) :167-175. (LI L W, WU Y P, MIAO F S.Prediction of non-equidistant landslide displacement time series based on grey wolf support vector machine [J].Journal of Zhejiang University (Engineering Science) , 2018, 52 (10) :167-175.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC201810020&amp;v=Mjg3MDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVTdyTFB5blJiYkc0SDluTnI0OUhaSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[115]</b>
                                         李麟玮, 吴益平, 苗发盛.基于灰狼支持向量机的非等时距滑坡位移预测[J].浙江大学学报 (工学版) , 2018, 52 (10) :167-175. (LI L W, WU Y P, MIAO F S.Prediction of non-equidistant landslide displacement time series based on grey wolf support vector machine [J].Journal of Zhejiang University (Engineering Science) , 2018, 52 (10) :167-175.) 
                                    </a>
                                </li>
                                <li id="789">


                                    <a id="bibliography_116" title=" DENG C, LIU Y, XU L, et al.A MapReduce-based parallel K-means clustering for large-scale CIM data verification[J].Concurrency and Computation Practice and Experience, 2016, 28 (11) :3096-3114." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD42D9D02BA804C34088B520303CF3C9E4&amp;v=MjYxNjVCRkdEN0huVE0rYkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmY2FyZTZhdGk0cjQwM0ZlTVBDQTg2eXhZYjRrMTRTbi9ocg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[116]</b>
                                         DENG C, LIU Y, XU L, et al.A MapReduce-based parallel K-means clustering for large-scale CIM data verification[J].Concurrency and Computation Practice and Experience, 2016, 28 (11) :3096-3114.
                                    </a>
                                </li>
                                <li id="791">


                                    <a id="bibliography_117" title=" HE Y, TAN H, LUO W, et al.MR-DBSCAN:a scalable MapReduce-based DBSCAN algorithm for heavily skewed data[J].Frontiers of Computer Science, 2014, 8 (1) :83-99." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300030304&amp;v=MDA3OTNNbndaZVp0RmlubFVyM0lKMXdVYnhFPU5qN0Jhcks4SHRMTXJJOUZaT2dQRDN3OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[117]</b>
                                         HE Y, TAN H, LUO W, et al.MR-DBSCAN:a scalable MapReduce-based DBSCAN algorithm for heavily skewed data[J].Frontiers of Computer Science, 2014, 8 (1) :83-99.
                                    </a>
                                </li>
                                <li id="793">


                                    <a id="bibliography_118" title=" LI J, CHEN Q, LIU B.Classification and disease probability prediction via machine learning programming based on multi-GPU cluster MapReduce system[J].Journal of Supercomputing, 2017, 73 (5) :1782-1809." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD8ACC45B6FDA290A3209D0FEC13DFB88&amp;v=MTIxOTdTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OajdCYXNld2I2Sy9xNG8zWXAxN2ZYNHd6MmNRNkQ5MFBIK1UyV0UwZXNiaU43S1hDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[118]</b>
                                         LI J, CHEN Q, LIU B.Classification and disease probability prediction via machine learning programming based on multi-GPU cluster MapReduce system[J].Journal of Supercomputing, 2017, 73 (5) :1782-1809.
                                    </a>
                                </li>
                                <li id="795">


                                    <a id="bibliography_119" title=" LU Y, CAO B, REGO C.A tabu search based clustering algorithm and its parallel implementation on Spark[J].Applied Soft Computing, 2018, 63:97-109." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5735B6E92EA92AAA53090450CD419590&amp;v=MjQxNDByR0ZCZmJPZFFMT2ZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mYmEvSGRTK3FmcE1acDUrQlg1SXZtY1c2VDkwU0h2bg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[119]</b>
                                         LU Y, CAO B, REGO C.A tabu search based clustering algorithm and its parallel implementation on Spark[J].Applied Soft Computing, 2018, 63:97-109.
                                    </a>
                                </li>
                                <li id="797">


                                    <a id="bibliography_120" title=" ZHOU A, WANG H, SONG P.Experiments on light vertex matching algorithm for multilevel partitioning of network topology[J].Procedia Engineering, 2012, 29:2715-2720." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300066838&amp;v=MTUyNjdpclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VWJ4RT1OaWZPZmJLN0h0RE5ySTlGWk8wSkJIOHhvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[120]</b>
                                         ZHOU A, WANG H, SONG P.Experiments on light vertex matching algorithm for multilevel partitioning of network topology[J].Procedia Engineering, 2012, 29:2715-2720.
                                    </a>
                                </li>
                                <li id="799">


                                    <a id="bibliography_121" title=" ANDRADE G, RAMOS G, MADEIRA D, et al.G-DBSCAN:a GPU accelerated algorithm for density-based clustering[J].Procedia Computer Science, 2013, 18 (1) :369-378." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13080100007701&amp;v=MDU0MTdNbndaZVp0RmlubFVyM0lKMXdVYnhFPU5pZk9mYks3SHRuTXJvOUZaT3NJQzN3NG9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[121]</b>
                                         ANDRADE G, RAMOS G, MADEIRA D, et al.G-DBSCAN:a GPU accelerated algorithm for density-based clustering[J].Procedia Computer Science, 2013, 18 (1) :369-378.
                                    </a>
                                </li>
                                <li id="801">


                                    <a id="bibliography_122" title=" MELO D, TOLEDO S, MOUR&#195;O F, et al.Hierarchical density-based clustering based on GPU accelerated data indexing strategy[J].Procedia Computer Science, 2016, 80:951-961." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Density-Based Clustering based on GPU Accelerated Data Indexing Strategy">
                                        <b>[122]</b>
                                         MELO D, TOLEDO S, MOUR&#195;O F, et al.Hierarchical density-based clustering based on GPU accelerated data indexing strategy[J].Procedia Computer Science, 2016, 80:951-961.
                                    </a>
                                </li>
                                <li id="803">


                                    <a id="bibliography_123" title=" PARSONS L, HAQUE E, LIU H.Subspace clustering for high dimensional data:a review [J].ACM SIGKDD Explorations Newsletter, 2004, 6 (1) :90-105." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000085405&amp;v=Mjc1NjBDSHc4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VWJ4RT1OaWZJWTdLN0h0ak5yNDlGWk9NSw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[123]</b>
                                         PARSONS L, HAQUE E, LIU H.Subspace clustering for high dimensional data:a review [J].ACM SIGKDD Explorations Newsletter, 2004, 6 (1) :90-105.
                                    </a>
                                </li>
                                <li id="805">


                                    <a id="bibliography_124" title=" YIN M, XIE S, WU Z, et al.Subspace clustering via learning an adaptive low-rank graph.[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3716-3728." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Subspace clustering via learning an adaptive low-rank graph">
                                        <b>[124]</b>
                                         YIN M, XIE S, WU Z, et al.Subspace clustering via learning an adaptive low-rank graph.[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3716-3728.
                                    </a>
                                </li>
                                <li id="807">


                                    <a id="bibliography_125" title=" AGRAWAL R, GEHRKE J, GUNOPULOS D, et al.Automatic subspace clustering of high dimensional data for data mining applications[C]// Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1998:94-105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic subspace clustering of high dimensional data for data mining applications">
                                        <b>[125]</b>
                                         AGRAWAL R, GEHRKE J, GUNOPULOS D, et al.Automatic subspace clustering of high dimensional data for data mining applications[C]// Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1998:94-105.
                                    </a>
                                </li>
                                <li id="809">


                                    <a id="bibliography_126" title=" CHENG C H, FU A W, ZHANG Y.Entropy-based subspace clustering for mining numerical data[C]// Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:84-93" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Entropy-based subspace clustering for mining numerical data">
                                        <b>[126]</b>
                                         CHENG C H, FU A W, ZHANG Y.Entropy-based subspace clustering for mining numerical data[C]// Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:84-93
                                    </a>
                                </li>
                                <li id="811">


                                    <a id="bibliography_127" title=" GOIL S, NAGESH H, CHOUDHARY A.MAFIA:efficient and scalable subspace clustering for very large data sets[C]// Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:443-452." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MAFIA:Efficient and scalable subspace clustering for very large data sets">
                                        <b>[127]</b>
                                         GOIL S, NAGESH H, CHOUDHARY A.MAFIA:efficient and scalable subspace clustering for very large data sets[C]// Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:443-452.
                                    </a>
                                </li>
                                <li id="813">


                                    <a id="bibliography_128" title=" AGGARWAL C C, YU P S.Finding generalized projected clusters in high dimensional spaces[C]// Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data.New York:ACM, 2000:70-81." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Finding generalized projected clusters in high dimensional spaces">
                                        <b>[128]</b>
                                         AGGARWAL C C, YU P S.Finding generalized projected clusters in high dimensional spaces[C]// Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data.New York:ACM, 2000:70-81.
                                    </a>
                                </li>
                                <li id="815">


                                    <a id="bibliography_129" title=" WOO K G, LEE J H, KIM M H, et al.FINDIT:a fast and intelligent subspace clustering algorithm using dimension voting[J].Information and Software Technology, 2004, 46 (4) :255-271." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501721381&amp;v=MjI1MDNVYnhFPU5pZk9mYks3SHRETnFvOUVZK2tPRDNRNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxdw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[129]</b>
                                         WOO K G, LEE J H, KIM M H, et al.FINDIT:a fast and intelligent subspace clustering algorithm using dimension voting[J].Information and Software Technology, 2004, 46 (4) :255-271.
                                    </a>
                                </li>
                                <li id="817">


                                    <a id="bibliography_130" title=" SIM K, GOPALKRISHNAN V, ZIMEK A, et al.A survey on enhanced subspace clustering[J].Data Mining and Knowledge Discovery, 2013, 26 (2) :332-397." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130122008732&amp;v=MTQ2MzE5ZmJ2bktyaWZaZVp2RnlublU3aktJRnNYTmo3QmFySzdIdERPclk5RmJPd01EaE04enhVU21EZDlTSDduM3hF&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[130]</b>
                                         SIM K, GOPALKRISHNAN V, ZIMEK A, et al.A survey on enhanced subspace clustering[J].Data Mining and Knowledge Discovery, 2013, 26 (2) :332-397.
                                    </a>
                                </li>
                                <li id="819">


                                    <a id="bibliography_131" title=" BOUGUILA N.A model-based approach for discrete data clustering and feature weighting using MAP and stochastic complexity[J].IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (12) :1649-1664." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A model-based approach for discrete data clustering and feature weighting using MAP and stochastic complexity">
                                        <b>[131]</b>
                                         BOUGUILA N.A model-based approach for discrete data clustering and feature weighting using MAP and stochastic complexity[J].IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (12) :1649-1664.
                                    </a>
                                </li>
                                <li id="821">


                                    <a id="bibliography_132" title=" CHEN L, WANG S, WANG K, et al.Soft subspace clustering of categorical data with probabilistic distance[J].Pattern Recognition, 2016, 51 (C) :322-332." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9BFC72BF5E0B097FF5CAF058B16727D2&amp;v=MDA5OTJRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mYnJLYUtMTHJmMHpZWjRQZm53d3lHQmw3MHdNUG4vbnBHQTBmN1dXUXM2ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPRw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[132]</b>
                                         CHEN L, WANG S, WANG K, et al.Soft subspace clustering of categorical data with probabilistic distance[J].Pattern Recognition, 2016, 51 (C) :322-332.
                                    </a>
                                </li>
                                <li id="823">


                                    <a id="bibliography_133" title=" DENG Z, CHOI K-S, WANG J, et al.A survey on soft subspace clustering[J].Information Sciences, 2014, 348:84-106." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3B6CED85BBFEA79C9D3ED34F63FA668C&amp;v=MzA0NjBHS0s1MjRkQUZwbDVlUTAreG1VYW5qd0lQSHptMmhRMkQ4T1NRN0xzQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaWZPZmJESw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[133]</b>
                                         DENG Z, CHOI K-S, WANG J, et al.A survey on soft subspace clustering[J].Information Sciences, 2014, 348:84-106.
                                    </a>
                                </li>
                                <li id="825">


                                    <a id="bibliography_134" title=" CHENG Y, CHURCH G M.Biclustering of expression data[C]// Proceedings of the 2000 International Conference of Intelligent Systems for Molecular Biology.New York:ACM, 2000:93-103." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Biclustering of expression data">
                                        <b>[134]</b>
                                         CHENG Y, CHURCH G M.Biclustering of expression data[C]// Proceedings of the 2000 International Conference of Intelligent Systems for Molecular Biology.New York:ACM, 2000:93-103.
                                    </a>
                                </li>
                                <li id="827">


                                    <a id="bibliography_135" title=" HALKIDI M, BATISTAKIS Y, VAZIRGIANNIS M.Cluster validity methods[C]// Proceedings of the 2002 International Conference on Special Interest Group on Management of Data.New York:ACM, 2002:127-131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cluster validity methods">
                                        <b>[135]</b>
                                         HALKIDI M, BATISTAKIS Y, VAZIRGIANNIS M.Cluster validity methods[C]// Proceedings of the 2002 International Conference on Special Interest Group on Management of Data.New York:ACM, 2002:127-131.
                                    </a>
                                </li>
                                <li id="829">


                                    <a id="bibliography_136" title=" THEODORIDIS S, KOUTROUMBAS K.Pattern Recognition [M].3rd ed.San Diego:Academic Press, 2006:56-63." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pattern Recognition">
                                        <b>[136]</b>
                                         THEODORIDIS S, KOUTROUMBAS K.Pattern Recognition [M].3rd ed.San Diego:Academic Press, 2006:56-63.
                                    </a>
                                </li>
                                <li id="831">


                                    <a id="bibliography_137" title=" JOS&#201;-GARC&#205;A A, G&#211;MEZ-FLORES W.Automatic clustering using nature-inspired metaheuristics:a survey[J].Applied Soft Computing, 2015, 41:192-213." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES520C43D733D488CEFF829F685D4F00BC&amp;v=MDE5ODN2RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mYmE2SHFMSXJQdENaK2g3Q0hReHZHTmxuRGQvUVFua3BCZEJmY1NVUmNqc0NPTg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[137]</b>
                                         JOS&#201;-GARC&#205;A A, G&#211;MEZ-FLORES W.Automatic clustering using nature-inspired metaheuristics:a survey[J].Applied Soft Computing, 2015, 41:192-213.
                                    </a>
                                </li>
                                <li id="833">


                                    <a id="bibliography_138" title=" LIAN C, RUAN S, DENOEUX T, et al.Joint tumor segmentation in PET-CT images using co-clustering and fusion based on belief functions[J].IEEE Transactions on Image Processing, 2019, 28 (2) :755-766." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint tumor segmentation in pet-ct images using co-clustering and fusion based on belief functions">
                                        <b>[138]</b>
                                         LIAN C, RUAN S, DENOEUX T, et al.Joint tumor segmentation in PET-CT images using co-clustering and fusion based on belief functions[J].IEEE Transactions on Image Processing, 2019, 28 (2) :755-766.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-15 14:12</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),1869-1882 DOI:10.11772/j.issn.1001-9081.2019010174            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>聚类算法综述</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%AB%A0%E6%B0%B8%E6%9D%A5&amp;code=35011879&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">章永来</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E8%80%80%E9%89%B4&amp;code=42202176&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周耀鉴</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%8C%97%E5%A4%A7%E5%AD%A6%E8%BD%AF%E4%BB%B6%E5%AD%A6%E9%99%A2&amp;code=0036109&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中北大学软件学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>大数据时代, 聚类这种无监督学习算法的地位尤为突出。近年来, 对聚类算法的研究取得了长足的进步。首先, 总结了聚类分析的全过程、相似性度量、聚类算法的新分类及其结果的评价等内容, 将聚类算法重新划分为大数据聚类与小数据聚类两个大类, 并特别对大数据聚类作了较为系统的分析与总结。此外, 概述并分析了各类聚类算法的研究进展及其应用概况, 并结合研究课题讨论了算法的发展趋势。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相似性度量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">大数据聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%8F%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">小数据聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BB%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类评价;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    章永来 (1978—) , 男, 浙江诸暨人, 助理教授, 博士, 主要研究方向:大数据分析与处理、医疗大数据、海洋大数据;;
                                </span>
                                <span>
                                    *周耀鉴 (1987—) , 男, 湖北武穴人, 助理教授, 博士, 主要研究方向:大数据分析与处理、海洋大数据、水下机器人。电子邮箱zhouyj@nuc.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (6160051296);</span>
                    </p>
            </div>
                    <h1><b>Review of clustering algorithms</b></h1>
                    <h2>
                    <span>ZHANG Yonglai</span>
                    <span>ZHOU Yaojian</span>
            </h2>
                    <h2>
                    <span>Software School, North University of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Clustering is very important as an unsupervised learning algorithm in the age of big data. Recently, considerable progress has been made in the analysis of clustering algorithm. Firstly, the whole process of clustering, similarity measurement, new classification of clustering algorithms and evaluation on their results were summarized. Clustering algorithms were divided into two categories: big data clustering and small data clustering, and the systematic analysis and summary of big data clustering were carried out particularly. Moreover, the research progress and application of various clustering algorithms were summarized and analyzed, and the development trend of clustering algorithms was discussed in combination with the research topics.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=similarity%20measurement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">similarity measurement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=big%20data%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">big data clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=small%20data%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">small data clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=clustering%20evaluation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">clustering evaluation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Yonglai, born in 1978, Ph. D. , assistant professor. His research interests include big data analysis and processing, medical big data, ocean big data. ;
                                </span>
                                <span>
                                    ZHOU Yaojian, born in 1987, Ph. D. , assistant professor. His research interests include big data analysis and processing, ocean big data, underwater robots.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-23</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (6160051296);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="279" name="279" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="280">把具有相似特性的实物放到一起是人类最原始的活动之一。这也是聚类的最初目的。早在1984年, Aldenderfer等<citation id="835" type="reference"><link href="559" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>就已经提出了聚类分析的四大功能:一是数据分类的进一步扩展;二是对实体归类的概念性探索;三是通过数据探索而生成假说;四是一种基于实际数据集归类假说的测试方式。在很多情况下, 样本数据集并没有分类, 即每一个数据样本都没有分类标签。一般而言, 聚类指将没有分类标签的数据集, 分为若干个簇的过程, 是一种无监督的分类方法<citation id="836" type="reference"><link href="561" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。实际上, 很难对聚类下一个明确的定义。2001年, Everitt等<citation id="837" type="reference"><link href="563" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>甚至指出提出聚类的正式定义不仅困难而且也没有必要, 因为聚类分析本身是一种建立在主观判断基础上的相对行之有效的方法<citation id="839" type="reference"><link href="565" rel="bibliography" /><link href="567" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。尽管如此, 聚类分析还是表达了一般认为的“类内的相似性与类间的排他性”的目标。Hansen等<citation id="838" type="reference"><link href="569" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>也已经作了数学上的阐述。给定一个数据样本集:</p>
                </div>
                <div class="p1">
                    <p id="281"><i>C</i>={<b><i>X</i></b><sub>1</sub>, <b><i>X</i></b><sub>2</sub>, …, <b><i>X</i></b><sub><i>j</i></sub>, …, <b><i>X</i></b><sub><i>N</i></sub>;<b><i>X</i></b><sub><i>j</i></sub>= (<i>x</i><sub><i>j</i>1</sub>, <i>x</i><sub><i>j</i>2</sub>, …, <i>x</i><sub><i>jd</i></sub>) ∈<b>R</b><sup><i>d</i></sup>}      (1) </p>
                </div>
                <div class="p1">
                    <p id="282">这里, <b><i>X</i></b><sub><i>j</i></sub>表示一个向量, 称为样本点或者样本;<i>X</i><sub><i>jd</i></sub>表示一个变量, 通常称为属性、特征、变量或维等。划分聚类将数据集分为<i>K</i>个簇, 需满足:</p>
                </div>
                <div class="p1">
                    <p id="283" class="code-formula">
                        <mathml id="283"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>C</mi><mo>=</mo><mo stretchy="false">{</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>C</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo stretchy="false">}</mo><mo>;</mo><mspace width="0.25em" /><mi>Κ</mi><mo>≤</mo><mi>Ν</mi></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≠</mo><mo>∅</mo><mo>;</mo><mspace width="0.25em" /><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Κ</mi></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∪</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow></mrow></mstyle><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>X</mi></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><mo>∩</mo><mi>C</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mo>∅</mo><mo>;</mo><mspace width="0.25em" /><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Κ</mi><mo>, </mo><mi>i</mi><mo>≠</mo><mi>j</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="284">而层次聚类是将数据集构建成一种树状的结构, 即:</p>
                </div>
                <div class="p1">
                    <p id="285" class="code-formula">
                        <mathml id="285"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>Η</mi><mo>=</mo><mo stretchy="false">{</mo><mi>Η</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>Η</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Η</mi><msub><mrow></mrow><mi>Q</mi></msub><mo stretchy="false">}</mo><mo>;</mo><mspace width="0.25em" /><mi>Q</mi><mo>≤</mo><mi>Ν</mi></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>Η</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Η</mi><msub><mrow></mrow><mi>l</mi></msub><mo>;</mo><mspace width="0.25em" /><mi>m</mi><mo>&gt;</mo><mi>l</mi></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⊂</mo><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mspace width="0.25em" /><mtext>o</mtext><mtext>r</mtext><mspace width="0.25em" /><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><mo>∩</mo><mi>C</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mo>∅</mo><mo>;</mo><mspace width="0.25em" /><mi>m</mi><mo>, </mo><mi>l</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Q</mi><mo>, </mo><mspace width="0.25em" /><mi>i</mi><mo>≠</mo><mi>j</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="286">聚类分析是伴随着统计学、计算机学与人工智能等领域科学的发展而逐步发展起来的, 为此, 这些领域若有较大的研究进展, 必然促进聚类分析算法的快速发展。比如机器学习领域的人工神经网络与支持向量机的发展就促生了基于神经网络的聚类方法与核聚类方法。目前, 基于人工神经网络的深度学习 (如:AlphaGo围棋系统) 也必将推动聚类分析方法的进一步发展。到目前为止, 聚类研究及其应用领域已经非常广泛, 因此, 本文主要以聚类分析算法为主要分析对象, 兼论聚类分析的全过程。</p>
                </div>
                <div class="p1">
                    <p id="287">关于聚类分析, 《数据挖掘概念与技术 (第二版) 》一书中已经有了经典的论述。然而, 聚类算法又有了长足的发展与进步。本文首先简要介绍了聚类分析的主要过程, 然后分析并总结了样本点之间的相似性度量方法, 提出了聚类算法的新分类方式, 并总结与分析了各种聚类算法, 还对如何评价聚类结果作了过程分析。最后, 依靠课题组承担的医疗与海洋大数据的聚类分析研究<citation id="840" type="reference"><link href="571" rel="bibliography" /><link href="573" rel="bibliography" /><link href="575" rel="bibliography" /><link href="577" rel="bibliography" /><link href="579" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>, 展望了聚类算法的发展趋势, 作为本文的结语。</p>
                </div>
                <h3 id="288" name="288" class="anchor-tag">1 聚类分析过程</h3>
                <div class="p1">
                    <p id="289">聚类分析是一个较为严密的数据分析过程。聚类分析的全过程如图1所示, 从聚类对象数据源开始到得到聚类结果的知识存档为止, 其中主要包括四个部分研究内容, 即特征选择或变换、聚类算法选择或设计、聚类结果评价与聚类结果物理解析等。</p>
                </div>
                <div class="area_img" id="290">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907002_290.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 聚类分析过程" src="Detail/GetImg?filename=images/JSJY201907002_290.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 聚类分析过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907002_290.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 1 <i>Clustering analysis process</i></p>

                </div>
                <h4 class="anchor-tag" id="291" name="291">1.1 <b>特征选择或变换</b></h4>
                <div class="p1">
                    <p id="292">一般情况下, 样本数据是杂乱无章的 (特别是大数据时代) , 聚类分析首先需要进行数据集的特征选择或变换。实际上, 特征选择与特征变换是降维技术的两大分类。特征选择指的是从数据样本集的所有特征 (或称属性) 中选择更有利于达到某种目标的若干属性, 即原始属性集的一个子集, 同时也达到了降低维度的目的;而特征变换则是指通过某种变换将原始输入空间的属性映射到一个新的特征空间, 然后在特征空间中根据规则选择某些较为重要的变换后的特征。由于特征选择并不改变其原有属性, 所以结果只是一个原始属性的优化特征子集, 保留了原属性的物理意义, 方便用户理解;而特征变换的结果失去了原始特征的物理意义, 但能够提取其隐含的特征信息, 移除原特征集属性之间的相关性与冗余性。特征选择或变换在聚类分析过程中占据极其重要的地位, 结果的优劣将直接影响最后的聚类效果, 应该引起足够的重视。有时, 特征选择或变换后得到的有效模式 (或称子集) 的作用甚至超过聚类算法本身的效用。</p>
                </div>
                <h4 class="anchor-tag" id="293" name="293">1.2 <b>聚类算法选择或设计</b></h4>
                <div class="p1">
                    <p id="294">依据特征选择或变换后的数据集特性, 选择或设计聚类算法, 是聚类分析的第二部分研究内容。如果样本集数据都是数值型数据, 在选择或者设计聚类算法时需要注意量纲不同的问题。一般情况下, 样本集数据不一定都是数值型数据, 因此, 聚类算法需要有处理非数值型数据的能力。各个样本点之间的相似性度量是聚类算法中的首要问题。相似性度量与经常提到的样本间“距离”有着相同的意义, 但是, 它们的取值却正好相反, 即相似性度量值越大, “距离”越近。同样, 相似性度量也是聚类分析全过程中的关键问题之一, 将在后文进行详细的介绍与分析。</p>
                </div>
                <h4 class="anchor-tag" id="295" name="295">1.3 <b>聚类结果评价与物理解析</b></h4>
                <div class="p1">
                    <p id="296">聚类簇只能依靠聚类结束准则函数得到<citation id="841" type="reference"><link href="581" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 需要特别指出的是, 这种准则函数一般由人为设定的终止条件实现, 而这些终止条件并没有统一的标准。由此可见聚类分析是一个主观的归类过程, 所以在聚类簇生成以后, 必须对聚类结果进行综合评价。聚类分析的本来目标是得到特定数据集中隐含的数据结构。更何况, 对于同样一个数据集, 不同的聚类算法一般会得到不同的聚类簇。然而, 对聚类结果作了评价之后, 仍然不能改变聚类分析是“通过数据探索而生成假说”的实质, 因此, 最后需要对聚类结果作物理上的解析。</p>
                </div>
                <div class="p1">
                    <p id="297">在聚类结果评价后一段较长的时间内, 需要对一种或者几种聚类结果假说, 总结出实际的物理意义。聚类簇的物理解析应该与具有实际工作经验的专家作深入的探讨与分析。最后才可以将探讨的结果加入到知识库, 作为进一步研究的依据。可见, 聚类物理解析并不属于学术研究的范畴, 而是一个长期的验证过程。</p>
                </div>
                <h3 id="298" name="298" class="anchor-tag">2 相似性度量</h3>
                <div class="p1">
                    <p id="299">聚类分析是将数据集的相似性样本归为若干类的方法, 因此, 如何度量样本之间的相似性是聚类算法的关键问题。假设样本间的相似性满足对称性、非负性和反身性, 则称样本间的相似性具有可度量性 (<i>Metric</i>) 。另外, 需要注意的是, 三角不等式的半度量 (<i>SemiMetric</i>) 和超度量 (<i>UltraMetric</i>) 这两种非可度量方式不在本文的探讨范围内。数据集的特征一般分为三种:连续性变量 (或称定量型变量) 、离散性变量 (或称定性型变量) 和混合变量。相应的, 有三种相似性度量方法。</p>
                </div>
                <h4 class="anchor-tag" id="300" name="300">2.1 <b>连续性变量的相似性度量</b></h4>
                <h4 class="anchor-tag" id="301" name="301">1) 欧氏距离 (<i>Euclidean Distance</i>) 。</h4>
                <div class="p1">
                    <p id="302">这是一种最常用的样本间距离度量方法, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="303" class="code-formula">
                        <mathml id="303"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>l</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="304">其中:<i>D</i>表示样本之间的距离;<b><i>X</i></b><sub><i>i</i></sub>与<b><i>X</i></b><sub><i>j</i></sub>表示一个向量, 或称为样本点或者样本;<i>l</i>是样本特征的维数;<i>x</i><sub><i>il</i></sub>与<i>x</i><sub><i>jl</i></sub>表示一个变量, 或称为属性;<i>d</i>表示样本的总维数, 即样本特征的总数量 (以下同) 。欧氏距离是一种二范数形式, 具有在特征空间中转化和旋转的不变性, 一般趋向于构建球形聚类簇。然而, 属性值相差较大或线性变换都会使相关性产生形变<citation id="842" type="reference"><link href="583" rel="bibliography" /><link href="585" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="305">为了解决这个问题, 需要标准化处理目标数据集, 使每一个属性对距离的贡献率相同, 这也是消除特征之间量纲差异的常规方式。在进行数据分析之前, 需要对样本集在均值与方差上作标准化处理<citation id="843" type="reference"><link href="587" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。标准化计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="306"><i>x</i><sub><i>il</i></sub>= (<i>x</i><sup>*</sup><sub><i>il</i></sub>-<i>m</i><sub><i>l</i></sub>) /<i>S</i><sub><i>l</i></sub>      (5) </p>
                </div>
                <div class="p1">
                    <p id="307">其中:<i>m</i>为均值;<i>S</i>为方差;*表示特征的原值 (以下同) 。另外, 为了去掉不同属性值间在量纲上的差别, 需要对样本集作正则化处理。例如在[0, 1]区间内的正则化公式为:</p>
                </div>
                <div class="p1">
                    <p id="308" class="code-formula">
                        <mathml id="308"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow><mo>*</mo></msubsup><mo>-</mo><mi>min</mi><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo></mrow><mrow><mi>max</mi><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo>-</mo><mi>min</mi><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="309" name="309">2) 切比雪夫距离 (Chebyshev Distance) 。</h4>
                <div class="p1">
                    <p id="310">在二维空间中, 切比雪夫距离的典型应用是解决国际象棋中的国王从一个格子走到另一个格子最少需要几步的问题。这种距离在模糊C-Means方法<citation id="844" type="reference"><link href="589" rel="bibliography" /><link href="591" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>中得到了有效应用。切比雪夫距离的公式可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="311" class="code-formula">
                        <mathml id="311"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>l</mi></munder><mo stretchy="false"> (</mo><mrow><mo>|</mo><mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>l</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>l</mi></mrow></msub></mrow><mo>|</mo></mrow><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="312">此公式的另外一种表示形式为:</p>
                </div>
                <div class="p1">
                    <p id="313" class="code-formula">
                        <mathml id="313"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>lim</mi></mrow></mstyle><mrow><mi>p</mi><mspace width="0.25em" /><mo>→</mo><mspace width="0.25em" /><mi>∞</mi></mrow></munder><mroot><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>l</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>l</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mi>p</mi></mroot><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="314" name="314">3) 曼哈顿距离 (Manhattan Distance) 。</h4>
                <div class="p1">
                    <p id="315">在城市中生活, 只能沿着街道从一个地方到另一个地方, 为此, 人们将生活中熟悉的城市街区距离 (City Block Distance) 形象地称为曼哈顿距离。该距离的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="316" class="code-formula">
                        <mathml id="316"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>l</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>l</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="317">曼哈顿距离在基于自适应谐振理论 (Adaptive Resonance Theory, ART) 的同步聚类 (SYnchronization Clustering, SYC) 中有较好的应用;但是, 需要注意的是这种距离不再符合在特征空间中转化和旋转的不变性。</p>
                </div>
                <h4 class="anchor-tag" id="318" name="318">4) 闵可夫斯基距离 (Minkowski Distance) 。</h4>
                <div class="p1">
                    <p id="319">闵可夫斯基距离是一种<i>p</i>范数的形式, 公式可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="320" class="code-formula">
                        <mathml id="320"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mroot><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>l</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mi>p</mi></msup></mrow><mi>p</mi></mroot><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="321">从式 (10) 可见:若<i>p</i>为无穷大时, 这种距离可以称为切比雪夫距离;若<i>p</i>=2时就是欧几里得距离;那么当<i>p</i>=1时, 就是曼哈顿距离。</p>
                </div>
                <h4 class="anchor-tag" id="322" name="322">5) 马氏距离 (Mahalanobis Distance) 。</h4>
                <div class="p1">
                    <p id="323">马氏距离是一种关于协方差矩阵的距离度量表示方法, 其公式为:</p>
                </div>
                <div class="p1">
                    <p id="324" class="code-formula">
                        <mathml id="324"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">S</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="325">其中:T表示转置, <b><i>S</i></b>为样本协方差矩阵。马氏距离的优点是距离与属性的量纲无关, 并排除了属性之间的相关性干扰。若各个属性之间独立同分布, 则协方差矩阵为单位矩阵。这样, 平方马氏距离也就转化为了欧氏距离<citation id="845" type="reference"><link href="593" rel="bibliography" /><link href="595" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="326" name="326">6) 对称点距离 (Point Symmetry Distance) 。</h4>
                <div class="p1">
                    <p id="327">当聚类存在对称模式时, 就可以使用对称点距离。其表示公式为:</p>
                </div>
                <div class="p1">
                    <p id="328" class="code-formula">
                        <mathml id="328"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder><mfrac><mrow><mo stretchy="false">∥</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo></mrow><mrow><mo stretchy="false">∥</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><mo>+</mo><mo stretchy="false">∥</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="329">对称点距离是该点到对称点和其他点距离的最小值。</p>
                </div>
                <h4 class="anchor-tag" id="330" name="330">7) 相关系数 (Correlation Coefficient) 。</h4>
                <div class="p1">
                    <p id="331">距离度量也可以源于相关系数<citation id="846" type="reference"><link href="597" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 如皮尔逊相关系数的定义为:</p>
                </div>
                <div class="p1">
                    <p id="332" class="code-formula">
                        <mathml id="332"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo>=</mo><mfrac><mrow><mtext>C</mtext><mtext>o</mtext><mtext>v</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><msqrt><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></msqrt><msqrt><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></msqrt></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="333" name="333">8) 余弦相似度 (Cosine Similarity) 。</h4>
                <div class="p1">
                    <p id="334">最后一种直接计算相似性的方法是余弦相似度。其表示形式为:</p>
                </div>
                <div class="p1">
                    <p id="335" class="code-formula">
                        <mathml id="335"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>cos</mi></mrow><mspace width="0.25em" /><mi>α</mi><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="336">这里, <i>S</i>表示样本之间的相似性 (以下同) 。在特征空间中, 两个样本越相似, 则它们越趋向于平行, 那么它们的余弦值也就越大。</p>
                </div>
                <div class="p1">
                    <p id="337">在这8类聚类相似度测量方法中, 需要注意的是最后三类相似性计算方法不再符合对称性、非负性与反身性的要求, 即属于非可度量的范畴。连续性变量的相似性度量方法在不同聚类算法中的应用, 如表1所示。</p>
                </div>
                <div class="area_img" id="338">
                    <p class="img_tit"><b>表</b>1 <b>连续性变量相似性度量及其应用</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Similarity measurement of continuous variables and its application</p>
                    <p class="img_note"></p>
                    <table id="338" border="1"><tr><td><br />度量方法</td><td>可度量性</td><td>应用领域</td></tr><tr><td><br />欧氏距离</td><td>是</td><td><i>K</i>-Means<sup> [21-23]</sup></td></tr><tr><td><br />闵可夫斯基距离</td><td>是</td><td>模糊C-Means<sup> [24-25]</sup></td></tr><tr><td><br />切比雪夫距离</td><td>是</td><td>模糊C-Means</td></tr><tr><td><br />曼哈顿距离</td><td>是</td><td>模糊ART<sup>[26-27]</sup></td></tr><tr><td><br />马氏距离</td><td>是</td><td>椭球ART<sup>[28]</sup>、超椭球<sup>[29]</sup></td></tr><tr><td><br />对称点距离</td><td>否</td><td>对称<i>K</i>-Means<sup> [30]</sup></td></tr><tr><td><br />皮尔逊相关系数</td><td>否</td><td>微阵列基因测序<sup>[31]</sup></td></tr><tr><td><br />余弦相似度</td><td>否</td><td>文本聚类<sup>[32]</sup></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="339" name="339">2.2 <b>离散变量的相似性度量</b></h4>
                <div class="p1">
                    <p id="340">依据特征变量的离散取值的不同, 其度量方法可以分为二值变量的相似性度量方法和多值变量的相似性度量方法。</p>
                </div>
                <h4 class="anchor-tag" id="341" name="341">2.2.1 二值变量的相似性度量方法</h4>
                <div class="p1">
                    <p id="342">二值型变量 (如性别) 是一种常见的特征取值类型, 在样本集中这种类型的变量较多。一般情况下, 可以将二值型变量使用数字“1”和“0”代替 (如男性为1, 女性为0) 。在这种假设前提下, 样本点<b><i>X</i></b><sub><i>i</i></sub>与<b><i>X</i></b><sub><i>j</i></sub>之间所有二值型变量相似性度量的计算方法, 如表2所示。表中<i>n</i>表示样本的二值型变量分别取对应数字值的特征的总个数 (如:<i>n</i><sub>10</sub>表示两个样本点中所有二值型变量, 第一个是1, 第二个是0。这样的二值型变量有<i>n</i><sub>10</sub>个) 。</p>
                </div>
                <div class="area_img" id="343">
                    <p class="img_tit"><b>表</b>2 <b>样本之间二值型变量相似性度量数值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Similarity measurement values of binary variables between samples</p>
                    <p class="img_note"></p>
                    <table id="343" border="1"><tr><td rowspan="2"><br /><b><i>X</i></b><sub><i>j</i></sub></td><td colspan="2"><br /><b><i>X</i></b><sub><i>i</i></sub></td><td rowspan="2">合计</td></tr><tr><td><br />1</td><td>0</td></tr><tr><td><br />1</td><td><i>n</i><sub>11</sub></td><td><i>n</i><sub>10</sub></td><td><i>n</i><sub>11</sub>+<i>n</i><sub>10</sub></td></tr><tr><td><br />0</td><td><i>n</i><sub>01</sub></td><td><i>n</i><sub>00</sub></td><td><i>n</i><sub>01</sub>+<i>n</i><sub>00</sub></td></tr><tr><td><br />合计</td><td><i>n</i><sub>11</sub>+<i>n</i><sub>01</sub></td><td><i>n</i><sub>10</sub>+<i>n</i><sub>00</sub></td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="344">根据表2可以计算出样本间所有二值型变量取不同数字值的总个数, 则相应的相似性度量公式为:</p>
                </div>
                <div class="p1">
                    <p id="345" class="code-formula">
                        <mathml id="345"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow></msub></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow></msub><mo>+</mo><mi>w</mi><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>0</mn></mrow></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mn>0</mn><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mrow></mfrac></mtd></mtr><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mi>S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="346">其中<i>w</i>为匹配系数, 表示变量取不同数值时的差异程度。一般的取值为1、2或1/2。特别的, 当<i>w</i>为1时, 这样的距离称为海明距离 (Hamming Distance) <citation id="847" type="reference"><link href="623" rel="bibliography" /><link href="625" rel="bibliography" /><sup>[<a class="sup">33</a>,<a class="sup">34</a>]</sup></citation>。另外, 需要指出的是二值型变量的相似性度量公式应该灵活使用。如数值“0”表示“不存在”时, 则<i>n</i><sub>00</sub>就可以从公式中删除掉;又如根据程度的不同, 可以为<i>n</i>添加不同的权值来进行计算。当然, 这些度量公式的灵活运用, 也必须以二值型变量的实际物理意义为依据。</p>
                </div>
                <h4 class="anchor-tag" id="347" name="347">2.2.2 多值变量的相似性度量方法</h4>
                <div class="p1">
                    <p id="348">实际上, 多值变量的相似性度量方法可以转化为多个二值变量来计算<citation id="848" type="reference"><link href="627" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>, 但是, 当变量的取值很大时, 简单的转化方法会显得力不从心, 因此, <i>Everitt</i>等<citation id="849" type="reference"><link href="629" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>提出了更有效的单匹配策略来解决多值变量的相似性度量问题, 相应的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="349" class="code-formula">
                        <mathml id="349"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mi>S</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>l</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mi>S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="350" class="code-formula">
                        <mathml id="350"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>l</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext>第</mtext><mi>l</mi><mtext>维</mtext><mtext>不</mtext><mtext>相</mtext><mtext>同</mtext></mtd></mtr><mtr><mtd><mi>w</mi><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>第</mtext><mi>l</mi><mtext>维</mtext><mtext>相</mtext><mtext>同</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="351">其中:<i>d</i>表示样本中多值变量的个数, 匹配系数<i>w</i>一般为1。需要特别注意两点:一是若样本中多值变量的取值个数相差较大时, 取值较多的变量的<i>w</i>值应该设定为比1更大的值 (由于这样的相同更为不易) ;二是若样本中多值变量的取值在量纲上存在较大差异时, 可以先进行归一化处理。最后, 需要指出的是多值变量的相似性度量结果应该是一个程度上的差别, 而不是数值上的差值, 为此, 在计算多值变量的相似性时, 应该事先根据实际物理意义, 确定程度上的差别等级 (如无差别、轻微、中等、严重等) 。</p>
                </div>
                <h4 class="anchor-tag" id="352" name="352">2.3 <b>混合变量的相似性度量</b></h4>
                <div class="p1">
                    <p id="353">现实世界的真实数据集总是既包含连续性特征又包含离散型特征, 那么, 如何来度量这种具有混合特征样本间的相似性呢?实际上, 混合变量的相似性度量可以转化为如何结合连续性变量与离散性变量的统一度量问题。真实数据集的相似性度量方法有两种:一是分别度量连续性变量与离散性变量, 然后使用一定的权值相加;二是使用式 (17) 计算:</p>
                </div>
                <div class="p1">
                    <p id="354" class="code-formula">
                        <mathml id="354"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mi>S</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>l</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mi>S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="355" class="code-formula">
                        <mathml id="355"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>l</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mtext>若</mtext><mtext>第</mtext><mi>l</mi><mtext>维</mtext><mtext>不</mtext><mtext>相</mtext><mtext>同</mtext></mtd></mtr><mtr><mtd><mi>w</mi><mo>, </mo><mspace width="0.25em" /><mtext>若</mtext><mtext>第</mtext><mi>l</mi><mtext>维</mtext><mtext>相</mtext><mtext>同</mtext></mtd></mtr></mtable></mrow><mo>, </mo><mspace width="0.25em" /><mtext>离</mtext><mtext>散</mtext><mtext>型</mtext><mtext>变</mtext><mtext>量</mtext></mtd></mtr><mtr><mtd><mn>1</mn><mo>-</mo><mrow><mo>|</mo><mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>l</mi></mrow></msub></mrow><mo>|</mo></mrow><mo>/</mo><mi>R</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>连</mtext><mtext>续</mtext><mtext>型</mtext><mtext>变</mtext><mtext>量</mtext></mtd></mtr></mtable></mrow><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="356" class="code-formula">
                        <mathml id="356"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mi>l</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>m</mi></munder><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>l</mi></mrow></msub><mo>-</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>m</mi></munder><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>l</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="357">这里, <i>m</i>是数据集中样本的总个数。另外, Chen等<citation id="850" type="reference"><link href="631" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>还提出了一种度量混合变量相似性的新方法。</p>
                </div>
                <div class="p1">
                    <p id="358">样本间的相似性度量方法是否合理, 将直接影响最终的聚类效果, 显得尤为重要。对于一个特定的样本集来说, 到底哪种相似性计算方法最合适?如何解释相似度的物理意义?这些问题, 至今也没有一个确切的答案, 因此, 聚类分析方法不可避免地具有主观性与对问题域的依赖性等特点。</p>
                </div>
                <h3 id="359" name="359" class="anchor-tag">3 聚类算法分类</h3>
                <div class="p1">
                    <p id="360">聚类算法一般可以用基于划分、基于密度、基于网格和基于约束等方式来进行分类。然而, 在大数据时代背景下, 随着数据量的不断增加及其数据形态的日益多样化, 聚类算法的应用更加广泛;同时对算法本身也提出了更高的要求。依据有效数据量10<sup>12</sup>字节为阈值, 本文将聚类算法分为小数据聚类和大数据聚类两大类<citation id="851" type="reference"><link href="633" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>。小数据聚类主要体现的是聚类的基本思想, 而大数据聚类的思想主要体现在理念、体系结构与架构等几个方面, 至于底层聚类的具体实现算法, 其实与小数据聚类算法并没有本质上的差别。换言之, 大数据聚类的具体实施算法依然采用小数据聚类技术。本文将迄今为止的聚类算法进行了重新划分, 并分别综述了小数据聚类和大数据聚类两种类型的算法, 将传统的基于划分、基于密度、基于网格等算法统一归类为基于划分的聚类算法。根据数据对象及其生成聚类簇形式的不同, 将小数据聚类算法重新分为传统聚类 (<i>Traditional Clustering</i>) 与智能聚类 (<i>Intelligent Clustering</i>) 两大类。其中, 传统聚类分为:划分聚类 (<i>Partitional Clustering</i>) 和层次聚类 (<i>Hierarchical Clustering</i>) 两个大类;智能聚类分为:人工神经网络聚类 (<i>Neural network</i>-<i>based Clustering</i>) 、核聚类 (<i>Kernel</i>-<i>based Clustering</i>) 、序列数据聚类 (<i>Sequential Data Clustering</i>) 、复杂网络聚类 (<i>Complex Network Clustering</i>) 与智能搜索聚类 (<i>Intelligent Search Clustering</i>) 等五个大类。将大数据聚类 (<i>Big Data Clustering</i>) 分为:并行聚类 (<i>Parallel Clustering</i>) 、分布式聚类 (<i>Distributed Clustering</i>) 和高维聚类 (<i>High</i>-<i>dimensional Clustering</i>) 等三个大类。聚类算法的新分类方式如图2所示。</p>
                </div>
                <div class="area_img" id="361">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907002_361.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 聚类算法分类" src="Detail/GetImg?filename=images/JSJY201907002_361.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 聚类算法分类  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907002_361.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Classification of clustering algorithms</i></p>

                </div>
                <h3 id="362" name="362" class="anchor-tag">4 小数据聚类算法</h3>
                <div class="p1">
                    <p id="363">本文将有效数据量在10<sup>12</sup>字节以下的聚类场合, 称之为小数据聚类, 并将小数据聚类分为传统聚类与智能聚类两个大类。</p>
                </div>
                <h4 class="anchor-tag" id="364" name="364">4.1 <b>传统聚类</b></h4>
                <div class="p1">
                    <p id="365">本文将传统聚类算法统一分类为划分聚类与层次聚类两大类。</p>
                </div>
                <h4 class="anchor-tag" id="366" name="366">4.1.1 划分聚类</h4>
                <div class="p1">
                    <p id="367">划分聚类算法针对一个包含<i>n</i>个样本的数据集, 先创建一个初始划分;然后采用一种迭代的重定位技术, 通过样本在类别间移动来改进聚类簇;最后通过一个聚类准则结束移动并判定结果的好坏。通常情况下的判定准则为平方误差准则:</p>
                </div>
                <div class="p1">
                    <p id="368" class="code-formula">
                        <mathml id="368"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mtext>E</mtext></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>p</mi><mo>∈</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi>p</mi><mo>-</mo><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="369">其中:<i>S</i><sub>E</sub>表示平方误差, <i>p</i>指样本点, <i>m</i>指每一个聚类簇的平均值, <i>C</i>是聚类簇, <i>k</i>为聚类簇的数目。</p>
                </div>
                <div class="p1">
                    <p id="370">划分聚类若从排列组合的角度分析, <i>N</i>个样本分为<i>K</i>类的不同聚类簇<citation id="852" type="reference"><link href="635" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>为:</p>
                </div>
                <div class="p1">
                    <p id="371" class="code-formula">
                        <mathml id="371"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Ν</mi><mo>, </mo><mi>Κ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Κ</mi><mo>!</mo></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mo stretchy="false"> (</mo></mstyle><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mi>Κ</mi><mo>-</mo><mi>m</mi></mrow></msup><mi>C</mi><msubsup><mrow></mrow><mi>Κ</mi><mi>m</mi></msubsup><mi>m</mi><msup><mrow></mrow><mi>Ν</mi></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="372">这种不同聚类簇的数量非常惊人, 例如将30个样本点分为3类的不同划分方式接近2×10<sup>14</sup>种。可见, 基于划分的算法不能在整个空间中寻找最优解, 必须使用其他方式, 其代表算法是<i>K</i>-Means<citation id="865" type="reference"><link href="637" rel="bibliography" /><link href="639" rel="bibliography" /><link href="641" rel="bibliography" /><sup>[<a class="sup">40</a>,<a class="sup">41</a>,<a class="sup">42</a>]</sup></citation>、混合密度聚类、图聚类、模糊聚类等。1967年提出的<i>K</i>-Means算法必须事先为每个类别确定一个聚类中心。该算法是应用较广、较高效的一种聚类方法。然而, 算法也存在明显的局限性:聚类结果的好坏依赖于初始聚类中心的选择;对异常样本点较为敏感;只能处理数值型的数据集合等。为克服这些局限性, 从20世纪60年代到现在, 许多研究者对<i>K</i>-Means算法进行了大量的改进:Bradley等<citation id="853" type="reference"><link href="643" rel="bibliography" /><sup>[<a class="sup">43</a>]</sup></citation>为克服初始中心的影响提出了一种改进策略;Pelleg等<citation id="854" type="reference"><link href="645" rel="bibliography" /><sup>[<a class="sup">44</a>]</sup></citation>为了加速迭代过程提出了算法的变体X-Means算法;Berkhin等<citation id="855" type="reference"><link href="647" rel="bibliography" /><sup>[<a class="sup">45</a>]</sup></citation>将<i>K</i>-Means算法拓展到了分布式聚类领域;Nguyen等<citation id="856" type="reference"><link href="649" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>提出的<i>K</i>-MODES算法, 克服了<i>K</i>-Means算法只能处理数值型数据的缺陷;<i>K</i>-MEDOIDS算法并不是计算求得聚类中心, 而是将某个样本直接代表该聚类, 这样能有效处理异常数据<citation id="866" type="reference"><link href="651" rel="bibliography" /><link href="653" rel="bibliography" /><sup>[<a class="sup">47</a>,<a class="sup">48</a>]</sup></citation>。混合密度聚类算法从概率分布的角度, 假设样本集有若干个内在的概率分布, 然后利用不同的概率分布来划分聚类簇。这样, 聚类过程变成了寻找几个概率分布参数的过程, 这也意味着需要人为地构造概率密度函数, 因此, 这类聚类算法将无监督学习转化成了有监督的分类方法<citation id="857" type="reference"><link href="655" rel="bibliography" /><sup>[<a class="sup">49</a>]</sup></citation>。这些概率分布一般为常用的分布, 如高斯分布、t分布等。最大化似然估计 (Maximum Likelihood Estimation) 是参数估计中最重要的算法。在这类算法中, 最常用的是期望最大化 (Expectation Maximization) 算法<citation id="867" type="reference"><link href="657" rel="bibliography" /><link href="659" rel="bibliography" /><sup>[<a class="sup">50</a>,<a class="sup">51</a>]</sup></citation>。Fraley等<citation id="858" type="reference"><link href="655" rel="bibliography" /><sup>[<a class="sup">49</a>]</sup></citation>还开发了一个期望最大化算法的软件包。另外, CAST (Cluster Analysis Statistical Test) 算法也被认为是一种基于概率模型的聚类算法<citation id="859" type="reference"><link href="661" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>。图聚类的典型算法为基于最小加权分割的CLICK (CLustering Identification via Connectivity Kernels) 算法<citation id="868" type="reference"><link href="663" rel="bibliography" /><link href="665" rel="bibliography" /><sup>[<a class="sup">53</a>,<a class="sup">54</a>]</sup></citation>。该算法正好将样本点与图的顶点、样本点之间的关系 (边及边上的权值) 建立了图论关联。研究者Ding等<citation id="860" type="reference"><link href="667" rel="bibliography" /><sup>[<a class="sup">55</a>]</sup></citation>提出的基于random walks的方法也是一种图聚类算法。谱聚类 (Spectral Clustering) 算法将聚类转化为解二次优化问题, 能够识别任意形状的聚类簇并可以收敛于全局最优解<citation id="869" type="reference"><link href="669" rel="bibliography" /><link href="671" rel="bibliography" /><sup>[<a class="sup">56</a>,<a class="sup">57</a>]</sup></citation>, 在图像分析领域有着广泛的应用。Lee等<citation id="861" type="reference"><link href="673" rel="bibliography" /><sup>[<a class="sup">58</a>]</sup></citation>和Shi等<citation id="862" type="reference"><link href="675" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>又提出了一种具有快捷性和自适应性的GRC (Graph-based Relaxed Clustering) 算法。1981年模糊聚类方法由Bezdek首次实现, 该算法就是大名鼎鼎的FCM (Fuzzy C-Means) 算法<citation id="863" type="reference"><link href="677" rel="bibliography" /><sup>[<a class="sup">60</a>]</sup></citation>, 是图像分割领域应用的较广泛的聚类算法。该算法使用隶属度来确定样本点的相似性, 是一种基于目标函数的模糊聚类方法。随后, Bezdek的研究团队又对模糊目标函数进行了全局性的优化处理<citation id="864" type="reference"><link href="679" rel="bibliography" /><sup>[<a class="sup">61</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="373">基于密度的划分聚类方法是一种将数据集看作低密度区域隔开的若干个高密度簇的集合, 该方法的主要特点是可以识别任何形状的簇。DBSCAN (Density Based Spatial Clustering of Applications with Noise) 算法就是一种基于密度的常用算法, 它提出了密度可接近性 (Density-reachability) 和密度可连接性 (Density-connectivity) 两个概念<citation id="870" type="reference"><link href="681" rel="bibliography" /><sup>[<a class="sup">62</a>]</sup></citation>。一个簇是基于密度可接近的最大密度可连接的对象集合, 除此之外, 其他的样本点被认为是异常点。Birant等<citation id="871" type="reference"><link href="683" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>在2007年又提出了一种新的ST-DBSCAN (Spatial-Temporal DBSCAN) 算法。该算法能在非空间值、空间值和时态值中发现聚类簇<citation id="872" type="reference"><link href="685" rel="bibliography" /><sup>[<a class="sup">64</a>]</sup></citation>。OPTICS (Ordering Points To Identify the Clustering Structure) 算法<citation id="873" type="reference"><link href="687" rel="bibliography" /><sup>[<a class="sup">65</a>]</sup></citation>是在一种在参数次序结构上基于密度的聚类算法。数据集中的每个样本点都有两个值:核心距离 (Core-distance) 和可接近距离 (Reachability-distance) 。DENCLUE (DENsity-based CLUstEring) 算法<citation id="874" type="reference"><link href="689" rel="bibliography" /><sup>[<a class="sup">66</a>]</sup></citation>在<i>K</i>-Means和DBSCAN的基础上派生出来的一组基于密度分布的聚类算法。STING (STatistical INformation Grid) 算法<citation id="875" type="reference"><link href="691" rel="bibliography" /><sup>[<a class="sup">67</a>]</sup></citation>是一种基于网格的多分辨率方法, 将空间划分为层次型的矩形单元。WaveCluster算法<citation id="876" type="reference"><link href="693" rel="bibliography" /><link href="695" rel="bibliography" /><sup>[<a class="sup">68</a>,<a class="sup">69</a>]</sup></citation>是一种使用信号处理过程的一种聚类算法。</p>
                </div>
                <div class="p1">
                    <p id="374">除了传统的划分聚类算法之外, 还出现了一些新的划分聚类算法, 如同步聚类、近邻传播聚类、密度峰值快速聚类与大规模数据集聚类等。</p>
                </div>
                <div class="p1">
                    <p id="375">同步聚类 (SYnchronization CLustering, SYC) 是Shao等<citation id="877" type="reference"><link href="697" rel="bibliography" /><sup>[<a class="sup">70</a>]</sup></citation>通过对动力学中同步现象的研究提出的基于自适应谐振理论 (Adaptive Resonance Theory, ART) 的聚类算法。同步聚类算法的主要思路是首先将样本集中的每个样本的每一维分量看作一个相位振子。在最初的振动阶段, 各个相位振子 (即每一个属性) 按照自己的固有本征频率运动;随着时间的推移, 那些比较接近的相位振子会相互影响而产生锁相现象 (即以相同的相位作同步运动) , 最后这种锁相的同步现象影响到了每一个样本点, 所有振子 (样本点) 最终形成了若干个作局部同步运动的聚类簇, 在同一个聚类簇的样本点作同步运动的振子相位相同。然而, 同步聚类算法的计算量较大, 对大规模数据集进行聚类时有相当大的局限性。同步聚类算法基于Kuramoto模型:</p>
                </div>
                <div class="p1">
                    <p id="376"><mathml id="377"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>d</mtext><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mtext>d</mtext><mi>t</mi></mrow></mfrac><mo>=</mo><mi>ω</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mi>Κ</mi><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mi>sin</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>; <i>i</i>=1, 2, …, <i>N</i>      (20) </p>
                </div>
                <div class="p1">
                    <p id="378">其中:<i>ω</i><sub><i>i</i></sub>为自然频率 (固有频率) , <i>θ</i><sub><i>i</i></sub>是第<i>i</i>个振荡器的相位, <i>K</i>是耦合强度。针对Kuramoto模型中的耦合强度引入一个分布, 认为所有的振荡器相互作用时, 耦合是全局的, 但每个振荡器的耦合强度<i>K</i><sub><i>i</i></sub>是不同的, 改变了Kuramoto模型各个振荡器具有相同的耦合强度的属性, 认为耦合强度和固有频率服从一个联合分布, 并且二者不相关。该算法巧妙地使用物理模型完成聚类, 是结合其他学科理论的重要研究成果, 然而其应用并不广泛。</p>
                </div>
                <div class="p1">
                    <p id="379">2007年, 近邻传播 (Affinity Propagation, AP) 聚类 (或称仿射传播聚类) 是Frey等<citation id="878" type="reference"><link href="699" rel="bibliography" /><sup>[<a class="sup">71</a>]</sup></citation>提出的不需要事先设定聚类个数且聚类速度较快的聚类算法。算法通过传递吸引度和归属度两个指标, 完成聚类中心的确定<citation id="880" type="reference"><link href="701" rel="bibliography" /><link href="703" rel="bibliography" /><sup>[<a class="sup">72</a>,<a class="sup">73</a>]</sup></citation>并且相对比较适合大规模数据集<citation id="879" type="reference"><link href="705" rel="bibliography" /><sup>[<a class="sup">74</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="380">2014年, 密度峰值快速聚类 (Fast Density Peaks Clustering, FDPC) 是意大利研究者Rodriguez等<citation id="881" type="reference"><link href="707" rel="bibliography" /><sup>[<a class="sup">75</a>]</sup></citation>提出的一种新的高效聚类算法。实际上, 该算法是一种欧氏距离与密度相结合的聚类方法。算法的主要思想是聚类中心具有比邻域高的密度、聚类中心与密度高的点具有相对大的距离。算法的优点是聚类簇数目直观产生、异常点自动发现和剔除、任意形状和任意映射空间维数都可完成聚类。由于该算法需要事先计算任意两个样本之间的距离, 故算法的相似度计算开销很大<citation id="882" type="reference"><link href="709" rel="bibliography" /><sup>[<a class="sup">76</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="381">大规模数据集聚类是指数据规模在10<sup>12</sup>字节以下, 能够在单机上执行的聚类算法。CLARANS (Clustering Large Applications based on RANdomized Sampling) 聚类<citation id="883" type="reference"><link href="711" rel="bibliography" /><sup>[<a class="sup">77</a>]</sup></citation>是对CLARA (Clustering LARge Applications) 算法的一次重大的改进。这两种算法本质上都是为了缩小检索空间的一种采样技术。不同的是CLARA算法是针对整个搜索过程, 而CLARANS算法则是针对特定的子图。数据量较大时, 上述算法无法将所有样本一次性读入内存。增量聚类, 也称在线聚类 (Incremental or Online Clustering) , 是一种不需要将所有样本点都一次性读入内存的方法<citation id="884" type="reference"><link href="713" rel="bibliography" /><sup>[<a class="sup">78</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="382">划分聚类算法的应用极其广泛, 如无监督的文本聚类、模糊聚类等。</p>
                </div>
                <h4 class="anchor-tag" id="383" name="383">4.1.2 层次聚类</h4>
                <div class="p1">
                    <p id="384">层次聚类具有一个分层的树形结构。按照构建树形结构的方式不同, 可以将聚类分为自顶向下和自底向上两种构建方式, 分别称为聚合型层次聚类 (<i>Agglomerative Hierarchical Clustering</i>) 与分裂型层次聚类 (<i>Divisive Hierarchical Clustering</i>) 。两种聚类算法都是在聚类过程中构建具有一定亲属关系的系统树图, 聚类的大体过程如图3所示。</p>
                </div>
                <div class="area_img" id="385">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907002_385.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 层次聚类构建示意图" src="Detail/GetImg?filename=images/JSJY201907002_385.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 层次聚类构建示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907002_385.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Schematic diagram of hierarchical clustering construction</i></p>

                </div>
                <div class="p1">
                    <p id="386">聚合型层次聚类, 也称自底向上的方法, 首先将每一个样本都称为一个聚类簇, 然后计算簇间的相似度, 分层合并, 直到最后只有一个簇为止或满足一定的终止条件;而分裂型层次聚类的迭代过程则正好相反。分裂型层次聚类, 也称自顶向下的方法, 首先将所有的样本都看作是一个聚类簇, 然后在每一步中, 上层聚类簇被分裂为下层更小的聚类簇, 直到每个簇只包含一个样本, 或者满足终止条件为止。由于分裂型算法具有较高的时间复杂度, 与聚合型算法相比, 分裂型算法并不常见。</p>
                </div>
                <div class="p1">
                    <p id="387">最早的层次聚类算法有<i>AGNES</i> (<i>AGglomerative NESting</i>) 算法、<i>DIANA</i> (<i>DIvisive ANAlysis</i>) 算法, 分别是聚合型与分裂型聚类的早期代表<citation id="885" type="reference"><link href="715" rel="bibliography" /><sup>[<a class="sup">79</a>]</sup></citation>。<i>BIRCH</i> (<i>Balanced Iterative Reducing and Clustering using Hierarchies</i>) 算法是一种改进的聚合型层次聚类算法<citation id="886" type="reference"><link href="717" rel="bibliography" /><sup>[<a class="sup">80</a>]</sup></citation>。该算法的基本思想是每一个数据点的重要性肯定存在差异性, 这样就能够差别化地对待不同的数据样本。<i>BIRCH</i>算法具有称为<i>CF</i>-<i>TREE</i> (<i>Clustering Feature Tree</i>) 的一种独特的数据结构。算法分为两个步骤:一是扫描所有的样本点, 构建内存树;二是利用算法建立树的叶子节点。</p>
                </div>
                <div class="p1">
                    <p id="388">一般的聚类算法, 都是以单一数据点为聚类的中心, 导致最终的聚类簇形状是球形的。为了聚类各种形状的簇, 可以采用<i>CURE</i> (<i>Clustering Using REpresentatives</i>) 算法。<i>Guha</i>等<citation id="887" type="reference"><link href="719" rel="bibliography" /><sup>[<a class="sup">81</a>]</sup></citation>在1998年提出了<i>CURE</i>算法, 该算法的优点是使用具有代表性的一些点来代替聚类簇中的一个中心样本点, 这样可以识别各种复杂形状和不同大小的簇, 并且具有异常点检测功能。1999年<i>Guha</i>等<citation id="888" type="reference"><link href="721" rel="bibliography" /><sup>[<a class="sup">82</a>]</sup></citation>提出的<i>ROCK</i> (<i>RObust Clustering using linKs</i>) 算法是对<i>CURE</i>算法的改进, 使之具有识别类别属性的功能。2007年, <i>Gelbard</i>等<citation id="889" type="reference"><link href="723" rel="bibliography" /><sup>[<a class="sup">83</a>]</sup></citation>提出了正二进制 (<i>Binary</i>-<i>positive</i>) 算法, 该算法将原始数据转化为二进制位数据, 这样样本点之间的相似性度量只在正比特位上进行, 直至找到聚类簇为止。</p>
                </div>
                <div class="p1">
                    <p id="389">层次聚类算法的主要应用领域包括基因表达谱分析、文本聚类、并行工程组结构等。</p>
                </div>
                <h4 class="anchor-tag" id="390" name="390">4.2 <b>智能聚类</b></h4>
                <div class="p1">
                    <p id="391">本文将智能聚类算法分为5大类:人工神经网络聚类、核聚类、序列数据聚类、复制网络聚类与智能搜索聚类。</p>
                </div>
                <h4 class="anchor-tag" id="392" name="392">4.2.1 人工神经网络聚类</h4>
                <div class="p1">
                    <p id="393">人工神经网络是一种竞争学习方法, 由大量神经元 (或称处理单元) 经相互连接而成的网络, 因模拟人类脑神经系统的结构而得名。深度学习也是一种层次较多的人工神经网络。深度学习的飞速发展使人工神经网络研究又上了一个更高的台阶。该聚类方法需要具备三个基本的条件:一是为了使每个输入样本在网络中具有尽量不同的输出, 除了一些随机分布的参数外, 每一个单元都必须是相同的;二是每个单元具有有限的强度;三是单元间又具有某些竞争性的机制。竞争函数可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="394" class="code-formula">
                        <mathml id="394"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi mathvariant="bold-italic">ω</mi><msub><mrow></mrow><mi>j</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></munderover><mi>ω</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mi>Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>θ</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="395">其中:<i>ω</i><sub><i>j</i></sub>是一个权值向量, <b><i>X</i></b>为输入样本, <i>S</i><sub><i>i</i></sub>各个神经元的输入, 神经元的输出<i>Y</i><sub><i>i</i></sub>是输入与此神经元的阈值在传递函数上的映射。人工神经网络通过建立网络模型后, 从输入数据中学习知识, 来调节神经元的权值向量与阈值, 直到网络的输出误差 (往往为输出结果与期望结果的范数) 达到预期后训练结束。人工神经网络的参数调节方式一般可以分为两种:网格搜索和随机搜索。当然, 也可以采用智能算法来搜索参数。</p>
                </div>
                <div class="p1">
                    <p id="396">竞争学习方法可分为两种:硬竞争学习和软竞争学习。硬竞争学习机制是用最优的权值向量去匹配输入模式<citation id="890" type="reference"><link href="725" rel="bibliography" /><sup>[<a class="sup">84</a>]</sup></citation>, 而软竞争学习是用相似度度量去匹配输入模式。自组织映射 (Self-Organizing Map, SOM) <citation id="891" type="reference"><link href="727" rel="bibliography" /><sup>[<a class="sup">85</a>]</sup></citation>就是一种利用人工神经网络进行聚类的算法。该方法将所有的样本点逐一进行处理, 并将聚类中心映射到二维空间, 从而实现可视化。Yin<citation id="892" type="reference"><link href="729" rel="bibliography" /><sup>[<a class="sup">86</a>]</sup></citation>提出了一种改进的自组织映射算法VISOM (VIsualization Self-Organizing Map) 大幅度提高了传统SOM算法的可视化特性。Cao等<citation id="893" type="reference"><link href="731" rel="bibliography" /><sup>[<a class="sup">87</a>]</sup></citation>提出的基于投影自适应谐振理论的人工神经网络聚类进一步提高了算法的性能。深度学习是人工神经网络的又一次飞跃式的发展。目前, 基于深度学习的聚类算法也成为了研究的热点问题<citation id="894" type="reference"><link href="733" rel="bibliography" /><link href="735" rel="bibliography" /><link href="737" rel="bibliography" /><sup>[<a class="sup">88</a>,<a class="sup">89</a>,<a class="sup">90</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="397" name="397">4.2.2 核聚类</h4>
                <div class="p1">
                    <p id="398">伴随着支持向量机<citation id="895" type="reference"><link href="739" rel="bibliography" /><sup>[<a class="sup">91</a>]</sup></citation>的强势推出, 从20世纪90年代以来, 基于核函数的方法在机器学习和模式识别领域变得越来越重要<citation id="896" type="reference"><link href="741" rel="bibliography" /><sup>[<a class="sup">92</a>]</sup></citation>。核聚类方法是将样本点从输入空间通过核函数映射到高维空间。这种非线性映射, 将不能线性可分的数据集在高维特征空间中变得线性可分, 从而在高维空间中利用线性方法完成聚类, 这样极大地提高了非线性聚类的性能和可伸缩性;但是这种核聚类方法计算复杂度很高, 需要使用<i>Mecer</i>理论进行核变换。因为直接寻找非线性映射<i>Φ</i>并不容易, 所以找到如下的函数:</p>
                </div>
                <div class="p1">
                    <p id="399"><i>K</i> (<b><i>X</i></b><sub><i>i</i></sub>, <b><i>X</i></b><sub><i>j</i></sub>) =〈<i>Φ</i> (<b><i>X</i></b><sub><i>i</i></sub>) ·<i>Φ</i> (<b><i>X</i></b><sub><i>j</i></sub>) 〉      (22) </p>
                </div>
                <div class="p1">
                    <p id="400">该式定义的函数<i>K</i>就是“核函数 (Kernel function) ”。核函数的基本作用是通过两个低维空间的向量, 计算出经过变换后在高维空间中的向量内积值。支持向量机模型的优化参数有:惩罚因子、核函数的宽度和不敏感参数。一般常常优化前两种参数。支持向量聚类算法是一种典型的无监督学习方法, 首先将输入映射到高维空间, 然后巧妙地结合高维空间的点在输入空间的位置特性, 进行聚类划分。常用的核函数有线性核函数、多项式核函数、高斯核函数与Sigmoid核函数。其中, 高斯核函数的用途较为广泛。</p>
                </div>
                <div class="p1">
                    <p id="401">1995年, Tax等<citation id="897" type="reference"><link href="743" rel="bibliography" /><sup>[<a class="sup">93</a>]</sup></citation>提出了支持向量领域描述 (Support Vector Domain Description, SVDD) 的概念, 并初步实现了算法。1999年, 他们进一步改进了支持向量领域描述算法。2004年, 他们又在理论上作了系统的阐述, 并将算法的名称改为支持向量数据描述 (Support Vector Data Description) 算法。该算法是一种全局优化的无监督一分类方法, 是一种典型的从数据集中分离出少量异常样本 (或称孤立点) 的检测方法。算法将数据集映射到高维空间后, 利用一个超球鲁棒地分离了异常样本, 并可以通过软间隔动态地控制异常点的数量和映射回输入空间后边界的平滑度。在此基础上, Ben-Hur等<citation id="901" type="reference"><link href="745" rel="bibliography" /><link href="747" rel="bibliography" /><sup>[<a class="sup">94</a>,<a class="sup">95</a>]</sup></citation>提出了支持向量聚类 (Support Vector Clustering, SVC) 算法。该算法实际上是在分离出异常点后, 映射回输入 (原始) 空间, 并且利用任意两个样本点之间连接线上的点在高维空间中的位置特性, 进行聚类划分。支持向量聚类算法在手写数字识别等方面的应用中取得了很好的效果。Wang等<citation id="898" type="reference"><link href="749" rel="bibliography" /><sup>[<a class="sup">96</a>]</sup></citation>将支持向量数据描述称为球形单分类器 (Spherical One-Class Classifier, SOCC) , 并在详细分析了球形单分类器缺点的基础上, 提出了一种与聚合型层次聚类相似的结构化单分类器 (Structured One-Class Classifier, TOCC) 。结构化单分类器首先在特征空间中构建一系列的超椭球, 然后用偶内点方法解决这些二次锥规划问题并且检测出孤立点。为了避免求二次锥规划问题, 以节省运算时间, 2010年Rajasegarar等<citation id="899" type="reference"><link href="751" rel="bibliography" /><sup>[<a class="sup">97</a>]</sup></citation>在结构化单分类器算法的基础上, 提出了一种称为中心超椭球支持向量机 (Centered-hyperEllipsoidal SVM, CESVM) 的单分类算法。该算法只计算一个线性的规划问题, 极大地降低了时间复杂度, 并应用于传感器网络入侵检测领域取得了较好的效果。Amami等<citation id="900" type="reference"><link href="753" rel="bibliography" /><sup>[<a class="sup">98</a>]</sup></citation>在增量支持向量机的基础上, 又提出了增量支持向量聚类 (Incremental Support Vector Clustering) 算法, 为聚类大规模数据集奠定了基础。</p>
                </div>
                <div class="p1">
                    <p id="402">核技巧利用核映射巧妙地解决了非线性问题, 应用非常广泛;但是, 为了解决二次规划问题或者计算核矩阵, 需要大量的计算时间, 为此, 在处理面向大数据集的核聚类时, 也可使用并行计算与云计算等方面的技术。</p>
                </div>
                <h4 class="anchor-tag" id="403" name="403">4.2.3 序列数据聚类</h4>
                <div class="p1">
                    <p id="404">序列数据是指在一定的测度范围内, 对某些属性多次测量所得到的数据。序列数据聚类与传统聚类的比较如图4所示。</p>
                </div>
                <div class="area_img" id="405">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907002_405.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 传统聚类与序列聚类的比较" src="Detail/GetImg?filename=images/JSJY201907002_405.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 传统聚类与序列聚类的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907002_405.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Comparison between traditional clustering and sequential clustering</i></p>

                </div>
                <div class="p1">
                    <p id="406">最常用的序列数据是时间序列数据, 相应的聚类算法称为时间序列聚类算法<citation id="902" type="reference"><link href="755" rel="bibliography" /><sup>[<a class="sup">99</a>]</sup></citation>。当时间序列数据有先验知识时, 可以采用聚类算法直接实现序列划分;当数据没有先验知识时, 可以采用聚类算法按数据之间的距离度量划分聚类簇。根据时间序列聚类的执行过程可以将算法大致分成以下三类:基于原始数据的时间序列聚类算法、基于特征的时间序列聚类算法和基于模型的时间序列聚类算法<citation id="903" type="reference"><link href="757" rel="bibliography" /><sup>[<a class="sup">100</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="407">轨迹聚类 (<i>Trajectory Clustering</i>) 也是一种典型的序列数据聚类算法。轨迹聚类算法研究可以追溯到1994年<citation id="904" type="reference"><link href="759" rel="bibliography" /><sup>[<a class="sup">101</a>]</sup></citation>。然而直到1999年, <i>Gaffney</i>等<citation id="905" type="reference"><link href="761" rel="bibliography" /><sup>[<a class="sup">102</a>]</sup></citation>提出了轨迹数据的定义及其轨迹聚类的混合回归模型。轨迹聚类将原来孤立点状分布的样本点, 扩展到在时间轴上的队列数据。国内的研究者习惯上把轨迹聚类形象地称为时空轨迹聚类。它不仅表示了研究对象的历史模式, 同时也隐含了对象的未来模式, 具有积极的预测意义。在大数据的背景下, 该聚类算法将会是目前及其以后长期的研究热点<citation id="906" type="reference"><link href="763" rel="bibliography" /><sup>[<a class="sup">103</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="408">序列数据聚类往往通过子序列的距离或者相似度来生成聚类簇, 但它和传统的聚类方法有本质的区别, 如在对比基因和蛋白质序列时, 聚类过程是通过一系列的替换、插入、删除 (或者称编辑) 等操作来完成。随着信息技术的发展, 近几十年来在基因测序、速度过程检测、文本挖掘、医疗诊断、股票市场分析、顾客交易、网页数据挖掘和机器感知分析等领域出现了大量的序列数据, 因此, 对这些数据的聚类是非监督学习领域的新挑战。目前, 对于序列数据聚类研究主要集中在三个方面:一是对序列数据的距离和相似性度量研究;二是通过特征提取将序列数据转换为熟悉的无序列样本数据;三是直接围绕序列数据的特性建立数学模型。</p>
                </div>
                <h4 class="anchor-tag" id="409" name="409">4.2.4 复杂网络聚类</h4>
                <div class="p1">
                    <p id="410">聚类分析是机器学习领域发展最快的研究方向之一。复杂网络聚类 (<i>Complex Network Clustering</i>) 与智能方法聚类的发展日新月异, 已经成为聚类算法分类方式的新成员。复杂网络聚类是伴随着互联网的飞速发展及其巨大影响力而提出的一类领域性较强的专用聚类方法<citation id="910" type="reference"><link href="765" rel="bibliography" /><link href="767" rel="bibliography" /><sup>[<a class="sup">104</a>,<a class="sup">105</a>]</sup></citation>。复杂网络自身具有无标度、小世界和分形的特点<citation id="911" type="reference"><link href="769" rel="bibliography" /><link href="771" rel="bibliography" /><sup>[<a class="sup">106</a>,<a class="sup">107</a>]</sup></citation>。复杂网络聚类可以分为基于优化的方法 (<i>optimization based method</i>) 和启发式方法 (<i>heuristic method</i>) 两种<citation id="907" type="reference"><link href="695" rel="bibliography" /><sup>[<a class="sup">69</a>]</sup></citation>, 例如, 利用启发式规则的<i>Girvan</i>-<i>Newman</i>算法<citation id="908" type="reference"><link href="773" rel="bibliography" /><sup>[<a class="sup">108</a>]</sup></citation>、郭玉泉等<citation id="909" type="reference"><link href="775" rel="bibliography" /><sup>[<a class="sup">109</a>]</sup></citation>提出的面向社区结构的分形聚类检测算法。</p>
                </div>
                <div class="p1">
                    <p id="411">复杂网络聚类的主要应用领域是流行病传播网、互联网上的社交网络、现实世界的社会系统、生物系统联系网等。</p>
                </div>
                <h4 class="anchor-tag" id="412" name="412">4.2.5 智能搜索聚类</h4>
                <div class="p1">
                    <p id="413">智能搜索聚类 (<i>Intelligent search clustering</i>) , 是指运用智能方法搜索解空间的启发式聚类算法。基于划分的聚类方法本身是一个<i>NP</i>难的问题, 而且搜索空间大小随着样本点的增加以指数级的方式增长, 以智能搜索解空间为研究路径, 研究者提出了复杂的智能方法聚类算法。如为了弥补<i>K</i>-<i>Means</i>聚类算法容易陷入局部最优解的问题, 可以结合进化算法, 模拟出多个种群, 并可以根据环境的不同, 动态改变其交叉变异概率, 增强种群的多样性。这样既解决了容易陷入局部最优的问题, 又保留了进化算法全局最优的收敛性。</p>
                </div>
                <div class="p1">
                    <p id="414">一般情况是, 研究者提出一种新型智能搜索算法, 就会被很快应用于某种聚类算法的应用领域, 并对应产生一种新的智能方法聚类算法。代表性的智能搜索算法有:进化算法<citation id="912" type="reference"><link href="777" rel="bibliography" /><sup>[<a class="sup">110</a>]</sup></citation> (包括遗传算法及结合量子理论的进化算法) 、退火算法<citation id="913" type="reference"><link href="779" rel="bibliography" /><sup>[<a class="sup">111</a>]</sup></citation>、<i>Tabu</i>搜索<citation id="914" type="reference"><link href="781" rel="bibliography" /><sup>[<a class="sup">112</a>]</sup></citation>、粒子群算法<citation id="915" type="reference"><link href="783" rel="bibliography" /><sup>[<a class="sup">113</a>]</sup></citation>及其他生物觅食算法 (如蜂群算法、鱼群算法、蛙跳算法、萤火虫算法) 等。2014年<i>Mirjalili</i>等<citation id="916" type="reference"><link href="785" rel="bibliography" /><sup>[<a class="sup">114</a>]</sup></citation>又提出了一种模仿狼群狩猎的新智能搜索算法, 称为灰狼算法。该算法在多参数的组合优化问题上表现出良好的性能<citation id="917" type="reference"><link href="787" rel="bibliography" /><sup>[<a class="sup">115</a>]</sup></citation>。</p>
                </div>
                <h3 id="415" name="415" class="anchor-tag">5 大数据聚类算法</h3>
                <div class="p1">
                    <p id="416"><i>IBM</i>和国际数据公司 (<i>International Data Corporation</i>, <i>IDC</i>) 分别提出了大数据的4<i>V</i>特点, 综合来看大数据具有5<i>V</i>特性, 即数据量庞大 (<i>Volume</i>) 、多样性或称异构数据 (<i>Variety</i>) 、实时性 (<i>Velocity</i>) 、真实性 (<i>Veracity</i>) 与大价值 (<i>Value</i>) 。本文将有效数据量在10<sup>12</sup>字节以上的聚类技术, 称之为大数据聚类。大数据聚类的核心思想是处理计算复杂度和计算成本, 与可扩展性和速度之间的关系问题, 因此, 大数据聚类算法关注的焦点是:以最小化地降低聚类质量为代价, 提高算法的可扩展性与执行速度。本文将大数据聚类分为分布式聚类 (<i>Distributed Clustering</i>) 、并行聚类 (<i>Parallel Clustering</i>) 和高维聚类 (<i>High</i>-<i>dimensional Clustering</i>) 等三个类别。其中并行聚类与分布式聚类算法, 需要在计算机集群中执行, 因此, 这两种算法合称为多机聚类 (<i>Multi</i>-<i>machine Clustering</i>) 。多机聚类的硬件架构如图5所示。</p>
                </div>
                <div class="area_img" id="417">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907002_417.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 多机聚类的硬件架构" src="Detail/GetImg?filename=images/JSJY201907002_417.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 多机聚类的硬件架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907002_417.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Hardware architecture of multi</i>-<i>machine clustering</i></p>

                </div>
                <div class="p1">
                    <p id="418">从图5可知, 多机聚类将数据划分到多台机器分开执行聚类的过程。若划分与处理过程是人为干预下执行的聚类, 称为并行聚类;若划分与处理过程是由分布式框架自动执行的聚类, 称为分布式聚类。因此, 分布式聚类的执行框架对用户来说, 既隐藏了负载均衡、出错控制与计算资源的分配等网络问题, 又自动执行数据划分、信息交换等数据处理问题, 体现了“计算向数据靠拢”的执行理念;而并行聚类则在处理网络与处理数据两个方面都需要人工干预, 需要消耗大量的时间与精力, 执行聚类的难度很大, 体现的是“数据向计算靠拢”的执行理念。多机聚类算法的核心问题在于, 在多台机器之间尽可能少地交换信息的情况下, 获得较好的聚类效果。在多机聚类过程中, 影响聚类效果的因素主要体现在聚类标准和数据划分两个方面:其一, 不同的机器可能使用不同的基本聚类算法;其二, 即使所有机器强制使用同一种基本聚类算法, 由于数据划分的不用, 聚类效果可能也会大相径庭。多机聚类算法的执行过程如图6所示。首先, 划分数据到不同的机器, 然后执行分组聚类;第二, 综合并分析分组聚类的结果;第三, 依据分析的结果, 自动改进聚类过程;第四, 重新进行分组聚类;依次循环执行, 直到符合判定准则或者满足终止条件。可见, 多机聚类算法是波浪式、循环、不断前进地构造聚类簇的过程。</p>
                </div>
                <div class="area_img" id="419">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907002_419.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 多机聚类算法的执行过程" src="Detail/GetImg?filename=images/JSJY201907002_419.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 多机聚类算法的执行过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907002_419.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Run process of multi</i>-<i>machine clustering</i></p>

                </div>
                <h4 class="anchor-tag" id="420" name="420">5.1 <b>分布式聚类</b></h4>
                <div class="p1">
                    <p id="421">负载均衡与数据交换等问题, 由分布式框架自动执行的聚类方式, 称为分布式聚类。对于分布式聚类算法的评价, 一般可以从三个方面入手:一是执行速度, 即数据量不变的情况下, 随着机器数量的增加, 执行时间的变化率;二是可伸缩性, 即执行时间不变的情况下, 机器数量的增加能够处理数据量的容忍度;三是数据吞吐量, 即机器数据不变的情况下, 随着执行时间的增加, 能够处理的数据规模。目前, 从分布式聚类的研究文献来看, 主要是使用<i>MapReduce</i>框架执行聚类。<i>MapReduce</i>框架的工作流程如图7所示。对于编程人员来说, 只需要编写<i>Map</i>函数与<i>Reduce</i>函数, 该框架会自动执行比较复杂的信息交换 (<i>Shuffle</i>) 过程。</p>
                </div>
                <div class="area_img" id="422">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907002_422.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 MapReduce的工作流程" src="Detail/GetImg?filename=images/JSJY201907002_422.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 <i>MapReduce</i>的工作流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907002_422.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Work flow of MapReduce</i></p>

                </div>
                <div class="p1">
                    <p id="423"><i>PK</i>-<i>Means</i> (<i>Parallel K</i>-<i>Means based on MapReduce</i>) 算法是较简单也是经典的<i>K</i>-<i>Means</i>算法在<i>MapReduce</i>大数据平台上的应用。<i>PK</i>-<i>Means</i>聚类算法在执行时间与可伸缩性两个方面都有线性的提高<citation id="918" type="reference"><link href="789" rel="bibliography" /><sup>[<a class="sup">116</a>]</sup></citation>。<i>MR</i>-<i>DBSCAN</i> (<i>DBSCAN based on MapReduce</i>) 算法在可伸缩性与数据吞吐量两个方面的性能也有了长足的进步<citation id="919" type="reference"><link href="791" rel="bibliography" /><sup>[<a class="sup">117</a>]</sup></citation>。另外, 为了提高大数据集的执行效率, 基于<i>GPU</i>的<i>MapReduce</i>聚类<citation id="920" type="reference"><link href="793" rel="bibliography" /><sup>[<a class="sup">118</a>]</sup></citation>与基于<i>SPARK</i>框架的聚类方法<citation id="921" type="reference"><link href="795" rel="bibliography" /><sup>[<a class="sup">119</a>]</sup></citation>, 在较新文献中也已经崭露头角。</p>
                </div>
                <h4 class="anchor-tag" id="424" name="424">5.2 <b>并行聚类</b></h4>
                <div class="p1">
                    <p id="425">相对于分布式聚类来说, 并行聚类算法的实现较为困难, 但是, 并行聚类最大的优势是执行过程尽在编程人员的掌握之中。<i>DBDC</i> (<i>Density Based Distributed Clustering</i>) 算法是<i>DBSCAN</i>算法在大数据聚类中的典型应用, 但是在聚类效果相同的前提下, 执行效率比<i>DBSCAN</i>提高了30倍<citation id="922" type="reference"><link href="561" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。图划分方法是一种常用的聚类技术。类似的, <i>ParMETIS</i> (<i>Parallel METIS</i>) 算法是<i>METIS</i>算法的大数据并行执行版本<citation id="923" type="reference"><link href="797" rel="bibliography" /><sup>[<a class="sup">120</a>]</sup></citation>, 是一种面向大数据的层次型并行聚类技术。该并行聚类算法也能在不损失聚类效果的情况下, 极大地提高执行效率。近几年, 聚类研究领域出现了基于<i>GPU</i> (<i>Graphic Processing Unit</i>) 的并行聚类算法。例如, <i>G</i>-<i>DBSCAN</i> (<i>Graphic DBSCAN</i>) 算法与<i>G</i>-<i>OPTICS</i> (<i>Graphic OPTICS</i>) 算法。<i>G</i>-<i>DBSCAN</i>聚类算法, 首先以距离为标准, 构建样本集图, 然后在图上执行聚类, 结果比基于<i>CPU</i>的方法聚类速度提高了112倍<citation id="924" type="reference"><link href="799" rel="bibliography" /><sup>[<a class="sup">121</a>]</sup></citation>。基于<i>GPU</i>的<i>G</i>-<i>OPTICS</i>算法显示了更好的聚类效果, 比基于<i>CPU</i>的方法聚类速度提高了200倍<citation id="925" type="reference"><link href="801" rel="bibliography" /><sup>[<a class="sup">122</a>]</sup></citation>。可见, 基于<i>GPU</i>的聚类技术将是未来较为热门的一种大数据聚类方法。</p>
                </div>
                <h4 class="anchor-tag" id="426" name="426">5.3 <b>高维聚类</b></h4>
                <div class="p1">
                    <p id="427">样本点维度极高的聚类算法, 称为高维聚类算法。当聚类高维数据时, 传统的方法是先降低维度 (或称为维度约简 (<i>Dimension Reduction</i>) , 简称降维) 。降维算法种类很多, 不在这里作过多介绍。需要注意的是该方法会不可避免地带来数据信息损失, 可能也会降低聚类的有效性;并且, 降维算法不适合处理甚高维的情况。直接对甚高维数据集进行传统聚类, 几乎很难找到聚类簇, 因此可以将数据集划分为若干个子空间, 这样, 原高维空间就是子空间的并集, 为此, 处理高维数据集的方法可以命名为子空间聚类 (<i>Subspace Clustering</i>) 算法<citation id="933" type="reference"><link href="803" rel="bibliography" /><link href="805" rel="bibliography" /><sup>[<a class="sup">123</a>,<a class="sup">124</a>]</sup></citation>。该种聚类算法可以分为硬子空间聚类 (<i>Hard Subspace Clustering</i>) 和软子空间聚类 (<i>Soft Subspace Clustering</i>) 。硬子空间聚类算法又分为自底向上和自顶向下两种, 如<i>CLIQUE</i> (<i>CLustering In QUEst</i>) <citation id="926" type="reference"><link href="807" rel="bibliography" /><sup>[<a class="sup">125</a>]</sup></citation>、<i>ENCLUS</i> (<i>ENtropy</i>-<i>based CLUStering</i>) <citation id="927" type="reference"><link href="809" rel="bibliography" /><sup>[<a class="sup">126</a>]</sup></citation>、<i>MAFIA</i> (<i>Merging of Adaptive Finite IntervAls</i>) <citation id="928" type="reference"><link href="811" rel="bibliography" /><sup>[<a class="sup">127</a>]</sup></citation>等算法为自底向上的类型;<i>ORCLUS</i> (<i>arbitrarily ORiented projected CLUster generation</i>) <citation id="929" type="reference"><link href="813" rel="bibliography" /><sup>[<a class="sup">128</a>]</sup></citation>、<i>FINDIT</i> (<i>a Fast and INtelligent subspace clustering algorithm using DImention voTing</i>) <citation id="930" type="reference"><link href="815" rel="bibliography" /><sup>[<a class="sup">129</a>]</sup></citation>等算法为自顶向下的类型。针对硬子空间聚类算法的研究相对比较成熟, <i>Sim</i>等<citation id="931" type="reference"><link href="817" rel="bibliography" /><sup>[<a class="sup">130</a>]</sup></citation>已经作了相关的详细阐述。软子空间算法主要是研究特征加权聚类<citation id="934" type="reference"><link href="819" rel="bibliography" /><link href="821" rel="bibliography" /><sup>[<a class="sup">131</a>,<a class="sup">132</a>]</sup></citation>, <i>Deng</i>等<citation id="932" type="reference"><link href="823" rel="bibliography" /><sup>[<a class="sup">133</a>]</sup></citation>已经作了深入的阐述。</p>
                </div>
                <div class="p1">
                    <p id="428">对于高维聚类, 还有一种聚类算法值得注意。那就是双聚类 (<i>Biclustering</i>) 算法。随着对基因数据的深入分析, 出现了众多的基因表达数据的情况, 产生了双聚类算法。这些数据大多以矩阵形式表示和存储, 并且维度极高 (有时可能达到几千万维) 。基因芯片数据中隐藏了大量有用的局部模式, 为寻找这些信息, 2000年<i>Cheng</i>等<citation id="935" type="reference"><link href="825" rel="bibliography" /><sup>[<a class="sup">134</a>]</sup></citation>提出了双聚类的概念:双聚类的目的是在基因表达数据矩阵中寻找满足条件的子矩阵, 使子矩阵中基因集在对应的条件集上一致表达。目前主要的几类双聚类模型为矩阵等值模型、矩阵加法模型、矩阵乘法模型和信息共演变模型。值得注意的是类似的子空间聚类方法的发展速度很快, 也是目前极高维聚类的一个研究热点。</p>
                </div>
                <h3 id="429" name="429" class="anchor-tag">6 算法对比分析</h3>
                <div class="p1">
                    <p id="430">划分聚类的应用较为广泛, 收敛速度也很快, 且能够比较容易地扩展, 以用于大规模的数据;缺点在于它倾向于聚集样本数量大小比较接近、以密度作为主要的区别手段来进行聚类分析, 并且初始聚类中心的选择和孤立点 (或噪声) 数据会对聚类分析产生较为明显的影响。由于划分聚类一般从总体上评判样本间的相似性, 故不适合聚类高维数据集。</p>
                </div>
                <div class="p1">
                    <p id="431">层次聚类方法较好地克服了划分聚类的一些缺陷, 适用于识别各种形状的聚类簇;由于聚类的层次性, 也可以控制不同层次水平的聚类簇, 以适应于不同粒度的应用场合, 同时以牺牲算法的时间复杂度作为代价。</p>
                </div>
                <div class="p1">
                    <p id="432">人工神经网络聚类是一种基于机器学习的聚类算法, 主要优点:一是聚类的灵活性, 可以通过调节参数, 理论上可以无限度地减少误差;二是聚类运算的并行性, 可以根据数据集维度的不同, 调节输入节点的个数, 并行计算;三是算法易于实现。同时算法主要缺点是容易出现过学习问题和聚类结果不稳定两个方面。</p>
                </div>
                <div class="p1">
                    <p id="433">核聚类算法主要用于非线性可分的数据集。这种数据集无法使用线性可分的聚类算法解决。该算法也巧妙地解决了<i>Bellman</i>教授提出的处理高维数据集时的“维度诅咒”问题。另外, 核聚类算法能有效地处理噪声与离群点的问题, 并不需要先验知识的指导。然而, 核函数的选择、特征空间与原始空间的关系、大数据样本集的核矩阵存储与计算等问题也亟待解决。</p>
                </div>
                <div class="p1">
                    <p id="434">序列数据聚类是针对特定的数据特征而提出的领域聚类分析算法, 是在时间与空间的可测度范围内, 对某些属性多次测量所得到的数据, 进行聚类的过程。</p>
                </div>
                <div class="p1">
                    <p id="435">大数据聚类主要着眼于超大规模数据集 (10<sup>12</sup>字节以上) 与高维数据集两个角度。超大规模数据集聚类算法与其他算法的主要区别是在理念、体系结构与架构方面。至于聚类的具体实施过程, 一般更倾向于采用较为简单的基本算法 (如<i>K</i>-<i>Means</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="436">表3选取了聚类算法的时间与空间计算复杂度、能否处理纵向的大规模数据集和横向的高维数据集等最受关注的三个方面, 总结比较了一些常用的典型算法的性能。</p>
                </div>
                <div class="area_img" id="437">
                    <p class="img_tit"><b>表</b>3 <b>典型聚类算法的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Performance comparison of typical clustering algorithms</i></p>
                    <p class="img_note"></p>
                    <table id="437" border="1"><tr><td>算法</td><td>计算复杂度</td><td>大规模</td><td>高维</td></tr><tr><td>K<font class="font5">-</font><font class="font6">Means</font></td><td>
O (NKd) (时间) , O (N+K) (空间) </td><td>是</td><td>否</td></tr><tr><td>CURE</td><td>O (N<font class="font7"><sup>2</sup></font><font class="font6">log</font><font class="font5">N) (时间) , O (N) (空间) </font></td><td>是</td><td>是</td></tr><tr><td>DBSCAN</td><td>O<font class="font5"> (</font><font class="font6">N</font><font class="font5">log</font><font class="font6">N</font><font class="font5">) (时间) </font></td><td>是</td><td>否</td></tr><tr><td>WaveCluster</td><td>O<font class="font5"> (</font><font class="font6">N</font><font class="font5">) (时间) </font></td><td>是</td><td>否</td></tr><tr><td>CLARANS</td><td>O<font class="font5"> (</font><font class="font6">N</font><font class="font7"><sup>2</sup></font><font class="font5">) (时间与空间) </font></td><td>是</td><td>否</td></tr><tr><td>FCM</td><td>接近 <font class="font6">O</font><font class="font5"> (</font><font class="font6">N</font><font class="font5">) (时间) </font></td><td>否</td><td>否</td></tr><tr><td>BIRCH</td><td>O<font class="font5"> (</font><font class="font6">N</font><font class="font7"><sup>2</sup></font><font class="font5">log</font><font class="font6">N</font><font class="font5">) (时间) </font></td><td>否</td><td>否</td></tr><tr><td>DENCLUE</td><td>O<font class="font5"> (</font><font class="font6">N</font><font class="font5">log</font><font class="font6">N</font><font class="font5">) (时间) </font></td><td>是</td><td>是</td></tr><tr><td>SVC</td><td>O<font class="font5"> ( (</font><font class="font6">N</font><font class="font5">-</font><font class="font6">n</font><font class="font8"><sub>bsv</sub></font><font class="font5">) </font><font class="font6">n</font><font class="font8"><sub>sv</sub></font><font class="font7"><sup>2</sup></font><font class="font5">) (时间) </font></td><td>是</td><td>是</td></tr><tr><td>SYC</td><td>O<font class="font5"> (</font><font class="font6">TN</font><font class="font7"><sup>2</sup></font><font class="font5">) (时间) </font></td><td>否</td><td>否</td></tr><tr><td>FDPC</td><td>O<font class="font5"> (</font><font class="font6">N</font><font class="font5">) 
 (时间) </font></td><td>是</td><td>是</td></tr><tr><td>G-OPTICS</td><td>O<font class="font5"> (</font><font class="font6">E</font><font class="font5">log</font><font class="font6">N</font><font class="font5">) (时间) </font></td><td>是</td><td>否</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:N表示数据的规模;K表示聚类簇的数目;n<sub>sv</sub>表示支持向量的个数;n<sub>bsv</sub>表示边界支持向量的个数;E表示样本图中边的数量。</p>
                    <p class="img_note"></p>
                </div>
                <h3 id="438" name="438" class="anchor-tag">7 聚类结果评价</h3>
                <div class="p1">
                    <p id="439">聚类算法是一种无监督的主观的分类方法。聚类结果的客观分析、聚类簇数目的合理性、聚类结构的物理意义等内容, 都属于聚类评价的范畴<citation id="936" type="reference"><link href="827" rel="bibliography" /><sup>[<a class="sup">135</a>]</sup></citation>。聚类结果的评价在很多文献中已经从有效性函数方面作了大量的研究, 本文主要是从聚类分析的三大类准则的角度作简要分析, 这些准则分别是外部准则 (<i>external criteria</i>) 、内部准则 (<i>internal criteria</i>) 与相对准则 (<i>relative criteria</i>) <citation id="937" type="reference"><link href="829" rel="bibliography" /><sup>[<a class="sup">136</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="440">外部准则是一种利用聚类结构先验知识的聚类结果评价方法。例如, 在聚类评估时可以借助专家的指导 (或者采用人工的标准方法) 来开展聚类结果分析。最后, 通过假设检验的方法比较算法结果与人工的经验结果的一致性, 从而确定聚类算法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="441">内部准则也是一种假设检验方法, 只是并不是利用先验知识, 而是利用样本集的内部特性来评估聚类结果的有效性。例如, 内部准则通过样本集的相似矩阵、<i>CPCC</i> (<i>CoPhenetic Correlation Coefficient</i>) 指标等来评估聚类结果。相对准则通过不同算法 (或者相同算法选择不同输入参数) 的聚类结果对比来综合评估结果的一种准则, 与外部准则和内部准则有很大的不同之处。实际上, 相对准则依据聚类分析四大功能之一的“实际数据集上的其他技术归类假说的测试方式”而产生。该准则并不使用假设检验方法, 故其计算量相对较小。</p>
                </div>
                <div class="p1">
                    <p id="442">外部准则较客观, 相对准则较不客观, 而内部准则居于两者之间。可见, 在聚类结果的评价过程中, 能使用外部准则最好, 其次是内部准则, 最后才选择相对准则。</p>
                </div>
                <div class="p1">
                    <p id="443">这里需要特别说明的是:由于确定聚类簇的数量比较困难 (有关聚类数目的确定可参见<i>Jos</i>é-<i>Garc</i>í<i>a</i>等<citation id="938" type="reference"><link href="831" rel="bibliography" /><sup>[<a class="sup">137</a>]</sup></citation>的深入分析) , 而且聚类数量评估是结果评估的中心问题<citation id="939" type="reference"><link href="833" rel="bibliography" /><sup>[<a class="sup">138</a>]</sup></citation>, 相对准则主要用于聚类簇数目的评估;相对准则也有很多有效性指标、停止规则、可视化方法、启发式方法等<citation id="940" type="reference"><link href="827" rel="bibliography" /><sup>[<a class="sup">135</a>]</sup></citation>来评估聚类簇的数目。</p>
                </div>
                <div class="p1">
                    <p id="444">聚类结果的评价是聚类分析中的一个重要步骤, 也是最困难、最无所适从的一步。聚类分析全过程中的最后一步是聚类结果的物理解析。该步骤是在结果评价后的长期实践中逐步探索形成知识的过程, 这里不再详细叙述。</p>
                </div>
                <h3 id="445" name="445" class="anchor-tag">8 结语</h3>
                <div class="p1">
                    <p id="446">课题组在聚类分析医疗与海洋大数据的需求背景下, 本文是聚类项目的研究成果。现将聚类算法的展望如下:</p>
                </div>
                <div class="p1">
                    <p id="447">1) 聚类分析不仅仅是选择或者设计聚类算法的过程, 数据预处理与特征提取其实非常重要。针对实际的数据源, 数据预处理时有大量细致入微的工作需要去完成, 特征提取的数据集质量的优劣也会直接影响最后的聚类结果。课题组建议预处理与特征提取尽量有领域专家的指导, 而且这一过程可能需要占用大量的时间与精力。</p>
                </div>
                <div class="p1">
                    <p id="448">2) 在实际应用中, 数据集会存在复杂性和多样性的特点, 选择任何一种聚类算法可能都不一定适用, 因此, 需要在了解基本聚类算法的优缺点基础上, 研究多种算法的融合问题。</p>
                </div>
                <div class="p1">
                    <p id="449">3) 在聚类实际数据集时, 能够产生任何形状聚类簇的算法将会是聚类算法研究的发展方向, 因此, 先使用核变换将数据集映射到高维空间, 再利用传统聚类算法的核聚类算法是一个重要的研究方向。</p>
                </div>
                <div class="p1">
                    <p id="450">4) 在大数据时代背景下, 大数据聚类算法研究会有较好的发展前景, 例如基于<i>GPU</i>的聚类、基于<i>SPARK</i>的聚类、基于图计算框架 (如<i>Pregel</i>) 的聚类等新的聚类理念与技术。</p>
                </div>
                <div class="p1">
                    <p id="451">5) 随着云计算、物联网与大数据技术的应用日趋成熟, 在这种情况下, 待聚类的数据复杂性前所未有, 因此, 在复杂数据背景下, 如何探讨聚类的有效性也是一个重要的、有难度的研究热点方向。</p>
                </div>
                <div class="p1">
                    <p id="452">6) 另外, 在查阅外文文献的过程中, 课题组研究人员发现, 对于同一个概念, 在不同的文献中, 可能会出现不同的称谓。如果将数据集看作一个由行和列构成的二维表, 不同的外文文献会将行称为<i>Samples</i>、<i>Data Objects</i>、<i>Patterns</i>、<i>Entities</i>、<i>Cases</i>、<i>Instances</i>、<i>Observances</i>、<i>Units</i>等;将列称为<i>Dimensions</i>、<i>Variables</i>、<i>Features</i>、<i>Properties</i>等;将聚类簇称为<i>Clusters</i>、<i>Subsets</i>、<i>Groups</i>、<i>Categories</i>等。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="559">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cluster Analysis">

                                <b>[1]</b> ALDENDERFER M S, BLASHFIELD R K.Cluster Analysis [M].Los Angeles:Sage Publications, 1984:2-12.
                            </a>
                        </p>
                        <p id="561">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data Clustering:Algorithms and Applications">

                                <b>[2]</b> AGGARWAL C C, REDDY C K.Data Clustering:Algorithms and Applications [M].London:Taylor and Francis Group, 2014:4-7.
                            </a>
                        </p>
                        <p id="563">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cluster Analysis">

                                <b>[3]</b> EVERITT B, LANDAU S, LEESE M.Cluster Analysis [M].4th ed.London:Arnold, 2001:144-201.
                            </a>
                        </p>
                        <p id="565">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Constructive feedforward ART clustering networks. II">

                                <b>[4]</b> BARALDI A, ALPAYDIN E.Constructive feedforward ART clustering networks—Part I and II [J].IEEE Transactions on Neural Networks, 2002, 13 (3) :645-677.
                            </a>
                        </p>
                        <p id="567">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000017469&amp;v=MTExMDVIb3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXdVYnhFPU5pZklZN0s3SHRqTnI0OUZaT29JQw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> JAIN A K, MURTY M N, FLYNN P J.Data clustering:a review[J].ACM Computing Surveys, 1999, 31 (3) :264-323.
                            </a>
                        </p>
                        <p id="569">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001140461&amp;v=MTY5NDJxZForWnVGaXJsVkwzSUpWMD1OajdCYXJPNEh0SE5yb3RGWU8wT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> HANSEN P, JAUMARD B.Cluster analysis and mathematical programming[J].Mathematical Programming, 1997, 79:191-215.
                            </a>
                        </p>
                        <p id="571">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 章永来.基于聚类的社区居民健康指数预测模型研究[D].北京:中国科学院大学, 2015. (ZHANG Y L.Research on prediction model of health index for community residents based on clustering[D].Beijing:University of Chinese Academy of Sciences, 2015.) 
                            </a>
                        </p>
                        <p id="573">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300158296&amp;v=MTE4NzNycVFUTW53WmVadEZpbmxVcjNJSjF3VWJ4RT1OaWZPZmJLOEh0TE9ySTlGWmU0SERuVS9vQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> ZHOU X, ZHANG Y, SHI M, et al.Early detection of liver disease using data visualisation and classification method[J].Biomedical Signal Processing and Control, 2014, 11:27-35.
                            </a>
                        </p>
                        <p id="575">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCW&amp;filename=SJCWD825BE71F4AB87CB9BBD401663677943&amp;v=MzA0MzdMVTA1dHBodzdxL3hhcz1OaWZJZWNld0hOUysyb2hFRXU5K2ZuUSt2R1FhbUUwSlRIL2pxaFEyZjdXVFRMNmNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> ZHANG Y, ZHOU X, SHI H, et al.Corrosion pitting damage detection of rolling bearings using data mining techniques[J].International Journal of Modeling, Identification and Control, 2015, 24 (3) :235-243.
                            </a>
                        </p>
                        <p id="577">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Time series prediction methods for depth-averaged current velocities of underwater gliders">

                                <b>[10]</b> ZHOU Y, YU J, WANG X.Time series prediction methods for depth-averaged current velocities of underwater gliders [J].IEEE Access, 2017, 5:5773-5784.
                            </a>
                        </p>
                        <p id="579">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201308034&amp;v=MjI4MjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMTHo3U1pMRzRIOUxNcDQ5R1lJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 章永来, 史海波, 尚文利, 等.面向乳腺癌辅助诊断的改进支持向量机方法[J].计算机应用研究, 2013, 30 (8) :2373-2376. (ZHANG Y L, SHI H B, SHANG W L, et al.Improved method for computer-aided diagnosis of breast cancer based on support vector machines [J].Application Research of Computers, 2013, 30 (8) :2373-2376.) 
                            </a>
                        </p>
                        <p id="581">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Impossibility Theorem for Clustering">

                                <b>[12]</b> KLEINBERG J.An impossibility theorem for clustering [C]// Proceedings of the 15th International Conference on Neural Information Processing Systems.Cambridge:MIT Press, 2002:463-470.
                            </a>
                        </p>
                        <p id="583">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pattern Classification">

                                <b>[13]</b> DUDA R O, HART P E, STORK D G.Pattern Classification [M].2nd ed.New York:John Wiley and Sons, 2001:47-56.
                            </a>
                        </p>
                        <p id="585">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE0BC821EBA1C25412EB510831D62A216&amp;v=MDU0NTN4Y1JuMDE0U1gvcXJ4TkJmN0RsUjd1WkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZjYTRiS0xFclk0d0Zwb09mMzQ4eQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> GAO J, WANG Y, LI J.Bounds on covering radius of linear codes with Chinese Euclidean distance over the finite non chain ring F-2+vF (2) [J].Information Processing Letters, 2018, 138:22-26.
                            </a>
                        </p>
                        <p id="587">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probability and Statistical Inference">

                                <b>[15]</b> HOGG R, TANIS E.Probability and Statistical Inference [M].7th ed.Upper Saddle River :Prentice Hall, 2005:120-145.
                            </a>
                        </p>
                        <p id="589">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=c-means clustering with the ll and l&amp;amp;infin; norms">

                                <b>[16]</b> BOBROWSKIL, BEZDEK J C.C-means clustering with the L<sub>1</sub> and L∞ norms [J].IEEE Transactions on Systems, Man, and Cybernetics, 1991, 21 (3) :545-554.
                            </a>
                        </p>
                        <p id="591">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES777F239568D78AEFA9BDBF9114051838&amp;v=MTM0OTNGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZiUy9HYWZPcklaQVl1TjdDM1JJdW1CaTQwMEpPZ25yclJNeGViZVZUYm1YQ09OdkZTaVdXcjdKSQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> ANTER A, HASSENIAN A E, OLIVA D.An improved fast fuzzy c-means using crow search optimization algorithm for crop identification in agricultural[J].Expert Systems with Applications, 2019, 118:340-354.
                            </a>
                        </p>
                        <p id="593">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A self-organizing network for hyperellipsoidal clustering">

                                <b>[18]</b> MAO J, JAIN A K.A self-organizing network for HyperEllipsoidal Clustering (HEC) [J].IEEE Transactions on Neural Networks, 1996, 7 (1) :16-29.
                            </a>
                        </p>
                        <p id="595">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Health assessment methods for wind turbines based on power prediction and Mahalanobis distance">

                                <b>[19]</b> ZHAN J, WANG R, YI L.Health assessment methods for wind turbines based on power prediction and Mahalanobis distance [J].International Journal of Pattern Recognition and Artificial Intelligence, 2019, 33 (2) :1951001.
                            </a>
                        </p>
                        <p id="597">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Finding Groups in Data:An Introduction to Cluster Analysis">

                                <b>[20]</b> KAUFMAN L, ROUSSEEUW P J.Finding Groups in Data:An Introduction to Cluster Analysis[M].New York:John Wiley and Sons, 2009:82-85.
                            </a>
                        </p>
                        <p id="599">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering">

                                <b>[21]</b> XU R, DONALD C W.Clustering[M].New York:John Wiley and Sons, 2009:12-95.
                            </a>
                        </p>
                        <p id="601">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cluster analysis of multivariate data: efficiency vs. interpretability of classifications">

                                <b>[22]</b> FORGY E W.Cluster analysis of multivariate data:efficiency vs.interpretability of classification[J].International Journal of Environmental Studies, 1965, 21 (3) :41-52.
                            </a>
                        </p>
                        <p id="603">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD34656CB852467C00E1E9C4D890AF25C0&amp;v=MDk0ODgzUDFOWWVrTENudEt6eFptNjBwME8zdVdwQnMxQ01TV1FNbWZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5qbkJhckM4R05USw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> ANTOINE G B, CATHY M R, ANDREA R.Clustering transformed compositional data using K-means, with applications in gene expression and bicycle sharing system data [J].Journal of Applied Statistics, 2019, 46 (1) :47-65.
                            </a>
                        </p>
                        <p id="605">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized fuzzy c-means clustering strategies using Lp norm distances">

                                <b>[24]</b> HATHAWAY R J, BEZDEK J C, HU Y.Generalized fuzzy c-means clustering strategies using L<sub>P</sub> norm distances [J].IEEE Transactions on Fuzzy Systems, 2000, 8 (5) :576-582.
                            </a>
                        </p>
                        <p id="607">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201608054&amp;v=MTkwNjRSN3FmWnVac0Z5L25VN3JMTHo3QmI3RzRIOWZNcDQ5QVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 耿宗科, 王长宾, 张振国.基于模糊c-means与自适应粒子群优化的模糊聚类算法[J].计算机科学, 2016, 43 (8) :267-272. (GENG Z K, WANG C B, ZHANG Z G.Fuzzy c-means and adaptive PSO based fuzzy clustering algorithm[J].Computer Science, 2016, 43 (8) :267-272.) 
                            </a>
                        </p>
                        <p id="609">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system">

                                <b>[26]</b> CARPENTER G A, GROSSBERG S, ROSEN D B.Fuzzy ART:fast stable learning and categorization of analog patterns by an adaptive resonance system[J].Neural Networks, 1991, 4 (6) :759-771.
                            </a>
                        </p>
                        <p id="611">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wireless network confidence level improvement via fusion adaptive resonance theory">

                                <b>[27]</b> CHANDRAPRABHA K, GEETHA B G.Wireless network confidence level improvement via fusion adaptive resonance theory[J].Cluster Computing, 2018 (2) :1-11.
                            </a>
                        </p>
                        <p id="613">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ellipsoid ART and ARTMAP for incremental unsupervised and supervised learning">

                                <b>[28]</b> ANAGNOSTOPOULOS G C, GEORGIOPOULOS M.Ellipsoid ART and ARTMAP for incremental unsupervised and supervised learning[C]// Proceedings of the 2001 International Society of Optical Engineering.Bellingham:SPIE Publications, 2001:1-6.
                            </a>
                        </p>
                        <p id="615">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738142&amp;v=MDQxNzBnN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmlmT2ZiSzdIdEROcVk5RlkrZ0hEWA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b> MOSHTAGHI M, RAJASEGARAR S, LECKIE C, et al.An efficient hyperellipsoidal clustering algorithm for resource-constrained environments[J].Pattern Recognition, 2011, 44 (9) :2197-2209.
                            </a>
                        </p>
                        <p id="617">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Modified Version of the K-Means Algorithm with a Distance Based on Cluster Symmetry">

                                <b>[30]</b> SU M C, CHOU C H.A modified version of the K-Means algorithm with a distance based on cluster symmetry [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :674-680.
                            </a>
                        </p>
                        <p id="619">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cluster analysis and display of genome-wide expression patterns">

                                <b>[31]</b> EISEN M B, SPELLMAN P T, BROWN P O, et al.Cluster analysis and display of genome-wide expression patterns [J].Proceedings of the National Academy of Sciences of the United States of America, 1998, 95 (25) :14863-14868.
                            </a>
                        </p>
                        <p id="621">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Comparison of Document Clustering Techniques">

                                <b>[32]</b> STEINBACH M, KARYPIS G, KUMAR V.A Comparison of Document Clustering Techniques [M].New York:John Wiley and Sons, 2000:50-56.
                            </a>
                        </p>
                        <p id="623">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vector Quantization and Signal Compression">

                                <b>[33]</b> GERSHO A, GRAY R M.Vector quantization and signal compression[J].Springer International, 1992, 159 (1) :407-485.
                            </a>
                        </p>
                        <p id="625">
                            <a id="bibliography_34" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD7CFDA2A82A546CB26F43265B8640959&amp;v=MDQyMDBJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mY2UvYmFlNDNvMDBiT2wrQ1hnL3ZHUVI3RWw1UzMza3FXQTlmN2FVVEwrV0NPTnZGU2lXV3I3Sg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[34]</b> ABDEL-GHAFFAR K A S.Sets of binary sequences with small total Hamming distances[J].Information Processing Letters, 2019, 142:27-29.
                            </a>
                        </p>
                        <p id="627">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled two-way clustering analysis of gene microarray data">

                                <b>[35]</b> GETZ G, LEVINE E, DOMANY E.Coupled two-way clustering analysis of gene microarray data[J].Proceedings of the National Academy of Sciences, 2000, 97 (22) :12079-12084.
                            </a>
                        </p>
                        <p id="629">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cluster Analysis">

                                <b>[36]</b> EVERITT B, HOTHORN T.Cluster Analysis[M].New York:John Wiley and Sons, 2011:111-134.
                            </a>
                        </p>
                        <p id="631">
                            <a id="bibliography_37" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7907A53B16E9FCCD1DE8A3860EF73FFB&amp;v=MDAwNjNtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mYlN4SHRhOXFvdzNaZTE2QlFwS3ZHSVNua3AxT1h6cXFoSkFEN1dYTTh6dENPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[37]</b> CHEN J Y, HE H H.A fast density-based data stream clustering algorithm with cluster centers self-determined for mixed data[J].Information Sciences, 2016, 345:271-293.
                            </a>
                        </p>
                        <p id="633">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Big Data Clustering:A Review">

                                <b>[38]</b> SHIRKHORSHIDI A S, AGHABOZORGI S, WAH T Y, et al.Big data clustering:a review[C]// Proceedings of the 14th International Conference on Computational Science and Its Applications.Berlin:Springer, 2014:707-720.
                            </a>
                        </p>
                        <p id="635">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Approaches to partition medical data using clustering algorithms">

                                <b>[39]</b> KALYANI P.Approaches to partition medical data using clustering algorithms[J].International Journal of Computer Applications, 2013, 49 (23) :7-10.
                            </a>
                        </p>
                        <p id="637">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Introduction to Combinatorial Mathematics">

                                <b>[40]</b> LIU G.Introduction to Combinatorial Mathematics[M].New York:McGraw Hill, 1968:12-16.
                            </a>
                        </p>
                        <p id="639">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Genetic K-means algorithm">

                                <b>[41]</b> KRISHNA K, MURTY M N.Genetic K-means algorithm[J].IEEE Transactions on Systems, Man, and Cybernetics:Part B, 1999, 29 (3) :433-439.
                            </a>
                        </p>
                        <p id="641">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FGKA:A fast genetic K-means clustering algorithm">

                                <b>[42]</b> LU Y, LU S, FOTOUHI F.FGKA:a fast genetic K-means clustering algorithm[C]// Proceedings of the 2004 ACM Symposium on Applied Computing.New York:ACM, 2004:622-623.
                            </a>
                        </p>
                        <p id="643">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Refining Initial Points for K-Means Clustering">

                                <b>[43]</b> BRADLEY P, FAYYAD U.Refining initial points for K-means clustering[C]// Proceedings of the 15th International Conference on Machine Learning.New York:ACM, 1998:91-99.
                            </a>
                        </p>
                        <p id="645">
                            <a id="bibliography_44" target="_blank" href="http://scholar.cnki.net/result.aspx?q=X-Means:extending k-Means with efficient estimation of the number of clusters">

                                <b>[44]</b> PELLEG D, MOORE A.X-means:extending K-means with efficient estimation of the number of the clusters[C]// Proceedings of the 17th International Conference on Machine Learning.New York:ACM, 2000:111-117.
                            </a>
                        </p>
                        <p id="647">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning simple relations:theory and applications">

                                <b>[45]</b> BERKHIN P, BECHER J.Learning simple relations:theory and applications[C]// Proceedings of the 2nd International Conference on Data Mining, Washington, DC:IEEE Computer Society, 2002:333-349.
                            </a>
                        </p>
                        <p id="649">
                            <a id="bibliography_46" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA41EE26395A2B0E167852DAB46E56028&amp;v=MjkwMTBkNFNndVQzaFl6RExlU1JiaVhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mY0s4SDZTNXJZbEdiZTUrRGc0NXVoY1Y3VA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[46]</b> NGUYEN H H.Privacy-preserving mechanisms for K-modes clustering[J].Computers and Security, 2018, 78:60-75.
                            </a>
                        </p>
                        <p id="651">
                            <a id="bibliography_47" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES28D8954DBDBEBEB50AB598FA7B6CA052&amp;v=MjY4MTdiK2RDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mYkd3YXRuRnFvc3hGcDk5ZVE1TXZSTVRtMDE0UVhlVTNSVkhmOEhsUg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[47]</b> LACKO D, HUYSMANST, VLEUGELS J, et al.Product sizing with 3D anthropometry and k-medoids clustering[J].Computer-Aided Design, 2017, 91:60-74.
                            </a>
                        </p>
                        <p id="653">
                            <a id="bibliography_48" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD132B2EA93EDBF0565B6A7BB295B67633&amp;v=MTU2NjR5aEFXbURrTVR3MlFyaHN3QzdTVFE3bWNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZmNhcks3SEtQTzJ2NU1aNTU3ZmdvNQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[48]</b> NAKAGAWA K, IMAMURA M, YOSHIDA K.Stock price prediction using k-medoids clustering with indexing dynamic time warping[J].Electronics and Communications in Japan, 2019, 102 (2) :3-8.
                            </a>
                        </p>
                        <p id="655">
                            <a id="bibliography_49" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800007182&amp;v=MzI2NzJRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VWJ4RT1Oam5CYXJLN0h0Zk9wNDlGWk9zSURYUTdvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[49]</b> FRALEY C, RAFTERY A.Model-based clustering, discriminant analysis, and density estimation [J].Journal of the American Statistical Association, 2002, 97:611-631.
                            </a>
                        </p>
                        <p id="657">
                            <a id="bibliography_50" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The EM Algorithm and Extensions">

                                <b>[50]</b> McLACHLAN G, KRISHNAN T.The EM Algorithm and Extensions[M].New York:John Wiley and Sons, 1997:6-9.
                            </a>
                        </p>
                        <p id="659">
                            <a id="bibliography_51" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES4A6188FE98C4652F9A8871301EF78AAE&amp;v=MjY5NDk3cS94YXM9TmlmT2ZiZkpHTkRFcC9rd2JlTjhDSG84eldBYW16ZDFUMzdockJOQUQ3V2NOTXZxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[51]</b> ZHOU Y, XU S, JIN C, et al.Multiple point sets registration based on expectation maximization algorithm [J].Computers and Electrical Engineering, 2018, 70:1-11.
                            </a>
                        </p>
                        <p id="661">
                            <a id="bibliography_52" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering gene expression patterns">

                                <b>[52]</b> BEN-DOR A, SHAMIR R, YAKHINI Z.Clustering gene expression patterns[J].Journal of Computational Biology, 1999, 6:281-297.
                            </a>
                        </p>
                        <p id="663">
                            <a id="bibliography_53" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CLICK: a clustering algorithm with applications to gene expression analysis">

                                <b>[53]</b> SHARAN R, SHAMIR R.CLICK:A clustering algorithm with applications to gene expression analysis[C]// Proceedings of the 8th International Conference on Intelligent Systems for Molecular Biology.San Diego:[s.n.], 2000:307-316.
                            </a>
                        </p>
                        <p id="665">
                            <a id="bibliography_54" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJRS&amp;filename=SJRSBD83030EC908205125BCB459C4E26103&amp;v=MDIyODBRbGZCckxVMDV0cGh3N3EveGFzPU5pZlpmY0hNRnRMTXJJOHdGK0lQQkg0NXloY1I3MDBPT252bnBXRXhETENTUkxxY0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPRw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[54]</b> NGUYEN M N, SIM A Y L, WAN Y, et al.Topology independent comparison of RNA 3D structures using the CLICK algorithm[J].Nucleic Acids Research, 2017, 45 (1) :e5.
                            </a>
                        </p>
                        <p id="667">
                            <a id="bibliography_55" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6E4857C69F4F1F934899A4715565823E&amp;v=MDE1MzRwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaWZPZmJYTkd0bkpxUHhEYlowTGVuMVB4aFVYNGpaME9YdmxyUmN3ZjdlY1I3bnFDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[55]</b> DING C F, LI K.Centrality ranking in multiplex networks using topologically biased random walks[J].Neuro-computing, 2018, 312:263-275.
                            </a>
                        </p>
                        <p id="669">
                            <a id="bibliography_56" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES24A84D79395484ED5F25AD179A0AAA90&amp;v=MjYwMzNhcz1OaWZPZmJHOGI5bkkyNGhNWitJS0NIUTl1bUlXbkQxNE9RdmpxeHRFZWNQbE5MT2ZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[56]</b> SHANG R, ZHANG Z, JIAO L, et al.Global discriminative-based nonnegative spectral clustering[J].Pattern Recognition, 2016, 55:172-182.
                            </a>
                        </p>
                        <p id="671">
                            <a id="bibliography_57" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201610021&amp;v=MTY2MTN5L25VN3JMTHo3QmQ3RzRIOWZOcjQ5SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[57]</b> 宋健, 许国艳, 夭荣朋.基于差分隐私的数据匿名化隐私保护方法[J].计算机应用, 2016, 36 (10) :2753-2757. (SONG J, XU G Y, YAO R P.Spectral clustering algorithm based on differential privacy protection[J].Journal of Computer Applications, 2016, 36 (10) :2753-2757.) 
                            </a>
                        </p>
                        <p id="673">
                            <a id="bibliography_58" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601417758&amp;v=MDA3MThkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VWJ4RT1OaWZPZmJLN0h0RE5xWTlFWU9vSUMza3hvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[58]</b> LEE C-H, ZAIANE O R, PARK H-H, et al.Clustering high dimensional data:a graph-based relaxed optimization approach[J].Information Sciences, 2008, 178 (23) :4501-4511.
                            </a>
                        </p>
                        <p id="675">
                            <a id="bibliography_59" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES20E8B149712152668B810F0763205918&amp;v=MTUxNTJhQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZiRzRhOW0rcm90TVkrb05EWGs3eVJBYm1EZDhTQW5pcXhRMmU3S1JUTHVYQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[59]</b> SHI D, WANG J, CHENG D, et al.A global-local affinity matrix model via EigenGap for graph-based subspace clustering [J].Pattern Recognition Letters, 2017, 89:67-72.
                            </a>
                        </p>
                        <p id="677">
                            <a id="bibliography_60" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pattern recognition with fuzzy objective function algorithms">

                                <b>[60]</b> BEZDEK J.Pattern Recognition with Fuzzy Objective Function Algorithms[M].New York:Plenum Press, 1981:37-89.
                            </a>
                        </p>
                        <p id="679">
                            <a id="bibliography_61" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy c-means clustering of incomplete data">

                                <b>[61]</b> HATHAWAY R, BEZDEK J.Fuzzy c-means clustering of incomplete data[J].IEEE Transactions on Systems, Man, and Cybernetics, 2001, 31 (5) :735-744.
                            </a>
                        </p>
                        <p id="681">
                            <a id="bibliography_62" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise">

                                <b>[62]</b> ESTER M, KRIEGEL H, SANDER J, et al.A density-based algorithm for discovering clusters in large spatial databases with noise[C]// Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.New York:AAAI Press, 1996:56-69.
                            </a>
                        </p>
                        <p id="683">
                            <a id="bibliography_63" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300248374&amp;v=MjgwNjlpbmxVcjNJSjF3VWJ4RT1OaWZPZmJLN0h0RE9ySTlGWnU4SEQzczlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0Rg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[63]</b> BIRANT D, KUT A.ST-DBSCAN:an algorithm for clustering spatial-temporal data[J].Data and Knowledge Engineering, 2007, 60 (1) :208-221.
                            </a>
                        </p>
                        <p id="685">
                            <a id="bibliography_64" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ST-DBSCAN clustering module in SpagoBI for hotspots distribution in Indonesia">

                                <b>[64]</b> TRISMININGSIH R, SHAZTIKA S S.ST-DBSCAN clustering module in SpagoBI for hotspots distribution in Indonesia[C]// Proceedings of the 3rd International Conference on Information Technology.New York:ACM, 2017:60-67.
                            </a>
                        </p>
                        <p id="687">
                            <a id="bibliography_65" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OPTICS: Ordering points to identify the clustering structure">

                                <b>[65]</b> ANKERST M, BREUNIG M, KRIEGEL H, et al.OPTICS:ordering points to identify the clustering structure[C]// Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1999:49-60.
                            </a>
                        </p>
                        <p id="689">
                            <a id="bibliography_66" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An efficient approach to clustering in large multimedia databases with noise">

                                <b>[66]</b> HINNEBURG A, KEIM D.An efficient approach to clustering in large multimedia databases with noise[C]// Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1998:58-65.
                            </a>
                        </p>
                        <p id="691">
                            <a id="bibliography_67" target="_blank" href="http://scholar.cnki.net/result.aspx?q=STING: a statistical information grid approach to spatial data mining">

                                <b>[67]</b> WANG W, YANG J, MUNTZ R.STING:a statistical information grid approach to spatial data mining[C]// Proceedings of the 23rd Conference on Very Large Data Bases.New York:ACM, 1997:186-195.
                            </a>
                        </p>
                        <p id="693">
                            <a id="bibliography_68" target="_blank" href="http://scholar.cnki.net/result.aspx?q=WaveCluster:A Multi-Resolution Clustering Approach for Very Large Spatial Databases">

                                <b>[68]</b> SHEIKHOIESIAMI G, CHATTERJEE S, ZHANG A.WaveCluster:a multi-resolution clustering approach for very large spatial databases[C]// Proceedings of the 24th Conference on Very Large Data Bases.New York:ACM, 1998:428-439.
                            </a>
                        </p>
                        <p id="695">
                            <a id="bibliography_69" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A comparative study of the parallel wavelet-based clustering algorithm on three-dimensional dataset">

                                <b>[69]</b> YILDIRIM A A, WATSON D.A comparative study of the parallel wavelet-based clustering algorithm on three-dimensional dataset[J].Journal of Supercomputing, 2015, 71 (7) :2365-2380.
                            </a>
                        </p>
                        <p id="697">
                            <a id="bibliography_70" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Synchronization inspired partitioning and hierarchical clustering">

                                <b>[70]</b> SHAO J, HE X, BOHM C, et al.Synchronization-inspired partitioning and hierarchical clustering[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (4) :893-905.
                            </a>
                        </p>
                        <p id="699">
                            <a id="bibliography_71" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering by passing messages between data points">

                                <b>[71]</b> FREY B J, DUECK D.Clustering by passing messages between data points[J].Science, 2007, 315 (5814) :972-976.
                            </a>
                        </p>
                        <p id="701">
                            <a id="bibliography_72" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201403042&amp;v=MDA0MjU3RzRIOVhNckk5QlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMTHo3QmI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[72]</b> 刘晓楠, 尹美娟, 李明涛, 等.面向大规模数据的分层近邻传播聚类算法[J].计算机科学, 2014, 41 (3) :185-188. (LIU X N, YIN M J, LI M T, et al.Hierarchical affinity propagation clustering for large-scale data set[J].Computer Science, 2014, 41 (3) :185-188.) 
                            </a>
                        </p>
                        <p id="703">
                            <a id="bibliography_73" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJAH&amp;filename=SJAHA71DC551782EDD163ED017A9E23848A3&amp;v=MTg2NjFMVTA1dHBodzdxL3hhcz1OaWZLWnNLL0g2Vy9xb3BFWStNTmVRaE56aEFRbjB0OVNYaVRwV2MzZXJxUVRjdWNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[73]</b> AKASH O M, AZMI M S B.A new similarity measure based affinity propagation for data clustering [J].Advanced Science Letters, 2018, 24 (2) :1130-1133.
                            </a>
                        </p>
                        <p id="705">
                            <a id="bibliography_74" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards controller placement problem for software-defined network using affinity propagation">

                                <b>[74]</b> ZHAO J L, QU H, ZHAO J H.Towards controller placement problem for software-defined network using affinity propagation[J].Electronics Letters, 2017, 53 (14) :928-929.
                            </a>
                        </p>
                        <p id="707">
                            <a id="bibliography_75" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering by fast search and find of densitypeaks">

                                <b>[75]</b> RODRIGUEZ A, LAIO A.Clustering by fast search and find of density peaks[J].Science, 2014, 344 (6191) :1492-1496.
                            </a>
                        </p>
                        <p id="709">
                            <a id="bibliography_76" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fast density peaks clustering algorithm based on pre-screening">

                                <b>[76]</b> XIAO X, DING S, SUN T.A fast density peaks clustering algorithm based on pre-screening[C]// Proceedings of the 2018 IEEE International Conference on Big Data and Smart Computing.Piscataway, NJ:IEEE, 2018:456-462.
                            </a>
                        </p>
                        <p id="711">
                            <a id="bibliography_77" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900004958&amp;v=MDY0MDhaT3NMQlhreG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmlmT2ZiSzdIdFRNcG85Rg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[77]</b> GHOSH S, MITRA S.Clustering large data with uncertainty[J].Applied Soft Computing, 2013, 13 (4) :1639-1645.
                            </a>
                        </p>
                        <p id="713">
                            <a id="bibliography_78" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDC67B9C49390E287DAE65A467915D0232&amp;v=MjY2MzM3bWRDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5qN0Jhc0MrR2FQRjNJdE1aK0lQZVg0eHlHSmluemw0T1h2a3F4czBmTWFVUg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[78]</b> LE H S, NGUYEN D T.Tune up fuzzy c-means for big data:some novel hybrid clustering algorithms based on initial selection and incremental clustering[J].International Journal of Fuzzy Systems, 2017, 19 (5) :1585-1602.
                            </a>
                        </p>
                        <p id="715">
                            <a id="bibliography_79" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical clustering multi-task learning for joint human action grouping and recognition">

                                <b>[79]</b> LIU A, SU Y, NIE W, et al.Hierarchical clustering multi-task learning for joint human action grouping and recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (1) :102-114.
                            </a>
                        </p>
                        <p id="717">
                            <a id="bibliography_80" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modified balanced iterative reducing and clustering using hierarchies (m-BIRCH)for visual clustering">

                                <b>[80]</b> MADAN S, DANA K J.Modified balanced iterative reducing and clustering using hierarchies (m-BIRCH) for visual clustering[J].Pattern Analysis and Applications, 2016, 19 (4) :1023-1040.
                            </a>
                        </p>
                        <p id="719">
                            <a id="bibliography_81" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100892160&amp;v=MjcxMjl4RT1OaWZPZmJLN0h0RE9ybzlGYk9JTkRYbzVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXdVYg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[81]</b> GUHA S, RASTOGI R, SHIM K, et al.CURE:an efficient clustering algorithm for large databases[J].Information Systems, 1998, 26 (1) :35-58.
                            </a>
                        </p>
                        <p id="721">
                            <a id="bibliography_82" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100892195&amp;v=MjA0MTVGYk9JTkRYVThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXdVYnhFPU5pZk9mYks3SHRET3JvOQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[82]</b> GUHA S, RASTOGI R, SHIM K.ROCK:a robust clustering algorithm for categorical attributes[J].Information Systems, 1999, 25 (5) :345-366.
                            </a>
                        </p>
                        <p id="723">
                            <a id="bibliography_83" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300248277&amp;v=MjkyNzdSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmlmT2ZiSzdIdERPckk5Rlp1OEhEbnMrb0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[83]</b> GELBARD R, GOLDMAN O, SPIEGLER I.Investigating diversity of clustering methods:an empirical comparison[J].Data and Knowledge Engineering, 2007, 63 (1) :155-166.
                            </a>
                        </p>
                        <p id="725">
                            <a id="bibliography_84" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of fuzzy clustering algorithms for pattern recognition. I">

                                <b>[84]</b> BARALDI A, BLONDA P.A survey of fuzzy clustering algorithms for pattern recognition—Part I and II[J].IEEE Transactions on Systems, Man, and Cybernetics—Part B:Cybernetics, 1999, 29 (6) :778-801.
                            </a>
                        </p>
                        <p id="727">
                            <a id="bibliography_85" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic Self-Organizing Maps with Controlled Growth for Knowledge Discovery">

                                <b>[85]</b> ALAHAKOON D, HALGAMUGE S K, SRINIVASAN B.Dynamic self-organizing maps with controlled growth for knowledge discovery[J].IEEE Transactions on Neural Networks, 2000, 11 (3) :601-614.
                            </a>
                        </p>
                        <p id="729">
                            <a id="bibliography_86" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ViSOM-a novel method for multivariate data projection and structure visualization">

                                <b>[86]</b> YIN H.VISOM:a novel method for multivariate data projection and structure[J].IEEE Transactions on Neural Networks, 2002, 13 (1) :237-243.
                            </a>
                        </p>
                        <p id="731">
                            <a id="bibliography_87" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamics of projective adaptive resonance theory model: the foundation of PART algorithm">

                                <b>[87]</b> CAO Y, WU J.Dynamics of projective adaptive resonance theory model:the foundation of PART algorithm[J].IEEE Transactions on Neural Network, 2004, 15 (2) :245-260.
                            </a>
                        </p>
                        <p id="733">
                            <a id="bibliography_88" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES06858764C5555FCEE730F682E7892A6A&amp;v=MDI3MjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaWZPZmJPK0Z0VEVxSWxCRis0S0NYbFB2R05tN1R4OVBubnFybWN5Y2J1V05MenVDT052RlNpV1dyNw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[88]</b> ZHANG Y, LU J, LIU F.Does deep learning help topic extraction?a kernel k-means clustering method with word embedding [J].Journal of Informetrics, 2018, 12 (4) :1099-1117.
                            </a>
                        </p>
                        <p id="735">
                            <a id="bibliography_89" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FlowNet:a deep learning framework for clustering and selection of streamlines and stream surfaces">

                                <b>[89]</b> HAN J, TAO J, WANG C.FlowNet:a deep learning framework for clustering and selection of streamlines and stream surfaces [J].IEEE Transactions on Visualization and Computer Graphics, 2018, 11:678-689.
                            </a>
                        </p>
                        <p id="737">
                            <a id="bibliography_90" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepThings:distributed adaptive deep learning inference on resource-constrained IoT edge clusters">

                                <b>[90]</b> ZHAO Z, BARIJOUGH K, GERSTLAUSER A.DeepThings:distributed adaptive deep learning inference on resource-constrained IoT edge clusters [J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (11) :2348-2359.
                            </a>
                        </p>
                        <p id="739">
                            <a id="bibliography_91" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339251&amp;v=MDM0NjRIdEhOckl4TVp1NE9ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpcmxWTDNJSlYwPU5qN0Jhck80&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[91]</b> CORTES C, VAPNIK V.Support vector networks[J].Machine Learning, 1995, 20:273-297.
                            </a>
                        </p>
                        <p id="741">
                            <a id="bibliography_92" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incorporating Invariances in Support Vector Learning Machines">

                                <b>[92]</b> SCHÖLKOPF B, BURGES C, VAPNIK V.Incorporating invariances in support vector learning machines[C]// Proceedings of the 1996 International Conference on Artificial Neural Networks.Berlin:Springer, 1996:47-52.
                            </a>
                        </p>
                        <p id="743">
                            <a id="bibliography_93" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300415525&amp;v=MzIwMjExd1VieEU9TmlmT2ZiSzdIdERPckk5RllPb0tDWDQ4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[93]</b> TAX D M J, DUIN R P W.Support vector domain description[J].Pattern Recognition Letters, 1999, 20 (11) :1191-1199.
                            </a>
                        </p>
                        <p id="745">
                            <a id="bibliography_94" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A support vector clustering method">

                                <b>[94]</b> BEN-HUR A, HORN D, SIEGELMANN H T, et al.A support vector clustering method[C]// Proceedings of the 2000 International Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2000:2724-2727.
                            </a>
                        </p>
                        <p id="747">
                            <a id="bibliography_95" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support vector clustering">

                                <b>[95]</b> BEN-HUR A, HORN D, SIEGELMANN H T, et al.Support vector clustering[J].Journal Machine Learning Research, 2001, 2:125-137.
                            </a>
                        </p>
                        <p id="749">
                            <a id="bibliography_96" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structured One-Class Classification">

                                <b>[96]</b> WANG D, YEUNG D S, TSANG T C C.Structured one-class classification[J].IEEE Transactions on Systems, Man, and Cybernetics, 2006, 36 (6) :1283-1295.
                            </a>
                        </p>
                        <p id="751">
                            <a id="bibliography_97" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Centered hyperspherical and hyperellipsoidal one-class support vector machines for anomaly detection in sensor networks">

                                <b>[97]</b> RAJASEGARAR S, LECKIE C, BEZDEK J C.Centered hyperspherical and hyperellipsoidal one-class support vector machines for anomaly detection in sensor networks[J].IEEE Transactions on Information Forensics and Security, 2010, 5 (3) :518-533.
                            </a>
                        </p>
                        <p id="753">
                            <a id="bibliography_98" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An incremental method combining density clustering and support vector machines for voice pathology detection">

                                <b>[98]</b> AMAMI R, SMITI A.An incremental method combining density clustering and support vector machines for voice pathology detection[J].Computers and Electrical Engineering, 2017, 57:257-265.
                            </a>
                        </p>
                        <p id="755">
                            <a id="bibliography_99" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201811004&amp;v=Mjk1ODRiYkc0SDluTnJvOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9uVTdyTExqZlM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[99]</b> 李海林, 梁叶.基于中心度的标签传播时间序列聚类方法[J].控制与决策, 2018, 33 (11) :33-41. (LI H L, LIANG Y.Time series clustering method with label propagation based on centrality [J].Control and Decision, 2018, 33 (11) :33-41.) 
                            </a>
                        </p>
                        <p id="757">
                            <a id="bibliography_100" >
                                    <b>[100]</b>
                                 熊英志.时间序列的特征表示与聚类方法研究[D].镇江:江苏大学, 2018:36-61. (XIONG Y Z.Research on feature representation and clustering algorithm for time series[D].Zhengjiang:Jiangsu University, 2018:36-61.) 
                            </a>
                        </p>
                        <p id="759">
                            <a id="bibliography_101" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Longitudinal data with serial correlation:a state space approach">

                                <b>[101]</b> JONES R H.Longitudinal data with serial correlation:a state space approach[J].Journal of the Royal Statistical Society, 1994, 36 (2) :231-239.
                            </a>
                        </p>
                        <p id="761">
                            <a id="bibliography_102" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Trajectory clustering with mixtures of regression models">

                                <b>[102]</b> GAFFNEY S, SMYTH P.Trajectory clustering with mixtures of regression models[C]// Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:63-72.
                            </a>
                        </p>
                        <p id="763">
                            <a id="bibliography_103" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDDB8B307A3B081731EF5C3620F4D81CEB&amp;v=MzA2MzU9TmpuQmFzZktGcVBQcjRnMFo1a1BCSDArekJkbW5Eb09TM25nckdReERicVZOcy90Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[103]</b> DU F, ZHU A, QI F.Interactive visual cluster detection in large geospatial datasets based on dynamic density volume visualization[J].Geocarto International, 2016, 31 (6) :597-611.
                            </a>
                        </p>
                        <p id="765">
                            <a id="bibliography_104" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD62ECD9B7DCFE697D315432292B8D9D95&amp;v=MjA0NTI2enA1UzMzZ3BSQkhjY2FkTWJPYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmpuQmFyVzZhNks0cHYxQ0VKaDVlWG93eUdJUQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[104]</b> HU A, CAO J, HU M, et al.Distributed control of cluster synchronisation in networks with randomly occurring non-linearities[J].International Journal of Systems Science, 2015, 65 (4) :1-10.
                            </a>
                        </p>
                        <p id="767">
                            <a id="bibliography_105" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Complex network clustering by multiobjective discrete particle swarm optimization based on decomposition">

                                <b>[105]</b> GONG M, CAI Q, CHEN X, et al.Complex network clustering by multiobjective discrete particle swarm optimization based on decomposition [J].IEEE Transactions on Evolutionary Computation, 2014, 18 (1) :82-97.
                            </a>
                        </p>
                        <p id="769">
                            <a id="bibliography_106" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Functional cartography of complex metabolic networks">

                                <b>[106]</b> GUIMERA R, AMARAL L A N.Functional cartography of complex metabolic networks[J].Nature, 2005, 433 (7028) :895-900.
                            </a>
                        </p>
                        <p id="771">
                            <a id="bibliography_107" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200901008&amp;v=MDIwNjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvblU3ckxOeWZUYkxHNEh0ak1ybzlGYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[107]</b> 杨博, 刘大有, LIU J-M, 等.复杂网络聚类方法[J].软件学报, 2009, 20 (1) :54-66. (YANG B, LIU D Y, LIU J-M, et al.Complex network clustering algorithms [J].Journal of Software, 2009, 20 (1) :54-66.) 
                            </a>
                        </p>
                        <p id="773">
                            <a id="bibliography_108" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Community structure in social and biological networks">

                                <b>[108]</b> GIRVAN M, NEWMAN M E J.Community structure in social and biological networks[J].Proceedings of the National Academy of Science, 2002, 99 (12) :7821-7826.
                            </a>
                        </p>
                        <p id="775">
                            <a id="bibliography_109" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201605037&amp;v=MDQ1NDZxZlp1WnNGeS9uVTdyTEx5SE1kN0c0SDlmTXFvOUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[109]</b> 郭玉泉, 李雄飞.复杂网络社区的分形聚类检测方法[J].吉林大学学报 (工学版) , 2016, 46 (5) :1633-1638. (GUO Y Q, LI X F.Fractal clustering method for uncovering community of complex network [J].Journal of Jilin University (Engineering and Technology Edition) , 2016, 46 (5) :1633-1638.) 
                            </a>
                        </p>
                        <p id="777">
                            <a id="bibliography_110" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201503025&amp;v=MTYzODhIOVRNckk5SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMSVRmU2RyRzQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[110]</b> 赵凤, 刘汉强, 范九伦.基于互补空间信息的多目标进化聚类图像分割[J].电子与信息学报, 2015, 37 (3) :672-678. (ZHAO F, LIU H Q, FAN J L.Multi-objective evolutionary clustering with complementary spatial information for image segmentation [J].Journal of Electronics and Information Technology, 2015, 37 (3) :672-678.) 
                            </a>
                        </p>
                        <p id="779">
                            <a id="bibliography_111" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200102007&amp;v=Mjc5ODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMTnlmVGJMRzRIdERNclk5Rlk0UUs=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[111]</b> 张引, 潘云鹤.基于模拟退火的最大似然聚类图像分割算法[J].软件学报, 2001, 12 (2) :212-218. (ZHANG Y, PAN Y H.Simulated annealing based maximum likelihood clustering algorithm for image segmentation [J].Journal of Software, 2001, 12 (2) :212-218.) 
                            </a>
                        </p>
                        <p id="781">
                            <a id="bibliography_112" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJIF&amp;filename=SJIF13022100009273&amp;v=MTE5NjRieEU9TmlmQ2FMSzdIdFBPcm85RlpPc0dEbnM2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[112]</b> GLOVER F.Tabu search, Part I[J].ORSA Journal of Computing, 1989, 1 (3) :190-206.
                            </a>
                        </p>
                        <p id="783">
                            <a id="bibliography_113" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC5DED3C8ED4FBA96111B92C86EF1B4F2&amp;v=MzEzMDBlZzVJeGhBUzZ6NFBRWDJScEJSQUQ3UG1RY3lkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzdxL3hhcz1OaWZPZmNDOWFxUzRyUHhORVo4TA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[113]</b> BOUYER A, HATAMLOU A.An efficient hybrid clustering method based on improved cuckoo optimization and modified particle swarm optimization algorithms[J].Applied Soft Computing, 2018, 67:172-182.
                            </a>
                        </p>
                        <p id="785">
                            <a id="bibliography_114" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14021300038273&amp;v=MjU4NjlUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VWJ4RT1OaWZPZmJLOEh0UE5ySTlGWk9nSERuczZvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[114]</b> MIRJALILI S, MIRJALILI S M, LEWIS A.Grey wolf optimizer[J].Advances in Engineering Software, 2014, 69 (3) :46-61.
                            </a>
                        </p>
                        <p id="787">
                            <a id="bibliography_115" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC201810020&amp;v=MDE0NzMzenFxQnRHRnJDVVI3cWZadVpzRnkvblU3ckxQeW5SYmJHNEg5bk5yNDlIWklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[115]</b> 李麟玮, 吴益平, 苗发盛.基于灰狼支持向量机的非等时距滑坡位移预测[J].浙江大学学报 (工学版) , 2018, 52 (10) :167-175. (LI L W, WU Y P, MIAO F S.Prediction of non-equidistant landslide displacement time series based on grey wolf support vector machine [J].Journal of Zhejiang University (Engineering Science) , 2018, 52 (10) :167-175.) 
                            </a>
                        </p>
                        <p id="789">
                            <a id="bibliography_116" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD42D9D02BA804C34088B520303CF3C9E4&amp;v=MjU4MzRmQnJMVTA1dHBodzdxL3hhcz1OaWZjYXJlNmF0aTRyNDAzRmVNUENBODZ5eFliNGsxNFNuL2hyQkZHRDdIblRNK2JDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[116]</b> DENG C, LIU Y, XU L, et al.A MapReduce-based parallel K-means clustering for large-scale CIM data verification[J].Concurrency and Computation Practice and Experience, 2016, 28 (11) :3096-3114.
                            </a>
                        </p>
                        <p id="791">
                            <a id="bibliography_117" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300030304&amp;v=MDU3MTE9Tmo3QmFySzhIdExNckk5RlpPZ1BEM3c5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjF3VWJ4RQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[117]</b> HE Y, TAN H, LUO W, et al.MR-DBSCAN:a scalable MapReduce-based DBSCAN algorithm for heavily skewed data[J].Frontiers of Computer Science, 2014, 8 (1) :83-99.
                            </a>
                        </p>
                        <p id="793">
                            <a id="bibliography_118" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD8ACC45B6FDA290A3209D0FEC13DFB88&amp;v=MDc1MzN0cGh3N3EveGFzPU5qN0Jhc2V3YjZLL3E0bzNZcDE3Zlg0d3oyY1E2RDkwUEgrVTJXRTBlc2JpTjdLWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[118]</b> LI J, CHEN Q, LIU B.Classification and disease probability prediction via machine learning programming based on multi-GPU cluster MapReduce system[J].Journal of Supercomputing, 2017, 73 (5) :1782-1809.
                            </a>
                        </p>
                        <p id="795">
                            <a id="bibliography_119" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5735B6E92EA92AAA53090450CD419590&amp;v=MjQxNTlXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZiYS9IZFMrcWZwTVpwNStCWDVJdm1jVzZUOTBTSHZuckdGQmZiT2RRTE9mQ09OdkZTaQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[119]</b> LU Y, CAO B, REGO C.A tabu search based clustering algorithm and its parallel implementation on Spark[J].Applied Soft Computing, 2018, 63:97-109.
                            </a>
                        </p>
                        <p id="797">
                            <a id="bibliography_120" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300066838&amp;v=MjQ1OThadEZpbmxVcjNJSjF3VWJ4RT1OaWZPZmJLN0h0RE5ySTlGWk8wSkJIOHhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[120]</b> ZHOU A, WANG H, SONG P.Experiments on light vertex matching algorithm for multilevel partitioning of network topology[J].Procedia Engineering, 2012, 29:2715-2720.
                            </a>
                        </p>
                        <p id="799">
                            <a id="bibliography_121" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13080100007701&amp;v=MzA0NTQzSUoxd1VieEU9TmlmT2ZiSzdIdG5Ncm85RlpPc0lDM3c0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[121]</b> ANDRADE G, RAMOS G, MADEIRA D, et al.G-DBSCAN:a GPU accelerated algorithm for density-based clustering[J].Procedia Computer Science, 2013, 18 (1) :369-378.
                            </a>
                        </p>
                        <p id="801">
                            <a id="bibliography_122" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Density-Based Clustering based on GPU Accelerated Data Indexing Strategy">

                                <b>[122]</b> MELO D, TOLEDO S, MOURÃO F, et al.Hierarchical density-based clustering based on GPU accelerated data indexing strategy[J].Procedia Computer Science, 2016, 80:951-961.
                            </a>
                        </p>
                        <p id="803">
                            <a id="bibliography_123" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000085405&amp;v=MjQ4OTJZN0s3SHRqTnI0OUZaT01LQ0h3OG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxd1VieEU9TmlmSQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[123]</b> PARSONS L, HAQUE E, LIU H.Subspace clustering for high dimensional data:a review [J].ACM SIGKDD Explorations Newsletter, 2004, 6 (1) :90-105.
                            </a>
                        </p>
                        <p id="805">
                            <a id="bibliography_124" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Subspace clustering via learning an adaptive low-rank graph">

                                <b>[124]</b> YIN M, XIE S, WU Z, et al.Subspace clustering via learning an adaptive low-rank graph.[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3716-3728.
                            </a>
                        </p>
                        <p id="807">
                            <a id="bibliography_125" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic subspace clustering of high dimensional data for data mining applications">

                                <b>[125]</b> AGRAWAL R, GEHRKE J, GUNOPULOS D, et al.Automatic subspace clustering of high dimensional data for data mining applications[C]// Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1998:94-105.
                            </a>
                        </p>
                        <p id="809">
                            <a id="bibliography_126" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Entropy-based subspace clustering for mining numerical data">

                                <b>[126]</b> CHENG C H, FU A W, ZHANG Y.Entropy-based subspace clustering for mining numerical data[C]// Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:84-93
                            </a>
                        </p>
                        <p id="811">
                            <a id="bibliography_127" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MAFIA:Efficient and scalable subspace clustering for very large data sets">

                                <b>[127]</b> GOIL S, NAGESH H, CHOUDHARY A.MAFIA:efficient and scalable subspace clustering for very large data sets[C]// Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 1999:443-452.
                            </a>
                        </p>
                        <p id="813">
                            <a id="bibliography_128" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Finding generalized projected clusters in high dimensional spaces">

                                <b>[128]</b> AGGARWAL C C, YU P S.Finding generalized projected clusters in high dimensional spaces[C]// Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data.New York:ACM, 2000:70-81.
                            </a>
                        </p>
                        <p id="815">
                            <a id="bibliography_129" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501721381&amp;v=MTk2OTRxUVRNbndaZVp0RmlubFVyM0lKMXdVYnhFPU5pZk9mYks3SHRETnFvOUVZK2tPRDNRNG9CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[129]</b> WOO K G, LEE J H, KIM M H, et al.FINDIT:a fast and intelligent subspace clustering algorithm using dimension voting[J].Information and Software Technology, 2004, 46 (4) :255-271.
                            </a>
                        </p>
                        <p id="817">
                            <a id="bibliography_130" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130122008732&amp;v=MDk0MDJqS0lGc1hOajdCYXJLN0h0RE9yWTlGYk93TURoTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[130]</b> SIM K, GOPALKRISHNAN V, ZIMEK A, et al.A survey on enhanced subspace clustering[J].Data Mining and Knowledge Discovery, 2013, 26 (2) :332-397.
                            </a>
                        </p>
                        <p id="819">
                            <a id="bibliography_131" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A model-based approach for discrete data clustering and feature weighting using MAP and stochastic complexity">

                                <b>[131]</b> BOUGUILA N.A model-based approach for discrete data clustering and feature weighting using MAP and stochastic complexity[J].IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (12) :1649-1664.
                            </a>
                        </p>
                        <p id="821">
                            <a id="bibliography_132" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9BFC72BF5E0B097FF5CAF058B16727D2&amp;v=MTU3MDRhQnVIWWZPR1FsZkJyTFUwNXRwaHc3cS94YXM9TmlmT2ZickthS0xMcmYwellaNFBmbnd3eUdCbDcwd01Qbi9ucEdBMGY3V1dRczZkQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[132]</b> CHEN L, WANG S, WANG K, et al.Soft subspace clustering of categorical data with probabilistic distance[J].Pattern Recognition, 2016, 51 (C) :322-332.
                            </a>
                        </p>
                        <p id="823">
                            <a id="bibliography_133" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3B6CED85BBFEA79C9D3ED34F63FA668C&amp;v=MDAzNTlodzdxL3hhcz1OaWZPZmJES0dLSzUyNGRBRnBsNWVRMCt4bVVhbmp3SVBIem0yaFEyRDhPU1E3THNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[133]</b> DENG Z, CHOI K-S, WANG J, et al.A survey on soft subspace clustering[J].Information Sciences, 2014, 348:84-106.
                            </a>
                        </p>
                        <p id="825">
                            <a id="bibliography_134" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Biclustering of expression data">

                                <b>[134]</b> CHENG Y, CHURCH G M.Biclustering of expression data[C]// Proceedings of the 2000 International Conference of Intelligent Systems for Molecular Biology.New York:ACM, 2000:93-103.
                            </a>
                        </p>
                        <p id="827">
                            <a id="bibliography_135" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cluster validity methods">

                                <b>[135]</b> HALKIDI M, BATISTAKIS Y, VAZIRGIANNIS M.Cluster validity methods[C]// Proceedings of the 2002 International Conference on Special Interest Group on Management of Data.New York:ACM, 2002:127-131.
                            </a>
                        </p>
                        <p id="829">
                            <a id="bibliography_136" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pattern Recognition">

                                <b>[136]</b> THEODORIDIS S, KOUTROUMBAS K.Pattern Recognition [M].3rd ed.San Diego:Academic Press, 2006:56-63.
                            </a>
                        </p>
                        <p id="831">
                            <a id="bibliography_137" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES520C43D733D488CEFF829F685D4F00BC&amp;v=MDA0MzdmT0dRbGZCckxVMDV0cGh3N3EveGFzPU5pZk9mYmE2SHFMSXJQdENaK2g3Q0hReHZHTmxuRGQvUVFua3BCZEJmY1NVUmNqc0NPTnZGU2lXV3I3SklGcG1hQnVIWQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[137]</b> JOSÉ-GARCÍA A, GÓMEZ-FLORES W.Automatic clustering using nature-inspired metaheuristics:a survey[J].Applied Soft Computing, 2015, 41:192-213.
                            </a>
                        </p>
                        <p id="833">
                            <a id="bibliography_138" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint tumor segmentation in pet-ct images using co-clustering and fusion based on belief functions">

                                <b>[138]</b> LIAN C, RUAN S, DENOEUX T, et al.Joint tumor segmentation in PET-CT images using co-clustering and fusion based on belief functions[J].IEEE Transactions on Image Processing, 2019, 28 (2) :755-766.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907002" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907002&amp;v=MTMyOTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5L25VN3JMTHo3QmQ3RzRIOWpNcUk5RlpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="11" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
