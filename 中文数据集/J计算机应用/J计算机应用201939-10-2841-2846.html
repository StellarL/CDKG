<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136455560127500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910009%26RESULT%3d1%26SIGN%3dErAhs8oc%252b6lCZAKfriUVEapzMZ0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910009&amp;v=MjE0NjBGckNVUjdxZlp1WnNGeW5oVUwzSUx6N0JkN0c0SDlqTnI0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="1 词向量 ">1 词向量</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 联合网络引入注意力机制模型 ">2 联合网络引入注意力机制模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="2.1 &lt;i&gt;CNN&lt;/i&gt;-&lt;i&gt;BiGRU&lt;/i&gt;&lt;b&gt;联合网络特征学习&lt;/b&gt;">2.1 <i>CNN</i>-<i>BiGRU</i><b>联合网络特征学习</b></a></li>
                                                <li><a href="#98" data-title="2.2 CNN-BiGRU-Attention&lt;b&gt;特征筛选&lt;/b&gt;">2.2 CNN-BiGRU-Attention<b>特征筛选</b></a></li>
                                                <li><a href="#108" data-title="2.3 C-BG-A&lt;b&gt;模型&lt;/b&gt;">2.3 C-BG-A<b>模型</b></a></li>
                                                <li><a href="#121" data-title="2.4 &lt;b&gt;模型训练&lt;/b&gt;">2.4 <b>模型训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#135" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#138" data-title="3.1 &lt;b&gt;实验环境与数据&lt;/b&gt;">3.1 <b>实验环境与数据</b></a></li>
                                                <li><a href="#140" data-title="3.2 &lt;b&gt;参数设置&lt;/b&gt;">3.2 <b>参数设置</b></a></li>
                                                <li><a href="#143" data-title="3.3 &lt;b&gt;评价指标&lt;/b&gt;">3.3 <b>评价指标</b></a></li>
                                                <li><a href="#155" data-title="3.4 &lt;b&gt;对比实验设置&lt;/b&gt;">3.4 <b>对比实验设置</b></a></li>
                                                <li><a href="#173" data-title="3.5 &lt;b&gt;实验结果分析&lt;/b&gt;">3.5 <b>实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#193" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="图1 Skip-gram模型">图1 Skip-gram模型</a></li>
                                                <li><a href="#84" data-title="图2 CNN模型的结构">图2 CNN模型的结构</a></li>
                                                <li><a href="#86" data-title="图3 BiGRU模型的结构">图3 BiGRU模型的结构</a></li>
                                                <li><a href="#97" data-title="图4 GRU模型">图4 GRU模型</a></li>
                                                <li><a href="#100" data-title="图5 注意力模型结构">图5 注意力模型结构</a></li>
                                                <li><a href="#110" data-title="图6 &lt;i&gt;C&lt;/i&gt;-&lt;i&gt;BG&lt;/i&gt;-&lt;i&gt;A&lt;/i&gt;模型">图6 <i>C</i>-<i>BG</i>-<i>A</i>模型</a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;实验环境配置&lt;/b&gt;"><b>表</b>1 <b>实验环境配置</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;参数设置&lt;/b&gt;"><b>表</b>2 <b>参数设置</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;评价指标相关参数含义&lt;/b&gt;"><b>表</b>3 <b>评价指标相关参数含义</b></a></li>
                                                <li><a href="#175" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;模型对比结果&lt;/b&gt;"><b>表</b>4 <b>模型对比结果</b></a></li>
                                                <li><a href="#181" data-title="图7 验证集准确率变化">图7 验证集准确率变化</a></li>
                                                <li><a href="#182" data-title="图8 验证集损失率变化">图8 验证集损失率变化</a></li>
                                                <li><a href="#184" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;预测值统计结果&lt;/b&gt;"><b>表</b>5 <b>预测值统计结果</b></a></li>
                                                <li><a href="#188" data-title="图9 模型的时间代价对比">图9 模型的时间代价对比</a></li>
                                                <li><a href="#190" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;单句测试结果&lt;/b&gt;"><b>表</b>6 <b>单句测试结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="224">


                                    <a id="bibliography_1" title="魏韡,向阳,陈千.中文文本情感分析综述[J].计算机应用,2011,31(12):3321-3323.(WEI W,XIANG Y,CHEN Q.Survey on Chinese text sentiment analysis[J].Journal of Computer Applications,2011,31(12):3321-3323.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201112042&amp;v=MTYwNDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFVMM0lMejdCZDdHNEg5RE5yWTlCWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        魏韡,向阳,陈千.中文文本情感分析综述[J].计算机应用,2011,31(12):3321-3323.(WEI W,XIANG Y,CHEN Q.Survey on Chinese text sentiment analysis[J].Journal of Computer Applications,2011,31(12):3321-3323.)
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_2" title="TURNEY P D.Thumbs up or thumbs down?:semantic orientation applied to unsupervised classification of reviews[C]//Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2002:417-424." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Thumbs up or thumbs down?Semantic orientation applied to unsupervised classification of reviews">
                                        <b>[2]</b>
                                        TURNEY P D.Thumbs up or thumbs down?:semantic orientation applied to unsupervised classification of reviews[C]//Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2002:417-424.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_3" title="NI X,XUE G,LING X,et al.Exploring in the weblog space by detecting informative and affective articles[C]//Proceedings of the16th International Conference on World Wide Web.New York:ACM,2007:281-290." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploring in the weblog space by detecting informative and affective articles">
                                        <b>[3]</b>
                                        NI X,XUE G,LING X,et al.Exploring in the weblog space by detecting informative and affective articles[C]//Proceedings of the16th International Conference on World Wide Web.New York:ACM,2007:281-290.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                    PANG B,LEE L,VAITHYANATHAN S.Thumbs up?:sentiment classification using machine learning techniques[C]//Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing.Stroudsburg,PA:Association for Computational Linguistics,2002:79-86.</a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_5" title="BENGIO Y,DUCHARME R,VINCENT P,et al.A neural probabilistic language model[J].Journal of Machine Learning Research,2003,3:1137-1155." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Neural Probabilistic Language Model">
                                        <b>[5]</b>
                                        BENGIO Y,DUCHARME R,VINCENT P,et al.A neural probabilistic language model[J].Journal of Machine Learning Research,2003,3:1137-1155.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_6" title="MIKOLOV T,CHEN K,CORRADO G,et al.Efficient estimation of word representations in vector space[EB/OL].[2017-08-04].http://www.surdeanu.info/mihai/teaching/ista555-spring15/readings/mikolov2013.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">
                                        <b>[6]</b>
                                        MIKOLOV T,CHEN K,CORRADO G,et al.Efficient estimation of word representations in vector space[EB/OL].[2017-08-04].http://www.surdeanu.info/mihai/teaching/ista555-spring15/readings/mikolov2013.pdf.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_7" title="MIKOLOV T,SUTSKEVER I,CHEN K,et al.Distributed representations of words and phrases and their compositionality[EB/OL].[2019-01-10].http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">
                                        <b>[7]</b>
                                        MIKOLOV T,SUTSKEVER I,CHEN K,et al.Distributed representations of words and phrases and their compositionality[EB/OL].[2019-01-10].http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_8" title="MNIH A,HINTON G E.A scalable hierarchical distributed language model[C]//Proceedings of the 21st International Conference on Neural Information Processing.New York:Curran Associates Inc.,2008:1081-1088." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A scalable hierarchical distributed language model">
                                        <b>[8]</b>
                                        MNIH A,HINTON G E.A scalable hierarchical distributed language model[C]//Proceedings of the 21st International Conference on Neural Information Processing.New York:Curran Associates Inc.,2008:1081-1088.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_9" title="KALCHBRENNER N,GREFENSTETTE E,BLUNSOM P.A convolutional neural network for modelling sentences[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2014:655-665." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Convolutional Neural Network for Modelling Sentences">
                                        <b>[9]</b>
                                        KALCHBRENNER N,GREFENSTETTE E,BLUNSOM P.A convolutional neural network for modelling sentences[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2014:655-665.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_10" title="KIM Y.Convolutional neural networks for sentence classification[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Proceeding.Stroudsburg,PA:Association for Computational Linguistics,2014:1746-1751." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for sentence classification">
                                        <b>[10]</b>
                                        KIM Y.Convolutional neural networks for sentence classification[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Proceeding.Stroudsburg,PA:Association for Computational Linguistics,2014:1746-1751.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_11" title="LEE J Y,DERNONCOURT F.Sequential short-text classification with recurrent and convolutional neural networks[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2016,515-520." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks">
                                        <b>[11]</b>
                                        LEE J Y,DERNONCOURT F.Sequential short-text classification with recurrent and convolutional neural networks[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2016,515-520.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_12" title="CHO K,van MERRIENBOER B,GULCEHRE C,et al.Learning phrase representions using RNN encoder-decoder for statistical machine translation[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg,PA:Association for Computational Linguistics,2014:1724-1734." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning phrase representations using RNN encoder-decoder for statistical machine translation">
                                        <b>[12]</b>
                                        CHO K,van MERRIENBOER B,GULCEHRE C,et al.Learning phrase representions using RNN encoder-decoder for statistical machine translation[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg,PA:Association for Computational Linguistics,2014:1724-1734.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_13" title="EBRAHIMI J,DOU D.Chain based RNN for relation classification[C]//Proceedings of the 2015 Annual Conference of the North American Chapter of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2015:1244-1249." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chain based RNN for relation classification">
                                        <b>[13]</b>
                                        EBRAHIMI J,DOU D.Chain based RNN for relation classification[C]//Proceedings of the 2015 Annual Conference of the North American Chapter of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2015:1244-1249.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_14" title="HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTQ5MDRxbzlGWk9vTERYVXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJVm9YYUJJPU5pZkpaYks5SHRqTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_15" title="ZHOU C,SUN C,LIU Z,et al.A C-LSTM neural network for text classification[EB/OL].[2019-01-09].https://arxiv.org/abs/1511.08630." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A C-LSTM Neural Network for Text Classification">
                                        <b>[15]</b>
                                        ZHOU C,SUN C,LIU Z,et al.A C-LSTM neural network for text classification[EB/OL].[2019-01-09].https://arxiv.org/abs/1511.08630.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_16" title="XIAO Z,LIANG P.Chinese sentiment analysis using bidirectional LSTM with word embedding[C]//Proceedings of the 2016 International Conference on Cloud Computing and Security,LNSC10040.Berlin:Springer,2016:601-610." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chinese sentiment analysis using bidirectional LSTM with word embedding">
                                        <b>[16]</b>
                                        XIAO Z,LIANG P.Chinese sentiment analysis using bidirectional LSTM with word embedding[C]//Proceedings of the 2016 International Conference on Cloud Computing and Security,LNSC10040.Berlin:Springer,2016:601-610.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_17" title="BAHDANAU D,CHO K,BENGIO Y.Neural machine translation by jointly learning to align and translate[EB/OL].[2018-03-20].https://arxiv.org/pdf/1409.0473v7.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">
                                        <b>[17]</b>
                                        BAHDANAU D,CHO K,BENGIO Y.Neural machine translation by jointly learning to align and translate[EB/OL].[2018-03-20].https://arxiv.org/pdf/1409.0473v7.pdf.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_18" title="MNIH V,HEESS N,GRAVES A,et al.Recurrent models of visual attention[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:2204-2212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent models of visual attention">
                                        <b>[18]</b>
                                        MNIH V,HEESS N,GRAVES A,et al.Recurrent models of visual attention[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:2204-2212.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_19" title="XU K,BA J,KIROS R,et al.Show,attend and tell:neural image caption generation with visual attention[EB/OL].[2018-03-20].https://arxiv.org/pdf/1502.03044.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention">
                                        <b>[19]</b>
                                        XU K,BA J,KIROS R,et al.Show,attend and tell:neural image caption generation with visual attention[EB/OL].[2018-03-20].https://arxiv.org/pdf/1502.03044.pdf.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_20" >
                                        <b>[20]</b>
                                    LUONG M PHAM H,MANNING C D.Effective approaches to attention-based neural machine translation[C]//Proceedings of the2015 Conference on Empirical Methods in Natural Language Processing.Stroudsburg,PA:Association for Computational Linguistics,2015:1412-1421.</a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                    胡荣磊,芮璐,齐筱,等.基于循环神经网络和注意力模型的文本情感分析[J/OL].计算机应用研究,2019,36(11).[2018-12-10].http://www.arocmag.com/article/02-2019-11-025.html.(HU R L,RUI L,QI X,et al.Text sentiment analysis based on recurrent neural network and attention model[J/OL].Application Research of Computers,2019,36(11).[2018-12-10].http://www.arocmag.com/article/02-2019-11-025.html.)</a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                    王伟,孙玉霞,齐庆杰,等.基于Bi GRU-Attention神经网络的文本情感分类模型[J/OL].计算机应用研究,2018,36(12)[2018-12-10].http://www.arocmag.com/article/02-2019-12-045.html(WANG W,SUN Y X,QI Q J,et al.Text sentiment classification model based on Bi GRU-Attention neural network[J/OL].Application Research of Computers,2018,36(12)[2018-12-10].http://www.arocmag.com/article/02-2019-12-045.html.)</a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_23" title="陈洁,邵志清,张欢欢,等.基于并行混合神经网络模型的短文本情感分析[J/OL].计算机应用,2019.[2018-12-10].http://kns.cnki.net/kcms/detail/51.1307.TP.20190329.1643.008.html.(CHEN J,SHAO Z Q,ZHANG H H,et al.Short text sentiment analysis based on parallel hybrid neural network model[J/OL].Journal of Computer Applications,2019.[2018-12-10].http://kns.cnki.net/kcms/detail/51.1307.TP.20190329.1643.008.html.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201908004&amp;v=MjYyMzZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVUwzSUx6N0JkN0c0SDlqTXA0OUY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                        陈洁,邵志清,张欢欢,等.基于并行混合神经网络模型的短文本情感分析[J/OL].计算机应用,2019.[2018-12-10].http://kns.cnki.net/kcms/detail/51.1307.TP.20190329.1643.008.html.(CHEN J,SHAO Z Q,ZHANG H H,et al.Short text sentiment analysis based on parallel hybrid neural network model[J/OL].Journal of Computer Applications,2019.[2018-12-10].http://kns.cnki.net/kcms/detail/51.1307.TP.20190329.1643.008.html.)
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_24" title="常丹,王玉珍.基于SVM的用户评论情感分析方法研究[J].枣庄学院学报,2019,36(2):73-78.(CHANG D,WANG YZ.Research on the method of user comment sentiment analysis based on SVM[J].Journal of Zaozhuang University,2019,36(2):73-78.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZZSZ201902012&amp;v=MTkzMzJyWTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFVMM0lQemZZZExHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                        常丹,王玉珍.基于SVM的用户评论情感分析方法研究[J].枣庄学院学报,2019,36(2):73-78.(CHANG D,WANG YZ.Research on the method of user comment sentiment analysis based on SVM[J].Journal of Zaozhuang University,2019,36(2):73-78.)
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_25" title="王煜涵,张春云,赵宝林,等.卷积神经网络下的Twitter文本情感分析[J].数据采集与处理,2018,33(5):921-927.(WANG Y H,ZHANG C Y,ZHAO B L,et al.Sentiment analysis of twitter data based on CNN[J].Journal of Data Acquisition and Processing,2018,33(5):921-927.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJCJ201805017&amp;v=MTkzNjFyQ1VSN3FmWnVac0Z5bmhVTDNJTmlmSVpMRzRIOW5NcW85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                        王煜涵,张春云,赵宝林,等.卷积神经网络下的Twitter文本情感分析[J].数据采集与处理,2018,33(5):921-927.(WANG Y H,ZHANG C Y,ZHAO B L,et al.Sentiment analysis of twitter data based on CNN[J].Journal of Data Acquisition and Processing,2018,33(5):921-927.)
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-03 15:05</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2841-2846 DOI:10.11772/j.issn.1001-9081.2019030579            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>CNN-BiGRU网络中引入注意力机制的中文文本情感分析</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">王丽亚</a>
                                <a href="javascript:;">刘昌辉</a>
                                <a href="javascript:;">蔡敦波</a>
                                <a href="javascript:;">卢涛</a>
                </h2>
                    <h2>

                    <span>武汉工程大学计算机科学与工程学院</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统卷积神经网络(CNN)中同层神经元之间信息不能互传,无法充分利用同一层次上的特征信息,缺乏句子体系特征的表示,从而限制了模型的特征学习能力,影响文本分类效果。针对这个问题,提出基于CNN-BiGRU联合网络引入注意力机制的模型,采用CNN-BiGRU联合网络进行特征学习。首先利用CNN提取深层次短语特征,然后利用双向门限循环神经网络(BiGRU)进行序列化信息学习以得到句子体系的特征和加强CNN池化层特征的联系,最后通过增加注意力机制对隐藏状态加权计算以完成有效特征筛选。在数据集上进行的多组对比实验结果表明,该方法取得了91.93%的F1值,有效地提高了文本分类的准确率,时间代价小,具有很好的应用能力。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E5%90%91%E9%97%A8%E9%99%90%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双向门限循环神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中文文本情感分析;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王丽亚(1994—),女,安徽安庆人,硕士研究生,主要研究方向:文本挖掘;电子邮箱lia.w@qq.com;
                                </span>
                                <span>
                                    刘昌辉(1965—),男,湖北武汉人,教授,博士,主要研究方向:机器人控制、软件系统、智能计算、信息处理;;
                                </span>
                                <span>
                                    蔡敦波(1981—),男,湖北武汉人,教授,博士,CCF会员,主要研究方向:人工智能的智能规划、自动推理、约束优化、文本挖掘;;
                                </span>
                                <span>
                                    卢涛(1980—),男,湖北武汉人,教授,博士,CCF会员,主要研究方向:模式识别、计算机视觉。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-09</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61103136,61502354);</span>
                                <span>武汉工程大学教育创新计划项目(CX2018196);</span>
                    </p>
            </div>
                    <h1><b>Chinese text sentiment analysis based on CNN-BiGRU network with attention mechanism</b></h1>
                    <h2>
                    <span>WANG Liya</span>
                    <span>LIU Changhui</span>
                    <span>CAI Dunbo</span>
                    <span>LU Tao</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Engineering, Wuhan Institute of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the traditional Convolutional Neural Network(CNN), the information cannot be transmitted to each other between the neurons of the same layer, the feature information at the same layer cannot be fully utilized, making the lack of the representation of the characteristics of the sentence system. As the result, the feature learning ability of model is limited and the text classification effect is influenced. Aiming at the problem, a model based on joint network CNN-BiGRU and attention mechanism was proposed. In the model, the CNN-BiGRU joint network was used for feature learning. Firstly, deep-level phrase features were extracted by CNN. Then, the Bidirectional Gated Recurrent Unit(BiGRU) was used for the serialized information learning to obtain the characteristics of the sentence system and strengthen the association of CNN pooling layer features. Finally, the effective feature filtering was completed by adding attention mechanism to the hidden state weighted calculation. Comparative experiments show that the method achieves 91.93% F1 value and effectively improves the accuracy of text classification with small time cost and good application ability.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Bidirectional%20Gated%20Recurrent%20Unit(BiGRU)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Bidirectional Gated Recurrent Unit(BiGRU);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Chinese%20text%20sentiment%20analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Chinese text sentiment analysis;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Liya,born in 1994,M.S.candidate.Her research interests include text mining.;
                                </span>
                                <span>
                                    LIU Changhui,born in 1965,Ph.D.,professor.His research interests include robot control,software systems,intelligent computing,information processing.;
                                </span>
                                <span>
                                    CAI Dunbo,born in 1981,Ph.D.,professor.His research interests include intelligent planning of artificial intelligence,automatic reasoning,constraint optimization,text mining.;
                                </span>
                                <span>
                                    LU Tao,born in 1980,Ph.D.,professor.His research interests include pattern recognition,computer vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-09</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61103136,61502354);</span>
                                <span>the Wuhan Institute of Technology Education Innovation Program Funding Project(CX2018196);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="54">随着互联网的飞速发展,神经网络迎来梦寐以求的大数据时代,其中,文本情感分析<citation id="274" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是一个重要的研究方向,能有效地分析文本所包含的情感信息,同时具有很高的商业价值。</p>
                </div>
                <div class="p1">
                    <p id="55">文本情感分析是自然语言处理(Natural Language Processing, NLP)领域的一个重要研究方向,主要目的是从原始文本中提取出评论人的主观情感,即对某个对象是积极还是消极的态度。主要分析方法可分为三类:基于有监督的学习、基于语言学和基于深度学习的方法<citation id="280" type="reference"><link href="226" rel="bibliography" /><link href="228" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。Pang等<citation id="275" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>针对电影评论,通过词袋模型分别加上贝叶斯、最大熵、支持向量机等各种分类器取得较好的分类效果。Bengio等<citation id="276" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>最早使用神经网络构建语言模型。Mikolov等<citation id="281" type="reference"><link href="234" rel="bibliography" /><link href="236" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>2013年在Log-Bilinear模型<citation id="277" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的基础上提出了word2vec技术。Kalchbrenner等<citation id="278" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出动态卷积神经网络(Dynamic Convolutional Neural Network, DCNN)的模型处理长度不同的文本,将卷积神经网络(Convolutional Neural Network, CNN)应用于NLP。Kim<citation id="279" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>对比了不同词向量构造方法,利用提前训练的词向量作为输入,通过CNN实现句子级的文本分类。但传统CNN无法深度学习池化后的特征,本文采用CNN-BiGRU联合网络进行特征学习。</p>
                </div>
                <div class="p1">
                    <p id="56">Lee等<citation id="282" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>利用基于循环神经网络(Recurrent Neural Network, RNN)和CNN训练文本的向量,通过普通人工神经网络(Artificial Neural Network, ANN)实现文本分类,说明了添加文本序列信息可以提高分类的准确率。普通RNN可以有效地利用近距离的语义特征<citation id="287" type="reference"><link href="246" rel="bibliography" /><link href="248" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>,但存在梯度消失的不足,为解决这一问题,RNN出现了多个变种循环神经网络模型。长短时记忆网络(Long Short-Term Memory,LSTM)<citation id="283" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,在文本上可以提取长距离的语义特征。Zhou等<citation id="284" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出C-LSTM进行文本分类,但忽略了文本上下文的特征联系。双向长短时记忆网络(Bidirectional LSTM, BiLSTM)<citation id="285" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>则由前向传播和后向传播的两个LSTM组合而成,提取全局特征弥补了LSTM的不足;但网络结构复杂,所需参数多,时间代价大。Cho等<citation id="286" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了门限循环单元(Gated Recurrent Unit,GRU),GRU网络结构相对LSTM更加简单,能有效缩短模型训练时间。双向门限循环神经网络(Bidirectional Gated Recurrent Unit, BiGRU)则是由正向GRU、反向GRU、正反向GRU的输出状态连接层组成的神经网络。综上,BiGRU相对于BiLSTM的网络结构较为简单,时间代价小。因此,本文采用BiGRU学习CNN池化后的特征得到句子体系的特征表示。</p>
                </div>
                <div class="p1">
                    <p id="57">Bahdanau等<citation id="288" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>最早提出了注意力机制理论,并将其应用到机器翻译领域。Mnih等<citation id="289" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>在RNN模型上使用了注意力机制来进行图像分类,使得注意力机制真正意义上流行了起来。Xu等<citation id="290" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>展示了如何使用学习得到的注意力机制为模型生成过程提供更多可解释性;Luong等<citation id="291" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出了全局、局部两种注意力机制,促进了基于注意力机制的模型在NLP的应用。胡荣磊等<citation id="292" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>将LSTM和前馈注意力模型相结合,提出了一种文本情感分析方案。王伟等<citation id="293" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>提出BiGRU-Attention模型进行情感分类。陈洁等<citation id="294" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>提出了基于并行混合神经网络模型的短文本情感分析方法,以上结合注意力机制进行情感分析的模型,皆说明了通过引入注意力机制能有效地提高了情感分类的准确度。</p>
                </div>
                <div class="p1">
                    <p id="58">由于利用传统CNN或BiGRU网络进行特征学习时,CNN在局部特征提取上较BiGRU具有优势,但缺乏句子体系的特征表示,而一个模型对特征的学习能力会直接影响文本分类效果,所用本文提出联合网络引入注意力机制的模型进行情感分析。</p>
                </div>
                <div class="p1">
                    <p id="59">本文的主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="60">1)针对中文文本,提出了CNN-BiGRU联合网络模型学习文本特征,充分利用CNN局部特征的强学习能力,使用BiGRU网络获取此层次前后向特征,提取句子体系的特征表示,从而提高文本情感分析的准确率,并在时间代价上验证了此网络的有效性。</p>
                </div>
                <div class="p1">
                    <p id="61">2)在联合网络模型上引入注意力模型,获取文本中的重点特征,降低噪声特征的干扰,从而进一步提高文本情感分析的准确率。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag">1 词向量</h3>
                <div class="p1">
                    <p id="63">本文实验采用谷歌开源工具word2vec来构建文本词向量。word2vec技术包括两种模型:连续词袋模型(CBOW)和Skip-gram模型。CBOW模型根据上下文预测一个词,而Skip-gram模型正好相反,是利用当前词来预测周围的词。本文使用Skip-gram模型。</p>
                </div>
                <div class="p1">
                    <p id="64">Skip-gram模型分为输入层、投影层和输出层,模型结构如图1所示。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Skip-gram模型" src="Detail/GetImg?filename=images/JSJY201910009_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Skip-gram模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Skip-gram model</p>

                </div>
                <div class="p1">
                    <p id="66">如果上下文窗口大小设置为4,且设当前词<i><b>W</b></i>(<i>t</i>)所对应的向量形式为<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>)),其周围4个词所对应的向量形式为<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>+2))、<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>+1))、<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>-1))、<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>-2)),则Skip-gram模型通过中间词预测周围词,是利用中间词向量<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>))的条件概率值来求解,如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>P</i>(<i><b>V</b></i>(<i><b>W</b></i>(<i>i</i>))|<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>)))      (1)</p>
                </div>
                <div class="p1">
                    <p id="68">其中:<i><b>V</b></i>(<i><b>W</b></i>(<i>i</i>))∈{<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>+2)),<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>+1)),<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>-1)),<i><b>V</b></i>(<i><b>W</b></i>(<i>t</i>-2))}。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 联合网络引入注意力机制模型</h3>
                <div class="p1">
                    <p id="70">本文提出了<i>CNN</i>-<i>BiGRU</i>联合网络引入注意力机制(<i>C</i>-<i>BG</i>-<i>A</i>)的文本情感分析方法。结构主要可分为<i>CNN</i>-<i>BiGRU</i>联合网络特征学习和<i>CNN</i>-<i>BiGRU</i>-<i>Attention</i>特征筛选。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">2.1 <i>CNN</i>-<i>BiGRU</i><b>联合网络特征学习</b></h4>
                <div class="p1">
                    <p id="72">本文利用<i>CNN</i>提取局部短语特征。<i>CNN</i>是一种前馈神经网络,模型结构主要包括输入层、卷积层、池化层、全连接层和输出层五部分,<i>CNN</i>的网络结构如图2所示。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">1)输入层。</h4>
                <div class="p1">
                    <p id="74">将嵌入层的输出作为输入,句子中每个词的词向量为<i><b>x</b></i><sub><i>i</i></sub>∈<b>R</b><sup><i>n</i></sup><sup>×</sup><sup><i>d</i></sup>,其中<i>n</i>是词数,<i>d</i>是向量维度,本文定为100维。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75">2)卷积层。</h4>
                <div class="p1">
                    <p id="76">通过设置好大小的滤波器来完成对输入文本句子特征的提取,如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="77"><i>c</i><sub><i>i</i></sub>= <i>f</i>(<i>ω</i>·<i><b>x</b></i><sub><i>i</i></sub><sub>:</sub><sub><i>i</i></sub><sub>+</sub><sub><i>h</i></sub><sub>-1</sub>+<i><b>b</b></i>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="78">其中:ω是卷积核;h是卷积核的尺寸;x<sub>i:i+h-1</sub>是i到i+h-1个词组成的句子向量;b是偏置项;通过卷积层后，得到特征矩阵c=[c<sub>1</sub>,c<sub>2</sub>，…，c<sub>n-h+1</sub>]。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">3)池化层。</h4>
                <div class="p1">
                    <p id="80">通过对卷积层之后得到的句子局部特征矩阵<i><b>c</b></i>进行下采样,求得局部值的最优解<i><b>M</b></i><sub><i>i</i></sub>。这里采用MaxPooling技术,如式(3)所示:</p>
                </div>
                <div class="p1">
                    <p id="81"><i><b>M</b></i><sub><i>i</i></sub>=max(<i>c</i><sub>1</sub>,<i>c</i><sub>2</sub>,…,<i>c</i><sub><i>n</i></sub><sub>-</sub><sub><i>h</i></sub><sub>+1</sub>)=max{<i><b>c</b></i>}      (3)</p>
                </div>
                <div class="p1">
                    <p id="82">由于BiGRU输入必须是序列化结构,池化将中断序列结构<i><b>c</b></i>,所以需要添加全连接层,将池化层后的向量<i><b>M</b></i><sub><i>i</i></sub>连接成特征矩阵<i><b>U</b></i>,如式(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="83"><i><b>U</b></i>=<i><b>M</b></i><sub>1</sub>,<i><b>M</b></i><sub>2</sub>,…,<i><b>M</b></i><sub><i>n</i></sub>      (4)</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CNN模型的结构" src="Detail/GetImg?filename=images/JSJY201910009_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CNN模型的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Structure of CNN model</p>

                </div>
                <div class="p1">
                    <p id="85">将新的连续高阶窗口<i><b>U</b></i>作为BiGRU的输入。BiGRU由正向GRU、反向GRU、正反向GRU的输出状态连接层组成,网络结构如图3所示。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 BiGRU模型的结构" src="Detail/GetImg?filename=images/JSJY201910009_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 BiGRU模型的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Structure of BiGRU model</p>

                </div>
                <div class="p1">
                    <p id="87">若记<i>t</i>时刻正向GRU输出的隐藏状态为<mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>→</mo></mover></math></mathml><sub><i>t</i></sub>,反向GRU输出的隐藏状态为<mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover></math></mathml><sub><i>t</i></sub>,则BiGRU输出的隐藏状态<i><b>h</b></i><sub><i>t</i></sub>,其具体的计算过程如式(5)～(7)所示:</p>
                </div>
                <div class="area_img" id="89">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201910009_08900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="90">其中:<i><b>w</b></i><sub><i>t</i></sub>、<i><b>v</b></i><sub><i>t</i></sub>是权值矩阵;<i><b>U</b></i><sub><i>t</i></sub>为<i>t</i>时刻的GRU输入;<i><b>b</b></i><sub><i>t</i></sub>为偏置向量。</p>
                </div>
                <div class="p1">
                    <p id="91">GRU是对LSTM的一种改进,它也存在一个贯穿始终的记忆状态单元(Memory Unit),该记忆单元用更新门代替原来LSTM中的遗忘门和输入门,即在网络结构上要比LSTM更为简单,且所需参数减少,从而能够提高模型训练速度。GRU的原理如图4所示。其具体的计算过程如式(8)～(11)所示:</p>
                </div>
                <div class="p1">
                    <p id="92"><i>z</i><sub><i>t</i></sub>=<i>σ</i>(<i><b>w</b></i><sub><i>z</i></sub>·<i><b>h</b></i><sub><i>t</i></sub><sub>-1</sub>,<i><b>U</b></i><sub><i>t</i></sub>)      (8)</p>
                </div>
                <div class="p1">
                    <p id="93"><i>r</i><sub><i>t</i></sub>=<i>σ</i>(<i><b>w</b></i><sub><i>r</i></sub>·<i><b>h</b></i><sub><i>t</i></sub><sub>-1</sub>,<i><b>U</b></i><sub><i>t</i></sub>)      (9)</p>
                </div>
                <div class="p1">
                    <p id="94"><mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi></mrow></math></mathml>·<i><b>h</b></i><sub><i>t</i></sub><sub>-1</sub>,<i><b>U</b></i><sub><i>t</i></sub>)      (10)</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>*</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>z</mi><msub><mrow></mrow><mi>t</mi></msub><mo>*</mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中:<i><b>w</b></i><sub><i>z</i></sub>、<i><b>w</b></i><sub><i>r</i></sub>、<i><b>w</b></i>为权值矩阵;<i>z</i><sub><i>t</i></sub>为更新门;<i>r</i><sub><i>t</i></sub>为重置门;<mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>为备选激活函数;<i><b>h</b></i><sub><i>t</i></sub>为激活函数;<i><b>U</b></i><sub><i>t</i></sub>为<i>t</i>时刻GRU的输入;<i>σ</i>为sigmoid激活函数。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 GRU模型" src="Detail/GetImg?filename=images/JSJY201910009_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 GRU模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 GRU model</p>

                </div>
                <h4 class="anchor-tag" id="98" name="98">2.2 CNN-BiGRU-Attention<b>特征筛选</b></h4>
                <div class="p1">
                    <p id="99">注意力机制通过对文本向量的语义编码分配不同的注意力权重,以区分文本中信息的重要性大小,提高分类的准确率。本文使用前馈注意力模型,注意力模型结构如图5所示。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 注意力模型结构" src="Detail/GetImg?filename=images/JSJY201910009_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 注意力模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.5 <i>Structure of Attention model</i></p>

                </div>
                <div class="p1">
                    <p id="101">1)生成目标注意力权重<i><b>v</b></i><sub><i>t</i></sub>,如式(12)所示:</p>
                </div>
                <div class="p1">
                    <p id="102"><i><b>v</b></i><sub><i>t</i></sub>=<i>σ</i>(<i><b>h</b></i><sub><i>t</i></sub>)      (12)</p>
                </div>
                <div class="p1">
                    <p id="103">其中:<i>σ</i>是一种注意力学习函数tanh;<i><b>h</b></i><sub><i>t</i></sub>是CNN-BiGRU网络输出的特征向量。</p>
                </div>
                <div class="p1">
                    <p id="104">2)注意力权重概率化,通过softmax函数生成概率向量<i><b>p</b></i><sub><i>t</i></sub>,如式(13)所示:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">3)注意力权重配置,将生成的注意力权重配置给对应的隐层状态语义编码<i><b>h</b></i><sub><i>t</i></sub>,使模型生成的注意力权重发挥作用,<i>α</i><sub><i>t</i></sub>是<i><b>h</b></i><sub><i>t</i></sub>的加权平均值,权值是<i><b>p</b></i><sub><i>t</i></sub>,如式(14)所示:</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi mathvariant="bold-italic">p</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">2.3 C-BG-A<b>模型</b></h4>
                <div class="p1">
                    <p id="109"><i>C</i>-<i>BG</i>-<i>A</i>模型网络结构如图6所示。主要包括三个部分:一是<i>CNN</i>+<i>BiGRU</i>的网络构建;二是引入注意力模型;三是用<i>sigmoid</i>分类器进行分类。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 C-BG-A模型" src="Detail/GetImg?filename=images/JSJY201910009_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 <i>C</i>-<i>BG</i>-<i>A</i>模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.6 <i>C</i>-<i>BG</i>-<i>A model</i></p>

                </div>
                <div class="p1">
                    <p id="111">下面针对<i>C</i>-<i>BG</i>-<i>A</i>模型进行详细介绍:</p>
                </div>
                <div class="p1">
                    <p id="112">1)将文本输入到<i>word</i>2<i>vec</i>模型中训练,句子中每个词的词向量为<i><b>x</b></i><sub><i>i</i></sub>∈<b>R</b><sup><i>n</i></sup><sup>×</sup><sup><i>d</i></sup>,其中<i>n</i>是词数,<i>d</i>是向量维度,则句子矩阵<i><b>S</b></i>可表示为<i><b>S</b></i>=(<i><b>x</b></i><sub>1</sub>,<i><b>x</b></i><sub>2</sub>,…,<i><b>x</b></i><sub><i>n</i></sub>)。</p>
                </div>
                <div class="p1">
                    <p id="113">2)添加CNN层,利用式(2)～(3)得到局部短语特征矩阵<i><b>M</b></i><sub><i>i</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="114">3)添加全连接层,利用式(4)将局部短语特征<i><b>M</b></i><sub><i>i</i></sub>拼接成序列结构<i><b>U</b></i>。</p>
                </div>
                <div class="p1">
                    <p id="115">4)添加BiGRU层,利用式(5)～(11)学习序列<i><b>U</b></i>得到句子特征表示<i><b>H</b></i><sub><i><b>c</b></i></sub><sub><i>t</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="116">5)引入注意力机制,利用式(12)～(14)得到<i><b>H</b></i><sub><i><b>c</b></i></sub><sub><i>t</i></sub>的加权平均值<i><b>A</b></i><sub><i><b>c</b></i></sub><sub><i>t</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="117">6)经过dropout层后,得到特征向量<i><b>A</b></i>。</p>
                </div>
                <div class="p1">
                    <p id="118">7)添加Dense层,参数为1,激活函数为sigmoid函数,对文本特征进行分类:</p>
                </div>
                <div class="p1">
                    <p id="119"><i><b>p</b></i>(<i>y</i>|<i><b>A</b></i>,<i><b>w</b></i>,<i><b>b</b></i>)=sigmoid(<i><b>w</b></i>·<i><b>A</b></i>+<i><b>b</b></i>)</p>
                </div>
                <div class="p1">
                    <p id="120">8)模型损失函数定为对数损失函数,模型会通过更新参数权值矩阵<i><b>w</b></i>和偏置向量<i><b>b</b></i>,从而达到优化模型的效果。</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121">2.4 <b>模型训练</b></h4>
                <div class="p1">
                    <p id="122">本文将情感分析问题看成一个二分类的问题,分类函数选为sigmoid函数,如式(15)所示:</p>
                </div>
                <div class="p1">
                    <p id="123"><i>p</i>(<i>y</i>=1|<i><b>x</b></i>,<i>ω</i>)=<i>h</i><sub><i>ω</i></sub>(<i><b>x</b></i>)=<i>g</i>(<i>ω</i><sup>T</sup><i><b>x</b></i>)=</p>
                </div>
                <div class="p1">
                    <p id="124">1/(1+exp(-<i>ω</i><sup>T</sup><i><b>x</b></i>))      (15)</p>
                </div>
                <div class="p1">
                    <p id="125">其中:样本是{<i><b>x</b></i>, <i>y</i>}, <i>y</i>是消极0或者积极1,<i><b>x</b></i>是样本特征向量;<i>ω</i>代表可训练参数。</p>
                </div>
                <div class="p1">
                    <p id="126">模型训练的目标实质就是最小化损失函数。本文在编译模型时,损失函数为对数损失函数,一般是与sigmoid相对应的损失函数。训练模型参数<i>ω</i>如式(16)所示:</p>
                </div>
                <div class="p1">
                    <p id="127" class="code-formula">
                        <mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ω</mi><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mi>h</mi><msub><mrow></mrow><mi mathvariant="bold-italic">ω</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>h</mi><msub><mrow></mrow><mi mathvariant="bold-italic">ω</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="128">其中:<i>y</i><sub><i>i</i></sub>为输入<i><b>x</b></i><sub><i>i</i></sub>的真实类别,<i>h</i><sub><i>ω</i></sub>(<i><b>x</b></i><sub><i>i</i></sub>)为预测输入<i><b>x</b></i><sub><i>i</i></sub>属于类别1的概率。</p>
                </div>
                <div class="p1">
                    <p id="129">模型的优化器选为Adam(Adaptive moment estimation),Adam优化算法是一种计算每个参数的自适应学习率的方法,结合了AdaGrad和RMSProp两种优化算法的优点。参数的更新不受梯度的伸缩变换影响,变化平稳,如式(17)～(21)所示:</p>
                </div>
                <div class="p1">
                    <p id="130"><i>m</i><sub><i>t</i></sub>= <i>β</i><sub>1</sub><i>m</i><sub><i>t</i></sub><sub>-1</sub>+(1-<i>β</i><sub>1</sub>)<i>g</i><sub><i>t</i></sub>      (17)</p>
                </div>
                <div class="p1">
                    <p id="131"><i><b>v</b></i><sub><i>t</i></sub>= <i>β</i><sub>2</sub><i><b>v</b></i><sub><i>t</i></sub><sub>-1</sub>+(1-<i>β</i><sub>2</sub>)<i>g</i><mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mn>2</mn></msubsup></mrow></math></mathml>      (18)</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>m</mi><msub><mrow></mrow><mi>t</mi></msub><mo>/</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>t</mi></msub><mo>/</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><mi>α</mi><mo>*</mo><mover accent="true"><mi>m</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>/</mo><mo stretchy="false">(</mo><msqrt><mrow><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></msqrt><mo>+</mo><mi>ε</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">其中:<i>α</i>=0.001; <i>β</i><sub>1</sub>=0.9; <i>β</i><sub>2</sub>=0.999;<i>ε</i>=1E-8;<i>g</i><sub><i>t</i></sub>代表时间步<i>t</i>的梯度,<i>g</i><sub><i>t</i></sub>=ᐁ<sub><i>θ</i></sub><i>J</i>(<i>θ</i><sub><i>t</i></sub><sub>-1</sub>)。</p>
                </div>
                <div class="p1">
                    <p id="134">评价函数为accuracy函数,用于评估当前训练模型的性能。</p>
                </div>
                <h3 id="135" name="135" class="anchor-tag">3 实验与分析</h3>
                <div class="p1">
                    <p id="136">在带有情感标签的中文购物评论文本上,对提出的<i>C</i>-<i>BG</i>-<i>A</i>模型情感分析方法进行验证与分析。实验环境配置数据如表1所示。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表</b>1 <b>实验环境配置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.1 <i>Experimental environment configuration</i></p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />实验环境</td><td>配置数据</td></tr><tr><td><br />    语言</td><td><i>Python</i>3.6</td></tr><tr><td><br />    工具</td><td><i>Jupyter notebook</i></td></tr><tr><td><br />    框架</td><td><i>Keras</i>2.1.5</td></tr><tr><td><br />    处理器</td><td><i>CPU</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="138" name="138">3.1 <b>实验环境与数据</b></h4>
                <div class="p1">
                    <p id="139">实验数据来自网络购物的评论,数据内容对象类型有酒店、牛奶、书籍、手机等。评论情感标签分为两类[0,1],消极的情感为0,积极的情感为1。例如:“地理位置优越,交通方便,饭店环境好,服务优良,洗衣迅捷”,情感为积极,“手机系统差,容易坏.部分按键不灵活,半年后就会出现在大问题.”,情感为消极。数据集设置为:总数21 105条,训练集16 884条,验证集2 000条,测试集2 221条。</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140">3.2 <b>参数设置</b></h4>
                <div class="p1">
                    <p id="141">参数设置会直接影响后续模型的分类效果,具体的参数设置如表2所示。</p>
                </div>
                <div class="area_img" id="142">
                    <p class="img_tit"><b>表</b>2 <b>参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.2 <i>Parameter settings</i></p>
                    <p class="img_note"></p>
                    <table id="142" border="1"><tr><td><br />参数</td><td>值</td><td rowspan="7"></td><td><br />参数</td><td>值</td></tr><tr><td><br /><i>Vec</i>_<i>window</i></td><td>7</td><td><br /><i>BiGRU</i>_<i>layers</i></td><td>2</td></tr><tr><td><br /><i>Vec</i>_<i>dim</i></td><td>100</td><td><br /><i>BiGRU</i>_<i>dim</i></td><td>100</td></tr><tr><td><br /><i>Cnn</i>_<i>win</i></td><td>3,4,5</td><td><br /><i>Dropout</i></td><td>0.5</td></tr><tr><td><br /><i>Cnn</i>_<i>win</i>_<i>num</i></td><td>128</td><td><br /><i>lr</i></td><td>0.001</td></tr><tr><td><br /><i>Cnn</i>_<i>activation</i></td><td><i>relu</i></td><td><br /><i>Bach</i>_<i>size</i></td><td>32</td></tr><tr><td><br /><i>VPool</i>_<i>size</i></td><td>4</td><td><br /><i>epochs</i></td><td>10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="143" name="143">3.3 <b>评价指标</b></h4>
                <div class="p1">
                    <p id="144">准确度(<i>Accuracy</i>)、精确度(<i>Precision</i>)、召回率(<i>Recall</i>)、<i>F</i>值(<i>F</i><sub><i>θ</i></sub>)这4个模型评测指标是NLP模型评估的常用标准。</p>
                </div>
                <div class="p1">
                    <p id="145">设总的测试集个数为<i>TP</i>+<i>TN</i>+<i>FP</i>+<i>FN</i>,其具体含义如表3所示。</p>
                </div>
                <div class="area_img" id="146">
                    <p class="img_tit"><b>表</b>3 <b>评价指标相关参数含义</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.3 Meanings of relevant parameters of evaluation indicators</p>
                    <p class="img_note"></p>
                    <table id="146" border="1"><tr><td rowspan="2"><br />实际值</td><td colspan="2"><br />预测值</td></tr><tr><td><br />Positive</td><td>Negative</td></tr><tr><td><br />  Positive</td><td><i>TP</i></td><td><i>FN</i></td></tr><tr><td><br />  Negative</td><td><i>FP</i></td><td><i>TN</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="147"><i>Accuracy</i>是对模型正确分类能力的评估,准确度越高,分类能力越好,如式(22)所示:</p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149"><i>Precision</i>的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="150" class="code-formula">
                        <mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="151"><i>Recall</i>是对查全率的评估,如式(24)所示:</p>
                </div>
                <div class="p1">
                    <p id="152" class="code-formula">
                        <mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="153"><i>F</i>1值是综合评价指标,如式(25)所示:</p>
                </div>
                <div class="p1">
                    <p id="154" class="code-formula">
                        <mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mo>*</mo><mo stretchy="false">(</mo><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo stretchy="false">)</mo></mrow><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">3.4 <b>对比实验设置</b></h4>
                <div class="p1">
                    <p id="156">实验设置以下多组对比实验,包括传统机器学习算法支持向量机(<i>Support Vector Machine</i>,<i>SVM</i>)与深度学习网络的比较,单一网络与联合网络的比较,及与引入注意力模型的网络对比。网络输入均是利用<i>word</i>2<i>vec</i>训练的词向量。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157">1)<i>SVM</i><citation id="295" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>:</h4>
                <div class="p1">
                    <p id="158">采用传统机器学习算法<i>SVM</i>进行情感分析。</p>
                </div>
                <h4 class="anchor-tag" id="159" name="159">2)<i>BiLSTM</i><citation id="296" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>:</h4>
                <div class="p1">
                    <p id="160">单一的<i>BiLSTM</i>网络。</p>
                </div>
                <h4 class="anchor-tag" id="161" name="161">3)<i>BiGRU</i>:</h4>
                <div class="p1">
                    <p id="162">单一的<i>BiGRU</i>网络。</p>
                </div>
                <h4 class="anchor-tag" id="163" name="163">4)<i>CNN</i><citation id="297" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>:</h4>
                <div class="p1">
                    <p id="164">单一的<i>CNN</i>网络。</p>
                </div>
                <h4 class="anchor-tag" id="165" name="165">5)<i>CNN</i>+<i>BiLSTM</i>:</h4>
                <div class="p1">
                    <p id="166">先添加一个<i>CNN</i>网络,再添加一个<i>BiLSTM</i>网络。</p>
                </div>
                <h4 class="anchor-tag" id="167" name="167">6)<i>CNN</i>+<i>BiGRU</i>:</h4>
                <div class="p1">
                    <p id="168">先添加一个<i>CNN</i>网络,再添加一个<i>BiGRU</i>网络。</p>
                </div>
                <h4 class="anchor-tag" id="169" name="169">7)<i>CNN</i>+<i>BiLSTM</i>+<i>Attention</i>(<i>CNN</i>+<i>BiLSTM</i>+<i>Att</i>):</h4>
                <div class="p1">
                    <p id="170">先添加一个<i>CNN</i>网络,再添加一个<i>BiLSTM</i>网络,最后引入<i>Attention</i>机制。</p>
                </div>
                <h4 class="anchor-tag" id="171" name="171">8)<i>CNN</i>+<i>BiGRU</i>+<i>Attention</i>(<i>C</i>-<i>BG</i>-<i>A</i>):</h4>
                <div class="p1">
                    <p id="172">先添加一个<i>CNN</i>网络,再添加一个<i>BiGRU</i>网络,最后引入<i>Attention</i>机制。</p>
                </div>
                <h4 class="anchor-tag" id="173" name="173">3.5 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="174">实验在测试集上计算出<i>Accuracy</i>值、<i>Precision</i>值、<i>Recall</i>值、<i>F</i>1值,对比结果如表4所示。</p>
                </div>
                <div class="area_img" id="175">
                    <p class="img_tit"><b>表</b>4 <b>模型对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.4 Comparison results of models</p>
                    <p class="img_note"></p>
                    <table id="175" border="1"><tr><td><br />模型</td><td><i>Accuracy</i></td><td><i>Precision</i></td><td><i>Recall</i></td><td><i>F</i>1</td></tr><tr><td><br />SVM</td><td>0.792 4</td><td>0.847 6</td><td>0.726 6</td><td>0.782 4</td></tr><tr><td><br />BiLSTM</td><td>0.876 2</td><td>0.872 7</td><td>0.875 1</td><td>0.873 9</td></tr><tr><td><br />BiGRU</td><td>0.881 1</td><td>0.873 3</td><td>0.886 1</td><td>0.879 7</td></tr><tr><td><br />CNN</td><td>0.887 0</td><td>0.882 3</td><td>0.888 0</td><td>0.885 1</td></tr><tr><td><br />CNN+BiLSTM</td><td>0.882 5</td><td><b>0.936</b><b>7</b></td><td>0.815 4</td><td>0.871 9</td></tr><tr><td><br />CNN+BiGRU</td><td>0.907 2</td><td>0.924 1</td><td>0.883 4</td><td>0.903 3</td></tr><tr><td><br />CNN+BiLSTM+Att</td><td>0.917 6</td><td>0.915 6</td><td>0.916 4</td><td>0.916 0</td></tr><tr><td><br />C-BG-A</td><td><b>0.920</b><b>3</b></td><td>0.913 0</td><td><b>0.925</b><b>6</b></td><td><b>0.919</b><b>3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="176">从表4显示的8组模型对比结果可看出:从两个综合评测指标<i>Accuracy</i>、<i>F</i>1上来看,C-BG-A准确率达到92.03%,<i>F</i>1值达到91.93%,均优于对比模型。SVM虽然取得较好的分类效果,但基于深度学习的7组模型明显优于SVM。第5、6与之前3组模型的对比,体现出本文提出的CNN-BiGRU联合网络提取特征的优势,因为CNN对文本特征的强学习能力有助于联合模型对深层次特征的学习,而双向循环神经网络对序列化特征前后的学习能力,在联合模型对CNN提取的特征进行再加工时,起到正影响的作用。第7、8与5、6组的对比,可知在联合模型的基础上添加注意力机制能有效地提高模型分类的准确度,因为注意力机制分配给特征不同的权重,让模型学习到了特征之间的轻重不同之分,有助于模型快速掌握重要的特征。第8与7组的比较,说明利用BiGRU学习CNN池化后的特征较BiLSTM效果更佳。</p>
                </div>
                <div class="p1">
                    <p id="177">为了更直观地反映基于深度学习的7组模型的优劣,本文选择绘画验证集的准确率(val_acc)和损失率(val_loss)变化图。准确率的变化如图7所示,损失率的变化如图8所示。</p>
                </div>
                <div class="p1">
                    <p id="178">从图7可看出:总体上7组模型准确度都不断上升,第二次迭代后均达86%以上,其中C-BG-A模型的准确度均达90%以上,且趋势平稳,不如其余6组模型波动性大。可见C-BG-A模型在提取文本特征上更为优秀和稳定,在短时间内准确度可达较高水平且趋势稳定,即在迭代次数较少的情况下也可以达到较高的准确度,在第5次迭代时达到最高值92.93%。</p>
                </div>
                <div class="p1">
                    <p id="179">从图7中也可发现,有BiGRU参与的模型准确度都能快速达到较高水平且波幅较平稳,相对与BiLSTM表现更好。</p>
                </div>
                <div class="p1">
                    <p id="180">模型损失率是越小越好。从图8中可看出:CNN模型的波动最大,其余6组都较为集中;细看可发现,C-BG-A模型的损失率第1次迭代结果就达到0.255 2,第2次迭代结果为最低值0.205 1,且趋势平稳。综合图7～8的分析结果,C-BG-A模型具有收敛性快、准确度高、稳定性强的特点,相比较其他6组模型在文本分类上更具有优势。</p>
                </div>
                <div class="area_img" id="181">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 验证集准确率变化" src="Detail/GetImg?filename=images/JSJY201910009_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 验证集准确率变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 val_acc change map</p>

                </div>
                <div class="area_img" id="182">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 验证集损失率变化" src="Detail/GetImg?filename=images/JSJY201910009_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 验证集损失率变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 val_loss change map</p>

                </div>
                <div class="p1">
                    <p id="183">为了量化模型的优劣,本文在测试集上进行预测,样本总数为2 221。实验统计了预测结果的<i>TP</i>值、<i>FP</i>值、<i>TN</i>值、<i>FN</i>值、<i>Right</i>值、<i>Wrong</i>值,<i>Right</i>为模型预测正确的样本数,<i>Wrong</i>为模型预测错误的样本数。输出结果为预测样本是1概率,为方便统计,将值大于0.5的定为1,其余为0。预测值统计的结果如表5所示。</p>
                </div>
                <div class="area_img" id="184">
                    <p class="img_tit"><b>表</b>5 <b>预测值统计结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.5 Statistical results of predicted values</p>
                    <p class="img_note"></p>
                    <table id="184" border="1"><tr><td><br />模型</td><td><i>TP</i></td><td><i>FP</i></td><td><i>TN</i></td><td><i>FN</i></td><td><i>Right</i></td><td><i>Wrong</i></td></tr><tr><td>SVM</td><td>829</td><td>149</td><td>931</td><td>312</td><td>1 760</td><td>461</td></tr><tr><td><br />BiLSTM</td><td>953</td><td>139</td><td>993</td><td>136</td><td>1 946</td><td>275</td></tr><tr><td><br />BiGRU</td><td>965</td><td>140</td><td>992</td><td>124</td><td>1 957</td><td>264</td></tr><tr><td><br />CNN</td><td>967</td><td>129</td><td>1 003</td><td>122</td><td>1 970</td><td>251</td></tr><tr><td><br />CNN+BiLSTM</td><td>888</td><td><b>60</b></td><td><b>1</b><b>072</b></td><td>201</td><td>1 960</td><td>261</td></tr><tr><td><br />CNN+BiGRU</td><td>962</td><td>79</td><td>1 053</td><td>127</td><td>2 015</td><td>206</td></tr><tr><td><br />CNN+BiLSTM+Att</td><td>998</td><td>92</td><td>1 040</td><td>91</td><td>2 038</td><td>183</td></tr><tr><td><br />C-BG-A</td><td><b>1</b><b>008</b></td><td>96</td><td>1 036</td><td><b>81</b></td><td><b>2</b><b>044</b></td><td><b>177</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="185">表5中,C-BG-A模型预测正确最多,错误最少,可见C-BG-A模型最优。</p>
                </div>
                <div class="p1">
                    <p id="186">针对模型的时间代价分析,本文给出5组对比模型完成每次迭代所需时间的对比图,如图9所示。</p>
                </div>
                <div class="p1">
                    <p id="187">实验尽可能统一其运行条件,在减少因各方面因素不同对其影响的情况下进行统计。从图9模型的时间代价来看,单模型迭代所用时间均为最少,其中,BiGRU所用时间最短,因为其网络结构最为简单。C-BG-A多数保持在117 s/epoch。在时间代价上,C-BG-A比CNN+BiLSTM+Attention更有优势,说明选择BiGRU模型能有效地缩短模型的训练时间,具有高效性。</p>
                </div>
                <div class="area_img" id="188">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910009_188.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 模型的时间代价对比" src="Detail/GetImg?filename=images/JSJY201910009_188.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 模型的时间代价对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910009_188.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Comparison time consumption of models</p>

                </div>
                <div class="p1">
                    <p id="189">利用模型对真实单句进行预测,展示了模型真实的运用功能。选句子时要避免与原数据中的句子相同,否则会影响评估模型性能。为方便了解输出结果含义,将值大于0.5的定为positive,其余定为negative。单句测试结果如表6所示。</p>
                </div>
                <div class="area_img" id="190">
                    <p class="img_tit"><b>表</b>6 <b>单句测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.6 Test results of simple sentences</p>
                    <p class="img_note"></p>
                    <table id="190" border="1"><tr><td><br />预测结果</td><td>句子</td><td>情绪</td></tr><tr><td><br />0.003 520 64</td><td>电池充完了电连手机都打不开.简直烂透了.真是金玉其外,败絮其中!连5号电池都不如</td><td>negative</td></tr><tr><td><br />0.989 450 90</td><td>酒店的环境非常好,价格也便宜,值得推荐</td><td>positive</td></tr><tr><td><br />0.005 012 37</td><td>手机屏幕较差,拍照也很粗糙</td><td>negative</td></tr><tr><td><br />0.904 739 40</td><td>质量不错,是正品,安装师傅也很好</td><td>positive</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="191">从表6单句测试结果上来看,C-BG-A模型对随机所选的单句的预测结果都是正确的,从而更加直观地说明了C-BG-A能在中文文本分类问题上的成功应用。</p>
                </div>
                <div class="p1">
                    <p id="192">综上,针对本文所用数据集,利用CNN-BiGRU联合模型提取特征优于单模型CNN或BiGRU,再引入注意力机制能进一步地提高文本分类的准确率,且模型具有高效性和很好的应用能力。</p>
                </div>
                <h3 id="193" name="193" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="194">本文提出了一种C-BG-A模型的中文文本情感分析方法。利用CNN局部特征的强学习能力,提取短语体系的特征。再利用BiGRU深度学习CNN中池化后连接组成的特征,加强短语特征之间的联系,从而使模型学习到更深层次的句子体系特征表示。最后引入注意力机制进行特征筛选,降低噪声干扰。在中文网络购物评论文本上进行训练和测试,实验结果表明本文模型有效地提高了文本情感分类准确率,且时间代价小。但由于本文模型使用的CNN网络深度不够,如何使用更深的CNN网络作为联合对象进一步提高分类准确率,是下一步工作的目标。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="224">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201112042&amp;v=MTQyMzVCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFVMM0lMejdCZDdHNEg5RE5yWTk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>魏韡,向阳,陈千.中文文本情感分析综述[J].计算机应用,2011,31(12):3321-3323.(WEI W,XIANG Y,CHEN Q.Survey on Chinese text sentiment analysis[J].Journal of Computer Applications,2011,31(12):3321-3323.)
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Thumbs up or thumbs down?Semantic orientation applied to unsupervised classification of reviews">

                                <b>[2]</b>TURNEY P D.Thumbs up or thumbs down?:semantic orientation applied to unsupervised classification of reviews[C]//Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2002:417-424.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploring in the weblog space by detecting informative and affective articles">

                                <b>[3]</b>NI X,XUE G,LING X,et al.Exploring in the weblog space by detecting informative and affective articles[C]//Proceedings of the16th International Conference on World Wide Web.New York:ACM,2007:281-290.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                PANG B,LEE L,VAITHYANATHAN S.Thumbs up?:sentiment classification using machine learning techniques[C]//Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing.Stroudsburg,PA:Association for Computational Linguistics,2002:79-86.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Neural Probabilistic Language Model">

                                <b>[5]</b>BENGIO Y,DUCHARME R,VINCENT P,et al.A neural probabilistic language model[J].Journal of Machine Learning Research,2003,3:1137-1155.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">

                                <b>[6]</b>MIKOLOV T,CHEN K,CORRADO G,et al.Efficient estimation of word representations in vector space[EB/OL].[2017-08-04].http://www.surdeanu.info/mihai/teaching/ista555-spring15/readings/mikolov2013.pdf.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">

                                <b>[7]</b>MIKOLOV T,SUTSKEVER I,CHEN K,et al.Distributed representations of words and phrases and their compositionality[EB/OL].[2019-01-10].http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A scalable hierarchical distributed language model">

                                <b>[8]</b>MNIH A,HINTON G E.A scalable hierarchical distributed language model[C]//Proceedings of the 21st International Conference on Neural Information Processing.New York:Curran Associates Inc.,2008:1081-1088.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Convolutional Neural Network for Modelling Sentences">

                                <b>[9]</b>KALCHBRENNER N,GREFENSTETTE E,BLUNSOM P.A convolutional neural network for modelling sentences[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2014:655-665.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for sentence classification">

                                <b>[10]</b>KIM Y.Convolutional neural networks for sentence classification[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Proceeding.Stroudsburg,PA:Association for Computational Linguistics,2014:1746-1751.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks">

                                <b>[11]</b>LEE J Y,DERNONCOURT F.Sequential short-text classification with recurrent and convolutional neural networks[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2016,515-520.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning phrase representations using RNN encoder-decoder for statistical machine translation">

                                <b>[12]</b>CHO K,van MERRIENBOER B,GULCEHRE C,et al.Learning phrase representions using RNN encoder-decoder for statistical machine translation[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.Stroudsburg,PA:Association for Computational Linguistics,2014:1724-1734.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chain based RNN for relation classification">

                                <b>[13]</b>EBRAHIMI J,DOU D.Chain based RNN for relation classification[C]//Proceedings of the 2015 Annual Conference of the North American Chapter of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2015:1244-1249.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MDAyNDFSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUlWb1hhQkk9TmlmSlpiSzlIdGpNcW85RlpPb0xEWFV4b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A C-LSTM Neural Network for Text Classification">

                                <b>[15]</b>ZHOU C,SUN C,LIU Z,et al.A C-LSTM neural network for text classification[EB/OL].[2019-01-09].https://arxiv.org/abs/1511.08630.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chinese sentiment analysis using bidirectional LSTM with word embedding">

                                <b>[16]</b>XIAO Z,LIANG P.Chinese sentiment analysis using bidirectional LSTM with word embedding[C]//Proceedings of the 2016 International Conference on Cloud Computing and Security,LNSC10040.Berlin:Springer,2016:601-610.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">

                                <b>[17]</b>BAHDANAU D,CHO K,BENGIO Y.Neural machine translation by jointly learning to align and translate[EB/OL].[2018-03-20].https://arxiv.org/pdf/1409.0473v7.pdf.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent models of visual attention">

                                <b>[18]</b>MNIH V,HEESS N,GRAVES A,et al.Recurrent models of visual attention[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:2204-2212.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention">

                                <b>[19]</b>XU K,BA J,KIROS R,et al.Show,attend and tell:neural image caption generation with visual attention[EB/OL].[2018-03-20].https://arxiv.org/pdf/1502.03044.pdf.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_20" >
                                    <b>[20]</b>
                                LUONG M PHAM H,MANNING C D.Effective approaches to attention-based neural machine translation[C]//Proceedings of the2015 Conference on Empirical Methods in Natural Language Processing.Stroudsburg,PA:Association for Computational Linguistics,2015:1412-1421.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                胡荣磊,芮璐,齐筱,等.基于循环神经网络和注意力模型的文本情感分析[J/OL].计算机应用研究,2019,36(11).[2018-12-10].http://www.arocmag.com/article/02-2019-11-025.html.(HU R L,RUI L,QI X,et al.Text sentiment analysis based on recurrent neural network and attention model[J/OL].Application Research of Computers,2019,36(11).[2018-12-10].http://www.arocmag.com/article/02-2019-11-025.html.)
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                王伟,孙玉霞,齐庆杰,等.基于Bi GRU-Attention神经网络的文本情感分类模型[J/OL].计算机应用研究,2018,36(12)[2018-12-10].http://www.arocmag.com/article/02-2019-12-045.html(WANG W,SUN Y X,QI Q J,et al.Text sentiment classification model based on Bi GRU-Attention neural network[J/OL].Application Research of Computers,2018,36(12)[2018-12-10].http://www.arocmag.com/article/02-2019-12-045.html.)
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201908004&amp;v=MTgwNjhIOWpNcDQ5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhVTDNJTHo3QmQ3RzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b>陈洁,邵志清,张欢欢,等.基于并行混合神经网络模型的短文本情感分析[J/OL].计算机应用,2019.[2018-12-10].http://kns.cnki.net/kcms/detail/51.1307.TP.20190329.1643.008.html.(CHEN J,SHAO Z Q,ZHANG H H,et al.Short text sentiment analysis based on parallel hybrid neural network model[J/OL].Journal of Computer Applications,2019.[2018-12-10].http://kns.cnki.net/kcms/detail/51.1307.TP.20190329.1643.008.html.)
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZZSZ201902012&amp;v=Mjk4MDJDVVI3cWZadVpzRnluaFVMM0lQemZZZExHNEg5ak1yWTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b>常丹,王玉珍.基于SVM的用户评论情感分析方法研究[J].枣庄学院学报,2019,36(2):73-78.(CHANG D,WANG YZ.Research on the method of user comment sentiment analysis based on SVM[J].Journal of Zaozhuang University,2019,36(2):73-78.)
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJCJ201805017&amp;v=MDA5MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFVMM0lOaWZJWkxHNEg5bk1xbzlFWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b>王煜涵,张春云,赵宝林,等.卷积神经网络下的Twitter文本情感分析[J].数据采集与处理,2018,33(5):921-927.(WANG Y H,ZHANG C Y,ZHAO B L,et al.Sentiment analysis of twitter data based on CNN[J].Journal of Data Acquisition and Processing,2018,33(5):921-927.)
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910009" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910009&amp;v=MjE0NjBGckNVUjdxZlp1WnNGeW5oVUwzSUx6N0JkN0c0SDlqTnI0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
