<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136482486533750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909043%26RESULT%3d1%26SIGN%3dXat5HZqXwIMCq9F0tu3bBUqrGjo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909043&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909043&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909043&amp;v=MzI3MTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WYnpOTHo3QmQ3RzRIOWpNcG85Qlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="1 本文方法 ">1 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="1.1 &lt;b&gt;改进的基于卷积神经网络的图像超分辨率重建方法&lt;/b&gt;">1.1 <b>改进的基于卷积神经网络的图像超分辨率重建方法</b></a></li>
                                                <li><a href="#67" data-title="1.2 &lt;b&gt;数据集的建立及图像预处理&lt;/b&gt;">1.2 <b>数据集的建立及图像预处理</b></a></li>
                                                <li><a href="#77" data-title="1.3 &lt;b&gt;特征提取&lt;/b&gt;">1.3 <b>特征提取</b></a></li>
                                                <li><a href="#86" data-title="1.4 &lt;b&gt;误差函数构造&lt;/b&gt;">1.4 <b>误差函数构造</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="2.1 &lt;b&gt;实验配置&lt;/b&gt;">2.1 <b>实验配置</b></a></li>
                                                <li><a href="#105" data-title="2.2 &lt;b&gt;实验结果分析&lt;/b&gt;">2.2 <b>实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="图1 本文改进的深度密集卷积神经网络结构">图1 本文改进的深度密集卷积神经网络结构</a></li>
                                                <li><a href="#69" data-title="图2 成像系统原理">图2 成像系统原理</a></li>
                                                <li><a href="#72" data-title="图3 数据集配准图">图3 数据集配准图</a></li>
                                                <li><a href="#74" data-title="图4 数据集高频信息配准图">图4 数据集高频信息配准图</a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;数据集组成&lt;/b&gt;"><b>表</b>1 <b>数据集组成</b></a></li>
                                                <li><a href="#85" data-title="图5 改良的密集块(IDB)示意图">图5 改良的密集块(IDB)示意图</a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;网络结构参数&lt;/b&gt;"><b>表</b>2 <b>网络结构参数</b></a></li>
                                                <li><a href="#107" data-title="图6 各类方法重建结果对比">图6 各类方法重建结果对比</a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同方法的重建结果的&lt;/b&gt;PSNR&lt;b&gt;和&lt;/b&gt;SSIM&lt;b&gt;对比&lt;/b&gt;"><b>表</b>3 <b>不同方法的重建结果的</b>PSNR<b>和</b>SSIM<b>对比</b></a></li>
                                                <li><a href="#112" data-title="图7 不同方法得到的PSNR/SSIM趋势">图7 不同方法得到的PSNR/SSIM趋势</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="136">


                                    <a id="bibliography_1" title=" CHEN Y,YANG W,TAN H,et al.Image enhancement for LD based imaging in turbid water [J].Optik,2016,127(2):517-521." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES04116EFDA2BDE6D01009CED2A44B8F48&amp;v=MDY1ODRGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMcTV3NjA9TmlmT2ZiTzhIOURLMnZreEZlbDllQWsvdXhZUzZqOTBPd3FXcm1NeGZjQ2NNNzZYQ09Odg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         CHEN Y,YANG W,TAN H,et al.Image enhancement for LD based imaging in turbid water [J].Optik,2016,127(2):517-521.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_2" title=" 谌雨章,叶婷,程超杰,et al.水下湍流成像退化及优化恢复研究[J].光电工程,2018,45(12):55-65.(CHEN Y Z,YE T,CHENG C J,et al.Degradation and optimal recovery of underwater turbulent imaging [J].Opto-Electronic Engineering,2018,45(12):55-65.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC201812007&amp;v=MTAyMzA1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqblZiek5JaW5NYmJHNEg5bk5yWTlGWTRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         谌雨章,叶婷,程超杰,et al.水下湍流成像退化及优化恢复研究[J].光电工程,2018,45(12):55-65.(CHEN Y Z,YE T,CHENG C J,et al.Degradation and optimal recovery of underwater turbulent imaging [J].Opto-Electronic Engineering,2018,45(12):55-65.)
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_3" title=" SCHETTINI R,CORCHS S.Underwater image processing:state of the art of restoration and image enhancement methods [J].EURASIP Journal on Advances in Signal Processing,2010(3):1-15." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Underwater Image Processing: State of the Art of Restoration and Image Enhancement Methods">
                                        <b>[3]</b>
                                         SCHETTINI R,CORCHS S.Underwater image processing:state of the art of restoration and image enhancement methods [J].EURASIP Journal on Advances in Signal Processing,2010(3):1-15.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_4" title=" 杨爱萍,郑佳,王建,等.基于颜色失真去除与暗通道先验的水下图像复原[J].电子与信息学报,2015,37(11):2541-2547.(YANG A P,ZHENG J,WANG J,et al.Underwater image restoration based on color cast removal and dark channel prior [J].Journal of Electronics and Information Technology,2015,37(11):2541-2547.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201511001&amp;v=MDM1MDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmJ6TklUZlNkckc0SDlUTnJvOUZaWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         杨爱萍,郑佳,王建,等.基于颜色失真去除与暗通道先验的水下图像复原[J].电子与信息学报,2015,37(11):2541-2547.(YANG A P,ZHENG J,WANG J,et al.Underwater image restoration based on color cast removal and dark channel prior [J].Journal of Electronics and Information Technology,2015,37(11):2541-2547.)
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_5" title=" 王鑫,朱行成,宁晨,等.融合暗原色先验和稀疏表示的水下图像复原[J].电子与信息学报,2018,40(2):264-271.(WANG X,ZHU X C,NING C,et al.Combination of dark-channel prior with sparse representation for underwater image restoration [J].Journal of Electronics and Information Technology,2018,40(2):264-271.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201802002&amp;v=MzIyMDhadVpzRnlqblZiek5JVGZTZHJHNEg5bk1yWTlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         王鑫,朱行成,宁晨,等.融合暗原色先验和稀疏表示的水下图像复原[J].电子与信息学报,2018,40(2):264-271.(WANG X,ZHU X C,NING C,et al.Combination of dark-channel prior with sparse representation for underwater image restoration [J].Journal of Electronics and Information Technology,2018,40(2):264-271.)
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_6" title=" 郭相凤,贾建芳,杨瑞峰,等.基于水下图像光学成像模型的清晰化算法[J].计算机应用,2012,32(10):2836-2839.(GUO X F,JIA J F,YANG R F,et al.Visibility enhancing algorithm based on optical imaging model for underwater images [J].Journal of Computer Applications,2012,32(10):2836-2839.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201210042&amp;v=MDQ0NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmJ6Tkx6N0JkN0c0SDlQTnI0OUJab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         郭相凤,贾建芳,杨瑞峰,等.基于水下图像光学成像模型的清晰化算法[J].计算机应用,2012,32(10):2836-2839.(GUO X F,JIA J F,YANG R F,et al.Visibility enhancing algorithm based on optical imaging model for underwater images [J].Journal of Computer Applications,2012,32(10):2836-2839.)
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_7" title=" QUEVEDO E,DELORY E,CALLIC G M,et al.Underwater video enhancement using multi-camera super-resolution [J].Optics Communications,2017,404:94-102." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBCC631182CC952E6B385E6E06D52CE8F&amp;v=MDQ4MTU0UFhtWHJCUkJmTERuTUxMcENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMcTV3NjA9TmlmT2ZjSExiZGZQcm81TlpwaDhCWGs3dWhCaDZUZA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         QUEVEDO E,DELORY E,CALLIC G M,et al.Underwater video enhancement using multi-camera super-resolution [J].Optics Communications,2017,404:94-102.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_8" title=" LI J,LI Y.Underwater image restoration algorithm for free-ascending deep-sea tripods [J].Optics and Laser Technology,2019,110:129-134." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES500B297C768C42D81B7149382A91B2B7&amp;v=MjQ5NDBIcVBPcG9nMlkrMEhmM2c3dXg0U21EaDhUSGJocEJCRWNMUG1SOGlZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeExxNXc2MD1OaWZPZmJhNA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         LI J,LI Y.Underwater image restoration algorithm for free-ascending deep-sea tripods [J].Optics and Laser Technology,2019,110:129-134.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_9" title=" 张颢,范新南,李敏,等.基于光学成像模型的水下图像超分辨率重构[J].计算机与现代化,2017(4):7-13.(ZHANG H,FAN X N,LI M,et al.Underwater image super-resolution reconstruction based on optical imaging model [J].Computer and Modernization,2017(4):7-13.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201704002&amp;v=MDk5OTJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WYnpOTHpUVFpyRzRIOWJNcTQ5RlpvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         张颢,范新南,李敏,等.基于光学成像模型的水下图像超分辨率重构[J].计算机与现代化,2017(4):7-13.(ZHANG H,FAN X N,LI M,et al.Underwater image super-resolution reconstruction based on optical imaging model [J].Computer and Modernization,2017(4):7-13.)
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_10" title=" LU H,LI Y,NAKASHIMA S,et al.Underwater image super-resolution by descattering and fusion [J].IEEE Access,2017,5:670-679." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Underwater image superresolution by descattering and fusion">
                                        <b>[10]</b>
                                         LU H,LI Y,NAKASHIMA S,et al.Underwater image super-resolution by descattering and fusion [J].IEEE Access,2017,5:670-679.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_11" title=" NAKAGAWA Y,KIHARA K,TADOH R,et al.Super resolving of the depth map for 3D reconstruction of underwater terrain using kinect [C]// Proceedings of the 2016 IEEE 22nd International Conference on Parallel and Distributed Systems.Piscataway,NJ:IEEE,2016,1:1237-1240." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super resolving of the depth map for 3D reconstruction of underwater terrain using kinect">
                                        <b>[11]</b>
                                         NAKAGAWA Y,KIHARA K,TADOH R,et al.Super resolving of the depth map for 3D reconstruction of underwater terrain using kinect [C]// Proceedings of the 2016 IEEE 22nd International Conference on Parallel and Distributed Systems.Piscataway,NJ:IEEE,2016,1:1237-1240.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_12" title=" LU H,LI Y,UEMURA T,et al.Low illumination underwater light field images reconstruction using deep convolutional neural networks [J].Future Generation Computer Systems,2018,82:142-148." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6DD345CA7F7917D43A1830EF6DDEE362&amp;v=MDEyMjV6NTFTMytYMmhSQkRjZmhScnlkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeExxNXc2MD1OaWZPZmJYTWF0TElxdncwWTUwSUJYMCt1eElRbQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         LU H,LI Y,UEMURA T,et al.Low illumination underwater light field images reconstruction using deep convolutional neural networks [J].Future Generation Computer Systems,2018,82:142-148.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_13" title=" PEREZ J,ATTANASIO A C,NECHYPORENKO N,et al.A deep learning approach for underwater image enhancement [C]// Proceedings of the 2017 International Work-Conference on the Interplay Between Natural and Artificial Computation,LNCS 10338.Berlin:Springer,2017:183-192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A deep learning approach for underwater image enhancement">
                                        <b>[13]</b>
                                         PEREZ J,ATTANASIO A C,NECHYPORENKO N,et al.A deep learning approach for underwater image enhancement [C]// Proceedings of the 2017 International Work-Conference on the Interplay Between Natural and Artificial Computation,LNCS 10338.Berlin:Springer,2017:183-192.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_14" title=" DONG C,LOY C C,HE K,et al.Learning a deep convolutional network for image super-resolution [C]// Proceedings of the 2014 European Conference on Computer Vision,LNCS 8692.Berlin:Springer,2014:184-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a deep convolutional network for image super-resolution">
                                        <b>[14]</b>
                                         DONG C,LOY C C,HE K,et al.Learning a deep convolutional network for image super-resolution [C]// Proceedings of the 2014 European Conference on Computer Vision,LNCS 8692.Berlin:Springer,2014:184-199.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_15" title=" 张清博,张晓晖,韩宏伟.一种基于深度卷积神经网络的水下光电图像质量优化方法[J].光学学报,2018,38(11):88-96.(ZHANG Q B,ZHANG X H,HAN H W.Optimization of underwater photoelectric image quality based on deep convolutional neural networks [J].Acta Optica Sinica,2018,38(11):88-96.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811012&amp;v=MjIyNjVMRzRIOW5Ocm85RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WYnpOSWpYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         张清博,张晓晖,韩宏伟.一种基于深度卷积神经网络的水下光电图像质量优化方法[J].光学学报,2018,38(11):88-96.(ZHANG Q B,ZHANG X H,HAN H W.Optimization of underwater photoelectric image quality based on deep convolutional neural networks [J].Acta Optica Sinica,2018,38(11):88-96.)
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     LIM B,SON S,KIM H,et al.Enhanced deep residual networks for single image super-resolution [C]// Proceedings of the 2017 Computer Vision and Pattern Recognition Workshops.Washington,DC:IEEE Computer Society,2017:136-144.</a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_17" title=" KIM J,LEE J K,LEE K M.Accurate image super-resolution using very deep convolutional networks [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:1646-1654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">
                                        <b>[17]</b>
                                         KIM J,LEE J K,LEE K M.Accurate image super-resolution using very deep convolutional networks [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:1646-1654.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_18" >
                                        <b>[18]</b>
                                     HE K,ZHANG X,REN S,et al.Spatial pyramid pooling in deep convolutional networks for visual recognition [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1904-1916.</a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_19" title=" HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016,1:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[19]</b>
                                         HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016,1:770-778.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_20" title=" TONG T,LI G,LIU X,et al.Image super-resolution using dense skip connections [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2017:4809-4817." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution Using Dense Skip Connections">
                                        <b>[20]</b>
                                         TONG T,LI G,LIU X,et al.Image super-resolution using dense skip connections [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2017:4809-4817.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     WANG Z,BOVIK A C,SHEIKH H R,et al.Image quality assessment:from error visibility to structural similarity [J].IEEE Transactions on Image Processing,2004,13(4):600-612.</a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_22" title=" TIMOFTE R,de SMET V,van GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution [C]// Proceedings of the 2014 Asian Conference on Computer Vision,LNCS 9006.Berlin:Springer,2014:111-126." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A+:Adjusted anchored neighborhood regression for fast superresolution">
                                        <b>[22]</b>
                                         TIMOFTE R,de SMET V,van GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution [C]// Proceedings of the 2014 Asian Conference on Computer Vision,LNCS 9006.Berlin:Springer,2014:111-126.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_23" title=" HUANG J-B,SINGH A,AHUJA N .Single image super-resolution from transformed self-exemplars [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015,1:5197-5206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single Image Super-resolution from Transformed Self-Exemplars">
                                        <b>[23]</b>
                                         HUANG J-B,SINGH A,AHUJA N .Single image super-resolution from transformed self-exemplars [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015,1:5197-5206.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-17 12:02</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2738-2743 DOI:10.11772/j.issn.1001-9081.2019020353            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的水下图像超分辨率重建方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E9%BE%99%E5%BD%AA&amp;code=41646325&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈龙彪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%8C%E9%9B%A8%E7%AB%A0&amp;code=32002862&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谌雨章</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%93%E6%99%A8&amp;code=40178626&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晓晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B9%E9%B9%8F&amp;code=41997824&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邹鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E5%AD%A6%E6%95%8F&amp;code=31682914&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡学敏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B9%96%E5%8C%97%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0234810&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">湖北大学计算机与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于水体本身的特性以及水中悬浮颗粒对光的吸收和散射作用,水下图像普遍存在信噪比(SNR)低、分辨率低等一系列问题,但大部分方法传统处理方法包含图像增强、复原及重建,都依赖退化模型,并存在算法病态性问题。为进一步提高水下图像恢复算法的效果和效率,提出了一种改进的基于深度卷积神经网络的图像超分辨率重建方法。该方法网络中引入了改良的密集块结构(IDB),能在有效解决深度卷积神经网络梯度弥散问题的同时提高训练速度。该网络对经过配准的退化前后的水下图像进行训练,得到水下低分辨率图像和高分辨率图像之间的一个映射关系。实验结果表明,在基于自建的水下图像作为训练集上,较卷积神经网络的单帧图像超分辨率重建算法(SRCNN),使用引入了改良的密集块结构(IDB)的深度卷积神经网络对水下图像进行重建,重建图像的峰值信噪比(PSNR)提升达到0.38 dB,结构相似度(SSIM)提升达到0.013,能有效地提高水下图像的重建质量。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨率重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BF%A1%E5%99%AA%E6%AF%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信噪比;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B0%B4%E4%B8%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">水下图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%98%A0%E5%B0%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">映射;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈龙彪(1997—),男,湖北咸宁人,主要研究方向:深度学习、图像处理;;
                                </span>
                                <span>
                                    *谌雨章(1984—),男,湖北武汉人,副教授,博士,主要研究方向:光电探测、图像处理;电子邮箱hubucyz@foxmail.com;
                                </span>
                                <span>
                                    王晓晨(1998—),男,河南郑州人,主要研究方向:深度学习、软件工程;;
                                </span>
                                <span>
                                    邹鹏(1997—),男,湖北鄂州人,主要研究方向:图像处理、深度学习;;
                                </span>
                                <span>
                                    胡学敏(1985—),男,湖南岳阳人,副教授,博士,主要研究方向:计算机视觉、智能系统。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年科学基金资助项目(61806076);</span>
                                <span>湖北省大学生创新创业训练计划基金资助项目(201710512051,201810512051);</span>
                    </p>
            </div>
                    <h1><b>Underwater image super-resolution reconstruction method based on deep learning</b></h1>
                    <h2>
                    <span>CHEN Longbiao</span>
                    <span>CHEN Yuzhang</span>
                    <span>WANG Xiaochen</span>
                    <span>ZOU Peng</span>
                    <span>HU Xuemin</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Information Engineering, Hubei University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Due to the characteristics of water itself and the absorption and scattering of light by suspended particles in the water, a series of problems, such as low Signal-to-Noise Ratio(SNR) and low resolution, exist in underwater images. Most of the traditional processing methods include image enhancement, restoration and reconstruction rely on degradation model and have ill-posed algorithm problem. In order to further improve the effects and efficiency of underwater image restoration algorithm, an improved image super-resolution reconstruction method based on deep convolutional neural network was proposed. An Improved Dense Block structure(IDB) was introduced into the network of the method, which can effectively solve the gradient disappearance problem of deep convolutional neural network and improve the training speed at the same time. The network was used to train the underwater images before and after the degradation by registration and obtained the mapping relation between the low-resolution image and the high-resolution image. The experimental results show that on a self-built underwater image training set, the underwater image reconstructed by the deep convolutional neural network with IDB has the Peak Signal-to-Noise Ratio(PSNR) and Structural SIMilarity(SSIM) improved by 0.38 dB and 0.013 respectively, compared with SRCNN(an image Super-Resolution method using Conventional Neural Network) and proposed method can effectively improve the reconstruction quality of underwater images.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=super-resolution%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">super-resolution reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Signal-to-Noise%20Ratio(SNR)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Signal-to-Noise Ratio(SNR);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=underwater%20image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">underwater image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=mapping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">mapping;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Longbiao, born in 1997. His research interests include deep learning, image processing.;
                                </span>
                                <span>
                                    CHEN Yuzhang, born in 1984, Ph. D. , associate professor. His research interests include photoelectric detection, image processing. ;
                                </span>
                                <span>
                                    WANG Xiaochen, born in 1998. His research interests include deep learning, software engineering. ;
                                </span>
                                <span>
                                    ZOU Peng, born in 1997. His research interests include image processing, deep learning. ;
                                </span>
                                <span>
                                    HU Xuemin, born in 1985, Ph. D. , associate professor. His research interests include computer vision, intelligent system.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-05</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by Youth Science Foundation of National Natural Science Foundation of China(61806076);</span>
                                <span>the Student&#39;s Platform for Innovation and Entrepreneurship Training Program of Hubei Province(201710512051,201810512051);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="50">伴随水下资源开发、水下环境监测以及海洋军事等诸多领域的兴起与蓬勃发展,水下成像探测一直是研究的热点。但鉴于水下环境与陆地环境之间的极大差异,水中存在的水体对光的吸收和散射等诸多因素,导致电子设备采集到的水下图像质量严重退化,存在对比度低、表面雾化等多方面不足,严重影响了信息的准确获取。传统的水下图像处理方法包含图像的增强、复原及超分辨率重建算法<citation id="182" type="reference"><link href="136" rel="bibliography" /><link href="138" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>,但普遍存在依赖退化模型及效率低的问题,从而在应用范围及实时性方面受到了限制。因此,找到一种快速有效的方法来对水下图像进行处理,从而获取到信噪比高、清晰度好的图像是十分有必要且迫切的<citation id="183" type="reference"><link href="140" rel="bibliography" /><link href="142" rel="bibliography" /><link href="144" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="51">近年来,学者们通过引入各种数学方法来优化水下图像复原及重建的质量,并取得了一定的成效。针对水下图像光照不均匀以及图像的纹理细节模糊、对比度低的问题,郭相凤等<citation id="184" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了一种基于水下图像光学成像模型的清晰化算法,但该算法在实时性上的表现却不够好。也有学者针对特定的应用场景设计了一些专门的图像复原算法<citation id="188" type="reference"><link href="148" rel="bibliography" /><link href="150" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>,这些算法在且仅在特定的场景下表现出对图像复原良好的性能。考虑到水下光学条件复杂,张颢等<citation id="185" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>和Lu等<citation id="186" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>通过分析水下光学成像模型,针对噪声去除,分别提出了自己的超分辨率重构算法,使得到的超分辨率图像有较好的噪声水平,但对于光照不足、色彩偏离严重的水下图像,他们的算法却显得乏力。Nakagawa等<citation id="187" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出一种基于kinect水下三维图像重建方法,该方法考虑了水中泥沙的影响,采用水下暗通道先验去模糊、加权引导图像超分辨率,有效克服了深度图精度低的缺点。</p>
                </div>
                <div class="p1">
                    <p id="52">本文结合课题组近几年在退化模型和恢复算法的研究成果,总结出目前图像恢复算法的一些共性问题:1)无法处理好降噪和进行对比度增强之间的先后关系,进而导致重建得到的图像存在噪声消除不够完善或者细节受损的问题;2)数字图像多为二维或者三维数字矩阵,数据量十分庞大,算法的迭代耗时过长,从而无法实现实时性。因此采用经典方法处理水下图片无法快速准确地得到高质量的水下图像复原图。</p>
                </div>
                <div class="p1">
                    <p id="53">深度学习成为近年来图像处理领域的研究热潮,已有学者也在这股热潮中提出了基于深度学习的水下图像处理算法。Lu等<citation id="189" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了一种解决低强度光环境下水下成像问题的光场成像方法,该方法通过使用深度估计的深卷积神经网络来解决光场图像的去散射问题。而Perez等<citation id="190" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一种基于深度学习的水下图像增强方法,通过图像恢复技术对卷积神经网络进行训练,提高了神经网络的泛化能力。自从Dong等<citation id="191" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出应用卷积神经网络的单帧图像超分辨率重建算法(an image Super-Resolution method using Conventional Neural Network, SRCNN)以后,就打开了深度学习通往图像超分辨率重建的大门,同时解决了以前需要人为设计特征提取方式的不便,实现了端对端的学习,提高了图像重建的精度。但SRCNN算法却也存在明显不足,表现在当SRCNN的层数较多时,便会出现梯度弥散的现象;而且,SRCNN使用的是单一的模糊因子进行训练的,会导致该网络对其他模糊程度的图像的复原效果不佳。针对SRCNN的上述不足,本文提出了一种改进的深度卷积神经网络,用以训练低分辨率水下图像和高分辨率图像之间的映射关系,从而达到对水下图像进行重建、提高图像分辨率的目的。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag">1 本文方法</h3>
                <h4 class="anchor-tag" id="55" name="55">1.1 <b>改进的基于卷积神经网络的图像超分辨率重建方法</b></h4>
                <div class="p1">
                    <p id="56">本文设计的改进的深度密集卷积神经网络结构如图1所示。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文改进的深度密集卷积神经网络结构" src="Detail/GetImg?filename=images/JSJY201909043_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文改进的深度密集卷积神经网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Improved structure of deep dense convolution neural network proposed in this paper</p>

                </div>
                <div class="p1">
                    <p id="58">改进的网络主要包含两大部分,即:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ζ</mi><mo>¯</mo></mover><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>A</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>+</mo><mi>B</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">其中:<mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ζ</mi><mo>¯</mo></mover><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></math></mathml>为最终的重建图像HR;<i>X</i>为摄像头采集的水下低分辨率图像;<i>A</i>(<i>X</i>)为由低分辨率图像进行双三次插值得到的图像信息ILF,是图像的低频部分;<i>B</i>(<i>X</i>)表示低分辨率图像经过深度卷积神经网络后的输出图像信息IHF,是图像的高频信息部分。众所周知,低分辨率图像与高分辨率图像的最大区别是在其基础上损失了重要的高频部分信息,而低频部分的信息与原图像基本一致,所以本文构建的网络能够有效避免对低频部分信息的重复学习,从而加快模型的收敛速度。</p>
                </div>
                <div class="p1">
                    <p id="62">其中用于学习图像高频部分信息的深度卷积神经网络是图1的上半部,该部分网络由四大部分组成,包括:</p>
                </div>
                <div class="p1">
                    <p id="63">1)用于学习低层特征的卷积层,其中包含两个Conv卷积层;</p>
                </div>
                <div class="p1">
                    <p id="64">2)用于学习高层特征的改良密集块,包含12个IDB;</p>
                </div>
                <div class="p1">
                    <p id="65">3)用于将学习的密集特征融合的融合层;</p>
                </div>
                <div class="p1">
                    <p id="66">4)用于生成HR高频特征的重构块,包含一个上采样层和一个卷积层。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">1.2 <b>数据集的建立及图像预处理</b></h4>
                <div class="p1">
                    <p id="68">受张清博等<citation id="192" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的研究启发,本实验使用自建数据集,为保证实验数据的多样性,本次实验在长江和东湖上进行。图2是此次实验成像系统原理。</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 成像系统原理" src="Detail/GetImg?filename=images/JSJY201909043_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 成像系统原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Imaging system principle</p>

                </div>
                <div class="p1">
                    <p id="70">图3(a)是长江水中测试靶1的图像,图3(b)是清水中测试靶1的图像。可以看出:清水中的图像的成像质量比较高,基本没有噪声;而长江水中的图像含较多噪声,清晰度很低。</p>
                </div>
                <div class="p1">
                    <p id="71">为更好地体现出提取高频信息进行网络训练的直观效果,将测试靶1的高频信息提取结果导出如图4所示,可以看到,图像的高频细节部分信息展现更为突出,而低频部分信息不再进入网络进行训练,能极大地提高网络训练的速度和效率。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 数据集配准图" src="Detail/GetImg?filename=images/JSJY201909043_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 数据集配准图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Dataset registration map</p>

                </div>
                <div class="p1">
                    <p id="73">为了解决数据难以获取的难点,采集了大量用于训练的数据集。将在自然光下采集得到的训练靶退化前后的图像进行配准得到尺寸为64×64的训练样本对。仿照文献<citation id="193" type="reference">[<a class="sup">16</a>]</citation>中对数据的处理办法,对图像先进行 64×64颜色空间转换并提取64×64通道,将得到的图像按照步长为10,处理为大小64×64的图像块。再利用旋转、镜像的方法扩充训练集,选取90%作为训练集,10%作为校验集,再选取在东湖中得到的测试靶的图像作为测试集。最后得到训练、校验、和测试图片的数目及尺寸如表1所示。采集到的这些训练集图像块将用于特征提取步骤中,对本文设计的卷积神经网络模型进行交叉训练。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 数据集高频信息配准图" src="Detail/GetImg?filename=images/JSJY201909043_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 数据集高频信息配准图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 High frequency information registration map of dataset</p>

                </div>
                <div class="area_img" id="75">
                                            <p class="img_tit">
                                                <b>表</b>1 <b>数据集组成</b>
                                                    <br />
                                                Tab. 1 Dataset composition
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_07500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909043_07500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_07500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 数据集组成" src="Detail/GetImg?filename=images/JSJY201909043_07500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="76">采集到的这些训练集图像块将用于特征提取步骤中,对本文设计的卷积神经网络模型进行交叉训练。而所谓交叉训练,就是将训练集分为10份,每一轮训练都会选取9份作为训练集数据,剩下的1份将用作校验集,并用其对网络参数进行调节,每一轮的校验集都不同,以此种方式循环10轮,便构成了交叉训练。通过此方式,能有效地筛选出对网络训练更有利的数据集,防止因为部分训练集效果不佳而对网络整体效果产生影响。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">1.3 <b>特征提取</b></h4>
                <div class="p1">
                    <p id="78">式(1)中的<i>A</i>(<i>X</i>)可通过直接对低分辨率图像进行低通滤波,然后进行双三插值得到;而用于训练<i>B</i>(<i>X</i>) 的输入则为通过高通滤波处理的低分辨率图像。利用前层学习得到的特征图与卷积核进行卷积操作提取局部特征,卷积结果经激活函数运算后得到当前层的特征图。</p>
                </div>
                <div class="p1">
                    <p id="79">得到如下的输出结果:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Μ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mi>x</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>×</mo><mi>W</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中:<i>x</i>为特征图中像素点的值,<i>l</i>为卷积层的层数,<i>i</i>和<i>j</i>为像素点的位置, <i>f</i>为神经网络采取的激活函数,<i>M</i>为卷积核,<i>W</i>和<i>b</i>分别为卷积核中的权值和常数偏置。本文提出的卷积神经网络中,受文献<citation id="194" type="reference">[<a class="sup">17</a>]</citation>中提出方法的引导,在每次卷积前都对图像进行补0操作,这样保证了所有的特征图和最终的输出图像在尺寸上都保持一致,避免出现图像因通过逐步卷积而会变得越来越小的问题。</p>
                </div>
                <div class="p1">
                    <p id="82">有研究表明,网络层次越深,则对图像的重建性能会有更好的体现,见文献<citation id="195" type="reference">[<a class="sup">18</a>]</citation>。但由于神经网络中梯度弥散现象,会导致深层次网络的重建效果甚至不如简单的3层卷积神经网络。文献<citation id="196" type="reference">[<a class="sup">19</a>]</citation>中提出了残差网络结构的概念,残差结构在深层网络中利用跨层将浅层和深层的卷积层连接,使卷积层对特征图的残差进行拟合,大幅降低了训练过程的计算复杂度,同时也减少了梯度弥散的现象。</p>
                </div>
                <div class="p1">
                    <p id="83">本文对残差网络的中间层进行改良,在其中引入了改良的密集块结构。而所谓密集块结构,见文献<citation id="197" type="reference">[<a class="sup">20</a>]</citation>,就是一种具有密集连接的卷积神经网络;在该网络中,任何两层之间都有直接的连接,也就是说,网络每一层的输入都是前面所有层输出的并集,而该层所学习的特征图也会被直接传给其后面所有层作为输入。</p>
                </div>
                <div class="p1">
                    <p id="84">而本文独具匠心,将残差块与密集块结构相结合,提出了改良的密集块结构(Improved Dense Block, IDB),如图5所示。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 改良的密集块(IDB)示意图" src="Detail/GetImg?filename=images/JSJY201909043_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 改良的密集块(IDB)示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Schematic diagram of IDB</p>

                </div>
                <h4 class="anchor-tag" id="86" name="86">1.4 <b>误差函数构造</b></h4>
                <div class="p1">
                    <p id="87">前文中已有提及,输入的低分辨率图像与其对应的高分辨图像的低频部分信息基本一致,从而导致两者的残差图像的像素值大多数很小甚至很多为0。为了学习高频部分端到端的映射函数<i>F</i>,本文方法中训练高频部分信息的密集卷积神经网络采用了残差学习的思想,在图像重建过程中构造了本网络模型的重建误差函数<i>H</i>(<i>λ</i>),定义为:</p>
                </div>
                <div class="area_img" id="202">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201909043_20200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="89">其中:<i>N</i>是训练样本的数量,<i>X</i><sub><i>i</i></sub>为输入的第<i>i</i>幅低分辨率图像,<i>λ</i>={<i>W</i><sub>1</sub>,<i>W</i><sub>2</sub>,…,<i>W</i><sub><i>n</i></sub>,<i>b</i><sub>1</sub>,<i>b</i><sub>2</sub>,…,<i>b</i><sub><i>n</i></sub>},<i>r</i>是标准的高分辨率输出<i>Z</i><sub><i>i</i></sub>与低分辨率输入<i>X</i><sub><i>i</i></sub>的残差图像信息。</p>
                </div>
                <h3 id="90" name="90" class="anchor-tag">2 实验结果与分析</h3>
                <div class="p1">
                    <p id="91">由于超分辨率重建的任务是得到与原始高分辨率图像尽可能相近的重建图像,因此对于数据集{<i>X</i><sub><i>i</i></sub>,<i>Z</i><sub><i>i</i></sub>}<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>训练的目标为求解使得原始高分辨率图像<i>Z</i><sub><i>i</i></sub>和重建图像<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ζ</mi><mo>¯</mo></mover></math></mathml><sub><i>i</i></sub>的平均欧氏距离最小的<i>W</i>和<i>b</i>,即为:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mo>,</mo><mi>b</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi>W</mi><mo>,</mo><mi>b</mi></mrow></munder><mfrac><mn>1</mn><mrow><mn>2</mn><mi>Ν</mi></mrow></mfrac><mo stretchy="false">∥</mo><mi>Ζ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>Ζ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">峰值信噪比(Peak Signal-to-Noise Ratio, PSNR)和结构相似度(Structural SIMilarity, SSIM)是衡量图像重建的质量时用得较广泛的两个指标<citation id="198" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>,前者对处理后的结果跟原图相比的误差进行定量计算,PSNR愈高,说明失真愈小;SSIM越逼近1,说明处理后的结构与原图结构极为近似,即生成的结果图更好。故本文采用这两个指标来客观评价各类重建方法的优劣,对于原始高分辨率图像<i>Z</i><sub><i>i</i></sub>和重建图像<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ζ</mi><mo>¯</mo></mover></math></mathml><sub><i>i</i></sub>,PSNR和SSIM的表达式为:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mi>S</mi><mi>Ν</mi><mi>R</mi><mo>=</mo><mn>1</mn><mn>0</mn><mo>⋅</mo><mrow><mi>lg</mi></mrow><mfrac><mrow><mi>Μ</mi><mi>Ν</mi></mrow><mrow><mo stretchy="false">∥</mo><mi>Ζ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>Ζ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>S</mi><mi>S</mi><mi>Ι</mi><mi>Μ</mi><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mfrac><mrow><mo stretchy="false">(</mo><mn>2</mn><mi>u</mi><msub><mrow></mrow><mi>f</mi></msub><mi>u</mi><msub><mrow></mrow><mrow><mi>f</mi><mo>⌒</mo><mspace width="0.25em" /></mrow></msub><mo>+</mo><mspace width="0.25em" /><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>σ</mi><msub><mrow></mrow><mrow><mrow><mi>f</mi><mi>f</mi></mrow><mo>⌒</mo><mspace width="0.25em" /></mrow></msub><mo>+</mo><mspace width="0.25em" /><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>u</mi><msub><mrow></mrow><mi>f</mi></msub><mspace width="0.25em" /><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mspace width="0.25em" /><mi>u</mi><msub><mrow></mrow><mrow><mi>f</mi><mo>⌒</mo><mspace width="0.25em" /></mrow></msub><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mspace width="0.25em" /><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>σ</mi><msub><mrow></mrow><mi>f</mi></msub><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mspace width="0.25em" /><mi>σ</mi><msub><mrow></mrow><mrow><mi>f</mi><mo>⌒</mo><mspace width="0.25em" /></mrow></msub><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mspace width="0.25em" /><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">其中:<i>M</i>、<i>N</i>均为图像的尺寸,<i>Z</i><sub><i>i</i></sub>为原始高分辨率图像,<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ζ</mi><mo>¯</mo></mover></math></mathml><sub><i>i</i></sub>为重建后的超分辨率图像,<i>u</i><sub><i>f</i></sub>和<i>σ</i><sub><i>f</i></sub>分别为真实高分辨率图像的平均灰度值和方差, <i>u</i><sub><i>f</i>⌒ </sub>和<i>σ</i><sub><i>f</i>⌒ </sub>分别为重建后图像的灰度平均值和方差, <i>σ</i><sub><i>ff</i>⌒ </sub>为原始高分辨率图像和重建图像的协方差,<i>C</i><sub>1</sub>、<i>C</i><sub>2</sub>均为常数,且取<i>C</i><sub>1</sub>=(<i>k</i><sub>1</sub>*<i>L</i>)<sup>2</sup>,<i>C</i><sub>2</sub>=(<i>k</i><sub>2</sub>*<i>L</i>)<sup>2</sup>,<i>k</i><sub>1</sub>=0.01,<i>k</i><sub>2</sub>=0.03,<i>L</i>=255。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">2.1 <b>实验配置</b></h4>
                <div class="p1">
                    <p id="101">本实验的训练平台为:操作系统为 Ubuntu 14.04,CPU为Core i7-7700K(Quad-core 4.2 GHz),显卡为 NIVID GTX 1080ti。训练测试平台为CUDA8.0,cuDNN5.1, Pytorch0.4,python3.5。在主要参数学习率的设置上,本文采用的是学习率衰减的办法,这样能有效防止训练时间过长。设定初始学习率为0.001,每迭代100 000次,学习率降为原来的1/10,采用随机梯度下降法(Stochastic Gradient Descent, SGD)对网络进行训练。</p>
                </div>
                <div class="p1">
                    <p id="102">网络结构参数如表2所示,其中kerner_size为卷积核的尺寸,padding为输入的每一条边补充0的层数,bias为是否添加偏置。</p>
                </div>
                <div class="area_img" id="103">
                                            <p class="img_tit">
                                                <b>表</b>2 <b>网络结构参数</b>
                                                    <br />
                                                Tab. 2 Network structure parameter
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909043_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 网络结构参数" src="Detail/GetImg?filename=images/JSJY201909043_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="104">为了更加全面地展示出本神经网络的效果,本文选取了4个相关的方法在相同的数据集中进行比较,这些方法包括:双三插值、A+<citation id="199" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、Self-Exp<citation id="200" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和SRCNN<citation id="201" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,对比中所用的方法的代码,均从作者项目代码的公开网页上获得,其中A+、Self-Exp以及SRCNN方法代码网址分别为http://www.vision.ee.ethz.ch/～timofter/software/AplusCodes_SR.zip;https://github.com/jbhuang0604/StructCompletion; http://mmlab.ie.cuhk.edu.hk/projects/SRCNN/SRCNN_v1.zip。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">2.2 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="106">图7所示的是东湖中测试靶5以及采样地点地质面貌的图像使用不同的方法来进行重建的图像。可以看出,就算是使用图像超分辨率重建领域比较好的算法SRCNN重建之后的图像也有较为明显的缺陷。那就是由于SRCNN网络仅由单一尺度的模糊因子(此实验中尺度因子为×3)来对网络进行训练,所以得到的模型仅对某个模糊范围内的图像有较好的重建效果,而对这种存在多种模糊因子的水下光电图像的重建效果可见一斑,使用该方法处理水下光电图像得到的重建图像的边缘有明显的振铃效应现象,而本文方法重建的图像在边缘细节部分的表现明显优于SRCNN方法。本文方法的重建图形的边缘更加清晰,细节复原度高,整体重建效果更好。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 各类方法重建结果对比" src="Detail/GetImg?filename=images/JSJY201909043_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 各类方法重建结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of reconstruction results by various methods</p>

                </div>
                <div class="p1">
                    <p id="108">采用PSNR和SSIM两个指标对五种算法的处理结果进行了客观评价,表3给出了几个不同的测试图片下的客观测试结果,平均值行为110张测试图的测试结果的平均值,且每一列前一数据表示PSNR值,后一数据表示SSIM值。由表3可以看出,本文方法的PSNR比其他三种方法有了明显的提升,分别提升了2.70 dB、0.50 dB和0.39 dB,SSIM平均分别提高了0.100、0.022、0.013;与SRCNN算法29.58相比较,提升也不少,表现在PSNR提升了0.38 dB,SSIM提升了0.013,图片重建质量有很大的提高。</p>
                </div>
                <div class="area_img" id="109">
                    <p class="img_tit"><b>表</b>3 <b>不同方法的重建结果的</b>PSNR<b>和</b>SSIM<b>对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 PSNR and SSIM comparison of reconstruction results by using different methods</p>
                    <p class="img_note"></p>
                    <table id="109" border="1"><tr><td rowspan="2">测试图</td><td colspan="2"><br />Bicubic</td><td rowspan="2"></td><td colspan="2"><br />A+</td><td rowspan="2"></td><td colspan="2"><br />Self-Exp</td><td rowspan="2"></td><td colspan="2"><br />SRCNN</td><td rowspan="2"></td><td colspan="2"><br />本文方法</td></tr><tr><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td></tr><tr><td>图1</td><td>28.69</td><td>0.810</td><td></td><td>31.36</td><td>0.891</td><td></td><td>31.38</td><td>0.890</td><td></td><td>31.47</td><td>0.892</td><td></td><td>31.59</td><td>0.900</td></tr><tr><td><br />图2</td><td>26.32</td><td>0.703</td><td></td><td>27.34</td><td>0.780</td><td></td><td>27.39</td><td>0.781</td><td></td><td>27.50</td><td>0.781</td><td></td><td>27.91</td><td>0.788</td></tr><tr><td><br />图3</td><td>25.87</td><td>0.664</td><td></td><td>26.75</td><td>0.731</td><td></td><td>26.76</td><td>0.731</td><td></td><td>26.90</td><td>0.733</td><td></td><td>27.13</td><td>0.741</td></tr><tr><td><br />图4</td><td>23.34</td><td>0.648</td><td></td><td>25.38</td><td>0.793</td><td></td><td>25.37</td><td>0.792</td><td></td><td>25.52</td><td>0.792</td><td></td><td>25.82</td><td>0.806</td></tr><tr><td><br />图5</td><td>24.79</td><td>0.785</td><td></td><td>29.36</td><td>0.892</td><td></td><td>29.40</td><td>0.893</td><td></td><td>29.58</td><td>0.893</td><td></td><td>30.36</td><td>0.913</td></tr><tr><td><br />平均值</td><td>25.89</td><td>0.731</td><td></td><td>28.09</td><td>0.809</td><td></td><td>28.20</td><td>0.818</td><td></td><td>28.21</td><td>0.818</td><td></td><td>28.59</td><td>0.831</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="110">另外,为更加直观反映本文提出算法的高效率,在相同迭代次数下, 分别得到了SRCNN算法网络和本文方法网络学习模型,再分别对上述测试靶图像进行图像超分辨率处理,可以得到相应的PSNR和SSIM值。以测试靶5为例,采用不同迭代次数得到的网络模型对其进行测试和记录;为使得结果对比更加鲜明,在趋势图中还加入了Bicubic、A+以及Self-Exp算法随迭代次数变化的PSNR和SSIM的走势,最终得到五种算法PSNR和SSIM值的趋势图,如图7所示。</p>
                </div>
                <div class="p1">
                    <p id="111">由图7可知,无论是PSNR还是SSIM,本文方法得到的结果整体均优于SRCNN与其他三种算法。相比于SRCNN方法,从PSNR的趋势图可以看出,本文提出的方法的网络收敛速度更快,在迭代次数较小时,随着迭代次数的增加PSNR值便快速增加,说明网络在低迭代次数下就能表现出的性能。而反观SRCNN网络,在低迭代次数时,随着网络迭代次数的增加,其训练效果的增长较为缓慢,网络整体收敛速度较慢,SRCNN需要500 000次迭代才能达到的效果,本文算法仅需180 000次左右,效率提升近3倍;在同样经过45万次迭代后,SRCNN算法网络还未收敛,而本文提出的算法网络已基本收敛,并达到了最佳的PSNR训练效果。再观察SSIM趋势图,其展示出的效果与PSNR趋势图所展现的结果基本一致,只是本文算法网络训练SSIM的效率不像PSNR一样地惊人,但整体还是优于SRCNN算法,且同样是在迭代次数为45万次时,本文网络收敛,SRCNN网络未收敛。以上测试结果从客观层面说明本文提出的方法时间复杂度低、收敛速度快,针对于水下图像的重建有更好的效果。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909043_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同方法得到的PSNR/SSIM趋势" src="Detail/GetImg?filename=images/JSJY201909043_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同方法得到的PSNR/SSIM趋势  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909043_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 PSNR/SSIM trend charts obtained by different methods</p>

                </div>
                <h3 id="113" name="113" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="114">本文基于深度学习的思想,提出了一种改进的深度密集卷积神经网络,主要表现在将残差块与密集块结构相结合,提出了改良的密集块结构(IDB)。由实验结果可知,在基于自建的水下图像作为训练集的条件下,相比SRCNN,使用融入IDB的改进卷积神经网络重建得到的水下图像的PSNR和SSIM的平均提升分别高达0.38 dB和 0.013,重建图像的质量明显提高,图像边缘保持和边缘锐度都得到明显改善,使整体图像更加清晰。</p>
                </div>
                <div class="p1">
                    <p id="115">但不可否认的是,本文的方法所用的训练集都是基于水下图像的,其可拓展性还有待研究,所以下一步的重点工作应放在研究本文方法在其他领域的适用程度,提高本方法的普适性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="136">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES04116EFDA2BDE6D01009CED2A44B8F48&amp;v=MzA4NDV4WVM2ajkwT3dxV3JtTXhmY0NjTTc2WENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMcTV3NjA9TmlmT2ZiTzhIOURLMnZreEZlbDllQWsvdQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> CHEN Y,YANG W,TAN H,et al.Image enhancement for LD based imaging in turbid water [J].Optik,2016,127(2):517-521.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC201812007&amp;v=MzE4NDU3cWZadVpzRnlqblZiek5JaW5NYmJHNEg5bk5yWTlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 谌雨章,叶婷,程超杰,et al.水下湍流成像退化及优化恢复研究[J].光电工程,2018,45(12):55-65.(CHEN Y Z,YE T,CHENG C J,et al.Degradation and optimal recovery of underwater turbulent imaging [J].Opto-Electronic Engineering,2018,45(12):55-65.)
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Underwater Image Processing: State of the Art of Restoration and Image Enhancement Methods">

                                <b>[3]</b> SCHETTINI R,CORCHS S.Underwater image processing:state of the art of restoration and image enhancement methods [J].EURASIP Journal on Advances in Signal Processing,2010(3):1-15.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201511001&amp;v=MjU5MjVyRzRIOVROcm85RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WYnpOSVRmU2Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 杨爱萍,郑佳,王建,等.基于颜色失真去除与暗通道先验的水下图像复原[J].电子与信息学报,2015,37(11):2541-2547.(YANG A P,ZHENG J,WANG J,et al.Underwater image restoration based on color cast removal and dark channel prior [J].Journal of Electronics and Information Technology,2015,37(11):2541-2547.)
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201802002&amp;v=MDA3ODFUZlNkckc0SDluTXJZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmJ6Tkk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 王鑫,朱行成,宁晨,等.融合暗原色先验和稀疏表示的水下图像复原[J].电子与信息学报,2018,40(2):264-271.(WANG X,ZHU X C,NING C,et al.Combination of dark-channel prior with sparse representation for underwater image restoration [J].Journal of Electronics and Information Technology,2018,40(2):264-271.)
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201210042&amp;v=MTU4NTM0OUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmJ6Tkx6N0JkN0c0SDlQTnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 郭相凤,贾建芳,杨瑞峰,等.基于水下图像光学成像模型的清晰化算法[J].计算机应用,2012,32(10):2836-2839.(GUO X F,JIA J F,YANG R F,et al.Visibility enhancing algorithm based on optical imaging model for underwater images [J].Journal of Computer Applications,2012,32(10):2836-2839.)
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBCC631182CC952E6B385E6E06D52CE8F&amp;v=MTQ0MjZmQnJMVTA1dHBoeExxNXc2MD1OaWZPZmNITGJkZlBybzVOWnBoOEJYazd1aEJoNlRkNFBYbVhyQlJCZkxEbk1MTHBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> QUEVEDO E,DELORY E,CALLIC G M,et al.Underwater video enhancement using multi-camera super-resolution [J].Optics Communications,2017,404:94-102.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES500B297C768C42D81B7149382A91B2B7&amp;v=MjA2OTgwPU5pZk9mYmE0SHFQT3BvZzJZKzBIZjNnN3V4NFNtRGg4VEhiaHBCQkVjTFBtUjhpWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMcTV3Ng==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> LI J,LI Y.Underwater image restoration algorithm for free-ascending deep-sea tripods [J].Optics and Laser Technology,2019,110:129-134.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201704002&amp;v=MDU0Mjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVmJ6Tkx6VFRackc0SDliTXE0OUZab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 张颢,范新南,李敏,等.基于光学成像模型的水下图像超分辨率重构[J].计算机与现代化,2017(4):7-13.(ZHANG H,FAN X N,LI M,et al.Underwater image super-resolution reconstruction based on optical imaging model [J].Computer and Modernization,2017(4):7-13.)
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Underwater image superresolution by descattering and fusion">

                                <b>[10]</b> LU H,LI Y,NAKASHIMA S,et al.Underwater image super-resolution by descattering and fusion [J].IEEE Access,2017,5:670-679.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super resolving of the depth map for 3D reconstruction of underwater terrain using kinect">

                                <b>[11]</b> NAKAGAWA Y,KIHARA K,TADOH R,et al.Super resolving of the depth map for 3D reconstruction of underwater terrain using kinect [C]// Proceedings of the 2016 IEEE 22nd International Conference on Parallel and Distributed Systems.Piscataway,NJ:IEEE,2016,1:1237-1240.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6DD345CA7F7917D43A1830EF6DDEE362&amp;v=Mjg0MjR3NjA9TmlmT2ZiWE1hdExJcXZ3MFk1MElCWDArdXhJUW16NTFTMytYMmhSQkRjZmhScnlkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeExxNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> LU H,LI Y,UEMURA T,et al.Low illumination underwater light field images reconstruction using deep convolutional neural networks [J].Future Generation Computer Systems,2018,82:142-148.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A deep learning approach for underwater image enhancement">

                                <b>[13]</b> PEREZ J,ATTANASIO A C,NECHYPORENKO N,et al.A deep learning approach for underwater image enhancement [C]// Proceedings of the 2017 International Work-Conference on the Interplay Between Natural and Artificial Computation,LNCS 10338.Berlin:Springer,2017:183-192.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a deep convolutional network for image super-resolution">

                                <b>[14]</b> DONG C,LOY C C,HE K,et al.Learning a deep convolutional network for image super-resolution [C]// Proceedings of the 2014 European Conference on Computer Vision,LNCS 8692.Berlin:Springer,2014:184-199.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811012&amp;v=MTUyNDk5bk5ybzlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqblZiek5JalhUYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 张清博,张晓晖,韩宏伟.一种基于深度卷积神经网络的水下光电图像质量优化方法[J].光学学报,2018,38(11):88-96.(ZHANG Q B,ZHANG X H,HAN H W.Optimization of underwater photoelectric image quality based on deep convolutional neural networks [J].Acta Optica Sinica,2018,38(11):88-96.)
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 LIM B,SON S,KIM H,et al.Enhanced deep residual networks for single image super-resolution [C]// Proceedings of the 2017 Computer Vision and Pattern Recognition Workshops.Washington,DC:IEEE Computer Society,2017:136-144.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">

                                <b>[17]</b> KIM J,LEE J K,LEE K M.Accurate image super-resolution using very deep convolutional networks [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016:1646-1654.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_18" >
                                    <b>[18]</b>
                                 HE K,ZHANG X,REN S,et al.Spatial pyramid pooling in deep convolutional networks for visual recognition [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(9):1904-1916.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[19]</b> HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2016,1:770-778.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution Using Dense Skip Connections">

                                <b>[20]</b> TONG T,LI G,LIU X,et al.Image super-resolution using dense skip connections [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2017:4809-4817.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 WANG Z,BOVIK A C,SHEIKH H R,et al.Image quality assessment:from error visibility to structural similarity [J].IEEE Transactions on Image Processing,2004,13(4):600-612.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A+:Adjusted anchored neighborhood regression for fast superresolution">

                                <b>[22]</b> TIMOFTE R,de SMET V,van GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution [C]// Proceedings of the 2014 Asian Conference on Computer Vision,LNCS 9006.Berlin:Springer,2014:111-126.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single Image Super-resolution from Transformed Self-Exemplars">

                                <b>[23]</b> HUANG J-B,SINGH A,AHUJA N .Single image super-resolution from transformed self-exemplars [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015,1:5197-5206.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909043" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909043&amp;v=MzI3MTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WYnpOTHo3QmQ3RzRIOWpNcG85Qlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
