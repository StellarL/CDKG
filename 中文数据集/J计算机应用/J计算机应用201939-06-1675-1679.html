<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136675238908750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906021%26RESULT%3d1%26SIGN%3dhRru1IPc3goNvsyN%252b5viV6uRSVI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906021&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906021&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906021&amp;v=MDA5OTMzenFxQnRHRnJDVVI3cWZadVpzRnkvaFdydkpMejdCZDdHNEg5ak1xWTlIWllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 YOLO检测过程 ">1 YOLO检测过程</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="1.1 &lt;b&gt;目标检测&lt;/b&gt;">1.1 <b>目标检测</b></a></li>
                                                <li><a href="#62" data-title="1.2 &lt;b&gt;目标识别定位&lt;/b&gt;">1.2 <b>目标识别定位</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="2 YOLO-IPA网络架构 ">2 YOLO-IPA网络架构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="2.1 RPN">2.1 RPN</a></li>
                                                <li><a href="#69" data-title="2.2 &lt;b&gt;批量归一化&lt;/b&gt;">2.2 <b>批量归一化</b></a></li>
                                                <li><a href="#85" data-title="2.3 &lt;i&gt;X&lt;/i&gt;&lt;b&gt;轴方向候选框扩展&lt;/b&gt;">2.3 <i>X</i><b>轴方向候选框扩展</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#87" data-title="3 实验结果及分析 ">3 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="3.1 &lt;b&gt;国际音标字符训练集&lt;/b&gt;">3.1 <b>国际音标字符训练集</b></a></li>
                                                <li><a href="#91" data-title="3.2 &lt;b&gt;实验步骤&lt;/b&gt;">3.2 <b>实验步骤</b></a></li>
                                                <li><a href="#96" data-title="3.3 &lt;b&gt;结果分析&lt;/b&gt;">3.3 <b>结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="图1 YOLO工作原理图">图1 YOLO工作原理图</a></li>
                                                <li><a href="#61" data-title="图2 YOLO模型">图2 YOLO模型</a></li>
                                                <li><a href="#66" data-title="图3 国际音标分布示例">图3 国际音标分布示例</a></li>
                                                <li><a href="#71" data-title="图4 YOLO-IPA网络结构">图4 YOLO-IPA网络结构</a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同算法的识别精度对比&lt;/b&gt;"><b>表</b>1 <b>不同算法的识别精度对比</b></a></li>
                                                <li><a href="#100" data-title="图5 国际音标字符识别结果">图5 国际音标字符识别结果</a></li>
                                                <li><a href="#102" data-title="图6 迭代次数变化时的YOLOv2和YOLO-IPA训练损失">图6 迭代次数变化时的YOLOv2和YOLO-IPA训练损失</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="124">


                                    <a id="bibliography_1" title="燕海雄, 江荻.国际音标符号的分类、名称、功能与Unicode编码[J].语言科学, 2007, 6 (6) :82-91. (YAN H X, JIANG D.The classifications, functions, Chinese names of IPA symbols and their unicode[J].Linguistic Sciences, 2007, 6 (6) :82-91.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYKE200706012&amp;v=MjE0ODU3RzRIdGJNcVk5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnZKUERUQWE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        燕海雄, 江荻.国际音标符号的分类、名称、功能与Unicode编码[J].语言科学, 2007, 6 (6) :82-91. (YAN H X, JIANG D.The classifications, functions, Chinese names of IPA symbols and their unicode[J].Linguistic Sciences, 2007, 6 (6) :82-91.) 
                                    </a>
                                </li>
                                <li id="126">


                                    <a id="bibliography_2" title="吕佳, 江荻.国际音标扩展表的分类、命名与功能[J].听力学及言语疾病杂志, 2013, 21 (6) :665-668. (LYU J, JIANG D.The classification, nomenclature and function of extensions to the international phonetic alphabet[J].Journal of Audiology and Speech Pathology, 2013, 21 (6) :665-668.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TLXJ201306032&amp;v=MTU0NDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnZKTVNIVFpMRzRIOUxNcVk5R1pvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        吕佳, 江荻.国际音标扩展表的分类、命名与功能[J].听力学及言语疾病杂志, 2013, 21 (6) :665-668. (LYU J, JIANG D.The classification, nomenclature and function of extensions to the international phonetic alphabet[J].Journal of Audiology and Speech Pathology, 2013, 21 (6) :665-668.) 
                                    </a>
                                </li>
                                <li id="128">


                                    <a id="bibliography_3" title="曹雨生, 徐昂.微机国际音标系统[J].民族语文, 1990 (1) :74-79. (CAO Y S, XU A.The international phonetic alphabet system in microcomputer[J].Minority Languages of China, 1990 (1) :74-79.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MZYW199001011&amp;v=MDEyMDViS3hGOUhNcm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnZKS0RmU2U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        曹雨生, 徐昂.微机国际音标系统[J].民族语文, 1990 (1) :74-79. (CAO Y S, XU A.The international phonetic alphabet system in microcomputer[J].Minority Languages of China, 1990 (1) :74-79.) 
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_4" title="潘晓声.国际音标符号名称的简称[J].民族语文, 2012 (5) :56-61. (PAN X S.The name abbreviation of international phonetic alphabet symbols[J].Minority Languages of China, 2012 (5) :56-61.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MZYW201205014&amp;v=MTc3Mjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3J2SktEZlNlYkc0SDlQTXFvOUVZSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        潘晓声.国际音标符号名称的简称[J].民族语文, 2012 (5) :56-61. (PAN X S.The name abbreviation of international phonetic alphabet symbols[J].Minority Languages of China, 2012 (5) :56-61.) 
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_5" title="PADEFOGED H, 石在.国际音标的一些主要特征[J].齐齐哈尔师范学院学报 (哲学社会科学版) , 1995 (2) :150-153. (PADEFOGED H, SHI Z.Some major features of the international phonetic alphabet[J].Journal of Qiqihar University (Philosophy&amp;amp;Social Science Edition) , 1995 (2) :150-153.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QQHD199502036&amp;v=MDY5NzBUTXJZOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3J2Sk5EekRhckt4Rjk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        PADEFOGED H, 石在.国际音标的一些主要特征[J].齐齐哈尔师范学院学报 (哲学社会科学版) , 1995 (2) :150-153. (PADEFOGED H, SHI Z.Some major features of the international phonetic alphabet[J].Journal of Qiqihar University (Philosophy&amp;amp;Social Science Edition) , 1995 (2) :150-153.) 
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_6" title="邱立松.国际音标字符识别算法的研究[D].上海师范大学, 2015:2-3. (QIU L S.Study on the recognition algorithm of international phonetic alphabet characters[D].Shanghai:Shanghai Normal University, 2015:2-3.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015369820.nh&amp;v=MjU3MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnZKVkYyNkc3QytGOW5PcjVFYlBJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        邱立松.国际音标字符识别算法的研究[D].上海师范大学, 2015:2-3. (QIU L S.Study on the recognition algorithm of international phonetic alphabet characters[D].Shanghai:Shanghai Normal University, 2015:2-3.) 
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_7" title="张玉叶, 姜彬, 李开端, 等.一种结合结构和统计特征的脱机数字识别方法[J].微型电脑应用, 2016, 32 (8) :76-79. (ZHANGY Y, JIANG B, LI K D, et al.An off-line handwritten numeral recognition method combined with the statistical characteristics and structural features[J].Microcomputer Applications, 2016, 32 (8) :76-79.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXDY201608025&amp;v=MTI3MjRkN0c0SDlmTXA0OUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3J2Sk1qWFA=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        张玉叶, 姜彬, 李开端, 等.一种结合结构和统计特征的脱机数字识别方法[J].微型电脑应用, 2016, 32 (8) :76-79. (ZHANGY Y, JIANG B, LI K D, et al.An off-line handwritten numeral recognition method combined with the statistical characteristics and structural features[J].Microcomputer Applications, 2016, 32 (8) :76-79.) 
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_8" title="陈东杰, 张文生, 杨阳.基于深度学习的高铁接触网定位器检测与识别[J].中国科学技术大学学报, 2017, 47 (4) :320-327. (CHEN D J, ZHANG W S, YANG Y.Detection and recognition of high-speed railway catenary locator based on deep learning[J].Journal of University of Science and Technology of China, 2017, 47 (4) :320-327.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZKJD201704006&amp;v=Mjg4NTdydkpQeWJCYXJHNEg5Yk1xNDlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        陈东杰, 张文生, 杨阳.基于深度学习的高铁接触网定位器检测与识别[J].中国科学技术大学学报, 2017, 47 (4) :320-327. (CHEN D J, ZHANG W S, YANG Y.Detection and recognition of high-speed railway catenary locator based on deep learning[J].Journal of University of Science and Technology of China, 2017, 47 (4) :320-327.) 
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_9" title="白翔, 杨明锟, 石葆光, 等.基于深度学习的场景文字检测与识别[J].中国科学:信息科学, 2018, 48 (5) :531-544. (BAI X, YANG M K, SHI B G, et al.Deep learning for scene text detection and recognition[J].SCIENTIA SINICA Informationis, 2018, 48 (5) :531-544.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201805006&amp;v=MDY4MzN5L2hXcnZKTlRmQWRyRzRIOW5NcW85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        白翔, 杨明锟, 石葆光, 等.基于深度学习的场景文字检测与识别[J].中国科学:信息科学, 2018, 48 (5) :531-544. (BAI X, YANG M K, SHI B G, et al.Deep learning for scene text detection and recognition[J].SCIENTIA SINICA Informationis, 2018, 48 (5) :531-544.) 
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_10" title="钟冲, 徐光柱.结合前景检测和深度学习的运动行人检测方法[J].计算机与数字工程, 2016, 44 (12) :2396-2399. (ZHONGC, XU G Z.Movement pedestrian detection method combined with foreground subtraction and deep learning[J].Computer&amp;amp;Digital Engineering, 2016, 44 (12) :2396-2399.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSSG201612023&amp;v=MDUyODRhYkc0SDlmTnJZOUhaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3J2Skx6N1k=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        钟冲, 徐光柱.结合前景检测和深度学习的运动行人检测方法[J].计算机与数字工程, 2016, 44 (12) :2396-2399. (ZHONGC, XU G Z.Movement pedestrian detection method combined with foreground subtraction and deep learning[J].Computer&amp;amp;Digital Engineering, 2016, 44 (12) :2396-2399.) 
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_11" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[11]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_12" title="GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[12]</b>
                                        GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_13" title="HE K M, ZHANG X Y, REN S Q, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//Proceedings of the 2014 European Conference on Computer Vision, LNCS 8691.Cham:Springer, 2014:346-361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">
                                        <b>[13]</b>
                                        HE K M, ZHANG X Y, REN S Q, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//Proceedings of the 2014 European Conference on Computer Vision, LNCS 8691.Cham:Springer, 2014:346-361.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_14" title="GIRSHICK R.Fast R-CNN[C]//ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[14]</b>
                                        GIRSHICK R.Fast R-CNN[C]//ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_15" title="REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[15]</b>
                                        REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_16" title="DAI J F, LI Y, HE K M, et al.R-FCN:object detection via region-based fully convolutional networks[C]//NIPS 2016:Proceedings of the 30th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2016:379-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object detection Via Region-based fully convolutional networks">
                                        <b>[16]</b>
                                        DAI J F, LI Y, HE K M, et al.R-FCN:object detection via region-based fully convolutional networks[C]//NIPS 2016:Proceedings of the 30th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2016:379-387.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_17" title="REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">
                                        <b>[17]</b>
                                        REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:779-788.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_18" title="LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//ECCV 2016:Proceedings of the 2016European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">
                                        <b>[18]</b>
                                        LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//ECCV 2016:Proceedings of the 2016European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_19" title="REDMON J, FARHADI A.YOLO9000:better, faster, stronger[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6517-6525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">
                                        <b>[19]</b>
                                        REDMON J, FARHADI A.YOLO9000:better, faster, stronger[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6517-6525.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-15 14:59</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1675-1679 DOI:10.11772/j.issn.1001-9081.2018112361            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>候选框密度可变的YOLO网络国际音标字符识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E4%BC%8A&amp;code=41987899&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑伊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BD%90%E5%86%AC%E8%8E%B2&amp;code=09335051&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">齐冬莲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%9C%87%E5%AE%87&amp;code=11177133&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王震宇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E4%BA%BA%E6%96%87%E5%AD%A6%E9%99%A2&amp;code=0157820&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江大学人文学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0157820&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江大学电气工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统方法对国际音标 (IPA) 的字符特征提取存在的识别精度低、实效性差等问题, 提出了一种候选框密度可变的YOLO网络国际音标字符识别方法。首先, 以YOLO网络为基础, 结合国际音标字符图像<i>X</i>轴方向排列紧密、字符种类和形态多样的特点来改变YOLO网络中候选框的分布密度;然后, 增加识别过程中候选框在<i>X</i>轴上的分布, 同时减小<i>Y</i>轴方向上的密度, 构成YOLO-IPA网络。对采集自《汉语方音字汇》的含有1 360张、共72类国际音标图像的数据集进行检验, 实验结果表明:所提方法对尺寸较大的字符识别率达到93.72%, 对尺寸较小的字符识别率达到89.31%, 较传统的字符识别算法, 大幅提高了识别准确性;同时, 在实验环境下检测速度小于1 s, 因而可满足实时应用的需求。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BD%E9%99%85%E9%9F%B3%E6%A0%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国际音标;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%97%E7%AC%A6%E6%A3%80%E6%B5%8B%E4%B8%8E%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">字符检测与识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=YOLO%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">YOLO网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    郑伊 (1989—) , 男, 河南平顶山人, 博士研究生, 主要研究方向:汉语语音史、计算语言学;;
                                </span>
                                <span>
                                    *齐冬莲 (1973—) , 女, 河南南阳人, 教授, 博士, 主要研究方向:图像识别、机器学习;qidl@ zju. edu. cn;
                                </span>
                                <span>
                                    王震宇 (1992—) , 男, 山东潍坊人, 博士研究生, 主要研究方向:图像识别。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61571394);</span>
                                <span>浙江省科技项目 (2019C01001);</span>
                                <span>浙江大学学科交叉预研专项 (2018FZA122);</span>
                    </p>
            </div>
                    <h1><b>YOLO network character recognition method with variable candidate box density for international phonetic alphabet</b></h1>
                    <h2>
                    <span>ZHENG Yi</span>
                    <span>QI Donglian</span>
                    <span>WANG Zhenyu</span>
            </h2>
                    <h2>
                    <span>School of Humanities, Zhejiang University</span>
                    <span>College of Electrical Engineering, Zhejiang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the low recognition accuracy and poor practicability of the traditional character feature extraction methods to International Phonetic Alphabet (IPA) , a You Only Look Once (YOLO) network character recognition method with variable candidate box density for IPA was proposed. Firstly, based on YOLO network and combined with three characteristics such as the characters of IPA are closely arranged on <i>X</i>-axis direction and have various types and forms, the distribution density of candidate box in YOLO network was changed. Then, with the distribution density of candidate box on the <i>X</i>-axis increased while the distribution density of candidate box on the <i>Y</i>-axis reduced, YOLO-IPA network was constructed. The proposed method was tested on the IPA dataset collected from <i>Chinese Dialect Vocabulary</i> with 1 360 images of 72 categories. The experimental results show that, the proposed method has the recognition rate of 93.72% for large characters and 89.31% for small characters. Compared with the traditional character recognition algorithms, the proposed method greatly improves the recognition accuracy. Meanwhile, the detection speed was improved to less than 1 s in the experimental environment. Therefore, the proposed method can meet the need of real-time application.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=International%20Phonetic%20Alphabet%20(IPA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">International Phonetic Alphabet (IPA) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=character%20detection%20and%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">character detection and recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=You%20Only%20Look%20Once%20(YOLO)%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">You Only Look Once (YOLO) network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHENG Yi, born in 1989, Ph. D. candidate. His research interests include Chinese phonetic history, computational linguistics. ;
                                </span>
                                <span>
                                    QI Donglian, born in 1973, Ph. D. , professor. Her research interests include image recognition, machine learning. ;
                                </span>
                                <span>
                                    WANG Zhenyu, born in 1992, Ph. D. candidate. His research interests include image recognition.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61571394);</span>
                                <span>the Science and Technology Project of Zhejiang Province (2019C01001);</span>
                                <span>the Interdisciplinary Preresearch Project of Zhejiang University (2018FZA122);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="44">国际语音字母表 (International Phonetic Alphabet, IPA) 是国际语音学会为世界各种语言提供的一套强大的语音标注系统, 通过采用一种简单的图表方式对音标符号进行分类和命名, 目前在国际语言学界以及语言教学领域得到广泛应用<citation id="162" type="reference"><link href="124" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在我国各地方言和少数民族语言文字保护工作中, 均需借助国际音标进行记录, 可以说国际音标是记录和传承民族、地区文化最重要的载体。</p>
                </div>
                <div class="p1">
                    <p id="45">目前, 国际音标有103个单独字母, 23个元音、72个辅音, 已发展成为一种独立复杂的符号系统<citation id="163" type="reference"><link href="126" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。但是, 现存的字符识别系统还不能对国际音标字符进行高效且准确的识别。</p>
                </div>
                <div class="p1">
                    <p id="46">从语言学背景角度分析, 其主要原因在于:首先, 国际音标是一种专门化的符号系统, 往往只有语言学家学习和使用, 应用环境相对封闭<citation id="164" type="reference"><link href="128" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>;其次, 涉及国际音标的著作较少, 影响也小, 所以缺乏专门的字符识别系统<citation id="165" type="reference"><link href="130" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>;再次, 早期的国际音标符号以拉丁字母为基础, 音标符号较少, 借助已有的拉丁字母识别系统, 可实现部分国际音标符号的识别<citation id="166" type="reference"><link href="132" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。然而, 随着数字化技术的出现、图像设备的普及以及互联网的迅速发展, 越来越多的图书文献以图像的形式出现, 这其中也包括以国际音标为载体所记录的文献。为此, 对于国际音标字符识别的研究也逐步引起学者的关注。</p>
                </div>
                <div class="p1">
                    <p id="47">从技术应用角度分析, 传统的字符识别被当作一个分类问题来解决, 从字符图像的获取到结果的输出, 必须经过5个步骤:图像的获取、字符图像的预处理、字符的特征提取、字符的识别分类和识别结果<citation id="167" type="reference"><link href="134" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。其中, 字符图像的特征提取最为关键, 决定着识别系统的准确率和识别速度。目前, 已有研究大多基于统计特征和结构特征提取字符图像的信息, 如四边码特征、粗网格特征、梯度角度直方图特征等<citation id="168" type="reference"><link href="136" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。但当使用此类特征提取方法时, 会产生相似字符区分度差、抗笔画粘连干扰能力弱、局部字符特征描述不足等缺点, 导致后续分类器的应用困难、识别准确性下降、模型训练速度减慢, 严重制约了国际音标字符识别技术的应用和发展。</p>
                </div>
                <div class="p1">
                    <p id="48">随着机器学习技术的发展, 基于深度学习的目标检测与定位识别方法得到了广泛引用<citation id="177" type="reference"><link href="138" rel="bibliography" /><link href="140" rel="bibliography" /><link href="142" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。卷积神经网络 (Convolution Neural Network, CNN) 作为深度学习常用模型之一, 在目标检测与识别方面发挥了举足轻重的作用。Krizhevsky 等<citation id="169" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>利用卷积神经网络对LSVRC- 2010 (Large Scale Visual Recognition Challenge- 2010) 和LSVRC- 2012 (Large Scale Visual Recognition Challenge- 2012) 数据集的1.2×10<sup>6</sup>张图像进行1 000种以上的分类, 获得了当时最高的分类准确率。基于深度学习的目标检测方法大致可以分为两类:一类是基于区域提名 (Region Proposal) 的目标检测方法, 如R-CNN (Region CNN) <citation id="170" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、SPP-net (Spatial Pyramid Pooling net) <citation id="171" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、Fast R-CNN (Fast R-CNN) <citation id="172" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、Faster R-CNN (Faster R-CNN) <citation id="173" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、R-FCN (Region-based Fully Convolutional Network) <citation id="174" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>;另一类是无需区域提名, 基于端到端 (End-to-End) 的目标检测方法, 如YOLO (You Only Look Once) <sup></sup><citation id="175" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、SSD (Single Shot multibox Detector) <citation id="176" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>等。基于区域提名的方法在精度上占据优势, 但端到端的方法在速度上的优势更加明显。</p>
                </div>
                <div class="p1">
                    <p id="49">YOLO是由Redmon等<citation id="178" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>于2016年提出的一种全新的端到端检测算法, 虽然YOLO也属于CNN, 但在检测过程中模糊了候选区域生成、候选区域特征提取、特征输入分类器验证的区别, 直接快速地完成了检测任务, 可满足实时性检测的需求。国际音标图像排列紧密, 且字符的种类、形态多样, 综合考虑检测准确性与检测速度, 本文提出了一种候选区域密度可变的YOLO国际音标字符识别方法YOLO-IPA, 结合国际音标图像特点, 合理分布候选框密度, 提高检测的准确性, 为音标记录文献提供一种稳定、高效、可实时应用的字符识别方法。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">1 YOLO检测过程</h3>
                <div class="p1">
                    <p id="51">YOLO网络将候选框提取、特征提取、目标分类、目标定位统一于一个神经网络中, 如图1所示。输入一幅完整的图像后, 首先调整尺寸的大小, 再输入卷积神经网络计算卷积特征, 并进行预测, 其中卷积部分负责提取特征, 全连接部分负责预测, 最后通过非极大值抑制过滤边界框, 最终剩下的边界框即为目标检测结果。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906021_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 YOLO工作原理图" src="Detail/GetImg?filename=images/JSJY201906021_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 YOLO工作原理图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906021_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Working diagram of YOLO</p>

                </div>
                <h4 class="anchor-tag" id="53" name="53">1.1 <b>目标检测</b></h4>
                <div class="p1">
                    <p id="54">YOLO通过使用来自整个图像的特征预测每个边界框, 同时可预测所有类的边界框。如图2所示, YOLO首先将输入图像划分为<i>S</i>×<i>S</i>网格。如果目标的中心落入网格单元, 那么网格单元就负责检测该目标。每个网格要预测<i>B</i>个边界框, 而每个边界框除了要回归自身的位置之外, 还要附带预测所存在目标的置信度, 如式 (1) 所示:</p>
                </div>
                <div class="p1">
                    <p id="55"><i>C</i>=Pr (<i>Object</i>) *<i>IOU</i><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>Τ</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="57">其中, <i>C</i>为每个候选框中所存在目标的置信度, 判断是否有目标物落入候选框对应的网格中:如果有目标落在一个网格里, 取1;否则取0。 <i>IOU</i><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>Τ</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup></mrow></math></mathml>是预测的边界框和真实目标之间的检测评价函数, 即预测结果与真实目标的交集再比上它们的并集, 如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>Ο</mi><mi>U</mi><msubsup><mrow></mrow><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>Τ</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>b</mi><mi>o</mi><mi>x</mi><mo stretchy="false"> (</mo><mi>Τ</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>b</mi></mstyle><mi>o</mi><mi>x</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>d</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>b</mi><mi>o</mi><mi>x</mi><mo stretchy="false"> (</mo><mi>Τ</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∪</mo><mi>b</mi></mstyle><mi>o</mi><mi>x</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>d</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">其中: <i>box</i> (<i>Truth</i>) 为真实目标边界框;<i>box</i> (<i>Pred</i>) 为预测边界框;<i>area</i> () 为以上两个边界框的交集或并集区域面积。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906021_061.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 YOLO模型" src="Detail/GetImg?filename=images/JSJY201906021_061.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 YOLO模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906021_061.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Model of YOLO</p>

                </div>
                <h4 class="anchor-tag" id="62" name="62">1.2 <b>目标识别定位</b></h4>
                <div class="p1">
                    <p id="63">每个边界框均要预测 (<i>X</i>, <i>Y</i>, <i>W</i>, <i>H</i>) 和置信度等5个值, 其中, <i>X</i>、<i>Y</i>为预测框中心相对于单元格边界的偏移, <i>W</i>、<i>H</i>为预测框宽高相对于整幅图像之比。同时, 每个网络将预测<i>C</i>个类别概率, 即Pr (<i>Class</i><sub><i>i</i></sub>|<i>Object</i>) , 该概率表示第<i>i</i>类物体中心落入该网格的概率。因此, 对于输入的每幅照片, 最终网络的输出为<i>S</i>×<i>S</i>× (5×<i>B</i>+<i>C</i>) 的一个向量。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">2 YOLO-IPA网络架构</h3>
                <div class="p1">
                    <p id="65">针对国际音标字符的结构特点, YOLO-IPA首先以YOLO网络结构为基础, 采用目标区域网络 (Region Proposal Network, RPN) 并去除全连接层, 使用锚箱来预测目标框;通过在所有卷积层上添加批量归一化处理, 改善收敛效果, 构成YOLOv2网络<citation id="179" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。其次, 针对国际音标图像中, 音标在<i>X</i>轴上排列较为密集的情况, 如图3所示, 增加识别过程中候选框在<i>X</i>轴上的分布密度, 同时减少<i>Y</i>轴方向上的数量, 最终构成YOLO-IPA网络, 如图4所示。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906021_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 国际音标分布示例" src="Detail/GetImg?filename=images/JSJY201906021_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 国际音标分布示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906021_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Examples of IPA distribution</p>

                </div>
                <h4 class="anchor-tag" id="67" name="67">2.1 RPN</h4>
                <div class="p1">
                    <p id="68">YOLO包含有全连接层, 可直接预测边界框的坐标值, 但Faster R-CNN仅用卷积层与RPN来预测锚箱的偏移值与置信度, 而不是直接预测坐标值。实际应用中, 通过预测偏移量而不是坐标值更能够简化问题, 降低神经网络的学习难度。因此, 本文将使用RPN代替YOLO的全连接层, 使用锚箱来预测边界框。虽然使用锚箱会让精确度有所下降, 但可同时实现对不低于一千个框的预测, 且大大提高了召回率。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">2.2 <b>批量归一化</b></h4>
                <div class="p1">
                    <p id="70">批量归一化可以显著改善收敛性能, 而不需要其他形式的正则化。通过在YOLO所有卷积层中添加批量归一化, mAP (mean Average Precision) 可获得超过2%的改进效果, 同时也有助于规范模型, 而不会出现过度拟合<citation id="180" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。因此, 本文在YOLO网络的基础上增加了批量归一化的方法。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906021_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 YOLO-IPA网络结构" src="Detail/GetImg?filename=images/JSJY201906021_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 YOLO-IPA网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906021_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Network structure of YOLO-IPA</p>

                </div>
                <div class="p1">
                    <p id="72">批量归一化采用式 (3) 对某一层网络的输入数据作归一化处理:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>x</mi><mo>^</mo><mspace width="0.25em" /></mrow><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mo stretchy="false"> (</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>-</mo><mi>E</mi><mo stretchy="false">[</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo>/</mo><msqrt><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">[</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中:<i>E</i>[<i>x</i><sup> (<i>k</i>) </sup>]为每一批训练数据神经元<i>x</i><sup> (<i>k</i>) </sup>的平均值;<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">[</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo></mrow></msqrt></mrow></math></mathml>为每一批训练数据神经元<i>x</i><sup> (<i>k</i>) </sup>的标准差。训练过程中采用批量随机梯度下降 (Stochastic Gradient Descent, SGD) 法。经式 (3) 变换后, 每个神经元的激活函数<i>x</i>形成了均值为0、方差为1的正态分布。该处理的目的是把数值往后续要进行的非线性变换的线性区拉动, 增大导数值, 增强反向传播信息流动性, 加快训练收敛速度。但是, 这样处理同时会导致网络的表达能力下降。为此, 通过训练学习, 在每个神经元中增加两个调节参数:<i>scale</i> (<i>γ</i>) 和<i>shift</i> (<i>β</i>) 。对变换后的激活反变换, 使得网络表达能力增强, 即对变换后的激活进行如下的尺度变换和平移操作, 式 (3) 的反操作为:</p>
                </div>
                <div class="p1">
                    <p id="76"><i>y</i><sup> (<i>k</i>) </sup>=<i>γ</i><sup> (<i>k</i>) </sup><i>x</i>^ <sup> (<i>k</i>) </sup>+<i>β</i><sup> (<i>k</i>) </sup>      (4) </p>
                </div>
                <div class="p1">
                    <p id="77">其中, <i>γ</i>、<i>β</i>为可学习重构参数。</p>
                </div>
                <div class="p1">
                    <p id="78">当<i>γ</i>、<i>β</i>满足式 (5) 时可以恢复出原始的某一层所学到的特征, 从而使得所构建网络可以恢复原始网络所要学习特征的分布。</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>γ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><msqrt><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">[</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo></mrow></msqrt></mtd></mtr><mtr><mtd><mi>β</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi>E</mi><mo stretchy="false">[</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">批量归一化网络层的前向传导过程如式 (6) 所示:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub><mo>←</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext>小</mtext><mtext>批</mtext><mtext>量</mtext><mtext>均</mtext><mtext>值</mtext></mtd></mtr><mtr><mtd><mi>σ</mi><msubsup><mrow></mrow><mi>B</mi><mn>2</mn></msubsup><mo>←</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>;</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>小</mtext><mtext>批</mtext><mtext>量</mtext><mtext>方</mtext><mtext>差</mtext></mtd></mtr><mtr><mtd><mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>^</mo><mspace width="0.25em" /></mrow><mo>←</mo><mfrac><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub></mrow><mrow><msqrt><mrow><mi>σ</mi><msubsup><mrow></mrow><mi>B</mi><mn>2</mn></msubsup><mo>+</mo><mi>ε</mi></mrow></msqrt></mrow></mfrac><mo>;</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>归</mtext><mtext>一</mtext><mtext>化</mtext></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>←</mo><mi>γ</mi><mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>^</mo><mspace width="0.25em" /></mrow><mo>+</mo><mi>β</mi><mo>≡</mo><mi>B</mi><mi>Ν</mi><msub><mrow></mrow><mrow><mi>γ</mi><mo>, </mo><mi>β</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>;</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>尺</mtext><mtext>度</mtext><mtext>变</mtext><mtext>换</mtext><mtext>与</mtext><mtext>平</mtext><mtext>移</mtext></mtd></mtr></mtable></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<i>μ</i><sub><i>B</i></sub>为小批量神经元<i>x</i>的平均值, <i>σ</i><sub><i>B</i></sub>为其方差, 利用均值和方差得到归一化结果<i>x</i><sub><i>i</i></sub>^ , 并输入批量归一化网络层经过尺度变换与平移最终得到输出<b><i>y</i></b><sub><i>i</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="83">训练时, 可根据小批量里的若干训练实例进行激活数值调整, 使用从所有训练实例中获得的统计量来代替小批量里<i>m</i>个训练实例获得的均值和方差统计量。因为, 每次做小批量训练时, 都会有小批量里<i>m</i>个训练实例获得的均值和方差, 当需要全局统计量时, 只要利用每个小批量的均值和方差统计量, 即可求其对应的数学期望, 从而得出全局统计量<i>E</i>[<i>x</i>]和<i>Var</i>[<i>x</i>], 如式 (7) :</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo><mo>←</mo><mi>E</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">[</mo><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo><mo>←</mo><mfrac><mi>m</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mi>E</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">[</mo><mi>σ</mi><msubsup><mrow></mrow><mi>B</mi><mn>2</mn></msubsup><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">2.3 <i>X</i><b>轴方向候选框扩展</b></h4>
                <div class="p1">
                    <p id="86"><i>YOLO</i>网络首先将输入图像分成<i>S</i>×<i>S</i>网格, 候选框将在<i>X</i>和<i>Y</i>轴上同等密度分布。对国际音标字符进行检测时, 如图3所示, 音标字符在图像中呈现出在<i>X</i>轴上紧密排列分布、<i>Y</i>轴上分布稀疏的特点, 原有的候选框分布规则将难以适用。针对这一问题, 本文在此前建立网络上增加AddBoxes层, 增加候选框在<i>X</i>轴方向的密度, 同时减小<i>Y</i>轴方向候选框密度, 构成YOLO-IPA网络。</p>
                </div>
                <h3 id="87" name="87" class="anchor-tag">3 实验结果及分析</h3>
                <div class="p1">
                    <p id="88">为了验证本文所设计网络结构在国际音标字符检测与识别中的有效性, 在<i>PC</i>上进行了实验。<i>PC</i>的基本配置如下:<i>CPU</i>双核2.8 <i>GHz</i>, <i>GPU</i>采用单块<i>TitanX</i>, 12 <i>GB</i>显存, 32 <i>GB</i>内存, <i>Ubuntu</i> 14.04操作系统。深度学习采用<i>Caffe</i>框架训练, 训练时间共18 <i>h</i>。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">3.1 <b>国际音标字符训练集</b></h4>
                <div class="p1">
                    <p id="90">本文使用的国际音标字符样本来自《汉语方音字汇》。该文献收录了20个汉语方言点的字音材料, 基本上可以代表汉语的各大方言:北京、济南、西安、太原、武汉、成都、合肥、扬州 (以上官话) , 苏州、温州 (以上吴语) , 长沙、双峰 (以上湘语) , 南昌 (赣语) , 梅县 (客家话) , 广州、阳江 (以上粤语) , 厦门、潮州、福州、建瓯 (以上闽语) 。全书共收入3 000个字目, 按普通话音序排列, 用国际音标标写方言读音, 该书是汉语语音研究的重要参考书。将全书扫描, 按书中表格截出音标字符, 国际音标字符如图3所示。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">3.2 <b>实验步骤</b></h4>
                <div class="p1">
                    <p id="92">实验步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="93">1) 训练算法。使用基于随机梯度下降 (<i>SGD</i>) 法衍生的批量归一化方法来训练数据。每次随机读取10幅图像进行训练, 动量项为0.9, 学习率为10<sup>-4</sup>, 偏置学习率为2×10<sup>-4</sup>, 权值衰减系数为5×10<sup>-4</sup>。前20层卷积层使用原<i>YOLO</i>网络的参数, 卷积层转化为<i>RPN</i>的卷积核参数用0来初始化, 原网络结构中的<i>dropout</i>操作被保留在原来的位置。</p>
                </div>
                <div class="p1">
                    <p id="94">2) 微调。通过反向传播算法微调所有层的参数, 在原<i>YOLO</i>网络的基础上对<i>RPN</i>进行微调, 并遵循<i>Fast R</i>-<i>CNN</i> 中“<i>image</i>-<i>centric</i>”采样策略进行训练。</p>
                </div>
                <div class="p1">
                    <p id="95">3) 训练数据。收集了1 360张、共72类国际音标字符图像, 并人工标定了训练与测试用的数据集。训练前没有对字符图像进行任何处理, 字符图像的分辨率为300万～2 000万像素。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">3.3 <b>结果分析</b></h4>
                <div class="p1">
                    <p id="97">为验证本文提出的增加<i>X</i>轴方向候选框数量以提高检测器性能的有效性, 比较了YOLO-IPA与利用选择性搜索 (Selective Search, SS) 和EB (Edge Boxes) 两种不同方法进行可能性目标区域定位, 然后分别对可能性目标区域进行方向梯度直方图 (Histogram of Oriented Gradient, HOG) 、尺度不变特征变换 (Scale-Invariant Feature Transform, SIFT) 特征提取, 最后使用<i>K</i>-近邻 (<i>K</i>-Nearest Neighbors, KNN) 、支持向量机 (Support Vector Machine, SVM) 、逻辑回归Softmax分类器分别进行实现识别的仿真结果。如表1所示, 本文以ŋ、γ、s、k、ɦ五种字符为例给出了不同算法的识别精度对比, 并以图5为例给出了不同字符的识别结果。</p>
                </div>
                <div class="p1">
                    <p id="98">从表1可知, 6种方法中, YOLO-IPA均实现了最高精度, YOLOv2次之, 而基于传统特征提取的识别算法明显差于深度学习方法。同时, 由表1实验结果可以看出, YOLO-IPA方法识别精度与字符大小相关, 例如对于尺寸较大的ŋ字符 (识别精度为93.72%) 相比其他较小的字符, 如s字符 (识别精度为89.31%) , 具有更高的识别精度, 但相比其他算法的识别精度已经有了大幅提升。</p>
                </div>
                <div class="area_img" id="99">
                    <p class="img_tit"><b>表</b>1 <b>不同算法的识别精度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Comparison of recognition accuracy of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="99" border="1"><tr><td rowspan="2"><br />算法模型</td><td colspan="5"><br />识别精度/%</td></tr><tr><td><br />ŋ</td><td>γ</td><td>s</td><td>k</td><td>ɦ</td></tr><tr><td>SS+HOG+SVM</td><td>67.11</td><td>57.65</td><td>52.16</td><td>61.32</td><td>62.13</td></tr><tr><td><br />EB+HOG+SVM</td><td>64.65</td><td>60.86</td><td>55.32</td><td>60.56</td><td>65.01</td></tr><tr><td><br />EB+KNN</td><td>70.88</td><td>68.15</td><td>65.69</td><td>69.44</td><td>71.37</td></tr><tr><td><br />EB+SIFT+SM</td><td>75.66</td><td>70.15</td><td>67.26</td><td>72.39</td><td>73.70</td></tr><tr><td><br />YOLOv2</td><td>88.36</td><td>74.32</td><td>75.68</td><td>86.65</td><td>87.01</td></tr><tr><td><br />YOLO-IPA</td><td>93.73</td><td>90.11</td><td>89.31</td><td>91.62</td><td>92.36</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906021_100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 国际音标字符识别结果" src="Detail/GetImg?filename=images/JSJY201906021_100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 国际音标字符识别结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906021_100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Character recognition results of IPA</p>

                </div>
                <div class="p1">
                    <p id="101">增加<i>X</i>轴方向上候选框的YOLO网络, 不仅大幅提升了识别精度, 训练速度也明显加快。如图6所示, 当迭代次数一定时, YOLO-IPA较YOLOv2的训练损失值更小。在该实验环境下, YOLO-IPA的检测时间小于1 s, 可满足实时检测的需要, 更适用于实时字符检测系统。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906021_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 迭代次数变化时的YOLOv2和YOLO-IPA训练损失" src="Detail/GetImg?filename=images/JSJY201906021_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 迭代次数变化时的YOLOv2和YOLO-IPA训练损失  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906021_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Training loss of YOLOv2 and YOLO-IPA with change of iteration times</p>

                </div>
                <h3 id="103" name="103" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="104">当前卷积神经网络已经广泛应用在目标识别与检测的研究中, 本文采用一种改进的<i>YOLO</i>目标检测架构对国际音标进行定位和状态识别。与传统方法相比, 该方法可以有效地对国际音标字符进行识别, 但对较小尺寸的目标识别准确率还有待进一步提高, 这也是下一步的研究工作。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="124">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYKE200706012&amp;v=MzA4MjBGckNVUjdxZlp1WnNGeS9oV3J2SlBEVEFhN0c0SHRiTXFZOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>燕海雄, 江荻.国际音标符号的分类、名称、功能与Unicode编码[J].语言科学, 2007, 6 (6) :82-91. (YAN H X, JIANG D.The classifications, functions, Chinese names of IPA symbols and their unicode[J].Linguistic Sciences, 2007, 6 (6) :82-91.) 
                            </a>
                        </p>
                        <p id="126">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TLXJ201306032&amp;v=Mjc4NDZHNEg5TE1xWTlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdydkpNU0hUWkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>吕佳, 江荻.国际音标扩展表的分类、命名与功能[J].听力学及言语疾病杂志, 2013, 21 (6) :665-668. (LYU J, JIANG D.The classification, nomenclature and function of extensions to the international phonetic alphabet[J].Journal of Audiology and Speech Pathology, 2013, 21 (6) :665-668.) 
                            </a>
                        </p>
                        <p id="128">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MZYW199001011&amp;v=MTMxNjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnZKS0RmU2ViS3hGOUhNcm85RVpZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>曹雨生, 徐昂.微机国际音标系统[J].民族语文, 1990 (1) :74-79. (CAO Y S, XU A.The international phonetic alphabet system in microcomputer[J].Minority Languages of China, 1990 (1) :74-79.) 
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MZYW201205014&amp;v=MTEyNjJDVVI3cWZadVpzRnkvaFdydkpLRGZTZWJHNEg5UE1xbzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>潘晓声.国际音标符号名称的简称[J].民族语文, 2012 (5) :56-61. (PAN X S.The name abbreviation of international phonetic alphabet symbols[J].Minority Languages of China, 2012 (5) :56-61.) 
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QQHD199502036&amp;v=Mjk5MjU3cWZadVpzRnkvaFdydkpORHpEYXJLeEY5VE1yWTlHWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>PADEFOGED H, 石在.国际音标的一些主要特征[J].齐齐哈尔师范学院学报 (哲学社会科学版) , 1995 (2) :150-153. (PADEFOGED H, SHI Z.Some major features of the international phonetic alphabet[J].Journal of Qiqihar University (Philosophy&amp;Social Science Edition) , 1995 (2) :150-153.) 
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015369820.nh&amp;v=MzExOTVoV3J2SlZGMjZHN0MrRjluT3I1RWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>邱立松.国际音标字符识别算法的研究[D].上海师范大学, 2015:2-3. (QIU L S.Study on the recognition algorithm of international phonetic alphabet characters[D].Shanghai:Shanghai Normal University, 2015:2-3.) 
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXDY201608025&amp;v=MDk2NDBGckNVUjdxZlp1WnNGeS9oV3J2Sk1qWFBkN0c0SDlmTXA0OUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>张玉叶, 姜彬, 李开端, 等.一种结合结构和统计特征的脱机数字识别方法[J].微型电脑应用, 2016, 32 (8) :76-79. (ZHANGY Y, JIANG B, LI K D, et al.An off-line handwritten numeral recognition method combined with the statistical characteristics and structural features[J].Microcomputer Applications, 2016, 32 (8) :76-79.) 
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZKJD201704006&amp;v=MDk1MDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdydkpQeWJCYXJHNEg5Yk1xNDlGWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>陈东杰, 张文生, 杨阳.基于深度学习的高铁接触网定位器检测与识别[J].中国科学技术大学学报, 2017, 47 (4) :320-327. (CHEN D J, ZHANG W S, YANG Y.Detection and recognition of high-speed railway catenary locator based on deep learning[J].Journal of University of Science and Technology of China, 2017, 47 (4) :320-327.) 
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201805006&amp;v=MTc3MjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnZKTlRmQWRyRzRIOW5NcW85RllvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>白翔, 杨明锟, 石葆光, 等.基于深度学习的场景文字检测与识别[J].中国科学:信息科学, 2018, 48 (5) :531-544. (BAI X, YANG M K, SHI B G, et al.Deep learning for scene text detection and recognition[J].SCIENTIA SINICA Informationis, 2018, 48 (5) :531-544.) 
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSSG201612023&amp;v=MjE0MTVoV3J2Skx6N1lhYkc0SDlmTnJZOUhaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>钟冲, 徐光柱.结合前景检测和深度学习的运动行人检测方法[J].计算机与数字工程, 2016, 44 (12) :2396-2399. (ZHONGC, XU G Z.Movement pedestrian detection method combined with foreground subtraction and deep learning[J].Computer&amp;Digital Engineering, 2016, 44 (12) :2396-2399.) 
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[11]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[12]</b>GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">

                                <b>[13]</b>HE K M, ZHANG X Y, REN S Q, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//Proceedings of the 2014 European Conference on Computer Vision, LNCS 8691.Cham:Springer, 2014:346-361.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[14]</b>GIRSHICK R.Fast R-CNN[C]//ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[15]</b>REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object detection Via Region-based fully convolutional networks">

                                <b>[16]</b>DAI J F, LI Y, HE K M, et al.R-FCN:object detection via region-based fully convolutional networks[C]//NIPS 2016:Proceedings of the 30th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2016:379-387.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">

                                <b>[17]</b>REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:779-788.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">

                                <b>[18]</b>LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//ECCV 2016:Proceedings of the 2016European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">

                                <b>[19]</b>REDMON J, FARHADI A.YOLO9000:better, faster, stronger[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6517-6525.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906021" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906021&amp;v=MDA5OTMzenFxQnRHRnJDVVI3cWZadVpzRnkvaFdydkpMejdCZDdHNEg5ak1xWTlIWllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
