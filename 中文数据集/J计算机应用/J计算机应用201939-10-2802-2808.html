<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136455192783750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910003%26RESULT%3d1%26SIGN%3dy%252biVXFs%252boExBQoSbegSK1v1Z5KI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910003&amp;v=MTQ3MTJyNDlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFU3dk1MejdCZDdHNEg5ak4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="1 本文方法 ">1 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="1.1 &lt;b&gt;教师网络的结构&lt;/b&gt;">1.1 <b>教师网络的结构</b></a></li>
                                                <li><a href="#71" data-title="1.2 &lt;b&gt;学生网络的结构&lt;/b&gt;">1.2 <b>学生网络的结构</b></a></li>
                                                <li><a href="#84" data-title="1.3 &lt;b&gt;知识蒸馏与传递&lt;/b&gt;">1.3 <b>知识蒸馏与传递</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="2 实验设置 ">2 实验设置</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="2.1 &lt;b&gt;数据集和评价指标&lt;/b&gt;">2.1 <b>数据集和评价指标</b></a></li>
                                                <li><a href="#109" data-title="2.2 &lt;b&gt;参数设置&lt;/b&gt;">2.2 <b>参数设置</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="3 实验结果分析 ">3 实验结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="3.1 &lt;b&gt;不同级别的特征图的重要性&lt;/b&gt;">3.1 <b>不同级别的特征图的重要性</b></a></li>
                                                <li><a href="#115" data-title="3.2 &lt;b&gt;不同知识蒸馏方式的比较&lt;/b&gt;">3.2 <b>不同知识蒸馏方式的比较</b></a></li>
                                                <li><a href="#120" data-title="3.3 &lt;b&gt;知识蒸馏在超分辨率中的有效性&lt;/b&gt;">3.3 <b>知识蒸馏在超分辨率中的有效性</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#130" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="图1 超分辨率教师网络的网络结构">图1 超分辨率教师网络的网络结构</a></li>
                                                <li><a href="#73" data-title="图2 超分辨率学生网络的网络结构">图2 超分辨率学生网络的网络结构</a></li>
                                                <li><a href="#91" data-title="图3 从教师网络到学生网络的知识蒸馏过程">图3 从教师网络到学生网络的知识蒸馏过程</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;教师网络指导学生网络使用不同级别的特征图的结果&lt;/b&gt;"><b>表</b>1 <b>教师网络指导学生网络使用不同级别的特征图的结果</b></a></li>
                                                <li><a href="#117" data-title="图4 学生网络使用不同知识蒸馏方法的峰值信噪比结果">图4 学生网络使用不同知识蒸馏方法的峰值信噪比结果</a></li>
                                                <li><a href="#123" data-title="图5 SNSR和SNSR(&lt;i&gt;&lt;b&gt;G&lt;/b&gt;&lt;/i&gt;&lt;sub&gt;mean&lt;/sub&gt;)的视觉效果对比">图5 SNSR和SNSR(<i><b>G</b></i><sub>mean</sub>)的视觉效果对比</a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;表&lt;/b&gt;2 SNSR&lt;b&gt;和&lt;/b&gt;SNSR(&lt;i&gt;&lt;b&gt;G&lt;/b&gt;&lt;/i&gt;&lt;sub&gt;mean&lt;/sub&gt;)&lt;b&gt;在不同放大倍数结果对比&lt;/b&gt;"><b>表</b>2 SNSR<b>和</b>SNSR(<i><b>G</b></i><sub>mean</sub>)<b>在不同放大倍数结果对比</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同通道数的知识蒸馏效果比较&lt;/b&gt;"><b>表</b>3 <b>不同通道数的知识蒸馏效果比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="164">


                                    <a id="bibliography_1" title="DONG C,LOY C C,HE K,et al.Learning a deep convolutional network for image super-resolution[C]//Proceedings of the 2014European Conference on Computer Vision,LNCS 8692.Berlin:Springer,2014:184-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a deep convolutional network for image super-resolution">
                                        <b>[1]</b>
                                        DONG C,LOY C C,HE K,et al.Learning a deep convolutional network for image super-resolution[C]//Proceedings of the 2014European Conference on Computer Vision,LNCS 8692.Berlin:Springer,2014:184-199.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_2" title="KIM J,LEE J K,LEE K M.Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the 2016IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1646-1654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate Image Super-Resolution Using Very Deep Convolutional Networks">
                                        <b>[2]</b>
                                        KIM J,LEE J K,LEE K M.Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the 2016IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1646-1654.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_3" title="SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-01-10].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[3]</b>
                                        SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-01-10].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_4" title="LIM B,SON S,KIM H,et al.Enhanced deep residual networks for single image super-resolution[C]//Proceedings of the 2017IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2017:1132-1140." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">
                                        <b>[4]</b>
                                        LIM B,SON S,KIM H,et al.Enhanced deep residual networks for single image super-resolution[C]//Proceedings of the 2017IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2017:1132-1140.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_5" title="LEDIG C,THEIS L,HUSZR F,et al.Photo-realistic single image super-resolution using a generative adversarial network[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:105-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo-realistic single image super-resolution using a generative adversarial network">
                                        <b>[5]</b>
                                        LEDIG C,THEIS L,HUSZR F,et al.Photo-realistic single image super-resolution using a generative adversarial network[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:105-114.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_6" title="HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[6]</b>
                                        HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_7" title="ZHANG Y,TIAN Y,KONG Y,et al.Residual dense network for image super-resolution[C]//Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2472-2481." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Residual dense network for image super-resolution">
                                        <b>[7]</b>
                                        ZHANG Y,TIAN Y,KONG Y,et al.Residual dense network for image super-resolution[C]//Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2472-2481.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_8" title="ZHANG Y,LI K,LI K,et al.Image super-resolution using very deep residual channel attention networks[C]//Proceedings of the2018 European Conference on Computer Vision,LNCS 11211.Berlin:Springer,2018:294-310." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Super-resolution Using Very Deep Residual Channel Attention Networks">
                                        <b>[8]</b>
                                        ZHANG Y,LI K,LI K,et al.Image super-resolution using very deep residual channel attention networks[C]//Proceedings of the2018 European Conference on Computer Vision,LNCS 11211.Berlin:Springer,2018:294-310.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_9" title="陈伟杰.卷积神经网络的加速及压缩[D].广州:华南理工大学,2017:1-7.(CHEN W J.The acceleration and compression of convolutional neural networks[D].Guangzhou:South China University of Technology,2017:1-7.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017857187.nh&amp;v=MTcxMTJxSkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFU3dk1WRjI2R2J1OUdkREU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        陈伟杰.卷积神经网络的加速及压缩[D].广州:华南理工大学,2017:1-7.(CHEN W J.The acceleration and compression of convolutional neural networks[D].Guangzhou:South China University of Technology,2017:1-7.)
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_10" title="HINTON G,VINYALS O,DEAN J.Distilling the knowledge in a neural network[EB/OL].[2019-01-10].https://arxiv.org/pdf/1503.02531.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distilling the knowledge in a neural network">
                                        <b>[10]</b>
                                        HINTON G,VINYALS O,DEAN J.Distilling the knowledge in a neural network[EB/OL].[2019-01-10].https://arxiv.org/pdf/1503.02531.pdf.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_11" title="REMERO A,BALLAS N,KAHOU S E,et al.Fit Nets:hints for thin deep nets[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6550v2.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fit Nets:hints for thin deep nets">
                                        <b>[11]</b>
                                        REMERO A,BALLAS N,KAHOU S E,et al.Fit Nets:hints for thin deep nets[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6550v2.pdf.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_12" title="HORWARD A G,ZHU M,CHEN B,et al.Mobile Nets:efficient convolutional neural networks for mobile vision applications[EB/OL].[2019-01-10].https://arxiv.org/pdf/1704.04861.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobile Nets:efficient convolutional neural networks for mobile vision applications">
                                        <b>[12]</b>
                                        HORWARD A G,ZHU M,CHEN B,et al.Mobile Nets:efficient convolutional neural networks for mobile vision applications[EB/OL].[2019-01-10].https://arxiv.org/pdf/1704.04861.pdf.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_13" title="DONG C,LOY C C,TANG X.Accelerating the super-resolution convolutional neural network[C]//Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:391-407." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating the Super-Resolution Convolutional Neural Network">
                                        <b>[13]</b>
                                        DONG C,LOY C C,TANG X.Accelerating the super-resolution convolutional neural network[C]//Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:391-407.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_14" title="LAI W,HUANG J,AHUJA N,et al.Deep Laplacian pyramid networks for fast and accurate super resolution[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:5835-5843." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution">
                                        <b>[14]</b>
                                        LAI W,HUANG J,AHUJA N,et al.Deep Laplacian pyramid networks for fast and accurate super resolution[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:5835-5843.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_15" title="TONG T,LI G,LIU X,et al.Image super-resolution using dense skip connections[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:4809-4817." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution Using Dense Skip Connections">
                                        <b>[15]</b>
                                        TONG T,LI G,LIU X,et al.Image super-resolution using dense skip connections[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:4809-4817.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_16" title="ZAGORUYKO S,KOMODAKIS N.Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer[EB/OL].[2019-01-10].https://arxiv.org/pdf/1612.03928.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer">
                                        <b>[16]</b>
                                        ZAGORUYKO S,KOMODAKIS N.Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer[EB/OL].[2019-01-10].https://arxiv.org/pdf/1612.03928.pdf.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_17" title="AGUSTSSON E,TIMOFTE R.NTIRE 2017 challenge on single image super-resolution:dataset and study[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2017:1122-1131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NTIRE 2017 challenge on single image super-resolution:dataset and study">
                                        <b>[17]</b>
                                        AGUSTSSON E,TIMOFTE R.NTIRE 2017 challenge on single image super-resolution:dataset and study[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2017:1122-1131.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_18" title="BEVILACQUA M,ROUMY A,GUILLEMOT C,et al.Low-complexity single image super-resolution based on nonnegative neighbor embedding[EB/OL].[2019-01-10].http://people.rennes.inria.fr/Aline.Roumy/publi/12bmvc_Bevilacqua_low Complexity SR.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Low-complexity single image super-resolution based on nonnegative neighbor embedding">
                                        <b>[18]</b>
                                        BEVILACQUA M,ROUMY A,GUILLEMOT C,et al.Low-complexity single image super-resolution based on nonnegative neighbor embedding[EB/OL].[2019-01-10].http://people.rennes.inria.fr/Aline.Roumy/publi/12bmvc_Bevilacqua_low Complexity SR.pdf.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                    ZEYDE R,ELAD M,PROTTER M.On single image scale-up using sparse-representations[C]//Proceedings of the 2010 International Conference on Curves and Surfaces,LNCS 6920.Berlin:Springer,2010:711-730.</a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_20" title="MARTIN D,FOWLKES C,TAL D,et al.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Proceedings of the 8th IEEE International Conference on Computer Vision.Piscataway:IEEE,2001:416-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics">
                                        <b>[20]</b>
                                        MARTIN D,FOWLKES C,TAL D,et al.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Proceedings of the 8th IEEE International Conference on Computer Vision.Piscataway:IEEE,2001:416-423.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_21" title="HUANG J,SINGH A,AHUJA N.Single image super-resolution from transformed self-exemplars[C]//Proceedings of the 2015IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:5197-5206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single image super-resolution from transformed self-exemplars">
                                        <b>[21]</b>
                                        HUANG J,SINGH A,AHUJA N.Single image super-resolution from transformed self-exemplars[C]//Proceedings of the 2015IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:5197-5206.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_22" title="KINGMA D P,BA J L.Adam:a method for stochastic optimization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6980.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A Method for Stochastic Optimization[C/OL]">
                                        <b>[22]</b>
                                        KINGMA D P,BA J L.Adam:a method for stochastic optimization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6980.pdf.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_23" title="ROMANO Y,ISIDORO J,MILANFAR P.RAISR:rapid and accurate image super resolution[J].IEEE Transactions on Computational Imaging,2017,3(1):110-125." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RAISR:rapid and accurate image super resolution">
                                        <b>[23]</b>
                                        ROMANO Y,ISIDORO J,MILANFAR P.RAISR:rapid and accurate image super resolution[J].IEEE Transactions on Computational Imaging,2017,3(1):110-125.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-18 09:26</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2802-2808 DOI:10.11772/j.issn.1001-9081.2019030516            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于知识蒸馏的超分辨率卷积神经网络压缩方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E9%92%A6%E6%B3%89&amp;code=36215191&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高钦泉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%B2%A9&amp;code=06683048&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵岩</a>
                                <a href="javascript:;">李根</a>
                                <a href="javascript:;">童同</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E7%89%A9%E7%90%86%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0094575&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福州大学物理与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%BB%BA%E7%9C%81%E5%8C%BB%E7%96%97%E5%99%A8%E6%A2%B0%E4%B8%8E%E5%8C%BB%E8%8D%AF%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福建省医疗器械与医药技术重点实验室(福州大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%BB%BA%E5%B8%9D%E8%A7%86%E4%BF%A1%E6%81%AF%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福建帝视信息科技有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对目前用于超分辨率图像重建的深度学习网络模型结构深且计算复杂度高,以及存储网络模型所需空间大,进而导致其无法在资源受限的设备上有效运行的问题,提出一种基于知识蒸馏的超分辨率卷积神经网络的压缩方法。该方法使用一个参数多、重建效果好的教师网络和一个参数少、重建效果较差的学生网络。首先训练好教师网络,然后使用知识蒸馏的方法将知识从教师网络转移到学生网络,最后在不改变学生网络的网络结构及参数量的前提下提升学生网络的重建效果。实验使用峰值信噪比(PSNR)评估重建质量的结果,使用知识蒸馏方法的学生网络与不使用知识蒸馏方法的学生网络相比,在放大倍数为3时,在4个公开测试集上的PSNR提升量分别为0.53 dB、0.37 dB、0.24 dB和0.45 dB。在不改变学生网络结构的前提下,所提方法显著地改善了学生网络的超分辨率重建效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">知识蒸馏;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络压缩;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%99%E5%B8%88%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">教师网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%A6%E7%94%9F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">学生网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    高钦泉(1986—),男,福建福清人,副研究员,博士,主要研究方向:人工智能、计算机视觉、医学图像处理与分析、计算机辅助手术导航;;
                                </span>
                                <span>
                                    赵岩(1994—),男,山西五台人,硕士研究生,主要研究方向:人工智能、计算机视觉;;
                                </span>
                                <span>
                                    李根(1984—),男,吉林延边人,高级工程师,博士,主要研究方向:人工智能、计算机视觉;;
                                </span>
                                <span>
                                    *童同(1986—),男,安徽安庆人,研究员,博士,主要研究方向:人工智能、计算机视觉、医学图像处理与分析。电子邮箱ttraveltong@imperial-vision.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61802065);</span>
                    </p>
            </div>
                    <h1><b>Compression method of super-resolution convolutional neural network based on knowledge distillation</b></h1>
                    <h2>
                    <span>GAO Qinquan</span>
                    <span>ZHAO Yan</span>
                    <span>LI Gen</span>
                    <span>TONG Tong</span>
            </h2>
                    <h2>
                    <span>College of Physics and Information Engineering, Fuzhou University</span>
                    <span>Key Laboratory of Medical Instrumentation &amp; Pharmaceutical Technology of Fujian Province (Fuzhou University)</span>
                    <span>Imperial Vision Technology Company Limited</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the deep structure and high computational complexity of current network models based on deep learning for super-resolution image reconstruction, as well as the problem that the networks can not operate effectively on resource-constrained devices caused by the high storage space requirement for the network models, a super-resolution convolutional neural network compression method based on knowledge distillation was proposed. This method utilizes a teacher network with large parameters and good reconstruction effect as well as a student network with few parameters and poor reconstruction effect. Firstly the teacher network was trained; then knowledge distillation method was used to transfer knowledge from teacher network to student network; finally the reconstruction effect of the student network was improved without changing the network structure and the parameters of the student network. The Peak Signal-to-Noise Ratio(PSNR) was used to evaluate the quality of reconstruction in the experiments. Compared to the student network without knowledge distillation method, the student network using the knowledge distillation method has the PSNR increased by 0.53 dB, 0.37 dB, 0.24 dB and 0.45 dB respectively on four public test sets when the magnification times is 3. Without changing the structure of student network, the proposed method significantly improves the super-resolution reconstruction effect of the student network.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=super-resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">super-resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=knowledge%20distillation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">knowledge distillation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network%20compression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network compression;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=teacher%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">teacher network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=student%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">student network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    GAO Qinquan,born in 1986,Ph.D.,associate research fellow.His research interests include artificial intelligence,computer vision,medical image processing and analysis,computer-aided surgical guidance technology.;
                                </span>
                                <span>
                                    ZHAO Yan,born in 1994,M.S.candidate.His research interests include artificial intelligence,computer vision.;
                                </span>
                                <span>
                                    LI Gen,born in 1984,Ph.D.,senior engineer.His research interests include artificial intelligence,computer vision.;
                                </span>
                                <span>
                                    TONG Tong,born in 1986,Ph.D.,research fellow.His research interests include artificial intelligence,computer vision,medical image processing and analysis.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-29</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61802065);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="50">超分辨率(Super Resolution, SR)是计算机视觉中的经典问题,在监控设备、卫星图像、医学成像和其他许多领域都具有重要的应用价值。单张图像超分辨率(Single-Image SR, SISR)的目的是从单个低分辨率(Low Resolution, LR)图像中恢复出与其对应的高分辨率(High Resolution, HR)图像。SR是一种病态问题,可以从给定的低分辨率图像获得多个高分辨率图像。在例如双线性(bilinear)插值、双三次(bicubic)插值等传统插值方法中,使用固定的公式来对输入的低分辨率图像内的邻域像素的信息执行加权平均,来计算放大的高分辨率图像中所丢失的信息。然而,这种插值方法不能产生足够的高频细节来产生清晰的高分辨率图像。</p>
                </div>
                <div class="p1">
                    <p id="51">近年来,由于GPU和深度学习(Deep Learning)技术的快速发展,卷积神经网络(Convolutional Neural Network, CNN)被广泛用于解决超分辨率问题,并且在超分辨率图像的重建中得到了显著的效果。Dong等<citation id="210" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出的超分辨率卷积神经网络(Super-Resolution CNN, SRCNN)算法首次将卷积神经网络应用在超分辨率问题中,它直接学习输入低分辨率图像与其对应的高分辨率图像之间的端到端映射。SRCNN的结构非常简单,仅使用了三个卷积层。它的成功说明使用卷积神经网络来解决超分辨率问题是一种有效的方法,并且可以重建高分辨率图像的大量高频细节。Kim等<citation id="211" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>受VGG-net<citation id="212" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>的启发提出了非常深的超分辨率卷积神经网络(image SR using Very Deep convolution network, VDSR)。VDSR的网络由20个卷积层组成。VDSR中使用了大量卷积层,它的应用效果也表明在超分辨率中“网络越深,效果越好”。</p>
                </div>
                <div class="p1">
                    <p id="52">Lim等<citation id="213" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出超分辨率的增强型深度残差网络(Enhanced Deep residual networks for SR, EDSR)是基于SRResNet<citation id="214" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的结构,并去除了其中不必要的模块来优化它,以简化网络结构。该文指出,原始的ResNet<citation id="215" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>最初提出是为解决图像分类和检测等更高级的计算机视觉问题,直接把ResNet的体系结构应用到像超分辨率重建这样的低级计算机视觉问题时,并不能达到最理想的结果。由于批量标准化(Batch Normalization, BN)层消耗了与它前面的卷积层相同大小的内存,在去掉这一步操作后,相同的计算资源下,EDSR就可以堆叠更多的网络层或者使每层提取更多的特征,从而得到更好的重建效果。</p>
                </div>
                <div class="p1">
                    <p id="53">此外,RDN(Residual Dense Network)<citation id="216" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和RIR(Residual In Residual)<citation id="217" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>分别提出了更深的SR网络,进一步提高了SR重建效果。</p>
                </div>
                <div class="p1">
                    <p id="54">先前的研究表明,通过增加卷积神经网络的深度可以显著改善超分辨率重建效果;但是,其计算时间和内存消耗也同时增加。在低功率的嵌入式终端设备的实际应用场景中,计算资源和存储资源是部署深度CNN模型的约束条件。在实际应用中部署这些先进的SR模型仍然是一个巨大的挑战。这促进了对神经网络的加速和压缩<citation id="218" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>的研究。Hinton等<citation id="219" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出的知识蒸馏(Knowledge Distillation, KD)技术,使用教师学生网络的方法简化深度网络的训练。该框架将知识从深层网络(称为教师网络)转移到较小的网络(称为学生网络)中,训练学生网络使学生网络能够学习教师网络的输出。Remero等<citation id="220" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>用强大的教师网络训练一个窄而深的学生网络,以便改善网络训练的过程。在文献<citation id="221" type="reference">[<a class="sup">11</a>]</citation>中,不仅使用教师网络的分类概率作为学习目标,而且也使用教师网络的中间特征图作为训练期间的学习目标。</p>
                </div>
                <div class="p1">
                    <p id="55">本文提出使用一种新颖的策略,该策略使用教师-学生网络来改善图像超分辨率效果。其中的学生网络使用基于MobileNet<citation id="222" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的结构,在不改变学生网络的模型结构、不增加计算时间的约束条件下,使用知识蒸馏来提高超分辨率学生网络(Student Net for SR, SNSR)模型的重建效果。把超分辨率教师网络(Teacher Net for SR, TNSR)的中间特征图的统计特征值作为知识传递到超分辨率学生网络。与未使用知识蒸馏的学生网络相比,网络的重建效果得到了提升,使得低功率的嵌入式终端设备能够有效地运行图像超分辨率模型。综上所述,本文的主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="56">1)将知识蒸馏方法用在超分辨率问题中。利用知识蒸馏将知识从超分辨率教师网络转移到超分辨率学生网络,在不改变其网络结构的前提下,大幅提高了学生网络的超分辨率重建性能。</p>
                </div>
                <div class="p1">
                    <p id="57">2)为了确定从教师网络到学生网络的有效知识传递方法,本文评估并比较了多种不同的特征图统计值提取方法,并选择最佳方式将教师网络的知识传递到学生网络。</p>
                </div>
                <div class="p1">
                    <p id="58">3)将MobileNet用于超分辨率学生网络。与超分辨率教师网络相比,超分辨率学生模型需要较少的计算资源,并且可以在低功耗嵌入式设备上有效运行,因此提供了一种在计算受限的设备上实时部署超分辨率重建模型的有效方式。</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag">1 本文方法</h3>
                <h4 class="anchor-tag" id="60" name="60">1.1 <b>教师网络的结构</b></h4>
                <div class="p1">
                    <p id="61">本文的教师网络的结构如图1所示,教师网络的结构由三个部分组成:</p>
                </div>
                <div class="p1">
                    <p id="62">1)特征提取和表示,由一个卷积层和一个非线性激活层组成。第一部分的操作<i><b>F</b></i><sub>1</sub>(<i><b>X</b></i>)可表示为:</p>
                </div>
                <div class="p1">
                    <p id="63"><i><b>F</b></i><sub>1</sub>(<i><b>X</b></i>)=max(0,<i><b>W</b></i><sub>1</sub>*<i><b>X</b></i>+<i><b>B</b></i><sub>1</sub>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="64">其中:<i><b>W</b></i><sub>1</sub>和<i><b>B</b></i><sub>1</sub>分别表示第一个卷积层中的权重和偏差, <i><b>W</b></i><sub>1</sub>由<i>c</i>个滤波器组成,卷积核大小为<i>k</i><sub>1</sub>×<i>k</i><sub>1</sub>;“*”表示卷积操作;<i><b>X</b></i>是输入的低分辨率图像图像。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910003_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 超分辨率教师网络的网络结构" src="Detail/GetImg?filename=images/JSJY201910003_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 超分辨率教师网络的网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910003_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Structure of super-resolution teacher network</p>

                </div>
                <div class="p1">
                    <p id="66">2)非线性映射,由10个残差块组成<citation id="223" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。每个残差块中有两个卷积层,如图1所示。每个卷积层后面都有一个非线性激活层ReLU。跳跃连接用于连接每个残差块的输入和输出特征。通过这种方式,仅学习每个残差块的输入和信息,它可以解决训练非常深的网络中的梯度消失问题<citation id="224" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。每个残差块可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="67"><i><b>F</b></i><sub>2</sub><sub><i>n</i></sub><sub>+1</sub>(<i><b>X</b></i>)=max(0,<i><b>W</b></i><sub>2</sub><sub><i>n</i></sub><sub>+1</sub>*<i><b>F</b></i><sub>2</sub><sub><i>n</i></sub>(<i><b>X</b></i>)+<i><b>B</b></i><sub>2</sub><sub><i>n</i></sub><sub>+1</sub>)+</p>
                </div>
                <div class="p1">
                    <p id="68"><i><b>F</b></i><sub>2</sub><sub><i>n</i></sub><sub>-1</sub>(<i><b>X</b></i>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="69">其中:<i>n</i>表示残差块的序号;<i><b>W</b></i><sub>2</sub><sub><i>n</i></sub><sub>+1</sub>和<i><b>B</b></i><sub>2</sub><sub><i>n</i></sub><sub>+1</sub>分别是残差块中第二个卷积层的权重和偏差;<i><b>F</b></i><sub>2</sub><sub><i>n</i></sub><sub>+1</sub>(<i><b>X</b></i>)表示每个残差块的输出;<i><b>F</b></i><sub>2</sub><sub><i>n</i></sub>(<i><b>X</b></i>)表示残差块中的第一个卷积层和非线性激活层的输出;<i><b>F</b></i><sub>2</sub><sub><i>n</i></sub><sub>-1</sub>(<i><b>X</b></i>)是残差块的输入特征图。</p>
                </div>
                <div class="p1">
                    <p id="70">3)使用反卷积层重建HR输出图像。在先前的研究中,如SRCNN<citation id="225" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和VDSR<citation id="226" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>,使用双三次插值将低分辨率图像重建成超分辨率图像,然后将其作为卷积神经网络的输入。由于所有卷积运算都是在高分辨率空间中执行的,因此计算复杂度非常高。另外,插值预处理步骤会影响超分辨率重建效果。针对这一问题,FSRCNN(Fast Super-Resolution Convolutional Neural Network)<citation id="227" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>中采用反卷积层来代替插值操作,学习低分辨率图像和高分辨率图像之间的放大映射。最近的研究成果例如LapSRN<citation id="228" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和SRDenseNet<citation id="229" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>也采用了反卷积层来重建超分辨率图像。反卷积层可以被认为是卷积层的逆操作,并且通常堆叠在超分辨率网络的末端<citation id="230" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。使用反卷积层进行放大有两个优点:第一,它可以加速超分辨率的重建过程,由于在网络的末端进行放大操作,所以仅在低分辨率空间中进行所有卷积操作。如果放大倍数是<i>r</i>,则它将大致降低至计算成本的1/<i>r</i><sup>2</sup>。第二,因为通过在超分辨率网络的末端添加反卷积层来接收大的感受野,模型能够根据低分辨率图像的上下文信息推断更多高频细节。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">1.2 <b>学生网络的结构</b></h4>
                <div class="p1">
                    <p id="72">学生网络的结构如图2所示。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910003_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 超分辨率学生网络的网络结构" src="Detail/GetImg?filename=images/JSJY201910003_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 超分辨率学生网络的网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910003_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.2 <i>Structure of super</i>-<i>resolution student network</i></p>

                </div>
                <div class="p1">
                    <p id="74">学生网络包括三个部分:第一部分是特征提取层,该部分与教师网络相同。<i>MobileNet</i>采用深度可分离卷积(<i>Depthwise Separable Convolution</i>)块在取得较好结果的情况下,能大幅降低模型的参数规模。受此启发,学生网络的第二部分的非线性映射使用了三个深度可分离卷积块。每个模块由深度卷积(<i>Depthwise Convolution</i>)层和逐点卷积层组成,逐点卷积是卷积核大小为1×1的卷积。深度卷积层将单个滤波器应用于每个输入通道,而逐点卷积应用1×1的卷积来组合深度卷积的输出。与<i>MobileNet</i>不同,本文没有使用批量标准化层,因为它会消耗大量的内存<citation id="231" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。深度可分离卷积和逐点卷积之后都是非线性激活层,深度卷积可以表述为:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ο</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>c</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi></mrow></munder><mi mathvariant="bold-italic">Κ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo>,</mo><mi>c</mi></mrow></msub><mo>×</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>l</mi><mo>+</mo><mi>j</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>c</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">其中:<i><b>K</b></i>是大小为<i>k</i><sub><i><b>K</b></i></sub>×<i>k</i><sub><i><b>K</b></i></sub>×<i>C</i><sub><i><b>I</b></i></sub>的深度卷积核;<i>k</i><sub><i><b>K</b></i></sub>×<i>k</i><sub><i><b>K</b></i></sub>是卷积核大小;<i>C</i><sub><i><b>I</b></i></sub>是输入通道的数量。在上述深度卷积中,第<i>c</i>个卷积核<i><b>K</b></i><sub><i>c</i></sub>仅与第<i>c</i>个输入特征映射<i><b>I</b></i><sub><i>c</i></sub>卷积,生成第<i>c</i>个输出特征映射<i><b>O</b></i><sub><i>c</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="77">对于标准卷积,其计算成本可以表示为<i>k</i><sub><i><b>K</b></i></sub>×<i>k</i><sub><i><b>K</b></i></sub>×<i>C</i><sub><i><b>I</b></i></sub>×<i>C</i><sub><i><b>O</b></i></sub>×<i>M</i>×<i>M</i>,其中:<i>C</i><sub><i><b>O</b></i></sub>是输出通道的数量,<i>C</i><sub><i><b>I</b></i></sub>是输入通道的数量,<i>M</i>×<i>M</i>是特征图的大小。</p>
                </div>
                <div class="p1">
                    <p id="78">与标准卷积相比,使用深度可分离卷积的计算成本可表示为:</p>
                </div>
                <div class="p1">
                    <p id="79">(<i>k</i><sub><i><b>K</b></i></sub>×<i>k</i><sub><i><b>K</b></i></sub>×<i>C</i><sub><i><b>I</b></i></sub>×<i>M</i>×<i>M</i>)+(1×1×<i>C</i><sub><i><b>I</b></i></sub>×<i>C</i><sub><i><b>O</b></i></sub>×<i>M</i>×<i>M</i>)</p>
                </div>
                <div class="p1">
                    <p id="80">其中:第一部分是深度卷积的计算成本,第二部分是逐点卷积的计算成本。然后比较两个计算成本之间的区别:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>k</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Κ</mi></msub><mo>×</mo><mi>k</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Κ</mi></msub><mo>×</mo><mi>C</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ι</mi></msub><mo>×</mo><mi>Μ</mi><mo>×</mo><mi>Μ</mi><mo>+</mo><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mi>C</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ι</mi></msub><mo>×</mo><mi>C</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ο</mi></msub><mo>×</mo><mi>Μ</mi><mo>×</mo><mi>Μ</mi></mrow><mrow><mi>k</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Κ</mi></msub><mo>×</mo><mi>k</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Κ</mi></msub><mo>×</mo><mi>C</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ι</mi></msub><mo>×</mo><mi>C</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ο</mi></msub><mo>×</mo><mi>Μ</mi><mo>×</mo><mi>Μ</mi></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mfrac><mn>1</mn><mrow><mi>C</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ο</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Κ</mi></msub><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">如果使用卷积核大小为3×3的滤波器,则深度可分离卷积块的计算量为标准卷积块的1/9到1/8。这可以显著节省大量的计算资源,但会降低少许精度<citation id="232" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="83">学生网络的第三部分是用于重建的反卷积层,这部分也与教师网络相同。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">1.3 <b>知识蒸馏与传递</b></h4>
                <div class="p1">
                    <p id="85">为了将有用的知识从教师网络传递到学生网络,分别从教师网络和学生网络提取统计图。如图3所示,使用教师网络的第4、第7和第10个残差块的输出计算统计特征图,并分别表示为<i><b>O</b></i><sub><i>T</i></sub><sub>_1</sub>、<i><b>O</b></i><sub><i>T</i></sub><sub>_2</sub>和<i><b>O</b></i><sub><i>T</i></sub><sub>_3</sub>,用来描述低级、中级、高级视觉信息。在学生网络中,本文使用第1、第2和第3个深度可分离卷积块的输出提取相应级别的统计特征图,分别表示为<i><b>O</b></i><sub><i>S</i></sub><sub>_1</sub>、<i><b>O</b></i><sub><i>S</i></sub><sub>_2</sub>和<i><b>O</b></i><sub><i>S</i></sub><sub>_3</sub>。之后,使用教师网络中<i><b>O</b></i><sub><i>T</i></sub><sub>_1</sub>、<i><b>O</b></i><sub><i>T</i></sub><sub>_2</sub>和<i><b>O</b></i><sub><i>T</i></sub><sub>_3</sub>的信息来指导训练学生网络中<i><b>O</b></i><sub><i>S</i></sub><sub>_1</sub>、<i><b>O</b></i><sub><i>S</i></sub><sub>_2</sub>和<i><b>O</b></i><sub><i>S</i></sub><sub>_3</sub>的信息。传递的统计特征是从网络的中间输出计算的,而不是直接使用网络的输出。在图像分类问题中已经证明了传递网络中间的特征图会比传递网络的输出效果更好<citation id="233" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。网络的中间特征图输出可以表示为张量<i><b>T</b></i>∈<b>R</b><sup><i>C</i></sup><sup>×</sup><sup><i>H</i></sup><sup>×</sup><sup><i>L</i></sup>,它由空间大小为<i>H</i>×<i>L</i>的<i>C</i>个特征通道组成。然后从张量<i><b>T</b></i>计算统计特征图<i><b>S</b></i>:</p>
                </div>
                <div class="p1">
                    <p id="86"><i>G</i>:<i><b>T</b></i>∈<b>R</b><sup><i>C</i></sup><sup>×</sup><sup><i>H</i></sup><sup>×</sup><sup><i>L</i></sup>→<i><b>S</b></i>∈<b>R</b><sup><i>H</i></sup><sup>×</sup><sup><i>L</i></sup>      (5)</p>
                </div>
                <div class="p1">
                    <p id="87">其中:<i>G</i>是将张量<i><b>T</b></i>映射到统计特征图<i><b>S</b></i>∈<b>R</b><sup><i>H</i></sup><sup>×</sup><sup><i><b>W</b></i></sup>的函数。本文用4种类型的映射函数来计算统计特征图:</p>
                </div>
                <div class="p1">
                    <p id="88">第一种映射函数表示为:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mi>i</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi mathvariant="bold-italic">Τ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mi>p</mi></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">首先求张量的均值,再对其求<i>p</i>(<i>p</i>≥1)次方进行传递。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910003_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 从教师网络到学生网络的知识蒸馏过程" src="Detail/GetImg?filename=images/JSJY201910003_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 从教师网络到学生网络的知识蒸馏过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910003_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Process of knowledge distillation from teacher network to student network</p>

                </div>
                <div class="p1">
                    <p id="92">第二种映射函数表示为:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">)</mo><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>i</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mo stretchy="false">(</mo></mstyle><mi mathvariant="bold-italic">Τ</mi><mtext> </mtext><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">首先对其求<i>p</i>(<i>p</i>≥1)次方,之后再对其求均值进行传递。</p>
                </div>
                <div class="p1">
                    <p id="95">最后两种映射函数分别将张量<i><b>T</b></i>特征图的最大值以及最小值作为统计特征图进行传递:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>C</mi></mrow></munder><mspace width="0.25em" /><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>C</mi></mrow></munder><mspace width="0.25em" /><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">在训练学生网络期间,从学生网络提取的统计特征图<i><b>S</b></i><sub>1</sub>、<i><b>S</b></i><sub>2</sub>和<i><b>S</b></i><sub>3</sub>学习从教师网络提取的统计特征图<i><b>T</b></i><sub>1</sub>、<i><b>T</b></i><sub>2</sub>和<i><b>T</b></i><sub>3</sub>的内容。另外,重建超分辨率图像同时学习训练目标图像的内容。因此,训练学生网络的总损失函数可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="98"><i>loss</i>=<i>λ</i><sub>0</sub>×<i>loss</i><sub>0</sub>(<i><b>Y</b></i>′,<i><b>Y</b></i>)+</p>
                </div>
                <div class="p1">
                    <p id="99"><i>λ</i><sub>1</sub>×<i>loss</i><sub>1</sub>(<i><b>O</b></i><sub><i>S</i></sub><sub>_1</sub>,<i><b>O</b></i><sub><i>T</i></sub><sub>_1</sub>)+</p>
                </div>
                <div class="p1">
                    <p id="100"><i>λ</i><sub>2</sub>×<i>loss</i><sub>2</sub>(<i><b>O</b></i><sub><i>S</i></sub><sub>_2</sub>,<i><b>O</b></i><sub><i>T</i></sub><sub>_2</sub>)+</p>
                </div>
                <div class="p1">
                    <p id="101"><i>λ</i><sub>3</sub>×<i>loss</i><sub>3</sub>(<i><b>O</b></i><sub><i>S</i></sub><sub>_3</sub>,<i><b>O</b></i><sub><i>T</i></sub><sub>_3</sub>)      (10)</p>
                </div>
                <div class="p1">
                    <p id="102">其中:<i>λ</i><sub><i>i</i></sub>是损失的权重系数;<i>loss</i><sub>0</sub>为计算重建图像<i><b>Y</b></i>′和训练目标图像<i><b>Y</b></i>之间的损失函数;<i>loss</i><sub>1</sub>、<i>loss</i><sub>2</sub>和<i>loss</i><sub>3</sub>分别代表学生网络和教师网络不同级别统计特征图之间的损失函数。本文使用Charbonnier函数<citation id="234" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>作为损失函数,并将其定义为:</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mi>c</mi><mi>h</mi><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">Y</mi><mo>′</mo></msup><mo>,</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">)</mo><mo>=</mo><msqrt><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><msup><mi mathvariant="bold-italic">Y</mi><mo>′</mo></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">其中<i>ε</i><sup>2</sup>设定为0.001。Charbonnier损失函数用于计算式(10)中的所有损失。</p>
                </div>
                <div class="area_img" id="105">
                    <p class="img_tit"><b>表</b>1 <b>教师网络指导学生网络使用不同级别的特征图的结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.1 Results of SNSR was guided by TNSR with different level feature maps</p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td rowspan="2" colspan="2">方法</td><td rowspan="2">放大<br />倍数</td><td colspan="2"><br />Set5</td><td rowspan="2"></td><td colspan="2"><br />Set14</td><td rowspan="2"></td><td colspan="2"><br />BSDS100</td><td rowspan="2"></td><td colspan="2"><br />Urban100</td></tr><tr><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td></tr><tr><td colspan="2"><br />Bicubic</td><td>3</td><td>30.40</td><td>0.868 8</td><td></td><td>27.55</td><td>0.775 4</td><td></td><td>27.21</td><td>0.738 9</td><td></td><td>24.46</td><td>0.734 9</td></tr><tr><td colspan="2"><br />TNSR</td><td><br />3</td><td>34.13</td><td>0.924 5</td><td></td><td>30.10</td><td>0.835 7</td><td></td><td>28.95</td><td>0.800 0</td><td></td><td>27.62</td><td>0.839 4</td></tr><tr><td colspan="2"><br />SNSR</td><td><br />3</td><td>32.07</td><td>0.899 1</td><td></td><td>28.90</td><td>0.813 0</td><td></td><td>28.08</td><td>0.777 4</td><td></td><td>25.49</td><td>0.778 4</td></tr><tr><td rowspan="11"><br />本<br />文<br />方<br />法</td><td><br /><i>λ</i><sub>0</sub>=1,<i>λ</i><sub>1</sub>=1,<i>λ</i><sub>2</sub>=0,<i>λ</i><sub>3</sub>=0</td><td><br />3</td><td>32.39</td><td>0.905 6</td><td></td><td>29.13</td><td>0.818 6</td><td></td><td>28.22</td><td>0.782 3</td><td></td><td>25.79</td><td>0.789 5</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=1,<i>λ</i><sub>1</sub>=0,<i>λ</i><sub>2</sub>=1,<i>λ</i><sub>3</sub>=0</td><td><br />3</td><td>32.39</td><td>0.905 4</td><td></td><td>29.13</td><td>0.818 3</td><td></td><td>28.22</td><td>0.781 8</td><td></td><td>25.79</td><td>0.789 5</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=1,<i>λ</i><sub>1</sub>=0,<i>λ</i><sub>2</sub>=0,<i>λ</i><sub>3</sub>=1</td><td><br />3</td><td>32.17</td><td>0.902 0</td><td></td><td>28.98</td><td>0.815 5</td><td></td><td>28.12</td><td>0.779 2</td><td></td><td>25.62</td><td>0.784 1</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=1,<i>λ</i><sub>1</sub>=1,<i>λ</i><sub>2</sub>=1,<i>λ</i><sub>3</sub>=0</td><td><br />3</td><td>32.38</td><td>0.905 3</td><td></td><td>29.12</td><td>0.818 5</td><td></td><td>28.23</td><td>0.779 2</td><td></td><td>25.79</td><td>0.784 1</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=1,<i>λ</i><sub>1</sub>=1,<i>λ</i><sub>2</sub>=0,<i>λ</i><sub>3</sub>=1</td><td><br />3</td><td>32.22</td><td>0.902 3</td><td></td><td>29.03</td><td>0.817 0</td><td></td><td>28.18</td><td>0.781 3</td><td></td><td>25.69</td><td>0.786 2</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=1,<i>λ</i><sub>1</sub>=0,<i>λ</i><sub>2</sub>=1,<i>λ</i><sub>3</sub>=1</td><td><br />3</td><td>32.29</td><td>0.903 9</td><td></td><td>29.07</td><td>0.817 9</td><td></td><td>28.19</td><td>0.781 9</td><td></td><td>25.72</td><td>0.787 9</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=2,<i>λ</i><sub>1</sub>=1,<i>λ</i><sub>2</sub>=1,<i>λ</i><sub>3</sub>=0</td><td><br />3</td><td><b>32.43</b></td><td><b>0.906</b><b>2</b></td><td></td><td>29.14</td><td><b>0.819</b><b>2</b></td><td><b></b></td><td><b>28.25</b></td><td><b>0.783</b><b>0</b></td><td><b></b></td><td><b>25.82</b></td><td>0.790 6</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=2,<i>λ</i><sub>1</sub>=1,<i>λ</i><sub>2</sub>=0,<i>λ</i><sub>3</sub>=1</td><td><br />3</td><td>32.27</td><td>0.903 1</td><td></td><td>29.03</td><td>0.816 9</td><td></td><td>28.17</td><td>0.780 9</td><td></td><td>25.68</td><td>0.786 2</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=2,<i>λ</i><sub>1</sub>=0,<i>λ</i><sub>2</sub>=1,<i>λ</i><sub>3</sub>=1</td><td><br />3</td><td>32.32</td><td>0.904 4</td><td></td><td>29.08</td><td>0.817 7</td><td></td><td>28.21</td><td>0.781 7</td><td></td><td>25.74</td><td>0.788 2</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=1,<i>λ</i><sub>1</sub>=1,<i>λ</i><sub>2</sub>=1,<i>λ</i><sub>3</sub>=1</td><td><br />3</td><td>32.30</td><td>0.904 0</td><td></td><td>29.05</td><td>0.817 3</td><td></td><td>28.19</td><td>0.781 4</td><td></td><td>25.73</td><td>0.788 2</td></tr><tr><td><br /><i>λ</i><sub>0</sub>=3,<i>λ</i><sub>1</sub>=1,<i>λ</i><sub>2</sub>=1,<i>λ</i><sub>3</sub>=1</td><td><br />3</td><td>32.38</td><td>0.905 8</td><td></td><td><b>29.16</b></td><td>0.819 1</td><td></td><td><b>28.25</b></td><td>0.782 8</td><td></td><td>25.81</td><td><b>0.790</b><b>7</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:<i>λ</i><sub><i>i</i></sub>(<i>i</i>=0,1,2,3)是式(10)中的权重系数;加粗部分为学生网络最好的结果。</p>
                    <p class="img_note"></p>
                </div>
                <h3 id="106" name="106" class="anchor-tag">2 实验设置</h3>
                <h4 class="anchor-tag" id="107" name="107">2.1 <b>数据集和评价指标</b></h4>
                <div class="p1">
                    <p id="108">本文使用公开的基准数据集进行训练和测试,其中DIV2K数据集<citation id="235" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和Flickr2K数据集作为训练集。DIV2K数据集由800张图像组成,Flickr2K数据集由2 650张图像组成。 在测试阶段,Set5数据集<citation id="236" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、Set14数据集<citation id="237" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、BSDS100数据集<citation id="238" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>和Urban100数据集<citation id="239" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>用于SR基准测试。本文使用峰值信噪比(Peak Signal-to-Noise Ratio, PSNR)和结构相似性(Structural SIMilarity index, SSIM)来评估超分辨率重建结果。峰值信噪比经常用作测量图像退化等领域中图像重建质量的方法,是目前使用最广泛的客观图像质量评价标准,峰值信噪比的值越高表示超分辨率的重建效果越好。结构相似性对亮度、对比度和结构这三个因素进行整合来衡量两张图像之间的相似性,范围在[0, 1],当结构相似性的值为1时,说明两张图像是一致的。由于超分辨率模型是在YCbCr空间中的亮度通道Y进行训练的,因此本文仅在亮度通道Y上计算峰值信噪比和结构相似性。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">2.2 <b>参数设置</b></h4>
                <div class="p1">
                    <p id="110">对于训练数据集,首先用双三次降采样方法对其分别进行2倍、3倍、4倍的下采样,得到其对应的低分辨率图像,再把低分辨率图像裁剪成尺寸为40×40的非重叠图像块作为网络的输入,把其对应的高分辨率图像块作为训练目标。每个图像块都转换到YCbCr空间,并提取其中的Y通道进行训练。在教师网络中,卷积核大小设置为3×3,每层卷积的通道数设置为64。在学生网络中,深度卷积层的卷积核大小设置为3×3,并且输出通道的数量设置为64。当放大倍数为2时,教师网络和学生网络末端的反卷积层的卷积核大小设置为4×4,步长设置为2,输出通道设置为1;当放大倍数为3时,教师网络和学生网络末端的反卷积层的卷积核大小设置为3×3,步长设置为3,输出通道设置为1;当放大倍数为4时,需要在网络末端放置两个反卷积层,每一个反卷积层对特征图进行2倍放大,其中:第一个反卷积层的卷积核大小设置为4×4,步长设置为2,输出通道设置为32;第二个反卷积层的卷积核大小设置为4×4,步长设置为2,输出通道设置为1。在训练期间,所有实验都使用Adam<citation id="240" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>进行优化,并使用其默认参数<i>β</i><sub>1</sub>=0.9, <i>β</i><sub>2</sub>=0.999。学习率设定为固定值0.000 1,批量大小(batch size)设置为32。每组实验在NVIDIA Titan X GPU上使用Tensorflow进行500 000次训练迭代后进行测试。</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag">3 实验结果分析</h3>
                <div class="p1">
                    <p id="112">首先将单独训练教师网络和学生网络,训练教师网络的作用是在接下来的实验中对学生网络进行指导,训练学生网络的目的是与经过教师网络指导后的学生网络进行对比。在利用教师网络指导学生网络的训练过程中,利用测试集选取出教师网络最好的迭代次数,将低分辨率图像分别输入教师网络和学生网络,但教师网络的参数保持不变,经教师网络指导的学生网络不使用已经训练好的学生网络的参数,从第0次迭代次数开始训练。本文实验均采用峰值信噪比和结构相似性作为评价指标。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113">3.1 <b>不同级别的特征图的重要性</b></h4>
                <div class="p1">
                    <p id="114">本文分析了不同级别的特征图在知识蒸馏过程中的重要性,本节在低分辨率放大倍数为3的情况下进行对比实验。在式(10)中设置不同的权重进行比较,其对比结果如表1所示。从表1可以看出,传递第一级别和第二级别的特征图信息显著改善了超分辨率重建效果。这表明高级特征(第三级别的特征图)在低级计算机视觉任务(在本文的例子中是超分辨率)中用处较小。因此,本文的后续比较实验中,仅使用第一级别和第二级别的特征图作为知识蒸馏过程所传递的特征图。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">3.2 <b>不同知识蒸馏方式的比较</b></h4>
                <div class="p1">
                    <p id="116">本节对使用不同知识蒸馏方式的统计特征图性能在低分辨率放大倍数为3的情况下进行对比实验。使用式(6)计算4个统计特征图,其中系数分别设置为1、2、3和4。这些统计特征图在本文的工作中表示为<i><b>G</b></i><sub>mean</sub>、(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup>、(<i><b>G</b></i><sub>mean</sub>)<sup>3</sup>和(<i><b>G</b></i><sub>mean</sub>)<sup>4</sup>。使用式(7)～(9)计算另外3个统计特征图,并分别表示为(<i><b>G</b></i><sup>2</sup>)<sub>mean</sub>、<i><b>G</b></i><sub>max</sub>和<i><b>G</b></i><sub>min</sub>。从教师网络和学生网络中提取相同蒸馏方法的统计图进行传递。在训练学生网络期间,学生网络的统计特征图学习教师网络的统计特征图。图4显示了学生网络使用了不同的知识蒸馏方式在峰值信噪比的表现。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910003_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 学生网络使用不同知识蒸馏方法的峰值信噪比结果" src="Detail/GetImg?filename=images/JSJY201910003_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 学生网络使用不同知识蒸馏方法的峰值信噪比结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910003_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 PSNR results of student network using different knowledge distillation methods</p>

                </div>
                <div class="p1">
                    <p id="118">当本节使用张量中每个通道的值作为知识进行传递时(<i><b>G</b></i><sub>mean</sub>)<sup><i>p</i></sup>和(<i><b>G</b></i><sup><i>p</i></sup>)<sub>mean</sub>,学生网络利用到了特征图的所有信息,其重建效果要显著优于仅使用了张量通道中的部分值<i><b>G</b></i><sub>max</sub>和<i><b>G</b></i><sub>min</sub>的方法。在训练过程中,本文将训练数据集归一化为[0,1],在使用知识蒸馏训练学生网络的过程中,使用(<i><b>G</b></i><sub>mean</sub>)<sup>3</sup>和(<i><b>G</b></i><sub>mean</sub>)<sup>4</sup>作为统计特征图进行传递时,对[0,1]内的值求3次方或者4次方时,其计算结果极小,使用这两种方法的特征图间的损失函数在计算总损失函数时无法提供很多有用的信息。</p>
                </div>
                <div class="p1">
                    <p id="119">同时,使用(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup>作为统计特征图进行传递获得了最高的峰值信噪比。因此,本章的后续实验最终选择了(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup>来计算统计特征图。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">3.3 <b>知识蒸馏在超分辨率中的有效性</b></h4>
                <div class="p1">
                    <p id="121">本文在表2对有教师网络指导的学生网络(表示为<i>SNSR</i>(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup>)和没有教师网络指导的学生网络(表示为SNSR)的超分辨率重建效果在不同放大倍数的情况下进行了比较,并且在表中给出了Urban100测试集在iPhone X运行所需要的时间,同时将本文的结果与移动设备中的非深度学习快速超分辨率(Rapid and Accurate Super Image Resolution, RAISR)算法<citation id="241" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>进行比较。</p>
                </div>
                <div class="p1">
                    <p id="122">如表2所示,学生网络的重建效果在所有测试数据集上都有了提升:峰值信噪比在放大倍数为2的情况下的提升范围为0.24～0.52 dB;在放大倍数为3的情况下的提升范围为0.24～0.53 dB;在放大倍数为4的情况下的提升范围为0.10～0.25 dB,并且它们的每秒浮点运算次数(Flops)和参数量(Params)也没有改变。使用SNSR和SNSR(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup>的结果的视觉比较如图5所示,可以明显地看出超分辨率重建效果的显著提升。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910003_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 SNSR和SNSR(Gmean)2的视觉效果对比" src="Detail/GetImg?filename=images/JSJY201910003_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 SNSR和SNSR(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup>的视觉效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910003_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Comparison of visual effect of SNSR and SNSR(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup></p>

                </div>
                <div class="p1">
                    <p id="124">图5中:图(a)是原图,即测试集的高分辨率图像,在测试阶段,本文将该高分辨率图像先缩小到原来的1/3后得到低分辨率图像;图(b)是将该低分辨率图像经过双三次插值放大3倍的图像;图(c)是将低分辨率图像经过不使用知识蒸馏方法时的学生网络所得到的结果;图(d)是将低分辨率图像经过使用知识蒸馏方法时的学生网络所得到的结果。图(b)为使用传统的插值方法对低分辨图像进行放大,该方法所生成的图像在比较图中最模糊,视觉效果最差。图(c)与图(d)相比,线条的波纹感较重,图(d)重建的图像与原图最相似,说明用知识蒸馏方法可以有效地提升该小型网络的超分辨率重建效果。</p>
                </div>
                <div class="p1">
                    <p id="125">同时改变教师网络中卷积的通道数和学生网络中深度级卷积的通道数进行对比实验,来验证知识蒸馏在超分辨率问题中的可行性。对于其通道数<i>c</i>=64,进行了通道数分别为<i>c</i>=32,<i>c</i>=128和<i>c</i>=256的三组对比实验,其余参数设置以及实验条件不变。在放大倍数为3时,学生网络不同通道数的比较结果如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="127">从表3可以看出:当通道数发生改变时,学生网络的峰值信噪比会随着通道数的增加而增加,而经过教师网络指导的学生网络也可以从相应的教师网络中进行知识蒸馏,并从中受益获得更高的峰值信噪比。但是当通道数减少时,教师网络的特征图提供的知识信息较少,所以学习效果较为一般,而当通道数越来越大时,学生网络与教师网络的差距会逐渐变小,所以从教师网络可以学到的知识也逐渐减少,峰值信噪比的提升值逐渐降低。</p>
                </div>
                <div class="area_img" id="128">
                                            <p class="img_tit">
                                                <b>表</b>2 SNSR<b>和</b>SNSR(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup><b>在不同放大倍数结果对比</b>
                                                    <br />
                                                Tab.2 Result comparison of SNSR and SNSR(<i><b>G</b></i><sub>mean</sub>)<sup>2</sup> at different magnification times
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910003_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201910003_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910003_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 SNSR和SNSR(Gmean)2在不同放大倍数结果对比" src="Detail/GetImg?filename=images/JSJY201910003_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>
                                <p class="img_note">注:加粗部分为同一放大倍数时学生网络最好的结果。</p>

                </div>
                <div class="area_img" id="129">
                                            <p class="img_tit">
                                                <b>表</b>3 <b>不同通道数的知识蒸馏效果比较</b>
                                                    <br />
                                                Tab.3 Result comparison of knowledge distillation with different channels
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910003_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201910003_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910003_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 不同通道数的知识蒸馏效果比较" src="Detail/GetImg?filename=images/JSJY201910003_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>
                                <p class="img_note">注:加粗部分为同一放大倍数时学生网络最好的结果。</p>

                </div>
                <h3 id="130" name="130" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="131">本文采用知识蒸馏将知识从教师网络转移到学生网络,在不改变其结构、不增加计算成本的前提下,大幅提高了学生网络的图像超分辨率重建效果,满足终端设备实时计算的要求。为了确定在教师网络和学生网络之间传递信息的有效方式,从两个网络中提取不同的统计图并进行比较。此外,学生网络利用深度可分离卷积来降低SR的计算成本,从而可以在低功率设备上部署超分辨率重建模型。但是学生网络未能完全学习到教师网络的知识,学生网络的重建效果与教师网络的重建效果依旧有明显的差距。在将来研究工作中,将研究构建更小、更高效的超分辨率神经网络。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="164">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a deep convolutional network for image super-resolution">

                                <b>[1]</b>DONG C,LOY C C,HE K,et al.Learning a deep convolutional network for image super-resolution[C]//Proceedings of the 2014European Conference on Computer Vision,LNCS 8692.Berlin:Springer,2014:184-199.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate Image Super-Resolution Using Very Deep Convolutional Networks">

                                <b>[2]</b>KIM J,LEE J K,LEE K M.Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the 2016IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1646-1654.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[3]</b>SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-01-10].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">

                                <b>[4]</b>LIM B,SON S,KIM H,et al.Enhanced deep residual networks for single image super-resolution[C]//Proceedings of the 2017IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2017:1132-1140.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo-realistic single image super-resolution using a generative adversarial network">

                                <b>[5]</b>LEDIG C,THEIS L,HUSZR F,et al.Photo-realistic single image super-resolution using a generative adversarial network[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:105-114.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[6]</b>HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Residual dense network for image super-resolution">

                                <b>[7]</b>ZHANG Y,TIAN Y,KONG Y,et al.Residual dense network for image super-resolution[C]//Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2472-2481.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Super-resolution Using Very Deep Residual Channel Attention Networks">

                                <b>[8]</b>ZHANG Y,LI K,LI K,et al.Image super-resolution using very deep residual channel attention networks[C]//Proceedings of the2018 European Conference on Computer Vision,LNCS 11211.Berlin:Springer,2018:294-310.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017857187.nh&amp;v=MTI4NDZ1OUdkREVxSkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFU3dk1WRjI2R2I=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>陈伟杰.卷积神经网络的加速及压缩[D].广州:华南理工大学,2017:1-7.(CHEN W J.The acceleration and compression of convolutional neural networks[D].Guangzhou:South China University of Technology,2017:1-7.)
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distilling the knowledge in a neural network">

                                <b>[10]</b>HINTON G,VINYALS O,DEAN J.Distilling the knowledge in a neural network[EB/OL].[2019-01-10].https://arxiv.org/pdf/1503.02531.pdf.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fit Nets:hints for thin deep nets">

                                <b>[11]</b>REMERO A,BALLAS N,KAHOU S E,et al.Fit Nets:hints for thin deep nets[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6550v2.pdf.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobile Nets:efficient convolutional neural networks for mobile vision applications">

                                <b>[12]</b>HORWARD A G,ZHU M,CHEN B,et al.Mobile Nets:efficient convolutional neural networks for mobile vision applications[EB/OL].[2019-01-10].https://arxiv.org/pdf/1704.04861.pdf.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating the Super-Resolution Convolutional Neural Network">

                                <b>[13]</b>DONG C,LOY C C,TANG X.Accelerating the super-resolution convolutional neural network[C]//Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:391-407.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution">

                                <b>[14]</b>LAI W,HUANG J,AHUJA N,et al.Deep Laplacian pyramid networks for fast and accurate super resolution[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:5835-5843.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution Using Dense Skip Connections">

                                <b>[15]</b>TONG T,LI G,LIU X,et al.Image super-resolution using dense skip connections[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:4809-4817.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer">

                                <b>[16]</b>ZAGORUYKO S,KOMODAKIS N.Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer[EB/OL].[2019-01-10].https://arxiv.org/pdf/1612.03928.pdf.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NTIRE 2017 challenge on single image super-resolution:dataset and study">

                                <b>[17]</b>AGUSTSSON E,TIMOFTE R.NTIRE 2017 challenge on single image super-resolution:dataset and study[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2017:1122-1131.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Low-complexity single image super-resolution based on nonnegative neighbor embedding">

                                <b>[18]</b>BEVILACQUA M,ROUMY A,GUILLEMOT C,et al.Low-complexity single image super-resolution based on nonnegative neighbor embedding[EB/OL].[2019-01-10].http://people.rennes.inria.fr/Aline.Roumy/publi/12bmvc_Bevilacqua_low Complexity SR.pdf.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                ZEYDE R,ELAD M,PROTTER M.On single image scale-up using sparse-representations[C]//Proceedings of the 2010 International Conference on Curves and Surfaces,LNCS 6920.Berlin:Springer,2010:711-730.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics">

                                <b>[20]</b>MARTIN D,FOWLKES C,TAL D,et al.A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Proceedings of the 8th IEEE International Conference on Computer Vision.Piscataway:IEEE,2001:416-423.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single image super-resolution from transformed self-exemplars">

                                <b>[21]</b>HUANG J,SINGH A,AHUJA N.Single image super-resolution from transformed self-exemplars[C]//Proceedings of the 2015IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:5197-5206.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A Method for Stochastic Optimization[C/OL]">

                                <b>[22]</b>KINGMA D P,BA J L.Adam:a method for stochastic optimization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6980.pdf.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RAISR:rapid and accurate image super resolution">

                                <b>[23]</b>ROMANO Y,ISIDORO J,MILANFAR P.RAISR:rapid and accurate image super resolution[J].IEEE Transactions on Computational Imaging,2017,3(1):110-125.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910003" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910003&amp;v=MTQ3MTJyNDlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFU3dk1MejdCZDdHNEg5ak4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
