<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136674842971250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906015%26RESULT%3d1%26SIGN%3dtJfFbzL3Ht8MWcleC85oAYQlNvU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906015&amp;v=MTEwMjl1WnNGeS9oV3I3QUx6N0JkN0c0SDlqTXFZOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="1 国内外研究现状 ">1 国内外研究现状</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="1.1 &lt;b&gt;传统问答方法&lt;/b&gt;">1.1 <b>传统问答方法</b></a></li>
                                                <li><a href="#65" data-title="1.2 &lt;b&gt;深度学习在问答匹配任务中的应用&lt;/b&gt;">1.2 <b>深度学习在问答匹配任务中的应用</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 AMCNNs ">2 AMCNNs</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="2.1 &lt;b&gt;字符的分布式表示&lt;/b&gt;">2.1 <b>字符的分布式表示</b></a></li>
                                                <li><a href="#83" data-title="2.2 &lt;b&gt;多尺度卷积神经网络架构&lt;/b&gt;">2.2 <b>多尺度卷积神经网络架构</b></a></li>
                                                <li><a href="#111" data-title="2.3 &lt;b&gt;基于注意力机制的多尺度卷积神经网络架构&lt;/b&gt;">2.3 <b>基于注意力机制的多尺度卷积神经网络架构</b></a></li>
                                                <li><a href="#124" data-title="2.4 &lt;b&gt;目标函数&lt;/b&gt;">2.4 <b>目标函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="3 数据集和实验设置 ">3 数据集和实验设置</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#130" data-title="3.1 &lt;i&gt;cMedQA&lt;/i&gt;&lt;b&gt;数据集&lt;/b&gt;">3.1 <i>cMedQA</i><b>数据集</b></a></li>
                                                <li><a href="#135" data-title="3.2 &lt;b&gt;评价方法&lt;/b&gt;">3.2 <b>评价方法</b></a></li>
                                                <li><a href="#142" data-title="3.3 &lt;b&gt;基线模型&lt;/b&gt;">3.3 <b>基线模型</b></a></li>
                                                <li><a href="#150" data-title="3.4 &lt;b&gt;实验设置&lt;/b&gt;">3.4 <b>实验设置</b></a></li>
                                                <li><a href="#153" data-title="3.5 &lt;b&gt;结果分析&lt;/b&gt;">3.5 <b>结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="图1 词的分布式表示">图1 词的分布式表示</a></li>
                                                <li><a href="#79" data-title="图2 字的分布式表示">图2 字的分布式表示</a></li>
                                                <li><a href="#96" data-title="图3 多尺度卷积神经网络架构">图3 多尺度卷积神经网络架构</a></li>
                                                <li><a href="#114" data-title="图4 MultiCNNs架构提取上下文信息的过程">图4 MultiCNNs架构提取上下文信息的过程</a></li>
                                                <li><a href="#115" data-title="图5 基于注意力机制的多尺度卷积神经网络 (AMCNNs) 架构">图5 基于注意力机制的多尺度卷积神经网络 (AMCNNs) 架构</a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;i&gt;cMedQA&lt;/i&gt;&lt;b&gt;数据集的统计信息&lt;/b&gt;"><b>表</b>1 <i>cMedQA</i><b>数据集的统计信息</b></a></li>
                                                <li><a href="#158" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;各模型的&lt;/b&gt;&lt;i&gt;Top&lt;/i&gt;- 1&lt;b&gt;准确率结果&lt;/b&gt;"><b>表</b>2 <b>各模型的</b><i>Top</i>- 1<b>准确率结果</b></a></li>
                                                <li><a href="#163" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同尺度卷积核的实验结果&lt;/b&gt;"><b>表</b>3 <b>不同尺度卷积核的实验结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="205">


                                    <a id="bibliography_1" title="FENG M W, XIANG B, GLASS M R, et al.Applying deep learning to answer selection:a study and an open task[C]//Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2015:813-820." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Applying deep learning to answer selection:a study and an open task">
                                        <b>[1]</b>
                                        FENG M W, XIANG B, GLASS M R, et al.Applying deep learning to answer selection:a study and an open task[C]//Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2015:813-820.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_2" title="TAN M, dos SANTOS C N, XIANG B, et al.Improved representation learning for question answer matching[C]//ACL 2016:Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.Cambridge, CA:MIT Press, 2016:464-473." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved Representation Learning for Question Answer Matching">
                                        <b>[2]</b>
                                        TAN M, dos SANTOS C N, XIANG B, et al.Improved representation learning for question answer matching[C]//ACL 2016:Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.Cambridge, CA:MIT Press, 2016:464-473.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_3" title="QIU X P, HUANG X J.Convolutional neural tensor network architecture for community-based question answering[C]//IJCAI2015:Proceedings of the 24th International Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:1305-1311." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Tensor Network Architecture for Community-Based Question Answering">
                                        <b>[3]</b>
                                        QIU X P, HUANG X J.Convolutional neural tensor network architecture for community-based question answering[C]//IJCAI2015:Proceedings of the 24th International Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:1305-1311.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_4" title="YIN W P, SCHTZE H, XIANG B, et al.ABCNN:attentionbased convolutional neural network for modeling sentence pairs[EB/OL].[2018-08-20].http://cn.arxiv.org/abs/1512.05193.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ABCNN:attentionbased convolutional neural network for modeling sentence pairs">
                                        <b>[4]</b>
                                        YIN W P, SCHTZE H, XIANG B, et al.ABCNN:attentionbased convolutional neural network for modeling sentence pairs[EB/OL].[2018-08-20].http://cn.arxiv.org/abs/1512.05193.pdf.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_5" title="JAIN S, DODIYA T.Rule based architecture for medical question answering system[C]//Soc Pro S 2012:Proceedings of the Second International Conference on Soft Computing for Problem Solving, AISC 236.Berlin:Springer, 2014:1225-1233." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rule based architecture for medical question answering system">
                                        <b>[5]</b>
                                        JAIN S, DODIYA T.Rule based architecture for medical question answering system[C]//Soc Pro S 2012:Proceedings of the Second International Conference on Soft Computing for Problem Solving, AISC 236.Berlin:Springer, 2014:1225-1233.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_6" title="WANG J, MAN C T, ZHAO Y F, et al.An answer recommendation algorithm for medical community question answering systems[C]//SOLI 2016:Proceedings of the 2016 IEEE International Conference on Service Operations and Logistics, and Informatics.Piscataway, NJ:IEEE, 2016:139-144." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An answer recommendation algorithm for medical community question answering systems">
                                        <b>[6]</b>
                                        WANG J, MAN C T, ZHAO Y F, et al.An answer recommendation algorithm for medical community question answering systems[C]//SOLI 2016:Proceedings of the 2016 IEEE International Conference on Service Operations and Logistics, and Informatics.Piscataway, NJ:IEEE, 2016:139-144.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_7" title="ABACHA A B, ZWEIGENBAUM P.Medical question answering:translating medical questions into SPARQL queries[C]//Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium.New York:ACM, 2012:41-50." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Medical question answering:translating medical questions into sparql queries">
                                        <b>[7]</b>
                                        ABACHA A B, ZWEIGENBAUM P.Medical question answering:translating medical questions into SPARQL queries[C]//Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium.New York:ACM, 2012:41-50.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_8" title="ABACHA A B, ZWEIGENBAUM P.MEANS:a medical questionanswering system combining NLP techniques and semantic Web technologies[J].Information Processing&amp;amp;Management, 2015, 51 (5) :570-594." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122600239096&amp;v=MDUzNjdkYXhvPU5pZk9mYks5SDlQT3FZOUZadWdHREhVL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxbw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        ABACHA A B, ZWEIGENBAUM P.MEANS:a medical questionanswering system combining NLP techniques and semantic Web technologies[J].Information Processing&amp;amp;Management, 2015, 51 (5) :570-594.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_9" title="LI T C, HAO Y, ZHU X Y, et al.A Chinese question answering system for specific domain[C]//WAIM 2014:Proceedings of the International Conference on Web-Age Information Management.Berlin:Springer, 2014:590-601." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Chinese question answering system for specific domain">
                                        <b>[9]</b>
                                        LI T C, HAO Y, ZHU X Y, et al.A Chinese question answering system for specific domain[C]//WAIM 2014:Proceedings of the International Conference on Web-Age Information Management.Berlin:Springer, 2014:590-601.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_10" title="YIN Y S, ZHANG Y, LIU X, et al.Health QA:a Chinese QAsummary system for smart health[C]//CSH 2014:Proceedings of the 2nd International Conference on Smart Health, LNCS 8549.Cham:Springer, 2014:51-62." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Health QA:a Chinese QAsummary system for smart health">
                                        <b>[10]</b>
                                        YIN Y S, ZHANG Y, LIU X, et al.Health QA:a Chinese QAsummary system for smart health[C]//CSH 2014:Proceedings of the 2nd International Conference on Smart Health, LNCS 8549.Cham:Springer, 2014:51-62.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_11" title="WANG B Y, NIU J B, MA L Q, et al.A Chinese question answering approach integrating count-based and embedding-based features[C]//Proceedings of the 2016 International Conference on Computer Processing of Oriental Languages, National CCF Conference on Natural Language Processing and Chinese Computing, LNCS 10102.Cham:Springer, 2016:934-941." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Chinese Question Answering Approach Integrating Count-Based and Embedding-Based Features">
                                        <b>[11]</b>
                                        WANG B Y, NIU J B, MA L Q, et al.A Chinese question answering approach integrating count-based and embedding-based features[C]//Proceedings of the 2016 International Conference on Computer Processing of Oriental Languages, National CCF Conference on Natural Language Processing and Chinese Computing, LNCS 10102.Cham:Springer, 2016:934-941.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_12" title="HU B T, LU Z D, LI H, et al.Convolutional neural network architectures for matching natural language sentences[C]//NIPS2014:Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge, CA:MIT Press, 2014:2042-2050." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">
                                        <b>[12]</b>
                                        HU B T, LU Z D, LI H, et al.Convolutional neural network architectures for matching natural language sentences[C]//NIPS2014:Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge, CA:MIT Press, 2014:2042-2050.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_13" title="ZHOU X Q, HU B T, CHEN Q C, et al.Answer sequence learning with neural networks for answer selection in community question answering[EB/OL].[2018-08-14].https://arxiv.org/abs/1506.06490.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Answer sequence learning with neural networks for answer selection in community question answering">
                                        <b>[13]</b>
                                        ZHOU X Q, HU B T, CHEN Q C, et al.Answer sequence learning with neural networks for answer selection in community question answering[EB/OL].[2018-08-14].https://arxiv.org/abs/1506.06490.pdf.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_14" title="TAN M, dos SANTOS C N, XIANG B, et al.LSTM-based deep learning models for non-factoid answer selection[EB/OL].[2018-08-20].https://arxiv.org/abs/1511.04108.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSTM-based deep learning models for non-factoid answer selection">
                                        <b>[14]</b>
                                        TAN M, dos SANTOS C N, XIANG B, et al.LSTM-based deep learning models for non-factoid answer selection[EB/OL].[2018-08-20].https://arxiv.org/abs/1511.04108.pdf.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_15" title="ZHANG S, ZHANG X, WANG H, et al.Chinese medical question answer matching using end-to-end character-level multi-scale CNNs[J].Applied Sciences, 2017, 7 (8) :767." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs">
                                        <b>[15]</b>
                                        ZHANG S, ZHANG X, WANG H, et al.Chinese medical question answer matching using end-to-end character-level multi-scale CNNs[J].Applied Sciences, 2017, 7 (8) :767.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_16" title="BENGIO Y, DUCHARME R, VINCENT P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3 (6) :1137-1155." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Neural Probabilistic Language Model">
                                        <b>[16]</b>
                                        BENGIO Y, DUCHARME R, VINCENT P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3 (6) :1137-1155.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_17" title="MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[EB/OL].[2018-08-23].http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">
                                        <b>[17]</b>
                                        MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[EB/OL].[2018-08-23].http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf.
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_18" title="WANG Z G, HAMZA W, FLORIAN R.Bilateral multi-perspective matching for natural language sentences[EB/OL].[2018-08-20].https://arxiv.org/abs/1702.03814.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bilateral multi-perspective matching for natural language sentences">
                                        <b>[18]</b>
                                        WANG Z G, HAMZA W, FLORIAN R.Bilateral multi-perspective matching for natural language sentences[EB/OL].[2018-08-20].https://arxiv.org/abs/1702.03814.pdf.
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_19" title="TADDY M.Document classification by inversion of distributed language representations[EB/OL].[2018-08-20].https://arxiv.org/abs/1504.07295.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Document classification by inversion of distributed language representations">
                                        <b>[19]</b>
                                        TADDY M.Document classification by inversion of distributed language representations[EB/OL].[2018-08-20].https://arxiv.org/abs/1504.07295.pdf.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_20" title="LIN Y K, LIU Z Y, SUN M S, et al.Learning entity and relation embeddings for knowledge graph completion[C]//AAAI 2014:Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI, 2015:2181-2187." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Learning entity and relation embeddings for knowledge graph completion,&amp;quot;">
                                        <b>[20]</b>
                                        LIN Y K, LIU Z Y, SUN M S, et al.Learning entity and relation embeddings for knowledge graph completion[C]//AAAI 2014:Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI, 2015:2181-2187.
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_21" title="ZHANG M S, ZHANG Y, CHE W X, et al.Character-level Chinese dependency parsing[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.Cambridge, MA:MIT Press, 2014:1326-1336." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Character-level chinese dependency parsing">
                                        <b>[21]</b>
                                        ZHANG M S, ZHANG Y, CHE W X, et al.Character-level Chinese dependency parsing[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.Cambridge, MA:MIT Press, 2014:1326-1336.
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_22" title="CHUNG J, CHO K, BENGIO Y.A character-level decoder without explicit segmentation for neural machine translation[EB/OL].[2018-08-15].https://arxiv.org/abs/1603.06147.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A character-level decoder without explicit segmentation for neural machine translation">
                                        <b>[22]</b>
                                        CHUNG J, CHO K, BENGIO Y.A character-level decoder without explicit segmentation for neural machine translation[EB/OL].[2018-08-15].https://arxiv.org/abs/1603.06147.pdf.
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_23" title="ZHANG X, ZHAO J B, LECUN Y.Character-level convolutional networks for text classification[C]//NIPS 2015:Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:649-657." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Character-level Convolutional Networks for Text Classification">
                                        <b>[23]</b>
                                        ZHANG X, ZHAO J B, LECUN Y.Character-level convolutional networks for text classification[C]//NIPS 2015:Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:649-657.
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_24" title="GOLUB D, HE X D.Character-level question answering with attention[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.Cambridge, MA:MITPress, 2016:1598-1607." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Character-level question answering with attention">
                                        <b>[24]</b>
                                        GOLUB D, HE X D.Character-level question answering with attention[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.Cambridge, MA:MITPress, 2016:1598-1607.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-29 10:10</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1639-1645 DOI:10.11772/j.issn.1001-9081.2018102184            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于注意力和字嵌入的中文医疗问答匹配方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%BF%97%E8%B1%AA&amp;code=41988795&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈志豪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%99%E7%BF%94&amp;code=22414787&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">余翔</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%AD%90%E8%BE%B0&amp;code=32199509&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘子辰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B1%E5%A4%A7%E4%BC%9F&amp;code=39421504&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邱大伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A1%BE%E6%9C%AC%E5%88%9A&amp;code=40566403&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">顾本刚</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学通信与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A7%BB%E5%8A%A8%E8%AE%A1%E7%AE%97%E4%B8%8E%E6%96%B0%E5%9E%8B%E7%BB%88%E7%AB%AF%E5%8C%97%E4%BA%AC%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E6%89%80)&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">移动计算与新型终端北京重点实验室(中国科学院计算技术研究所)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对当前的分词工具在中文医疗领域无法有效切分出所有医学术语, 且特征工程需消耗大量人力成本的问题, 提出了一种基于注意力机制和字嵌入的多尺度卷积神经网络建模方法。该方法使用字嵌入结合多尺度卷积神经网络用以提取问题句子和答案句子不同尺度的上下文信息, 并引入注意力机制来强调问题和答案句子之间的相互影响, 该方法能有效学习问题句子和正确答案句子之间的语义关系。由于中文医疗领域问答匹配任务没有标准的评测数据集, 因此使用公开可用的中文医疗问答数据集 (cMedQA) 进行评测, 实验结果表明该方法优于词匹配、字匹配和双向长短时记忆神经网络 (BiLSTM) 建模方法, 并且Top-1准确率为65.43%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自然语言处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%97%AE%E7%AD%94%E5%AF%B9%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">问答对匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%97%E5%B5%8C%E5%85%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">字嵌入;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *陈志豪 (1994—) , 男, 重庆人, 硕士研究生, 主要研究方向:自然语言处理、智能问答系统;chenzhihao@ ict. ac. cn;
                                </span>
                                <span>
                                    余翔 (1964—) , 男, 四川成都人, 正高级工程师, 主要研究方向:数字通信、无线信号处理;;
                                </span>
                                <span>
                                    刘子辰 (1985—) , 男, 山东临沂人, 助理研究员, 主要研究方向:网络通信、大数据挖掘;;
                                </span>
                                <span>
                                    邱大伟 (1991—) , 男, 内蒙古赤峰市人, 博士研究生, 主要研究方向:模式识别、机器学习、自然语言处理;;
                                </span>
                                <span>
                                    顾本刚 (1992—) , 男, 安徽淮南人, 硕士研究生, 主要研究方向:网络通信。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-31</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重大科技专项 (2016ZX03002010-003);</span>
                    </p>
            </div>
                    <h1><b>Chinese medical question answer matching method based on attention mechanism and character embedding</b></h1>
                    <h2>
                    <span>CHEN Zhihao</span>
                    <span>YU Xiang</span>
                    <span>LIU Zichen</span>
                    <span>QIU Dawei</span>
                    <span>GU Bengang</span>
            </h2>
                    <h2>
                    <span>School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications</span>
                    <span>Beijing Key Laboratory of Mobile Computing and Pervasive Device (Institute of Computing Technology, Chinese Academy of Sciences)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problems that the current word segmentation tool can not effectively distinguish all medical terms in Chinese medical field, and feature engineering has high labor cost, a multi-scale Convolutional Neural Network (CNN) modeling method based on attention mechanism and character embedding was proposed. In the proposed method, character embedding was combined with multi-scale CNN to extract context information at different scales of question and answer sentences, and attention mechanism was introduced to emphasize the interaction between question sentences and answer sentences, meanwhile the semantic relationship between the question sentence and the correct answer sentence was able to be effectively learned. Since the question and answer matching task in Chinese medical field does not have a standard evaluation dataset, the proposed method was evaluated using the publicly available Chinese Medical Question and Answer dataset (cMedQA) . The experimental results show that the proposed method is superior to word matching, character matching and Bi-directional Long Short-Term Memory network (BiLSTM) modeling method, and the Top-1 accuracy is 65.43 %.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=natural%20language%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">natural language processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=question%20answer%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">question answer matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=character%20embedding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">character embedding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Zhihao, born in 1994, M. S. candidate. His research interests include natural language processing, intelligent question answer system. ;
                                </span>
                                <span>
                                    YU Xiang, born in 1964, Ph. D. , senior engineer. His reasearch interest includes digital communication, wireless signal processing. ;
                                </span>
                                <span>
                                    LIU Zichen, born in 1985, Ph. D. , research assistant. His research interests include telecommunication, big data mining. ;
                                </span>
                                <span>
                                    QIU Dawei, born in 1991, Ph. D. candidate. His research interests include pattern recognition, machine learning, natural language processing. ;
                                </span>
                                <span>
                                    GU Bengang, born in 1994, M. S. candidate. His research interest includes telecommunication.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-31</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by National Science and Technology Major Project (2016ZX03002010-003);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="52">随着互联网的快速发展, 愈来愈多的人倾向于在健康医疗网站上提问来寻求健康帮助, 例如中国的寻医问药网、39健康网和丁香园等。此类网站为患者和医生提供了一个在线交流的平台, 便于用户随时随地获取高质量的医疗健康推荐。患者只需描述其自身的症状并发布问题, 就能得到指定的医生或任意医生的回复和建议。然而, 大多数情况下, 许多用户提出的问题都相似, 这一方面给医生专家带来了巨大的回复负担, 另一方面延长了患者等待回复的时间。因此, 为了提高用户体验, 有必要设计一种方法来有效地处理医疗问答匹配的问题, 即从已有的医疗答复记录中自动选择与用户问题匹配最佳的答复推荐给用户。</p>
                </div>
                <div class="p1">
                    <p id="53">本文重点关注的是中文医疗问答匹配和答案选择的问题, 其中所考虑的问答语言均限于中文。相比于Feng等<citation id="253" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和Tan等<citation id="254" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>在英语语言环境下的开放领域问答匹配的研究, 本文所讨论的问题更具挑战性, 原因有两点:1) 领域受限性质;2) 中文语言具有一些特殊的特征。</p>
                </div>
                <div class="p1">
                    <p id="54">进一步的讨论如下:</p>
                </div>
                <div class="p1">
                    <p id="55">首先, 由于汉语是以字为基本的书写单位, 词语之间没有明显的区分标记, 因此分词是大多数中文自然语言处理 (Natural Language Processing, NLP) 任务中不可或缺的数据预处理步骤, 如词性标注 (Part-of-Speech tagging, POS) 和语义分析。由此可见, 分词的准确与否大大影响了下游任务的准确性。尽管已有的分词工具 (如:ICTCLAS、jieba和HanLP) 的性能已经达到了满足大多数实际应用的水平, 但它们一旦发生偏差, 将通过管道不可避免的影响整个系统框架, 导致整体性能下降<citation id="255" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。此外, 当直接应用于医学文本时, 包含的各种医学术语会导致这些通用分词工具的性能进一步下降。例如, 药物名称“活血止痛片”和“维生素C黄连上清片”被jieba分词工具错误的划分为了“活血 止痛 片”和“维生素 C 黄连 上 清 片”。尽管引入特定领域的词典可以减轻专业术语对分词的负面影响, 但构建此类词典似乎总是令人望而却步, 因为它涉及大量的手工劳动并需要大量特定领域的专业知识。更糟糕的是, 在处理在线社区发布的未经编辑的问题和答案时, 预定义的词典往往不合适。因为它们通常是以非正式的表达形式编写的, 往往包含许多简写词和非标准的缩略词, 甚至是错误的拼写和不合适的语法结构的句子。例如, 问句“嘴骑魔拖车摔肿了怎么消下去现在还疼不能吃饭感觉越来越大也不能喝水”将“摩托车”误写成了“魔拖车”, 另外全句没有任何标点符号, 逻辑表达混乱。虽然, 通用分词工具都能够加载定制的领域词典, 但是定制词典需要耗费大量的人工时间, 且定制的词典也不可能覆盖所有的领域词。</p>
                </div>
                <div class="p1">
                    <p id="56">为了避免上述问题, 提出采用字嵌入的端到端的神经网络框架。该框架采用的是字级的表示, 即用字嵌入方式替代传统的词嵌入方式。此种方式既可以避免数据预处理时的分词步骤, 也可以避免由分词错误引起的其他组件的性能下降。问题和答案的表示向量分别使用中文字进行预训练得到, 并且类似于词嵌入, 将每个字描述为固定长度的向量。</p>
                </div>
                <div class="p1">
                    <p id="57">由于在中文语言中字所含的语义信息比词语的语义信息少, 若采用统计方法则可能需要使用语言模型或词性标注等方式来抽取相关的语义信息。然而, 卷积神经网络 (Convolutional Neural Network, CNN) 强调<i>N</i>-Gram内的本地交互, 能够自动捕获字和词语的局部语义信息, 无需其他方法辅助, 因此本文引入CNN来构建模型。又因为中文词语或短语通常由2至5个字构成, 所以采用多尺度卷积神经网络 (Multi-scale CNNs, MultiCNNs) 来提取不同尺度的上下文信息, 由此可以更好地编码问题和答案。因此, 本文提到的MultiCNNs模型由一组不同尺度的卷积核组成。</p>
                </div>
                <div class="p1">
                    <p id="58">大多数之前的工作都是将问题和答案两个句子分别表示, 很少考虑一个句子对另一个句子的影响。这忽略了两个句子在同一任务背景下的相互影响, 也与人类在比较两个句子时的行为相矛盾。人们通常从另一个句子中提取与身份、同义词、反义词和其他关系相关的部分来找到一句话中的关键部分。受Yin等<citation id="256" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出的基于注意力 (Attention) 机制对句子对联合建模的方法的启发, 本文引入Attention机制将问题和答案两个句子一起建模, 用一个句子的内容来指导另一个句子的表示。因此, 本文提出了基于注意力和字嵌入的卷积神经网络 (CNNs based on Attention Mechanism and character embedding, AMCNNs) 框架。</p>
                </div>
                <div class="p1">
                    <p id="59">答案推荐是问答系统的目标。对于每个问题描述, 答案候选池都包含100个候选答案, 其中有一个或多个相关答案和多个不相关答案。问答匹配任务的目标就是从候选答案中找出精确度分数最高 (Top-1) 的一个答案, 并将其推荐给用户。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">1 国内外研究现状</h3>
                <h4 class="anchor-tag" id="61" name="61">1.1 <b>传统问答方法</b></h4>
                <div class="p1">
                    <p id="62">Jain等<citation id="257" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了基于规则的医学领域问答系统架构, 并详细讨论了基于规则的问题处理和答案检索的复杂性。然而, 由于用户问题总是以大量不同的方式呈现, 因此基于规则的方法可能无法涵盖所有表达方式。</p>
                </div>
                <div class="p1">
                    <p id="63">Wang等<citation id="258" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了另一种方法, 首先将句子划分成单词以训练每个句子的词向量, 然后通过计算每个单词之间的相似性来评估每个问答对的相似性。而Abacha等<sup></sup><citation id="259" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>则是将问题翻译成机器可读的表示形式。 因此, 该方法能够将各种自然语言表达问题转换为标准的表示形式。 后来, Abacha等<sup></sup><citation id="260" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>通过在表示和查询层应用语义技术来扩展他们以前的工作, 以便创建结构化查询以匹配知识库中的条目。这些方法取决于手工设计的模式和特征, 需要巨大的人力和专业知识。</p>
                </div>
                <div class="p1">
                    <p id="64">现有研究提出了一些关于中文问答的模型。 Li等<citation id="261" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>构建了一个用于音乐领域的语义匹配模型, 能够自动将问题翻译成SPARQL查询语句来获得最终答案。Yin等<citation id="262" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>针对在线健康专家问答服务效率低下的问题, 开发了用于对相似的问题和答案进行分组的分层聚类方法和用于检索相关答案的扩展相似性评估算法, 用于从已有的专家答案中进一步提取出答案。然而, 这些方法都将分词作为中文文本处理的必要步骤, 虽然在一般领域中分词工具能达到研究者们的期望, 但它们并未考虑误差所带来的影响。 Wang等<citation id="263" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation> 提出了一种集成基于计数和基于嵌入的特征的方法;他们还在研究中指出, 基于字的模型优于基于词的模型, 从中得到启示:处理汉字可以避免分词错误带来的不利影响。</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">1.2 <b>深度学习在问答匹配任务中的应用</b></h4>
                <div class="p1">
                    <p id="66">近年来, 由于深度学习技术无需任何语言工具、特征工程或其他资源, 因此, 愈来愈多的自然语言处理 (NLP) 任务都采用了该技术。</p>
                </div>
                <div class="p1">
                    <p id="67">Feng等<citation id="264" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>设计了6个深度学习模型, 并在保险领域进行了问答匹配的实验, 该实验结果为其他问答匹配任务研究者提供了有价值的指导 (例如, 卷积层后不需要全连接层) 。Hu等<citation id="265" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了两种不同的卷积模型来学习句子的表示, 这是使用神经网络解决一般句子匹配问题的先驱工作。之后, Feng等<citation id="266" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和Zhou等<citation id="267" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>采用CNNs来学习问答对的表示, 进一步用于计算不同问题与候选答案之间的相似性。后来, 为了从句子中提取序列信息, Tan等<citation id="270" type="reference"><link href="207" rel="bibliography" /><link href="231" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">14</a>]</sup></citation>利用递归神经网络 (Recurrent Neural Network, RNN) 及其变体长短期记忆网络 (Long Short-Term Memory network, LSTM) 来学习句子级表示;值得注意的是, 他们还利用注意力机制来增强问题和答案之间的语义关联。Yin等<citation id="268" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>设计了3种基于注意力机制的卷积神经网络 (Attention-Based CNN, ABCNN) 来建模问答对, 并分别在答案选择 (Answer Selection, AS) 、释义识别 (Paraphrase Identification, PI) 和文本蕴含 (Textual Entailment, TE) 3个任务上进行了模型验证。Zhang等<citation id="269" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出端到端的字嵌入多尺度卷积神经网络模型, 用于医疗领域的问答匹配任务。</p>
                </div>
                <div class="p1">
                    <p id="68">但是, 上述所有研究都与英文文本相关。当直接用于处理中文文档时, 所提出方法的性能会出现相当大的下降, 是由于中文与英文的结构有很大不同。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 AMCNNs</h3>
                <div class="p1">
                    <p id="70">本文将详细介绍用于中文医疗问答对匹配的多尺度卷积神经网络模型, 其基于注意力机制和字嵌入。首先, 本文讨论编码中文医疗文本的正确嵌入方式;其次, 详细描述字嵌入多尺度卷积神经网络架构和本文提出的基于注意力机制的字嵌入多尺度神经网络架构。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">2.1 <b>字符的分布式表示</b></h4>
                <div class="p1">
                    <p id="72">在许多自然语言处理任务中, 一个基本的步骤就是将文本序列转换成机器可读的特征, 该特征通常是固定长度的向量。</p>
                </div>
                <div class="p1">
                    <p id="73">近年来, 基于嵌入的方式在文本特征提取中得到了广泛应用, 证明了它在语义表示上的效用。而用得最多的一种嵌入方式就是词嵌入 (word-embedding) 方式, 也即分布式词表示。Bengio等<citation id="271" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出了一个神经网络语言模型 (Neural Network Language Model, NNLM) , 它将神经网络与自然语言处理相结合以训练词嵌入。之后, Mikolov等<citation id="272" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>受NNLM的启发提出了一个非常高效的语言模型:Word2Vec。而且, 近年来Word2Vec受到越来越多的关注并成功应用于许多NLP任务, 例如句子匹配<citation id="273" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、文档分类<citation id="274" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和知识图谱抽取<citation id="275" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="74">Word2Vect模型的一个例子如图1所示。给定句子序列<i>Sent</i>=[<b><i>w</i></b><sub>1</sub>, <b><i>w</i></b><sub>2</sub>, …, <b><i>w</i></b><sub><i>l</i></sub>], 其中<i>l</i>是序列的长度。<b><i>w</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>d</i><sub><i>w</i></sub></sup>表示句子中位置为<i>i</i>的词的词向量。使<i>Context</i> (<b><i>w</i></b><sub><i>i</i></sub>) =[<b><i>w</i></b><sub><i>i</i>-<i>k</i></sub>, <b><i>w</i></b><sub><i>i</i>-<i>k</i>+1</sub>, …, <b><i>w</i></b><sub><i>i</i>-1</sub>, <b><i>w</i></b><sub><i>i</i>+1</sub>, …, <b><i>w</i></b><sub><i>i</i>+<i>k</i>-1</sub>, <b><i>w</i></b><sub><i>i</i>+<i>k</i></sub>]表示词<b><i>w</i></b><sub><i>i</i></sub>的上下文, 其中2<i>k</i>表示上下文窗口的尺寸。而使<i>p</i> (<b><i>w</i></b><sub><i>i</i></sub>|<i>Context</i> (<b><i>w</i></b><sub><i>i</i></sub>) ) 表示句子中第<i>i</i>个词是<b><i>w</i></b><sub><i>i</i></sub>的概率。该模型的目标是优化对数最大似然估计 (log (<i>MLE</i>) ) :</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mo stretchy="false"> (</mo><mi>Μ</mi><mi>L</mi><mi>E</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mrow><mi>log</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">然而, 当遇到未登录词或稀缺词时, word-embedding方式在中文处理上的质量会有所下降。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 词的分布式表示" src="Detail/GetImg?filename=images/JSJY201906015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 词的分布式表示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Distributed representation of words</p>

                </div>
                <div class="p1">
                    <p id="78">受Wang等 <citation id="276" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的工作的启发, 本文将句子分成单独的字, 如图2所示, 在上下文窗口中, 每个字被用来预测它们中间的字。利用gensim工具训练字向量, 训练完之后, 每个字就被映射到了一个固定长度的向量<b><i>c</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>d</i><sub><b><i>c</i></b></sub></sup>, 其中<i>d</i><sub><b><i>c</i></b></sub>表示向量的维度。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906015_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 字的分布式表示" src="Detail/GetImg?filename=images/JSJY201906015_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 字的分布式表示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906015_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Distributed representation of characters</p>

                </div>
                <div class="p1">
                    <p id="80">给定句子序列<i>Sent</i>=[<b><i>c</i></b><sub>1</sub>, <b><i>c</i></b><sub>2</sub>, …, <b><i>c</i></b><sub><i>l</i></sub>], 其中<i>l</i>表示序列中字的数量, 而<b><i>c</i></b><sub><i>i</i></sub>的上下文表示则是<i>Context</i> (<b><i>c</i></b><sub><i>i</i></sub>) =[<b><i>c</i></b><sub><i>i</i>-<i>k</i></sub>, <b><i>c</i></b><sub><i>i</i>-<i>k</i>+1</sub>, …, <b><i>c</i></b><sub><i>i</i>-1</sub>, <b><i>c</i></b><sub><i>i</i>+1</sub>, …, <b><i>c</i></b><sub><i>i</i>+<i>k</i>-1</sub>, <b><i>c</i></b><sub><i>i</i>+<i>k</i></sub>]。因此, 式 (1) 可以修改为:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi mathvariant="bold-italic">c</mi></msub><mo stretchy="false"> (</mo><mi>Μ</mi><mi>L</mi><mi>E</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mrow><mi>log</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">字嵌入能够避免分词算法引发的错误造成的影响<citation id="277" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。此外, 由于字的数量比词的数量少, 新字或稀有字的数量也比新词或稀有词的数量少, 因此字级的表示方式能降低句子的表示向量尺寸。目前, 字嵌入方式已经应用于许多自然语言处理任务, 例如机器翻译<citation id="278" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、文本分类<citation id="279" type="reference"><link href="249" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、英文场景问答<citation id="280" type="reference"><link href="251" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>和汉语句法依赖解析<citation id="281" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。但是与英文相比, 中文字嵌入方式主要是为了缓解分词不准确带来的影响。另外, 汉字的数量比英文字母数量 (只有26个) 多太多且中文文本中的汉字也比英文的字符包含更多的信息。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">2.2 <b>多尺度卷积神经网络架构</b></h4>
                <div class="p1">
                    <p id="84">目前, 卷积神经网络因其能够捕获本地上下文信息的能力而在许多<i>NLP</i>任务中得到了应用。该网络不依赖于其他如词性标注或解析树之类的外部信息。通常, 一个卷积神经网络架构由两部分组成:卷积和池化。卷积步骤利用固定大小的滑动窗口提取本地上下文特征, 而池化步骤选择从前一层提取的特征的最大值或平均值以降低输出维度, 但保留了最突出的信息。然而, 单尺度卷积神经网络 (<i>Single</i>-<i>scale CNN</i>, <i>SingleCNN</i>) 架构只有一个固定卷积窗口的特征映射, 也即它能捕获的信息很少。考虑到汉语短语的表达结构, <i>SingleCNN</i>架构可能不足以提取字信息。而多尺度的卷积神经网络 (<i>MultiCNNs</i>) 架构的工作方式与<i>SingleCNN</i>架构的工作方式相似, 唯一不同之处在于采用了多个不同尺度的特征映射来提取信息。该架构如图3所示, 问题和答案句子分别由固定长度的字符嵌入序列表示:[<b><i>c</i></b><sub>1</sub>, <b><i>c</i></b><sub>2</sub>, …, <b><i>c</i></b><sub><i>l</i></sub>]。字向量的维度由<i>d</i><sub><i>c</i></sub>表示, 且向量中的每个元素都是实数, 则<b><i>c</i></b><sub><i>l</i></sub>∈<b>R</b><sup><i>d</i><sub><i>c</i></sub></sup>。每个句子需要归一化为一个固定长度的序列, 即若句子长度小于某个阈值就添加0补齐, 相反若大于某个阈值就裁剪掉多余部分。经过字嵌入层后, 每个问题和答案分别由矩阵<b><i>Q</i></b><sub><i>e</i></sub>∈<b>R</b><sup><i>l</i>×<i>d</i><sub><i>c</i></sub></sup>和<b><i>A</i></b><sub><i>e</i></sub>∈<b>R</b><sup><i>l</i>×<i>d</i><sub><i>c</i></sub></sup>表示。</p>
                </div>
                <div class="p1">
                    <p id="85">假设存在一个卷积核尺寸集合<i>S</i>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>t</i></sub>}, 其中第<i>i</i>个卷积神经网络卷积核的尺寸表示为<i>s</i><sub><i>i</i></sub>。当给定序列<b><i>Z</i></b>=[<b><i>z</i></b><sub>1</sub>, <b><i>z</i></b><sub>2</sub>, …, <b><i>z</i></b><sub><i>l</i>-<i>s</i><sub><i>i</i></sub>+1</sub>], 其中<b><i>z</i></b><sub><i>i</i></sub>=[<b><i>c</i></b><sub>1</sub>, <b><i>c</i></b><sub>2</sub>, …, <b><i>c</i></b><sub><i>i</i>+<i>s</i><sub><i>i</i></sub>-1</sub>]表示句子中连续<i>s</i><sub><i>i</i></sub>个字向量拼接的结果, 也即每个特征映射提取出的信息。因此, 可以定义卷积运算的计算式如下:</p>
                </div>
                <div class="p1">
                    <p id="86"><b><i>O</i></b><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>=<i>f</i> (<b><i>w</i></b><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>·[<b><i>z</i></b><sub>1</sub>, <b><i>z</i></b><sub>2</sub>, …, <b><i>z</i></b><sub><i>l</i>-<i>s</i><sub><i>i</i></sub>+1</sub>]+<b><i>b</i></b><sup><i>s</i><sub><i>i</i></sub></sup>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<b><i>O</i></b><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>∈<b>R</b><sup><i>l</i>-<i>s</i><sub><i>i</i></sub>+1</sup>;矩阵<b><i>w</i></b><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>∈<b>R</b><sup><i>s</i><sub><i>i</i></sub>×<i>d</i><sub><i>c</i></sub></sup>和<b><i>b</i></b><sup><i>s</i><sub><i>i</i></sub></sup>是需要训练的参数;<i>f</i> (·) 表示激活函数;<b><i>W</i></b>·<b><i>Z</i></b>表示矩阵<b><i>W</i></b>和矩阵<b><i>Z</i></b>的对应元素相乘。若特征映射的数量为<i>d</i>, 则卷积层的输出为<b><i>O</i></b><sup><i>s</i><sub><i>i</i></sub></sup>=[<b><i>O</i></b><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>, <b><i>O</i></b><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>, …, <b><i>O</i></b><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>]。由此可见, 嵌入矩阵<b><i>Q</i></b><sub><i>e</i></sub>∈<i>R</i><sup><i>l</i>×<i>d</i><sub><i>c</i></sub></sup>和<b><i>A</i></b><sub><i>e</i></sub>∈<b>R</b><sup><i>l</i>×<i>d</i><sub><i>c</i></sub></sup>通过卷积神经层并共享相同卷积参数 (<b><i>w</i></b><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>和<b><i>b</i></b><sup><i>s</i><sub><i>i</i></sub></sup>) 之后转变为矩阵<b><i>Q</i></b><sub><i>o</i></sub>∈<b>R</b><sup> (<i>l</i>-<i>s</i><sub><i>i</i></sub>+1) ×<i>d</i></sup>和<b><i>A</i></b><sub><i>o</i></sub>∈<b>R</b><sup> (<i>l</i>-<i>s</i><sub><i>i</i></sub>+1) ×<i>d</i></sup>。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906015_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 多尺度卷积神经网络架构" src="Detail/GetImg?filename=images/JSJY201906015_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 多尺度卷积神经网络架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906015_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Framework of multi-scale CNNs</p>

                </div>
                <div class="p1">
                    <p id="97">在卷积层之后应用池化层来抽取出最突出的信息, 可以选择最大池化和平均池化, 本文选择使用最大池化方式从每个过滤器中选择出最大值。经过池化层后, 第<i>i</i>个卷积层的输出就可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="98"><b><i>p</i></b><sup><i>s</i><sub><i>i</i></sub></sup>=[max <b><i>O</i></b><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>S</mi><msub><mrow></mrow><mi>Ι</mi></msub></mrow></msubsup></mrow></math></mathml>, max <b><i>O</i></b><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>, …, max <b><i>O</i></b><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>]      (4) </p>
                </div>
                <div class="p1">
                    <p id="102">式中:max <b><i>O</i></b><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml> (<i>j</i>∈[1, <i>d</i>]) 表示从<b><i>O</i></b><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>中选择最大的值, 由此<b><i>p</i></b><sup><i>s</i><sub><i>i</i></sub></sup>∈<b>R</b><sup><i>d</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="105">与SingleCNN架构不同的是, 在经过最大池化层之后, 需要分别将不同尺度过滤器的最大池化结果向量拼接为一个长向量作为问题或答案的最终表示:</p>
                </div>
                <div class="p1">
                    <p id="106"><b><i>p</i></b>=[<b><i>p</i></b><sup><i>s</i><sub>1</sub></sup>, <b><i>p</i></b><sup><i>s</i><sub>2</sub></sup>, …, <b><i>p</i></b><sup><i>s</i><sub><i>t</i></sub></sup>]      (5) </p>
                </div>
                <div class="p1">
                    <p id="107">本文用向量<b><i>Q</i></b><sub><i>p</i></sub>∈<b>R</b><sup><i>t</i>×<i>d</i></sup>和<b><i>A</i></b><sub><i>p</i></sub>∈<b>R</b><sup><i>t</i>×<i>d</i></sup>分别表示问题和答案经过卷积、最大池化和拼接之后的最终表示形式, 然后通过式 (6) 计算它们的相似度:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">p</mi></msub><mo>, </mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi mathvariant="bold-italic">p</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mi>C</mi><mi>o</mi><mi>s</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">p</mi></msub><mo>, </mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mrow><mi>p</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">p</mi></msub><mspace width="0.25em" /><mo>⋅</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi mathvariant="bold-italic">p</mi></msub><mo stretchy="false">∥</mo></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi mathvariant="bold-italic">p</mi></msub><mo stretchy="false">∥</mo><mo>×</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi mathvariant="bold-italic">p</mi></msub><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">式中:‖·‖表示向量的长度。</p>
                </div>
                <div class="p1">
                    <p id="110">多尺度卷积神经网络架构完全能够从中文文本中提取相关的语言特征。图4充分展示了使用MultiCNNs架构从一个固定长度区域句子中提取局部上下文信息的过程。已经知道, 不同的中文短语通常包含不同数量的字。 因此, 单尺度卷积神经网络将在固定长度区域上执行卷积运算, 这类似于将几个相邻字组合成单词。因此, MultiCNNs架构在不同的固定长度区域上执行卷积运算, 并提取不同数量的相邻字嵌入。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">2.3 <b>基于注意力机制的多尺度卷积神经网络架构</b></h4>
                <div class="p1">
                    <p id="112">基于注意力机制的多尺度卷积神经网络 (<i>AMCNNs</i>) 架构的工作方式与<i>MultiCNNs</i>相似, 唯一不同之处在于两个句子经过嵌入层后会分别与注意力特征矩阵<b><i>A</i></b>相乘, 得到一个注意力特征映射矩阵, 并与经过嵌入层后得到的矩阵一起作为卷积层的输入。该架构如图5所示。由于考虑到两个句子在同一任务背景下的相互影响, 因此用一个句子的内容来指导另一个句子的表示。</p>
                </div>
                <div class="p1">
                    <p id="113">AMCNNs利用一个注意力特征矩阵<b><i>A</i></b>来影响卷积运算。注意力特征旨在对卷积中与问题 (或答案) 单位相关的答案 (或问题) 单位赋予更高的权重。如图5所示, 矩阵<b><i>A</i></b>是通过左侧的字嵌入表示单元和右侧的字嵌入表示单元相匹配产生的。<b><i>A</i></b>中第<i>i</i>行的注意值表示<i>s</i><sub><i>q</i></sub>的第<i>i</i>个单位相对于<i>s</i><sub><i>a</i></sub>的注意力分布, <b><i>A</i></b>中第<i>j</i>列的注意值表示<i>s</i><sub><i>a</i></sub>的第<i>j</i>个单位相对于<i>s</i><sub><i>q</i></sub>的注意力分布。<b><i>A</i></b>可以视为<i>s</i><sub><i>q</i></sub> (或<i>s</i><sub><i>a</i></sub>) 在行 (或列) 方向的新的嵌入表示, 因为<b><i>A</i></b>的每一行 (或列) 是<i>s</i><sub><i>q</i></sub> (或<i>s</i><sub><i>a</i></sub>) 上的一个字的新的特征向量。因此, 将这个新的特征向量与字嵌入表示组合并将它们用作卷积运算的输入就有理可依了。通过将注意力特征矩阵<b><i>A</i></b>转换为图5中的两个与字嵌入表示相同格式的灰色矩阵来实现这一点。因此, 卷积层的新输入具有每个句子的两个特征映射。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906015_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 MultiCNNs架构提取上下文信息的过程" src="Detail/GetImg?filename=images/JSJY201906015_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 MultiCNNs架构提取上下文信息的过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906015_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Process of extracting context information by MultiCNNs framework</p>

                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906015_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于注意力机制的多尺度卷积神经网络 (AMCNNs) 架构" src="Detail/GetImg?filename=images/JSJY201906015_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基于注意力机制的多尺度卷积神经网络 (AMCNNs) 架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906015_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Framework of AMCNNs</p>

                </div>
                <div class="p1">
                    <p id="116">接下来, 采用文献<citation id="282" type="reference">[<a class="sup">4</a>]</citation>的方法定义注意力矩阵<b><i>A</i></b>∈<b>R</b><sup><i>d</i><sub><i>c</i></sub>×<i>d</i><sub><i>c</i></sub></sup>, 如式 (7) 所示:</p>
                </div>
                <div class="p1">
                    <p id="117"><b><i>A</i></b><sub><i>i</i>, <i>j</i></sub>=<i>match</i>-<i>score</i> (<b><i>Q</i></b><sub><i>e</i></sub>[<i>i</i>, :], <b><i>A</i></b><sub><i>e</i></sub>[<i>j</i>, :])      (7) </p>
                </div>
                <div class="p1">
                    <p id="118">式中<i>match</i>-<i>score</i> (·) 函数有多种方式定义, 但本文采用的是欧几里得距离算法定义该函数, 即<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>y</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="120">给定注意力矩阵<b><i>A</i></b>, 以此分别产生问题和答案的注意力映射矩阵, 计算式如下:</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">Q</mi><mo>, </mo><mi>a</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>0</mn></msub><mo>⋅</mo><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">A</mi><mo>, </mo><mi>a</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>⋅</mo><mi mathvariant="bold-italic">A</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">式中:权重矩阵<b><i>W</i></b><sub>0</sub>∈<b>R</b><sup><i>l</i>×<i>d</i><sub><i>c</i></sub></sup>, <b><i>W</i></b><sub>1</sub>∈<b>R</b><sup><i>l</i>×<i>d</i><sub><i>c</i></sub></sup>都是模型训练时需要学习的参数。</p>
                </div>
                <div class="p1">
                    <p id="123">分别得到问题和答案的注意力映射矩阵之后, 然后将它们组合为3阶的张量传递给卷积层进行卷积运算。卷积层的结构和运算过程以及在此之后的池化步骤都与图4的MultiCNNs架构一样。最后将最大池化后的问题向量和答案向量进行相似度计算, 从而得到它们的匹配结果。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">2.4 <b>目标函数</b></h4>
                <div class="p1">
                    <p id="125">给定一个问题<i>q</i><sub><i>i</i></sub>, 它的真实答案是<i>a</i><sup>+</sup><sub><i>i</i></sub>, 而<i>a</i><sup>-</sup><sub><i>i</i></sub>是从候选答案池中随机选择的错误答案。一个有效的网络模型应该能够最大化<i>Sim</i> (<i>q</i><sub><i>i</i></sub>, <i>a</i><sup>+</sup><sub><i>i</i></sub>) , 且最小化<i>Sim</i> (<i>q</i><sub><i>i</i></sub>, <i>a</i><sup>-</sup><sub><i>i</i></sub>) 。为了训练以上神经网络, 本文采用文献<citation id="283" type="reference">[<a class="sup">1</a>,<a class="sup">4</a>,<a class="sup">13</a>,<a class="sup">16</a>,<a class="sup">17</a>]</citation>中的方法定义训练损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="126"><i>Η</i>=max{0, <i>M</i>-<i>Sim</i> (<i>q</i><sub><i>i</i></sub>, <i>a</i><sup>+</sup><sub><i>i</i></sub>) +<i>Sim</i> (<i>q</i><sub><i>i</i></sub>, <i>a</i><sup>-</sup><sub><i>i</i></sub>) }       (9) </p>
                </div>
                <div class="p1">
                    <p id="127">式中:<i>M</i>是一个常数, 表示边界值。如果<i>Sim</i> (<i>q</i><sub><i>i</i></sub>, <i>a</i><sup>+</sup><sub><i>i</i></sub>) -<i>Sim</i> (<i>q</i><sub><i>i</i></sub>, <i>a</i><sup>-</sup><sub><i>i</i></sub>) &gt;<i>M</i>, 则损失函数为0, 且网络参数不需要更新。</p>
                </div>
                <div class="p1">
                    <p id="128">当训练该网络时, 分别提供每个问题与其真实答案和随机采样的错误答案给网络。然后, 计算损失函数<i>H</i>, 并且使用优化器 (如GradientDescentOptimizer或AdagradOptimizer) 以更新网络参数。</p>
                </div>
                <h3 id="129" name="129" class="anchor-tag">3 数据集和实验设置</h3>
                <h4 class="anchor-tag" id="130" name="130">3.1 <i>cMedQA</i><b>数据集</b></h4>
                <div class="p1">
                    <p id="131">当前在中文医疗领域问答匹配任务上没有标准的评测数据集, 因此选择从专业的医疗健康网站上收集中文医疗问答数据集 (<i>Chinese Medical Question and Answer dataset</i>, <i>cMedQA</i>) <citation id="284" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。该数据集是一个基于中文医疗问答的公开可用的数据集, 其中的问答数据收集于寻医问药网。问题通常是由用户提出, 而答案来自于专业医生的可靠回复。通常一个问题会得到多个医生的回复, 即一个问题有多个正确答案。</p>
                </div>
                <div class="p1">
                    <p id="132">如表1所示, 该数据集已经被划分为了三个子集:训练集、开发集和测试集。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表</b>1 <i>cMedQA</i><b>数据集的统计信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Statistical information for cMedQA dataset</i></p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td>数据集</td><td>问题<br />句子数</td><td>答案<br />句子数</td><td>每句问题<br />平均词数</td><td>每句答案<br />平均词数</td><td>每句问题<br />平均字数</td><td>每句答案<br />平均字数</td></tr><tr><td>训练集</td><td>50 000</td><td>94 134</td><td>97</td><td>169</td><td>120</td><td>212</td></tr><tr><td><br />开发集</td><td>2 000</td><td>3 774</td><td>94</td><td>172</td><td>117</td><td>216</td></tr><tr><td><br />测试集</td><td>2 000</td><td>3 835</td><td>96</td><td>168</td><td>119</td><td>211</td></tr><tr><td><br />总计</td><td>54 000</td><td>10 1743</td><td>96</td><td>169</td><td>119</td><td>212</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="134">训练该模型时, 训练集中的每个问题<i>q</i><sub><i>i</i></sub>都有一个正确答案<i>a</i><sup>+</sup><sub><i>i</i></sub>和一个从候选答案池中随机采样的答案<i>a</i><sup>-</sup><sub><i>i</i></sub>, 从而设定训练集中每个问题关联30个 (<i>q</i><sub><i>i</i></sub>, <i>a</i><sup>+</sup><sub><i>i</i></sub>, <i>a</i><sup>-</sup><sub><i>i</i></sub>) 问题答案对。因此, 在训练阶段, 每一轮总共有1 500 000问答对被依次送入网络中进行训练。开发集和测试集则是每个问题对应100个候选答案, 其中包括一个或多个正确答案。开发集用于网络参数的优化, 而测试集用于评估模型。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">3.2 <b>评价方法</b></h4>
                <div class="p1">
                    <p id="136"><i>Top</i>-K精度 (<i>ACC</i>@K) 普遍作为信息检索任务的评价标准, 其定义如式 (10) 所示:</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>C</mi><mi>C</mi><mo>@</mo><mi>Κ</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mn>1</mn></mstyle><mo stretchy="false">[</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">式中:<i>a</i><sub><i>i</i></sub>代表问题<i>q</i><sub><i>i</i></sub>的正确答案;<i>c</i><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>指候选池中相似度分数最高的<i>K</i>个答案。<i>a</i><sub><i>i</i></sub>∈<i>c</i><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>是函数1[·]的条件, 当条件满足时, 其值为1;否则, 其值为0。</p>
                </div>
                <div class="p1">
                    <p id="141">由于答案选择 (AS) 任务与信息检索任务不同, AS要求得到一个可能性最大的答案, 即返回候选答案池相似度分数最高的一个答案。因此, 本文选择top-1精度 (ACC@1) 作为本文模型评估的标准。</p>
                </div>
                <h4 class="anchor-tag" id="142" name="142">3.3 <b>基线模型</b></h4>
                <div class="p1">
                    <p id="143">目前优秀的问答匹配模型都是基于英文语境, 且面向开放领域问答, 因此本文所描述模型不与它们进行比较, 而是设计了一些基线模型用于实验效果对比, 如下所示:</p>
                </div>
                <div class="p1">
                    <p id="144">1) 字匹配 (<i>Character Matching</i>) 。字匹配方式统计问题与答案中相同字的数量。</p>
                </div>
                <div class="p1">
                    <p id="145">2) 词匹配 (<i>Word Matching</i>) 。与字匹配相似, 统计问题与答案中相同词语的数量。但是需要使用分词工具进行分词, 因此本文选择了两种分词工具 (<i>Jieba</i>和<i>ICTCLAS</i>) 用以展示不同分词工具对模型性能的影响。</p>
                </div>
                <div class="p1">
                    <p id="146">3) <i>BM</i>25。<i>BM</i>25 (最佳匹配) 是信息检索中的一种排序函数, 该函数定义如下:</p>
                </div>
                <div class="p1">
                    <p id="147" class="code-formula">
                        <mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>B</mi><mi>Μ</mi><mn>2</mn><mn>5</mn><mo stretchy="false"> (</mo><mi>q</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>q</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mrow><mfrac><mrow><mi>Ι</mi><mi>D</mi><mi>F</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>⋅</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>⋅</mo><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>k</mi><mo>⋅</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>b</mi><mo>+</mo><mi>b</mi><mo>⋅</mo><mo stretchy="false">|</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mo>/</mo><mo stretchy="false">|</mo><mi>a</mi><mo stretchy="false">|</mo><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="148">式中:<i>IDF</i> (<i>w</i><sub><i>j</i></sub>) 是问句中词 (或字) <i>w</i><sub><i>j</i></sub>的逆文档频率;|<i>a</i>|是答案<i>a</i><sub><i>i</i></sub>的长度, |<i>a</i>|<sub>avg</sub>是答案的平均长度;<i>f</i> (<i>w</i><sub><i>j</i></sub>, <i>a</i><sub><i>i</i></sub>) 表示在<i>a</i><sub><i>i</i></sub>中的<i>w</i><sub><i>j</i></sub>的频率。<i>k</i>和<i>b</i>是需要设定的参数, 在本文中, 设定<i>k</i>=2.0, <i>b</i>=0.75.</p>
                </div>
                <div class="p1">
                    <p id="149">4) BiLSTM。Tan等<citation id="285" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>使用双向长短时记忆神经网络 (Bi-directional LSTM, BiLSTM) 来学习问题和答案的语义表示。BiLSTM是循环神经网络 (RNN) 的变体, 能够捕获长文本序列的语义信息。</p>
                </div>
                <h4 class="anchor-tag" id="150" name="150">3.4 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="151">本文提出的模型均是使用深度学习框架<i>TensorFlow</i>训练和测试的。为了比较字符嵌入和词嵌入的性能, 选择使用<i>Jieba</i>和<i>ICTCLAS</i>两个分词工具用于分词。同时本文使用<i>gensim</i>工具预先训练字向量和词向量, 训练完之后的每个字 (词) 由<i>d</i><sub><i>c</i></sub>=200的一维向量表示。字向量在模型训练过程中也是作为参数被优化。在模型训练中, 批次数量设置为100, 而问题和答案的最大序列长度均为400字符和200词。</p>
                </div>
                <div class="p1">
                    <p id="152">对于CNN网络, 滤波器具有3种不同大小的尺寸, 分别为2、3、4, 但特征映射数量均为800个。在本文中使用AdagradOptimizer优化器, 学习率初始化值设置为0.01, 边界值<i>M</i>设置为0.5。</p>
                </div>
                <h4 class="anchor-tag" id="153" name="153">3.5 <b>结果分析</b></h4>
                <div class="p1">
                    <p id="154">在<i>cMedQA</i>数据集<citation id="286" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>上采用不同方法得到的实验结果如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="155">如表2所示, 第1～6行提供的是没有使用神经网络架构的基线模型的实验结果。其中, 第1～3行展示的是基于匹配的方式得到的结果, 可以发现词匹配和字匹配的效果相差1个百分点左右, 这是由于词中包含了比字更多的信息。相较于词 (字) 匹配的方式, <i>BM</i>25方式利用了更多的统计信息得到了11个百分点的提升。</p>
                </div>
                <div class="p1">
                    <p id="156">第7～9行展示的是基于嵌入的方式得到的结果。从中可以发现, 字嵌入的效果明显由于词嵌入, 说明能够从字中更好地提取嵌入方式的语义信息;而使用<i>ICTCLAS</i>分词工具得到的词嵌入结果优于使用<i>jieba</i>分词工具得到的结果, 表明了分词工具的性能对实验结果有较大的影响。</p>
                </div>
                <div class="p1">
                    <p id="157">第10～12行展示了词嵌入的深度神经网络进行建模的结果, 其使用<i>jieba</i>分词工具进行文本预处理。从中可以发现, <i>BiLSTM</i><citation id="287" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>网络架构的效果显著优于卷积神经网络架构。原因可能是循环神经网络及其变体能够抽取整个句子的语义信息, 能有效降低问题和答案的语义代沟。该实验结果与<i>Zhang</i>等<citation id="288" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的实验结果一致。</p>
                </div>
                <div class="area_img" id="158">
                    <p class="img_tit"><b>表</b>2 <b>各模型的</b><i>Top</i>- 1<b>准确率结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Top</i>- 1 <i>accuracy results of different model</i></p>
                    <p class="img_note"></p>
                    <table id="158" border="1"><tr><td rowspan="2"><br />编号</td><td rowspan="2">嵌入方式</td><td rowspan="2">模型</td><td colspan="2"><br />准确率/%</td></tr><tr><td><br />开发集</td><td>测试集</td></tr><tr><td><br />1</td><td rowspan="6"><i>None</i></td><td><br /><i>Word Matching</i> (<i>Jieba</i>) </td><td>37.05</td><td>36.60</td></tr><tr><td><br />2</td><td><br /><i>Word Matching</i> (<i>ICTCLAS</i>) </td><td>35.11</td><td>36.22</td></tr><tr><td><br />3</td><td><br /><i>Character Matching</i></td><td>33.65</td><td>34.90</td></tr><tr><td><br />4</td><td><br /><i>BM</i>25 (<i>Jieba</i>) </td><td>37.60</td><td>40.00</td></tr><tr><td><br />5</td><td><br /><i>BM</i>25 (<i>ICTCLAS</i>) </td><td>40.25</td><td>41.25</td></tr><tr><td><br />6</td><td><br /><i>BM</i>25 (<i>character</i>) </td><td>44.80</td><td>45.40</td></tr><tr><td><br />7</td><td><i>Word</i> (<i>Jieba</i>) </td><td rowspan="3"><i>Embedding Matching</i></td><td><br />24.55</td><td>23.65</td></tr><tr><td><br />8</td><td><i>Word</i> (<i>ICTCLAS</i>) </td><td><br />27.85</td><td>29.10</td></tr><tr><td><br />9</td><td><i>Character</i></td><td><br />30.80</td><td>32.30</td></tr><tr><td><br />10</td><td rowspan="3"><i>Word Embedding</i><br /> (<i>Jieba</i>) </td><td><br /><i>BiLSTM</i></td><td>51.70</td><td>50.10</td></tr><tr><td><br />11</td><td><br /><i>MultiCNNs</i></td><td>48.40</td><td>47.75</td></tr><tr><td><br />12</td><td><br /><i>AMCNNs</i></td><td>51.25</td><td>50.01</td></tr><tr><td><br />13</td><td rowspan="3"><i>Word Embedding</i><br /> (<i>ICTCLAS</i>) </td><td><br /><i>BiLSTM</i></td><td>56.15</td><td>56.02</td></tr><tr><td><br />14</td><td><br /><i>MultiCNNs</i></td><td>53.06</td><td>52.34</td></tr><tr><td><br />15</td><td><br /><i>AMCNNs</i></td><td>53.50</td><td>51.78</td></tr><tr><td><br />16</td><td rowspan="3"><i>Character</i><br /><i>Embedding</i></td><td><br /><i>BiLSTM</i></td><td>61.65</td><td>60.78</td></tr><tr><td><br />17</td><td><br /><i>MultiCNNs</i></td><td>65.35</td><td>64.73</td></tr><tr><td><br />18</td><td><br /><i>AMCNNs</i></td><td>66.80</td><td>65.43</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="159">第13～15行也是采用了词嵌入的深度神经网络建模的结果, 其使用<i>ICTCLAS</i>分词工具进行文本预处理。与第10～12行的实验对比结果一样, <i>BiLSTM</i>的效果仍然优于卷积神经网络。同时, 可以发现使用<i>ICTCLAS</i>分词工具进行文本预处理后进行建模的方法优于使用<i>Jieba</i>分词工具的方法。并且在词嵌入方式中比较了未加<i>Attention</i>机制 (<i>MultiCNNs</i>) 和加了<i>Attention</i>机制 (<i>AMCNNs</i>) 的多尺度卷积神经网络的性能。通过对比可以看出, 不管是采用<i>Jieba</i>工具还是<i>ICTCLAS</i>工具, 都是结合注意力机制的卷积神经网络架构优于卷积神经网络架构, 同时进一步证明了分词工具对模型的性能有较大的影响。因此, 为了避免分词工具的影响, 采用字嵌入方式是有必要的。</p>
                </div>
                <div class="p1">
                    <p id="160">第16～18行采用字嵌入方式分别训练了<i>BiLSTM</i>、<i>MutliCNNs</i>和<i>AMCNNs</i>。从结果中可以发现, <i>MultiCNNs</i>的准确率比<i>BiLSTM</i>提升了约4个百分点, 而<i>AMCNNs</i>的准确率相较于<i>MultiCNNs</i>则提升了约2个百分点, 表明增加的<i>Attention</i>机制是能够改善<i>MutliCNNs</i>架构的性能的。同时, 对比第10～15行的效果能够看出, 字嵌入方式明显优于词嵌入, 准确率提升了约10个百分点。</p>
                </div>
                <div class="p1">
                    <p id="161">由以上实验结果可得, 基于神经网络的方式明显优于词 (字) 匹配方式, 说明了语言表达的多样性和非正规性很有可能降低匹配的效果;而字嵌入方式明显优于词嵌入, 不论是单独使用还是在神经网络中。以上结论表明本文提出的基于注意力机制的字嵌入多尺度卷积神经网络用于医疗领域问答匹配是可行的。</p>
                </div>
                <div class="p1">
                    <p id="162">通过对<i>CNN</i>的研究可知, 该网络架构处理文本的原理类似于N-<i>Gram</i>语言模型, <i>N</i>一般在[2, 4]。同时, 考虑到中文词语通常由2至4个汉字构成, 即卷积操作的窗口尺寸大小应该在[2, 4]。因此本文对比了2, 3, 4三种尺度的不同组合的效果, 结果如表4所示。根据实验结果, 最终选择 (2, 3, 4) 的尺度组合。从表4中可以明显看出, 当设置卷积核尺度为 (3, 4) 时得到的效果最优。究其原因可能因为语料库中由3个字或4个字组合成的短语包含的信息更加丰富。</p>
                </div>
                <div class="area_img" id="163">
                    <p class="img_tit"><b>表</b>3 <b>不同尺度卷积核的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Experimental results of convolution kernels at different scales</p>
                    <p class="img_note"></p>
                    <table id="163" border="1"><tr><td rowspan="2"><br />卷积核<br />尺度</td><td colspan="2"><br />准确率/%</td><td rowspan="2"></td><td rowspan="2">卷积核<br />尺度</td><td colspan="2"><br />准确率/%</td></tr><tr><td><br />开发集</td><td>测试集</td><td><br />开发集</td><td>测试集</td></tr><tr><td> (3, 4) </td><td>64.90</td><td>64.33</td><td></td><td> (2, 4) </td><td>64.95</td><td>64.15</td></tr><tr><td><br /> (2, 3) </td><td>65.00</td><td>64.30</td><td></td><td> (2, 3, 4) </td><td>65.80</td><td>64.43</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="164" name="164" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="165">本文设计并实现了基于注意力机制的字嵌入多尺度卷积神经网络模型, 该模型用于处理中文医疗领域的问答匹配问题。同时, 该模型不需要任何额外的特征工程、句法信息或者基于规则的模板。根据实验结果可知, 该模型优于词嵌入方式, 而且相较于无注意力机制的多尺度卷积神经网络和<i>BiLSTM</i>, 能更好地捕获字符级信息。</p>
                </div>
                <div class="p1">
                    <p id="166">在接下来的工作中, 我们将研究新的注意力机制, 以便能更加准确地找到问题和答案之间的语义联系, 以此来进一步提升该模型的效果。同时, 我们将扩展本文模型, 使其能够有效融合医疗领域的知识库并应用于智能问答系统中。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="205">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Applying deep learning to answer selection:a study and an open task">

                                <b>[1]</b>FENG M W, XIANG B, GLASS M R, et al.Applying deep learning to answer selection:a study and an open task[C]//Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2015:813-820.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved Representation Learning for Question Answer Matching">

                                <b>[2]</b>TAN M, dos SANTOS C N, XIANG B, et al.Improved representation learning for question answer matching[C]//ACL 2016:Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.Cambridge, CA:MIT Press, 2016:464-473.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Tensor Network Architecture for Community-Based Question Answering">

                                <b>[3]</b>QIU X P, HUANG X J.Convolutional neural tensor network architecture for community-based question answering[C]//IJCAI2015:Proceedings of the 24th International Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:1305-1311.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ABCNN:attentionbased convolutional neural network for modeling sentence pairs">

                                <b>[4]</b>YIN W P, SCHTZE H, XIANG B, et al.ABCNN:attentionbased convolutional neural network for modeling sentence pairs[EB/OL].[2018-08-20].http://cn.arxiv.org/abs/1512.05193.pdf.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rule based architecture for medical question answering system">

                                <b>[5]</b>JAIN S, DODIYA T.Rule based architecture for medical question answering system[C]//Soc Pro S 2012:Proceedings of the Second International Conference on Soft Computing for Problem Solving, AISC 236.Berlin:Springer, 2014:1225-1233.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An answer recommendation algorithm for medical community question answering systems">

                                <b>[6]</b>WANG J, MAN C T, ZHAO Y F, et al.An answer recommendation algorithm for medical community question answering systems[C]//SOLI 2016:Proceedings of the 2016 IEEE International Conference on Service Operations and Logistics, and Informatics.Piscataway, NJ:IEEE, 2016:139-144.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Medical question answering:translating medical questions into sparql queries">

                                <b>[7]</b>ABACHA A B, ZWEIGENBAUM P.Medical question answering:translating medical questions into SPARQL queries[C]//Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium.New York:ACM, 2012:41-50.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122600239096&amp;v=MjUyODRVcjNJSjFvZGF4bz1OaWZPZmJLOUg5UE9xWTlGWnVnR0RIVS9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>ABACHA A B, ZWEIGENBAUM P.MEANS:a medical questionanswering system combining NLP techniques and semantic Web technologies[J].Information Processing&amp;Management, 2015, 51 (5) :570-594.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Chinese question answering system for specific domain">

                                <b>[9]</b>LI T C, HAO Y, ZHU X Y, et al.A Chinese question answering system for specific domain[C]//WAIM 2014:Proceedings of the International Conference on Web-Age Information Management.Berlin:Springer, 2014:590-601.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Health QA:a Chinese QAsummary system for smart health">

                                <b>[10]</b>YIN Y S, ZHANG Y, LIU X, et al.Health QA:a Chinese QAsummary system for smart health[C]//CSH 2014:Proceedings of the 2nd International Conference on Smart Health, LNCS 8549.Cham:Springer, 2014:51-62.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Chinese Question Answering Approach Integrating Count-Based and Embedding-Based Features">

                                <b>[11]</b>WANG B Y, NIU J B, MA L Q, et al.A Chinese question answering approach integrating count-based and embedding-based features[C]//Proceedings of the 2016 International Conference on Computer Processing of Oriental Languages, National CCF Conference on Natural Language Processing and Chinese Computing, LNCS 10102.Cham:Springer, 2016:934-941.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">

                                <b>[12]</b>HU B T, LU Z D, LI H, et al.Convolutional neural network architectures for matching natural language sentences[C]//NIPS2014:Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge, CA:MIT Press, 2014:2042-2050.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Answer sequence learning with neural networks for answer selection in community question answering">

                                <b>[13]</b>ZHOU X Q, HU B T, CHEN Q C, et al.Answer sequence learning with neural networks for answer selection in community question answering[EB/OL].[2018-08-14].https://arxiv.org/abs/1506.06490.pdf.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSTM-based deep learning models for non-factoid answer selection">

                                <b>[14]</b>TAN M, dos SANTOS C N, XIANG B, et al.LSTM-based deep learning models for non-factoid answer selection[EB/OL].[2018-08-20].https://arxiv.org/abs/1511.04108.pdf.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs">

                                <b>[15]</b>ZHANG S, ZHANG X, WANG H, et al.Chinese medical question answer matching using end-to-end character-level multi-scale CNNs[J].Applied Sciences, 2017, 7 (8) :767.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Neural Probabilistic Language Model">

                                <b>[16]</b>BENGIO Y, DUCHARME R, VINCENT P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3 (6) :1137-1155.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">

                                <b>[17]</b>MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[EB/OL].[2018-08-23].http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bilateral multi-perspective matching for natural language sentences">

                                <b>[18]</b>WANG Z G, HAMZA W, FLORIAN R.Bilateral multi-perspective matching for natural language sentences[EB/OL].[2018-08-20].https://arxiv.org/abs/1702.03814.pdf.
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Document classification by inversion of distributed language representations">

                                <b>[19]</b>TADDY M.Document classification by inversion of distributed language representations[EB/OL].[2018-08-20].https://arxiv.org/abs/1504.07295.pdf.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Learning entity and relation embeddings for knowledge graph completion,&amp;quot;">

                                <b>[20]</b>LIN Y K, LIU Z Y, SUN M S, et al.Learning entity and relation embeddings for knowledge graph completion[C]//AAAI 2014:Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI, 2015:2181-2187.
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Character-level chinese dependency parsing">

                                <b>[21]</b>ZHANG M S, ZHANG Y, CHE W X, et al.Character-level Chinese dependency parsing[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.Cambridge, MA:MIT Press, 2014:1326-1336.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A character-level decoder without explicit segmentation for neural machine translation">

                                <b>[22]</b>CHUNG J, CHO K, BENGIO Y.A character-level decoder without explicit segmentation for neural machine translation[EB/OL].[2018-08-15].https://arxiv.org/abs/1603.06147.pdf.
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Character-level Convolutional Networks for Text Classification">

                                <b>[23]</b>ZHANG X, ZHAO J B, LECUN Y.Character-level convolutional networks for text classification[C]//NIPS 2015:Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:649-657.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Character-level question answering with attention">

                                <b>[24]</b>GOLUB D, HE X D.Character-level question answering with attention[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.Cambridge, MA:MITPress, 2016:1598-1607.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906015" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906015&amp;v=MTEwMjl1WnNGeS9oV3I3QUx6N0JkN0c0SDlqTXFZOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
