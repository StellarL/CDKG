<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136459146065000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910020%26RESULT%3d1%26SIGN%3dpgUU%252f37bw6ReuKpqp1fgTTPu35c%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910020&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910020&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910020&amp;v=MTYxNTQ5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtVci9NTHo3QmQ3RzRIOWpOcjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#67" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="1 数据集及其预处理 ">1 数据集及其预处理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="1.1 &lt;b&gt;基准值处理&lt;/b&gt;">1.1 <b>基准值处理</b></a></li>
                                                <li><a href="#85" data-title="1.2 &lt;b&gt;使用&lt;/b&gt;eCognition&lt;b&gt;标注未标注数据&lt;/b&gt;">1.2 <b>使用</b>eCognition<b>标注未标注数据</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="2 遥感图像语义分割模型 ">2 遥感图像语义分割模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="2.1 &lt;b&gt;基于改进&lt;/b&gt;U-net&lt;b&gt;的遥感图像语义分割模型&lt;/b&gt;">2.1 <b>基于改进</b>U-net<b>的遥感图像语义分割模型</b></a></li>
                                                <li><a href="#138" data-title="2.2 &lt;b&gt;基于全连接条件随机场的细分割&lt;/b&gt;">2.2 <b>基于全连接条件随机场的细分割</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#149" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#150" data-title="3.1 &lt;b&gt;实验&lt;/b&gt;">3.1 <b>实验</b></a></li>
                                                <li><a href="#173" data-title="3.2 &lt;b&gt;分析&lt;/b&gt;">3.2 <b>分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#206" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="图1 所有区块的轮廓与正射影像镶嵌重叠">图1 所有区块的轮廓与正射影像镶嵌重叠</a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;六种地物类型及其基准值&lt;/b&gt;"><b>表</b>1 <b>六种地物类型及其基准值</b></a></li>
                                                <li><a href="#80" data-title="图2 语义对象分类示例">图2 语义对象分类示例</a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;本文四种地物类型及其基准值&lt;/b&gt;"><b>表</b>2 <b>本文四种地物类型及其基准值</b></a></li>
                                                <li><a href="#88" data-title="图5 U-net架构">图5 U-net架构</a></li>
                                                <li><a href="#90" data-title="图3 使用eCognition标注的遥感图像及其基准值">图3 使用eCognition标注的遥感图像及其基准值</a></li>
                                                <li><a href="#91" data-title="图4 遥感图像及其初始分割">图4 遥感图像及其初始分割</a></li>
                                                <li><a href="#110" data-title="图6 D-Unet架构">图6 D-Unet架构</a></li>
                                                <li><a href="#116" data-title="图7 ReLU与ELU函数">图7 ReLU与ELU函数</a></li>
                                                <li><a href="#123" data-title="图8 标准卷积">图8 标准卷积</a></li>
                                                <li><a href="#127" data-title="图9 深度卷积">图9 深度卷积</a></li>
                                                <li><a href="#128" data-title="图10 点卷积">图10 点卷积</a></li>
                                                <li><a href="#148" data-title="图11 D-Unet_CRF模型">图11 D-Unet_CRF模型</a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;训练模型的部分超参数&lt;/b&gt;"><b>表</b>3 <b>训练模型的部分超参数</b></a></li>
                                                <li><a href="#161" data-title="图12 学习率与迭代次数的关系">图12 学习率与迭代次数的关系</a></li>
                                                <li><a href="#162" data-title="图13 准确率与迭代次数的关系">图13 准确率与迭代次数的关系</a></li>
                                                <li><a href="#163" data-title="图14 训练损失与迭代次数的关系">图14 训练损失与迭代次数的关系</a></li>
                                                <li><a href="#164" data-title="图15 验证准确率与迭代次数的关系">图15 验证准确率与迭代次数的关系</a></li>
                                                <li><a href="#165" data-title="图16 验证损失与迭代次数的关系">图16 验证损失与迭代次数的关系</a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;数据可视化转换&lt;/b&gt;"><b>表</b>4 <b>数据可视化转换</b></a></li>
                                                <li><a href="#182" data-title="图17 不同模型分割预测结果">图17 不同模型分割预测结果</a></li>
                                                <li><a href="#199" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同方法分割地物信息的性能对比&lt;/b&gt;"><b>表</b>5 <b>不同方法分割地物信息的性能对比</b></a></li>
                                                <li><a href="#200" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;不同方法的模型体积以及参数量&lt;/b&gt;"><b>表</b>6 <b>不同方法的模型体积以及参数量</b></a></li>
                                                <li><a href="#201" data-title="&lt;b&gt;表&lt;/b&gt;7 &lt;b&gt;不同方法的复杂性对比&lt;/b&gt;"><b>表</b>7 <b>不同方法的复杂性对比</b></a></li>
                                                <li><a href="#204" data-title="&lt;b&gt;表&lt;/b&gt;8 &lt;b&gt;不同方法使用&lt;/b&gt;&lt;i&gt;Potsdam&lt;/i&gt;&lt;b&gt;数据集 测试的准确率对比&lt;/b&gt;"><b>表</b>8 <b>不同方法使用</b><i>Potsdam</i><b>数据集 测试的准确率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="247">


                                    <a id="bibliography_1" title="高海燕,吴波.结合像元形状特征分割的高分辨率影像面向对象分类[J].遥感信息,2010(6):67-72.(GAO H Y,WU B.Object-oriented classification of high spatial resolution remote sensing imagery based on image segmentation with pixel shape feature[J].Remote Sensing Information,2010(6):67-72.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXX201006016&amp;v=MjQ4MTh0R0ZyQ1VSN3FmWnVac0Z5amtVci9NUENyVGRyRzRIOUhNcVk5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        高海燕,吴波.结合像元形状特征分割的高分辨率影像面向对象分类[J].遥感信息,2010(6):67-72.(GAO H Y,WU B.Object-oriented classification of high spatial resolution remote sensing imagery based on image segmentation with pixel shape feature[J].Remote Sensing Information,2010(6):67-72.)
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_2" title="巫兆聪,胡忠文,张谦,等.结合光谱、纹理与形状结构信息的遥感影像分割方法[J].测绘学报,2013,42(1):44-50.(WU Z C,HUZ W,ZHANG Q,et al.On combining spectral,textural and shape features for remote sensing image segmentation[J].Acta Geodaetica et Cartographica Sinica,2013,42(1):44-50.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201301009&amp;v=Mjc4NTZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWprVXIvTUppWFRiTEc0SDlMTXJvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        巫兆聪,胡忠文,张谦,等.结合光谱、纹理与形状结构信息的遥感影像分割方法[J].测绘学报,2013,42(1):44-50.(WU Z C,HUZ W,ZHANG Q,et al.On combining spectral,textural and shape features for remote sensing image segmentation[J].Acta Geodaetica et Cartographica Sinica,2013,42(1):44-50.)
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_3" title="YUAN J,WANG D,LI R.Remote sensing image segmentation by combining spectral and texture features[J].IEEE Transactions on Geoscience and Remote Sensing,2014,52(1):16-24." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Remote Sensing Image Segmentation by Combining Spectral and TextureFeatures">
                                        <b>[3]</b>
                                        YUAN J,WANG D,LI R.Remote sensing image segmentation by combining spectral and texture features[J].IEEE Transactions on Geoscience and Remote Sensing,2014,52(1):16-24.
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_4" title="PALENICHKA R,DOYON F,LAKHSSASSI A,et al.Multi-scale segmentation of forest areas and tree detection in Li DAR images by the attentive vision method[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2013,6(3):1313-1323." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale segmentation of forest areas and tree detection in Li DAR images by the attentive vision method">
                                        <b>[4]</b>
                                        PALENICHKA R,DOYON F,LAKHSSASSI A,et al.Multi-scale segmentation of forest areas and tree detection in Li DAR images by the attentive vision method[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2013,6(3):1313-1323.
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_5" title="都伟冰,王双亭,王春来.基于机载Li DAR粗糙度指数和回波强度的道路提取[J].测绘科学技术学报,2013,30(1):63-67.(DU W B,WANG S T,WANG C L.Road extraction based on roughness index and echo intensity of airborne Li DAR[J].Journal of Geomatics Science and Technology,2013,30(1):63-67.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFJC201301017&amp;v=MTI3MjBMeXZCYmJHNEg5TE1ybzlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1VyL00=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        都伟冰,王双亭,王春来.基于机载Li DAR粗糙度指数和回波强度的道路提取[J].测绘科学技术学报,2013,30(1):63-67.(DU W B,WANG S T,WANG C L.Road extraction based on roughness index and echo intensity of airborne Li DAR[J].Journal of Geomatics Science and Technology,2013,30(1):63-67.)
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_6" title="张曦,胡根生,梁栋,等.基于时频特征的高分辨率遥感图像道路提取[J].地理空间信息,2016,14(6):18-21,24.(ZHANG X,HU G S,LIANG D,et al.Road extraction from high resolution remote sensing image based on time frequency feature[J].Geospatial Information,2016,14(6):18-21,24.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DXKJ201606006&amp;v=MTMyMDRaTEc0SDlmTXFZOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWprVXIvTUlUWEE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        张曦,胡根生,梁栋,等.基于时频特征的高分辨率遥感图像道路提取[J].地理空间信息,2016,14(6):18-21,24.(ZHANG X,HU G S,LIANG D,et al.Road extraction from high resolution remote sensing image based on time frequency feature[J].Geospatial Information,2016,14(6):18-21,24.)
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_7" title="周绍光,陈超,赫春晓.基于形状先验和Graph Cuts原理的道路分割新方法[J].测绘通报,2013(12):55-57.(ZHOU SG,CHEN C,HE C X.A new road segmentation based on shape prior and graph cuts[J].Bulletin of Surveying and Mapping,2013(12):55-57.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHTB201312016&amp;v=MjQyMDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtVci9NSmlYZmJMRzRIOUxOclk5RVlvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        周绍光,陈超,赫春晓.基于形状先验和Graph Cuts原理的道路分割新方法[J].测绘通报,2013(12):55-57.(ZHOU SG,CHEN C,HE C X.A new road segmentation based on shape prior and graph cuts[J].Bulletin of Surveying and Mapping,2013(12):55-57.)
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_8" title="周家香,周安发,陶超,等.一种高分辨率遥感影像城区道路网提取方法[J].中南大学学报(自然科学版),2013,44(6):2385-2391.(ZHOU J X,ZHOU A F,TAO C,et al.A methodology for urban roads network extraction from high resolution remote sensing imagery[J].Journal of Central South University(Science&amp;amp;Technology),2013,44(6):2385-2391.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNGD201306028&amp;v=MDk0OTBMTXFZOUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWprVXIvTVB5UE1hckc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        周家香,周安发,陶超,等.一种高分辨率遥感影像城区道路网提取方法[J].中南大学学报(自然科学版),2013,44(6):2385-2391.(ZHOU J X,ZHOU A F,TAO C,et al.A methodology for urban roads network extraction from high resolution remote sensing imagery[J].Journal of Central South University(Science&amp;amp;Technology),2013,44(6):2385-2391.)
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_9" title="曾发明,杨波,吴德文,等.基于Canny边缘检测算子的矿区道路提取[J].国土资源遥感,2013,25(4):72-78.(ZENGF M,YANG B,WU D W,et al.Extraction of roads in mining area based on Canny edge detection operator[J].Remote Sensing for Land&amp;amp;Resources,2013,25(4):72-78.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GTYG201304014&amp;v=MTE5ODdmWnVac0Z5amtVci9NSWpuU2FiRzRIOUxNcTQ5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        曾发明,杨波,吴德文,等.基于Canny边缘检测算子的矿区道路提取[J].国土资源遥感,2013,25(4):72-78.(ZENGF M,YANG B,WU D W,et al.Extraction of roads in mining area based on Canny edge detection operator[J].Remote Sensing for Land&amp;amp;Resources,2013,25(4):72-78.)
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_10" title="BEUMIER C,IDRISSA M.Building change detection from uniform regions[C]//Proceeddings of the 2012 Iberoamerican Congress on Pattern Recognition,LNCS 7441.Berlin:Springer,2012:648-655." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Building change detection from uniform regions">
                                        <b>[10]</b>
                                        BEUMIER C,IDRISSA M.Building change detection from uniform regions[C]//Proceeddings of the 2012 Iberoamerican Congress on Pattern Recognition,LNCS 7441.Berlin:Springer,2012:648-655.
                                    </a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_11" title="HUANG X,ZHANG L,ZHU T.Building change detection from multitemporal high-resolution remotely sensed images based on a morphological building index[J].IEEE Journal of Selected Topics in Applied Earth Observations&amp;amp;Remote Sensing,2013,7(1):105-115." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Building Change Detection From Multitemporal High-Resolution Remotely Sensed Images Based on a Morphological Building Index">
                                        <b>[11]</b>
                                        HUANG X,ZHANG L,ZHU T.Building change detection from multitemporal high-resolution remotely sensed images based on a morphological building index[J].IEEE Journal of Selected Topics in Applied Earth Observations&amp;amp;Remote Sensing,2013,7(1):105-115.
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_12" title="李炜明,吴毅红,胡占义.视角和光照显著变化时的变化检测方法研究[J].自动化学报,2009,35(5):449-461.(LI WM,WU Y H,HU Z Y.Urban change detection under large view and illumination variations[J].Acta Automatica Sinica,2009,35(5):449-461.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200905001&amp;v=MDc4NTlNS0NMZlliRzRIdGpNcW85RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtVci8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        李炜明,吴毅红,胡占义.视角和光照显著变化时的变化检测方法研究[J].自动化学报,2009,35(5):449-461.(LI WM,WU Y H,HU Z Y.Urban change detection under large view and illumination variations[J].Acta Automatica Sinica,2009,35(5):449-461.)
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_13" title="田昊,杨剑,汪彦明,等.基于先验形状约束水平集模型的建筑物提取方法[J].自动化学报,2010,36(11):1502-1511.(TIAN H,YANG J,WANG Y M,et al.Towards automatic building extraction:variational level set model using prior shape knowledge[J].Acta Automatica Sinica,2010,36(11):1502-1511.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201011003&amp;v=MzA5NDBGckNVUjdxZlp1WnNGeWprVXIvTUtDTGZZYkc0SDlITnJvOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        田昊,杨剑,汪彦明,等.基于先验形状约束水平集模型的建筑物提取方法[J].自动化学报,2010,36(11):1502-1511.(TIAN H,YANG J,WANG Y M,et al.Towards automatic building extraction:variational level set model using prior shape knowledge[J].Acta Automatica Sinica,2010,36(11):1502-1511.)
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                    LECUN Y,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_15" title="KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York:Curran Associates Inc.,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[15]</b>
                                        KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York:Curran Associates Inc.,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_16" title="HE K,GKIOXARI G,DOLLR P,et al.Mask R-CNN[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2961-2969." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[16]</b>
                                        HE K,GKIOXARI G,DOLLR P,et al.Mask R-CNN[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2961-2969.
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_17" title="LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[17]</b>
                                        LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651.
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_18" title="FARFADE S S,SABERIAN M J,LI L.Multi-view face detection using deep convolutional neural networks[C]//Proceedings of the5th ACM International Conference on Multimedia Retrieval.New York:ACM,2015:643-650." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Multi-view face detection using deep convolutional neural networks&amp;quot;">
                                        <b>[18]</b>
                                        FARFADE S S,SABERIAN M J,LI L.Multi-view face detection using deep convolutional neural networks[C]//Proceedings of the5th ACM International Conference on Multimedia Retrieval.New York:ACM,2015:643-650.
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                    RONNEBERGER O,FISCHER P,BROX T.U-net:convolutional networks for biomedical image segmentation[C]//MICCAI2015:Proceedings of the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer,2015:234-241.</a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_20" title="GLOROT X,BORDES A,BENGIO Y.Deep sparse rectifier neural networks[J].Journal of Machine Learning Research,2011,15:315-323." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep sparse rectifier neural networks">
                                        <b>[20]</b>
                                        GLOROT X,BORDES A,BENGIO Y.Deep sparse rectifier neural networks[J].Journal of Machine Learning Research,2011,15:315-323.
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_21" title="CLEVERT D,UNTERTHINER T,HOCHREITER S.Fast and accurate deep network learning by Exponential Linear Units(ELUs)[EB/OL].[2019-01-10].http://de.arxiv.org/pdf/1511.07289." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and accurate deep network learning by Exponential Linear Units(ELUs)">
                                        <b>[21]</b>
                                        CLEVERT D,UNTERTHINER T,HOCHREITER S.Fast and accurate deep network learning by Exponential Linear Units(ELUs)[EB/OL].[2019-01-10].http://de.arxiv.org/pdf/1511.07289.
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_22" title="CHOLLET F.Xception:deep learning with depthwise separable convolutions[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition,2017:1800-1807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">
                                        <b>[22]</b>
                                        CHOLLET F.Xception:deep learning with depthwise separable convolutions[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition,2017:1800-1807.
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_23" title="HOWARD A G,ZHU M,CHEN B,et al.Mobile Nets:efficient convolutional neural networks for mobile vision applications[EB/OL].[2019-01-10].https://arxiv.org/pdf/1704.04861.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobile Nets:efficient convolutional neural networks for mobile vision applications">
                                        <b>[23]</b>
                                        HOWARD A G,ZHU M,CHEN B,et al.Mobile Nets:efficient convolutional neural networks for mobile vision applications[EB/OL].[2019-01-10].https://arxiv.org/pdf/1704.04861.pdf.
                                    </a>
                                </li>
                                <li id="293">


                                    <a id="bibliography_24" title="KR&lt;image id=&quot;350&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910020_35000.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;HENB&lt;image id=&quot;351&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910020_35100.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;HL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of the 2011 International Conference on Neural Information Processing Systems.New York:Curran Associates Inc.,2011:109-117." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">
                                        <b>[24]</b>
                                        KR&lt;image id=&quot;350&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910020_35000.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;HENB&lt;image id=&quot;351&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910020_35100.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;HL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of the 2011 International Conference on Neural Information Processing Systems.New York:Curran Associates Inc.,2011:109-117.
                                    </a>
                                </li>
                                <li id="295">


                                    <a id="bibliography_25" title="ALTMAIER A,KANY C.Digital surface model generation from CORONA satellite images[J].ISPRS Journal of Photogrammetry and Remote Sensing,2002,56(4):221-235." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501968955&amp;v=MTQ3MTE4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSUY4VmFoWT1OaWZPZmJLN0h0RE5xbzlFYmUwSEJYaw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                        ALTMAIER A,KANY C.Digital surface model generation from CORONA satellite images[J].ISPRS Journal of Photogrammetry and Remote Sensing,2002,56(4):221-235.
                                    </a>
                                </li>
                                <li id="297">


                                    <a id="bibliography_26" title="许玥.基于改进Unet的遥感影像语义分割在地表水体变迁中的应用[D].重庆:重庆师范大学,2019:16-35.(XU Y.Application of semantic segmentation of remote sensing image based on improved unet in surface water change[D].Chongqing:Chongqing Normal University,2019:16-35.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1019132072.nh&amp;v=MjU4NzJyWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1VyL01WRjI2RjdLN0hOSEw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                        许玥.基于改进Unet的遥感影像语义分割在地表水体变迁中的应用[D].重庆:重庆师范大学,2019:16-35.(XU Y.Application of semantic segmentation of remote sensing image based on improved unet in surface water change[D].Chongqing:Chongqing Normal University,2019:16-35.)
                                    </a>
                                </li>
                                <li id="299">


                                    <a id="bibliography_27" title="ZHENG S,JAYASUMANA S,ROMERA-PAREDES B,et al.Conditional random fields as recurrent neural networks[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway:IEEE,2015:1529-1537." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional Random Fields as Recurrent Neural Networks">
                                        <b>[27]</b>
                                        ZHENG S,JAYASUMANA S,ROMERA-PAREDES B,et al.Conditional random fields as recurrent neural networks[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway:IEEE,2015:1529-1537.
                                    </a>
                                </li>
                                <li id="301">


                                    <a id="bibliography_28" title="GLOROT X,BENGIO Y.Understanding the difficulty of training deep feedforward neural networks[J].Journal of Machine Learning Research,2010,9:249-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">
                                        <b>[28]</b>
                                        GLOROT X,BENGIO Y.Understanding the difficulty of training deep feedforward neural networks[J].Journal of Machine Learning Research,2010,9:249-256.
                                    </a>
                                </li>
                                <li id="303">


                                    <a id="bibliography_29" title="ODENA A.Semi-supervised learning with generative adversarial networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1606.01583.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with generative adversarial networks">
                                        <b>[29]</b>
                                        ODENA A.Semi-supervised learning with generative adversarial networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1606.01583.pdf.
                                    </a>
                                </li>
                                <li id="305">


                                    <a id="bibliography_30" title="HUANG B,ZHAO B,SONG Y.Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery[J].Remote Sensing of Environment,2018,214:73-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES4CED1B8095ECFB349DC260F200A21550&amp;v=MTg5MzJRbGZCckxVMDV0cGh4TG0rd0t3PU5pZk9mYmZMYTZYTjNZZEZiZTU2ZndwTHpCSWFua3gvVG4rVXJoSTFDTENWUUwrZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPRw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[30]</b>
                                        HUANG B,ZHAO B,SONG Y.Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery[J].Remote Sensing of Environment,2018,214:73-86.
                                    </a>
                                </li>
                                <li id="307">


                                    <a id="bibliography_31" title="VOLPI M,TUIA D.Dense semantic labeling of subdecimeter resolution images with convolutional neural networks[J].IEEETransactions on Geoscience and Remote Sensing,2016,55(2):881-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dense semantic labeling of subdecimeter resolution images with convolutional neural networks">
                                        <b>[31]</b>
                                        VOLPI M,TUIA D.Dense semantic labeling of subdecimeter resolution images with convolutional neural networks[J].IEEETransactions on Geoscience and Remote Sensing,2016,55(2):881-893.
                                    </a>
                                </li>
                                <li id="309">


                                    <a id="bibliography_32" title="SHERRAH J.Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery[EB/OL].[2019-01-10].https://arxiv.org/pdf/1606.02585.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery">
                                        <b>[32]</b>
                                        SHERRAH J.Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery[EB/OL].[2019-01-10].https://arxiv.org/pdf/1606.02585.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-21 10:37</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2905-2914 DOI:10.11772/j.issn.1001-9081.2019030529            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习模型的遥感图像分割方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AE%B8%E7%8E%A5&amp;code=42251588&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">许玥</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E6%A2%A6%E5%A6%82&amp;code=42899617&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯梦如</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%9A%AE%E5%AE%B6%E7%94%9C&amp;code=39411722&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">皮家甜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%8B%87&amp;code=14203418&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈勇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0124704&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆师范大学计算机与信息科学学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E5%B8%82%E6%95%B0%E5%AD%97%E5%86%9C%E4%B8%9A%E6%9C%8D%E5%8A%A1%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆市数字农业服务工程研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>利用遥感图像快速准确地检测地物信息是当前的研究热点。针对遥感图像地表物的传统人工目视解译分割方法效率低下和现有基于深度学习的遥感图像分割算法在复杂场景下准确率不高、背景噪声多的问题,提出一种基于改进的U-net架构与全连接条件随机场的图像分割算法。首先,融合VGG16和U-net构建新的网络模型,以有效提取具有高背景复杂度的遥感图像特征;然后,通过选取适当的激活函数和卷积方式,在提高图像分割准确率的同时显著降低模型预测时间;最后,在保证分割精度的基础上,使用全连接条件随机场进一步优化分割结果,以获得更加细致的分割边缘。在ISPRS提供的标准数据集Potsdam上进行的仿真测试表明,相较于U-net,所提算法的准确率、召回率和均交并比(MIoU)分别提升了15.06个百分点、29.11个百分点和0.366 2,平均绝对误差(MAE)降低了0.028 92。实验结果验证了该算法具备有效性和鲁棒性,是一种有效的遥感图像地表物提取算法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度可分离卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E8%BF%9E%E6%8E%A5%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全连接条件随机场;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    许玥(1993—),男,甘肃酒泉人,硕士研究生,主要研究方向:深度学习、计算机视觉;;
                                </span>
                                <span>
                                    冯梦如(1995—),女,陕西榆林人,硕士研究生,主要研究方向:深度学习、计算机视觉;;
                                </span>
                                <span>
                                    *皮家甜(1990—),男,湖北潜江人,讲师,博士,主要研究方向:计算机视觉、机器人视觉感知;电子邮箱pijiatian@cqnu.edu.cn;
                                </span>
                                <span>
                                    陈勇(1971—),男,重庆人,副教授,博士,主要研究方向:数字图像处理、信息安全。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-01</p>

            </div>
                    <h1><b>Remote sensing image segmentation method based on deep learning model</b></h1>
                    <h2>
                    <span>XU Yue</span>
                    <span>FENG Mengru</span>
                    <span>PI Jiatian</span>
                    <span>CHEN Yong</span>
            </h2>
                    <h2>
                    <span>College of Computer and Information Science, Chongqing Normal University</span>
                    <span>Chongqing Engineering Research Center for Digital Agricultural Service</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To detect surface object information quickly and accurately by using remote sensing images is a current research hot spot. In order to solve the problems of inefficiency of the traditional manual visual interpretation segmentation method as well as the low accuracy and a lot of background noise of the existing remote sensing image segmentation based on deep learning in complex scenes, an image segmentation algorithm based on improved U-net network architecture and fully connected conditional random field was proposed. Firstly, a new network model was constructed by integrating VGG16 and U-net to effectively extract the features of remote sensing images with highly complex background. Then, by selecting the appropriate activation function and convolution method, the image segmentation accuracy was improved while the model prediction time was significantly reduced. Finally, on the basis of guaranteeing the segmentation accuracy, the segmentation result was further improved by using fully connected conditional random field. The simulation test on the standard dataset Potsdam provided by ISPRS showed that the accuracy, recall and the Mean Intersection over Union(MIoU) of the proposed algorithm were increased by 15.06 percentage points, 29.11 percentage points and 0.366 2 respectively, and the Mean Absolute Error(MAE) of the algorithm was reduced by 0.028 92 compared with those of U-net. Experimental results verify that the proposed algorithm is an effective and robust algorithm for extracting surface objects from remote sensing images.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depth%20separable%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depth separable convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fully%20connected%20conditional%20random%20field&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fully connected conditional random field;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XU Yue,born in 1993,M.S.candidate.His research interests include deep learning,computer vision.;
                                </span>
                                <span>
                                    FENG Mengru,born in 1995,M.S.candidate.Her research interests include deep learning,computer vision.;
                                </span>
                                <span>
                                    PI Jiatian,born in 1990,Ph.D.,lecturer.His research interests include computer vision,robot visual perception.;
                                </span>
                                <span>
                                    CHEN Yong,born in 1971,Ph.D.,associate professor.His research interests include digital image processing,information security.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-01</p>
                            </div>


        <!--brief start-->
                        <h3 id="67" name="67" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="68">对地物分割的研究已有几十年的历史,国内外学者针对不同的应用场景和数据源,提出并发表了众多方法和研究成果。传统的分割方法多基于阈值设定,所以针对不同地物的分布、形状、结构、纹理与色调信息,<citation id="311" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>采用的方法也不一。</p>
                </div>
                <div class="p1">
                    <p id="69">针对植被分割,巫兆聪等<citation id="312" type="reference"><link href="249" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>结合光谱纹理和形状结构信息分割森林植被,改善了分割质量;Yuan等<citation id="313" type="reference"><link href="251" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出结合光谱和纹理特征的遥感图像分割方法提高了不同地物目标的分割效率和准确度;Palenichka等<citation id="314" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出基于视觉注意的森林植被多尺度分割方法。针对不透水表面分割有最近邻<citation id="315" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、支持向量机<citation id="316" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、隶属度函数<citation id="317" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、形态学滤波<citation id="318" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、矢量化<citation id="319" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等方法。针对建筑物分割有均质区域识别<citation id="320" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、形态学房屋指数计算<citation id="321" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、聚类提取城市变化<citation id="322" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、先验形状约束水平集模型<citation id="323" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>等方法和模型。以上方法只能分割单一种类地物,针对某种地物的信息特征采用对应方法,所以以上方法不能用于解决本文的多分类问题。</p>
                </div>
                <div class="p1">
                    <p id="70">现阶段深度学习被普遍应用在计算机视觉领域,其中卷积神经网络(Convolutional Neural Network, CNN)<citation id="324" type="reference"><link href="273" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>以其局部权值共享的特殊结构以及良好的容错能力、并行处理能力和自学习能力被广泛地应用于图像分类<citation id="325" type="reference"><link href="275" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、对象检测<citation id="326" type="reference"><link href="277" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、语义分割<citation id="327" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、人脸识别<citation id="328" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>等诸多计算机视觉领域。</p>
                </div>
                <div class="p1">
                    <p id="71">在Long等<citation id="329" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了全卷积网络(Fully Convolutional Network, FCN)原理之后,卷积神经网络的分类目标由对象精确至像素,拓展到了语义分割领域,这种end-to-end的全卷积神经网络被用来解决PASCAL VOC2012和Microsoft COCO等数据集的像素分类问题,达到了很好的效果并被作为基准。这些数据集来自日常人类视角的生活场景,而在本文中所使用的是地球观测数据,因同为语义分割任务则选用与FCN同样是全卷积神经网络的encoder-decoder结构的U-net<citation id="330" type="reference"><link href="283" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>作为基础构架,但由于U-net构架较浅不能明确表征复杂的地物特征,所以在基础构架上增加网络层数以表征更高维的特征信息,构建为D-Unet;针对U-net的激活函数——线性整流函数(Rectified Linear Unit, ReLU)<citation id="331" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>易使神经元失活的问题,在D-Unet中使用ELU(Exponential Linear Units)函数<citation id="332" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>替换;针对加深网络层而导致的模型体积与参数量激增的问题,使用深度可分离卷积(Depthwise Separable Convolution)<citation id="335" type="reference"><link href="289" rel="bibliography" /><link href="291" rel="bibliography" /><sup>[<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>替换标准卷积构建新的轻量级网络模型DS-Unet,降低训练与预测时的计算量,提高模型运行效率;针对神经网络过拟合问题,使用Dropout<citation id="333" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>降低网络层节点间的关联性,提高了模型泛化能力;最后将神经网络输出结果作为全连接条件随机场(Fully Connected Conditional Random Field)<citation id="334" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>的输入,对分割结果进一步优化,进行对比实验。改进的模型拥有很强的学习能力,拥有较强的泛化能力,性能稳定、鲁棒性强。</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag">1 数据集及其预处理</h3>
                <div class="p1">
                    <p id="73">研究采用国际摄影测量与遥感学会(International Society for Photogrammetry and Remote Sensing, ISPRS)提供的机载图像数据集2D Semantic Labeling Potsdam。Potsdam是一座典型的历史悠久的城市,拥有大型建筑、狭窄的街道和密集的沉降结构,这为遥感图像地物测绘提供了先决条件。该数据集包括高分辨率的真实正射影像(True Ortho Photo, TOP)和从密集图像匹配技术派生的数字表面模型(Digital Surface Model, DSM)<citation id="336" type="reference"><link href="295" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。TOP和DSM的地面采样距离为5 cm,这为分割后依据像素点统计分类目标地物提供先决条件。该数据集包含38个(相同大小的)TOP区块(6 000×6 000 像素),如图1所示。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 所有区块的轮廓与正射影像镶嵌重叠" src="Detail/GetImg?filename=images/JSJY201910020_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 所有区块的轮廓与正射影像镶嵌重叠  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Overlapping of contours and ortho photo for all blocks</p>

                </div>
                <div class="p1">
                    <p id="75">数据集被手动分类为6个最常见的土地覆盖类别、地物类型及其基准值(ground truth, gt)如表1所示。</p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表</b>1 <b>六种地物类型及其基准值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.1 Six categories of surface objects and their ground truths</p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td><br />地物类型</td><td>颜色通道(<i>R</i>,<i>G</i>,<i>B</i>)</td></tr><tr><td><br />   Impervious surfaces</td><td>255,255,255</td></tr><tr><td><br />   Building</td><td>0,0,255</td></tr><tr><td><br />   Low vegetation</td><td>0,255,255</td></tr><tr><td><br />   Tree</td><td>0,255,0</td></tr><tr><td><br />   Car</td><td>255,255,0</td></tr><tr><td><br />   Clutter/background</td><td>255,0,0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">1.1 <b>基准值处理</b></h4>
                <div class="p1">
                    <p id="78">本文分割目标为4类(植被、不透水表面、建筑、背景)。因Potsdam数据集提供的gt共分为6类(不透水表面、建筑、低植被、树木、车辆、背景),而本文所探讨的是遥感图像地物测绘,不透水表面上的车辆并不作为地物信息所统计,因车辆与不透水表面在二维空间上重合,则车辆类归类为不透水表面类。本文主要探讨卷积模型的优化改进,卷积神经网络的输入只有R、G、B三个通道,没有使用数据集所提供的DSM;又因低植被与树木的光谱、形状、空间信息相似,必须借助于DSM作为分类的依据,则最终把树木与低植被归为植被类,用以统计植被覆盖。不透水表面与车辆、低植被与树木的类别合并后详见表2。</p>
                </div>
                <div class="p1">
                    <p id="79">图2为语义对象分类的示例。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 语义对象分类示例" src="Detail/GetImg?filename=images/JSJY201910020_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 语义对象分类示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Example of semantic object classification</p>

                </div>
                <div class="p1">
                    <p id="81">数据集中提供的gt的每个通道的光谱分辨率为8位,而深度学习框架需要输入的gt为灰度图像,通过对RGB图像的<i>R</i>、<i>G</i>、<i>B</i>三个分量进行加权平均(加权平均算法)达到灰度化处理的目的,加权平均算法如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="82"><i>g</i>=0.298 9<i>R</i>+0.587 0<i>G</i>+0.114 0<i>B</i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="83">RGB值与灰度值的转化对应关系如表2所示。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表</b>2 <b>本文四种地物类型及其基准值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.2 Four categories of surface objects in this paper and their ground truths</p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td><br />Potsdam<br />地物类型</td><td>颜色通道<br />(<i>R</i>,<i>G</i>,<i>B</i>)</td><td>本文<br />地物类型</td><td>颜色通道<br />(Gray)</td></tr><tr><td><br />Clutter/<br />background</td><td>255,0,0</td><td>Clutter/<br />background</td><td>0</td></tr><tr><td><br />Low vegetation<br />Tree</td><td>0,255,255<br />0,255,0</td><td>Vegetation</td><td>1</td></tr><tr><td><br />Car<br />Impervious surfaces</td><td>255,255,0<br />255,255,255</td><td>Impervious <br />surfaces</td><td>2</td></tr><tr><td><br />Building</td><td>0,0,255</td><td>Building</td><td>3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">1.2 <b>使用</b>eCognition<b>标注未标注数据</b></h4>
                <div class="p1">
                    <p id="86">原数据集提供38个区块的TOP,仅部分TOP提供标记的gt,其余场景的gt未发布,所以使用eCognition对未标注的14个TOP进行标注。</p>
                </div>
                <div class="p1">
                    <p id="87">eCognition采用面向对象的分类技术对像素进行分类,这是一种基于目标对象的分类方法,因其能充分利用遥感图像的光谱、纹理、形状、空间信息、相邻关系等特征对像素分类,所以精度相对较高,能够接近人工目视解译精度,所以本文采用此方法标注剩余数据。面向对象的技术有两个重要特征和技术关键:选取合适的分割尺度对图像进行分割,使检测的地物能在最合适(图像首先被分割成一个个object,然后进行sample标记,最合适意味着用最少的object表达最为精确的地物边缘)的分割尺度中凸显出来;选取分割对象的多种典型特征建立地物的分类规则进行检测或分类。<citation id="337" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation></p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 U-net架构" src="Detail/GetImg?filename=images/JSJY201910020_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 U-net架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 U-net architecture</p>

                </div>
                <div class="p1">
                    <p id="89">本文首先使用eCognition中的multiresolution segmentation算法根据不同图像的特征设定不同的参数对地物信息进行初始分割,遥感图像(图3(a))的部分区域(图4(a))的初始分割结果如图4(b)所示,此图像采用的参数Scale Parameter、Shape及Compactness分别为100、0.1与0.5。其中Scale Parameter表示分割的区块大小,一般参数设置越小,区块越小,分割越为精细;Shape表示形状参数,它与color(颜色参数)的权重和为1;compactness代表紧凑度,它与smoothness(平滑度)的权重和为1。然后选取合适的特征作为地物的分类规则,本文针对不同地物的光谱反射不同选择Layer Values特征,依据不透水表面与建筑物的面积、长宽差异度大而选定Area(Pxl)、Length/Width特征,依据植被与其他地物的边界光滑性差异度大而选择shape index(地物边长与其面积开四次方的比值)特征。最后使用eCognition中的classification算法对每个像素点进行分类,得到gt,如图3(b)所示。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 使用eCognition标注的遥感图像及其基准值" src="Detail/GetImg?filename=images/JSJY201910020_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 使用eCognition标注的遥感图像及其基准值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Remote sensing images marked with eCognition and its ground truth</p>

                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 遥感图像及其初始分割" src="Detail/GetImg?filename=images/JSJY201910020_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 遥感图像及其初始分割  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Remote sensing image and its initial segmentation map</p>

                </div>
                <h3 id="92" name="92" class="anchor-tag">2 遥感图像语义分割模型</h3>
                <div class="p1">
                    <p id="93">本文改进的遥感图像语义分割深度神经网络命名为D-Unet与DS-Unet,用于从遥感图像中提取有效的地物信息。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94">2.1 <b>基于改进</b>U-net<b>的遥感图像语义分割模型</b></h4>
                <h4 class="anchor-tag" id="95" name="95">2.1.1 改进U-net构架</h4>
                <div class="p1">
                    <p id="96">U-net常被用于医学图像的分割,它所处理的医学图像背景单一、复杂度低,所以使用低复杂度的基础模型U-net能够达到很高的精度且在精度与复杂度之间达到平衡,U-net的网络架构如图5所示。</p>
                </div>
                <div class="p1">
                    <p id="97">本文所使用的Potsdam遥感图像数据背景复杂,包含丰富的地物种类,且遥感光谱范围广泛,U-net并不能有效地提取复杂的遥感图像的像素特征,所以通过加深U-net的深度构建D-Unet以提取更加复杂的光谱特征。D-Unet的网络架构如图6所示。</p>
                </div>
                <div class="p1">
                    <p id="98">网络的左半部分为下采样模块,依据VGG16所构建,它是卷积神经网络中的一种典型结构,通过逐渐缩减输入数据的空间维度以提取高维特征。其核心为5组conv与MaxPooling,其中第1、2组采用2次3×3的卷积运算,卷积核数量分别是64与128,第3、4、5组采用3次3×3的卷积运算,卷积核数量分别为256、512、512。在每一个卷积运算以后加入BN(Batch Normalization)层,对网络层的每一层的特征都做归一化,使得每层的特征分布更加均匀,在提高模型收敛速度的同时又能够提高模型的容错能力。<citation id="338" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation></p>
                </div>
                <div class="p1">
                    <p id="99">网络的右半部分与左半部分呈中心对称,它由一系列的上采样层构成,其核心为与下采样相对应的5组Upsampling与conv,每一组conv的输入除了上一层进行Upsampling得到的深层抽象特征外,还有与其对应的下采样层输出的浅层局部特征,将深层特征与浅层特征通过Concatente方式融合,从而恢复了特征图细节并保证其相应的空间信息维度不变。<citation id="339" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation></p>
                </div>
                <div class="p1">
                    <p id="100">具体来说,将类别表示为<i>δ</i>={0,1,…,<i>M</i>}(本文中<i>M</i>=3),设定输入的RGB遥感图像与其gt共有<i>N</i>个,表示为集合形式为:</p>
                </div>
                <div class="p1">
                    <p id="101">{(<i><b>R</b></i><sub><i>i</i></sub>,<i><b>G</b></i><sub><i>i</i></sub>)|<i><b>R</b></i><sub><i>i</i></sub>∈<i>δ</i><sup><i>H</i></sup><sup>×</sup><sup><i>B</i></sup><sup>×3</sup>,<i><b>G</b></i><sub><i>i</i></sub>∈<i>δ</i><sup><i>H</i></sup><sup>×</sup><sup><i>B</i></sup>;<i>i</i>=0,1,…,<i>N</i>}</p>
                </div>
                <div class="p1">
                    <p id="102">其中:<i><b>R</b></i><sub><i>i</i></sub>表示输入的遥感图像;<i><b>G</b></i><sub><i>i</i></sub>表示与遥感图像相对应的gt;<i>H</i>与<i>B</i>分别表示遥感图像的高与宽。网络层的参数为<i><b>W</b></i>=<i><b>w</b></i><sup>(1)</sup>,<i><b>w</b></i><sup>(2)</sup>,…,<i><b>w</b></i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>,其中<i>L</i>为网络层数。网络的各个层定义为<i>t</i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>(<i>r</i>;<i><b>w</b></i><sup><i>L</i></sup>),则模型定义如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="103"><i>f</i>(<i>r</i>;<i><b>W</b></i>)=<i>t</i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>(<i>t</i><sup>(</sup><sup><i>L</i></sup><sup>-1)</sup>(…<i>t</i><sup>(2)</sup>(<i>t</i><sup>(1)</sup>(<i>r</i>;<i><b>w</b></i><sup>(1)</sup>);<i><b>w</b></i><sup>(2)</sup>)…;</p>
                </div>
                <div class="p1">
                    <p id="104"><i><b>w</b></i><sup>(</sup><sup><i>L</i></sup><sup>-1)</sup>);<i><b>w</b></i><sup>(</sup><sup><i>L</i></sup><sup>)</sup>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="105">其中: <i>f</i>(<i>r</i>;<i><b>W</b></i>)的第<i>o</i>个分量<i>f</i><sub><i>o</i></sub>(<i>r</i>;<i><b>W</b></i>)表示像素<i>r</i>属于类别<i>o</i>的得分。</p>
                </div>
                <div class="p1">
                    <p id="106">在网络层的最后使用softmax函数计算输入遥感图像像素点属于某类的概率,定义如式(3)所示:</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><mo stretchy="false">(</mo><mi>o</mi><mo stretchy="false">|</mo><mi>r</mi><mo>,</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>o</mi></msub><mo stretchy="false">(</mo><mi>r</mi><mo>;</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mi>Μ</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><mi>r</mi><mo>;</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">最终使用多分类的对数损失函数计算预测值与真实值之间的差值并不断修正,得到最优参数<i><b>W</b></i><sup>*</sup>,定义如式(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ζ</mi><mo>,</mo><mspace width="0.25em" /><mi>ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">R</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mi>log</mi><mspace width="0.25em" /><mi>ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">R</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>Ν</mi></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Η</mi><mi mathvariant="bold-italic">W</mi></mrow></munderover><mrow></mrow></mstyle><mi>r</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mspace width="0.25em" /><mi>log</mi><mspace width="0.25em" /><mi>ρ</mi><mo stretchy="false">(</mo><mi>g</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">|</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 D-Unet架构" src="Detail/GetImg?filename=images/JSJY201910020_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 D-Unet架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 D-Unet architecture</p>

                </div>
                <h4 class="anchor-tag" id="111" name="111">2.1.2 选取激活函数</h4>
                <div class="p1">
                    <p id="112">图像分割是对像素点进行逐一分类,因线性模型的表达能力不足,则引入激活函数加入非线性因素以解决分类问题。</p>
                </div>
                <div class="p1">
                    <p id="113">U-net中使用的是常见的ReLU激活函数,其定义如式(5)所示:</p>
                </div>
                <div class="p1">
                    <p id="114"><i>f</i>(<i>x</i>)=max(0,<i>x</i>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="115">从图7可看出:因其只有线性关系所以计算速度很快,虽然其在输入值为正时不存在梯度饱和的问题,但是一旦输入值为负则神经元不被激活。尽管此问题在前向传播过程中影响不大,但在反向传播过程中梯度会变为0,这将导致神经元失活。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 ReLU与ELU函数" src="Detail/GetImg?filename=images/JSJY201910020_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 ReLU与ELU函数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 ReLU &amp; ELU functions</p>

                </div>
                <div class="p1">
                    <p id="117">ELU函数是针对ReLU函数的一个改进型,其定义如式(6)所示:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>a</mi><mo stretchy="false">(</mo><mi>exp</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mi>x</mi><mo>≤</mo><mn>0</mn></mtd></mtr><mtr><mtd><mi>x</mi><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">从图7可看出:相对于ReLU函数,ELU函数在输入为负值时有输出,而且这部分输出还具有一定的抗干扰能力,这样可以消除在反向传播过程中的神经元失活问题。由于其特性,使用ELU激活函数替换ReLU函数构建的D-Unet(ELU)模型的分类的准确率比D-Unet(ReLU)高。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">2.1.3 更改卷积方式</h4>
                <div class="p1">
                    <p id="121">在2.1.1节中为了增强网络的特征提取能力加深网络层,导致模型参数急剧增加,使得模型训练、预测时计算时间冗长,受到的<i>Xception</i><citation id="340" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>与<i>MobileNet</i><citation id="341" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>的启发,使用深度可分离卷积替代标准卷积以减少网络模型的参数量。<citation id="342" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation></p>
                </div>
                <div class="p1">
                    <p id="122"><i>Xception</i><citation id="343" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>中指出,对于卷积来说卷积核可以看作是一个三维的滤波器:通道维+空间维(<i>Feature Map</i>的宽和高),常规的卷积操作其实就是实现通道相关性和空间相关性的联合映射。在深度可分离卷积中提出将通道和区域分离,也就是对空间信息和深度信息进行去耦,将空间信息与深度信息分开映射能够达到更好的效果,并且在此过程中通过拆分标准卷积有效地减少了参数量,降低了模型的复杂度,同时提高了模型的泛化能力<citation id="344" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>。图8为标准卷积核。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 标准卷积" src="Detail/GetImg?filename=images/JSJY201910020_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 标准卷积  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.8 <i>Standard convolution</i></p>

                </div>
                <div class="p1">
                    <p id="124">假设输入特征图大小为<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>×<i>M</i>,输出特征图大小为<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>×<i>N</i>,卷积核大小为<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>,则标准卷积的计算量如式(7)所示:</p>
                </div>
                <div class="p1">
                    <p id="125"><i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×<i>M</i>×<i>N</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>      (7)</p>
                </div>
                <div class="p1">
                    <p id="126">图9与图10分别为深度卷积与点卷积。</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 深度卷积" src="Detail/GetImg?filename=images/JSJY201910020_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 深度卷积  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Depthwise convolution</p>

                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 点卷积" src="Detail/GetImg?filename=images/JSJY201910020_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 点卷积  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Pointwise convolution</p>

                </div>
                <div class="p1">
                    <p id="129">深度可分离卷积的计算量为深度卷积和点卷积的计算量之和,如式(8)所示:</p>
                </div>
                <div class="p1">
                    <p id="130"><i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×<i>M</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>+<i>M</i>×<i>N</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>      (8)</p>
                </div>
                <div class="p1">
                    <p id="131">深度可分离卷积与传统卷积的计算量之比,如式(9)所示:</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>×</mo><mi>Μ</mi><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub><mo>+</mo><mi>Μ</mi><mo>×</mo><mi>Ν</mi><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub></mrow><mrow><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>×</mo><mi>Μ</mi><mo>×</mo><mi>Ν</mi><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><mi>D</mi><msub><mrow></mrow><mi>k</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">当卷积核大小为3×3时,理论上深度可分离卷积的计算量为标准卷积的1/9。</p>
                </div>
                <div class="p1">
                    <p id="134">本文主要利用深度可分离卷积降低参数量的特性,在D-Unet(ELU)基础上构建轻量级模型DS-Unet(ELU),其参数量大致减为原参数量的1/6,模型的预测时间大致为原时间的1/3。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">2.1.4 预防过拟合</h4>
                <div class="p1">
                    <p id="136">过拟合是所有深度模型在训练过程中都会遇到的问题,一般可以采用<i>Dropout</i>正则化解决。<i>Dropout</i>是以某个概率值暂时丢弃隐藏层的神经元来达到对网络进行“瘦身”的目的以降低网络的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="137">当某一节点在某次迭代中被随机选为抛弃点,那么神经网络在此次迭代中的<i>forward</i>过程会将此节点输出设为0,在<i>backward</i>过程中不会更新其权重和偏置项,则在某次迭代中节点随机失活不会参与训练,减弱了神经元节点间的联合适应性,增强了泛化能力。在此模型中添加rate=0.5的<i>Dropout</i>层用以防止过拟合。</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138">2.2 <b>基于全连接条件随机场的细分割</b></h4>
                <div class="p1">
                    <p id="139">全卷积神经网络虽然能够实现像素级别的分类,但是得到的分割结果往往不够精细,存在边界不平滑和像素点定位不准确等问题,主要原因在于全卷积网络在像素点分类过程中很难考量到像素与像素之间的空间关系,导致像素级分类结果缺乏空间一致性。已有研究表明,使用全卷积网络得到像素级分类结果之后,再使用条件随机场(<i>Conditional Random Field</i>, <i>CRF</i>)综合图像的空间信息,能够得到更加精细并且具有空间一致性的结果<citation id="345" type="reference"><link href="293" rel="bibliography" /><link href="299" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">27</a>]</sup></citation>。针对本文中的分割问题,使用条件随机场考量像素点之间的空间位置关系,可进一步改进像素分割结果。条件随机场试图对多个变量在给定观测值后的条件概率进行建模。具体来说,若令<i><b>X</b></i>=(<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>)为观测向量,<i><b>Y</b></i>=(<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>,…, <i>y</i><sub><i>n</i></sub>)为与之相应的标记向量,则条件随机场的目标是构建条件概率模型<i>p</i>(<i>x</i>|<i>y</i>)。</p>
                </div>
                <div class="p1">
                    <p id="140">条件随机场的能量函数<i>E</i>(<i><b>Y</b></i>|<i><b>X</b></i>)主要由一阶势函数<i>φ</i><sup>1</sup>和二阶势函数<i>φ</i><sup>2</sup>组成。在像素级分类任务中,通过训练使条件随机场的势能最小,则可以使相似的像素有较大的概率分为同一类别,定义如式(10)所示:</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>Ι</mi></munder><mrow></mrow></mstyle><mi>φ</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>e</mi></munder><mrow></mrow></mstyle><mi>φ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">其中:<i>I</i>表示输入图像的像素集合;<i>x</i><sub><i>i</i></sub>为输入图像像素观测向量<i><b>X</b></i>=(<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>N</i></sub>)的任意一个值,<i>N</i>为输入图像的像素个数; <i>y</i><sub><i>i</i></sub>为输入图像像素标签类别向量<i><b>Y</b></i>=(<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>,…, <i>y</i><sub><i>N</i></sub>)中的任意一个值; <i>y</i><sub><i>j</i></sub>为输入图像像素邻近像素被划分的类别;<i>e</i>表示图像中<i>x</i><sub><i>i</i></sub>与其邻域像素的集合。在全连接条件随机场中,像素归属其对应标签的势能函数定义如式(11)所示:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>φ</mi></mstyle><msub><mrow></mrow><mi>u</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></munder><mi>φ</mi></mstyle><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">其中:<i>E</i>(<i>x</i>)为像素归属标签的总能量;<i>φ</i><sub><i>u</i></sub>(<i>x</i><sub><i>i</i></sub>)为一阶势能,表示为当不考虑像素间关系时将像素<i>i</i>归属为标签<i>x</i><sub><i>i</i></sub>的能量;<i>φ</i><sub><i>p</i></sub>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)为点对能量势函数,描述像素点间的空间位置关系。全卷积网络中的输出为像素点属于某种类别的概率值,这与一阶势能所表达的概念一样。使用全卷积网络的输出当作一阶势能的输入,二阶势能函数对像素点之间的关系进行约束,保证相邻的像素能够在更大的程度上被归为同一类别,其定义如式(12)所示:</p>
                </div>
                <div class="p1">
                    <p id="145" class="code-formula">
                        <mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>φ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub></mtd></mtr><mtr><mtd><mi>e</mi><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo></mrow></msup><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≠</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="146">其中:<i><b>I</b></i><sub><i>i</i></sub>和<i><b>I</b></i><sub><i>j</i></sub>分别表示像素<i>i</i>和<i>j</i>的特征向量;‖<i><b>I</b></i><sub><i>i</i></sub>-<i><b>I</b></i><sub><i>j</i></sub>‖表示像素间的差异。</p>
                </div>
                <div class="p1">
                    <p id="147">将全卷积神经网络最后一层的输出作为全连接条件随机场的输入,则与D-Unet构建为D-Unet_CRF,如图11所示。</p>
                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 D-Unet_CRF模型" src="Detail/GetImg?filename=images/JSJY201910020_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 D-Unet_CRF模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.11 D-Unet_CRF model</p>

                </div>
                <h3 id="149" name="149" class="anchor-tag">3 实验与分析</h3>
                <h4 class="anchor-tag" id="150" name="150">3.1 <b>实验</b></h4>
                <h4 class="anchor-tag" id="151" name="151">3.1.1 实验环境</h4>
                <div class="p1">
                    <p id="152">本文实验环境分为网络训练和网络测试两个部分。</p>
                </div>
                <div class="p1">
                    <p id="153">网络训练部分使用<i>TeslaV</i>100,内存16 <i>GB</i>,共享内存8 <i>GB</i>;软件环境为<i>Ubuntu</i>16.04,<i>Python</i>3.5,<i>TensorFlow</i>1.9.0,<i>Keras</i>2.2.4。</p>
                </div>
                <div class="p1">
                    <p id="154">网络测试部分使用<i>MacBookPro</i>,<i>CPU</i> 2.2 <i>GHz Intel Core i</i>7,内存16 <i>GB</i>,显卡<i>Intel Iris Pro</i> 1 536 <i>MB</i>;软件环境为<i>MacOS Mojave</i>10.14.3,<i>Python</i>3.5,<i>Tensorflow</i>1.9.0,<i>Keras</i>2.2.4。</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">3.1.2 数据增强</h4>
                <div class="p1">
                    <p id="156">在第1章中的38张<i>TOP</i>及其对应的<i>gt</i>中随机选取24张进行随机切割、旋转、镜像、模糊、光照调整以及噪声(高斯噪声、椒盐噪声)等数据增强处理,生成30万张256×256大小的训练及验证集,剩余的14张<i>TOP</i>及其对应的<i>gt</i>用作测试集。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157">3.1.3 模型训练</h4>
                <div class="p1">
                    <p id="158">在3.1.1节中的实验环境下对图5所示的网络模型进行训练。模型在使用<i>Tensorflow</i>作为后台的<i>Keras</i>深度学习框架下训练。权值初始化采用<i>Glorot</i>等<citation id="346" type="reference"><link href="301" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>提出的方法。模型训练采用批处理方式,将300 000×0.75(75%的图片作为训练集,其余作为验证集)张图片每32张作为一个批次(<i>batch</i>)输入模型进行训练,总共需要3 984个批次完成一个<i>epoch</i>,设定模型总共遍历数据集50轮。训练时使用<i>multi</i>_<i>gpu</i>_<i>model</i>()函数同时调用4个<i>GPU</i>进行,优化器选用<i>sgd</i>,损失函数使用多分类的对数损失函数。在模型训练的过程中使用<i>Callbacks</i>函数中的<i>ModelCheckpoint</i>存储最优模型,设定监测对象为准确率,当它最大时自动保存最优权值;训练时使用<i>Callbacks</i>函数中的<i>EarlyStopping</i>监测模型训练情况,当达到指标时可提前终止训练,节省时间并且能够预防过拟合;训练过程中的学习率使用<i>Callbacks</i>函数中的<i>ReduceLROnPlateau</i>自适应调整,以适应训练过程中的动态变化,如图12所示;训练过程使用<i>Tensorboard</i>监测并可视化。训练模型的部分参数如表3所示。</p>
                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表</b>3 <b>训练模型的部分超参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.3 <i>Partial hyperparameters of training model</i></p>
                    <p class="img_note"></p>
                    <table id="159" border="1"><tr><td><br />参数</td><td>值</td></tr><tr><td><br /><i>batchsize</i></td><td>32</td></tr><tr><td><br /><i>max</i>_<i>epoch</i></td><td>50</td></tr><tr><td><br /><i>base</i>_<i>lr</i></td><td>0.01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="160">从图13～14可看出,随着迭代次数的增加,曲线波动减小,准确率与训练损失变化趋于平缓,模型趋于收敛。图15～16反映了随着迭代次数的增加,验证集的准确率与损失值变化趋于平缓,损失函数基本收敛,表明模型达到了最优。</p>
                </div>
                <div class="area_img" id="161">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 学习率与迭代次数的关系" src="Detail/GetImg?filename=images/JSJY201910020_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 学习率与迭代次数的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.12 <i>Relationship between learning rate and iteration times</i></p>

                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 准确率与迭代次数的关系" src="Detail/GetImg?filename=images/JSJY201910020_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 准确率与迭代次数的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.13 <i>Relationship between accuracy and iteration times</i></p>

                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 训练损失与迭代次数的关系" src="Detail/GetImg?filename=images/JSJY201910020_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 训练损失与迭代次数的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.14 <i>Relationship between training loss and iteration times</i></p>

                </div>
                <div class="area_img" id="164">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 验证准确率与迭代次数的关系" src="Detail/GetImg?filename=images/JSJY201910020_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图15 验证准确率与迭代次数的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.15 <i>Relationship between validation accuracy and iteration times</i></p>

                </div>
                <div class="area_img" id="165">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图16 验证损失与迭代次数的关系" src="Detail/GetImg?filename=images/JSJY201910020_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图16 验证损失与迭代次数的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.16 <i>Relationship between validation loss and iteration times</i></p>

                </div>
                <h4 class="anchor-tag" id="166" name="166">3.1.4 数据可视化</h4>
                <div class="p1">
                    <p id="167">使用训练模型预测的结果被写入灰度图中,像素值位于0～3,将其转化为<i>RGB</i>图像,转化关系如表4所示。</p>
                </div>
                <div class="area_img" id="168">
                    <p class="img_tit"><b>表</b>4 <b>数据可视化转换</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.4 <i>Data visualization transformation</i></p>
                    <p class="img_note"></p>
                    <table id="168" border="1"><tr><td><br />本文地物类型</td><td>颜色通道(<i>Gray</i>)</td><td>颜色通道(R,G,B)</td></tr><tr><td><br /><i>Clutter</i>/<i>background</i></td><td>0</td><td>85,85,85</td></tr><tr><td><br /><i>Vegetation</i></td><td>1</td><td>170,170,170</td></tr><tr><td><br /><i>Impervious surface</i></td><td>2</td><td>255,255,255</td></tr><tr><td><br /><i>Building</i></td><td>3</td><td>0,0,0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="169" name="169">3.1.5 基于像素的面积测算</h4>
                <div class="p1">
                    <p id="170">数据集<i>Potsdam</i>的地面采样距离为5 <i>cm</i>,为依据像素点数量进行面积测算提供了先决条件,每个像素点所表示的面积为5×5 <i>cm</i><sup>2</sup>,则只要统计像素数量即可进行遥感图像主要地物信息的面积测算,如式(13)所示:</p>
                </div>
                <div class="p1">
                    <p id="171"><i>Area</i>=<i>g</i>×<i>n</i>      (13)</p>
                </div>
                <div class="p1">
                    <p id="172">其中:<i>g</i>为地面采样距离;<i>n</i>为像素点数量。</p>
                </div>
                <h4 class="anchor-tag" id="173" name="173">3.2 <b>分析</b></h4>
                <h4 class="anchor-tag" id="174" name="174">3.2.1 性能评价</h4>
                <div class="p1">
                    <p id="175">对分类后的遥感图像,使用混淆矩阵<citation id="347" type="reference"><link href="303" rel="bibliography" /><link href="305" rel="bibliography" /><sup>[<a class="sup">29</a>,<a class="sup">30</a>]</sup></citation>、均交并比(<i>Mean Intersection over Union</i>, <i>MIoU</i>)与平均绝对误差(<i>Mean Absolute Error</i>, <i>MAE</i>)进行评估。遥感图像信息提取被视为一种多分类问题,可用混淆矩阵将预测输出的分类结果和<i>gt</i>进行像素级比较,评价每个像素的预测输出结果,即该像素分类结果取真阳性(<i>True Positive</i>,<i>TP</i>)、假阳性(<i>False Positive</i>,<i>FP</i>)、真阴性(<i>True Negative</i>,<i>TN</i>)、假阴性(<i>False Negative</i>,<i>FN</i>)四种结果中的一种,然后根据这四个指标计算以下指标:</p>
                </div>
                <div class="p1">
                    <p id="176">准确率(<i>Accuracy</i>)表示预测正确的像素占总像素的比例,定义为:</p>
                </div>
                <div class="p1">
                    <p id="177" class="code-formula">
                        <mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="178">精确率(Precision)表示预测为正例的样本中真正正例的比例,定义为:</p>
                </div>
                <div class="p1">
                    <p id="179" class="code-formula">
                        <mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="180">召回率(Recall)表示样本中有多少正例被正确预测,定义为:</p>
                </div>
                <div class="p1">
                    <p id="181" class="code-formula">
                        <mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="182">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910020_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图17 不同模型分割预测结果" src="Detail/GetImg?filename=images/JSJY201910020_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图17 不同模型分割预测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910020_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.17 Different model segmentation prediction results</p>

                </div>
                <div class="p1">
                    <p id="183">通过精确率和召回率可计算F1-<i>Score</i>,便于进行标准化的衡量,它表示精确率与召回率的加权调和平均,定义为:</p>
                </div>
                <div class="p1">
                    <p id="184" class="code-formula">
                        <mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>F</mtext><mn>1</mn><mo>-</mo><mtext>S</mtext><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mtext>e</mtext><mo>=</mo><mfrac><mrow><mn>2</mn><mtext>Ρ</mtext><mtext>R</mtext></mrow><mrow><mtext>Ρ</mtext><mo>+</mo><mtext>R</mtext></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="185">F1-<i>Score</i>的取值范围为0～1,1代表模型的输出最好,0代表模型的输出结果最差。</p>
                </div>
                <div class="p1">
                    <p id="186">MIoU为语义分割常用的评价指标,其计算两个集合的交集和并集之比,在语义分割的问题中,这两个集合为gt和预测值。交并比反映了预测图与gt之间的重合度,比值越接近1则重合度越高,语义分割质量越高,其定义为:</p>
                </div>
                <div class="p1">
                    <p id="187" class="code-formula">
                        <mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi mathvariant="bold-italic">Ι</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Κ</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>Κ</mi></munderover><mrow><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>-</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="188">其中:<i>i</i>表示真实值; <i>j</i>表示预测值;<i>k</i>为类别数量。</p>
                </div>
                <div class="p1">
                    <p id="189"><i>MAE</i>表示预测值与gt值之间的绝对差,其值越小,像素分类越精确。若预测的像素类别值的集合为{<i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>,…, <i>p</i><sub><i>n</i></sub>},gt像素类别值集合为{<i>t</i><sub>1</sub>,<i>t</i><sub>2</sub>,…,<i>t</i><sub><i>n</i></sub>},则<i>MAE</i>定义如式(19):</p>
                </div>
                <div class="p1">
                    <p id="190" class="code-formula">
                        <mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>A</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="191" name="191">3.2.2 结果分析</h4>
                <div class="p1">
                    <p id="192">图17前两列为<i>TOP</i>、<i>gt</i>以及使用不同方法分割并可视化的结果,其中<i>D</i>-<i>Unet</i>(<i>ELU</i>)_<i>CRF</i>相对于<i>D</i>-<i>Unet</i>(<i>ELU</i>)是在粗分割的基础上使用<i>CRF</i>进行了细分割。</p>
                </div>
                <div class="p1">
                    <p id="193">对比<i>D</i>-<i>Unet</i>(<i>ReLU</i>)与<i>U</i>-<i>net</i>(<i>ReLU</i>)的分割结果可看出:<i>U</i>-<i>net</i>网络构架改进后能够更精确地对地物像素点进行分类;在使用<i>ELU</i>函数替换<i>ReLU</i>函数后,<i>D</i>-<i>Unet</i>(<i>ELU</i>)的分割结果明显优于<i>D</i>-<i>Unet</i>(<i>ReLU</i>);在使用深度可分离卷积代替标准卷积构建为<i>DS</i>-<i>Unet</i>(<i>ELU</i>)后,模型分割准确率会略微下降,但是其由于网络参数少,预测时间减少很多,可用于对预测精度要求不高但有时效性要求的场景下。对比<i>D</i>-<i>Unet</i>(<i>ELU</i>)_<i>CRF</i>与<i>D</i>-<i>Unet</i>(<i>ELU</i>)的分割结果可看出:在使用全连接条件随机场对粗分割结果进行优化后,在保证地物区域完整性的前提下精细化边界区域,增强了地物信息的完整性,得到了更为优异的分割结果。</p>
                </div>
                <div class="p1">
                    <p id="194">图17后六列是对前两列的局部细节展示,分别是各个模型对植被、不透水表面(白色区域为不透水表面)与建筑物的分割结果。从中可看出:<i>D</i>-<i>Unet</i>(<i>ELU</i>)_<i>CRF</i>所得结果在精确度、边缘明晰度上要明显优于其他方法。在使用全连接条件随机场细分割后,对于<i>D</i>-<i>Unet</i>(<i>ELU</i>)存在的明显错分类进行了纠正,边缘也更加接近<i>gt</i>;而<i>D</i>-<i>Unet</i>(<i>ReLU</i>)与<i>DS</i>-<i>Unet</i>(<i>ELU</i>)分割出的结果孤立点较多, 且建筑物分割不太完整, 边缘的错误比较显著,<i>U</i>-<i>net</i>(<i>ReLU</i>)模型甚至存在严重的错分类与欠分割问题,在建筑物的像素分类上尤为明显。</p>
                </div>
                <div class="p1">
                    <p id="195">从表5可看出:<i>D</i>-<i>Unet</i>(<i>ReLU</i>)的准确率、精确率、召回率、F1-<i>score</i>、<i>MIoU</i>分别较<i>U</i>-<i>net</i>提升了12.47个百分点、22.25个百分点、25.98个百分点、0.260 4、0.323 5,平均绝对误差降低了0.017 44,验证了模型结构改进的有效性;<i>D</i>-<i>Unet</i>(<i>ELU</i>)的准确率、精确率、召回率、F1-<i>score</i>、<i>MIoU</i>分别较<i>D</i>-<i>Unet</i>(<i>ReLU</i>)提升了2.59个百分点、2.12个百分点、4.13个百分点、0.025 7、0.042 7,平均绝对误差降低了0.011 48,表明改用<i>ELU</i>函数后模型分割的效果更佳。</p>
                </div>
                <div class="p1">
                    <p id="196">在使用全连接条件随机场对<i>D</i>-<i>Unet</i>(<i>ELU</i>)的粗分割结果优化以后,遥感图像分割的准确率准确率、精确率、召回率、F1-<i>score</i>、<i>MIoU</i>较<i>D</i>-<i>Unet</i>(<i>ELU</i>)有略微的提升,平均绝对误差略微下降,说明了所构建的<i>D</i>-<i>Unet</i>(<i>ELU</i>)_<i>CRF</i>模型的有效性。</p>
                </div>
                <div class="p1">
                    <p id="197">在使用深度可分离卷积替换标准卷积后,模型的表现性能有所下降,<i>DS</i>-<i>Unet</i>(<i>ELU</i>)的准确率、精确率、召回率、F1-<i>score</i>、<i>MIoU</i>分别较<i>D</i>-<i>Unet</i>(<i>ELU</i>)下降了2.85个百分点、1.98个百分点、3.7个百分点、0.028 6、0.047 4,<i>DS</i>-<i>Unet</i>(<i>ELU</i>)的平均绝对误差较<i>D</i>-<i>Unet</i>(<i>ELU</i>)升高了0.015 42,但是与<i>D</i>-<i>Unet</i>(<i>ReLU</i>)相比性能差异不大。其优势体现在模型体积更小与预测效率的提升,因为其参数量更小,为标准卷积构成网络的1/6～1/5,具体见表6～7。</p>
                </div>
                <div class="p1">
                    <p id="198">表7比较了<i>D</i>-<i>Unet</i>(<i>ELU</i>)、<i>DS</i>-<i>Unet</i>(<i>ELU</i>)与最先进的深度模型<i>SegNet</i>、<i>FCN</i>-8<i>s</i>、<i>DeconvNet</i>、<i>Deeplab</i>-<i>ResNet</i>以及<i>RefineNet</i>的复杂性。通过使用<i>Intel Core i</i>7对图像(平均尺寸2 392×2 191像素)执行测试的时间获得时间复杂度,同时表7还展示了深度模型的空间复杂度。可看出本文提出的<i>D</i>-<i>Unet</i>(<i>ELU</i>)模型在时间与空间复杂度上具有一定的竞争力,在使用深度可分离卷积卷积构建为<i>DS</i>-<i>Unet</i>(<i>ELU</i>)后在预测时间的消耗上大为减少,模型的大小也大为缩减。</p>
                </div>
                <div class="area_img" id="199">
                    <p class="img_tit"><b>表</b>5 <b>不同方法分割地物信息的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.5 <i>Performance comparison of different methods for segmentation of ground object information</i></p>
                    <p class="img_note"></p>
                    <table id="199" border="1"><tr><td>分割方法</td><td>准确率/%</td><td>损失率/%</td><td>平均绝对误差</td><td>精确率/%</td><td>召回率/%</td><td>F1值</td><td><i>MIoU</i></td></tr><tr><td><i>D</i>-<i>Unet</i>(<i>ReLU</i>)</td><td>89.00</td><td>30.56</td><td>0.049 58</td><td>90.46</td><td>87.70</td><td>0.890 6</td><td>0.802 8</td></tr><tr><td><br /><i>D</i>-<i>Unet</i>(<i>ELU</i>)</td><td>91.59</td><td>22.29</td><td>0.038 10</td><td>92.45</td><td>90.83</td><td>0.916 3</td><td>0.845 5</td></tr><tr><td><br /><i>DS</i>-<i>Unet</i>(<i>ELU</i>)</td><td>88.74</td><td>29.90</td><td>0.053 52</td><td>90.47</td><td>87.13</td><td>0.887 7</td><td>0.798 1</td></tr><tr><td><br /><i>U</i>-<i>net</i>(<i>ReLU</i>)</td><td>76.53</td><td>40.28</td><td>0.067 02</td><td>68.21</td><td>61.72</td><td>0.630 2</td><td>0.479 3</td></tr><tr><td><br /><i>D</i>-<i>Unet</i>(<i>ELU</i>)_<i>CRF</i></td><td>—</td><td>—</td><td>0.034 57</td><td>93.02</td><td>91.23</td><td>0.921 2</td><td>0.853 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="200">
                    <p class="img_tit"><b>表</b>6 <b>不同方法的模型体积以及参数量</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.6 <i>Model volume and parameter quantities of different methods</i></p>
                    <p class="img_note"></p>
                    <table id="200" border="1"><tr><td><br />分割方法</td><td>模型体积/10<sup>6</sup></td><td>总参数量</td><td>可训练参数量</td><td>不可训练参数量</td></tr><tr><td><i>D</i>-<i>Unet</i>(<i>ReLU</i>)</td><td>279.5</td><td>31 821 061</td><td>31 804 165</td><td>16 896</td></tr><tr><td><br /><i>D</i>-<i>Unet</i>(<i>ELU</i>)</td><td>279.5</td><td>31 821 061</td><td>31 804 165</td><td>16 896</td></tr><tr><td><br /><i>DS</i>-<i>Unet</i>(<i>ELU</i>)</td><td>32.2</td><td>5 613 729</td><td>5 590 817</td><td>22 912</td></tr><tr><td><br /><i>U</i>-<i>net</i>(<i>ReLU</i>)</td><td>94.3</td><td>—</td><td>—</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="201">
                    <p class="img_tit"><b>表</b>7 <b>不同方法的复杂性对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.7 <i>Comparison of the complexity of different methods</i></p>
                    <p class="img_note"></p>
                    <table id="201" border="1"><tr><td><br />分割方法</td><td>模型体积/10<sup>6</sup></td><td>预测时间/<i>s</i></td></tr><tr><td><br /><i>SegNet</i></td><td>112.0</td><td>17</td></tr><tr><td><br /><i>FCN</i>-8<i>s</i></td><td>512.0</td><td>11</td></tr><tr><td><br /><i>DeconvNet</i></td><td>961.0</td><td>18</td></tr><tr><td><br /><i>Deeplab</i>-<i>ResNet</i></td><td>503.0</td><td>47</td></tr><tr><td><br /><i>RefineNet</i></td><td>234.0</td><td>21</td></tr><tr><td><br /><i>D</i>-<i>Unet</i>(<i>ELU</i>)</td><td>279.5</td><td>45</td></tr><tr><td><br /><i>DS</i>-<i>Unet</i>(<i>ELU</i>)</td><td>32.2</td><td>15</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="202">在<i>Potsdam</i>的基准测试中,不同方法的定量比较如表8所示。其中:<i>SVL</i>_3算法使用<i>SVL</i>(<i>Stair Vision Library</i>)、归一化数字植被指数(<i>Normalized Digital Vegetation Index</i>,<i>NDVI</i>)、饱和度和归一化数字地表模型(<i>Normalized Digital Surface Model</i>,<i>NDSM</i>)训练基于<i>AdaBoost</i>的分类器以获得最优结果。<i>Volpi</i>等<citation id="348" type="reference"><link href="307" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>的算法<i>UZ</i>_1与<i>AZ</i>1算法分别构建了不同的<i>encoder</i>-<i>decoder</i>结构的卷积神经网络。<i>RIT</i>_<i>L</i>2算法使用<i>RGB</i>与合成数据(红外线(<i>Infrared Radiation</i>,<i>IR</i>)、<i>NDVI</i>和<i>NDSM</i>)训练两个<i>SegNet</i>并进行特征融合。<i>Sherrah</i><citation id="349" type="reference"><link href="309" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>的算法<i>DST</i>_2使用<i>FCN</i>作为卷积模型并应用<i>CRF</i>作后处理。</p>
                </div>
                <div class="p1">
                    <p id="203">从表8可看出,所有的方法都获得了良好的结果,本文所提出的分割模型<i>D</i>-<i>Unet</i>(<i>ELU</i>)的准确率最高,轻量级模型<i>DS</i>-<i>Unet</i>(<i>ELU</i>)也获得了较高的准确率。</p>
                </div>
                <div class="area_img" id="204">
                    <p class="img_tit"><b>表</b>8 <b>不同方法使用</b><i>Potsdam</i><b>数据集 测试的准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.8 <i>Accuracy comparison of different methods using</i><i>Potsdam dataset test</i></p>
                    <p class="img_note"></p>
                    <table id="204" border="1"><tr><td><br />算法名</td><td>分割方法</td><td>准确率/%</td></tr><tr><td><br /><i>SVL</i>_3</td><td><i>SVL</i>-<i>features</i>+<i>NDSM</i>+<i>Boosting</i></td><td>77.20</td></tr><tr><td><br /><i>UZ</i>_1</td><td><i>CNN</i>+<i>NDSM</i>+<i>Deconvolution</i></td><td>85.80</td></tr><tr><td><br /><i>AZ</i>1</td><td><i>CNN</i>+<i>DSM</i></td><td>89.20</td></tr><tr><td><br /><i>RIT</i>_<i>L</i>2</td><td><i>SegNet</i>+<i>NDSM</i></td><td>89.40</td></tr><tr><td><br /><i>DST</i>_2</td><td><i>FCN</i>+<i>DSM</i>+<i>RF</i>+<i>CRF</i></td><td>89.70</td></tr><tr><td><br /><i>D</i>-<i>Unet</i>(<i>ELU</i>)</td><td><i>D</i>-<i>Unet</i>+<i>ELU</i></td><td>91.59</td></tr><tr><td><br /><i>DS</i>-<i>Unet</i>(<i>ELU</i>)</td><td><i>DS</i>-<i>Unet</i>+<i>ELU</i></td><td>88.74</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="205">本文所提出的网络优势在于只使用原始的<i>RGB</i>数据训练单个网络,在数据量以及模型对地物的分割精确率上都具有优势。</p>
                </div>
                <h3 id="206" name="206" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="207">针对目前人为勘测地物信息的任务耗费人力物力、办事效率低下等问题,本文提出了一种全卷积神经网络和全连接条件随机场的遥感图像地物信息分割方法,构建<i>D</i>-<i>Unet</i>(<i>ELU</i>)和<i>DS</i>-<i>Unet</i>(<i>ELU</i>)模型,对遥感图像实现像素级别的分类,解决了传统方法普遍存在的过程繁琐、方法普适性低、泛化能力弱的问题。其中<i>D</i>-<i>Unet</i>(<i>ELU</i>)的分割准确率高,但时效性较低,适用于对分割精度高但时效性要求不高的应用场景;<i>DS</i>-<i>Unet</i>(<i>ELU</i>)的分割精确度较<i>D</i>-<i>Unet</i>(<i>ELU</i>)略微降低,但时效性高,适用于对精度要求不严苛却对时效性要求高且设备性能低的应用场景。本文首先通过对遥感图像进行标注、数据增强,将处理好的数据放入<i>D</i>-<i>Unet</i>与<i>DS</i>-<i>Unet</i>模型中拟合;然后将输出结果放入全连接条件随机场中进一步处理,使得分割结果更接近<i>gt</i>;最后利用遥感图像具有地面采样距离这一特点提出了基于像素的面积测算方法。与测试集进行对比后发现所提出的方法能够精确地分割目标地物,具有高效性、可实施性。但因参数<i>batchsize</i>受限于硬件设备,不能设置为较为理想的数值,在一定程度上影响了边缘分割的精细度, 训练模型的时效性还有待提升。如何在保证精确性的情况下进一步降低模型参数减少模型训练时间与预测时间是接下来的工作重心。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="247">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXX201006016&amp;v=MDUyMjhIOUhNcVk5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtVci9NUENyVGRyRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>高海燕,吴波.结合像元形状特征分割的高分辨率影像面向对象分类[J].遥感信息,2010(6):67-72.(GAO H Y,WU B.Object-oriented classification of high spatial resolution remote sensing imagery based on image segmentation with pixel shape feature[J].Remote Sensing Information,2010(6):67-72.)
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201301009&amp;v=MzA5ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWprVXIvTUppWFRiTEc0SDlMTXJvOUZiWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>巫兆聪,胡忠文,张谦,等.结合光谱、纹理与形状结构信息的遥感影像分割方法[J].测绘学报,2013,42(1):44-50.(WU Z C,HUZ W,ZHANG Q,et al.On combining spectral,textural and shape features for remote sensing image segmentation[J].Acta Geodaetica et Cartographica Sinica,2013,42(1):44-50.)
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Remote Sensing Image Segmentation by Combining Spectral and TextureFeatures">

                                <b>[3]</b>YUAN J,WANG D,LI R.Remote sensing image segmentation by combining spectral and texture features[J].IEEE Transactions on Geoscience and Remote Sensing,2014,52(1):16-24.
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale segmentation of forest areas and tree detection in Li DAR images by the attentive vision method">

                                <b>[4]</b>PALENICHKA R,DOYON F,LAKHSSASSI A,et al.Multi-scale segmentation of forest areas and tree detection in Li DAR images by the attentive vision method[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2013,6(3):1313-1323.
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFJC201301017&amp;v=MTI1MjBGckNVUjdxZlp1WnNGeWprVXIvTUx5dkJiYkc0SDlMTXJvOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>都伟冰,王双亭,王春来.基于机载Li DAR粗糙度指数和回波强度的道路提取[J].测绘科学技术学报,2013,30(1):63-67.(DU W B,WANG S T,WANG C L.Road extraction based on roughness index and echo intensity of airborne Li DAR[J].Journal of Geomatics Science and Technology,2013,30(1):63-67.)
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DXKJ201606006&amp;v=MDA4MDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1VyL01JVFhBWkxHNEg5Zk1xWTlGWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>张曦,胡根生,梁栋,等.基于时频特征的高分辨率遥感图像道路提取[J].地理空间信息,2016,14(6):18-21,24.(ZHANG X,HU G S,LIANG D,et al.Road extraction from high resolution remote sensing image based on time frequency feature[J].Geospatial Information,2016,14(6):18-21,24.)
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHTB201312016&amp;v=MjQwMzlHRnJDVVI3cWZadVpzRnlqa1VyL01KaVhmYkxHNEg5TE5yWTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>周绍光,陈超,赫春晓.基于形状先验和Graph Cuts原理的道路分割新方法[J].测绘通报,2013(12):55-57.(ZHOU SG,CHEN C,HE C X.A new road segmentation based on shape prior and graph cuts[J].Bulletin of Surveying and Mapping,2013(12):55-57.)
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNGD201306028&amp;v=MjM0NTN5amtVci9NUHlQTWFyRzRIOUxNcVk5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>周家香,周安发,陶超,等.一种高分辨率遥感影像城区道路网提取方法[J].中南大学学报(自然科学版),2013,44(6):2385-2391.(ZHOU J X,ZHOU A F,TAO C,et al.A methodology for urban roads network extraction from high resolution remote sensing imagery[J].Journal of Central South University(Science&amp;Technology),2013,44(6):2385-2391.)
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GTYG201304014&amp;v=MjkwMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1VyL01Jam5TYWJHNEg5TE1xNDlFWUlRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>曾发明,杨波,吴德文,等.基于Canny边缘检测算子的矿区道路提取[J].国土资源遥感,2013,25(4):72-78.(ZENGF M,YANG B,WU D W,et al.Extraction of roads in mining area based on Canny edge detection operator[J].Remote Sensing for Land&amp;Resources,2013,25(4):72-78.)
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Building change detection from uniform regions">

                                <b>[10]</b>BEUMIER C,IDRISSA M.Building change detection from uniform regions[C]//Proceeddings of the 2012 Iberoamerican Congress on Pattern Recognition,LNCS 7441.Berlin:Springer,2012:648-655.
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Building Change Detection From Multitemporal High-Resolution Remotely Sensed Images Based on a Morphological Building Index">

                                <b>[11]</b>HUANG X,ZHANG L,ZHU T.Building change detection from multitemporal high-resolution remotely sensed images based on a morphological building index[J].IEEE Journal of Selected Topics in Applied Earth Observations&amp;Remote Sensing,2013,7(1):105-115.
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200905001&amp;v=MDY2MDZHNEh0ak1xbzlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1VyL01LQ0xmWWI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>李炜明,吴毅红,胡占义.视角和光照显著变化时的变化检测方法研究[J].自动化学报,2009,35(5):449-461.(LI WM,WU Y H,HU Z Y.Urban change detection under large view and illumination variations[J].Acta Automatica Sinica,2009,35(5):449-461.)
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201011003&amp;v=MjUwNDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWprVXIvTUtDTGZZYkc0SDlITnJvOUZaNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>田昊,杨剑,汪彦明,等.基于先验形状约束水平集模型的建筑物提取方法[J].自动化学报,2010,36(11):1502-1511.(TIAN H,YANG J,WANG Y M,et al.Towards automatic building extraction:variational level set model using prior shape knowledge[J].Acta Automatica Sinica,2010,36(11):1502-1511.)
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                LECUN Y,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[15]</b>KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York:Curran Associates Inc.,2012:1097-1105.
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[16]</b>HE K,GKIOXARI G,DOLLR P,et al.Mask R-CNN[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2961-2969.
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[17]</b>LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(4):640-651.
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Multi-view face detection using deep convolutional neural networks&amp;quot;">

                                <b>[18]</b>FARFADE S S,SABERIAN M J,LI L.Multi-view face detection using deep convolutional neural networks[C]//Proceedings of the5th ACM International Conference on Multimedia Retrieval.New York:ACM,2015:643-650.
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                RONNEBERGER O,FISCHER P,BROX T.U-net:convolutional networks for biomedical image segmentation[C]//MICCAI2015:Proceedings of the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer,2015:234-241.
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep sparse rectifier neural networks">

                                <b>[20]</b>GLOROT X,BORDES A,BENGIO Y.Deep sparse rectifier neural networks[J].Journal of Machine Learning Research,2011,15:315-323.
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and accurate deep network learning by Exponential Linear Units(ELUs)">

                                <b>[21]</b>CLEVERT D,UNTERTHINER T,HOCHREITER S.Fast and accurate deep network learning by Exponential Linear Units(ELUs)[EB/OL].[2019-01-10].http://de.arxiv.org/pdf/1511.07289.
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">

                                <b>[22]</b>CHOLLET F.Xception:deep learning with depthwise separable convolutions[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition,2017:1800-1807.
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobile Nets:efficient convolutional neural networks for mobile vision applications">

                                <b>[23]</b>HOWARD A G,ZHU M,CHEN B,et al.Mobile Nets:efficient convolutional neural networks for mobile vision applications[EB/OL].[2019-01-10].https://arxiv.org/pdf/1704.04861.pdf.
                            </a>
                        </p>
                        <p id="293">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">

                                <b>[24]</b>KR<image id="350" type="formula" href="images/JSJY201910020_35000.jpg" display="inline" placement="inline"><alt></alt></image>HENB<image id="351" type="formula" href="images/JSJY201910020_35100.jpg" display="inline" placement="inline"><alt></alt></image>HL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of the 2011 International Conference on Neural Information Processing Systems.New York:Curran Associates Inc.,2011:109-117.
                            </a>
                        </p>
                        <p id="295">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501968955&amp;v=MDI2ODVIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJRjhWYWhZPU5pZk9mYks3SHRETnFvOUViZTBIQlhrOG9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b>ALTMAIER A,KANY C.Digital surface model generation from CORONA satellite images[J].ISPRS Journal of Photogrammetry and Remote Sensing,2002,56(4):221-235.
                            </a>
                        </p>
                        <p id="297">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1019132072.nh&amp;v=MzIyNjFyQ1VSN3FmWnVac0Z5amtVci9NVkYyNkY3SzdITkhMclpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b>许玥.基于改进Unet的遥感影像语义分割在地表水体变迁中的应用[D].重庆:重庆师范大学,2019:16-35.(XU Y.Application of semantic segmentation of remote sensing image based on improved unet in surface water change[D].Chongqing:Chongqing Normal University,2019:16-35.)
                            </a>
                        </p>
                        <p id="299">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional Random Fields as Recurrent Neural Networks">

                                <b>[27]</b>ZHENG S,JAYASUMANA S,ROMERA-PAREDES B,et al.Conditional random fields as recurrent neural networks[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway:IEEE,2015:1529-1537.
                            </a>
                        </p>
                        <p id="301">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">

                                <b>[28]</b>GLOROT X,BENGIO Y.Understanding the difficulty of training deep feedforward neural networks[J].Journal of Machine Learning Research,2010,9:249-256.
                            </a>
                        </p>
                        <p id="303">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with generative adversarial networks">

                                <b>[29]</b>ODENA A.Semi-supervised learning with generative adversarial networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1606.01583.pdf.
                            </a>
                        </p>
                        <p id="305">
                            <a id="bibliography_30" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES4CED1B8095ECFB349DC260F200A21550&amp;v=MTkzOThVMDV0cGh4TG0rd0t3PU5pZk9mYmZMYTZYTjNZZEZiZTU2ZndwTHpCSWFua3gvVG4rVXJoSTFDTENWUUwrZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[30]</b>HUANG B,ZHAO B,SONG Y.Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery[J].Remote Sensing of Environment,2018,214:73-86.
                            </a>
                        </p>
                        <p id="307">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dense semantic labeling of subdecimeter resolution images with convolutional neural networks">

                                <b>[31]</b>VOLPI M,TUIA D.Dense semantic labeling of subdecimeter resolution images with convolutional neural networks[J].IEEETransactions on Geoscience and Remote Sensing,2016,55(2):881-893.
                            </a>
                        </p>
                        <p id="309">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery">

                                <b>[32]</b>SHERRAH J.Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery[EB/OL].[2019-01-10].https://arxiv.org/pdf/1606.02585.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910020" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910020&amp;v=MTYxNTQ5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtVci9NTHo3QmQ3RzRIOWpOcjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
