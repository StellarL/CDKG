<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136455302002500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910005%26RESULT%3d1%26SIGN%3dHCon8mGCOTVmWZTno7%252bNny83YZw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910005&amp;v=MjEwMzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVUw3TUx6N0JkN0c0SDlqTnI0OUZZWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#57" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="2 相关知识 ">2 相关知识</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="2.1 &lt;b&gt;互信息与最大相关最小冗余性&lt;/b&gt;(mRMR)">2.1 <b>互信息与最大相关最小冗余性</b>(mRMR)</a></li>
                                                <li><a href="#77" data-title="2.2 &lt;b&gt;评价指标&lt;/b&gt;">2.2 <b>评价指标</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="3 EF-MLFS ">3 EF-MLFS</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="4 实验与分析 ">4 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="4.1 &lt;b&gt;实验数据及设置&lt;/b&gt;">4.1 <b>实验数据及设置</b></a></li>
                                                <li><a href="#107" data-title="4.2 &lt;b&gt;实验结果与分析&lt;/b&gt;">4.2 <b>实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;多标记数据集详细信息&lt;/b&gt;"><b>表</b>1 <b>多标记数据集详细信息</b></a></li>
                                                <li><a href="#109" data-title="图1 不同方法在平均准确率指标上的实验结果对比">图1 不同方法在平均准确率指标上的实验结果对比</a></li>
                                                <li><a href="#110" data-title="图2 不同方法在覆盖率指标上的实验结果对比">图2 不同方法在覆盖率指标上的实验结果对比</a></li>
                                                <li><a href="#119" data-title="图3 不同方法在海明损失指标上的实验结果对比">图3 不同方法在海明损失指标上的实验结果对比</a></li>
                                                <li><a href="#120" data-title="图4 不同方法在&lt;i&gt;One&lt;/i&gt;-&lt;i&gt;Error&lt;/i&gt;指标上的实验结果对比">图4 不同方法在<i>One</i>-<i>Error</i>指标上的实验结果对比</a></li>
                                                <li><a href="#121" data-title="图5 不同方法在排序损失指标上的实验结果对比">图5 不同方法在排序损失指标上的实验结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="164">


                                    <a id="bibliography_1" title="朱越,姜远,周志华.一种基于多示例多标记学习的新标记学习方法[J].中国科学:信息科学,2018,48(12):1670-1680.(ZHU Y,JIANG Y,ZHOU Z H.Multi-instance multi-label new label learning[J].Scientia Sinica(Information),2018,48(12):1670-1680.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201812007&amp;v=MDA4NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFVMN1BOVGZBZHJHNEg5bk5yWTlGWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        朱越,姜远,周志华.一种基于多示例多标记学习的新标记学习方法[J].中国科学:信息科学,2018,48(12):1670-1680.(ZHU Y,JIANG Y,ZHOU Z H.Multi-instance multi-label new label learning[J].Scientia Sinica(Information),2018,48(12):1670-1680.)
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_2" title="SCHAPIRE R E,SINGER Y.Boos Texter:a boosting-based system for text categorization[J].Machine Learning,2000,39(2/3):135-168." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340071&amp;v=MTcwNDh6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFVydkxJVms9Tmo3QmFyTzRIdEhOckl0RlpPd09ZM2s1&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        SCHAPIRE R E,SINGER Y.Boos Texter:a boosting-based system for text categorization[J].Machine Learning,2000,39(2/3):135-168.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_3" title="DIPLARIS S,TSOUMAKAS G,MITKAS P A,et al.Protein classification with multiple algorithms[C]//Proceedings of the 2005Panhellenic Conference on Informatics,LNCS 3746.Berlin:Springer,2005:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Protein classification with multiple algorithms">
                                        <b>[3]</b>
                                        DIPLARIS S,TSOUMAKAS G,MITKAS P A,et al.Protein classification with multiple algorithms[C]//Proceedings of the 2005Panhellenic Conference on Informatics,LNCS 3746.Berlin:Springer,2005:448-456.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_4" title="彭利红,刘海燕,任日丽,等.基于多标记学习预测药物-靶标相互作用[J].计算机工程与应用,2017,53(15):260-265.(PENG L H,LIU H Y,REN R L,et al.Predicting drug-target interactions with multi-label learning[J].Computer Engineering and Applications,2017,53(15):260-265.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201715045&amp;v=MTkwMTg3UEx6N01hYkc0SDliTnFvOUJZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVUw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        彭利红,刘海燕,任日丽,等.基于多标记学习预测药物-靶标相互作用[J].计算机工程与应用,2017,53(15):260-265.(PENG L H,LIU H Y,REN R L,et al.Predicting drug-target interactions with multi-label learning[J].Computer Engineering and Applications,2017,53(15):260-265.)
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_5" title="LIU G P,LI G Z,WANG Y L,et al.Modelling of inquiry diagnosis for coronary heart disease in traditional Chinese medicine by using multi-label learning[J].BMC Complementary and Alternative Medicine,2010,10(1):No.37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modelling of inquiry diagnosis for coronary heart disease in traditional Chinese medicine by using multi-label learning">
                                        <b>[5]</b>
                                        LIU G P,LI G Z,WANG Y L,et al.Modelling of inquiry diagnosis for coronary heart disease in traditional Chinese medicine by using multi-label learning[J].BMC Complementary and Alternative Medicine,2010,10(1):No.37.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_6" title="DAI L,ZHANG J,LI C,et al.Multi-label feature selection with application to TCM state identification[J/OL].Concurrency and Computation:Practice and Experience,2018:No.e4634.[2019-01-10].https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4634." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-label feature selection with application to TCM state identification">
                                        <b>[6]</b>
                                        DAI L,ZHANG J,LI C,et al.Multi-label feature selection with application to TCM state identification[J/OL].Concurrency and Computation:Practice and Experience,2018:No.e4634.[2019-01-10].https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4634.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_7" title="GUYON I,ELISSEEFF A.An introduction to variable and feature selection[J].Journal of Machine Learning Research,2003,3:1157-1182." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An introduction to variable and feature selection">
                                        <b>[7]</b>
                                        GUYON I,ELISSEEFF A.An introduction to variable and feature selection[J].Journal of Machine Learning Research,2003,3:1157-1182.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_8" title="宋国杰,唐世渭,杨冬青,等.基于最大熵原理的空间特征选择方法[J].软件学报,2003,14(9):1544-1550.(SONG GJ,TANG S W,YANG D Q,et al.A spatial feature selection method based on maximum entropy theory[J].Journal of Software,2003,14(9):1544-1550.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200309006&amp;v=MjY5MTMzenFxQnRHRnJDVVI3cWZadVpzRnluaFVMN1BOeWZUYkxHNEh0TE1wbzlGWW9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        宋国杰,唐世渭,杨冬青,等.基于最大熵原理的空间特征选择方法[J].软件学报,2003,14(9):1544-1550.(SONG GJ,TANG S W,YANG D Q,et al.A spatial feature selection method based on maximum entropy theory[J].Journal of Software,2003,14(9):1544-1550.)
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_9" title="GUYON I,WESTON J,BARNHILL S,et al.Gene selection for cancer classification using support vector machines[J].Machine Learning,2002,46(1/2/3):389-422." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340375&amp;v=MDYyMTU0SHRITnJJdEZaK3dLWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJsVXJ2TElWaz1OajdCYXJP&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        GUYON I,WESTON J,BARNHILL S,et al.Gene selection for cancer classification using support vector machines[J].Machine Learning,2002,46(1/2/3):389-422.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_10" title="DY J G.BRODLEY C E.Feature selection for unsupervised learning[J].Journal of Machine Learning Research,2004,5:845-889." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature selection for unsupervised learning">
                                        <b>[10]</b>
                                        DY J G.BRODLEY C E.Feature selection for unsupervised learning[J].Journal of Machine Learning Research,2004,5:845-889.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_11" title="BATTITI R.Using mutual information for selecting features in supervised neural net learning[J].IEEE Transactions on Neural Networks,1994,5(4):537-550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using mutual information for selecting features in supervised neural net learning">
                                        <b>[11]</b>
                                        BATTITI R.Using mutual information for selecting features in supervised neural net learning[J].IEEE Transactions on Neural Networks,1994,5(4):537-550.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_12" title="PENG H,LONG F,DING C.Feature selection based on mutual information criteria of max-dependency,max-relevance,and minredundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2005,27(8):1226-1238." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy">
                                        <b>[12]</b>
                                        PENG H,LONG F,DING C.Feature selection based on mutual information criteria of max-dependency,max-relevance,and minredundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2005,27(8):1226-1238.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_13" title="LIN Y,LIU J,LIU J,et al.Multi-label feature selection based on max-dependency and min-redundancy[J].Neurocomputing,2015,168:92-103." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES8FF17D624B5694A7C672D13D80FCFB0D&amp;v=MDgwNzB1SFlmT0dRbGZCckxVMDV0cGh4Ynk4d2E4PU5pZk9mYnZPYU5ETDI0bEhZSmtLQ25VOXZoRmc3RGgvUEg3aDJCbzFEOEhpTjdyckNPTnZGU2lXV3I3SklGcG1hQg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        LIN Y,LIU J,LIU J,et al.Multi-label feature selection based on max-dependency and min-redundancy[J].Neurocomputing,2015,168:92-103.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_14" title="LEE J,KIM D.Feature selection for multi-label classification using multivariate mutual information[J].Pattern Recognition Letters,2013,34(3):349-357." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600191957&amp;v=MjU3NjRJVm9YYXhVPU5pZk9mYks4SHRETXFZOUZaZUlPQlhrK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        LEE J,KIM D.Feature selection for multi-label classification using multivariate mutual information[J].Pattern Recognition Letters,2013,34(3):349-357.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_15" title="朱颢东,陈宁,李红婵.优化的互信息特征选择方法[J].计算机工程与应用,2010,46(26):122-124.(ZHU H D,CHEN N,LI H C.Optimized mutual information feature selection method[J].Computer Engineering and Applications,2010,46(26):122-124.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201026004&amp;v=MzEyMDZHNEg5SE9xWTlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFVMN1BMejdNYWI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        朱颢东,陈宁,李红婵.优化的互信息特征选择方法[J].计算机工程与应用,2010,46(26):122-124.(ZHU H D,CHEN N,LI H C.Optimized mutual information feature selection method[J].Computer Engineering and Applications,2010,46(26):122-124.)
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_16" title="WANG J,WEI J,YANG Z,et al.Feature selection by maximizing independent classification information[J].IEEE Transactions on Knowledge and Data Engineering,2017,29(4):828-841." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature selection by maximizing independent classification information">
                                        <b>[16]</b>
                                        WANG J,WEI J,YANG Z,et al.Feature selection by maximizing independent classification information[J].IEEE Transactions on Knowledge and Data Engineering,2017,29(4):828-841.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_17" title="BROWN G,POCOCK A,ZHAO M,et al.Conditional likelihood maximization:a unifying framework for information theoretic feature selection[J].Journal of Machine Learning Research,2012,13:27-66." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional likelihood maximisation: A unifying framework for information theoretic feature selection">
                                        <b>[17]</b>
                                        BROWN G,POCOCK A,ZHAO M,et al.Conditional likelihood maximization:a unifying framework for information theoretic feature selection[J].Journal of Machine Learning Research,2012,13:27-66.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_18" title="蔡亚萍,杨明.一种利用局部标记相关性的多标记特征选择算法[J].南京大学学报(自然科学版),2016,52(4):693-704.(CAI Y P,YANG M.A multi-label feature selection algorithm by exploiting label correlations locally[J].Journal of Nanjing University(Natural Science Edition),2016,52(4):693-704.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJDZ201604014&amp;v=Mjc4MzJGeW5oVUw3UEt5ZlBkTEc0SDlmTXE0OUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        蔡亚萍,杨明.一种利用局部标记相关性的多标记特征选择算法[J].南京大学学报(自然科学版),2016,52(4):693-704.(CAI Y P,YANG M.A multi-label feature selection algorithm by exploiting label correlations locally[J].Journal of Nanjing University(Natural Science Edition),2016,52(4):693-704.)
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_19" title="杨明,蔡亚萍.一种结合标记相关性的半监督多标记特征选择及分类方法:CN201610256462.9[P].2016-09-28[2019-01-10].(YANG M,CAI Y P.A semi-supervised multi-label feature selection and classification method combined with marker correlation:CN201610256462.9[P].2016-09-28[2019-01-10].)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SCPD&amp;filename=CN105975978A&amp;v=MDY5NTVtVXJnPUppTzZIcmF4R2RURnFJYzBDKzRQRDMxTHh4WVQ2em9PUzNmbXBXRmFlN0tXUmJxZVorVnVGaXo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        杨明,蔡亚萍.一种结合标记相关性的半监督多标记特征选择及分类方法:CN201610256462.9[P].2016-09-28[2019-01-10].(YANG M,CAI Y P.A semi-supervised multi-label feature selection and classification method combined with marker correlation:CN201610256462.9[P].2016-09-28[2019-01-10].)
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_20" title="BRAYTEE A,LIU W,CATCHPOOLE D R,et al.Multi-label feature selection using correlation information[C]//Proceedings of the 2017 ACM Conference on Information and Knowledge Management.New York:ACM,2017:1649-1656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-label feature selection using correlation information">
                                        <b>[20]</b>
                                        BRAYTEE A,LIU W,CATCHPOOLE D R,et al.Multi-label feature selection using correlation information[C]//Proceedings of the 2017 ACM Conference on Information and Knowledge Management.New York:ACM,2017:1649-1656.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_21" title="LIU L,ZHANG J,LI P,et al.A label correlation based weighting feature selection approach for multi-label data[C]//Proceedings of the 2016 International Conference on Web-Age Information Management,LNCS 9659.Cham:Springer,2016:369-379." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Label Correlation Based Weighting Feature Selection Approach for Multi-label Data">
                                        <b>[21]</b>
                                        LIU L,ZHANG J,LI P,et al.A label correlation based weighting feature selection approach for multi-label data[C]//Proceedings of the 2016 International Conference on Web-Age Information Management,LNCS 9659.Cham:Springer,2016:369-379.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_22" title="SPOLA&lt;image id=&quot;252&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910005_25200.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;R N,MONARD M C,TSOUMAKAS G.A systematic review of multi-label feature selection and a new method based on label construction[J].Neurocomputing,2016,180:3-15." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESCF123CFEE91C152261E4DBDFBB1F5B79&amp;v=MjQzMzBTUk43MldDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4Ynk4d2E4PU5pZk9mY0RPSDlQUDNQa3dFZUlPZjMwOHpSUVY2MHA1UEEyVzJtQkhlTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        SPOLA&lt;image id=&quot;252&quot; type=&quot;formula&quot; href=&quot;images/JSJY201910005_25200.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;R N,MONARD M C,TSOUMAKAS G.A systematic review of multi-label feature selection and a new method based on label construction[J].Neurocomputing,2016,180:3-15.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_23" title="SHANNON C E.A mathematical theory of communication[J].Bell System Technical Journal,1948,27(4):623-656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A mathematical theory of communication">
                                        <b>[23]</b>
                                        SHANNON C E.A mathematical theory of communication[J].Bell System Technical Journal,1948,27(4):623-656.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_24" title="ZHANG M,ZHOU Z.ML-KNN:a lazy learning approach to multi-label learning[J].Pattern Recognition,2007,40(7):2038-2048." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739502&amp;v=MTQ4NDBZK2dHQ1h3N29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUlWb1hheFU9TmlmT2ZiSzdIdEROcVk5Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                        ZHANG M,ZHOU Z.ML-KNN:a lazy learning approach to multi-label learning[J].Pattern Recognition,2007,40(7):2038-2048.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_25" title="ZHANG M,PENA J M,ROBLES V.Feature selection for multilabel naive Bayes classification[J].Information Sciences,2009,179(19):3218-3229." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601417399&amp;v=MTQ1OTBkR2VycVFUTW53WmVadEZpbmxVcjNJSVZvWGF4VT1OaWZPZmJLN0h0RE5xWTlFWU9vSUQzVXdvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                        ZHANG M,PENA J M,ROBLES V.Feature selection for multilabel naive Bayes classification[J].Information Sciences,2009,179(19):3218-3229.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_26" title="ZHANG Y,ZHOU Z H.Multi-label dimensionality reduction via dependence maximization[C]//Proceedings of the 23rd National Conference on Artificial Intelligence.Menlo Park,CA:AAAIPress,2008,3:1503-1505." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-label dimensionality reduction via dependency maximization">
                                        <b>[26]</b>
                                        ZHANG Y,ZHOU Z H.Multi-label dimensionality reduction via dependence maximization[C]//Proceedings of the 23rd National Conference on Artificial Intelligence.Menlo Park,CA:AAAIPress,2008,3:1503-1505.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_27" title="LIM H,LEE J,KIM D.Optimization approach for feature selection in multi-label classification[J].Pattern Recognition Letters,2017,89:25-30." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES0781C7C87CBDF2DC9DCAC730EE1640D9&amp;v=MTY5MzUvRnRDL3FQeE5ZNWg5ZUFvN3UyVWFua3dNTzNqaHJHZEFlTFNRUmM2V0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieTh3YTg9TmlmT2ZiTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                        LIM H,LEE J,KIM D.Optimization approach for feature selection in multi-label classification[J].Pattern Recognition Letters,2017,89:25-30.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-03 15:49</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2815-2821 DOI:10.11772/j.issn.1001-9081.2019030483            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>多标签学习中基于互信息的快速特征选择方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E6%B4%AA%E5%B3%B0&amp;code=11143243&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐洪峰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E6%8C%AF%E5%BC%BA&amp;code=41550180&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙振强</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E7%BB%8F%E6%B5%8E%E4%B8%8E%E7%AE%A1%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=0186961&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州师范大学经济与管理学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8E%A6%E9%97%A8%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0125037&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">厦门大学信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统的基于启发式搜索的多标记特征选择算法时间复杂度高的问题,提出一种简单快速的多标记特征选择(EF-MLFS)方法。首先使用互信息(MI)衡量每个维度的特征与每一维标记之间的相关性,然后将所得相关性相加并排序,最后按照总的相关性大小进行特征选择。将所提方法与六种现有的比较有代表性的多标记特征选择方法作对比,如最大依赖性最小冗余性(MDMR)算法和基于朴素贝叶斯的多标记特征选择(MLNB)方法等。实验结果表明,EF-MLFS方法进行特征选择并分类的结果在平均准确率、覆盖率、海明损失等常见的多标记分类评价指标上均达最优;该方法无需进行全局搜索,因此时间复杂度相较于MDMR、对偶多标记应用(PMU)等方法也有明显降低。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多标签学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征选择;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%92%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">互信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E8%AE%B0%E7%9B%B8%E5%85%B3%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标记相关性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *徐洪峰(1977—),男,江西上饶人,副教授,博士研究生,CCF会员,主要研究方向:机器学习、深度学习、计算机网络、企业信息化;电子邮箱homny@sina.com;
                                </span>
                                <span>
                                    孙振强(1993—),男,吉林吉林人,硕士研究生,主要研究方向:机器学习、数据挖掘。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>贵州省科学技术厅基金资助项目(黔科合J字[2011]2215号);</span>
                    </p>
            </div>
                    <h1><b>Fast feature selection method based on mutual information in multi-label learning</b></h1>
                    <h2>
                    <span>XU Hongfeng</span>
                    <span>SUN Zhenqiang</span>
            </h2>
                    <h2>
                    <span>School of Economics and Management, Guizhou Normal University</span>
                    <span>School of Informatics, Xiamen University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Concerning the high time complexity of traditional heuristic search-based multi-label feature selection algorithm, an Easy and Fast Multi-Label Feature Selection(EF-MLFS) method was proposed. Firstly, Mutual Information(MI) was used to measure the features and the correlations between the labels of each dimension; then, the obtained correlations were added up and ranked; finally, feature selection was performed according to the total correlation. The proposed method was compared to six existing representative multi-label feature selection methods such as Max-Dependency and Min-Redundancy(MDMR) algorithm, Multi-Label Naive Bayes(MLNB) method. Experimental results show that the average precision, coverage, Hamming Loss and other common multi-label classification indicators are optimal after feature selection and classificationby using EF-MLFS method. In addition, global search is not required in the method, so the time complexity is significantly reduced compared with MDMR and Pairwise Mutli-label Utility(PMU).</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-label%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-label learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature selection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=mutual%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">mutual information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=label%20correlation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">label correlation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XU Hongfeng,born in 1977,Ph.D.candidate,assistant professor.His research interests include machine learning,deep learning,computer network,enterprise informatization.;
                                </span>
                                <span>
                                    SUN Zhenqiang,born in 1993,M.S.candidate.His research interests include machine learning,data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-25</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Department of Science and Technology of Guizhou Province([2011]2215);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="57" name="57" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="58">在传统的监督学习任务中,每个样本被默认为只具有一种语义信息,也就是只包含一种分类标签。然而,这样的假设往往与现实世界的真实情况不符,例如,在图片分类任务中,一张沙滩风景图片往往同时包含“大海”“轮船”“落日”等景物,由此可见,使用单一标记无法充分表达其语义信息,同样使用传统的单标记分类方法将很难对这种情况进行准确的分类。多标记学习(Multi-label Learning)<citation id="218" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>应运而生,并在信息检索<citation id="219" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、生物信息<citation id="220" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、药物研发<citation id="221" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>以及传统中医诊断<citation id="222" type="reference"><link href="172" rel="bibliography" /><link href="174" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>等领域取得了出色的成果。</p>
                </div>
                <div class="p1">
                    <p id="59">与传统监督学习相比,多标记学习的输入输出空间维度更大。在大多数情况下,相较于单标记数据集,为了支持多个标记的学习任务,多标记数据的特征往往更加冗余和稀疏。然而,过高的特征维度将会导致维度灾难的发生,这将使得多标记学习任务效率变得低效和困难。因此,有效地解决多标记学习任务中的维度灾难问题变得十分关键。</p>
                </div>
                <div class="p1">
                    <p id="60">特征选择方法是直接从原始特征空间选择特征子集,因此保留了原始特征的物理含义,具有极强的可解释性和易操作性,尤其在高维数据集和有限数据集的数据预处理任务中具有十分重要的地位。特征选择方法根据一定的评价标准,从原始特征空间中选择一组最优特征子集,从而降低特征维度,提高分类性能。目前比较常见的评价标准有:依赖性度量、距离度量和信息度量。同时,区分多标记学习任务与传统学习框架的特点就是其不同标记之间往往具有相关性,而对这些相关性的有效利用将会有效降低学习任务的难度。根据朱越等<citation id="223" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>的研究,目前标记相关性可以分为一阶相关性、二阶相关性和三阶相关性。</p>
                </div>
                <div class="p1">
                    <p id="61">目前,大部分的研究工作均为对特征选择过程中使用的“最大相关性,最小冗余性”标准中的相关性和冗余性进行相关分析,从而导致了计算资源的浪费。本文在进行一系列实验时观察到,大多数多标记数据集由于其特征空间的高维性和稀疏性,其特征与特征之间的冗余程度往往很小。因此在多标记特征选择问题当中,冗余性的加入往往无法提升特征选择的效果,反而会增加不必要的计算资源的浪费。本文提出了一种简单快速的多标记特征选择(Easy and Fast Multi-Label Feature Selection, EF-MLFS)方法,是一种只利用特征-标记相关性的极简且效果很好的特征选择算法。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="63">在多标记特征选择发展的几十年里,涌现出了许多杰出的工作。为了更好地区分每种方法的相似和不同之处,相关学者对已有的特征选择方法进行了分类,目前广泛认可的分类方法有两种:第一种分类方法从特征选择的策略角度出发,将特征选择方法分为封装(wrapper)方法<citation id="224" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、过滤(filter)方法<citation id="225" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和嵌入(embedded)方法<citation id="226" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>;第二种分类方法从标记利用角度出发,将特征选择方法分为有监督(supervised)<citation id="227" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、无监督(unsupervised)<citation id="228" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和半监督(semi-supervised)<citation id="229" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>特征选择方法。</p>
                </div>
                <div class="p1">
                    <p id="64">本文主要利用互信息(Mutual Information, MI)和标记相关性(Label Correlation)进行特征选择。在利用信息论进行特征选择的众多方法中,最经典的两种方法被称为互信息特征选择(MI Feature Selection, MIFS)<citation id="230" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和 最大相关最小冗余性(maximum Relevance Minimum Redundancy, mRMR)<citation id="231" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。在MIFS方法中利用互信息估计某一维特征的信息量,并利用“贪心”搜索方法选择最优特征子集;在mRMR方法中选择出的特征子集具有“最大相关性,最小冗余性”的特点。除此之外,还有许多学者利用信息论进行特征选择的优秀工作,例如Lin等<citation id="232" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出最大独立性最小冗余性(Max-Dependency and Min-Redundancy, MDMR)算法;Lee等<citation id="233" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>利用多元互信息提出一种对偶多标记应用(Pairwise Mutli-label Utility, PMU);宋国杰等<citation id="234" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出一种基于最大熵原理特征选择方法;朱颢东等<citation id="235" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出一种优化的互信息特征选择算法等。</p>
                </div>
                <div class="p1">
                    <p id="65">除此之外,近几年利用标记信息进行特征选择的方法也不断涌现。例如,Wang等<citation id="236" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>通过最大化非独立分类信息进行特征选择;Brown等<citation id="237" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出一种针对信息论的联合特征选择框架;蔡亚萍等<citation id="238" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出一种利用局部标记相关性的特征选择方法;杨明等<citation id="239" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出一种结合标记相关性的半监督特征选择方法;Braytee等<citation id="240" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出一种基于非负矩阵分解的特征选择方法;Liu等<citation id="241" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出一种基于标记相关性的加权特征选择方法;Monard等<citation id="242" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>提出了一种根据原始标记相关性进行标记空间重构的方法。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag">2 相关知识</h3>
                <h4 class="anchor-tag" id="67" name="67">2.1 <b>互信息与最大相关最小冗余性</b>(mRMR)</h4>
                <div class="p1">
                    <p id="68">信息论<citation id="243" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>已经被广泛应用于诸多领域。作为信息理论的重要组成部分,互信息是一种衡量相关性的有效手段,描述了两组变量之间信息共享的程度。例如,两组随机变量<i>A</i>和<i>B</i>之间的互信息可以定义如下:</p>
                </div>
                <div class="p1">
                    <p id="69"><i><b>I</b></i>(<i>A</i>;<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>b</mi><mo>∈</mo><mi>B</mi></mrow></munder><mi>p</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo><mrow><mi>lg</mi></mrow><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="70">其中: <i>p</i>(<i>a</i>,<i>b</i>)为<i>A</i>、<i>B</i>的联合概率分布。</p>
                </div>
                <div class="p1">
                    <p id="71">mRMR作为特征选择的经典算法,已经演变出许多变种。mRMR的作者认为“前<i>m</i>个最好的特征不一定是最好的<i>m</i>个特征”,因为前<i>m</i>个特征可能存在大量相关信息,这些特征之间具有高度相关性,所以作者提出mRMR框架,并将其形式化描述为:</p>
                </div>
                <div class="p1">
                    <p id="72">max <i>Φ</i>(<i>D</i>,<i>R</i>)=<i>D</i>-<i>R</i></p>
                </div>
                <div class="p1">
                    <p id="73">其中:<i>D</i>和<i>R</i>分别代表独立性(Dependence)和冗余性(Redundancy)。<i>D</i>和<i>R</i>的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mi mathvariant="bold-italic">Ι</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>c</i>)</p>
                </div>
                <div class="p1">
                    <p id="75"><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mi>S</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mi mathvariant="bold-italic">Ι</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i><b>x</b></i><sub><i>j</i></sub>)</p>
                </div>
                <div class="p1">
                    <p id="76">其中:<i>S</i>为选择的特征子集;<i>c</i>为对应的标记。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">2.2 <b>评价指标</b></h4>
                <div class="p1">
                    <p id="78">考虑与其他方法的比较,本文中选取了海明损失、<i>One</i>-<i>Error</i>、覆盖率、排序损失和平均准确率作为评价指标。本文将统一使用<i><b>y</b></i><sub><i>i</i></sub>∈<i>L</i>表示真实标记, <i><b>y</b></i>′<sub><i>i</i></sub>表示对特征向量<i><b>x</b></i><sub><i>i</i></sub>的预测标签,<i>N</i>表示样本数量,<i>m</i>表示标记维度。</p>
                </div>
                <div class="p1">
                    <p id="79">1)海明损失(Hamming Loss, HL):</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mfrac><mrow><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo>♁</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>m</mi></mfrac></mrow></mstyle></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中:♁为异或运算。该指标计算的是在所有<i>N</i>*<i>m</i>个预测标记中犯错的比例,其值在0～1,指标值越小越好。</p>
                </div>
                <div class="p1">
                    <p id="82">2)One-Error(OE):</p>
                </div>
                <div class="area_img" id="253">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201910005_25300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="83">该指标反映的是在预测的标记序列中,要覆盖所有的相关标记需要的搜索深度,其值越小表示所有相关标记均被排在比较靠前的位置。</p>
                </div>
                <div class="p1">
                    <p id="254">3)覆盖率(Coverage,CV):</p>
                </div>
                <div class="area_img" id="255">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201910005_25500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="256">该指标反映的是在预测的标记序列中，要覆盖所有的相关标记需要的搜索深度，其值越小表示所有相关标记均被排在比较靠前的位置。</p>
                </div>
                <div class="p1">
                    <p id="84">4)排序损失(Ranking Loss, RL):</p>
                </div>
                <div class="p1">
                    <p id="85"><i>RL</i>=</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle><mrow><mo>|</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>≤</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo stretchy="false">(</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mo>∈</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>×</mo><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">其中: <i><b>y</b></i><sub><i>i</i></sub>和<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>¯</mo></mover></math></mathml><sub><i>i</i></sub>分别表示<i><b>x</b></i><sub><i>i</i></sub>的相关标记集和无关标记集,该指标计算相关与无关标记对出现错误的比例,指标值越小越好。</p>
                </div>
                <div class="p1">
                    <p id="88">5)平均准确率(Average Precision, AP):</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>γ</mi><mo>∈</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi>γ</mi><mo>∈</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>:</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>γ</mi><mo>≤</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>γ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>γ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">该指标衡量的是按照预测值排序的标记序列中,被排在相关标记之前的标记仍是相关标记的情况,指标值越大越好。</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag">3 EF-MLFS</h3>
                <div class="p1">
                    <p id="92">在多标记特征选择问题中,“最大相关性,最小冗余性”是一个经典且证实有效的特征选择标准。目前该标准的优化算法仍然是建立在最大相关性和最小冗余性的基础上。但对主要多标记数据集上进行实验时发现,特征间的冗余性或许并不是特征选择过程中所必须考虑到的因素,而特征与标记相关性衡量可能对特征选择结果起到决定性作用。同时,忽略特征间冗余性的影响,将会大幅简化算法运算过程,从而提高算法性能。根据“奥卡姆剃刀”原则忽略特征冗余度的影响,不仅未降低算法性能,反而在大多数数据集上大幅提升了特征选择的效果。</p>
                </div>
                <div class="p1">
                    <p id="93">在多数的多标记数据集中,标记-特征相关性往往可以提供重要信息,利用这些信息将有效降低学习任务的难度,同时提升学习结果的鲁棒性。在多标记数据集中,特征往往是连续或高度离散的,同时十分稀疏。因此,无论是使用互信息,还是用欧氏距离的方法衡量特征间的冗余度,其结果可以发现特征与特征间的冗余性几乎为零,其效果往往微乎其微甚至对特征-标记相关性的使用产生负面影响。同时,多标记的监督学习中最重要的就是对标签的有效利用,因此本文从最简单设想切入,利用互信息来衡量特征-标记相关性,同时忽略掉冗余性的影响,最后依据特征-标记相关性的大小来进行特征选择。</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表</b>1 <b>多标记数据集详细信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.1 <i>Detail information of multi</i>-<i>label datasets</i></p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td><br />数据集</td><td>样本数量</td><td>特征数量</td><td>分类数量</td><td>训练集</td><td>测试集</td><td>数据类型</td></tr><tr><td><br /><i>Arts</i></td><td>5 000</td><td>462</td><td>26</td><td>2 000</td><td>3 000</td><td><i>Text</i></td></tr><tr><td><br /><i>Business</i></td><td>5 000</td><td>438</td><td>30</td><td>2 000</td><td>3 000</td><td><i>Text</i></td></tr><tr><td><br /><i>Education</i></td><td>5 000</td><td>550</td><td>33</td><td>2 000</td><td>3 000</td><td><i>Text</i></td></tr><tr><td><br /><i>Health</i></td><td>5 000</td><td>612</td><td>32</td><td>2 000</td><td>3 000</td><td><i>Text</i></td></tr><tr><td><br /><i>Science</i></td><td>5 000</td><td>743</td><td>40</td><td>3 000</td><td>2 000</td><td><i>Text</i></td></tr><tr><td><br /><i>Yeast</i></td><td>2 417</td><td>103</td><td>14</td><td>918</td><td>1 499</td><td><i>Biology</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="95"><i>EF</i>-<i>MLFS</i>方法的计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="96">首先,使用互信息计算每一维特征-标记之间的相关性矩阵<i><b>R</b></i>:</p>
                </div>
                <div class="p1">
                    <p id="97"><i><b>R</b></i><sub><i>i</i></sub><sub>, </sub><sub><i>j</i></sub>=<i><b>I</b></i>(<i><b>f</b></i><sub><i>i</i></sub>;<i><b>l</b></i><sub><i>j</i></sub>)</p>
                </div>
                <div class="p1">
                    <p id="98">其中:<i><b>R</b></i><sub><i>i</i></sub><sub>, </sub><sub><i>j</i></sub>为相关性矩阵的每一个元素; <i><b>f</b></i><sub><i>i</i></sub>表示第<i>i</i>维特征向量;<i><b>l</b></i><sub><i>j</i></sub>表示第<i>j</i>维标记向量。假设所选数据集中有<i>N</i>维特征向量,<i>K</i>维标记向量,则矩阵<i><b>R</b></i>为一个<i>N</i>*<i>K</i>的矩阵。该矩阵中蕴含的就是特征-标记相关性信息,数值越大代表特征与标记之间的相关性越强。</p>
                </div>
                <div class="p1">
                    <p id="99">如果某一维度特征与所有标记相关性的总和最大,则它就是最重要的特征,根据重要性进行排序,将会得到一个特征重要性的有序向量。</p>
                </div>
                <div class="p1">
                    <p id="100">根据以上假设,本文将矩阵<i><b>R</b></i>按列相加,得到特征重要性向量<i><b>I</b></i>:</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ι</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi mathvariant="bold-italic">R</mi></mstyle><msub><mrow></mrow><mrow><mo>:</mo><mo>;</mo><mi>n</mi></mrow></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">最后根据向量<i><b>I</b></i>的大小从大到小进行排序并记录相应的特征维度标记,得到最后的特征重要性排序向量<i><b>I</b></i><b>-</b><i><b>Rank</b></i>。</p>
                </div>
                <h3 id="103" name="103" class="anchor-tag">4 实验与分析</h3>
                <div class="p1">
                    <p id="104">为了评测<i>EF</i>-<i>MLFS</i>算法的性能,本文将在6个真实数据集上与其他6种特征选择算法进行对比实验。实验采用多标记最近邻(<i>Multi</i>-<i>Label</i> K <i>Nearest Neighbors</i>, <i>ML</i>-<i>KNN</i>)<citation id="244" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>作为分类算法对特征选择后的数据集进行评估,并将近邻数量k设置为15。对比算法的参数设置依照原论文给出的推荐参数进行设置。<i>ML</i>-<i>KNN</i>相关代码可以在<i>LAMDA</i>实验室主页进行下载(<i>http</i>://<i>lamda</i>.<i>nju</i>.<i>edu</i>.<i>cn</i>/<i>CH</i>.<i>MainPage</i>.<i>ashx</i>)。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">4.1 <b>实验数据及设置</b></h4>
                <div class="p1">
                    <p id="106">本实验数据主要来自于公开多标记数据集网站<i>MULAN</i>(<i>http</i>://<i>mulan</i>.<i>sourceforge</i>.<i>net</i>),其中<i>Arts</i>、<i>Business</i>、<i>Education</i>、<i>Health</i>、<i>Science</i>五个网页数据集属于<i>Yahoo</i>数据集,每个数据集含5 000个样本,提取的特征表示不同的词在文本中的频率,标记表示文本的类别信息;<i>Yeast</i>为生物数据集,包含2 417个样本,其特征维度是103。表1列出了所使用数据集的详细信息。同时,为比较<i>EF</i>-<i>MlFS</i>算法的性能,实验将与基于朴素贝叶斯的多标记特征选择(<i>Multi</i>-<i>Label Naive Bayes</i>, <i>MLNB</i>)<citation id="245" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、<i>PMU</i><citation id="246" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、<i>MDDMspc</i>(<i>Mutli</i>-<i>label Dimensionality Reduction via Dependecnce Maximization with Uncorrelated Feature Constraint</i>)<citation id="247" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、<i>MDDMproj</i>(<i>Multi</i>-<i>label Dimension Reduction via Dependence Maximization with Uncorrelated Projection Constraint</i>)<citation id="248" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、<i>MDMR</i><citation id="249" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>以及<i>COMI</i>(<i>Convex Optimization and MI</i>)<citation id="250" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>6种多标记特征选择算法在上述评价指标上进行比较,其中<i>COMI</i>方法<citation id="251" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>为<i>Lim</i>等于2017年提出的一种基于互信息与凸优化的方法,该方法改进了以往基于启发式搜索的特征选择策略,在<i>mRMR</i>方法的基础上利用互信息计算相关性与冗余性,是目前基于互信息进行特征选择的众多方法中比较有代表性的一种。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">4.2 <b>实验结果与分析</b></h4>
                <div class="p1">
                    <p id="108">为对比各算法所能达到的最好分类效果,本文实验将最优特征子集维度从1调整至最大并绘制得分曲线,虽然<i>MLNB</i>算法最大特征数量与其他算法不同,但仍具有可比性。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910005_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同方法在平均准确率指标上的实验结果对比" src="Detail/GetImg?filename=images/JSJY201910005_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 不同方法在平均准确率指标上的实验结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910005_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.1 <i>Experimental result comparison of different methods on Average Precision</i></p>

                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910005_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同方法在覆盖率指标上的实验结果对比" src="Detail/GetImg?filename=images/JSJY201910005_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同方法在覆盖率指标上的实验结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910005_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.2 <i>Experimental result comparison of different methods on Coverage</i></p>

                </div>
                <div class="p1">
                    <p id="111">图1～5分别为7种方法在不同数据集上的特征选择结果。其中,海明损失、覆盖率、排序损失、<i>One</i>-<i>Error</i>四项指标数值越小表示分类性能越优,平均准确率指标数值越大表示分类性能越优。</p>
                </div>
                <div class="p1">
                    <p id="112">从图1～5可以看出:</p>
                </div>
                <div class="p1">
                    <p id="113">1)实验曲线在前几个特征呈明显上升或下降趋势,在达到最优值后呈相反趋势变化,说明特征选择算法对大多数数据集有效,并且可以选择出效果明显好于使用所有特征进行分类的特征子集。</p>
                </div>
                <div class="p1">
                    <p id="114">2)<i>EF</i>-<i>MLFS</i>方法在<i>Arts</i>、<i>Business</i>、<i>Education</i>、<i>Health</i>、<i>Science</i>、<i>Yeast</i>共6个数据集上的5种评价指标均可以达到最优的效果,同时在<i>Arts</i>与<i>Health</i>等文本数据集上效果也非常显著,说明本文方法泛化效果与选择结果均优于其他方法。</p>
                </div>
                <div class="p1">
                    <p id="115">3)<i>MDDM</i>是一种基于矩阵分解的方法,因此其在时间性能上有明显优势。考虑到<i>EF</i>-<i>MLFS</i>方法几乎不需要任何复杂的矩阵运算,大部分时间消耗来自于计算特征-标记相关性矩阵,因此<i>EF</i>-<i>MLFS</i>方法相较于<i>MDMR</i>、<i>MLNB</i>、<i>PMU</i>三种方法在时间性能上均有三个量级的提升效果。</p>
                </div>
                <div class="p1">
                    <p id="116">4)<i>EF</i>-<i>MLFS</i>方法具有最高的性能时间比,可以在提高性能的前提下,大幅提高算法效率。</p>
                </div>
                <div class="p1">
                    <p id="117">5)<i>COMI</i>方法虽然也是一种基于互信息的多标记特征选择方法,但原文中并未对其数据预处理进行详细描述,同时其特征选择过程中涉及到全局优化的过程,虽然较传统的启发式搜索方法在时间性能上有较大提升,但在性能和时间上均未达到最优效果。因此<i>EF</i>-<i>MLFS</i>方法虽然简单,但是仍可达到较优的性能。</p>
                </div>
                <div class="p1">
                    <p id="118">总体来讲,<i>EF</i>-<i>MLFS</i>算法在上述对比指标下均有不错的表现,尤其<i>EF</i>-<i>MLFS</i>算法是一种快速有效的特征选择方法,该方法具有更强的鲁棒性和更好的泛化性,可以适应多个应用场景并找到最优特征子集。同时,上述实验也验证了第3章中关于特征空间冗余性的假设。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910005_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同方法在海明损失指标上的实验结果对比" src="Detail/GetImg?filename=images/JSJY201910005_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同方法在海明损失指标上的实验结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910005_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.3 <i>Experimental result comparison of different methods on Hamming loss</i></p>

                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910005_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同方法在One-Error指标上的实验结果对比" src="Detail/GetImg?filename=images/JSJY201910005_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同方法在<i>One</i>-<i>Error</i>指标上的实验结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910005_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.4 <i>Experimental result comparison of different methods on One</i>-<i>Error</i></p>

                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910005_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同方法在排序损失指标上的实验结果对比" src="Detail/GetImg?filename=images/JSJY201910005_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同方法在排序损失指标上的实验结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910005_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.5 <i>Experimental result comparison of different methods on Ranking loss</i></p>

                </div>
                <h3 id="122" name="122" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="123">本文在针对经典特征选择框架“最大相关性,最小冗余性”进行研究时发现,在大多数公开的多标记数据集中,冗余性的加入并不能有效提高多标记特征选择的效果,反而会加大特征选择过程的运算量,降低选择效率。因此,根据“奥卡姆剃刀”的核心原则“如无必要,勿增实体”的思想,将冗余性去掉,只利用特征-标记相关性进行多标记特征选择,并得到了很好的结果。</p>
                </div>
                <div class="p1">
                    <p id="124">但是,本文仍有诸多不足之处,需要在未来的工作当中进行改进。例如,如何有效定量验证数据当中冗余性是否必要,如何更加准确地衡量特征标记相关性,以及如何更加有效地挖掘特征-标记相关性当中蕴含的信息。同时针对标记相关性的研究工作也日益增多,对如何有效利用标记之间的相关性也是未来的研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="164">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201812007&amp;v=MTMwMzVGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFVMN1BOVGZBZHJHNEg5bk5yWTk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>朱越,姜远,周志华.一种基于多示例多标记学习的新标记学习方法[J].中国科学:信息科学,2018,48(12):1670-1680.(ZHU Y,JIANG Y,ZHOU Z H.Multi-instance multi-label new label learning[J].Scientia Sinica(Information),2018,48(12):1670-1680.)
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340071&amp;v=MDc5NTR3T1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFVydkxJVms9Tmo3QmFyTzRIdEhOckl0RlpP&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>SCHAPIRE R E,SINGER Y.Boos Texter:a boosting-based system for text categorization[J].Machine Learning,2000,39(2/3):135-168.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Protein classification with multiple algorithms">

                                <b>[3]</b>DIPLARIS S,TSOUMAKAS G,MITKAS P A,et al.Protein classification with multiple algorithms[C]//Proceedings of the 2005Panhellenic Conference on Informatics,LNCS 3746.Berlin:Springer,2005:448-456.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201715045&amp;v=MTM3NDhIOWJOcW85QllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhVTDdQTHo3TWFiRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>彭利红,刘海燕,任日丽,等.基于多标记学习预测药物-靶标相互作用[J].计算机工程与应用,2017,53(15):260-265.(PENG L H,LIU H Y,REN R L,et al.Predicting drug-target interactions with multi-label learning[J].Computer Engineering and Applications,2017,53(15):260-265.)
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modelling of inquiry diagnosis for coronary heart disease in traditional Chinese medicine by using multi-label learning">

                                <b>[5]</b>LIU G P,LI G Z,WANG Y L,et al.Modelling of inquiry diagnosis for coronary heart disease in traditional Chinese medicine by using multi-label learning[J].BMC Complementary and Alternative Medicine,2010,10(1):No.37.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-label feature selection with application to TCM state identification">

                                <b>[6]</b>DAI L,ZHANG J,LI C,et al.Multi-label feature selection with application to TCM state identification[J/OL].Concurrency and Computation:Practice and Experience,2018:No.e4634.[2019-01-10].https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4634.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An introduction to variable and feature selection">

                                <b>[7]</b>GUYON I,ELISSEEFF A.An introduction to variable and feature selection[J].Journal of Machine Learning Research,2003,3:1157-1182.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200309006&amp;v=MjM4NjRSN3FmWnVac0Z5bmhVTDdQTnlmVGJMRzRIdExNcG85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>宋国杰,唐世渭,杨冬青,等.基于最大熵原理的空间特征选择方法[J].软件学报,2003,14(9):1544-1550.(SONG GJ,TANG S W,YANG D Q,et al.A spatial feature selection method based on maximum entropy theory[J].Journal of Software,2003,14(9):1544-1550.)
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340375&amp;v=MjgyNTU5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJsVXJ2TElWaz1OajdCYXJPNEh0SE5ySXRGWit3S1kzazV6QmRoNGo5&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>GUYON I,WESTON J,BARNHILL S,et al.Gene selection for cancer classification using support vector machines[J].Machine Learning,2002,46(1/2/3):389-422.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature selection for unsupervised learning">

                                <b>[10]</b>DY J G.BRODLEY C E.Feature selection for unsupervised learning[J].Journal of Machine Learning Research,2004,5:845-889.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using mutual information for selecting features in supervised neural net learning">

                                <b>[11]</b>BATTITI R.Using mutual information for selecting features in supervised neural net learning[J].IEEE Transactions on Neural Networks,1994,5(4):537-550.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy">

                                <b>[12]</b>PENG H,LONG F,DING C.Feature selection based on mutual information criteria of max-dependency,max-relevance,and minredundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2005,27(8):1226-1238.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES8FF17D624B5694A7C672D13D80FCFB0D&amp;v=MDE1MzdCdUhZZk9HUWxmQnJMVTA1dHBoeGJ5OHdhOD1OaWZPZmJ2T2FOREwyNGxIWUprS0NuVTl2aEZnN0RoL1BIN2gyQm8xRDhIaU43cnJDT052RlNpV1dyN0pJRnBtYQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>LIN Y,LIU J,LIU J,et al.Multi-label feature selection based on max-dependency and min-redundancy[J].Neurocomputing,2015,168:92-103.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600191957&amp;v=MzA1MzNpZk9mYks4SHRETXFZOUZaZUlPQlhrK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUlWb1hheFU9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>LEE J,KIM D.Feature selection for multi-label classification using multivariate mutual information[J].Pattern Recognition Letters,2013,34(3):349-357.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201026004&amp;v=MjgzMzQ5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhVTDdQTHo3TWFiRzRIOUhPcVk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>朱颢东,陈宁,李红婵.优化的互信息特征选择方法[J].计算机工程与应用,2010,46(26):122-124.(ZHU H D,CHEN N,LI H C.Optimized mutual information feature selection method[J].Computer Engineering and Applications,2010,46(26):122-124.)
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature selection by maximizing independent classification information">

                                <b>[16]</b>WANG J,WEI J,YANG Z,et al.Feature selection by maximizing independent classification information[J].IEEE Transactions on Knowledge and Data Engineering,2017,29(4):828-841.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional likelihood maximisation: A unifying framework for information theoretic feature selection">

                                <b>[17]</b>BROWN G,POCOCK A,ZHAO M,et al.Conditional likelihood maximization:a unifying framework for information theoretic feature selection[J].Journal of Machine Learning Research,2012,13:27-66.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJDZ201604014&amp;v=MDMwMTQ5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhVTDdQS3lmUGRMRzRIOWZNcTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>蔡亚萍,杨明.一种利用局部标记相关性的多标记特征选择算法[J].南京大学学报(自然科学版),2016,52(4):693-704.(CAI Y P,YANG M.A multi-label feature selection algorithm by exploiting label correlations locally[J].Journal of Nanjing University(Natural Science Edition),2016,52(4):693-704.)
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SCPD&amp;filename=CN105975978A&amp;v=MTUyODVicWVaK1Z1Rml6bVVyZz1KaU82SHJheEdkVEZxSWMwQys0UEQzMUx4eFlUNnpvT1MzZm1wV0ZhZTdLV1I=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>杨明,蔡亚萍.一种结合标记相关性的半监督多标记特征选择及分类方法:CN201610256462.9[P].2016-09-28[2019-01-10].(YANG M,CAI Y P.A semi-supervised multi-label feature selection and classification method combined with marker correlation:CN201610256462.9[P].2016-09-28[2019-01-10].)
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-label feature selection using correlation information">

                                <b>[20]</b>BRAYTEE A,LIU W,CATCHPOOLE D R,et al.Multi-label feature selection using correlation information[C]//Proceedings of the 2017 ACM Conference on Information and Knowledge Management.New York:ACM,2017:1649-1656.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Label Correlation Based Weighting Feature Selection Approach for Multi-label Data">

                                <b>[21]</b>LIU L,ZHANG J,LI P,et al.A label correlation based weighting feature selection approach for multi-label data[C]//Proceedings of the 2016 International Conference on Web-Age Information Management,LNCS 9659.Cham:Springer,2016:369-379.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESCF123CFEE91C152261E4DBDFBB1F5B79&amp;v=MDg4NzlXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeGJ5OHdhOD1OaWZPZmNET0g5UFAzUGt3RWVJT2YzMDh6UlFWNjBwNVBBMlcybUJIZU1TUk43Mg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>SPOLA<image id="252" type="formula" href="images/JSJY201910005_25200.jpg" display="inline" placement="inline"><alt></alt></image>R N,MONARD M C,TSOUMAKAS G.A systematic review of multi-label feature selection and a new method based on label construction[J].Neurocomputing,2016,180:3-15.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A mathematical theory of communication">

                                <b>[23]</b>SHANNON C E.A mathematical theory of communication[J].Bell System Technical Journal,1948,27(4):623-656.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739502&amp;v=MDkyNDc9TmlmT2ZiSzdIdEROcVk5RlkrZ0dDWHc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSVZvWGF4VQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b>ZHANG M,ZHOU Z.ML-KNN:a lazy learning approach to multi-label learning[J].Pattern Recognition,2007,40(7):2038-2048.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601417399&amp;v=Mjc4MjBIdEROcVk5RVlPb0lEM1V3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSVZvWGF4VT1OaWZPZmJLNw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b>ZHANG M,PENA J M,ROBLES V.Feature selection for multilabel naive Bayes classification[J].Information Sciences,2009,179(19):3218-3229.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-label dimensionality reduction via dependency maximization">

                                <b>[26]</b>ZHANG Y,ZHOU Z H.Multi-label dimensionality reduction via dependence maximization[C]//Proceedings of the 23rd National Conference on Artificial Intelligence.Menlo Park,CA:AAAIPress,2008,3:1503-1505.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES0781C7C87CBDF2DC9DCAC730EE1640D9&amp;v=MDczMjBGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieTh3YTg9TmlmT2ZiTy9GdEMvcVB4Tlk1aDllQW83dTJVYW5rd01PM2pockdkQWVMU1FSYzZXQ09Odg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b>LIM H,LEE J,KIM D.Optimization approach for feature selection in multi-label classification[J].Pattern Recognition Letters,2017,89:25-30.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910005" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910005&amp;v=MjEwMzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVUw3TUx6N0JkN0c0SDlqTnI0OUZZWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
