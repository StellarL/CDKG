<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136445119971250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911018%26RESULT%3d1%26SIGN%3dscHPj%252bBrQCryWco31%252bLP9NYSwF4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911018&amp;v=MjkzNDBMejdCZDdHNEg5ak5ybzlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublY3ekw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#81" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="1 网络框架 ">1 网络框架</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="1.1 Mask R-CNN&lt;b&gt;算法&lt;/b&gt;">1.1 Mask R-CNN<b>算法</b></a></li>
                                                <li><a href="#98" data-title="1.2 &lt;b&gt;改进的&lt;/b&gt;Mask R-CNN&lt;b&gt;算法&lt;/b&gt;">1.2 <b>改进的</b>Mask R-CNN<b>算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="2 网络训练 ">2 网络训练</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#115" data-title="2.1 &lt;b&gt;数据集选取与制作&lt;/b&gt;">2.1 <b>数据集选取与制作</b></a></li>
                                                <li><a href="#118" data-title="2.2 &lt;b&gt;预训练&lt;/b&gt;">2.2 <b>预训练</b></a></li>
                                                <li><a href="#120" data-title="2.3 &lt;b&gt;聚类选取初始候选框&lt;/b&gt;">2.3 <b>聚类选取初始候选框</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#127" data-title="3.1 &lt;b&gt;实验可视化结果&lt;/b&gt;">3.1 <b>实验可视化结果</b></a></li>
                                                <li><a href="#130" data-title="3.2 &lt;b&gt;数据增强对比实验&lt;/b&gt;">3.2 <b>数据增强对比实验</b></a></li>
                                                <li><a href="#135" data-title="3.3 &lt;b&gt;采用不同策略训练网络的检测结果对比&lt;/b&gt;">3.3 <b>采用不同策略训练网络的检测结果对比</b></a></li>
                                                <li><a href="#138" data-title="3.4 &lt;b&gt;消融实验&lt;/b&gt;">3.4 <b>消融实验</b></a></li>
                                                <li><a href="#141" data-title="3.5 &lt;b&gt;不同目标检测算法结果对比&lt;/b&gt;">3.5 <b>不同目标检测算法结果对比</b></a></li>
                                                <li><a href="#145" data-title="3.6 &lt;b&gt;不同数据集实验结果&lt;/b&gt;">3.6 <b>不同数据集实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#153" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="图1 Mask R-CNN结构">图1 Mask R-CNN结构</a></li>
                                                <li><a href="#100" data-title="图2 改进的Mask R-CNN框架">图2 改进的Mask R-CNN框架</a></li>
                                                <li><a href="#108" data-title="图3 FCN掩码学习过程">图3 FCN掩码学习过程</a></li>
                                                <li><a href="#110" data-title="图4 特征学习">图4 特征学习</a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;模型参数设置&lt;/b&gt;"><b>表</b>1 <b>模型参数设置</b></a></li>
                                                <li><a href="#129" data-title="图5 可视化效果">图5 可视化效果</a></li>
                                                <li><a href="#132" data-title="图6 图像增强示例">图6 图像增强示例</a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;数据增强实验对比&lt;/b&gt;"><b>表</b>2 <b>数据增强实验对比</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同&lt;/b&gt;anchors&lt;b&gt;实验结果&lt;/b&gt;"><b>表</b>3 <b>不同</b>anchors<b>实验结果</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;检测模型各部分的贡献度&lt;/b&gt;"><b>表</b>4 <b>检测模型各部分的贡献度</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同目标检测算法实验结果&lt;/b&gt;"><b>表</b>5 <b>不同目标检测算法实验结果</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表&lt;/b&gt;6 INRIA&lt;b&gt;数据集上实验结果&lt;/b&gt;"><b>表</b>6 INRIA<b>数据集上实验结果</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表&lt;/b&gt;7 COCO2017&lt;b&gt;数据集上实验结果&lt;/b&gt;"><b>表</b>7 COCO2017<b>数据集上实验结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="173">


                                    <a id="bibliography_1" title=" PAPAGEORGIOU C P,OREN M,POGGIO T.A general framework for object detection [C]// Proceedings of the 6th IEEE International Conference on Computer Vision.Piscatway:IEEE,1998:555-562." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A general framework for object detection">
                                        <b>[1]</b>
                                         PAPAGEORGIOU C P,OREN M,POGGIO T.A general framework for object detection [C]// Proceedings of the 6th IEEE International Conference on Computer Vision.Piscatway:IEEE,1998:555-562.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_2" title=" DALAL N,TRIGGS B.Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">
                                        <b>[2]</b>
                                         DALAL N,TRIGGS B.Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:886-893.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_3" title=" WANG X Y,HAN T,YAN S C.An HOG-LBP human detector with partial occlusion handling [C]// Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway:IEEE,2009:32-39." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An HOG-LBP human detector w ith partial occlusion handling">
                                        <b>[3]</b>
                                         WANG X Y,HAN T,YAN S C.An HOG-LBP human detector with partial occlusion handling [C]// Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway:IEEE,2009:32-39.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_4" title=" GIRSHICK R,DONAHUE J,DARRELL T,et al.Region-based convolutional networks for accurate object detection and segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(1):142-158." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Region-Based Convolutional Networks for Accurate Object Detection and Segmentation">
                                        <b>[4]</b>
                                         GIRSHICK R,DONAHUE J,DARRELL T,et al.Region-based convolutional networks for accurate object detection and segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(1):142-158.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_5" title=" LOWE D G.Object recognition from local scale-invariant features [C]// Proceedings of the 1999 International Conference on Computer Vision.Piscataway:IEEE,1999:1150-1157." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object Recognition from Local Scale-Invariant Features">
                                        <b>[5]</b>
                                         LOWE D G.Object recognition from local scale-invariant features [C]// Proceedings of the 1999 International Conference on Computer Vision.Piscataway:IEEE,1999:1150-1157.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_6" title=" LOWE D G.Distinctive image features from scale-invariant keypoints [J].International Journal of Computer Vision,2004,60(2):91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjcyNTVsVXIzTUkxMD1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rmly&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         LOWE D G.Distinctive image features from scale-invariant keypoints [J].International Journal of Computer Vision,2004,60(2):91-110.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_7" title=" WANG S F,YAN J H,WANG Z G.Improved moving object detection algorithm based on local united feature [J].Chinese Journal of Scientific Instrument,2015,36(10):2241-2248." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201510011&amp;v=MDA0NjNVUjdxZlp1WnNGeW5uVjd6TFBEelRiTEc0SDlUTnI0OUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         WANG S F,YAN J H,WANG Z G.Improved moving object detection algorithm based on local united feature [J].Chinese Journal of Scientific Instrument,2015,36(10):2241-2248.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_8" title=" VIOLA P A,JONES M J.Rapid object detection using a boosted cascade of simple features [C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2001:511-518." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rapid object detection using a boosted cascade of simple features">
                                        <b>[8]</b>
                                         VIOLA P A,JONES M J.Rapid object detection using a boosted cascade of simple features [C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2001:511-518.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_9" title=" DALAL N,TRIGGS B.Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of Oriented Gradients for Human Detection">
                                        <b>[9]</b>
                                         DALAL N,TRIGGS B.Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:886-893.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_10" title=" GIRSHICK R.Fast R-CNN [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway:IEEE,2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[10]</b>
                                         GIRSHICK R.Fast R-CNN [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway:IEEE,2015:1440-1448.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     REN S Q,HE K M,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.</a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_12" title=" HE K M,GKIOXARI G,GIRSHICK R,et al.Mask R-CNN [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2980-2988." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[12]</b>
                                         HE K M,GKIOXARI G,GIRSHICK R,et al.Mask R-CNN [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2980-2988.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_13" title=" REDMON J,DIVVALA S K,GIRSHICK R,et al.You only look once:unified,real-time object detection [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified Real-Time Object Detection">
                                        <b>[13]</b>
                                         REDMON J,DIVVALA S K,GIRSHICK R,et al.You only look once:unified,real-time object detection [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:779-788.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_14" title=" REDMON J,FARHADI A.YOLO9000:better,faster,stronger [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6517-6525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">
                                        <b>[14]</b>
                                         REDMON J,FARHADI A.YOLO9000:better,faster,stronger [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6517-6525.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     REDMON J,FARHADI A.YOLOv3:an incremental improvement [EB/OL].[2019- 03- 26].https://arxiv.org/pdf/1804.02767.pdf.</a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_16" title=" LIU W,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector [C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">
                                        <b>[16]</b>
                                         LIU W,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector [C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:21-37.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_17" title=" 张中宝,王洪元,杨薇.基于Faster-RCNN的遥感图像飞机检测算法[J].南京师大学报(自然科学版),2018,41(4):79-86.(ZHANG Z B,WANG H Y,YANG W.Remote sensing image aircraft detection algorithm based on Faster RCNN [J].Journal of Nanjing Normal University (Natural Science Edition),2018,41(4):79-86.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJSF201804014&amp;v=MTc2NjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WN3pMS3lmWWFMRzRIOW5NcTQ5RVlJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         张中宝,王洪元,杨薇.基于Faster-RCNN的遥感图像飞机检测算法[J].南京师大学报(自然科学版),2018,41(4):79-86.(ZHANG Z B,WANG H Y,YANG W.Remote sensing image aircraft detection algorithm based on Faster RCNN [J].Journal of Nanjing Normal University (Natural Science Edition),2018,41(4):79-86.)
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_18" title=" YANG W,ZHANG J,ZHANG Z B,et al.Research on real-time vehicle detection algorithm based on deep learning [C]// Proceedings of the 2018 Chinese Conference on Pattern Recognition and Computer Vision.Berlin:Springer,2018:126-127." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on real-time vehicle detection algorithm based on deep learning">
                                        <b>[18]</b>
                                         YANG W,ZHANG J,ZHANG Z B,et al.Research on real-time vehicle detection algorithm based on deep learning [C]// Proceedings of the 2018 Chinese Conference on Pattern Recognition and Computer Vision.Berlin:Springer,2018:126-127.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_19" title=" YANG W,ZHANG J,WANG H Y,et al.A vehicle real-time detection algorithm based on YOLOv2 framework [C]// Proceedings of the 2018 Real-Time Image and Video Processing.Bellingham,WA:SPIE,2018:106700N." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A vehicle real-time detection algorithm based on YOLOv2 framework">
                                        <b>[19]</b>
                                         YANG W,ZHANG J,WANG H Y,et al.A vehicle real-time detection algorithm based on YOLOv2 framework [C]// Proceedings of the 2018 Real-Time Image and Video Processing.Bellingham,WA:SPIE,2018:106700N.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_20" title=" PHAM M T,LEFEVRE S.Buried object detection from B-Scan ground penetrating radar data using Faster-RCNN [C]// Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium.Piscataway:IEEE,2018:6804-6807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Buried object detection from B-scan ground penetrating radar data using Faster-RCNN">
                                        <b>[20]</b>
                                         PHAM M T,LEFEVRE S.Buried object detection from B-Scan ground penetrating radar data using Faster-RCNN [C]// Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium.Piscataway:IEEE,2018:6804-6807.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_21" title=" KIM J,BATCHULUUN G,PARK K.Pedestrian detection based on Faster R-CNN in nighttime by fusing deep convolutional features of successive images [J].Expert Systems with Applications,2018,114:15-33." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES587D064B9347194FB191A70E0274BBBE&amp;v=MDM1ODlpZk9mYmF3R2FYTXFZczNiZWdMQzMwd3kyQmg2elo4T1hqaTJSSTNmcmJtTjhqcUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhicTd3NnM9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         KIM J,BATCHULUUN G,PARK K.Pedestrian detection based on Faster R-CNN in nighttime by fusing deep convolutional features of successive images [J].Expert Systems with Applications,2018,114:15-33.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_22" title=" SCHWEITZER D,AGRAWAL R.Multi-class object detection from aerial images using Mask R-CNN [C]// Proceedings of the 2018 IEEE International Conference on Big Data.Piscataway:IEEE,2018:3470-3477." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-class object detection from aerial images using Mask R-CNN">
                                        <b>[22]</b>
                                         SCHWEITZER D,AGRAWAL R.Multi-class object detection from aerial images using Mask R-CNN [C]// Proceedings of the 2018 IEEE International Conference on Big Data.Piscataway:IEEE,2018:3470-3477.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_23" title=" WEI X,XIE C,WU J.Mask-CNN:localizing parts and selecting descriptors for fine-grained bird species categorization [J].Pattern Recognition,2018,76:704-714." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES96B207EC1C99A62A53D84860E84EC7DB&amp;v=MTI0NjNuUXM3dENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhicTd3NnM9TmlmT2ZicStiTlBNcVBvMlpaZ0dCUTAveldjVzZVdDFUSGZrckdjOWZjZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         WEI X,XIE C,WU J.Mask-CNN:localizing parts and selecting descriptors for fine-grained bird species categorization [J].Pattern Recognition,2018,76:704-714.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_24" title=" ANGELOVA A,ZHU S H,LIN Y Q.Image segmentation for large-scale subcategory flower recognition [C]// Proceedings of the 2013 IEEE Workshop on Applications of Computer Vision.Piscataway:IEEE,2013:39-45." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image segmentation for large-scale subcategory flower recognition">
                                        <b>[24]</b>
                                         ANGELOVA A,ZHU S H,LIN Y Q.Image segmentation for large-scale subcategory flower recognition [C]// Proceedings of the 2013 IEEE Workshop on Applications of Computer Vision.Piscataway:IEEE,2013:39-45.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_25" title=" KRAUSE J,STARK M,DENG J,et al.3D object representations for fine-grained categorization [C]// Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops.Washington,DC:IEEE Computer Society,2013:554-561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3d object representations for fine-grained categorization">
                                        <b>[25]</b>
                                         KRAUSE J,STARK M,DENG J,et al.3D object representations for fine-grained categorization [C]// Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops.Washington,DC:IEEE Computer Society,2013:554-561.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_26" title=" HUANG S,XU Z,TAO D,et al.Part-stacked CNN for fine-grained visual categorization [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1173-1182." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Part-Stacked CNN for Fine-Grained Visual Categorization">
                                        <b>[26]</b>
                                         HUANG S,XU Z,TAO D,et al.Part-stacked CNN for fine-grained visual categorization [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1173-1182.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_27" title=" LIN D,SHEN Y,LU C,et al.Deep LAC:deep localization,alignment and classification for fine-grained recognition [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1666-1674." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep lac:Deep localization, alignment and classification for fine-grained recognition">
                                        <b>[27]</b>
                                         LIN D,SHEN Y,LU C,et al.Deep LAC:deep localization,alignment and classification for fine-grained recognition [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1666-1674.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_28" title=" ZHANG Y,WEI X,WU J,et al.Weakly supervised fine-grained categorization with part-based image representation [J].IEEE Transactions on Image Processing,2016,25(4):1713-1725." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weakly supervised finegrained categorization with part-based image representation">
                                        <b>[28]</b>
                                         ZHANG Y,WEI X,WU J,et al.Weakly supervised fine-grained categorization with part-based image representation [J].IEEE Transactions on Image Processing,2016,25(4):1713-1725.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_29" title=" XIE G,ZHANG X,YANG W,et al.LG-CNN:from local parts to global discrimination for fine-grained recognition [J].Pattern Recognition,2017,71:118-131." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB898C212027A896401D52C6286BC38D8&amp;v=Mjg0MzRHd0Y5bS9yWTVIWk9rSWZYUXd5UklUNjB0NFNnemtyaG96QzhHWFRjNlhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4YnE3dzZzPU5pZk9mYw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                         XIE G,ZHANG X,YANG W,et al.LG-CNN:from local parts to global discrimination for fine-grained recognition [J].Pattern Recognition,2017,71:118-131.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_30" title=" LEE S,CHAN C,MAYO S J,et al.How deep learning extracts and learns leaf features for plant classification [J].Pattern Recognition,2017,71:1-13." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC1122BF934EB79A212805AF5E44662F1&amp;v=MDg2MjRZZk9HUWxmQnJMVTA1dHBoeGJxN3c2cz1OaWZPZmNDNUg5UE8zZmxNWis5NmZuc3d2aFFTNkRkOVRRNlVxV2N4ZmJTU1I4eWVDT052RlNpV1dyN0pJRnBtYUJ1SA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[30]</b>
                                         LEE S,CHAN C,MAYO S J,et al.How deep learning extracts and learns leaf features for plant classification [J].Pattern Recognition,2017,71:1-13.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_31" title=" DAI J,HE K,SUN J.Instance-aware semantic segmentation via multi-task network cascades [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:3150-3158." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Instance-aware Semantic Segmentation via Multi-task Network Cascades">
                                        <b>[31]</b>
                                         DAI J,HE K,SUN J.Instance-aware semantic segmentation via multi-task network cascades [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:3150-3158.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_32" title=" LI Y,QI H Z,DAI J,et al.Fully convolutional instance-aware semantic segmentation [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4438-4446." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional instance-aware semantic segmentation">
                                        <b>[32]</b>
                                         LI Y,QI H Z,DAI J,et al.Fully convolutional instance-aware semantic segmentation [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4438-4446.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_33" >
                                        <b>[33]</b>
                                     LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:3431-3440.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-23 13:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3210-3215 DOI:10.11772/j.issn.1001-9081.2019051051            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进的Mask R-CNN的行人细粒度检测算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E7%B9%81&amp;code=41153481&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱繁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B4%AA%E5%85%83&amp;code=24489626&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王洪元</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%BB%A7&amp;code=24805024&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张继</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B8%B8%E5%B7%9E%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0268985&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">常州大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对复杂场景下行人检测效果差的问题,采用基于深度学习的目标检测中领先的研究成果,提出了一种基于改进Mask R-CNN框架的行人检测算法。首先,采用<i>K</i>-means算法对行人数据集的目标框进行聚类得到合适的长宽比,通过增加一组长宽比(2∶5)使12种anchors适应图像中行人的尺寸;然后,结合细粒度图像识别技术,实现行人的高定位精度;其次,采用全卷积网络(FCN)分割前景对象,并进行像素预测获得行人的局部掩码(上半身、下半身),实现对行人的细粒度检测;最后,通过学习行人的局部特征获得行人的整体掩码。为了验证改进算法的有效性,将其与当前具有代表性的目标检测方法(如更快速的区域卷积神经网络(Faster R-CNN)、YOLOv2、R-FCN)在同数据集上进行对比。实验结果表明,改进的算法提高了行人检测的速度和精度,并且降低了误检率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Mask%20R-CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Mask R-CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3EK%3C%2Fi%3E-means%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>K</i>-means算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%86%E7%B2%92%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">细粒度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全卷积网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    朱繁(1994—),女,江苏淮安人,硕士研究生,主要研究方向:计算机视觉;;
                                </span>
                                <span>
                                    *王洪元(1960—),男,江苏常熟人,教授,博士,CCF会员,主要研究方向:计算机视觉,电子邮箱,hywang@cczu.edu.cn;
                                </span>
                                <span>
                                    张继(1981—),男,江苏常州人,讲师,硕士,CCF会员,主要研究方向:计算机视觉。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61572085);</span>
                    </p>
            </div>
                    <h1><b>Fine-grained pedestrian detection algorithm based on improved Mask R-CNN</b></h1>
                    <h2>
                    <span>ZHU Fan</span>
                    <span>WANG Hongyuan</span>
                    <span>ZHANG Ji</span>
            </h2>
                    <h2>
                    <span>College of Information Science and Engineering, Changzhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of poor pedestrian detection effect in complex scenes, a pedestrian detection algorithm based on improved Mask R-CNN framework was proposed with the use of the leading research results in deep learning-based object detection. Firstly, <i>K</i>-means algorithm was used to cluster the object frames of the pedestrian datasets to obtain the appropriate aspect ratio. By adding the set of aspect ratio(2∶5), 12 anchors were able to be adapted to the size of the pedestrian in the image. Secondly, combined with the technology of fine-grained image recognition, the high accuracy of pedestrian positioning was realized. Thirdly, the foreground object was segmented by the Full Convolutional Network(FCN), and pixel prediction was performed to obtain the local mask(upper body, lower body) of the pedestrian, so as to achieve the fine-grained detection of pedestrians. Finally, the overall mask of the pedestrian was obtained by learning the local features of the pedestrian. In order to verify the effectiveness of the improved algorithm, the proposed algorithm was compared with the current representative object detection methods(such as Faster Region-based Convolutional Neural Network(Faster R-CNN), YOLOv2 and R-FCN(Region-based Fully Convolutional Network)) on the same dataset. The experimental results show that the improved algorithm increases the speed and accuracy of pedestrian detection and reduces the false positive rate.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Mask%20R-CNN(Region%20with%20Convolutional%20Neural%20Network)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Mask R-CNN(Region with Convolutional Neural Network);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pedestrian%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pedestrian detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3EK%3C%2Fi%3E-means%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>K</i>-means algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fine-grained&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fine-grained;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fully%20Convolutional%20Network(FCN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fully Convolutional Network(FCN);</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHU Fan, born in 1994, M. S. candidate. Her research interests include computer vision. ;
                                </span>
                                <span>
                                    WANG Hongyuan, born in 1960, Ph. D., professor. His research interests include computer vision. ;
                                </span>
                                <span>
                                    ZHANG Ji, born in 1981, M. S., lecturer. His research interests include computer vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China(61572085);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="81" name="81" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="82">行人检测技术由于应用的广泛性使其在计算机视觉领域成为一个重要的分支,对视频监控、车辆辅助驾驶、智能机器人等多个领域提供了重要的技术支持。它与行人重识别、目标跟踪等领域的联系密切相关,被认为是一个图像检索的子问题。</p>
                </div>
                <div class="p1">
                    <p id="83">传统的行人检测方法大多以图像识别为基础,并基于人工设计的特征提取器进行特征的提取。首先, 在图片上使用穷举法选出所有物体可能出现的目标区域框; 然后,对这些区域框提取Haar<citation id="239" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、方向梯度直方图(Histogram of Oriented Gradient,HOG)<citation id="240" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、局部二值模式(Local Binary Pattern, LBP)<citation id="241" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等特征,并使用图像识别方法分类得到所有分类成功的区域;最后,通过非极大值抑制将结果输出。但这种方法不仅复杂度高、鲁棒性差,而且产生了大量的候选区冗余区域。</p>
                </div>
                <div class="p1">
                    <p id="84">2014年,Girshick等<citation id="242" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>设计了基于区域的卷积神经网络(Region with Convolutional Neural Network, R-CNN),使用候选区域(Region Proposal)和分类卷积神经网络训练模型用于检测。这使得目标检测与识别技术取得了巨大突破,并掀起了基于深度学习的目标检测与识别的热潮。R-CNN算法遵循了传统目标检测的思路,同样采用提取框、对每个框提取特征、图像分类、非极大值抑制4个步骤进行目标检测,只不过在提取特征这一步,将传统的特征(如尺度不变特征变换(Scale-Invariant Feature Transform, SIFT)<citation id="247" type="reference"><link href="181" rel="bibliography" /><link href="183" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>、HOG特征<citation id="248" type="reference"><link href="185" rel="bibliography" /><link href="187" rel="bibliography" /><link href="189" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>等)换成了深度卷积网络提取的特征。快速区域卷积神经网络(Fast Region-based Convolutional Neural Network, Fast R-CNN)<citation id="243" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、更快速的区域卷积神经网络(Faster Region-based Convolutional Neural Network, Faster R-CNN)<citation id="244" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、Mask R-CNN<citation id="245" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等深度网络框架都是建立在R-CNN的基础之上,以及单阶段检测器算法YOLO(You Only Look Once)<citation id="249" type="reference"><link href="197" rel="bibliography" /><link href="199" rel="bibliography" /><link href="201" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>、SSD(Single Shot MultiBoxDetector)<citation id="246" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>,均获得了更多研究者的追捧<citation id="250" type="reference"><link href="205" rel="bibliography" /><link href="207" rel="bibliography" /><link href="209" rel="bibliography" /><link href="211" rel="bibliography" /><link href="213" rel="bibliography" /><link href="215" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="85">随着深度网络框架的逐步成熟,对于行人检测技术的要求也进一步地提升。而对于在复杂场景下或者目标较远的行人图像,行人检测的检测问题依旧存在,并且对于检测精度和检测速度有着更高的要求。因此考虑到使用细粒度图像的技术,细粒度图像识别被认为是一个具有挑战性的计算机视觉问题,由于高度相似的从属类别引起的小的类间变化,以及姿势、尺度和旋转的大的类内变化。细粒度识别任务,如识别鸟类<citation id="251" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、花<citation id="252" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>和汽车<citation id="253" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>等,在计算机视觉和模式识别的应用中很受欢迎。细粒度识别更有利于学习目标的关键部分,这有助于区分不同子类的对象并匹配相同子类的对象<citation id="254" type="reference"><link href="223" rel="bibliography" /><link href="225" rel="bibliography" /><link href="227" rel="bibliography" /><link href="229" rel="bibliography" /><link href="231" rel="bibliography" /><sup>[<a class="sup">26</a>,<a class="sup">27</a>,<a class="sup">28</a>,<a class="sup">29</a>,<a class="sup">30</a>]</sup></citation>,可以更加准确地学习行人的特征。</p>
                </div>
                <div class="p1">
                    <p id="86">因此,本文采用基于深度学习的目标检测中领先的研究成果,提出将Mask R-CNN结构用于行人检测,主要工作包含以下几个部分:</p>
                </div>
                <div class="p1">
                    <p id="87">1)数据集选取与制作阶段,在已标注好的数据集上,采用水平翻转及加噪的方式对数据集进行扩充,实现数据增强。</p>
                </div>
                <div class="p1">
                    <p id="88">2)数据训练阶段,采用<i>K</i>-means算法对数据进行聚类获得合适的anchors的长宽比,并采用全卷积网络(Fully Convolutional Network, FCN)构建部位分割模型,分别提取行人的上半身、下半身和整体的特征,将这些特征信息融合完成行人的检测。</p>
                </div>
                <div class="p1">
                    <p id="89">3)模型评估阶段,从检测精度、检测速度和误检率3个指标对本文的模型进行评估,并与当前主流的目标检测算法进行实验对比,验证本文算法的可行性和有效性。</p>
                </div>
                <h3 id="90" name="90" class="anchor-tag">1 网络框架</h3>
                <h4 class="anchor-tag" id="91" name="91">1.1 Mask R-CNN<b>算法</b></h4>
                <div class="p1">
                    <p id="92">本文采用Mask R-CNN算法实现对图像中行人的检测,网络结构如图1所示。首先对输入(input)任意尺寸大小的行人图像进行卷积特征提取构成特征图(feature map),之后在区域生成网络(Region Proposal Network, RPN)中,使得区域生成(proposals)、分类(category)、回归(bbox regression)共用卷积层,加快计算速度。与之并行的特征金字塔网络(Feature Pyramid Network, FPN)在实现行人检测的同时把行人目标的像素分割出来,并给出行人在图片中的位置坐标(coordinates)。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911018_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Mask R-CNN结构" src="Detail/GetImg?filename=images/JSJY201911018_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Mask R-CNN结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911018_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of Mask R-CNN</p>

                </div>
                <div class="p1">
                    <p id="94">Mask R-CNN算法采用两阶段检测方法。第一阶段是生成目标候选区域,提出候选对象边界框(与Faster R-CNN算法相同);在第二阶段, Mask R-CNN为每个感兴趣区域(Region of Interest, RoI)输出二进制掩码,与预测类和边界框偏移并行,其中分类取决于掩码预测(例如文献<citation id="255" type="reference">[<a class="sup">31</a>,<a class="sup">32</a>]</citation>)。在训练期间,Mask R-CNN算法为每个采样的RoI上的多任务损失函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="95"><i>L</i>=<i>L</i><sub>cls</sub>+<i>L</i><sub>box</sub>+<i>L</i><sub>mask</sub>      (1)</p>
                </div>
                <div class="p1">
                    <p id="96">其中:<i>L</i><sub>cls</sub>表示分类损失,<i>L</i><sub>box</sub>表示边界框损失,<i>L</i><sub>mask</sub>表示分割损失。</p>
                </div>
                <div class="p1">
                    <p id="97">Mask R-CNN算法提出了一个RoIAlign层,采用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,避免对RoI边界或区间进行任何量化(例如,使用<i>x</i>/16而不是[<i>x</i>/16]),从而将整个特征聚集过程转化为一个连续的操作。在具体的算法操作上,RoIAlign并不是简单地补充出候选区域边界上的坐标点进行池化,而是通过:1)遍历每一个候选区域,保持浮点数边界不做量化;2)将候选区域分割成<i>K</i>×<i>K</i>个单元,每个单元的边界也不做量化;3)在每个单元中计算固定四个坐标位置,用双线性内插的方法计算出这四个位置的值,然后进行最大池化操作。RoIAlign是在Mask R-CNN中区域特征聚集方式,可以很好地解决RoI池化操作中两次量化造成的区域不匹配(mis-alignment)的问题,进而提升检测模型的准确性。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">1.2 <b>改进的</b>Mask R-CNN<b>算法</b></h4>
                <div class="p1">
                    <p id="99">本文在原有Mask R-CNN检测框架的基础上,做了一些改进,网络框架如图2所示。同样为两个部分:第一部分是生成候选区域;第二部分是学习全局和局部图像块的特征,主要是借助FCN学习部位分割模型(part-based segmentation model),具体见1.2.3节内容。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911018_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 改进的Mask R-CNN框架" src="Detail/GetImg?filename=images/JSJY201911018_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 改进的Mask R-CNN框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911018_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Framework of improved Mask R-CNN</p>

                </div>
                <h4 class="anchor-tag" id="101" name="101">1.2.1 区域生成网络</h4>
                <div class="p1">
                    <p id="102">RPN是在最后一层特征图上进行特征提取,采用滑动窗口的方式扫描整张图像,寻找存在的目标区域(anchor)。对于图像上的每一个位置考虑9个可能的候选窗口:3种尺度(128<sup>2</sup>,256<sup>2</sup>,512<sup>2</sup>)和3种长宽比(1∶1,1∶2,2∶1)。在不同的尺寸大小和长宽比下,在该图像上会产生将近20 000个目标区域,并且这些区域相互重叠,尽可能地覆盖在整张图像上。RPN为每个anchor生成两个输出,即anchor类别和边框调整。对于互相重叠的多个anchor,采用非极大值抑制给出目标的粗略结果,保留拥有最高前景分数的anchor,因此,使用RPN预测可以选出最好的包含目标的anchor,并应用边框进行精调。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">1.2.2 特征金字塔网络</h4>
                <div class="p1">
                    <p id="104">由于RPN是在得到的最后一层特征图上进行特征提取,在顶层的特征中不能完整地反映目标的信息。因此,结合多层级的特征可以大幅提高多尺度检测的准确性。FPN主要解决目标检测的多尺度问题,通过简单的网络连接改变,在基本不增加原有模型计算量的情况下,可以大幅度提升目标的检测性能。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">1.2.3 图像分割</h4>
                <div class="p1">
                    <p id="106">本文使用FCN用于图像的分割,FCN可以采用任何分辨率的输入图像,并产生相同大小的输出。FCN不仅在细粒度图像中定位目标,而且还将分割预测视为目标掩码。对于有效的训练,所有训练和测试细粒度图像保持其原始图像的分辨率。</p>
                </div>
                <div class="p1">
                    <p id="107">FCN掩码学习过程如图3所示。首先将原始图片通过FCN进行像素预测,获得目标的局部掩码,如果像素预测为目标的局部位置(上半身、下半身),则保留掩码的实际值,即对行人进行细粒度检测;否则,如果像素指示区域是背景,则掩码中这些背景区域的值被重置为零值。对于图像中的每一个行人,均会学习到每个人的全局和局部特征,学习的FCN模型也能够返回更准确的目标掩码。同时,这些目标掩码还可以通过找到它们的包围矩形来定位目标位置。本文实验中,采用FCN- 8<citation id="256" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>来学习和预测目标掩码。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911018_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 FCN掩码学习过程" src="Detail/GetImg?filename=images/JSJY201911018_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 FCN掩码学习过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911018_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 FCN mask learning process</p>

                </div>
                <div class="p1">
                    <p id="109">特征学习如图4所示。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 特征学习" src="Detail/GetImg?filename=images/JSJY201911018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 特征学习  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Feature learning</p>

                </div>
                <div class="p1">
                    <p id="111">图4中三个流分别对应行人的整体、上半身和下半身图像块,通过卷积、激活、池化、判别器选择等一系列操作,分别学习这三个图像块的特征。为提高图像中行人的检测精度,让不同细粒度的特征参与行人检测,因此,本文结合不同细粒度图像特征,可以增强行人检测的鲁棒性。</p>
                </div>
                <h3 id="112" name="112" class="anchor-tag">2 网络训练</h3>
                <div class="p1">
                    <p id="113">本文采用改进的Mask R-CNN结构为模型,训练行人检测器。为加快训练速度并防止过拟合,在训练期间需设置合理的参数,具体参数设置如表1所示。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表</b>1 <b>模型参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Model parameter settings</p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td><br />参数</td><td>描述</td><td>值</td></tr><tr><td><br />momentum</td><td>冲量常数</td><td>0.9</td></tr><tr><td><br />decay</td><td>权值衰减系数</td><td>0.000 5</td></tr><tr><td><br />batchsize</td><td>批量大小</td><td>64</td></tr><tr><td><br />lr</td><td>学习率</td><td>0.000 1</td></tr><tr><td><br />iteration</td><td>迭代次数</td><td>50 000</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">2.1 <b>数据集选取与制作</b></h4>
                <div class="p1">
                    <p id="116">基于深度学习的行人检测方法需要大量的数据集, 因此,本文从最具典型的COCO2014数据集中选取具有代表性的图像,主要包括复杂场景下、行人密集、光照变化明显等难检测的行人图像1 500张,以及2018年江苏省研究生计算机视觉创新实践大赛官方给出的205张行人图像。使用labelme软件完成数据集的标注工作,主要标注行人上半身、下半身和全身的标签信息。其中训练集包含正样本图像1 455张,行人数目为4 368个;测试集包含正样本图像250张,行人数目为756个。</p>
                </div>
                <div class="p1">
                    <p id="117">针对不同的数据集及不同大小的目标,修改anchor的大小和数量,可以加快收敛速度,提高检测精度。考虑到2018年江苏省研究生计算机视觉创新实践大赛官方给出的数据集中行人姿势、动作的特点,采用1.2.1节中的3种尺度和3种长宽比并不合理,因此本文增加1组长宽比,此时anchor对应长宽比为(1∶1,1∶2,2∶1,2∶5),尺度不变。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">2.2 <b>预训练</b></h4>
                <div class="p1">
                    <p id="119">为减少训练时间,采用MS-COCO预训练模型进行训练。在COCO2014数据集上训练20个循环(epoch)后得预训练参数。选择了ResNet50网络作为特征提取网络,需要检测的物体只有行人,再加上背景则一共有两类。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">2.3 <b>聚类选取初始候选框</b></h4>
                <div class="p1">
                    <p id="121">在网络训练阶段,随着迭代次数的不断增加,网络学习到行人的全局特征,预测框的参数不断调整,最终接近真实框。为了加快收敛速度,提高行人检测的位置精度,本文通过分析图像中行人宽高的特点,采用<i>K</i>-means算法进行聚类,本文的<i>K</i>-means聚类算法采用欧氏距离来衡量数据对象间的距离,其中<i>K</i>-means聚类算法通过给定bounding boxes的anchors数量的中心位置,计算两者之间的欧氏距离,选取距离真实框最近的一个anchor。重复这样的操作,直至满足所给定的anchors数量。最终确定anchor的长宽比为(1∶1,1∶2,2∶1,2∶5)。</p>
                </div>
                <div class="p1">
                    <p id="122">预测框和真实框的交并比 (Intersection Over Union,IOU)是反映预测框与真实框差异的重要指标,<i>IOU</i>值越大,则(1-<i>IOU</i>)的值就越小,这表明两者差异越小,“距离”越近。聚类的目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>Ν</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>Μ</mi></munder><mo stretchy="false">(</mo></mstyle></mrow></mstyle><mn>1</mn><mo>-</mo><mi>Ι</mi><mi>Ο</mi><mi>U</mi><mo stretchy="false">(</mo><mi>B</mi><mi>o</mi><mi>x</mi><mo stretchy="false">[</mo><mi>Ν</mi><mo stretchy="false">]</mo><mo>,</mo><mi>Τ</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo stretchy="false">[</mo><mi>Μ</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">其中:<i>N</i>表示聚类的类别,<i>M</i>表示聚类的样本集,<i>Box</i>[<i>N</i>]表示聚类得到预测框的宽高,<i>Truth</i>[<i>M</i>]表示真实框的宽高。</p>
                </div>
                <h3 id="125" name="125" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="126">本文实验环境为:ubuntu18.04,64位操作系统,深度学习框架为TensorFlow,1个GPU,代码运行环境为Python3.6.3。对于行人图像目标检测,本文采用检测精度(Average Precision,AP)、误检率(False Positive Rate,FRP)、检测速度(Detection Rate,DR)3个指标,其中DR表示每张图片的检测时间,单位:秒。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.1 <b>实验可视化结果</b></h4>
                <div class="p1">
                    <p id="128">本文实验的可视化结果如图5所示。从数据集中选取了2张典型的图像,包括光照变化明显、行人密集、姿势复杂的图像,用矩形框正确标记出图像中的行人,并利用实例分割将每个行人作为一个实例显现地标记出来。</p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911018_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 可视化效果" src="Detail/GetImg?filename=images/JSJY201911018_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 可视化效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911018_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Visualization effects</p>

                </div>
                <h4 class="anchor-tag" id="130" name="130">3.2 <b>数据增强对比实验</b></h4>
                <div class="p1">
                    <p id="131">由于深度学习需要大量的数据量,为了增加训练集的数据量,本文对现有的数据集采用水平翻转、加噪等方式对数据进行扩充,如图6所示。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911018_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 图像增强示例" src="Detail/GetImg?filename=images/JSJY201911018_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 图像增强示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911018_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Image enhancement example</p>

                </div>
                <div class="p1">
                    <p id="133">根据表2的实验可知,当训练集从1 455张行人图像扩充至5 820张行人图像后,目标的检测精度提高了9.58%,误检率降低了2.64%。因此,对数据集进行合理的扩充,有利于网络充分学习行人图像的特征,提高目标的检测性能。接下来的实验均是在数据集扩充的基础上进行。</p>
                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表</b>2 <b>数据增强实验对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Experimental comparison of data enhancement</p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td><br />图像数量</td><td>AP/%</td><td>FPR/%</td><td>DR/s</td></tr><tr><td><br />1 455</td><td>74.81</td><td>4.42</td><td>0.153</td></tr><tr><td><br />5 820</td><td>84.39</td><td>1.78</td><td>0.153</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">3.3 <b>采用不同策略训练网络的检测结果对比</b></h4>
                <div class="p1">
                    <p id="136">为了验证本文所提出方法的有效性,分别采用不同策略对网络进行训练与测试,具体检测结果如表3所示。原始Mask R-CNN算法中anchors的个数为9,即3种长宽比(1∶1,1∶2,2∶1)和3种尺度(128<sup>2</sup>,256<sup>2</sup>,512<sup>2</sup>)。本文根据行人数据集的特点采用<i>K</i>-means聚类的方法获得适合本实验数据的长宽比为(1∶1,1∶2,2∶1,2∶5),最终将anchor数量从9增加至12。如表3所示,对比两种策略实验结果可发现:选用合适的anchors的尺度和长宽比,可使平均检测精度提升6.53个百分点,误检率降低2.15个百分点。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表</b>3 <b>不同</b>anchors<b>实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Experimental results of different anchors</p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />anchors</td><td>AP/%</td><td>FPR/%</td><td>DR/s</td></tr><tr><td><br />9</td><td>77.86</td><td>3.93</td><td>0.144</td></tr><tr><td><br />12</td><td>84.39</td><td>1.78</td><td>0.153</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="138" name="138">3.4 <b>消融实验</b></h4>
                <div class="p1">
                    <p id="139">为了分析目标的每个部分对于模型的贡献度,本文对数据集进行了消融实验。将在原始整体特征学习的模型的基础上只加入上半身特征学习的模型表示为part- 1,只加入下半身特征学习的模型表示为part- 2。实验结果如表4所示,根据表4可知,本文只加入上半身特征学习后检测精度提高了2.27个百分点,只加入下半身特征学习后检测精度提高了0.76 个百分点。二者特征学习均加入之后的检测精度整体上提高了3.24个百分点。</p>
                </div>
                <div class="area_img" id="140">
                                            <p class="img_tit">
                                                <b>表</b>4 <b>检测模型各部分的贡献度</b>
                                                    <br />
                                                Tab. 4 Contribution of each part of the detection model
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911018_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201911018_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911018_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 检测模型各部分的贡献度" src="Detail/GetImg?filename=images/JSJY201911018_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h4 class="anchor-tag" id="141" name="141">3.5 <b>不同目标检测算法结果对比</b></h4>
                <div class="p1">
                    <p id="142">本文将改进后的算法(即本文算法)同具有代表性的目标检测算法进行比较,包括单阶段检测器(回归系列算法)中的YOLOv2算法、YOLOv3算法和SSD算法;两阶段检测器(区域建议系列算法)中的R-FCN算法、Faster R-CNN算法和Mask R-CNN算法。其中单阶段检测器将目标检测视作单个回归问题,网络结构简单,直接给出最终的检测结果,其检测速度较快,但准确率较低;两阶段检测器首先生成可能包含物体的候选区域,之后对候选区域作进一步的分类和校准,得到最终的检测结果,其准确率较高,但检测速度较慢。</p>
                </div>
                <div class="p1">
                    <p id="143">实验对比结果如表5所示,对比几种目标检测算法的实验结果可发现:本文在Mask R-CNN基础上考虑细粒度图像的特点,增加anchors的个数,在提高检测精度的同时提高了检测速度。本文算法较回归算法中YOLOv2算法的检测精度提高了9.06个百分点,误检率降低了1.09个百分点;较区域建议系列算法中Faster R-CNN算法的检测精度提高了1.90个百分点,误检率降低了0.49个百分点;较Mask R-CNN算法的检测精度提高了3.24个百分点,且误检率降低了0.55个百分点,而检测速度略低于单阶段检测器算法,约0.1个百分点;但相较区域建议系列算法有了明显的提高。</p>
                </div>
                <div class="area_img" id="144">
                    <p class="img_tit"><b>表</b>5 <b>不同目标检测算法实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Experimental results of different object detection algorithms</p>
                    <p class="img_note"></p>
                    <table id="144" border="1"><tr><td><br />算法</td><td>AP/%</td><td>FPR/%</td><td>DR/s</td></tr><tr><td><br />YOLOv2</td><td>75.33</td><td>2.87</td><td>0.041</td></tr><tr><td><br />YOLOv3</td><td>72.16</td><td>2.93</td><td>0.051</td></tr><tr><td><br />SSD</td><td>64.30</td><td>4.37</td><td>0.059</td></tr><tr><td><br />R-FCN</td><td>51.98</td><td>5.32</td><td>0.085</td></tr><tr><td><br />Faster R-CNN</td><td>82.49</td><td>2.27</td><td>0.416</td></tr><tr><td><br />Mask R-CNN</td><td>81.15</td><td>2.33</td><td>0.267</td></tr><tr><td><br />本文方法</td><td>84.39</td><td>1.78</td><td>0.153</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="145" name="145">3.6 <b>不同数据集实验结果</b></h4>
                <div class="p1">
                    <p id="146">为了验证本文算法的普适性,将训练出来的模型分别在INRIA数据集、COCO2017数据集上进行测试。</p>
                </div>
                <h4 class="anchor-tag" id="147" name="147">3.6.1 INRIA数据集实验结果</h4>
                <div class="p1">
                    <p id="148">数据集INRIA的测试集有288张正样本(包含2 416个行人),453张负样本(包含1 126个行人)。在INRIA上的实验结果如表6所示,可以发现: 本文算法相较于其他的检测算法在检测精度仍占有一定的优势,检测精度比YOLOv3算法和Mask R-CNN算法分别提高了9.56个百分点、2.18个百分点;检测速度相较于Mask R-CNN算法仍提高了0.411个百分点,略低于单阶段检测器YOLOv2和YOLOv3算法的检测速度。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表</b>6 INRIA<b>数据集上实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 6 Experimental results on INRIA dataset</p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td><br />算法</td><td>AP/%</td><td>FPR/%</td><td>DR/s</td></tr><tr><td><br />YOLOv2</td><td>75.33</td><td>4.76</td><td>0.157</td></tr><tr><td><br />YOLOv3</td><td>63.12</td><td>4.48</td><td>0.285</td></tr><tr><td><br />Mask R-CNN</td><td>70.50</td><td>1.83</td><td>0.762</td></tr><tr><td><br />本文方法</td><td>72.68</td><td>2.65</td><td>0.351</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="150" name="150">3.6.2 COCO2017数据集实验结果</h4>
                <div class="p1">
                    <p id="151">COCO2017数据集的测试集包含40 670张图像,从中随机挑选200张图像进行测试。在挑选出来的200张图像上的实验结果如表7所示。可以发现:本文算法的检测精度较YOLOv2算法、YOLOv3算法、Mask R-CNN算法分别提高了11.54个百分点、7.61个百分点、5.48个百分点;检测速度与其他算法十分接近。</p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表</b>7 COCO2017<b>数据集上实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 7 Experimental results on COCO2017 dataset</p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td><br />算法</td><td>AP/%</td><td>FPR/%</td><td>DR/s</td></tr><tr><td><br />YOLOv2</td><td>65.33</td><td>5.43</td><td>0.063</td></tr><tr><td><br />YOLOv3</td><td>69.26</td><td>8.65</td><td>0.078</td></tr><tr><td><br />Mask R-CNN</td><td>71.39</td><td>1.83</td><td>0.197</td></tr><tr><td><br />本文方法</td><td>76.87</td><td>1.25</td><td>0.153</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="153" name="153" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="154">本文针对复杂场景下的行人图像进行深入研究,在初始Mask R-CNN框架的基础上,采用数据增强的方式对数据集进行扩充,针对数据集的特点采用<i>K</i>-means算法调整anchor数量和大小,结合ResNet50、FPN、FCN等架构来提升行人的检测能力,并结合了行人细粒度属性,有效地提高了行人的检测精度。由于行人检测与行人重识别、行人跟踪等领域密切相关,因此行人检测技术的提升有利于行人重识别、行人跟踪技术的提升。但本文对于行人的检测速度仍低于单阶段检测器的检测速度,因此,接下来对于检测速度的提升还有待研究。并且近年来,很多研究者致力于提取更多信息辅助检测(如光流信息、运动信息和环境信息等),提高特征表达能力,未来将对其进行更深一步的探讨。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="173">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A general framework for object detection">

                                <b>[1]</b> PAPAGEORGIOU C P,OREN M,POGGIO T.A general framework for object detection [C]// Proceedings of the 6th IEEE International Conference on Computer Vision.Piscatway:IEEE,1998:555-562.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">

                                <b>[2]</b> DALAL N,TRIGGS B.Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:886-893.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An HOG-LBP human detector w ith partial occlusion handling">

                                <b>[3]</b> WANG X Y,HAN T,YAN S C.An HOG-LBP human detector with partial occlusion handling [C]// Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway:IEEE,2009:32-39.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Region-Based Convolutional Networks for Accurate Object Detection and Segmentation">

                                <b>[4]</b> GIRSHICK R,DONAHUE J,DARRELL T,et al.Region-based convolutional networks for accurate object detection and segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(1):142-158.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object Recognition from Local Scale-Invariant Features">

                                <b>[5]</b> LOWE D G.Object recognition from local scale-invariant features [C]// Proceedings of the 1999 International Conference on Computer Vision.Piscataway:IEEE,1999:1150-1157.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjM5OTFsVXIzTUkxMD1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rmly&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> LOWE D G.Distinctive image features from scale-invariant keypoints [J].International Journal of Computer Vision,2004,60(2):91-110.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201510011&amp;v=MDU3NDdmWnVac0Z5bm5WN3pMUER6VGJMRzRIOVROcjQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> WANG S F,YAN J H,WANG Z G.Improved moving object detection algorithm based on local united feature [J].Chinese Journal of Scientific Instrument,2015,36(10):2241-2248.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rapid object detection using a boosted cascade of simple features">

                                <b>[8]</b> VIOLA P A,JONES M J.Rapid object detection using a boosted cascade of simple features [C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2001:511-518.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of Oriented Gradients for Human Detection">

                                <b>[9]</b> DALAL N,TRIGGS B.Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2005:886-893.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[10]</b> GIRSHICK R.Fast R-CNN [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway:IEEE,2015:1440-1448.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 REN S Q,HE K M,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[12]</b> HE K M,GKIOXARI G,GIRSHICK R,et al.Mask R-CNN [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2980-2988.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified Real-Time Object Detection">

                                <b>[13]</b> REDMON J,DIVVALA S K,GIRSHICK R,et al.You only look once:unified,real-time object detection [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:779-788.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">

                                <b>[14]</b> REDMON J,FARHADI A.YOLO9000:better,faster,stronger [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6517-6525.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 REDMON J,FARHADI A.YOLOv3:an incremental improvement [EB/OL].[2019- 03- 26].https://arxiv.org/pdf/1804.02767.pdf.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">

                                <b>[16]</b> LIU W,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector [C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:21-37.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJSF201804014&amp;v=MTEzODU3cWZadVpzRnlublY3ekxLeWZZYUxHNEg5bk1xNDlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 张中宝,王洪元,杨薇.基于Faster-RCNN的遥感图像飞机检测算法[J].南京师大学报(自然科学版),2018,41(4):79-86.(ZHANG Z B,WANG H Y,YANG W.Remote sensing image aircraft detection algorithm based on Faster RCNN [J].Journal of Nanjing Normal University (Natural Science Edition),2018,41(4):79-86.)
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on real-time vehicle detection algorithm based on deep learning">

                                <b>[18]</b> YANG W,ZHANG J,ZHANG Z B,et al.Research on real-time vehicle detection algorithm based on deep learning [C]// Proceedings of the 2018 Chinese Conference on Pattern Recognition and Computer Vision.Berlin:Springer,2018:126-127.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A vehicle real-time detection algorithm based on YOLOv2 framework">

                                <b>[19]</b> YANG W,ZHANG J,WANG H Y,et al.A vehicle real-time detection algorithm based on YOLOv2 framework [C]// Proceedings of the 2018 Real-Time Image and Video Processing.Bellingham,WA:SPIE,2018:106700N.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Buried object detection from B-scan ground penetrating radar data using Faster-RCNN">

                                <b>[20]</b> PHAM M T,LEFEVRE S.Buried object detection from B-Scan ground penetrating radar data using Faster-RCNN [C]// Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium.Piscataway:IEEE,2018:6804-6807.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES587D064B9347194FB191A70E0274BBBE&amp;v=MjE5Mjc4T1hqaTJSSTNmcmJtTjhqcUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhicTd3NnM9TmlmT2ZiYXdHYVhNcVlzM2JlZ0xDMzB3eTJCaDZ6Wg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> KIM J,BATCHULUUN G,PARK K.Pedestrian detection based on Faster R-CNN in nighttime by fusing deep convolutional features of successive images [J].Expert Systems with Applications,2018,114:15-33.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-class object detection from aerial images using Mask R-CNN">

                                <b>[22]</b> SCHWEITZER D,AGRAWAL R.Multi-class object detection from aerial images using Mask R-CNN [C]// Proceedings of the 2018 IEEE International Conference on Big Data.Piscataway:IEEE,2018:3470-3477.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES96B207EC1C99A62A53D84860E84EC7DB&amp;v=MjE2NDFRMC96V2NXNlV0MVRIZmtyR2M5ZmNmblFzN3RDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4YnE3dzZzPU5pZk9mYnErYk5QTXFQbzJaWmdHQg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> WEI X,XIE C,WU J.Mask-CNN:localizing parts and selecting descriptors for fine-grained bird species categorization [J].Pattern Recognition,2018,76:704-714.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image segmentation for large-scale subcategory flower recognition">

                                <b>[24]</b> ANGELOVA A,ZHU S H,LIN Y Q.Image segmentation for large-scale subcategory flower recognition [C]// Proceedings of the 2013 IEEE Workshop on Applications of Computer Vision.Piscataway:IEEE,2013:39-45.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3d object representations for fine-grained categorization">

                                <b>[25]</b> KRAUSE J,STARK M,DENG J,et al.3D object representations for fine-grained categorization [C]// Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops.Washington,DC:IEEE Computer Society,2013:554-561.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Part-Stacked CNN for Fine-Grained Visual Categorization">

                                <b>[26]</b> HUANG S,XU Z,TAO D,et al.Part-stacked CNN for fine-grained visual categorization [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1173-1182.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep lac:Deep localization, alignment and classification for fine-grained recognition">

                                <b>[27]</b> LIN D,SHEN Y,LU C,et al.Deep LAC:deep localization,alignment and classification for fine-grained recognition [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1666-1674.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weakly supervised finegrained categorization with part-based image representation">

                                <b>[28]</b> ZHANG Y,WEI X,WU J,et al.Weakly supervised fine-grained categorization with part-based image representation [J].IEEE Transactions on Image Processing,2016,25(4):1713-1725.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB898C212027A896401D52C6286BC38D8&amp;v=Mjk3MTE3dzZzPU5pZk9mY0d3RjltL3JZNUhaT2tJZlhRd3lSSVQ2MHQ0U2d6a3Job3pDOEdYVGM2WENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhicQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b> XIE G,ZHANG X,YANG W,et al.LG-CNN:from local parts to global discrimination for fine-grained recognition [J].Pattern Recognition,2017,71:118-131.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_30" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC1122BF934EB79A212805AF5E44662F1&amp;v=MjQyMzRjeGZiU1NSOHllQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeGJxN3c2cz1OaWZPZmNDNUg5UE8zZmxNWis5NmZuc3d2aFFTNkRkOVRRNlVxVw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[30]</b> LEE S,CHAN C,MAYO S J,et al.How deep learning extracts and learns leaf features for plant classification [J].Pattern Recognition,2017,71:1-13.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Instance-aware Semantic Segmentation via Multi-task Network Cascades">

                                <b>[31]</b> DAI J,HE K,SUN J.Instance-aware semantic segmentation via multi-task network cascades [C]// Proceedings of the 29th IEEE Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:3150-3158.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional instance-aware semantic segmentation">

                                <b>[32]</b> LI Y,QI H Z,DAI J,et al.Fully convolutional instance-aware semantic segmentation [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4438-4446.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_33" >
                                    <b>[33]</b>
                                 LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:3431-3440.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911018&amp;v=MjkzNDBMejdCZDdHNEg5ak5ybzlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublY3ekw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
