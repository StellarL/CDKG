<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136445079190000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911017%26RESULT%3d1%26SIGN%3dpe%252fe12bvnEICu6DsqG02lEygV2s%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911017&amp;v=MjY3NjBMejdCZDdHNEg5ak5ybzlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublY3N0s=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="1 多层次结构生成对抗网络 ">1 多层次结构生成对抗网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="1.1 &lt;b&gt;生成对抗网络&lt;/b&gt;">1.1 <b>生成对抗网络</b></a></li>
                                                <li><a href="#60" data-title="1.2 &lt;b&gt;多层次结构生成对抗网络模型&lt;/b&gt;">1.2 <b>多层次结构生成对抗网络模型</b></a></li>
                                                <li><a href="#85" data-title="1.3 &lt;b&gt;层次结构编码&lt;/b&gt;">1.3 <b>层次结构编码</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="2 实验及结果分析 ">2 实验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="2.1 &lt;b&gt;实验环境和数据集&lt;/b&gt;">2.1 <b>实验环境和数据集</b></a></li>
                                                <li><a href="#102" data-title="2.2 &lt;b&gt;评价标准&lt;/b&gt;">2.2 <b>评价标准</b></a></li>
                                                <li><a href="#110" data-title="2.3 &lt;b&gt;实验结果&lt;/b&gt;">2.3 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="图1 多层次结构生成对抗网络模型">图1 多层次结构生成对抗网络模型</a></li>
                                                <li><a href="#79" data-title="图2 判别器结构">图2 判别器结构</a></li>
                                                <li><a href="#98" data-title="图3 层次结构编码模型结构">图3 层次结构编码模型结构</a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;在&lt;/b&gt;&lt;i&gt;CUB&lt;/i&gt;&lt;b&gt;数据集上每个层次的&lt;/b&gt;&lt;i&gt;Inception score&lt;/i&gt;&lt;b&gt;对比&lt;/b&gt;"><b>表</b>1 <b>在</b><i>CUB</i><b>数据集上每个层次的</b><i>Inception score</i><b>对比</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;五种&lt;/b&gt;&lt;i&gt;GAN&lt;/i&gt;&lt;b&gt;模型在&lt;/b&gt;&lt;i&gt;CUB&lt;/i&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;Oxford&lt;/i&gt;-102&lt;b&gt;数据集上的&lt;/b&gt;&lt;i&gt;Inception score&lt;/i&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;Human rank&lt;/i&gt;&lt;b&gt;评分&lt;/b&gt;"><b>表</b>2 <b>五种</b><i>GAN</i><b>模型在</b><i>CUB</i><b>和</b><i>Oxford</i>-102<b>数据集上的</b><i>Inception score</i><b>和</b><i>Human rank</i><b>评分</b></a></li>
                                                <li><a href="#116" data-title="图4 四种&lt;i&gt;GAN&lt;/i&gt;模型在&lt;i&gt;CUB&lt;/i&gt;数据集上的生成结果">图4 四种<i>GAN</i>模型在<i>CUB</i>数据集上的生成结果</a></li>
                                                <li><a href="#117" data-title="图5 四种&lt;i&gt;GAN&lt;/i&gt;模型在&lt;i&gt;CUB&lt;/i&gt;数据集上的 生成结果的细节(喙、翅膀)对比">图5 四种<i>GAN</i>模型在<i>CUB</i>数据集上的 生成结果的细节(喙、翅膀)对比</a></li>
                                                <li><a href="#119" data-title="图6 三种&lt;i&gt;GAN&lt;/i&gt;模型在&lt;i&gt;Oxford&lt;/i&gt;-102数据集上的生成结果">图6 三种<i>GAN</i>模型在<i>Oxford</i>-102数据集上的生成结果</a></li>
                                                <li><a href="#121" data-title="图7 三种&lt;i&gt;GAN&lt;/i&gt;模型在&lt;i&gt;Oxford&lt;/i&gt;-102数据集上的 生成结果的细节(花瓣)对比">图7 三种<i>GAN</i>模型在<i>Oxford</i>-102数据集上的 生成结果的细节(花瓣)对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="165">


                                    <a id="bibliography_1" title=" DOSOVITSKIY A,SPRINGENBERG J T,BROX T.Learning to generate chairs with convolutional neural networks[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015:1538-1546." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to generate chairs with convolutional neural networks">
                                        <b>[1]</b>
                                         DOSOVITSKIY A,SPRINGENBERG J T,BROX T.Learning to generate chairs with convolutional neural networks[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015:1538-1546.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_2" title=" EHSANI K,BAGHERINEZHAD H,REDMON J,et al.Who let the dogs out?modeling dog behavior from visual data[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:4051-4060." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Who let the dogs out?modeling dog behavior from visual data">
                                        <b>[2]</b>
                                         EHSANI K,BAGHERINEZHAD H,REDMON J,et al.Who let the dogs out?modeling dog behavior from visual data[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:4051-4060.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_3" title=" 郭雨潇,陈雷霆,董悦.单帧图像下的环境光遮蔽估计[J].计算机研究与发展,2019,56(2):385-393.(GUO Y X,CHEN L T,DONG Y.Inferring ambient occlusion from a single image[J].Journal of Computer Research and Development,2019,56(2):385-393.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902014&amp;v=MDU5MzJGeW5uVjc3Tkx5dlNkTEc0SDlqTXJZOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         郭雨潇,陈雷霆,董悦.单帧图像下的环境光遮蔽估计[J].计算机研究与发展,2019,56(2):385-393.(GUO Y X,CHEN L T,DONG Y.Inferring ambient occlusion from a single image[J].Journal of Computer Research and Development,2019,56(2):385-393.)
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_4" title=" 赵树阳,李建武.基于生成对抗网络的低秩图像生成方法[J].自动化学报,2018,44(5):829-839.(ZHAO S Y,LI J W.Generative adversarial network for generating low-rank images[J].Acta Automatica Sinica,2018,44(5):829-839.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805007&amp;v=MTEwNDViRzRIOW5NcW85Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WNzdOS0NMZlk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         赵树阳,李建武.基于生成对抗网络的低秩图像生成方法[J].自动化学报,2018,44(5):829-839.(ZHAO S Y,LI J W.Generative adversarial network for generating low-rank images[J].Acta Automatica Sinica,2018,44(5):829-839.)
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_5" title=" 何新宇,张晓龙.基于深度神经网络的肺炎图像识别模型[J].计算机应用,2019,39(6):1680-1684.(HE X Y,ZHANG X L.Pneumonia image recognition model based on deep neural network[J].Journal of Computer Applications,2019,39(6):1680-1684.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906022&amp;v=MTA3NTBqTXFZOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVjc3Tkx6N0JkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         何新宇,张晓龙.基于深度神经网络的肺炎图像识别模型[J].计算机应用,2019,39(6):1680-1684.(HE X Y,ZHANG X L.Pneumonia image recognition model based on deep neural network[J].Journal of Computer Applications,2019,39(6):1680-1684.)
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_6" title=" REMATAS K,KEMELMACHER-SHLIZERMAN I,CURLESS B,et al.Soccer on your tabletop[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:4738-4747." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Soccer on your tabletop">
                                        <b>[6]</b>
                                         REMATAS K,KEMELMACHER-SHLIZERMAN I,CURLESS B,et al.Soccer on your tabletop[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:4738-4747.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_7" title=" van DEN OORD A,KALCHBRENNER N,KAVUKCUOGLU K.Pixel recurrent neural networks[C]// Proceedings of the 33rd International Conference on Machine Learning.New York:ACM,2016:1747-1756." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pixel recurrent neural networks">
                                        <b>[7]</b>
                                         van DEN OORD A,KALCHBRENNER N,KAVUKCUOGLU K.Pixel recurrent neural networks[C]// Proceedings of the 33rd International Conference on Machine Learning.New York:ACM,2016:1747-1756.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_8" title=" GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Nets">
                                        <b>[8]</b>
                                         GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:2672-2680.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_9" title=" PAN Z,YU W,YI X,et al.Recent progress on Generative Adversarial Networks (GANs):a survey[J].IEEE Access,2019,7:36322-36333." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recent progress on Generative Adversarial Networks (GANs) a survey">
                                        <b>[9]</b>
                                         PAN Z,YU W,YI X,et al.Recent progress on Generative Adversarial Networks (GANs):a survey[J].IEEE Access,2019,7:36322-36333.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_10" title=" CAO Y J,JIA L L,CHEN Y X,et al.Recent advances of generative adversarial networks in computer vision[J].IEEE Access,2019,7:14985-15006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recent advances of generative adversarial networks in computer vision">
                                        <b>[10]</b>
                                         CAO Y J,JIA L L,CHEN Y X,et al.Recent advances of generative adversarial networks in computer vision[J].IEEE Access,2019,7:14985-15006.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_11" title=" WANG X,GUPTA A.Generative image modeling using style and structure adversarial networks[C]// Proceedings of the 2016 European Conference on Computer Vision.Cham:Springer,2016:318-335." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative image modeling using style and structure adversarial networks">
                                        <b>[11]</b>
                                         WANG X,GUPTA A.Generative image modeling using style and structure adversarial networks[C]// Proceedings of the 2016 European Conference on Computer Vision.Cham:Springer,2016:318-335.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_12" title=" DENTON E L,CHINTALA S,SZLAM A,et al.Deep generative image models using a Laplacian pyramid of adversarial networks[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.New York:ACM,2015:1486-1494." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks">
                                        <b>[12]</b>
                                         DENTON E L,CHINTALA S,SZLAM A,et al.Deep generative image models using a Laplacian pyramid of adversarial networks[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.New York:ACM,2015:1486-1494.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_13" title=" DURUGKAR I,GEMP I,MAHADEVAN S.Generative multi-adversarial networks[EB/OL].[2018- 06- 20].https://www.taodocs.com/p-110588603.html." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative multi-adversarial networks">
                                        <b>[13]</b>
                                         DURUGKAR I,GEMP I,MAHADEVAN S.Generative multi-adversarial networks[EB/OL].[2018- 06- 20].https://www.taodocs.com/p-110588603.html.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_14" title=" REED S,AKATA Z,YAN X,et al.Generative adversarial text-to-image synthesis[C]// Proceedings of the 33rd International Conference on Machine Learning.New York:ACM,2016:1060-1069." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial text to image synthesis">
                                        <b>[14]</b>
                                         REED S,AKATA Z,YAN X,et al.Generative adversarial text-to-image synthesis[C]// Proceedings of the 33rd International Conference on Machine Learning.New York:ACM,2016:1060-1069.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_15" title=" REED S,AKATA Z,MOHAN S,et al.Learning what and where to draw[C]// Proceedings of International Conference on Neural Information Processing Systems.New York:ACM,2016:217-225." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning what and where to draw">
                                        <b>[15]</b>
                                         REED S,AKATA Z,MOHAN S,et al.Learning what and where to draw[C]// Proceedings of International Conference on Neural Information Processing Systems.New York:ACM,2016:217-225.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_16" title=" ZHANG Z,XIE Y,YANG L.Photographic text-to-image synthesis with a hierarchically-nested adversarial network[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:6199-6208." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photographic text-to-image synthesis with a hierarchically-nested adversarial network">
                                        <b>[16]</b>
                                         ZHANG Z,XIE Y,YANG L.Photographic text-to-image synthesis with a hierarchically-nested adversarial network[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:6199-6208.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_17" title=" ZHANG H,XU T,LI H,et al.StackGAN++:realistic image synthesis with stacked generative adversarial networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2019,41(8):1947-1962." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=StackGAN++ realistic image synthesis with stacked generative adversarial networks">
                                        <b>[17]</b>
                                         ZHANG H,XU T,LI H,et al.StackGAN++:realistic image synthesis with stacked generative adversarial networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2019,41(8):1947-1962.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_18" title=" LU J,YANG J,BATRA D,et al.Hierarchical question-image co-attention for visual question answering[C]// Proceedings of the 30th International Conference on Neural Information Processing Systems.New York:ACM,2016:289-297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Question-Image Co-Attention for Visual Question Answering">
                                        <b>[18]</b>
                                         LU J,YANG J,BATRA D,et al.Hierarchical question-image co-attention for visual question answering[C]// Proceedings of the 30th International Conference on Neural Information Processing Systems.New York:ACM,2016:289-297.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_19" title=" HU B,LU Z,LI H,et al.Convolutional neural network architectures for matching natural language sentences[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.New York:ACM,2014:2042-2050." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">
                                        <b>[19]</b>
                                         HU B,LU Z,LI H,et al.Convolutional neural network architectures for matching natural language sentences[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.New York:ACM,2014:2042-2050.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_20" title=" ABADI M,BARHAM P,CHEN J,et al.TensorFlow:a system for large-scale machine learning[C]// Proceedings of the 2016 Conference on Operating Systems Design and Implementation.Piscataway:IEEE,2016:265-283." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:A System for Large-Scale Machine Learning">
                                        <b>[20]</b>
                                         ABADI M,BARHAM P,CHEN J,et al.TensorFlow:a system for large-scale machine learning[C]// Proceedings of the 2016 Conference on Operating Systems Design and Implementation.Piscataway:IEEE,2016:265-283.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_21" title=" WAH C,BRANSON S,WELINDER P,et al.The Caltech-UCSD Birds-200-2011 dataset:computation &amp;amp; neural systems technical report [R].Pasadena,CA,USA:California Institute of Technology,2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Caltech-UCSD Birds-200-2011 Dataset">
                                        <b>[21]</b>
                                         WAH C,BRANSON S,WELINDER P,et al.The Caltech-UCSD Birds-200-2011 dataset:computation &amp;amp; neural systems technical report [R].Pasadena,CA,USA:California Institute of Technology,2011.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_22" title=" NILSBACK M E,ZISSERMAN A.Automated flower classification over a large number of classes[C]// Proceedings of the 6th Indian Conference on Computer Vision,Graphics &amp;amp; Image Processing.Piscataway:IEEE,2008:722-729." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated Flower Classification over a Large Number of Classes">
                                        <b>[22]</b>
                                         NILSBACK M E,ZISSERMAN A.Automated flower classification over a large number of classes[C]// Proceedings of the 6th Indian Conference on Computer Vision,Graphics &amp;amp; Image Processing.Piscataway:IEEE,2008:722-729.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_23" title=" SALIMANS T,GOODFELLOW I J,ZAREMBA W,et al.Improved techniques for training GANs[C]// Proceedings of International Conference on Neural Information Processing Systems.New York:ACM,2016:2234-2242." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training GANs">
                                        <b>[23]</b>
                                         SALIMANS T,GOODFELLOW I J,ZAREMBA W,et al.Improved techniques for training GANs[C]// Proceedings of International Conference on Neural Information Processing Systems.New York:ACM,2016:2234-2242.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-27 12:49</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3204-3209 DOI:10.11772/j.issn.1001-9081.2019051077            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>多层次结构生成对抗网络的文本生成图像方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E9%92%B0&amp;code=38234289&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙钰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%9E%97%E7%87%95&amp;code=39694699&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李林燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E5%AD%90%E5%AF%92&amp;code=40990640&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶子寒</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E4%BC%8F%E5%8E%9F&amp;code=31281244&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡伏原</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A5%9A%E9%9B%AA%E5%B3%B0&amp;code=37558013&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">奚雪峰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0228583&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州科技大学电子与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E5%B8%82%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%9C%8D%E5%8A%A1%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0262405&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州市大数据与信息服务重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E7%BB%8F%E8%B4%B8%E8%81%8C%E4%B8%9A%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=1500763&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州经贸职业技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%8B%8F%E7%9C%81%E5%BB%BA%E7%AD%91%E6%99%BA%E6%85%A7%E8%8A%82%E8%83%BD%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江苏省建筑智慧节能重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E5%B8%82%E8%99%9A%E6%8B%9F%E7%8E%B0%E5%AE%9E%E6%99%BA%E8%83%BD%E4%BA%A4%E4%BA%92%E5%8F%8A%E5%BA%94%E7%94%A8%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州市虚拟现实智能交互及应用技术重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>近年来,生成对抗网络(GAN)在从文本描述到图像的生成中已经取得了显著成功,但仍然存在图像边缘模糊、局部纹理不清晰以及生成样本方差小等问题。针对上述不足,在叠加生成对抗网络模型(StackGAN++)基础上,提出了一种多层次结构生成对抗网络(MLGAN)模型,该网络模型由多个生成器和判别器以层次结构并列组成。首先,引入层次结构编码方法和词向量约束来改变网络中各层次生成器的条件向量,使图像的边缘细节和局部纹理更加清晰生动;然后,联合训练生成器和判别器,借助多个层次的生成图像分布共同逼近真实图像分布,使生成样本方差变大,增加生成样本的多样性;最后,从不同层次的生成器生成对应文本的不同尺度图像。实验结果表明,在CUB和Oxford-102数据集上MLGAN模型的Inception score分别达到了4.22和3.88,与StackGAN++相比,分别提高了4.45%和3.74%。MLGAN模型在解决生成图像的边缘模糊和局部纹理不清晰方面有了一定提升,其生成的图像更接近真实图像。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本生成图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多层次结构生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B1%82%E6%AC%A1%E5%9B%BE%E5%83%8F%E5%88%86%E5%B8%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多层次图像分布;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84%E7%BC%96%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">层次结构编码;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    孙钰(1995—),男,江苏靖江人,硕士研究生,CCF会员,主要研究方向:图像处理、深度学习、生成对抗网络;;
                                </span>
                                <span>
                                    李林燕(1983—),女,湖南岳阳人,高级工程师,硕士,主要研究方向:地理信息处理;;
                                </span>
                                <span>
                                    叶子寒(1996—),男,江西上饶人,CCF会员,主要研究方向:图像处理、深度学习、生成对抗网络;;
                                </span>
                                <span>
                                    *胡伏原(1978—),男,湖南岳阳人,教授,博士,CCF会员,主要研究方向:图像处理、模式识别、信息安全,电子邮箱,fuyuanhu@mail.usts.edu.cn;
                                </span>
                                <span>
                                    奚雪峰(1978—),男,江苏苏州人,副教授,博士,CCF会员,主要研究方向:自然语言处理、机器学习、大数据处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61876121,61472267);</span>
                                <span>江苏省重点研发计划项目(BE2017663);</span>
                                <span>苏州市科技发展计划项目(SZS201609);</span>
                                <span>江苏省研究生科研创新项目(KYCX18_2549);</span>
                    </p>
            </div>
                    <h1><b>Text-to-image synthesis method based on multi-level structure generative adversarial networks</b></h1>
                    <h2>
                    <span>SUN Yu</span>
                    <span>LI Linyan</span>
                    <span>YE Zihan</span>
                    <span>HU Fuyuan</span>
                    <span>XI Xuefeng</span>
            </h2>
                    <h2>
                    <span>College of Electronic and Information Engineering, Suzhou University of Science and Technology</span>
                    <span>Suzhou Key Laboratory for Big Data and Information Service</span>
                    <span>Suzhou Institute of Trade and Commerce</span>
                    <span>Jiangsu Key Laboratory of Intelligent Building Energy Efficiency</span>
                    <span>Virtual Reality Key Laboratory of Intelligent Interaction and Application Technology of Suzhou</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In recent years, the Generative Adversarial Network(GAN) has achieved remarkable success in text-to-image synthesis, but there are still problems such as edge blurring of images, unclear local textures, small sample variance. In view of the above shortcomings, based on Stack Generative Adversarial Network model(StackGAN++), a Multi-Level structure Generative Adversarial Networks(MLGAN) model was proposed, which is composed of multiple generators and discriminators in a hierarchical structure. Firstly, hierarchical structure coding method and word vector constraint were introduced to change the condition vector of generator of each level in the network, so that the edge details and local textures of the image were clearer and more vivid. Then, the generator and the discriminator were jointed by trained to approximate the real image distribution by using the generated image distribution of multiple levels, so that the variance of the generated sample became larger, and the diversity of the generated sample was increased. Finally, different scale images of the corresponding text were generated by generators of different levels. The experimental results show that the Inception scores of the MLGAN model reached 4.22 and 3.88 respectively on CUB and Oxford-102 datasets, which were respectively 4.45% and 3.74% higher than that of StackGAN++. The MLGAN model has improvement in solving edge blurring and unclear local textures of the generated image, and the image generated by the model is closer to the real image.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Network(GAN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Network(GAN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text-to-image%20synthesis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text-to-image synthesis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-Level%20structure%20Generative%20Adversarial%20Networks(MLGAN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-Level structure Generative Adversarial Networks(MLGAN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-level%20image%20distribution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-level image distribution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hierarchical%20coding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hierarchical coding;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SUN Yu, born in 1995, M. S. candidate. His research interests include image processing, deep learning, generative adversarial networks. ;
                                </span>
                                <span>
                                    LI Linyan, born in 1983, M. S., senior engineer. Her research interests include geographic information processing. ;
                                </span>
                                <span>
                                    YE Zihan, born in 1996. His research interests include image processing, deep learning, generative adversarial networks. ;
                                </span>
                                <span>
                                    HU Fuyuan, born in 1978, Ph. D., professor. His research interests include image processing, pattern recognition, information security. ;
                                </span>
                                <span>
                                    XI Xuefeng, born in 1978, Ph. D., associate professor. His research interests include natural language processing, machine learning, big data processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China(61876121,61472267);</span>
                                <span>the Primary Research&amp;Development Plan of Jiangsu Province(BE2017663);</span>
                                <span>the Foundation of Key Laboratory in Science and Technology Development Project of Suzhou(SZS201609);</span>
                                <span>the Graduate Research and Innovation Plan of Jiangsu Province(KYCX18_2549);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="50">生成图像建模是计算机视觉中的一个基本问题,在图像和视觉计算、图像和语言处理、信息安全、人机交互等领域已有广泛应用。最近几年,随着深度学习的发展,生成图像方法取得了显著进展:Dosovitskiy等<citation id="211" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>训练反卷积神经网络(Convolutional Neural Network, CNN)来生成3D椅子、桌子和汽车;Ehsani等<citation id="212" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>用视觉数据构建狗的行为模型;郭雨潇等<citation id="213" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出了一种基于单张图像的环境光遮蔽估计算法;赵树阳等<citation id="214" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出一种非监督式的由图像生成图像的低秩纹理生成对抗网络模型,用于生成低秩图像;何新宇等<citation id="215" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了一种基于深度卷积神经网络(Deep Convolutional Neural Network, DCNN)的肺炎图像识别模型用于肺炎图像的识别;Rematas等<citation id="216" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>使用足球比赛视频数据训练网络,从而提取3D网格信息,进行动态3D重建。此外,利用神经网络来模拟像素空间的条件分布的自回归模型(例如,像素递归神经网络(Pixel Recurrent Neural Network, PixelRNN))<citation id="217" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>也已经产生了清晰的合成图像。最近,生成对抗网络(Generative Adversarial Network, GAN)<citation id="218" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>已经显示出其具有强大的性能和潜力来生成更清晰和质量更高的样本图像。</p>
                </div>
                <div class="p1">
                    <p id="51">自Goodfellow等<citation id="219" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>于2014年提出生成对抗网络后,该网络模型得到了学术界和工业界的广泛关注。与传统机器学习方法不同,生成对抗网络最大的特点在于引入了对抗机制,能够利用少量标签数据和大量无标签数据建模,直接生成与目标数据一致的生成数据,如图像、视频以及音乐<citation id="220" type="reference"><link href="181" rel="bibliography" /><link href="183" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>等。在最初的设计中,GAN由一个生成器和一个判别器组成,生成器和判别器以相互交替的方式进行对抗训练。训练生成器以产生符合真实数据分布的样本来欺骗判别器,同时优化判别器用以区分真实样本和生成器产生的假样本。</p>
                </div>
                <div class="p1">
                    <p id="52">随着生成模型的不断提出,生成对抗网络已经在复杂的多模态数据建模和合成真实世界图像方面展示了其强大的效果: Wang等<citation id="221" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>利用提出的样式结构生成对抗网络(Style and Structure Generative Adversarial Networks, S2-GAN)模型以结构生成和样式生成两部分相结合的方法实现室内场景图像的生成; Denton等<citation id="222" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在拉普拉斯金字塔框架内建立了多个GAN模型,以前一层级的输出为条件生成残差图像,然后作为下一层级的输入,最后生成图像; Durugkar等<citation id="223" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>使用多个判别器和一个生成器来增加生成器接收的有效反馈,增强生成的图像效果; Reed等<citation id="224" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了GAN-INT-CLS模型,首次利用GAN有效地生成以文本描述为条件的64×64图像。然而,在许多情况下,他们合成的图像缺少逼真的细节和生动的物体部分,例如鸟的喙、眼睛和翅膀; 此外,他们无法合成更高分辨率的图像(例如128×128或256×256)。Reed等<citation id="225" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>为了更好地根据文本描述控制图像中物体的具体位置,提出了GAWWN(Generative Adversarial What-Where Network)模型,把额外的位置信息与文本一起作为约束条件加入到生成器和判别器的训练中。Zhang等<citation id="226" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>在网络层次结构中引入了层次嵌套对抗性目标,提出了高清晰生成对抗网络(High-Definition Generative Adversarial Network, HDGAN)模型,规范了中间层的表示,并帮助生成器捕获复杂的图像统计信息。Zhang等<citation id="227" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了一种叠加生成对抗网络(Stacked Generative Adversarial Network, StackGAN++)模型,把生成高质量图像的复杂问题分解成一些更好控制的子问题。在第一阶段利用文本描述粗略勾画物体的主要形状和颜色,生成低分辨率图像;在第二阶段,将第一阶段的结果和文本描述作为输入,生成256×256的高分辨率图像。然而,生成图像的局部纹理相对模糊。</p>
                </div>
                <div class="p1">
                    <p id="53">为了进一步增强生成图像的边缘和局部纹理细节,提高生成图像的质量,本文在叠加生成对抗网络模型的基础上提出了一种多层次结构生成对抗网络(Multi-Level structure Generative Adversarial Network, MLGAN)模型。该模型引入了层次结构编码(hierarchical coding)方法<citation id="228" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,对文本进行预处理,改变各层次生成器的条件变量,侧重关注生成图像的边缘和局部纹理。另外,基于多层次结构,网络联合训练生成器和判别器以近似多层次分布,在每一层生成器处捕获图像分布,根据多个图像分布共同逼近真实图像分布,提高生成样本的多样性。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag">1 多层次结构生成对抗网络</h3>
                <h4 class="anchor-tag" id="55" name="55">1.1 <b>生成对抗网络</b></h4>
                <div class="p1">
                    <p id="56">生成对抗网络采用了对抗的思想,其核心来自博弈论中的纳什均衡。对抗的双方分别由生成器<i>G</i>(Generator)和判别器<i>D</i>(Discriminator)组成,其中生成器的目的是尽量学习真实的数据分布,判别器<i>D</i>的目的在于尽量正确判断输入数据是来自真实数据还是生成器生成的数据。两个模型交替训练并且相互竞争。在训练中,优化生成器<i>G</i>捕捉样本数据的分布,用服从某一分布(均匀分布、高斯分布等)的噪声<i>z</i>生成一个类似真实训练样本的数据<i>P</i><sub>data</sub>;判别器<i>D</i>是一个二分类器,优化判别器<i>D</i>以估计一个样本来自于训练数据(而非生成数据)的概率,如果样本来自于真实的训练数据,<i>D</i>输出大概率,否则,<i>D</i>输出小概率。生成器<i>G</i>和判别器<i>D</i>交替训练的表达式为:</p>
                </div>
                <div class="p1">
                    <p id="57"><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>V</mi><mo stretchy="false">(</mo><mi>D</mi></mrow></math></mathml>;<i>G</i>)=<i>E</i><sub><i>x</i></sub><sub>～</sub><sub><i>P</i></sub><sub>data</sub>[log <i>D</i>(<i>X</i>)]+<i>E</i><sub><i><b>z</b></i></sub><sub>～</sub><sub><i>P</i></sub><sub><i><b>z</b></i></sub>[log(1-<i>D</i>(<i>G</i>(<i><b>z</b></i>)))]      (1)</p>
                </div>
                <div class="p1">
                    <p id="59">其中:<i>x</i>是来自真实数据分布<i>P</i><sub>data</sub>的真实图像,<i><b>z</b></i>是从分布<i>P</i><sub><i><b>z</b></i></sub>采样的噪声矢量(例如均匀或者高斯分布)。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.2 <b>多层次结构生成对抗网络模型</b></h4>
                <div class="p1">
                    <p id="61">为了进一步增强生成图像的边缘和局部纹理细节,提高样本多样性和生成图像的质量,本文提出了一种多层次结构生成对抗网络模型,该模型由多个生成器(<i>G</i><sub><i>s</i></sub>)和判别器(<i>D</i><sub><i>s</i></sub>)以层次结构并列组成。如图1所示,整个网络可以分为三个层次,在第一层次生成低分辨率图像,侧重关注图像整体的形状和颜色;在第二和第三层次生成高分辨率图像,借助整个网络联合训练,以近似多层次图像分布,提升图像的边缘和局部纹理细节。每个层次网络由上采样层、生成器、卷积层、残差层以及判别器组成。上采样层将输入的向量转换为不同层次对应的张量,通过生成器以及卷积层生成具有3×3卷积的对应比例图像。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911017_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 多层次结构生成对抗网络模型" src="Detail/GetImg?filename=images/JSJY201911017_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 多层次结构生成对抗网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911017_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Multi-level structure generative adversarial network model</p>

                </div>
                <h4 class="anchor-tag" id="63" name="63">1.2.1 多层次图像分布</h4>
                <div class="p1">
                    <p id="64">与叠加生成对抗网络不同的是,本文的网络具有多层次结构,采用噪声向量<i><b>z</b></i>～<i>P</i><sub>noice</sub>和不同的条件变量<i>c</i>作为生成器的输入。生成网络中包含3个生成器来生成不同分辨率的图像。<i>P</i><sub>noice</sub>是高斯分布,通常选择为标准正态分布。条件变量<i>c</i>由句子向量以及词向量组成。潜变量(<i><b>z</b></i>,<i>c</i><sub><i>i</i></sub>)逐层转换为隐藏特征<i>h</i><sub><i>i</i></sub>。本文首先通过非线性变换计算每个生成器<i>G</i><sub><i>i</i></sub>的隐藏特征<i>h</i><sub><i>i</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="65"><i>h</i><sub>0</sub>=<i>F</i><sub>0</sub>(<i><b>z</b></i>,<i>F</i>′(<i><b>c</b></i><sub>s</sub>))      (2)</p>
                </div>
                <div class="p1">
                    <p id="66"><i>h</i><sub><i>i</i></sub>=<i>F</i><sub><i>i</i></sub>(<i>h</i><sub><i>i</i></sub><sub>-1</sub>,<i>F</i>′(<i><b>c</b></i><sub>w</sub>,<i>h</i><sub><i>i</i></sub><sub>-1</sub>)); <i>i</i>=1,2,…,<i>m</i>-1      (3)</p>
                </div>
                <div class="p1">
                    <p id="67">其中:<i>h</i><sub><i>i</i></sub>代表第<i>i</i>分支的隐藏特征;<i>F</i>′为调节增强<citation id="229" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,将句子向量和词向量转化为条件向量;<i>m</i>是层次的总数;<i><b>c</b></i><sub>s</sub>为句子向量;<i><b>c</b></i><sub>w</sub>为词向量;<i>F</i><sub><i>i</i></sub>为神经网络模型(参见图1)。</p>
                </div>
                <div class="p1">
                    <p id="68">然后基于不同层次的隐藏特征(<i>h</i><sub>0</sub>,<i>h</i><sub>1</sub>,…,<i>h</i><sub><i>m</i></sub><sub>-1</sub>),将其作为对应层次的生成器(<i>G</i><sub>0</sub>,<i>G</i><sub>1</sub>,…,<i>G</i><sub><i>m</i></sub><sub>-1</sub>)的输入,生成不同分辨率的图像。如式(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>s</i><sub><i>i</i></sub>=<i>G</i><sub><i>i</i></sub>(<i>h</i><sub><i>i</i></sub>); <i>i</i>=0,1,…,<i>m</i>-1      (4)</p>
                </div>
                <div class="p1">
                    <p id="70">其中:<i>G</i><sub><i>i</i></sub>是第<i>i</i>分支的生成器,<i>s</i><sub><i>i</i></sub>是第<i>i</i>分支的生成图像。</p>
                </div>
                <div class="p1">
                    <p id="71">在每个生成器<i>G</i><sub><i>i</i></sub>生成样本之后,将真实的图像<i>x</i><sub><i>i</i></sub>和生成器生成的假样本<i>s</i><sub><i>i</i></sub>作为输入,输入到对应的判别器<i>D</i><sub><i>i</i></sub>中进行训练,通过最小化交叉熵损失(见式(5)),将输入的图像分为两大类,分别是真实图像和生成图像。</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mi>D</mi></msub><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>-</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">其中:<i>x</i><sub><i>i</i></sub>来自第<i>i</i>尺度的真实图像分布<i>P</i><sub><i>datai</i></sub>,<i>s</i><sub><i>i</i></sub>来自相同尺度的生成模型分布<i>P</i><sub><i>Gi</i></sub>。3个判别器并行训练,并且每个判别器都专注于自己对应的单个尺度图像。</p>
                </div>
                <div class="p1">
                    <p id="74">接着通过训练判别器,引导最小化损失函数(见式(6))来优化生成器,以共同逼近多尺度图像分布(<i>P</i><sub><i>data</i></sub><sub>0</sub>,<i>P</i><sub><i>data</i></sub><sub>1</sub>,…,<i>P</i><sub><i>datam</i></sub><sub>-1</sub>)。</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula"><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>G</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>L</mi></mstyle><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></math></mathml>;<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="76">其中<i>L</i><sub><i>Gi</i></sub>是近似第<i>i</i>尺度的图像分布的损失函数。在训练中,生成器<i>G</i><sub><i>i</i></sub>最大化log(<i>D</i><sub><i>i</i></sub>(<i>s</i><sub><i>i</i></sub>)),而不是最小化log(1-<i>D</i><sub><i>i</i></sub>(<i>s</i><sub><i>i</i></sub>)),这样可以减轻梯度消失问题<citation id="230" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>; 并且在训练过程中,生成器<i>G</i><sub><i>i</i></sub>和判别器<i>D</i><sub><i>i</i></sub>交替优化直到最终收敛。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">1.2.2 有条件和无条件图像生成</h4>
                <div class="p1">
                    <p id="78">如图2所示,对于无条件图像生成,判别器被训练成区分真实图像和伪造图像。对于条件图像生成,将图像及调节变量输入到判别器以确定图像与条件是否匹配,引导生成器近似条件图像分布。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911017_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 判别器结构" src="Detail/GetImg?filename=images/JSJY201911017_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 判别器结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911017_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Discriminator structure</i></p>

                </div>
                <div class="p1">
                    <p id="80">对于模型中的判别器<i>D</i><sub><i>i</i></sub>,现在由两部分组成,即无条件损失和条件损失。</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mi>D</mi></msub><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>-</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">无条件损失确定图像是真实还是伪造,而条件损失确定图像和条件是否匹配。 因此,每个生成器<i>G</i><sub><i>i</i></sub>的损失函数转换为:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>E</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>c</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">因此,每个层次的生成器<i>G</i><sub><i>i</i></sub>的最终损失函数通过把式(8)代入到式(6)来计算。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">1.3 <b>层次结构编码</b></h4>
                <div class="p1">
                    <p id="86">在已有的生成模型中,都是将整个文本描述转化成条件变量作为生成器的输入,这样虽然可以使生成的图像大致符合文本的描述,但是图像边缘和局部纹理细节比较模糊,同时还会产生很多不符合文本描述的样本。基于这个问题,本文引入了层次结构编码,更加注重生成图像的边缘和局部纹理细节。</p>
                </div>
                <div class="p1">
                    <p id="87">如图3所示,该层次结构编码模型包含4个层次,文本描述经过文本嵌入、卷积层、最大池化和<i>LSTM</i>(<i>Long Short</i>-<i>Term Memory</i>)特征提取,最终得到需要的短语向量和单词向量。在卷积层,使用三种大小的卷积核进行卷积;在池化层,对输出的三种卷积结果进行一次最大池化。</p>
                </div>
                <div class="p1">
                    <p id="88">层次结构编码的具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="89">1)首先给出单词的编码<i>S</i>={<i>q</i><sub>1</sub>,<i>q</i><sub>2</sub>,…,<i>q</i><sub><i>k</i></sub>},同时将文本描述映射到一个向量空间以得到<i>S</i><sup><i>w</i></sup>={<i>q</i><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>w</mi></msubsup></mrow></math></mathml>,<i>q</i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>w</mi></msubsup></mrow></math></mathml>,…,<i>q</i><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>w</mi></msubsup></mrow></math></mathml>}。</p>
                </div>
                <div class="p1">
                    <p id="90">2)然后利用1维DCNN(1-DCNN)作用于<i>S</i><sup><i>w</i></sup>,对其中每一个单词嵌入向量进行卷积操作,计算单词向量和卷积核的内积,使用三种大小的卷积核,以此来计算短语特征。对于第<i>k</i>个字,卷积后的输出为:</p>
                </div>
                <div class="p1">
                    <p id="91"><i>q</i><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mo>,</mo><mi>k</mi></mrow><mi>p</mi></msubsup></mrow></math></mathml>=tanh(<i>W</i><sup><i>s</i></sup><sub><i>c</i></sub><i>q</i><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>:</mo><mi>k</mi><mo>+</mo><mi>s</mi><mo>-</mo><mn>1</mn></mrow><mi>w</mi></msubsup></mrow></math></mathml>); <i>s</i>∈{1,2,3}      (9)</p>
                </div>
                <div class="p1">
                    <p id="92">其中:<i>W</i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>s</mi></msubsup></mrow></math></mathml>是重量参数,<i>s</i>为卷积核的大小。</p>
                </div>
                <div class="p1">
                    <p id="93">3)将特征<i>S</i><sup><i>w</i></sup>送入剩下两个卷积核进行卷积操作,在卷积之前需要适当地进行填充,以在卷积之后序列的长度保持不变。</p>
                </div>
                <div class="p1">
                    <p id="94">4)接着在3个卷积核卷积之后,本文对卷积的结果做一次最大池化,以获得短语的特征,结果如下:</p>
                </div>
                <div class="p1">
                    <p id="95"><i>q</i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>p</mi></msubsup></mrow></math></mathml>=max(<i>q</i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow><mi>p</mi></msubsup></mrow></math></mathml>,<i>q</i><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow><mi>p</mi></msubsup></mrow></math></mathml>,<i>q</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mi>k</mi></mrow><mi>p</mi></msubsup></mrow></math></mathml>); <i>k</i>∈{1,2,…,<i>K</i>}      (10)</p>
                </div>
                <div class="p1">
                    <p id="96">5)最后将得到的最大池化结果送入到LSTM中,使用LSTM对<i>q</i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>p</mi></msubsup></mrow></math></mathml>进行编码,提取特征。</p>
                </div>
                <div class="p1">
                    <p id="97">这样的合并方法与文献<citation id="231" type="reference">[<a class="sup">19</a>]</citation>方法不同之处在于它在每个时间步长自适应地选择不同的特征,同时保留原始序列长度和顺序。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911017_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 层次结构编码模型结构" src="Detail/GetImg?filename=images/JSJY201911017_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 层次结构编码模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911017_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Hierarchical coding model structure</p>

                </div>
                <h3 id="99" name="99" class="anchor-tag">2 实验及结果分析</h3>
                <h4 class="anchor-tag" id="100" name="100">2.1 <b>实验环境和数据集</b></h4>
                <div class="p1">
                    <p id="101">本文算法采用深度学习框架<i>Tensorflow</i><citation id="232" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>,实验环境为<i>ubantu</i>14.04操作系统,使用4块<i>NVIDIA</i> 1080<i>Ti</i>图形处理器(<i>GPU</i>)加速运算。同时,在<i>CUB</i><citation id="233" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和<i>Oxford</i>-102<citation id="234" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>数据集上训练所有模型。对于所有数据集,本文设置<i>Ng</i>=32,<i>Nd</i>=64并在每个生成器之间使用两个残差块,同时使用学习率为0.000 2的ADAM求解器。如表1所示,CUB数据集包含200种鸟类,共有11 788种图片。本文将8 855张图片作为训练数据集和2 933张图片作为测试数据集。由于该数据集中80%的鸟类图像的目标所占区域比例小于0.5<citation id="235" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>,所以在训练之前先对所有图像进行预处理,确保鸟类目标所占区域的比例大于图像尺寸的0.75。Oxford-102数据集包含102种花的类别,共有8 189种图像。本文将7 034张图片作为训练数据集和1 155张图片作为测试数据集。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">2.2 <b>评价标准</b></h4>
                <div class="p1">
                    <p id="103">对于<i>GAN</i>模型的评估通常都选用定性评估,即需要借助人工检验生成图像的视觉保真度来进行。这种方法耗时长,且主观性较强,具备一定的误导性。因此,本文主要使用2种评价标准对生成图像的质量和多样性进行评价。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">1)数值评估方法<i>Inception score</i><citation id="236" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>进行定量评估。</h4>
                <div class="p1">
                    <p id="105">数值评估方法表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="106"><i>I</i>=exp(<i>E</i><sub><i>x</i></sub><i>D</i><sub>KL</sub>(<i>p</i>(<i>y</i>|<i>x</i>)‖<i>p</i>(<i>y</i>)))      (11)</p>
                </div>
                <div class="p1">
                    <p id="107">其中:<i>x</i>表示一个生成的样本,<i>y</i>表示与样本对应的文本标签,<i>p</i>(<i>y</i>)是边缘分布,<i>p</i>(<i>y</i>|<i>x</i>)是条件分布。边缘分布<i>p</i>(<i>y</i>)和条件分布<i>p</i>(<i>y</i>|<i>x</i>)之间的KL散度(Kullback-Leibler divergence)要大,这样能够生成多样的高质量图像。在本文的实验中,给CUB和Oxford-102数据集一个inception模型,对每个模型的样本进行评估。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">2)Human rank进行定性评估。</h4>
                <div class="p1">
                    <p id="109">在CUB和Oxford-102测试集中随机选择50个文本描述,对于每个句子,生成模型生成5个图像。将5个图像和对应的文本描述给不同的人按不同的方法进行图像质量的排名,最后计算平均排名来评价生成图像的质量和多样性。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">2.3 <b>实验结果</b></h4>
                <div class="p1">
                    <p id="111">表1为<i>MLGAN</i>与<i>StackGAN</i>++模型在<i>CUB</i>数据集上每一层次的<i>Inception score</i>对比。从表中第一行数据可以看出,在第一层次,两个模型的生成图像的<i>Inception score</i>相同;在第二层,<i>MLGAN</i>模型的约束条件中加入了词向量,与<i>StackGAN</i>++相比,<i>Inception score</i>从3.35增加到了3.47;在第三层,<i>MLGAN</i>模型的约束条件中加入了词向量,与<i>StackGAN</i>++相比,<i>Inception score</i>从4.04增加到了4.22。因此,根据第二和第三层次的<i>Inception score</i>增加,<i>MLGAN</i>模型在分辨率为128×128和256×256的生成图像上的效果要优于<i>StackGAN</i>++模型,也进一步验证了<i>MLGAN</i>模型的可行性。</p>
                </div>
                <div class="area_img" id="112">
                    <p class="img_tit"><b>表</b>1 <b>在</b><i>CUB</i><b>数据集上每个层次的</b><i>Inception score</i><b>对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Comparison of Inception score of each level on CUB dataset</i></p>
                    <p class="img_note"></p>
                    <table id="112" border="1"><tr><td><br />层次</td><td><i>StackGAN</i>++</td><td><i>MLGAN</i></td></tr><tr><td><br />64×64</td><td>2.66±0.03</td><td>2.66±0.02</td></tr><tr><td><br />128×128</td><td>3.35±0.02</td><td>3.47±0.02</td></tr><tr><td><br />256×256</td><td>4.04±0.05</td><td>4.22±0.04</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="113">表2为各种模型在<i>CUB</i>和<i>Oxford</i>-102数据集上的<i>Inception score</i>和<i>Human rank</i>结果对比。与<i>StackGAN</i>++相比,<i>MLGAN</i>在<i>CUB</i>数据集上的<i>Inception score</i>提高了4.45%(从4.04到4.22),在<i>Oxford</i>-102数据集上的<i>Human rank</i>提高了3.74%(从3.74到3.88)。通过实验结果分析,<i>MLGAN</i>在<i>Inception score</i>的评分上高于其他<i>GAN</i>模型<citation id="237" type="reference"><link href="191" rel="bibliography" /><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><link href="197" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>;从直观的视觉角度<i>Human rank</i>的评分低于其他<i>GAN</i>模型。表明本文的模型所生成的样本图像质量和多样性有所增强,更接近真实图像。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表</b>2 <b>五种</b><i>GAN</i><b>模型在</b><i>CUB</i><b>和</b><i>Oxford</i>-102<b>数据集上的</b><i>Inception score</i><b>和</b><i>Human rank</i><b>评分</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Inception scores and Human rank scores of five GAN models on CUB and Oxford</i>-102 <i>datasets</i></p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td><br />评价标准</td><td>数据集</td><td><i>GAN</i>-<i>INT</i>-<i>CLS</i></td><td><i>GAWWN</i></td><td><i>StackGAN</i>++</td><td><i>HDGAN</i></td><td><i>MLGAN</i></td></tr><tr><td rowspan="2"><br /><i>Inception score</i></td><td><br /><i>CUB</i></td><td>2.88±0.04</td><td>3.62±0.07</td><td>4.04±0.05</td><td>4.15±0.05</td><td>4.22±0.04</td></tr><tr><td><br /><i>Oxford</i>-102</td><td>2.66±0.03</td><td>—</td><td>3.74±0.03</td><td>3.45±0.07</td><td>3.88±0.03</td></tr><tr><td rowspan="2"><br /><i>Human rank</i></td><td><br /><i>CUB</i></td><td>2.81±0.03</td><td>1.99±0.04</td><td>1.37±0.02</td><td>—</td><td>1.24±0.03</td></tr><tr><td><br /><i>Oxford</i>-102</td><td>1.87±0.03</td><td>—</td><td>1.13±0.03</td><td>—</td><td>1.08±0.02</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="115">图4为四种<i>GAN</i>模型在<i>CUB</i>数据集上的生成结果。图5为四种<i>GAN</i>模型在<i>CUB</i>数据集上的生成结果的细节(喙、翅膀)对比。从图4中可以看出,<i>GANINT</i>-<i>CLS</i>生成的64×64图像只能反映鸟类的一般形状和颜色。缺乏生动的部分(例如喙和腿)和清晰的边缘细节,这使得图像既不够逼真也不具有足够高的分辨率。通过使用额外的条件变量,<i>GAWWN</i>在<i>CUB</i>数据集上获得了更高的<i>Inception score</i>,生成的128×128图像分辨率更高,但在边缘细节和局部纹理上没有大的改善。相比之下,<i>StackGAN</i>++生成了256×256图像,在边缘细节和局部纹理上有了一定的改善,但仍然无法与正常拍出的图像相比。而本文的模型在生成256×256图像的同时,增强了图像的边缘细节和局部纹理特征,使生成的图像更接近于真实的图像。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911017_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 四种GAN模型在CUB数据集上的生成结果" src="Detail/GetImg?filename=images/JSJY201911017_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 四种<i>GAN</i>模型在<i>CUB</i>数据集上的生成结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911017_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Results of four GAN models on CUB dataset</i></p>

                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911017_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 四种GAN模型在CUB数据集上的 生成结果的细节(喙、翅膀)对比" src="Detail/GetImg?filename=images/JSJY201911017_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 四种<i>GAN</i>模型在<i>CUB</i>数据集上的 生成结果的细节(喙、翅膀)对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911017_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Details</i> (<i>beak</i>, <i>wings</i>) <i>comparison of</i><i>the results of four GAN models on CUB dataset</i></p>

                </div>
                <div class="p1">
                    <p id="118">图6为三种<i>GAN</i>模型在<i>Oxford</i>-102数据集上的生成结果。从图中可以看出,本文模型生成的图像中鸟的喙、翅膀以及脚部更加清晰,边缘和细节更加逼真,与其他模型相比取得了较优的效果。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 三种GAN模型在Oxford-102数据集上的生成结果" src="Detail/GetImg?filename=images/JSJY201911017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 三种<i>GAN</i>模型在<i>Oxford</i>-102数据集上的生成结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Results of three GAN models on Oxford</i>-102 <i>dataset</i></p>

                </div>
                <div class="p1">
                    <p id="120">图7为三种<i>GAN</i>模型在<i>Oxford</i>-102数据集上的生成结果的细节(花瓣)对比。从图中可以看出,本文模型生成的图像中的花更加清晰,花瓣的边缘和细节更加逼真,与其他模型相比取得了较优的效果。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 三种GAN模型在Oxford-102数据集上的 生成结果的细节(花瓣)对比" src="Detail/GetImg?filename=images/JSJY201911017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 三种<i>GAN</i>模型在<i>Oxford</i>-102数据集上的 生成结果的细节(花瓣)对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Details</i> (<i>petal</i>) <i>comparison of the results of</i><i>three GAN models on Oxford</i>-102 <i>dataset</i></p>

                </div>
                <h3 id="122" name="122" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="123">本文基于叠加生成对抗网络模型的基础上,引入了层次结构编码,通过条件变量的转换和多层次生成图像,从整体到部分改善生成图像质量。实验结果表明,在相同的数据集上,多层次结构生成对抗网络生成的图像具有更清晰的边缘细节和局部纹理,使生成的图像更接近真实图像。该方法虽然在生成图像方面已经得到不错的效果,但是对于生活中的复杂场景依然很难建模,如何处理这一问题有待进一步研究。同时,生成的图像与训练数据相似,缺乏多样性,因此打算将零样本学习和生成对抗网络结合,合成新类别图像,这也将是下一步研究的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="165">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to generate chairs with convolutional neural networks">

                                <b>[1]</b> DOSOVITSKIY A,SPRINGENBERG J T,BROX T.Learning to generate chairs with convolutional neural networks[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2015:1538-1546.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Who let the dogs out?modeling dog behavior from visual data">

                                <b>[2]</b> EHSANI K,BAGHERINEZHAD H,REDMON J,et al.Who let the dogs out?modeling dog behavior from visual data[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:4051-4060.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902014&amp;v=MjgxMTh0R0ZyQ1VSN3FmWnVac0Z5bm5WNzdOTHl2U2RMRzRIOWpNclk5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 郭雨潇,陈雷霆,董悦.单帧图像下的环境光遮蔽估计[J].计算机研究与发展,2019,56(2):385-393.(GUO Y X,CHEN L T,DONG Y.Inferring ambient occlusion from a single image[J].Journal of Computer Research and Development,2019,56(2):385-393.)
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805007&amp;v=MjAyNDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WNzdOS0NMZlliRzRIOW5NcW85Rlk0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 赵树阳,李建武.基于生成对抗网络的低秩图像生成方法[J].自动化学报,2018,44(5):829-839.(ZHAO S Y,LI J W.Generative adversarial network for generating low-rank images[J].Acta Automatica Sinica,2018,44(5):829-839.)
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906022&amp;v=MDY5NjBMejdCZDdHNEg5ak1xWTlIWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublY3N04=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 何新宇,张晓龙.基于深度神经网络的肺炎图像识别模型[J].计算机应用,2019,39(6):1680-1684.(HE X Y,ZHANG X L.Pneumonia image recognition model based on deep neural network[J].Journal of Computer Applications,2019,39(6):1680-1684.)
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Soccer on your tabletop">

                                <b>[6]</b> REMATAS K,KEMELMACHER-SHLIZERMAN I,CURLESS B,et al.Soccer on your tabletop[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:4738-4747.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pixel recurrent neural networks">

                                <b>[7]</b> van DEN OORD A,KALCHBRENNER N,KAVUKCUOGLU K.Pixel recurrent neural networks[C]// Proceedings of the 33rd International Conference on Machine Learning.New York:ACM,2016:1747-1756.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Nets">

                                <b>[8]</b> GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:2672-2680.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recent progress on Generative Adversarial Networks (GANs) a survey">

                                <b>[9]</b> PAN Z,YU W,YI X,et al.Recent progress on Generative Adversarial Networks (GANs):a survey[J].IEEE Access,2019,7:36322-36333.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recent advances of generative adversarial networks in computer vision">

                                <b>[10]</b> CAO Y J,JIA L L,CHEN Y X,et al.Recent advances of generative adversarial networks in computer vision[J].IEEE Access,2019,7:14985-15006.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative image modeling using style and structure adversarial networks">

                                <b>[11]</b> WANG X,GUPTA A.Generative image modeling using style and structure adversarial networks[C]// Proceedings of the 2016 European Conference on Computer Vision.Cham:Springer,2016:318-335.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks">

                                <b>[12]</b> DENTON E L,CHINTALA S,SZLAM A,et al.Deep generative image models using a Laplacian pyramid of adversarial networks[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.New York:ACM,2015:1486-1494.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative multi-adversarial networks">

                                <b>[13]</b> DURUGKAR I,GEMP I,MAHADEVAN S.Generative multi-adversarial networks[EB/OL].[2018- 06- 20].https://www.taodocs.com/p-110588603.html.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial text to image synthesis">

                                <b>[14]</b> REED S,AKATA Z,YAN X,et al.Generative adversarial text-to-image synthesis[C]// Proceedings of the 33rd International Conference on Machine Learning.New York:ACM,2016:1060-1069.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning what and where to draw">

                                <b>[15]</b> REED S,AKATA Z,MOHAN S,et al.Learning what and where to draw[C]// Proceedings of International Conference on Neural Information Processing Systems.New York:ACM,2016:217-225.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photographic text-to-image synthesis with a hierarchically-nested adversarial network">

                                <b>[16]</b> ZHANG Z,XIE Y,YANG L.Photographic text-to-image synthesis with a hierarchically-nested adversarial network[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2018:6199-6208.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=StackGAN++ realistic image synthesis with stacked generative adversarial networks">

                                <b>[17]</b> ZHANG H,XU T,LI H,et al.StackGAN++:realistic image synthesis with stacked generative adversarial networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2019,41(8):1947-1962.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Question-Image Co-Attention for Visual Question Answering">

                                <b>[18]</b> LU J,YANG J,BATRA D,et al.Hierarchical question-image co-attention for visual question answering[C]// Proceedings of the 30th International Conference on Neural Information Processing Systems.New York:ACM,2016:289-297.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">

                                <b>[19]</b> HU B,LU Z,LI H,et al.Convolutional neural network architectures for matching natural language sentences[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.New York:ACM,2014:2042-2050.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:A System for Large-Scale Machine Learning">

                                <b>[20]</b> ABADI M,BARHAM P,CHEN J,et al.TensorFlow:a system for large-scale machine learning[C]// Proceedings of the 2016 Conference on Operating Systems Design and Implementation.Piscataway:IEEE,2016:265-283.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Caltech-UCSD Birds-200-2011 Dataset">

                                <b>[21]</b> WAH C,BRANSON S,WELINDER P,et al.The Caltech-UCSD Birds-200-2011 dataset:computation &amp; neural systems technical report [R].Pasadena,CA,USA:California Institute of Technology,2011.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated Flower Classification over a Large Number of Classes">

                                <b>[22]</b> NILSBACK M E,ZISSERMAN A.Automated flower classification over a large number of classes[C]// Proceedings of the 6th Indian Conference on Computer Vision,Graphics &amp; Image Processing.Piscataway:IEEE,2008:722-729.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training GANs">

                                <b>[23]</b> SALIMANS T,GOODFELLOW I J,ZAREMBA W,et al.Improved techniques for training GANs[C]// Proceedings of International Conference on Neural Information Processing Systems.New York:ACM,2016:2234-2242.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911017&amp;v=MjY3NjBMejdCZDdHNEg5ak5ybzlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublY3N0s=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
