<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775611690000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904014%26RESULT%3d1%26SIGN%3dN5JT2XaS7xHFtuRpyRd0oy0O7gM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904014&amp;v=MzA5NDBGckNVUjdxZlp1WnNGeURnVjd2T0x6N0JkN0c0SDlqTXE0OUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="2 结合节点描述属性信息的网络表示学习 ">2 结合节点描述属性信息的网络表示学习</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="2.1 &lt;b&gt;相关定义&lt;/b&gt;">2.1 <b>相关定义</b></a></li>
                                                <li><a href="#89" data-title="2.2 &lt;b&gt;模型框架&lt;/b&gt;">2.2 <b>模型框架</b></a></li>
                                                <li><a href="#130" data-title="2.3 &lt;b&gt;联合训练及其优化算法&lt;/b&gt;">2.3 <b>联合训练及其优化算法</b></a></li>
                                                <li><a href="#192" data-title="2.4 &lt;b&gt;算法时间复杂度分析&lt;/b&gt;">2.4 <b>算法时间复杂度分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#202" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#203" data-title="3.1 &lt;b&gt;实验设定&lt;/b&gt;">3.1 <b>实验设定</b></a></li>
                                                <li><a href="#230" data-title="3.2 &lt;b&gt;结果分析&lt;/b&gt;">3.2 <b>结果分析</b></a></li>
                                                <li><a href="#239" data-title="3.3 &lt;b&gt;参数敏感性分析&lt;/b&gt;">3.3 <b>参数敏感性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#248" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="图1 网络中具有相同属性值的链接节点对占比情况">图1 网络中具有相同属性值的链接节点对占比情况</a></li>
                                                <li><a href="#80" data-title="图2 NPA-NRL模型框架">图2 NPA-NRL模型框架</a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;符号定义&lt;/b&gt;"><b>表</b>1 <b>符号定义</b></a></li>
                                                <li><a href="#208" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;实验数据集&lt;/b&gt;"><b>表</b>2 <b>实验数据集</b></a></li>
                                                <li><a href="#229" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;最优超参数设定&lt;/b&gt;"><b>表</b>3 <b>最优超参数设定</b></a></li>
                                                <li><a href="#233" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;链路预测性能&lt;/b&gt;AUC&lt;b&gt;值&lt;/b&gt;"><b>表</b>4 <b>链路预测性能</b>AUC<b>值</b></a></li>
                                                <li><a href="#241" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同算法在&lt;/b&gt;GPLUS&lt;b&gt;网络上的节点分类性能比较&lt;/b&gt;"><b>表</b>5 <b>不同算法在</b>GPLUS<b>网络上的节点分类性能比较</b></a></li>
                                                <li><a href="#242" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;不同算法在&lt;/b&gt;OKLAHOMA&lt;b&gt;网络上的节点分类性能比较&lt;/b&gt;"><b>表</b>6 <b>不同算法在</b>OKLAHOMA<b>网络上的节点分类性能比较</b></a></li>
                                                <li><a href="#243" data-title="&lt;b&gt;表&lt;/b&gt;7 &lt;b&gt;不同算法在&lt;/b&gt;UNC&lt;b&gt;网络上的节点分类性能比较&lt;/b&gt;"><b>表</b>7 <b>不同算法在</b>UNC<b>网络上的节点分类性能比较</b></a></li>
                                                <li><a href="#247" data-title="图3 超参数对节点分类性能影响">图3 超参数对节点分类性能影响</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="301">


                                    <a id="bibliography_1" title="丁兆云, 贾焰, 周斌.微博数据挖掘研究综述[J].计算机研究与发展, 2014, 51 (4) :691-706. (DING Z Y, JIA Y, ZHOU B.Survey of data mining for microblogs[J].Journal of Computer Research and Development, 2014, 51 (4) :691-706.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201404001&amp;v=MjUwODRkTEc0SDlYTXE0OUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd2T0x5dlM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        丁兆云, 贾焰, 周斌.微博数据挖掘研究综述[J].计算机研究与发展, 2014, 51 (4) :691-706. (DING Z Y, JIA Y, ZHOU B.Survey of data mining for microblogs[J].Journal of Computer Research and Development, 2014, 51 (4) :691-706.) 
                                    </a>
                                </li>
                                <li id="303">


                                    <a id="bibliography_2" title="PEROZZI B, AL-RFOU R, SKIENA S.Deep Walk:online learning of social representations[C]//KDD 2014:Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2014:701-710." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepwalk:Online learning of social representations">
                                        <b>[2]</b>
                                        PEROZZI B, AL-RFOU R, SKIENA S.Deep Walk:online learning of social representations[C]//KDD 2014:Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2014:701-710.
                                    </a>
                                </li>
                                <li id="305">


                                    <a id="bibliography_3" title="TANG J, QU M, WANG M, et al.LINE:large-scale information network embedding[C]//WWW 2015:Proceedings of the 24th International Conference on World Wide Web.Geneva, Switzerland:International World Wide Web Conferences Steering Committee, 2015:1067-1077." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LINE:large-scale information network embedding">
                                        <b>[3]</b>
                                        TANG J, QU M, WANG M, et al.LINE:large-scale information network embedding[C]//WWW 2015:Proceedings of the 24th International Conference on World Wide Web.Geneva, Switzerland:International World Wide Web Conferences Steering Committee, 2015:1067-1077.
                                    </a>
                                </li>
                                <li id="307">


                                    <a id="bibliography_4" title="WANG D, CUI P, ZHU W.Structural deep network embedding[C]//KDD 2016:Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2016:1225-1234." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structural deep network embedding">
                                        <b>[4]</b>
                                        WANG D, CUI P, ZHU W.Structural deep network embedding[C]//KDD 2016:Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2016:1225-1234.
                                    </a>
                                </li>
                                <li id="309">


                                    <a id="bibliography_5" title="李志宇, 梁循, 周小平, 等.一种大规模网络中基于节点结构特征映射的链接预测方法[J].计算机学报, 2016, 39 (10) :1947-1964. (LI Z Y, LIANG X, ZHOU X P, et al.A link prediction method for large-scale networks[J].Chinese Journal of Computers, 2016, 39 (10) :1947-1964.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201610002&amp;v=MTY1NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd2T0x6N0Jkckc0SDlmTnI0OUZab1E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        李志宇, 梁循, 周小平, 等.一种大规模网络中基于节点结构特征映射的链接预测方法[J].计算机学报, 2016, 39 (10) :1947-1964. (LI Z Y, LIANG X, ZHOU X P, et al.A link prediction method for large-scale networks[J].Chinese Journal of Computers, 2016, 39 (10) :1947-1964.) 
                                    </a>
                                </li>
                                <li id="311">


                                    <a id="bibliography_6" title="WANG Z, CHEN C, LI W.Predictive network representation learning for link prediction[C]//SIGIR 2017:Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2017:969-972." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predictive network representation learning for link prediction">
                                        <b>[6]</b>
                                        WANG Z, CHEN C, LI W.Predictive network representation learning for link prediction[C]//SIGIR 2017:Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2017:969-972.
                                    </a>
                                </li>
                                <li id="313">


                                    <a id="bibliography_7" title="YANG J, Mc AULEY J, LESKOVEC J.Community detection in networks with node attributes[C]//ICDM 2013:Proceedings of the2013 IEEE 13th International Conference on Data Mining.Piscataway, NJ:IEEE, 2013:1151-1156." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Community Detection in Networks with Node Attributes">
                                        <b>[7]</b>
                                        YANG J, Mc AULEY J, LESKOVEC J.Community detection in networks with node attributes[C]//ICDM 2013:Proceedings of the2013 IEEE 13th International Conference on Data Mining.Piscataway, NJ:IEEE, 2013:1151-1156.
                                    </a>
                                </li>
                                <li id="315">


                                    <a id="bibliography_8" title="Mc PHERSON M, SMITH-LOVIN L, COOK J M.Birds of a feather:homophily in social networks[J].Annual Review of Sociology, 2001, 27 (1) :415-444." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SARD&amp;filename=SARD14080500000646&amp;v=MDI3MzQ5RlpPc1BDbmcvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0ZzUWJoUT1OaXpaYXJLOEh0bk1xbw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Mc PHERSON M, SMITH-LOVIN L, COOK J M.Birds of a feather:homophily in social networks[J].Annual Review of Sociology, 2001, 27 (1) :415-444.
                                    </a>
                                </li>
                                <li id="317">


                                    <a id="bibliography_9" title="AIELLO L M, BARRAT A, SCHIFANELLA R, et al.Friendship prediction and homophily in social media[J].ACM Transactions on the Web, 2012, 6 (2) :1-33." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000006044&amp;v=MDcxOTZVcjNJS0ZzUWJoUT1OaWZJWTdLN0h0ak5yNDlGWk9zSkRIZzlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        AIELLO L M, BARRAT A, SCHIFANELLA R, et al.Friendship prediction and homophily in social media[J].ACM Transactions on the Web, 2012, 6 (2) :1-33.
                                    </a>
                                </li>
                                <li id="319">


                                    <a id="bibliography_10" title="TRAUD A L, MUCHA P J, PORTER M A.Social structure of Facebook networks[J].Physica A:Statistical Mechanics and its Applications, 2012, 391 (16) :4165-4180." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100529270&amp;v=Mjk4MDhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRnNRYmhRPU5pZk9mYks3SHRET3JvOUZZZWtHRG5zNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        TRAUD A L, MUCHA P J, PORTER M A.Social structure of Facebook networks[J].Physica A:Statistical Mechanics and its Applications, 2012, 391 (16) :4165-4180.
                                    </a>
                                </li>
                                <li id="321">


                                    <a id="bibliography_11" title="YANG C, LIU Z, ZHAO D, et al.Network representation learning with rich text information[C]//IJCAI 2015:Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:2111-2117." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network representation learning with rich text informa-tion">
                                        <b>[11]</b>
                                        YANG C, LIU Z, ZHAO D, et al.Network representation learning with rich text information[C]//IJCAI 2015:Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:2111-2117.
                                    </a>
                                </li>
                                <li id="323">


                                    <a id="bibliography_12" title="ZHANG D, YIN J, ZHU X, et al.Homophily, structure, and content augmented network representation learning[C]//ICDM2016:Proceedings of the 16th IEEE International Conference on Data Mining Series.Piscataway, NJ:IEEE, 2016:609-618." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Homophily,structure,and content augmented network representation learning">
                                        <b>[12]</b>
                                        ZHANG D, YIN J, ZHU X, et al.Homophily, structure, and content augmented network representation learning[C]//ICDM2016:Proceedings of the 16th IEEE International Conference on Data Mining Series.Piscataway, NJ:IEEE, 2016:609-618.
                                    </a>
                                </li>
                                <li id="325">


                                    <a id="bibliography_13" title="LI H, WANG H, YANG Z, et al.Variation autoencoder based network representation learning for classification[C]//ACL2017:Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA, USA:ACL, 2017:56-61." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Variation autoencoder based network representation learning for classification">
                                        <b>[13]</b>
                                        LI H, WANG H, YANG Z, et al.Variation autoencoder based network representation learning for classification[C]//ACL2017:Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA, USA:ACL, 2017:56-61.
                                    </a>
                                </li>
                                <li id="327">


                                    <a id="bibliography_14" title="MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[C]//NIPS 2013:Proceedings of the Twenty-Seventh Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2013:3111-3119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">
                                        <b>[14]</b>
                                        MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[C]//NIPS 2013:Proceedings of the Twenty-Seventh Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2013:3111-3119.
                                    </a>
                                </li>
                                <li id="329">


                                    <a id="bibliography_15" title="ZHANG D, YIN J, ZHU X, et al.User profile preserving social network embedding[C]//IJCAI 2017:Proceedings of the 26th International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:3378-3384." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=User profile preserving social network embedding">
                                        <b>[15]</b>
                                        ZHANG D, YIN J, ZHU X, et al.User profile preserving social network embedding[C]//IJCAI 2017:Proceedings of the 26th International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:3378-3384.
                                    </a>
                                </li>
                                <li id="331">


                                    <a id="bibliography_16" title="RAHIMI A, RECHT B.Random features for large-scale kernel machines[C]//NIPS 2008:Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2008:1177-1184." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Random Features for Large-Scale Kernel Machines">
                                        <b>[16]</b>
                                        RAHIMI A, RECHT B.Random features for large-scale kernel machines[C]//NIPS 2008:Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2008:1177-1184.
                                    </a>
                                </li>
                                <li id="333">


                                    <a id="bibliography_17" title="温雯, 黄家明, 蔡瑞初, 等.一种融合节点先验信息的图表示学习方法[J].软件学报, 2018, 29 (3) :786-798. (WEN W, HUANG J M, CAI R C, et al.Graph embedding by incorporating prior knowledge on vertex information[J].Journal of Software, 2018, 29 (3) :786-798.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201803017&amp;v=MzA0NzZWN3ZPTnlmVGJMRzRIOW5Nckk5RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        温雯, 黄家明, 蔡瑞初, 等.一种融合节点先验信息的图表示学习方法[J].软件学报, 2018, 29 (3) :786-798. (WEN W, HUANG J M, CAI R C, et al.Graph embedding by incorporating prior knowledge on vertex information[J].Journal of Software, 2018, 29 (3) :786-798.) 
                                    </a>
                                </li>
                                <li id="335">


                                    <a id="bibliography_18" title="YANG C, SUN M, LIU Z, et al.Fast network embedding enhancement via high order proximity approximation[C]//IJCAI2017:Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:19-25." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast network embedding enhancement via high order proximity approximation">
                                        <b>[18]</b>
                                        YANG C, SUN M, LIU Z, et al.Fast network embedding enhancement via high order proximity approximation[C]//IJCAI2017:Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:19-25.
                                    </a>
                                </li>
                                <li id="337">


                                    <a id="bibliography_19" title="HUANG X, LI J, HU X.Accelerated attributed network embedding[C]//SDM 2017:Proceedings of the 2017 SIAM International Conference on Data Mining.Philadelphia:SIAM, 2017:633-641." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerated attributed network embedding">
                                        <b>[19]</b>
                                        HUANG X, LI J, HU X.Accelerated attributed network embedding[C]//SDM 2017:Proceedings of the 2017 SIAM International Conference on Data Mining.Philadelphia:SIAM, 2017:633-641.
                                    </a>
                                </li>
                                <li id="339">


                                    <a id="bibliography_20" title="Le CUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[20]</b>
                                        Le CUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436-444.
                                    </a>
                                </li>
                                <li id="341">


                                    <a id="bibliography_21" title="CAO S, LU W, XU Q.Deep neural networks for learning graph representations[C]//AAAI 2016:Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2016:1145-1152." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for learning graph representations">
                                        <b>[21]</b>
                                        CAO S, LU W, XU Q.Deep neural networks for learning graph representations[C]//AAAI 2016:Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2016:1145-1152.
                                    </a>
                                </li>
                                <li id="343">


                                    <a id="bibliography_22" title="LIAO L, HE X, ZHANG H, et al.Attributed social network embedding[J].IEEE Transactions on Knowledge and Data Engineering, 2018, 30 (12) :2257-2270." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attributed social network embedding">
                                        <b>[22]</b>
                                        LIAO L, HE X, ZHANG H, et al.Attributed social network embedding[J].IEEE Transactions on Knowledge and Data Engineering, 2018, 30 (12) :2257-2270.
                                    </a>
                                </li>
                                <li id="345">


                                    <a id="bibliography_23" title="HAMILTON W L, YING R, LESKOVEC J.Representation learning on graphs:methods and applications[EB/OL].[2018-05-10].https://arxiv.org/pdf/1709.05584." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Representation learning on graphs:methods and applications">
                                        <b>[23]</b>
                                        HAMILTON W L, YING R, LESKOVEC J.Representation learning on graphs:methods and applications[EB/OL].[2018-05-10].https://arxiv.org/pdf/1709.05584.
                                    </a>
                                </li>
                                <li id="347">


                                    <a id="bibliography_24" title="SIETSMA J, DOW R J F.Creating artificial neural networks that generalize[J].Neural Networks, 1991, 4 (1) :67-79." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Creating artificial neural networks that generalize">
                                        <b>[24]</b>
                                        SIETSMA J, DOW R J F.Creating artificial neural networks that generalize[J].Neural Networks, 1991, 4 (1) :67-79.
                                    </a>
                                </li>
                                <li id="349">


                                    <a id="bibliography_25" title="HORNIK K, STINCHCOMBE M, WHITE H.Multilayer feedforward networks are universal approximators[J].Neural Networks, 1989, 2 (5) :359-366." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multilayer feedforward networks are universal approximators">
                                        <b>[25]</b>
                                        HORNIK K, STINCHCOMBE M, WHITE H.Multilayer feedforward networks are universal approximators[J].Neural Networks, 1989, 2 (5) :359-366.
                                    </a>
                                </li>
                                <li id="351">


                                    <a id="bibliography_26" title="GLOROT X, BENGIO Y.Understanding the difficulty of training deep feedforward neural networks[C]//AISTATS 2010:Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics.Cambridge, MA:MIT Press, 2010:249-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">
                                        <b>[26]</b>
                                        GLOROT X, BENGIO Y.Understanding the difficulty of training deep feedforward neural networks[C]//AISTATS 2010:Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics.Cambridge, MA:MIT Press, 2010:249-256.
                                    </a>
                                </li>
                                <li id="353">


                                    <a id="bibliography_27" title="KINGMA D P, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-05-10].https://arxiv.org/pdf/1412.6980." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">
                                        <b>[27]</b>
                                        KINGMA D P, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-05-10].https://arxiv.org/pdf/1412.6980.
                                    </a>
                                </li>
                                <li id="355">


                                    <a id="bibliography_28" title="LESKOVEC J, MCAULEY J J.Learning to discover social circles in ego networks[C]//NIPS 2012:Proceedings of the Twenty-sixth Annual Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:539-547." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to discover social circles in ego networks">
                                        <b>[28]</b>
                                        LESKOVEC J, MCAULEY J J.Learning to discover social circles in ego networks[C]//NIPS 2012:Proceedings of the Twenty-sixth Annual Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:539-547.
                                    </a>
                                </li>
                                <li id="357">


                                    <a id="bibliography_29" title="吕琳媛.复杂网络链路预测[J].电子科技大学学报, 2010, 39 (5) :651-661. (LYU L Y.Link prediction on complex networks[J].Journal of University of Electronic Science and Technology of China, 2010, 39 (5) :651-661.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DKDX201005006&amp;v=MzA3NTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWN3ZPSVNiUGRyRzRIOUhNcW85RllvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                        吕琳媛.复杂网络链路预测[J].电子科技大学学报, 2010, 39 (5) :651-661. (LYU L Y.Link prediction on complex networks[J].Journal of University of Electronic Science and Technology of China, 2010, 39 (5) :651-661.) 
                                    </a>
                                </li>
                                <li id="359">


                                    <a id="bibliography_30" title="FAN R E, CHANG K W, HSIEH C J, et al.LIBLINEAR:a library for large linear classification[J].Journal of Machine Learning Research, 2008, 9:1871-1874." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LIBLINEAR: A library for large linear classification">
                                        <b>[30]</b>
                                        FAN R E, CHANG K W, HSIEH C J, et al.LIBLINEAR:a library for large linear classification[J].Journal of Machine Learning Research, 2008, 9:1871-1874.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-17 16:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),1012-1020 DOI:10.11772/j.issn.1001-9081.2018081851            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>融合节点描述属性信息的网络表示学习算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%AD%A3%E9%93%AD&amp;code=39982222&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘正铭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E5%AE%8F&amp;code=21334685&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马宏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%A0%91%E6%96%B0&amp;code=28184305&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘树新</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%B5%B7%E6%B6%9B&amp;code=10912997&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李海涛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B8%B8%E5%9C%A3&amp;code=41473325&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">常圣</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E5%AE%B6%E6%95%B0%E5%AD%97%E4%BA%A4%E6%8D%A2%E7%B3%BB%E7%BB%9F%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0291391&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国家数字交换系统工程技术研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为融合节点描述信息提升网络表示学习质量, 针对社会网络中节点描述属性信息存在的语义信息分散和不完备性问题, 提出一种融合节点描述属性的网络表示 (NPA-NRL) 学习算法。首先, 对属性信息进行独热编码, 并引入随机扰动的数据集增强策略解决属性信息不完备问题;然后, 将属性编码和结构编码拼接作为深度神经网络输入, 实现两方面信息的相互补充制约;最后, 设计了基于网络同质性的属性相似性度量函数和基于SkipGram模型的结构相似性度量函数, 通过联合训练实现融合语义信息挖掘。在GPLUS、OKLAHOMA和UNC三个真实网络数据集上的实验结果表明, 和经典的DeepWalk、TADW (Text-Associated DeepWalk) 、UPP-SNE (User Profile Preserving Social Network Embedding) 和SNE (Social Network Embedding) 算法相比, NPA-NRL算法的链路预测AUC (Area Under Curve of ROC) 值平均提升2.75%, 节点分类F1值平均提升7.10%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%8A%82%E7%82%B9%E6%8F%8F%E8%BF%B0%E5%B1%9E%E6%80%A7%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">节点描述属性信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络表示学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">复杂网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘正铭 (1995—) , 男, 四川南充人, 硕士研究生, CCF会员, 主要研究方向:网络大数据分析、网络表示学习;电子邮箱liuzhengming_wy@163.com;
                                </span>
                                <span>
                                    马宏 (1968—) , 男, 江苏东台人, 研究员, 硕士, 主要研究方向:社会网络分析、电信网关防;;
                                </span>
                                <span>
                                    刘树新 (1987—) , 男, 山东临朐人, 助理研究员, 博士, 主要研究方向:复杂网络、网络数据挖掘;;
                                </span>
                                <span>
                                    李海涛 (1982—) , 男, 山东泰安人, 讲师, 硕士, 主要研究方向:网络数据挖掘;;
                                </span>
                                <span>
                                    常圣 (1988—) , 男, 河南郑州人, 硕士研究生, 主要研究方向:网络数据挖掘。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61521003, 61803384);</span>
                    </p>
            </div>
                    <h1><b>Network representation learning algorithm incorporated with node profile attribute information</b></h1>
                    <h2>
                    <span>LIU Zhengming</span>
                    <span>MA Hong</span>
                    <span>LIU Shuxin</span>
                    <span>LI Haitao</span>
                    <span>CHANG Sheng</span>
            </h2>
                    <h2>
                    <span>National Digital Switching System Engineering & Technological Research Center</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to enhance the network representation learning quality with node profile information, and focus on the problems of semantic information dispersion and incompleteness of node profile attribute information in social network, a network representation learning algorithm incorporated with node profile information was proposed, namely NPA-NRL. Firstly, attribute information were encoded by one-hot encoding, and a data augmentation method of random perturbation was introduced to overcome the incompleteness of node profile attribute information. Then, attribute coding and structure coding were combined as the input of deep neural network to realize mutual complementation of the two types of information. Finally, an attribute similarity measure function based on network homogeneity and a structural similarity measure function based on SkipGram model were designed to mine fused semantic information through joint training. The experimental results on three real network datasets including GPLUS, OKLAHOMA and UNC demonstrate that, compared with the classic DeepWalk, Text-Associated DeepWalk (TADW) , User Profile Preserving Social Network Embedding (UPP-SNE) and Social Network Embedding (SNE) algorithms, the proposed NPA-NRL algorithm has a 2.75% improvement in average Area Under Curve of ROC (AUC) value on link prediction task, and a 7.10% improvement in average F1 value on node classification task.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=node%20profile%20attribute%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">node profile attribute information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=information%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">information fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=network%20representation%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">network representation learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=complex%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">complex network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LIU Zhengming, born in 1995, M. S. candidate. His research interests include network big data analysis, network representation learning.;
                                </span>
                                <span>
                                    MA Hong, born in 1968, M. S. , research fellow. His research interests include social network analysis, telecommunications gateway defense.;
                                </span>
                                <span>
                                    LIU Shuxin, born in 1987, Ph. D. , assistant research fellow. His research interests include complex network, network data mining.;
                                </span>
                                <span>
                                    LI Haitao, born in 1982, M. S. , lecturer. His research interests include network data mining.;
                                </span>
                                <span>
                                    CHANG Sheng, born in 1988, M. S. candidate. His research interests include network data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-06</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61521003, 61803384);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="64">随着互联网技术的不断发展演化, 诸如Facebook、Twitter、微信和微博等社交网络应用迅猛发展, 社会网络数据逐渐呈现出规模复杂非线性和类型多样性等特点<citation id="361" type="reference"><link href="301" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 给基于特征工程抽取节点特征的网络分析技术带来严峻挑战。</p>
                </div>
                <div class="p1">
                    <p id="65">近年来, 受表示学习方法启发, 大量学者提出针对网络分析任务的网络表示学习算法, 旨在利用机器学习算法实现网络节点的自动特征抽取, 进而快速高效地应用于后续网络分析任务, 如节点分类<citation id="362" type="reference"><link href="303" rel="bibliography" /><link href="305" rel="bibliography" /><link href="307" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>和链路预测<citation id="363" type="reference"><link href="307" rel="bibliography" /><link href="309" rel="bibliography" /><link href="311" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>等。</p>
                </div>
                <div class="p1">
                    <p id="66">然而, 大部分算法仅考虑网络结构信息进行表示学习<citation id="366" type="reference"><link href="303" rel="bibliography" /><link href="305" rel="bibliography" /><link href="307" rel="bibliography" /><link href="309" rel="bibliography" /><link href="311" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>, 忽略了现实世界中网络节点包含的丰富节点描述属性信息, 如性别、住址和毕业院校等。从社会心理学角度来看, 节点之间的属性相似性对连边关系的形成具有重要影响<citation id="364" type="reference"><link href="313" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 因而整个网络呈现出同质性的特点, 即具有连边关系的节点通常具有相似的属性信息<citation id="367" type="reference"><link href="315" rel="bibliography" /><link href="317" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。为验证上述观点, 本文基于俄克拉何马州大学 (University of Oklahoma, OKLAHOMA) 和北卡罗来纳大学教堂山分校 (University of North Carolina at Chapel Hill, UNC) 两个真实Facebook高校网络数据集<citation id="365" type="reference"><link href="319" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>进行统计分析。如图1所示, 本文分析了具有连边关系的节点对中具有相同描述属性值的占比情况。从统计结果可看出, 具有连边关系的节点对中具有相同描述属性信息的节点对平均占比37%, 在部分描述属性信息上甚至超过90% (如图1中的节点状态标记属性) 。该现象再次印证了结合节点描述属性信息进行网络表示学习的重要意义。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904014_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络中具有相同属性值的链接节点对占比情况" src="Detail/GetImg?filename=images/JSJY201904014_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 网络中具有相同属性值的链接节点对占比情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904014_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Proportion of linked node pairs with the same attribute value in social network</p>

                </div>
                <div class="p1">
                    <p id="68">值得注意的是, 已有一些算法考虑利用节点文本内容属性信息进行表示学习<citation id="369" type="reference"><link href="321" rel="bibliography" /><link href="323" rel="bibliography" /><link href="325" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>, 如科学引文网络中节点的文本特征。然而, 这类方法并不能有效利用社会网络中节点的描述属性信息。主要原因有以下两点:一是节点文本内容属性信息通常具有话题中心性, 围绕某个中心内容而展开。如引文网络中的文章摘要、社交网络中的评论内容等。相比之下, 节点描述属性信息包含多种不同类型的节点属性表示, 语义信息相对分散。二是社会网络中的用户需要手动增加描述属性信息, 出于用户个人保密以及平台隐私保护等原因导致采集到的用户描述属性信息存在不同程度缺失 (在Facebook数据集<citation id="368" type="reference"><link href="319" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>上的统计分析发现, 属性信息平均缺失比例接近20%, 单一属性信息缺失最高达到50%) , 描述属性信息的不完备性导致信息部分丢失, 融合难度较大。</p>
                </div>
                <div class="p1">
                    <p id="69">针对上述问题, 本文提出了一种融合节点描述属性信息的网络表示 (Network Representation Learning with Node Profile Attributes, NPA-NRL) 学习算法。首先, 在数据编码阶段引入基于随机扰动的数据集增强策略迫使模型学习更加鲁棒的表示向量, 解决属性信息不完备问题。其次, 采用将结构编码和属性编码拼接作为神经网络输入的优先融合策略, 实现训练过程中两方面信息的相互补充与制约;同时, 利用深度前馈神经网络的深度特征抽取能力, 获得更加精准的特征表示。最后, 分别设计了基于结构相似性和属性相似性保留的优化损失函数, 通过联合训练, 实现两方面信息的融合表示学习, 充分挖掘语义信息。通过在多个公开数据集上的链路预测和节点分类实验, 验证了本文算法的有效性。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="71">DeepWalk算法<citation id="370" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>首次提出了随机游走思想:首先在基于网络结构的采样中得到随机游走序列, 然后将其作为自然语言处理中SkipGram词向量模型<citation id="371" type="reference"><link href="327" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的输入来建模网络结构信息, 最终得到节点表示向量。在此基础上, UPP-SNE (User Profile Preserving Social Network Embedding) 算法<citation id="372" type="reference"><link href="329" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了融合节点描述属性信息进行表示学习的思路:首先通过<i>K</i>核映射<citation id="373" type="reference"><link href="331" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>对节点描述属性信息进行非线性映射得到表示向量 <i>f</i> (<i>v</i>) , 然后利用DeepWalk框架将网络结构信息编码到<i>f</i> (<i>v</i>) 中, 实现结构和属性信息的相互约束补充。温雯等<citation id="374" type="reference"><link href="333" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了GeVI (Graph embedding by incorporating prior knowledge on Vertices Information) 算法, 通过重定义SkipGram模型中条件概率分布为已知节点属性特征向量和节点表示向量的内积, 实现两方面信息的融合表示。</p>
                </div>
                <div class="p1">
                    <p id="72">文献<citation id="375" type="reference">[<a class="sup">18</a>]</citation>将基于矩阵分解的方法概括为同一框架:1) 构建节点间的关系矩阵;2) 对该矩阵进行矩阵分解操作得到节点表示向量。Yang等<citation id="376" type="reference"><link href="321" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出了TADW (Text-Associated DeepWalk) 算法, 证明了DeepWalk算法等价于将特定关系矩阵<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow><mo>*</mo><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></msup></mrow></math></mathml>分解为<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mi>d</mi><mo>*</mo><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></msup></mrow></math></mathml>和<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mi>d</mi><mo>*</mo><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></msup></mrow></math></mathml>。在此基础上, 将节点文本内容属性信息矩阵<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mi>m</mi><mo>*</mo><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></msup></mrow></math></mathml>嵌入到<b><i>M</i></b>的矩阵分解中, 最终将<b><i>W</i></b>和<b><i>T</i></b>拼接得到节点的表示向量。Huang等<citation id="377" type="reference"><link href="337" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了AANE (Accelerated Attributed Network Embedding) 算法:首先通过节点属性信息构造属性相似性矩阵, 再对其进行矩阵分解得到节点的表示向量。最后, 在损失函数增加了基于连接关系约束的正则项, 使得连接关系紧密的节点具有相近的表示向量, 实现两方面信息的融合学习。</p>
                </div>
                <div class="p1">
                    <p id="77">近年来, 深度学习在图像、文本和语音等领域取得较大进展, 这主要得益于深度神经网络对复杂非线性关系的强大表示能力<citation id="378" type="reference"><link href="339" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。而大规模社会网络信息也具有复杂非线性的特点, 因此出现了大量基于深度神经网络的表示学习算法<citation id="382" type="reference"><link href="307" rel="bibliography" /><link href="325" rel="bibliography" /><link href="341" rel="bibliography" /><link href="343" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">13</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。SDNE (Structural Deep Network Embedding) 算法<citation id="379" type="reference"><link href="307" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>首次将深度自编码神经网络用于网络表示学习, 将网络节点间一阶和二阶邻居关系分别作为自编码神经网络的有监督和无监督信息进行训练, 有效实现了网络节点间的一、二阶相似性保留。DNGR (Deep Neural Networks for Graph Representations) 算法<citation id="380" type="reference"><link href="341" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>从保留网络全局信息的角度出发, 提出使用随机冲浪模型生成的PPMI (Positive Pointwise Mutual Information) 矩阵作为堆叠自编码神经网络的输入来进行表示学习, 并从理论上分析了基于矩阵分解的方法是一种线性映射关系, 难以有效挖掘网络的复杂非线性关系。SNE (Social Network Embedding) 算法<citation id="381" type="reference"><link href="343" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>考虑融合节点属性信息进行表示学习, 将独热编码的节点ID (IDentifier) 编码向量和节点属性信息编码向量作为神经网络输入, 再通过多层感知机提取低维特征表示, 最后以最大化原始网络节点邻居共现概率为优化目标训练得到最优的神经网络参数。</p>
                </div>
                <div class="p1">
                    <p id="78">从表示学习模型上来看:上述基于网络结构或者融合节点属性的表示方法中, 其核心模型可以归结为三类:词向量模型、矩阵分解模型和深度神经网络模型。词向量模型的优势在于训练过程只依赖于局部信息, 便于在线处理, 训练速度较快;然而由于词向量模型属于浅层模型, 难以充分挖掘网络复杂非线性关系。矩阵分解模型已被证明属于线性映射过程, 也不适合网络复杂非线性关系挖掘<citation id="383" type="reference"><link href="341" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。相比之下, 深度神经网络作为一种有效的非线性特征提取手段, 其主要优势有以下两点:一是基于参数共享的正则化策略。基于矩阵分解和随机游走的方法给每个节点独立映射为一个参数向量, 而神经网络共享唯一一组参数, 参数共享策略可以作为一种有效的正则化手段<citation id="384" type="reference"><link href="345" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。二是有效捕获网络复杂非线性关系。神经网络通过非线性激活函数可有效实现非线性映射。</p>
                </div>
                <div class="p1">
                    <p id="79">从信息融合方式上来看:上述融合节点属性的表示方法中, 除SNE算法外, 均是采用先表示得到节点属性信息的表示向量, 再考虑结构信息进行优化更新, 这种先表示后融合的训练方式在后续实验中被验证是一种低效的融合方式, 不能有效发挥节点结构信息和属性信息之间的相互补充与制约。相比之下, SNE算法提出的直接将两方面信息的原始表示拼接作为输入数据进行训练的先融合后表示的策略, 能够更好实现两方面信息的融合表示。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904014_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 NPA-NRL模型框架" src="Detail/GetImg?filename=images/JSJY201904014_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 NPA-NRL模型框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904014_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Framework of NPA-NRL</p>

                </div>
                <h3 id="81" name="81" class="anchor-tag">2 结合节点描述属性信息的网络表示学习</h3>
                <h4 class="anchor-tag" id="82" name="82">2.1 <b>相关定义</b></h4>
                <div class="p1">
                    <p id="83">为更好地描述所提模型及其具体算法, 首先给出相关定义及其主要符号表示如表1所示。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表</b>1 <b>符号定义</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Symbol definition</i></p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td><br />符号</td><td>定义</td></tr><tr><td><br /><i>V</i></td><td>网络节点集合, <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></math>表示节点数量</td></tr><tr><td><br /><i>v</i><sub><i>i</i></sub></td><td>标号为<i>i</i>的节点, <i>v</i><sub><i>i</i></sub>∈<i>V</i></td></tr><tr><td><br /><i>s</i><sub><i>ij</i></sub></td><td>节点<i>v</i><sub><i>i</i></sub>和<i>v</i><sub><i>j</i></sub>之间的连接权重</td></tr><tr><td><br /><i>Nr</i> (<i>v</i><sub><i>i</i></sub>) </td><td>表示节点<i>v</i><sub><i>i</i></sub>的邻居节点集合</td></tr><tr><td><br /><i>d</i></td><td>表示向量的维度大小</td></tr><tr><td><br /><b><i>y</i></b><sub><i>i</i></sub></td><td>节点<i>v</i><sub><i>i</i></sub>的表示向量, <b><i>y</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>d</i></sup></td></tr><tr><td><br /><b><i>Y</i></b></td><td>节点表示向量矩阵, <b><i>y</i></b><sub><i>i</i></sub>为其<i>i</i>行向量</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="85"><b>定义</b>1 用<i>G</i>= (<i>V</i>, <b><i>S</i></b>, <i>C</i>) 表示包含描述属性信息的社会网络, 其中:<i>V</i>={<i>v</i><sub>1</sub>, <i>v</i><sub>2</sub>, …, <i>v</i><sub><i>n</i></sub>}表示网络节点集合, <i>n</i>表示节点数量;<b><i>S</i></b>表示一个权重矩阵, <i>s</i><sub><i>ij</i></sub>表示节点之间的连接权重, <i>s</i><sub><i>ij</i></sub>&gt;0表示节点<i>v</i><sub><i>i</i></sub>和<i>v</i><sub><i>j</i></sub>之间存在连接关系, 反之则无连接关系;<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∪</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow></mrow></mstyle><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>表示节点对应的描述属性信息, <i>p</i><sub><i>i</i></sub>={<i>p</i><sub><i>i</i>1</sub>, <i>p</i><sub><i>i</i>2</sub>, …, <i>p</i><sub><i>im</i></sub>}表示节点<i>v</i><sub><i>i</i></sub>对应的描述属性信息, <i>m</i>表示节点描述属性信息维度。</p>
                </div>
                <div class="p1">
                    <p id="87"><b>定义</b>2 网络表示学习。给定网络<i>G</i>= (<i>V</i>, <b><i>S</i></b>, <i>C</i>) , 网络表示学习的目的是学习映射函数<i>f</i>:<i>v</i><sub><i>i</i></sub>→<b><i>y</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>d</i></sup>, 且<mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo>≪</mo><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></math></mathml>, 使得节点表示向量<b><i>y</i></b><sub><i>i</i></sub>同时保留节点间结构相似性和属性相似性信息。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">2.2 <b>模型框架</b></h4>
                <div class="p1">
                    <p id="90">本文的网络表示学习方法应当满足以下要求:一是能够应对节点描述属性信息存在的语义信息相对分散和不完备等问题, 二是能够有效保留网络结构相似性和属性相似性信息。针对上述要求, 本文提出了NPA-NRL算法, 其模型框架如图2所示。下面分别从数据编码、特征提取和数据解码三个阶段详细描述。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">2.2.1 数据编码</h4>
                <div class="p1">
                    <p id="92">为方便进一步采用深度神经网络进行特征提取, 首先分别对节点网络结构信息和描述属性信息分别进行编码表示。</p>
                </div>
                <div class="p1">
                    <p id="93">结构信息编码:对于任意网络<i>G</i>, 其邻接矩阵<b><i>S</i></b>编码了全部网络结构信息。因此, 对于任意节点<i>v</i><sub><i>i</i></sub>, 将其在<b><i>S</i></b>中的对应列向量<b><i>s</i></b><sub><i>i</i></sub>作为其结构信息的编码向量, 包含其所有的局部邻居结构信息。</p>
                </div>
                <div class="p1">
                    <p id="94">属性信息编码:在实际网络中, 节点描述属性信息通常是分类型数据, 如性别只包含{男, 女}两种情况。分类数据的主要特点是:数据模式较为复杂, 各个属性之间无直接联系, 属性值有限且没有明显的顺序和大小之分。因此, 本文采用独热编码将属性分类数据编码为二值特征向量, 例如, 一个性别为男性的节点, 该属性的表示向量为<citation id="385" type="reference">[<a class="sup">1</a>]</citation>。这样做的好处是能够保证在编码空间属性值的独立性, 保证属性值的无序性。对于任意节点<i>v</i><sub><i>i</i></sub>, 考虑到各个属性之间的独立性, 本文将各属性的表示向量拼接作为最终的节点属性信息编码向量<b><i>a</i></b><sub><i>i</i></sub>。设<b><i>a</i></b><sub><i>ij</i></sub>为节点<i>v</i><sub><i>i</i></sub>对应属性<i>p</i><sub><i>ij</i></sub>的编码向量, ♁表示向量拼接, 则</p>
                </div>
                <div class="p1">
                    <p id="95"><b><i>a</i></b><sub><i>i</i></sub>=<b><i>a</i></b><sub><i>i</i>1</sub>♁<b><i>a</i></b><sub><i>i</i>2</sub>♁…♁<b><i>a</i></b><sub><i>im</i></sub>      (1) </p>
                </div>
                <div class="p1">
                    <p id="96">针对属性信息不完备问题, 传统方法是利用统计学原理对缺失数据进行填补, 如通过对数据集已有属性值取众数和平均值等手段赋予缺失属性一个缺省值。然而这种处理方法未有效结合网络结构信息, 处理结果往往加剧了数据噪声。因此, 本文设计了两个方面的措施应对该问题:一方面将结构编码和属性编码拼接作为神经网络输入, 实现结构信息和属性信息的相互补充, 这部分工作将在下一节中详细阐述;另一方面, 受文献<citation id="386" type="reference">[<a class="sup">24</a>]</citation>启发, 通过随机扰动策略实现数据集增强。</p>
                </div>
                <div class="p1">
                    <p id="97">随机扰动策略的基本思想是:对于任意输入数据样本<b><i>a</i></b>, 本文以概率<i>ε</i>对其添加随机扰动, <i>ε</i>为扰动概率。通过随机映射<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>q</mi><mo stretchy="false"> (</mo><mrow><mover accent="true"><mi mathvariant="bold-italic">A</mi><mo>˜</mo></mover><mo>|</mo></mrow><mi mathvariant="bold-italic">a</mi><mo stretchy="false">) </mo></mrow></math></mathml>得到部分损坏的<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">A</mi><mo>˜</mo></mover></math></mathml>作为模型训练的输入向量。为贴合节点描述属性信息部分缺失的情况, 本文将随机扰动设置为随机将部分属性置零作为输入向量。通过训练过程中数据变换, 迫使神经网络挖掘更具区分性的特征表示。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">2.2.2 特征提取</h4>
                <div class="p1">
                    <p id="101">在得到节点结构编码向量和属性编码向量后, 希望通过深度神经网络获得更加抽象的数据特征, 挖掘更加准确的节点表示, 应对存在的属性信息不完备问题。</p>
                </div>
                <div class="p1">
                    <p id="102">一方面, 在输入向量构造上, 采用提前融合策略促进知识交互。如图2所示, 本文直接将节点结构信息编码向量和节点描述属性信息编码向量拼接作为深度神经网络输入。这种提前融合策略在大量端到端的深度学习模型都取得了较好性能<citation id="387" type="reference"><link href="323" rel="bibliography" /><link href="325" rel="bibliography" /><link href="327" rel="bibliography" /><link href="329" rel="bibliography" /><link href="331" rel="bibliography" /><link href="333" rel="bibliography" /><link href="335" rel="bibliography" /><link href="337" rel="bibliography" /><link href="339" rel="bibliography" /><link href="341" rel="bibliography" /><link href="343" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>, 能够有效实现两方面信息的相互补充制约。定义输入向量<b><i>x</i></b><sub><i>i</i></sub>表示如下:</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>♁</mo><mover accent="true"><mi mathvariant="bold-italic">A</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">另一方面, 在神经网络设计上, 采用塔式结构的深度前馈网络提取数据的抽象特征表示。万能近似定理表明, 深度前馈网络具有非常强大的数据拟合能力<citation id="388" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>, 在网络数据特征抽取中也表现出较好性能<citation id="389" type="reference"><link href="307" rel="bibliography" /><link href="343" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">22</a>]</sup></citation>。因此, 如图2所示, 将节点输入向量<b><i>x</i></b><sub><i>i</i></sub>送入深度前馈网络, 通过逐层减少隐藏层单元数量, 获得更加抽象的节点特征表示。记隐层输出向量为<b><i>y</i></b><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>、 <b><i>y</i></b><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>、…、 <b><i>y</i></b><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>Κ</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 具体定义如下:</p>
                </div>
                <div class="p1">
                    <p id="108"><b><i>y</i></b><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>=ReLU (<b><i>W</i></b><sup> (<i>k</i>) </sup><b><i>y</i></b><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>+<b><i>b</i></b><sup> (<i>k</i>) </sup>) ; <i>k</i>=1, 2, …, <i>K</i>      (3) </p>
                </div>
                <div class="p1">
                    <p id="111">其中:<b><i>W</i></b><sup> (<i>k</i>) </sup>和<b><i>b</i></b><sup> (<i>k</i>) </sup>分别表示第<i>k</i>层的权重矩阵和偏置向量;<i>φ</i> (<b><i>x</i></b>) 表示非线性激活函数; <b><i>y</i></b><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mn>0</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>=<b><i>x</i></b><sub><i>i</i></sub>表示输入向量。ReLU (<b><i>x</i></b>) 为线性整流函数, ReLU (<b><i>x</i></b>) =max (0, <b><i>x</i></b>) 。本文将神经网络最后一层的输出向量<b><i>y</i></b><sup> (<i>K</i>) </sup>作为最后的节点表示向量<b><i>y</i></b>, 即<b><i>y</i></b>= <b><i>y</i></b><sup> (<i>K</i>) </sup>。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113">2.2.3 数据解码</h4>
                <div class="p1">
                    <p id="114">表示学习的目的在于有效保留原始网络信息, 挖掘潜在特征表示, 更好地服务于后续的数据挖掘任务<citation id="390" type="reference"><link href="339" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。为充分挖掘原始网络语义信息, 本文算法得到的节点表示向量 <b><i>y</i></b>应当同时实现网络结构相似性保留和节点属性相似性保留。因此, 分别设计相应的优化损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="115">结构相似性保留:文献<citation id="391" type="reference">[<a class="sup">6</a>]</citation>指出, 节点邻居节点集合对于推理节点间的结构相似性关系具有重要作用。因此, 类似于SkipGram词向量模型<citation id="392" type="reference"><link href="327" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>中通过词向量来预测上下文词, 可以将节点邻居节点看作一种“上下文”, 通过最大化节点预测其邻居“上下文”节点的概率, 实现网络结构相似性保留。定义结构优化损失函数<i>L</i><sub>str</sub>如下:</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow></munder><mrow><mi>log</mi></mrow></mstyle><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>Ν</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mrow><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow></munder><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Ν</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></munder><mrow><mi>log</mi></mrow></mstyle><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mrow><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">其中:<i>Nr</i> (<i>v</i><sub><i>i</i></sub>) 表示节点<i>v</i><sub><i>i</i></sub>的邻居节点集合。预测概率<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mrow><mo stretchy="false">) </mo></mrow></math></mathml>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mrow><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">其中: <b><i>y</i></b><sub><i>i</i></sub>和<b><i>y</i></b>′<sub><i>i</i></sub>分别表示节点<i>v</i><sub><i>i</i></sub>的表示向量和作为“上下文”节点的表示向量 (属于辅助参数) 。考虑到<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mrow><mo stretchy="false">) </mo></mrow></math></mathml>分母的计算需要遍历所有网络节点, 因此, 为提高计算效率, 使用文献<citation id="393" type="reference">[<a class="sup">14</a>]</citation>提出的负采样策略, 则结构优化损失函数的简化定义如下:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow></munder><mo stretchy="false">{</mo></mstyle><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>δ</mi><mo stretchy="false"> (</mo><mrow><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∈</mo><mi>Ν</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></munder><mi mathvariant="bold">E</mi></mstyle><msub><mrow></mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>δ</mi><mo stretchy="false"> (</mo><mrow><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">其中:<i>δ</i> (<b><i>x</i></b>) =1/ (1+exp (-<b><i>x</i></b>) ) 为Sigmoid函数, 根据文献<citation id="394" type="reference">[<a class="sup">14</a>]</citation>建议, 取负样本数量为<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>5</mn><mo>, </mo><mi>Ρ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo><mo>∝</mo><mo stretchy="false">[</mo><mfrac><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mi>r</mi><mi>e</mi><mi>e</mi><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><mo>∈</mo><mi>V</mi></mrow></munder><mi>d</mi></mstyle><mi>e</mi><mi>g</mi><mi>r</mi><mi>e</mi><mi>e</mi><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow></mfrac><mo stretchy="false">]</mo><msup><mrow></mrow><mrow><mn>3</mn><mo>/</mo><mn>4</mn></mrow></msup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="125">属性相似性保留:网络同质性表明, 具有连接关系的节点间通常具有相似的属性信息<citation id="395" type="reference"><link href="313" rel="bibliography" /><link href="315" rel="bibliography" /><link href="317" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。因此, 本文提出了基于连接关系约束的属性相似性保留方法, 使得具有连接关系的节点间的表示向量在表示空间距离相近, 实现节点属性信息相似性保留。定义损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow></munder><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Ν</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></munder><mi>s</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>d</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">其中:<i>s</i><sub><i>ij</i></sub>表示节点对 (<i>v</i><sub><i>i</i></sub>, <i>v</i><sub><i>j</i></sub>) 之间的连接权重。<i>d</i> (<i>v</i><sub><i>i</i></sub>, <i>v</i><sub><i>j</i></sub>) 表示节点在低维表示空间的属性相似性度量函数, 归一化定义如下:</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>2</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">由式 (8) 可知, 节点间的属性相似性和节点间表示向量的距离成反比:当节点在低维空间的距离较近时, <i>d</i> (<i>v</i><sub><i>i</i></sub>, <i>v</i><sub><i>j</i></sub>) 趋于1;反之, <i>d</i> (<i>v</i><sub><i>i</i></sub>, <i>v</i><sub><i>j</i></sub>) 趋于0。节点对 (<i>v</i><sub><i>i</i></sub>, <i>v</i><sub><i>j</i></sub>) 之间的连接权重<i>s</i><sub><i>ij</i></sub>可以看作一个惩罚系数, 迫使模型为连接关系紧密的节点学习更加相近的表示向量。通过最小化损失函数式 (7) , 实现网络节点间的属性相似性信息保留。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130">2.3 <b>联合训练及其优化算法</b></h4>
                <div class="p1">
                    <p id="131">为有效保留原始网络信息, 实现两方面信息的融合表示, 定义联合训练优化损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="132"><i>L</i>=<i>L</i><sub>str</sub>+<i>λL</i><sub>attri</sub>      (9) </p>
                </div>
                <div class="p1">
                    <p id="133">其中:<i>λ</i>为属性相似性保留权重系数。通过调整不同权重比例, 实现两方面信息重要性的动态调整, 更好地贴合不同数据场景。记模型参数集和为<i>θ</i>=<i>θ</i><sub><i>h</i></sub>∪<i>θ</i><sub><i>c</i></sub>, 其中:<i>θ</i><sub><i>h</i></sub>为隐藏层模型参数集合, 即<i>θ</i><sub><i>h</i></sub>={<b><i>W</i></b><sup> (<i>k</i>) </sup>, <b><i>b</i></b><sup> (<i>k</i>) </sup>;<i>k</i>=1, 2, …, <i>K</i>};<i>θ</i><sub><i>c</i></sub>为节点作为“上下文”节点表示向量的辅助参数, 即<i>θ</i><sub><i>c</i></sub>={<b><i>y</i></b>′<sub><i>i</i></sub>;<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow><mo stretchy="false">}</mo></mrow></math></mathml>。模型训练过程就是最小化损失函数<i>L</i>, 通过梯度反向传播实现网络参数集<i>θ</i>更新。其中, 最关键的步骤就是求偏导<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac></mrow></math></mathml>, 下面给出具体推导过程。对式 (9) 求偏导可得:</p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac><mo>+</mo><mi>λ</mi><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="137">首先, 针对<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac></mrow></math></mathml>, 为方便求导, 对<i>L</i><sub>str</sub>变换形式如下:</p>
                </div>
                <div class="p1">
                    <p id="139" class="code-formula">
                        <mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow></munder><mspace width="0.25em" /></mstyle><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Ν</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></munder><mspace width="0.25em" /></mstyle><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable columnalign="left"><mtr><mtd><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∈</mo><mo stretchy="false">{</mo><mspace width="0.25em" /><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">}</mo><mspace width="0.25em" /><mstyle displaystyle="true"><mo>∪</mo><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mi>Ν</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></munder><mo stretchy="false">{</mo></mstyle><mspace width="0.25em" /><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mrow><mi>log</mi></mrow><mo stretchy="false">[</mo><mi>δ</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mi>log</mi></mrow><mo stretchy="false">[</mo><mn>1</mn><mo>-</mo><mi>δ</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="140">其中:<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>为指示函数, 当<i>v</i><sub><i>k</i></sub>=<i>v</i><sub><i>j</i></sub>时, <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>值为1;否则为0。下面针对单个样本进行分析, 对于样本 (<i>v</i><sub><i>i</i></sub>, <i>v</i><sub><i>j</i></sub>, <i>v</i><sub><i>k</i></sub>) , 对应结构损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mrow><mi>log</mi></mrow><mo stretchy="false">[</mo><mi>δ</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>-</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mi>log</mi></mrow><mo stretchy="false">[</mo><mn>1</mn><mo>-</mo><mi>δ</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">则</p>
                </div>
                <div class="p1">
                    <p id="145" class="code-formula">
                        <mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo>=</mo><mo>-</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">[</mo><mn>1</mn><mo>-</mo><mi>δ</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">[</mo><mn>1</mn><mo>-</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mi>δ</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mi>δ</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="146">同理, 对于<mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></mfrac></mrow></math></mathml>, 由链式法则可得, </p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149">对于<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></math></mathml>, 类似于式 (13) 可得:</p>
                </div>
                <div class="p1">
                    <p id="151" class="code-formula">
                        <mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mo stretchy="false">[</mo><mi>δ</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mo>-</mo><mspace width="0.25em" /><mi>Ι</mi><mo stretchy="false"> (</mo><mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mi mathvariant="bold-italic">y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>k</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="152">对于<mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></mfrac></mrow></math></mathml>, 由于<b><i>y</i></b><sub><i>i</i></sub>= <b><i>y</i></b><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>Κ</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>=<i>δ</i> (<b><i>W</i></b><sup> (<i>K</i>) </sup><b><i>y</i></b><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>Κ</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>+<b><i>b</i></b><sup> (<i>K</i>) </sup>) , 所以非常容易求解<mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>Κ</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac></mrow></math></mathml>和<mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>Κ</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac></mrow></math></mathml>, 基于反向传播, 可以迭代求得<mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac></mrow></math></mathml>和<mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac><mo stretchy="false"> (</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Κ</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>。<mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>r</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac></mrow></math></mathml>至此计算完成。</p>
                </div>
                <div class="p1">
                    <p id="161">下面进一步求解<mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac></mrow></math></mathml>, 由于<i>L</i><sub>attri</sub>仅为<i>θ</i><sub><i>h</i></sub>的函数, 因此, 只需要求解<mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></mfrac></mrow></math></mathml>。由链式法则可得:</p>
                </div>
                <div class="p1">
                    <p id="164" class="code-formula">
                        <mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi></mrow><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="165">为方便求导, <i>L</i><sub>attri</sub>变换形式如下:</p>
                </div>
                <div class="p1">
                    <p id="166" class="code-formula">
                        <mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow></munder><mspace width="0.25em" /></mstyle><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Ν</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></munder><mi>s</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mfrac><mn>2</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="167">下面针对单个样本进行分析, 给定样本 (<i>v</i><sub><i>i</i></sub>, <i>v</i><sub><i>j</i></sub>) , 对应损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="168"><i>L</i><sub>attri</sub> (<i>v</i><sub><i>i</i></sub>, <i>v</i><sub><i>j</i></sub>) =<i>s</i><sub><i>ij</i></sub>{log[1+exp (‖<b><i>y</i></b><sub><i>i</i></sub>-<b><i>y</i></b><sub><i>j</i></sub>‖<mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>) ]-log 2}      (18) </p>
                </div>
                <div class="p1">
                    <p id="170">则</p>
                </div>
                <div class="p1">
                    <p id="171" class="code-formula">
                        <mathml id="171"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>s</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mspace width="0.25em" /><mi>exp</mi><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mo>⋅</mo><mn>2</mn><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="172">同理可得, </p>
                </div>
                <div class="p1">
                    <p id="173" class="code-formula">
                        <mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>s</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mspace width="0.25em" /><mi>exp</mi><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mo>⋅</mo><mn>2</mn><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="174">对于<mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">y</mi></mrow><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></mfrac></mrow></math></mathml>, 基于反向传播, 可以完成相应的偏导数求解, 在此不再赘述。</p>
                </div>
                <div class="p1">
                    <p id="176">根据上述分析, NPA-NRL算法的具体描述如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="177">算法1 NPA-NRL算法。</p>
                </div>
                <div class="p1">
                    <p id="178">输入 包含描述属性信息的社会网络<i>G</i>= (<i>V</i>, <b><i>S</i></b>, <i>C</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="179">输出 网络节点表示矩阵<b><i>Y</i></b>和更新后的参数<i>θ</i>。</p>
                </div>
                <div class="p1">
                    <p id="180">预处理阶段:</p>
                </div>
                <div class="p1">
                    <p id="181">步骤1 采样边集划分训练集和验证集;</p>
                </div>
                <div class="p1">
                    <p id="182">步骤2 模型参数初始化模型参数<i>θ</i>。</p>
                </div>
                <div class="p1">
                    <p id="183">模型训练阶段:</p>
                </div>
                <div class="p1">
                    <p id="184">步骤3 定义待采样节点集合<i>V</i><sub><i>c</i></sub>=<i>V</i>={<i>v</i><sub>1</sub>, <i>v</i><sub>2</sub>, …, <i>v</i><sub><i>N</i></sub>};</p>
                </div>
                <div class="p1">
                    <p id="185">步骤4 从<i>V</i><sub><i>c</i></sub>中随机采样包含<i>bs</i>个节点的训练样本<i>V</i><sub><i>t</i></sub>, 并将<i>V</i><sub><i>t</i></sub>从待采样节点集合<i>V</i><sub><i>c</i></sub>中删除;</p>
                </div>
                <div class="p1">
                    <p id="186">步骤5 按照式 (1) ～ (2) 对<i>V</i><sub><i>t</i></sub>中节点进行编码得到神经网络输入向量矩阵<b><i>X</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="187">步骤6 计算式 (9) 的梯度, 根据相应的优化方法进行参数更新;</p>
                </div>
                <div class="p1">
                    <p id="188">步骤7 重复步骤4～6, 直到待采样节点集合<i>V</i><sub><i>c</i></sub>为空结束;</p>
                </div>
                <div class="p1">
                    <p id="189">步骤8 在验证集上计算链路预测指标AUC (Area under the Curve of ROC) 值并保存;</p>
                </div>
                <div class="p1">
                    <p id="190">步骤9 重复步骤3～8, 直到在验证集上的当前AUC值小于前10次AUC平均值结束。</p>
                </div>
                <div class="p1">
                    <p id="191">算法1在预处理阶段:一方面通过对网络边集随机采样划分训练集和验证集, 训练集用于梯度计算及参数更新, 验证集用于判断模型收敛情况;另一方面, 使用Xavier<citation id="396" type="reference"><link href="351" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>初始化方法进行神经网络隐层参数<i>θ</i><sub><i>h</i></sub>初始化, 作为“上下文”节点的辅助参数使用高斯分布进行初始化<i>θ</i><sub><i>c</i></sub>～N (0, 1) 。在模型训练阶段, 采用学习率自适应的Adam优化方法<citation id="397" type="reference"><link href="353" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>计算参数更新, 将动量并入梯度计算中, 实现算法的快速收敛。然而, 片面追求最优解并非算法的最终目标, 模型泛化能力才是关注重点, 因此, 采用提前终止策略防止模型过拟合。链路预测任务能够反映节点表示向量的网络重构能力, 因此, 本文中迭代停止条件设置为:验证集上的链路预测AUC指标值小于前10次的平均值。</p>
                </div>
                <h4 class="anchor-tag" id="192" name="192">2.4 <b>算法时间复杂度分析</b></h4>
                <div class="p1">
                    <p id="193">NPA-NRL算法训练过程包括数据编码和参数更新两部分, 下面分别进行相应的时间复杂度分析。</p>
                </div>
                <div class="p1">
                    <p id="194">在输入数据编码过程中, 引入了随机噪声到数据编码中, 时间复杂度为<i>O</i> (<i>nd</i>) , 其中:<i>n</i>为网络中的节点个数, <i>d</i>为迭代次数。该过程的复杂度与输入数据规模呈线性关系。</p>
                </div>
                <div class="p1">
                    <p id="195">在模型训练参数更新过程中, 对于单个训练数据, 前向传播过程中, 给定第<i>i</i>层的计算时间复杂度为<i>O</i> (<i>n</i><sub>in</sub> (<i>i</i>) <i>n</i><sub>out</sub> (<i>i</i>) +<i>n</i><sub>out</sub> (<i>i</i>) ) 。设网络中节点平均邻居为<mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover></math></mathml><sub><i>r</i></sub>, 由式 (6) 可知, 基于网络结构相似性保留的损失函数<i>L</i><sub>str</sub>计算时间复杂度为<mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>。</mo></mrow></math></mathml>由式 (7) 可知, 基于属性相似性保留的损失函数<i>L</i><sub>attri</sub>计算时间复杂度为<i>O</i> (<mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover></math></mathml><sub><i>r</i></sub>) 。梯度反向传播过程中, 给定第<i>i</i>层的计算时间复杂度也为<i>O</i> (<i>n</i><sub>in</sub> (<i>i</i>) <i>n</i><sub>out</sub> (<i>i</i>) +<i>n</i><sub>out</sub> (<i>i</i>) ) 。因此, 单个样本完成一次训练的时间复杂度为<mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>2</mn><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mn>2</mn></mstyle><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="200">综上所述, 由于<mathml id="201"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ν</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>r</mi></msub><mo>, </mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>, </mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>≪</mo><mi>n</mi><mo>, </mo><mtext>Ν</mtext><mtext>Ρ</mtext><mtext>A</mtext><mo>-</mo><mtext>Ν</mtext><mtext>R</mtext><mtext>L</mtext></mrow></math></mathml>算法整体时间复杂度为O (n) , 与网络规模呈线性关系, 适合大规模数据处理中的实际应用。</p>
                </div>
                <h3 id="202" name="202" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="203" name="203">3.1 <b>实验设定</b></h4>
                <h4 class="anchor-tag" id="204" name="204">3.1.1 实验数据集</h4>
                <div class="p1">
                    <p id="205">本文在三个真实网络数据集上进行实验仿真, 实验数据集统计指标如表2所示。数据集介绍如下:</p>
                </div>
                <div class="p1">
                    <p id="206"><i>GPLUS</i> (<i>Google Plus</i>) <citation id="398" type="reference"><link href="355" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>:来自一个<i>Google Plus</i>用户的自中心网络, 节点表示用户的“朋友”, 连边表示有社交联系的用户。包含4 450个节点和1 473 709条连边, 每一个节点包含6个用户描述属性信息:性别、机构、职业、姓氏、地区、大学。本文使用性别作为类别标签。</p>
                </div>
                <div class="p1">
                    <p id="207"><i>OKLAHOMA</i>、<i>UNC</i><citation id="399" type="reference"><link href="319" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>: 来自<i>Traud</i>等<citation id="400" type="reference"><link href="319" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>以高校为单位采集的<i>Facebook</i>社交关系数据集中的两个。其中<i>OKLAHOMA</i>包含17 425个节点和892 528条连边, <i>UNC</i>包含18 163个节点和766 800条连边。每一个节点包含7个用户描述属性信息:学生/教师状态标记、性别、主修、辅修、住址、年级、高中。本文使用状态标记作为类别标签。</p>
                </div>
                <div class="area_img" id="208">
                    <p class="img_tit"><b>表</b>2 <b>实验数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Experimental datasets</i></p>
                    <p class="img_note"></p>
                    <table id="208" border="1"><tr><td><br />数据集</td><td>节点数</td><td>连边数</td><td>集聚系数</td></tr><tr><td><br /><i>GPLUS</i></td><td>4 450</td><td>1 473 709</td><td>0.468</td></tr><tr><td><br /><i>OKLAHOMA</i></td><td>17 425</td><td>892 528</td><td>0.230</td></tr><tr><td><br /><i>UNC</i></td><td>18 163</td><td>766 800</td><td>0.202</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="209" name="209">3.1.2 评价任务及其指标</h4>
                <div class="p1">
                    <p id="210">本文通过现有文献中最为常见的链路预测和节点分类两个任务, 来评测网络表示学习算法性能。链路预测任务用来评价节点表示的重构网络结构能力, 节点分类任务用于评价节点表示能否有效应用到后续的网络分析任务。</p>
                </div>
                <h4 class="anchor-tag" id="211" name="211">1) 链路预测。</h4>
                <div class="p1">
                    <p id="212">参考文献<citation id="402" type="reference">[<a class="sup">4</a>,<a class="sup">22</a>]</citation>, 随机选择10%网络连边作为测试集, 另10%作为验证集用于超参数调节, 剩余80%作为训练集。本文使用AUC指标<citation id="401" type="reference"><link href="357" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>从整体上衡量算法的精确度。</p>
                </div>
                <div class="p1">
                    <p id="213">在链路预测实验中, 使用余弦相似性作为节点间具有连边的得分值。AUC可以理解为在测试集中边的得分值比随机选择一条不存在的边的分数值高的概率。独立比较<i>n</i>次, 记测试集中边的分数值大于不存在的边的分数值的比较次数为<i>n</i>′, 分数值相等的比较次数为<i>n</i>", 则AUC定义为:</p>
                </div>
                <div class="p1">
                    <p id="214" class="code-formula">
                        <mathml id="214"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>U</mi><mi>C</mi><mo>=</mo><mfrac><mrow><msup><mi>n</mi><mo>′</mo></msup><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>n</mi><mo>"</mo></mrow><mi>n</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="215" name="215">2) 节点分类。</h4>
                <div class="p1">
                    <p id="216">模型训练超参数设置与链路预测任务中参数设置保持一致。训练集中包含80%的网络连边和所有的节点描述属性信息 (不包括类别标签) 。然后, 对于所有模型, 使用LibLinear工具包<citation id="403" type="reference"><link href="359" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>中的线性支持向量机 (Support Vector Machine, SVM) 构建分类器。为反映表示向量在所有节点上的分类性能综合评价, 本文采用<i>F</i><sub>1</sub>指标作为衡量表示向量在所有节点上的分类性能的综合评价指标。</p>
                </div>
                <div class="p1">
                    <p id="217">在多类别节点分类实验中, 对于任意标签<i>A</i>, <i>TP</i> (<i>A</i>) 、<i>FP</i> (<i>A</i>) 和<i>FN</i> (<i>A</i>) 分别代表预测结果为标签<i>A</i>的预测正确的正类数、预测错误的负类数和预测错误的正类数, <i>C</i>是全部标签列表的集合。则<i>F</i><sub>1</sub>定义为:</p>
                </div>
                <div class="p1">
                    <p id="218" class="code-formula">
                        <mathml id="218"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>⋅</mo><mi>R</mi></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>+</mo><mi>R</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="219">其中:</p>
                </div>
                <div class="p1">
                    <p id="220" class="code-formula">
                        <mathml id="220"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>A</mi><mo>∈</mo><mi>C</mi></mrow></munder><mi>Τ</mi></mstyle><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>A</mi><mo>∈</mo><mi>C</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mi>Τ</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>A</mi><mo>∈</mo><mi>C</mi></mrow></munder><mi>Τ</mi></mstyle><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>A</mi><mo>∈</mo><mi>C</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mi>Τ</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>+</mo><mi>F</mi><mi>Ν</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="221" name="221">3.1.3 对比算法及其参数设置</h4>
                <div class="p1">
                    <p id="222">实验对比算法如下:</p>
                </div>
                <div class="p1">
                    <p id="223"><i>DeepWalk</i><citation id="404" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>:基于截断随机游走和<i>SkipGram</i>模型获得节点表示向量。该算法仅利用了网络结构信息, 作为其他融合算法的参考。</p>
                </div>
                <div class="p1">
                    <p id="224"><i>TADW</i><citation id="405" type="reference"><link href="321" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>:基于矩阵分解形式的融合算法, 结合节点属性信息矩阵进行联合矩阵分解, 获得节点的融合表示向量。该算法作为基于矩阵分解的融合对比算法。</p>
                </div>
                <div class="p1">
                    <p id="225"><i>UPP</i>-<i>SNE</i><citation id="406" type="reference"><link href="329" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>:在<i>DeepWalk</i>算法基础上, 改进<i>SkipGram</i>模型, 通过向量内积实现两方面信息的知识交互。该算法作为基于随机游走的融合对比算法。</p>
                </div>
                <div class="p1">
                    <p id="226"><i>SNE</i><citation id="407" type="reference"><link href="343" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>:基于深度神经网络的融合算法, 利用深度神经网络的非线性特征提取能力, 挖掘节点间的复杂非线性关系, 实现节点融合表示向量特征提取。该算法作为基于深度神经网络的融合对比算法。</p>
                </div>
                <div class="p1">
                    <p id="227"><i>NPA</i>-<i>NRL</i>:本文提出的一种基于深度神经网络的融合算法。通过在训练过程引入基于网络同质性原理的属性相似性度量函数以及基于随机扰动的数据集增强策略, 有效应对节点描述属性信息存在的语义信息分散和属性不完备问题。<i>NPA</i>-<i>NRL</i> (-) 是该算法的退化算法, 在训练过程中未加入随机扰动, 作为对比算法验证增加属性相似性度量函数和数据集增强策略发挥的不同作用。</p>
                </div>
                <div class="p1">
                    <p id="228">本文中对比算法的实验参数参考相应文献的推荐设置:所有算法的表示向量维度统一设置为128维, 与文献<citation id="408" type="reference">[<a class="sup">15</a>]</citation>保持一致。除表示向量维度外, 基于矩阵分解的<i>SVD</i>算法和<i>UPP</i>-<i>SNE</i>算法无其他参数。基于随机游走的<i>DeepWalk</i>算法和<i>UPP</i>-<i>SNE</i>算法按照文献<citation id="409" type="reference">[<a class="sup">15</a>]</citation>中提供参数进行设置, 每个节点的随机游走次数<i>r</i>=40, 随机游走长度<i>l</i>=40, 窗口大小<i>t</i>=10, 迭代次数<i>d</i>=40。基于深度神经网络的SNE算法按照<citation id="410" type="reference">[<a class="sup">22</a>]</citation>中提供的最优参数进行设置, 对于新增数据集GPLUS, 批量训练大小参数为<i>bs</i>=64, 初始学习率为<i>η</i>=0.000 1, 输入属性串接权重<i>λ</i>=0.8。本文NPA-NRL算法的参数设置参考文献<citation id="411" type="reference">[<a class="sup">22</a>]</citation>思路, 通过比较给定参数空间内验证集上的链路预测性能, 得到最优参数设定如表3所示。</p>
                </div>
                <div class="area_img" id="229">
                    <p class="img_tit"><b>表</b>3 <b>最优超参数设定</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Optimal hyper-parameter settings</p>
                    <p class="img_note"></p>
                    <table id="229" border="1"><tr><td><br />数据集</td><td><i>ε</i></td><td><i>bs</i></td><td><i>λ</i></td></tr><tr><td><br />GPLUS</td><td>0.5</td><td>64</td><td>0.4</td></tr><tr><td><br />OKLAHOMA</td><td>0.5</td><td>128</td><td>0.3</td></tr><tr><td><br />UNC</td><td>0.5</td><td>128</td><td>0.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="230" name="230">3.2 <b>结果分析</b></h4>
                <h4 class="anchor-tag" id="231" name="231">3.2.1 链路预测实验分析</h4>
                <div class="p1">
                    <p id="232">表4记录了<i>NPA</i>-<i>NRL</i>算法和其他对比算法在三个实验数据集上的链路预测AUC值。从实验结果可看出, 相对于其他对比算法, 本文算法性能提高了0.83%～6.30%, 平均提升2.75%。融合算法 (<i>TADW</i>、<i>UPP</i>-<i>SNE</i>、<i>SNE</i>和<i>NPA</i>-<i>NRL</i>) 和仅考虑网络结构信息的<i>DeepWalk</i>算法相比, 平均提高3.14%, 这也验证了融合网络节点属性信息进行表示学习的重要意义。</p>
                </div>
                <div class="area_img" id="233">
                    <p class="img_tit"><b>表</b>4 <b>链路预测性能</b>AUC<b>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 4 AUC <i>score of link prediction</i></p>
                    <p class="img_note"></p>
                    <table id="233" border="1"><tr><td><br />算法</td><td><i>GPLUS</i></td><td><i>OKLAHOMA</i></td><td><i>UNC</i></td></tr><tr><td><br /><i>DeepWalk</i></td><td>0.889</td><td>0.936</td><td>0.929</td></tr><tr><td><br /><i>TADW</i></td><td>0.908</td><td>0.950</td><td>0.938</td></tr><tr><td><br /><i>UPP</i>-<i>SNE</i></td><td>0.926</td><td>0.957</td><td>0.947</td></tr><tr><td><br /><i>SNE</i></td><td>0.931</td><td>0.961</td><td>0.958</td></tr><tr><td><br /><i>NPA</i>-<i>NRL</i></td><td>0.945</td><td>0.969</td><td>0.970</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="234" name="234">3.2.2 节点分类实验分析</h4>
                <div class="p1">
                    <p id="235">下面通过节点分类实验, 进一步分析网络节点表示应用到后续网络分析任务中性能表现。通过设置不同比例的有标签数据训练分类器, 评估各算法得到的表示向量的节点分类性能。每个训练率下, 重复50次实验, 记节点分类的平均F<sub>1</sub>值如表5～7所示 (括号内为标准差) , 下面进行具体分析:</p>
                </div>
                <div class="p1">
                    <p id="236">首先, 和链路预测结果一样, 仅利用网络结构信息的<i>DeepWalk</i>算法性能最低, 这再次验证节点属性信息对于更加精确的节点表示的重要作用。其次, 本文算法与同样考虑两方面信息的<i>TADW</i>算法及<i>UPP</i>-<i>SNE</i>算法相比仍然具有优势。在训练数据较少的情况下, <i>TADW</i>算法在<i>OKLAHOMA</i>和<i>UNC</i>两个数据集上的节点分类性能甚至低于<i>DeepWalk</i>算法。这是因为<i>TADW</i>算法是基于线性映射的方式得到节点表示向量, 难以有效应对节点描述属性信息中数据不完备问题。相比之下, <i>UPP</i>-<i>SNE</i>算法利用<i>SkipGram</i>模型有效实现两方面信息的融合, 节点分类性能进一步提高。然而<i>SkipGram</i>模型这种浅层模型难以有效挖掘节点网络结构信息和描述属性信息之间的复杂非线性关系, 因此性能并非最优。</p>
                </div>
                <div class="p1">
                    <p id="237">最后, 和同样采用深度神经网络进行融合表示学习的</p>
                </div>
                <div class="p1">
                    <p id="238"><i>SNE</i>算法相比, 本文算法利用节点描述属性信息的能力上效果更显著:一方面, 由于在训练过程中考虑基于连接关系约束的属性相似性保留优化损失函数, <i>NPA</i>-<i>NRL</i> (-) 算法更加充分利用了节点属性同质性信息, 在三个数据集上的节点分类性能平均提高2.28%;另一方面, 通过在训练过程中引入对输入数据的随机扰动, 使得<i>NPA</i>-<i>NRL</i>算法能够在节点属性信息不完备的情况下, 获得具有鲁棒性的表示向量。在训练率仅为2%的情况下, 本文<i>NPA</i>-<i>NRL</i>算法和<i>SNE</i>算法相比, 节点分类性能分别提高了6.05% (<i>GPLUS</i>) 、6.13% (<i>OKLAHOMA</i>) 和4.12% (<i>UNC</i>) , 表明<i>NPA</i>-<i>NRL</i>算法所得表示向量具有较强区分性。和对比算法相比, <i>NPA</i>-<i>NRL</i>算法在三个数据集上的节点分类性能平均提升7.10%。</p>
                </div>
                <h4 class="anchor-tag" id="239" name="239">3.3 <b>参数敏感性分析</b></h4>
                <div class="p1">
                    <p id="240">本节进一步分析超参数对<i>NPA</i>-<i>NRL</i>算法性能影响。如图3所示, 测试了算法在不同参数选择下的节点分类性能指标<i>F</i><sub>1</sub>值的变化趋势。在实验过程中, 除测试参数外, 其余参数设定为默认值, 分类器训练数据比例设置为10%。</p>
                </div>
                <div class="area_img" id="241">
                    <p class="img_tit"><b>表</b>5 <b>不同算法在</b>GPLUS<b>网络上的节点分类性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Comparison of node classification performance of different algorithms on GPLUS network</p>
                    <p class="img_note"></p>
                    <table id="241" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="7"><br />不同训练率下的<i>F</i><sub>1</sub>值</td></tr><tr><td><br />2%</td><td>4%</td><td>6%</td><td>8%</td><td>10%</td><td>20%</td><td>40%</td></tr><tr><td><br />DeepWalk</td><td>0.542 (±0.012) </td><td>0.604 (±0.010) </td><td>0.635 (±0.012) </td><td>0.652 (±0.009) </td><td>0.664 (±0.007) </td><td>0.671 (±0.005) </td><td>0.684 (±0.007) </td></tr><tr><td><br />TADW</td><td>0.559 (±0.013) </td><td>0.618 (±0.010) </td><td>0.654 (±0.090) </td><td>0.682 (±0.090) </td><td>0.707 (±0.006) </td><td>0.714 (±0.005) </td><td>0.722 (±0.005) </td></tr><tr><td><br />UPP-SNE</td><td>0.668 (±0.009) </td><td>0.675 (±0.008) </td><td>0.683 (±0.009) </td><td>0.691 (±0.009) </td><td>0.695 (±0.008) </td><td>0.703 (±0.007) </td><td>0.711 (±0.008) </td></tr><tr><td><br />SNE</td><td>0.678 (±0.009) </td><td>0.690 (±0.008) </td><td>0.699 (±0.008) </td><td>0.715 (±0.008) </td><td>0.725 (±0.006) </td><td>0.733 (±0.004) </td><td>0.739 (±0.006) </td></tr><tr><td><br />NPA-NRL (-) </td><td>0.697 (±0.006) </td><td>0.705 (±0.006) </td><td>0.714 (±0.007) </td><td>0.727 (±0.006) </td><td>0.736 (±0.005) </td><td>0.742 (±0.003) </td><td>0.746 (±0.002) </td></tr><tr><td><br />NPA-NRL</td><td>0.719 (±0.005) </td><td>0.721 (±0.004) </td><td>0.728 (±0.005) </td><td>0.740 (±0.004) </td><td>0.747 (±0.003) </td><td>0.750 (±0.003) </td><td>0.752 (±0.003) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="242">
                    <p class="img_tit"><b>表</b>6 <b>不同算法在</b>OKLAHOMA<b>网络上的节点分类性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 6 Comparison of node classification performance of different algorithms on OKLAHOMA network</p>
                    <p class="img_note"></p>
                    <table id="242" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="7"><br />不同训练率下的<i>F</i><sub>1</sub>值</td></tr><tr><td><br />2%</td><td>4%</td><td>6%</td><td>8%</td><td>10%</td><td>20%</td><td>40%</td></tr><tr><td><br />DeepWalk</td><td>0.829 (±0.008) </td><td>0.847 (±0.008) </td><td>0.862 (±0.007) </td><td>0.874 (±0.008) </td><td>0.882 (±0.005) </td><td>0.887 (±0.004) </td><td>0.890 (±0.005) </td></tr><tr><td><br />TADW</td><td>0.796 (±0.008) </td><td>0.824 (±0.007) </td><td>0.848 (±0.008) </td><td>0.870 (±0.006) </td><td>0.875 (±0.005) </td><td>0.892 (±0.003) </td><td>0.897 (±0.005) </td></tr><tr><td><br />UPP-SNE</td><td>0.824 (±0.008) </td><td>0.868 (±0.007) </td><td>0.876 (±0.007) </td><td>0.891 (±0.007) </td><td>0.897 (±0.006) </td><td>0.910 (±0.006) </td><td>0.915 (±0.005) </td></tr><tr><td><br />SNE</td><td>0.832 (±0.008) </td><td>0.865 (±0.008) </td><td>0.887 (±0.007) </td><td>0.896 (±0.007) </td><td>0.911 (±0.006) </td><td>0.922 (±0.003) </td><td>0.928 (±0.004) </td></tr><tr><td><br />NPA-NRL (-) </td><td>0.869 (±0.006) </td><td>0.898 (±0.006) </td><td>0.913 (±0.007) </td><td>0.920 (±0.006) </td><td>0.928 (±0.004) </td><td>0.939 (±0.002) </td><td>0.944 (±0.004) </td></tr><tr><td><br />NPA-NRL</td><td>0.883 (±0.005) </td><td>0.911 (±0.004) </td><td>0.924 (±0.004) </td><td>0.929 (±0.004) </td><td>0.936 (±0.003) </td><td>0.945 (±0.002) </td><td>0.948 (±0.003) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="243">
                    <p class="img_tit"><b>表</b>7 <b>不同算法在</b>UNC<b>网络上的节点分类性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 7 Comparison of node classification performance of different algorithms on UNC network</p>
                    <p class="img_note"></p>
                    <table id="243" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="7"><br />不同训练率下的<i>F</i><sub>1</sub>值</td></tr><tr><td><br />2%</td><td>4%</td><td>6%</td><td>8%</td><td>10%</td><td>20%</td><td>40%</td></tr><tr><td><br />DeepWalk</td><td>0.814 (±0.008) </td><td>0.833 (±0.008) </td><td>0.848 (±0.008) </td><td>0.859 (±0.007) </td><td>0.868 (±0.005) </td><td>0.877 (±0.004) </td><td>0.883 (±0.004) </td></tr><tr><td><br />TADW</td><td>0.802 (±0.008) </td><td>0.810 (±0.007) </td><td>0.811 (±0.007) </td><td>0.837 (±0.005) </td><td>0.846 (±0.005) </td><td>0.864 (±0.004) </td><td>0.874 (±0.005) </td></tr><tr><td><br />UPP-SNE</td><td>0.828 (±0.007) </td><td>0.856 (±0.008) </td><td>0.871 (±0.007) </td><td>0.878 (±0.007) </td><td>0.883 (±0.006) </td><td>0.894 (±0.004) </td><td>0.898 (±0.005) </td></tr><tr><td><br />SNE</td><td>0.849 (±0.008) </td><td>0.861 (±0.007) </td><td>0.873 (±0.007) </td><td>0.884 (±0.006) </td><td>0.893 (±0.006) </td><td>0.905 (±0.002) </td><td>0.912 (±0.003) </td></tr><tr><td><br />NPA-NRL (-) </td><td>0.868 (±0.007) </td><td>0.884 (±0.006) </td><td>0.896 (±0.006) </td><td>0.906 (±0.006) </td><td>0.914 (±0.005) </td><td>0.922 (±0.002) </td><td>0.929 (±0.003) </td></tr><tr><td><br />NPA-NRL</td><td>0.884 (±0.005) </td><td>0.895 (±0.006) </td><td>0.907 (±0.005) </td><td>0.915 (±0.005) </td><td>0.922 (±0.004) </td><td>0.926 (±0.003) </td><td>0.932 (±0.002) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="244">首先, 从如图3 (a) 的对比实验中发现, 随着表示向量维度增加, 节点分类性能逐步提升, 当表示向量增加到一定程度后, 算法性能有轻微的增长且趋于稳定。同时发现:在OKLAHOMA和UNC两个相对稀疏的网络中, 节点分类性能随着表示向量维度增加, 节点分类性能提升较快。这是因为在稀疏网络中, 节点结构特征相对分散, 需要更多的维度来表示节点间的相似性和差异性。因此, 本文算法仿真中设置默认值为128, 在保证较优性能的同时使用较少的特征维度。</p>
                </div>
                <div class="p1">
                    <p id="245">其次, 通过调整<i>λ</i>取值, 进一步研究属性权重大小对节点分类性能的影响。当<i>λ</i>较小时, 随着<i>λ</i>增加, 节点分类性能逐渐提升。如图3 (b) 所示, 当<i>λ</i>在0.3附近时在三个数据集中均取得较优性能。当<i>λ</i>较大时, 节点分类性能逐渐下降, 这是因为较大的<i>λ</i>使得所有节点表示向量趋于相同, 减弱了节点间结构特征信息对表示向量的影响, 导致分类性能下降。因此, 本文算法仿真中设置默认值为0.3。</p>
                </div>
                <div class="p1">
                    <p id="246">最后, 图3 (c) 给出了节点分类<i>F</i><sub>1</sub>值随随机扰动概率的变化趋势。随着随机扰动概率增加, 节点分类性能逐渐提升。当<i>ε</i>超过一定值时, 节点分类性能逐渐下降, 说明过量随机噪声引入导致节点属性的关键信息丢失。因此, 本文算法仿真中设置默认值为0.5。</p>
                </div>
                <div class="area_img" id="247">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904014_247.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 超参数对节点分类性能影响" src="Detail/GetImg?filename=images/JSJY201904014_247.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 超参数对节点分类性能影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904014_247.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Influence of hyper-parameters to node classification performance</p>

                </div>
                <h3 id="248" name="248" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="249">本文针对融合节点描述属性进行表示学习过程中面临的节点描述属性信息语义信息分散和信息不完备问题, 分别提出了两项优化策略:一是设计了基于网络同质性原理的节点属性相似性度量函数和基于<i>SkipGram</i>模型的结构相似性度量函数, 通过联合训练实现融合语义信息挖掘。二是设计了基于随机扰动的数据集增强策略, 结合优先融合策略, 有效实现网络结构信息和描述属性信息的相互补充制约, 在属性信息不完备的情况下, 能够获得较为鲁棒的节点表示。实验结果表明, 本文算法能够有效提升节点表示向量在链路预测和节点分类任务中性能。然而, 本文算法仅适用于同质信息网络的表示学习, 网络同质性原理在异质信息网络中不再成立。后续将对此展开深入分析研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="301">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201404001&amp;v=MDU5MDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWN3ZPTHl2U2RMRzRIOVhNcTQ5RlpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>丁兆云, 贾焰, 周斌.微博数据挖掘研究综述[J].计算机研究与发展, 2014, 51 (4) :691-706. (DING Z Y, JIA Y, ZHOU B.Survey of data mining for microblogs[J].Journal of Computer Research and Development, 2014, 51 (4) :691-706.) 
                            </a>
                        </p>
                        <p id="303">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepwalk:Online learning of social representations">

                                <b>[2]</b>PEROZZI B, AL-RFOU R, SKIENA S.Deep Walk:online learning of social representations[C]//KDD 2014:Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2014:701-710.
                            </a>
                        </p>
                        <p id="305">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LINE:large-scale information network embedding">

                                <b>[3]</b>TANG J, QU M, WANG M, et al.LINE:large-scale information network embedding[C]//WWW 2015:Proceedings of the 24th International Conference on World Wide Web.Geneva, Switzerland:International World Wide Web Conferences Steering Committee, 2015:1067-1077.
                            </a>
                        </p>
                        <p id="307">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structural deep network embedding">

                                <b>[4]</b>WANG D, CUI P, ZHU W.Structural deep network embedding[C]//KDD 2016:Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2016:1225-1234.
                            </a>
                        </p>
                        <p id="309">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201610002&amp;v=MjM3ODVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd2T0x6N0Jkckc0SDlmTnI0OUZab1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>李志宇, 梁循, 周小平, 等.一种大规模网络中基于节点结构特征映射的链接预测方法[J].计算机学报, 2016, 39 (10) :1947-1964. (LI Z Y, LIANG X, ZHOU X P, et al.A link prediction method for large-scale networks[J].Chinese Journal of Computers, 2016, 39 (10) :1947-1964.) 
                            </a>
                        </p>
                        <p id="311">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predictive network representation learning for link prediction">

                                <b>[6]</b>WANG Z, CHEN C, LI W.Predictive network representation learning for link prediction[C]//SIGIR 2017:Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2017:969-972.
                            </a>
                        </p>
                        <p id="313">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Community Detection in Networks with Node Attributes">

                                <b>[7]</b>YANG J, Mc AULEY J, LESKOVEC J.Community detection in networks with node attributes[C]//ICDM 2013:Proceedings of the2013 IEEE 13th International Conference on Data Mining.Piscataway, NJ:IEEE, 2013:1151-1156.
                            </a>
                        </p>
                        <p id="315">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SARD&amp;filename=SARD14080500000646&amp;v=MTg0MTVsVXIzSUtGc1FiaFE9Tml6WmFySzhIdG5NcW85RlpPc1BDbmcvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Mc PHERSON M, SMITH-LOVIN L, COOK J M.Birds of a feather:homophily in social networks[J].Annual Review of Sociology, 2001, 27 (1) :415-444.
                            </a>
                        </p>
                        <p id="317">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000006044&amp;v=MDkxNjdsVXIzSUtGc1FiaFE9TmlmSVk3SzdIdGpOcjQ5RlpPc0pESGc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>AIELLO L M, BARRAT A, SCHIFANELLA R, et al.Friendship prediction and homophily in social media[J].ACM Transactions on the Web, 2012, 6 (2) :1-33.
                            </a>
                        </p>
                        <p id="319">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100529270&amp;v=MDAzMzFJS0ZzUWJoUT1OaWZPZmJLN0h0RE9ybzlGWWVrR0RuczVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyMw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>TRAUD A L, MUCHA P J, PORTER M A.Social structure of Facebook networks[J].Physica A:Statistical Mechanics and its Applications, 2012, 391 (16) :4165-4180.
                            </a>
                        </p>
                        <p id="321">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network representation learning with rich text informa-tion">

                                <b>[11]</b>YANG C, LIU Z, ZHAO D, et al.Network representation learning with rich text information[C]//IJCAI 2015:Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:2111-2117.
                            </a>
                        </p>
                        <p id="323">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Homophily,structure,and content augmented network representation learning">

                                <b>[12]</b>ZHANG D, YIN J, ZHU X, et al.Homophily, structure, and content augmented network representation learning[C]//ICDM2016:Proceedings of the 16th IEEE International Conference on Data Mining Series.Piscataway, NJ:IEEE, 2016:609-618.
                            </a>
                        </p>
                        <p id="325">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Variation autoencoder based network representation learning for classification">

                                <b>[13]</b>LI H, WANG H, YANG Z, et al.Variation autoencoder based network representation learning for classification[C]//ACL2017:Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA, USA:ACL, 2017:56-61.
                            </a>
                        </p>
                        <p id="327">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">

                                <b>[14]</b>MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[C]//NIPS 2013:Proceedings of the Twenty-Seventh Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2013:3111-3119.
                            </a>
                        </p>
                        <p id="329">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=User profile preserving social network embedding">

                                <b>[15]</b>ZHANG D, YIN J, ZHU X, et al.User profile preserving social network embedding[C]//IJCAI 2017:Proceedings of the 26th International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:3378-3384.
                            </a>
                        </p>
                        <p id="331">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Random Features for Large-Scale Kernel Machines">

                                <b>[16]</b>RAHIMI A, RECHT B.Random features for large-scale kernel machines[C]//NIPS 2008:Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2008:1177-1184.
                            </a>
                        </p>
                        <p id="333">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201803017&amp;v=MzIxNzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3dk9OeWZUYkxHNEg5bk1ySTlFWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>温雯, 黄家明, 蔡瑞初, 等.一种融合节点先验信息的图表示学习方法[J].软件学报, 2018, 29 (3) :786-798. (WEN W, HUANG J M, CAI R C, et al.Graph embedding by incorporating prior knowledge on vertex information[J].Journal of Software, 2018, 29 (3) :786-798.) 
                            </a>
                        </p>
                        <p id="335">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast network embedding enhancement via high order proximity approximation">

                                <b>[18]</b>YANG C, SUN M, LIU Z, et al.Fast network embedding enhancement via high order proximity approximation[C]//IJCAI2017:Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2017:19-25.
                            </a>
                        </p>
                        <p id="337">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerated attributed network embedding">

                                <b>[19]</b>HUANG X, LI J, HU X.Accelerated attributed network embedding[C]//SDM 2017:Proceedings of the 2017 SIAM International Conference on Data Mining.Philadelphia:SIAM, 2017:633-641.
                            </a>
                        </p>
                        <p id="339">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[20]</b>Le CUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436-444.
                            </a>
                        </p>
                        <p id="341">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for learning graph representations">

                                <b>[21]</b>CAO S, LU W, XU Q.Deep neural networks for learning graph representations[C]//AAAI 2016:Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2016:1145-1152.
                            </a>
                        </p>
                        <p id="343">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attributed social network embedding">

                                <b>[22]</b>LIAO L, HE X, ZHANG H, et al.Attributed social network embedding[J].IEEE Transactions on Knowledge and Data Engineering, 2018, 30 (12) :2257-2270.
                            </a>
                        </p>
                        <p id="345">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Representation learning on graphs:methods and applications">

                                <b>[23]</b>HAMILTON W L, YING R, LESKOVEC J.Representation learning on graphs:methods and applications[EB/OL].[2018-05-10].https://arxiv.org/pdf/1709.05584.
                            </a>
                        </p>
                        <p id="347">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Creating artificial neural networks that generalize">

                                <b>[24]</b>SIETSMA J, DOW R J F.Creating artificial neural networks that generalize[J].Neural Networks, 1991, 4 (1) :67-79.
                            </a>
                        </p>
                        <p id="349">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multilayer feedforward networks are universal approximators">

                                <b>[25]</b>HORNIK K, STINCHCOMBE M, WHITE H.Multilayer feedforward networks are universal approximators[J].Neural Networks, 1989, 2 (5) :359-366.
                            </a>
                        </p>
                        <p id="351">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">

                                <b>[26]</b>GLOROT X, BENGIO Y.Understanding the difficulty of training deep feedforward neural networks[C]//AISTATS 2010:Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics.Cambridge, MA:MIT Press, 2010:249-256.
                            </a>
                        </p>
                        <p id="353">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">

                                <b>[27]</b>KINGMA D P, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-05-10].https://arxiv.org/pdf/1412.6980.
                            </a>
                        </p>
                        <p id="355">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to discover social circles in ego networks">

                                <b>[28]</b>LESKOVEC J, MCAULEY J J.Learning to discover social circles in ego networks[C]//NIPS 2012:Proceedings of the Twenty-sixth Annual Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:539-547.
                            </a>
                        </p>
                        <p id="357">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DKDX201005006&amp;v=MjA4MjZHNEg5SE1xbzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3dk9JU2JQZHI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b>吕琳媛.复杂网络链路预测[J].电子科技大学学报, 2010, 39 (5) :651-661. (LYU L Y.Link prediction on complex networks[J].Journal of University of Electronic Science and Technology of China, 2010, 39 (5) :651-661.) 
                            </a>
                        </p>
                        <p id="359">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LIBLINEAR: A library for large linear classification">

                                <b>[30]</b>FAN R E, CHANG K W, HSIEH C J, et al.LIBLINEAR:a library for large linear classification[J].Journal of Machine Learning Research, 2008, 9:1871-1874.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904014" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904014&amp;v=MzA5NDBGckNVUjdxZlp1WnNGeURnVjd2T0x6N0JkN0c0SDlqTXE0OUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
