<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637138991862447500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201903015%26RESULT%3d1%26SIGN%3dXPOanJnoIZW%252fwN8QoQ5NZQsttxw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903015&amp;v=MjQ0NTlHRnJDVVI3cWZadVpwRmlEbFc3clBMejdCZDdHNEg5ak1ySTlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#85" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="1 相关研究 ">1 相关研究</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="1.1 &lt;b&gt;传统方法&lt;/b&gt;">1.1 <b>传统方法</b></a></li>
                                                <li><a href="#95" data-title="1.2 &lt;b&gt;深度学习方法&lt;/b&gt;">1.2 <b>深度学习方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#100" data-title="2 算法设计 ">2 算法设计</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#102" data-title="2.1 Faster R-CNN&lt;b&gt;整体结构&lt;/b&gt;">2.1 Faster R-CNN<b>整体结构</b></a></li>
                                                <li><a href="#107" data-title="2.2 SSD&lt;b&gt;整体结构&lt;/b&gt;">2.2 SSD<b>整体结构</b></a></li>
                                                <li><a href="#112" data-title="2.3 &lt;b&gt;改进方法&lt;/b&gt;">2.3 <b>改进方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="3 实验 ">3 实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#138" data-title="3.1 &lt;b&gt;数据集&lt;/b&gt;">3.1 <b>数据集</b></a></li>
                                                <li><a href="#141" data-title="3.2 &lt;b&gt;模型训练&lt;/b&gt;">3.2 <b>模型训练</b></a></li>
                                                <li><a href="#143" data-title="3.3 &lt;b&gt;实验结果与分析&lt;/b&gt;">3.3 <b>实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#155" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="图1 以VGG为骨干网络的Faster R-CNN结构示意图">图1 以VGG为骨干网络的Faster R-CNN结构示意图</a></li>
                                                <li><a href="#109" data-title="图2 以VGG为骨干网络的SSD结构示意图">图2 以VGG为骨干网络的SSD结构示意图</a></li>
                                                <li><a href="#123" data-title="图3 SSD中的特征金字塔结构示意图">图3 SSD中的特征金字塔结构示意图</a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;本文数据集中各类别数量&lt;/b&gt;"><b>表</b>1 <b>本文数据集中各类别数量</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同模型在本文数据集上的测试结果&lt;/b&gt;"><b>表</b>2 <b>不同模型在本文数据集上的测试结果</b></a></li>
                                                <li><a href="#149" data-title="图3 不同α和β值的测试结果">图3 不同α和β值的测试结果</a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同模型在&lt;/b&gt;MSCOCO&lt;b&gt;数据集上的测试结果&lt;/b&gt;"><b>表</b>4 <b>不同模型在</b>MSCOCO<b>数据集上的测试结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" WANG F-Y. Agent-based control for networked traffic management systems [J]. IEEE Intelligent Systems, 2005, 20 (5) : 92-96." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Agent-based control for networked traffic management systems">
                                        <b>[1]</b>
                                         WANG F-Y. Agent-based control for networked traffic management systems [J]. IEEE Intelligent Systems, 2005, 20 (5) : 92-96.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" ROSSETTI R J F, FERREIRA P A F, BRAGA R A M, et al. Towards an artificial traffic control system [C]// Proceedings of the 2008 11th International IEEE Conference on Intelligent Transportation Systems. Piscataway, NJ: IEEE, 2008: 14-19." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards an artificial traffic control system">
                                        <b>[2]</b>
                                         ROSSETTI R J F, FERREIRA P A F, BRAGA R A M, et al. Towards an artificial traffic control system [C]// Proceedings of the 2008 11th International IEEE Conference on Intelligent Transportation Systems. Piscataway, NJ: IEEE, 2008: 14-19.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 赵娜, 袁家斌, 徐晗.智能交通系统综述[J].计算机科学, 2014, 41 (11) :7-11. (ZHAO N, YUAN J B, XU H. Survey on intelligent transport system [J]. Computer Science, 2014, 41 (11) : 7-11.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201411003&amp;v=MjI4NzZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURsVzdyUEx6N0JiN0c0SDlYTnJvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         赵娜, 袁家斌, 徐晗.智能交通系统综述[J].计算机科学, 2014, 41 (11) :7-11. (ZHAO N, YUAN J B, XU H. Survey on intelligent transport system [J]. Computer Science, 2014, 41 (11) : 7-11.) 
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 刘小明, 何忠贺.城市智能交通系统技术发展现状及趋势[J].自动化博览, 2015 (1) :58-60. (LIU X M, HE Z H. Development and tendency of intelligent transportation systems in China [J]. Automation Panorama, 2015 (1) : 58-60.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDBN201501023&amp;v=MDc0OTlQUHluSllMRzRIOVRNcm85SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3I=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         刘小明, 何忠贺.城市智能交通系统技术发展现状及趋势[J].自动化博览, 2015 (1) :58-60. (LIU X M, HE Z H. Development and tendency of intelligent transportation systems in China [J]. Automation Panorama, 2015 (1) : 58-60.) 
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" MICHALOPOULOS P G. Vehicle detection video through image processing: the autoscope system [J]. IEEE Transactions on Vehicular Technology, 1991, 40 (1) : 21-29." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vehicle detection video through image processing: the autoscope system">
                                        <b>[5]</b>
                                         MICHALOPOULOS P G. Vehicle detection video through image processing: the autoscope system [J]. IEEE Transactions on Vehicular Technology, 1991, 40 (1) : 21-29.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" SUN Z, BEBIS G, MILLER R. On-road vehicle detection using Gabor filters and support vector machines [C]// Proceedings of the 2002 14th International Conference on Digital Signal Processing. Piscataway, NJ: IEEE, 2002: 1019-1022." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On-road vehicle detection using gabor filters and support vector machines">
                                        <b>[6]</b>
                                         SUN Z, BEBIS G, MILLER R. On-road vehicle detection using Gabor filters and support vector machines [C]// Proceedings of the 2002 14th International Conference on Digital Signal Processing. Piscataway, NJ: IEEE, 2002: 1019-1022.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" TZOMAKAS C, von SEELEN W. Vehicle detection in traffic scenes using shadows [EB/OL]. [2018- 07- 02].http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=EB25161C6B0FFE3581F4DF3532E6DE28 doi=10.1.1.45.3234&amp;amp;rep=rep1&amp;amp;type=pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vehicle detection in traffic scenes using shadows">
                                        <b>[7]</b>
                                         TZOMAKAS C, von SEELEN W. Vehicle detection in traffic scenes using shadows [EB/OL]. [2018- 07- 02].http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=EB25161C6B0FFE3581F4DF3532E6DE28 doi=10.1.1.45.3234&amp;amp;rep=rep1&amp;amp;type=pdf.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" TSAI L-W, HSIEH J-W, FAN K-C. Vehicle detection using normalized color and edge map [J]. IEEE Transactions on Image Processing, 2007, 16 (3) : 850-864." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vehicle detection using normalized color and edge map">
                                        <b>[8]</b>
                                         TSAI L-W, HSIEH J-W, FAN K-C. Vehicle detection using normalized color and edge map [J]. IEEE Transactions on Image Processing, 2007, 16 (3) : 850-864.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 宋晓琳, 邬紫阳, 张伟伟.基于阴影和类Haar特征的动态车辆检测[J].电子测量与仪器学报, 2015, 29 (9) :1340-1347. (SONG X L, WU Z Y, ZHANG W W. Dynamic vehicle detection based on shadow and Haar-like feature[J]. Journal of Electronic Measurement and Instrumentation, 2015, 29 (9) : 1340-1347.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZIY201509014&amp;v=MDc3NDk5VE1wbzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3clBJVGZDZDdHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         宋晓琳, 邬紫阳, 张伟伟.基于阴影和类Haar特征的动态车辆检测[J].电子测量与仪器学报, 2015, 29 (9) :1340-1347. (SONG X L, WU Z Y, ZHANG W W. Dynamic vehicle detection based on shadow and Haar-like feature[J]. Journal of Electronic Measurement and Instrumentation, 2015, 29 (9) : 1340-1347.) 
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     LeCUN Y, BENGIO Y, HINTON G. Deep learning [J]. Nature, 2015, 521 (7553) : 436-444.</a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" REDMON J, DIVVALA S, GIRSHICK R, et al. You only look once: unified, real-time object detection [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2016: 779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">
                                        <b>[11]</b>
                                         REDMON J, DIVVALA S, GIRSHICK R, et al. You only look once: unified, real-time object detection [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2016: 779-788.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     LIU W, ANGUELOV D, ERHAN D, et al. SSD: single shot multibox detector [C]// Proceedings of the 2016 European Conference on Computer Vision. Berlin: Springer, 2016: 21-37.</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     REN S, HE K, GIRSHICK R, et al. Faster R-CNN: towards real-time object detection with region proposal networks [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) : 1137-1149.</a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" YU F, KOLTUN V. Multi-scale context aggregation by dilated convolutions [EB/OL]. (2016- 04- 30) [2018- 07- 29]. https://arxiv.org/pdf/1511.07122v3.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">
                                        <b>[14]</b>
                                         YU F, KOLTUN V. Multi-scale context aggregation by dilated convolutions [EB/OL]. (2016- 04- 30) [2018- 07- 29]. https://arxiv.org/pdf/1511.07122v3.pdf.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" LIN T-Y, DOLL&#193;R P, GIRSHICK R, et al. Feature pyramid networks for object detection [EB/OL]. [2018- 07- 11]. https://arxiv.org/pdf/1612.03144.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature Pyramid Networks for Object Detection[C/OL]">
                                        <b>[15]</b>
                                         LIN T-Y, DOLL&#193;R P, GIRSHICK R, et al. Feature pyramid networks for object detection [EB/OL]. [2018- 07- 11]. https://arxiv.org/pdf/1612.03144.pdf.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" LIN T-Y, GOYALP, GIRSHICK R, et al. Focal loss for dense object detection [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 2017: 2999-3007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focal loss for dense object detection">
                                        <b>[16]</b>
                                         LIN T-Y, GOYALP, GIRSHICK R, et al. Focal loss for dense object detection [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 2017: 2999-3007.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" ZHAN C, DUAN X, XU S, et al. An improved moving object detection algorithm based on frame difference and edge detection [C]// Proceedings of the 4th International Conference on Image and Graphics. Washington, DC: IEEE Computer Society, 2007: 519-523." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Improved Moving Object Detection Algorithm Based on Frame Difference and Edge Detection">
                                        <b>[17]</b>
                                         ZHAN C, DUAN X, XU S, et al. An improved moving object detection algorithm based on frame difference and edge detection [C]// Proceedings of the 4th International Conference on Image and Graphics. Washington, DC: IEEE Computer Society, 2007: 519-523.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" HORN B K P, SCHUNCK B G. Determining optical flow [J]. Artificial Intelligence, 1981, 17 (1/2/3) : 185-203." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Determining optical flow">
                                        <b>[18]</b>
                                         HORN B K P, SCHUNCK B G. Determining optical flow [J]. Artificial Intelligence, 1981, 17 (1/2/3) : 185-203.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" HAN X, ZHANG D Q, YU H H. System and method for video detection and tracking: U.S. Patent Application 13/720, 653 [P]. 2014- 06- 19." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=System and method for video detection and tracking">
                                        <b>[19]</b>
                                         HAN X, ZHANG D Q, YU H H. System and method for video detection and tracking: U.S. Patent Application 13/720, 653 [P]. 2014- 06- 19.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" LOWE D G. Distinctive image features from scale-invariant keypoints [J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjEwNDYva1c3L0FKVms9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZp&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         LOWE D G. Distinctive image features from scale-invariant keypoints [J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" PAPAGEORGIOU C P, OREN M, POGGIO T. A general framework for object detection [C]// Proceedings of the 6th International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 1998: 555-562." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A general framework for object detection">
                                        <b>[21]</b>
                                         PAPAGEORGIOU C P, OREN M, POGGIO T. A general framework for object detection [C]// Proceedings of the 6th International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 1998: 555-562.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" DALAL N, TRIGGS B. Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2005, 1: 886-893" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">
                                        <b>[22]</b>
                                         DALAL N, TRIGGS B. Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2005, 1: 886-893
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" OJALA T, PIETIKINEN M, HARWOOD D. A comparative study of texture measures with classification based on featured distribution [J]. Pattern Recognition, 1996, 29 (1) : 51-59." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600742093&amp;v=MzEyODViSzdIdEROcVk5RlkrOE5ESFU2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpKS0Y0Y2J4VT1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         OJALA T, PIETIKINEN M, HARWOOD D. A comparative study of texture measures with classification based on featured distribution [J]. Pattern Recognition, 1996, 29 (1) : 51-59.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" NG P C, HENIKOFF S. SIFT: predicting amino acid changes that affect protein function [J]. Nucleic Acids Research, 2003, 31 (13) : 3812-3814." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SIFT: Predicting amino acid changes that affect protein function">
                                        <b>[24]</b>
                                         NG P C, HENIKOFF S. SIFT: predicting amino acid changes that affect protein function [J]. Nucleic Acids Research, 2003, 31 (13) : 3812-3814.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" SCHAPIRE R E, SINGER Y. Improved boosting algorithms using confidence-rated predictions [J]. Machine Learning, 1999, 37 (3) : 297-336." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340003&amp;v=MTU3MzF1Rmkva1c3L0FKVms9Tmo3QmFyTzRIdEhOckl0RlpPc01ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWita&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         SCHAPIRE R E, SINGER Y. Improved boosting algorithms using confidence-rated predictions [J]. Machine Learning, 1999, 37 (3) : 297-336.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" CHEN P-H, LIN C-J, SCH&#214;LKOPF B. A tutorial on &lt;i&gt;v&lt;/i&gt;-support vector machines [J]. Applied Stochastic Models in Business and Industry, 2005, 21 (2) : 111-136." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A tutorial on v-support vector machines">
                                        <b>[26]</b>
                                         CHEN P-H, LIN C-J, SCH&#214;LKOPF B. A tutorial on &lt;i&gt;v&lt;/i&gt;-support vector machines [J]. Applied Stochastic Models in Business and Industry, 2005, 21 (2) : 111-136.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" 刘操, 郑宏, 黎曦, 等.基于多通道融合HOG特征的全天候运动车辆检测方法[J].武汉大学学报 (信息科学版) , 2015, 40 (8) :1048-1053. (LIU C, ZHENG H, LI X, et al. A method of moving vehicle detection in all-weather based on melted multi-channel HOG feature [J]. Geomatics and Information Science of Wuhan University, 2015, 40 (8) : 1048-1053.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH201508010&amp;v=MjgxNTQ5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3JQTWlYSVpyRzRIOVRNcDQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         刘操, 郑宏, 黎曦, 等.基于多通道融合HOG特征的全天候运动车辆检测方法[J].武汉大学学报 (信息科学版) , 2015, 40 (8) :1048-1053. (LIU C, ZHENG H, LI X, et al. A method of moving vehicle detection in all-weather based on melted multi-channel HOG feature [J]. Geomatics and Information Science of Wuhan University, 2015, 40 (8) : 1048-1053.) 
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" title=" KRIZHEVSKY A, SUTSKEVER I, HINTON G E. ImageNet classification with deep convolutional neural networks [C]// Proceedings of the 2012 Advances in Neural Information Processing Systems. Piscataway, NJ: IEEE, 2012: 1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[28]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HINTON G E. ImageNet classification with deep convolutional neural networks [C]// Proceedings of the 2012 Advances in Neural Information Processing Systems. Piscataway, NJ: IEEE, 2012: 1097-1105.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_29" title=" SERMANET P, EIGEN D, ZHANG X, et al. OverFeat: integrated recognition, localization and detection using convolutional networks [EB/OL]. (2014- 02- 24) [2018- 07- 28]. https://arxiv.org/pdf/1312.6229v4.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Over Feat:integrated recognition,localization and detection using convolutional networks">
                                        <b>[29]</b>
                                         SERMANET P, EIGEN D, ZHANG X, et al. OverFeat: integrated recognition, localization and detection using convolutional networks [EB/OL]. (2014- 02- 24) [2018- 07- 28]. https://arxiv.org/pdf/1312.6229v4.pdf.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_30" title=" GIRSHICK R, DONAHUE J, DARRELL T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2014: 580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[30]</b>
                                         GIRSHICK R, DONAHUE J, DARRELL T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2014: 580-587.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_31" title=" UIJLINGS J R R, van de SANDE K E A, GEVERS T, et al. Selective search for object recognition [J]. International Journal of Computer Vision, 2013, 104 (2) : 154-171." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MTA3MDVUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpKS0Y0Y2J4VT1OajdCYXJLN0h0bk1yWTlGWk9vTUNuODlvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[31]</b>
                                         UIJLINGS J R R, van de SANDE K E A, GEVERS T, et al. Selective search for object recognition [J]. International Journal of Computer Vision, 2013, 104 (2) : 154-171.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_32" >
                                        <b>[32]</b>
                                     GIRSHICK R. Fast R-CNN [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision. Piscataway, NJ: IEEE, 2015: 1440-1448.</a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_33" title=" JEONG J, PARK H, KWAK N. Enhancement of SSD by concatenating feature maps for object detection [EB/OL]. (2017- 05- 26) [2018- 07- 29]. https://arxiv.org/pdf/1705.09587v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhancement of SSD by concatenating feature maps for object detection">
                                        <b>[33]</b>
                                         JEONG J, PARK H, KWAK N. Enhancement of SSD by concatenating feature maps for object detection [EB/OL]. (2017- 05- 26) [2018- 07- 29]. https://arxiv.org/pdf/1705.09587v1.pdf.
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_34" title=" FU C-Y, LIU W, RANGA A, et al. DSSD: deconvolutional single shot detector [EB/OL]. (2017- 01- 23) [2018- 07- 28]. https://arxiv.org/pdf/1701.06659v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DSSD:Deconvolutional Single Shot Detector[C/OL]">
                                        <b>[34]</b>
                                         FU C-Y, LIU W, RANGA A, et al. DSSD: deconvolutional single shot detector [EB/OL]. (2017- 01- 23) [2018- 07- 28]. https://arxiv.org/pdf/1701.06659v1.pdf.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_35" title=" REDMON J, FARHADI A. YOLO9000: better, faster, stronger [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2017: 6517-6525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">
                                        <b>[35]</b>
                                         REDMON J, FARHADI A. YOLO9000: better, faster, stronger [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2017: 6517-6525.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_36" title=" REDMON J, FARHADI A. YOLOv3: an incremental improvement [EB/OL]. (2018- 04- 08) [2018- 07- 30]. https://arxiv.org/pdf/1804.02767v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An Incremental Improvement[C/OL]">
                                        <b>[36]</b>
                                         REDMON J, FARHADI A. YOLOv3: an incremental improvement [EB/OL]. (2018- 04- 08) [2018- 07- 30]. https://arxiv.org/pdf/1804.02767v1.pdf.
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_37" title=" HE K, GKIOXARI G, DOLLAR P, et al. Mask R-CNN [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 2017: 2980-2988." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[37]</b>
                                         HE K, GKIOXARI G, DOLLAR P, et al. Mask R-CNN [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 2017: 2980-2988.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_38" title=" SIMONYAN K, ZISSERMAN A. Very deep convolutional networks for large-scale image recognition [EB/OL]. (2015- 04- 10) [2018- 07- 25]. https://arxiv.org/pdf/1409.1556v6.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[38]</b>
                                         SIMONYAN K, ZISSERMAN A. Very deep convolutional networks for large-scale image recognition [EB/OL]. (2015- 04- 10) [2018- 07- 25]. https://arxiv.org/pdf/1409.1556v6.pdf.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_39" title=" HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2016: 770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[39]</b>
                                         HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2016: 770-778.
                                    </a>
                                </li>
                                <li id="81">


                                    <a id="bibliography_40" >
                                        <b>[40]</b>
                                     SZEGEDY C, VANHOUCKE V, IOFFE S, et al. Rethinking the inception architecture for computer vision [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ:IEEE, 2016: 2818-2826.</a>
                                </li>
                                <li id="83">


                                    <a id="bibliography_41" title=" LIN T-Y, MAIRE M, BELONGIE S, et al. Microsoft COCO: common objects in context [C]// Proceedings of the 2014 European Conference on Computer Vision. Berlin: Springer, 2014: 740-755.This work is partially supported by the Shanghai Science and Technology Commission Fund Project (17511104502) .XU Zihao, born in 1995, M. S. candidate. His research interests include deep learning, computer vision.HUANG Weiquan, born in 1994, M. S. candidate. His research interests include deep learning, computer vision.WANG Yin, born in 1979, Ph. D., professor. His research interests include deep learning, artificial intelligence." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:common objects in context">
                                        <b>[41]</b>
                                         LIN T-Y, MAIRE M, BELONGIE S, et al. Microsoft COCO: common objects in context [C]// Proceedings of the 2014 European Conference on Computer Vision. Berlin: Springer, 2014: 740-755.This work is partially supported by the Shanghai Science and Technology Commission Fund Project (17511104502) .XU Zihao, born in 1995, M. S. candidate. His research interests include deep learning, computer vision.HUANG Weiquan, born in 1994, M. S. candidate. His research interests include deep learning, computer vision.WANG Yin, born in 1979, Ph. D., professor. His research interests include deep learning, artificial intelligence.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-09-29 09:44</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(03),700-705 DOI:10.11772/j.issn.1001-9081.2018071587            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的监控视频中多类别车辆检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E5%AD%90%E8%B1%AA&amp;code=41275246&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐子豪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E4%BC%9F%E6%B3%89&amp;code=37174453&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄伟泉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%83%A4&amp;code=08965967&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王胤</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%8C%E6%B5%8E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%B3%BB&amp;code=0118734&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">同济大学计算机科学与技术系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%8E%E6%9C%8D%E5%8A%A1%E8%AE%A1%E7%AE%97%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%90%8C%E6%B5%8E%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">嵌入式系统与服务计算教育部重点实验室(同济大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统机器学习算法在交通监控视频的车辆检测中易受视频质量、拍摄角度、天气环境等客观因素影响, 预处理过程繁琐、难以进行泛化、鲁棒性差等问题, 结合空洞卷积、特征金字塔、焦点损失, 提出改进的更快的区域卷积神经网络 (Faster R-CNN) 和单阶段多边框检测检测器 (SSD) 两种深度学习模型进行多类别车辆检测。首先从监控视频中截取的不同时间的851张标注图构建数据集;然后在保证训练策略相同的情况下, 对两种改进后的模型与原模型进行训练;最后对每个模型的平均准确率进行评估。实验结果表明, 与原Faster R-CNN和SSD模型相比, 改进后的Faster R-CNN和SSD模型的平均准确率分别提高了0.8个百分点和1.7个百分点, 两种深度学习方法较传统方法更适应复杂情况下的车辆检测任务, 前者准确度较高、速度较慢, 更适用于视频离线处理, 后者准确度较低、速度较快, 更适用于视频实时检测。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BD%A6%E8%BE%86%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">车辆检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空洞卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%87%91%E5%AD%97%E5%A1%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征金字塔;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%84%A6%E7%82%B9%E6%8D%9F%E5%A4%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">焦点损失;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *徐子豪 (1995—) , 男, 黑龙江哈尔滨人, 硕士研究生, 主要研究方向:深度学习、计算机视觉;电子邮箱xzh0326@tongji.edu.cn;
                                </span>
                                <span>
                                    黄伟泉 (1994—) , 男, 广东广州人, 硕士研究生, 主要研究方向:深度学习、计算机视觉;;
                                </span>
                                <span>
                                    王胤 (1979—) , 男, 湖南长沙人, 教授, 博士生导师, 博士, 主要研究方向:深度学习、人工智能。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>上海科委基金资助项目 (17511104502);</span>
                    </p>
            </div>
                    <h1><b>Multi-class vehicle detection in surveillance video based on deep learning</b></h1>
                    <h2>
                    <span>XU Zihao</span>
                    <span>HUANG Weiquan</span>
                    <span>WANG Yin</span>
            </h2>
                    <h2>
                    <span>Department of Computer Science and Technology, Tongji University</span>
                    <span>Key Laboratory of Embedded Systems and Service Computing (Tongji University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Since performance of traditional machine learning methods of detecting vehicles in traffic surveillance video is influenced by objective factors such as video quality, shooting angle and weather, which results in complex preprocessing, hard generalization and poor robustness, combined with dilated convolution, feature pyramid and focal loss, two deep learning models which are improved Faster R-CNN (Faster Regions with Convolutional Neural Network) and SSD (Single Shot multibox Detector) model were proposed for vehicle detection. Firstly, a dataset was composed of 851 labeled images captured from the surveillance video at different time. Secondly, improved and original models were trained under same training strategies. Finally, average accuracy of each model were calculated to evaluate. Experimental results show that compared with original Faster R-CNN and SSD, the average accuracies of the improved models improve 0.8 percentage points and 1.7 percentage points respectively. Both deep learning methods are more suitable for vehicle detection in complicated situation than traditional methods. The former has higher accuracy and slower speed, which is more suitable for video off-line processing, while the latter has lower accuracy and higher speed, which is more suitable for video real-time detection.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=vehicle%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">vehicle detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dilated%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dilated convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20pyramid&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature pyramid;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=focal%20loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">focal loss;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XU Zihao, born in 1995, M. S. candidate. His research interests include deep learning, computer vision.;
                                </span>
                                <span>
                                    HUANG Weiquan, born in 1994, M. S. candidate. His research interests include deep learning, computer vision.;
                                </span>
                                <span>
                                    WANG Yin, born in 1979, Ph. D. , professor. His research interests include deep learning, artificial intelligence.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-01</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Shanghai Science and Technology Commission Fund Project (17511104502);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="85" name="85" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="86">随着经济和城镇化建设的快速发展, 我国各城市的道路和车辆总量不断增长, 交管部门的管理压力与日俱增。虽然高清监控摄像头已经在绝大多数路口部署, 但每日产生的视频量也越来越庞大, 通过人工进行视频实时监控或离线处理既费时又费力, 而且容易延误和遗漏, 所以亟须寻找一种自动化方法辅助人工进行监控处理, 这也是智能交通系统的核心<citation id="158" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation> 。</p>
                </div>
                <div class="p1">
                    <p id="87">交通监控视频中蕴含丰富的信息, 是智能交通监控系统的重要数据来源。监控视频可以应用在车辆违法行为判断、跨摄像头车辆追踪、分时段分车道车流量统计等实际场景, 而车辆检测则是车辆行为分析和智能交通监控的重要基础。</p>
                </div>
                <div class="p1">
                    <p id="88">我国的相关管理部门一直在积极改进交通视频监控系统, 但由于视频监控系统建设时间以及监控需求不同, 监控视频的分辨率、摄像角度、摄像方向都有很大差异, 加之不同的时间、天气, 如:夜间车辆灯光、恶劣天气的能见度、大风带来的摄像头抖动等因素都会严重影响视频质量。这些因素使得获取到的视频质量良莠不齐, 而传统车辆检测方法<citation id="159" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation> 应对复杂场景往往表现较差, 好的表现更依赖于好的视频质量和简单场景, 这是车辆检测在实际应用上表现不佳的重要原因。</p>
                </div>
                <div class="p1">
                    <p id="89">近几年, 深度学习方法在计算机视觉领域不断取得突破<citation id="160" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 一些基本任务也都有了优秀的解决方案, 基于深度学习的目标检测算法也在众多检测算法中异军突起, 其准确率远远超过传统检测算法, 鲁棒性也更强。深度学习方法使用卷积神经网络 (Convolutional Neural Network, CNN) 摆脱了传统机器学习方法预处理及构造特征的繁琐过程, 同时大幅降低了因角度、遮挡等因素造成的误检和漏检, 对复杂场景的适应性更强。目前, 深度学习的目标检测方法主要分为以文献<citation id="161" type="reference">[<a class="sup">11</a>]</citation>和文献<citation id="162" type="reference">[<a class="sup">12</a>]</citation>为代表的单阶段模型和以文献<citation id="163" type="reference">[<a class="sup">13</a>]</citation>为代表的两阶段模型两大类。</p>
                </div>
                <div class="p1">
                    <p id="90">本文将更快的区域卷积神经网络 (Faster Regions with CNN, Faster R-CNN) 、单阶段多边框检测检测器 (Single Shot multibox Detector, SSD) 等深度学习的目标检测模型引入到交通监控视频中进行多类别车辆检测, 并在基本模型基础上尝试使用更佳的骨干网络作特征提取, 同时融合进空洞卷积<citation id="164" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、特征金字塔<citation id="165" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、焦点损失函数<citation id="166" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>等对基础网络进行优化。本文基于获取的监控视频构建了多类别车辆检测数据集, 并以此为基础对不同模型的检测效果、应用场景进行比较分析。实验结果显示经过上述方法改进的模型效果好于基础模型, 单阶段模型准确度较低, 但速度较快, 而两阶段模型准确度较高, 速度较慢, 所以对于在线监控可以选择速度更快的单阶段模型, 而离线处理可以选择准确度更高的两阶段模型。</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag">1 相关研究</h3>
                <div class="p1">
                    <p id="92">视频流的本质是一帧帧图像, 而需要检测的目标往往是视频中运动的物体, 所以一种简单又实用的思路是利用视频图像中背景基本不动而前景持续运动的特点, 通过比较帧间像素点强度的变化和相关性判断运动区域, 这个区域即为检测的运动物体。应用这种思路并普遍使用的检测方法有:帧差法<citation id="167" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、光流法<citation id="168" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>等。这类方法虽然计算速度快, 但没有完整利用单帧图像的整体信息, 难以扩展到多类别检测, 准确率较低, 鲁棒性也较差。另一种研究思路<citation id="169" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>是将视频流分成一帧帧图像处理, 将视频中的目标检测转变成图像中的目标检测任务, 这种思路虽然计算速度较慢, 但充分利用了图像信息, 准确性更高, 鲁棒性更强, 应用更广。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">1.1 <b>传统方法</b></h4>
                <div class="p1">
                    <p id="94">在深度学习热潮兴起前, 计算机视觉领域的研究者们通常使用传统目标检测算法完成这一任务。传统方法的步骤主要分为三步:候选区域提取、区域特征提取、特征分类。因为传统方法计算速度快, 所以候选区域提取常采用贪心的滑动窗口策略, 使用不同尺寸的滑动窗口对图片进行逐行扫描, 每个窗口区域使用人为划定或特征提取算法进行特征提取, 文献<citation id="173" type="reference">[<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>,<a class="sup">24</a>]</citation>详述了多种常用的特征提取算法。最后将特征向量送入预先训练好的分类器进行分类, 统计每个窗口的分类结果整合成最终的检测结果。比较经典并且推广到实际应用中的算法有:文献<citation id="170" type="reference">[<a class="sup">25</a>]</citation>进行的人脸识别, 文献<citation id="171" type="reference">[<a class="sup">26</a>]</citation>进行的行人重识别等, 同样类似的方法也曾被应用在视频中的车辆检测<citation id="172" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">1.2 <b>深度学习方法</b></h4>
                <div class="p1">
                    <p id="96">自从2012年文献<citation id="174" type="reference">[<a class="sup">28</a>]</citation>提出深度学习分类模型开始, 基于深度卷积神经网络的模型成为了图像识别与检测领域的首选之一。首先使用深度学习方法进行目标检测并取得很大进展的方法是2013 年文献<citation id="175" type="reference">[<a class="sup">29</a>]</citation>提出的OverFeat, 该方法开始尝试使用CNN提取图片特征, 利用多尺度滑动窗口算法进行检测, 取得了很好的效果。</p>
                </div>
                <div class="p1">
                    <p id="97">2014年文献<citation id="176" type="reference">[<a class="sup">30</a>]</citation>提出的区域卷积神经网络 (Regions with CNN, R-CNN) 完整地将CNN融合进目标检测任务中, 成为深度学习进行目标检测的奠基之作。R-CNN利用文献<citation id="177" type="reference">[<a class="sup">31</a>]</citation>提取约2 000个候选框, 每个候选框通过CNN进行特征提取, 结合多个二分类支持向量机 (Support Vector Machine, SVM) 实现候选区域目标的多分类, 最后利用非极大值抑制 (Non-Maximum Suppression, NMS) 算法和框回归对候选框进行筛选融合和微调。R-CNN在检测准确度上大大超过了传统方法, 但由于流程复杂, 模型需要多阶段训练, 预测一张图速度过慢, 这些因素导致R-CNN无法真正进行实际应用。2015年文献<citation id="178" type="reference">[<a class="sup">32</a>]</citation>在R-CNN的基础上主要针对候选框特征重复提取的问题进行改进, 提出了Fast R-CNN, 它在速度和精度上较R-CNN有了很大提升。Fast R-CNN只对图片通过CNN进行一次前向运算提取特征, 利用特征图坐标对应关系将提取的2 000个候选框映射到底层特征图中, 并且利用提出的感兴趣区域 (Regions of Interest, ROI) 池化结构有效解决了特征图上不同尺寸的候选框需要缩放到同一尺寸的问题。这一操作减少了大量重复的运算, 大大提高了检测速度。同时, Fast R-CNN不再使用多个SVM进行分类, 而是在特征向量后直接连接Softmax层和全连接层作框分类和框回归, 将分类损失和边框回归损失结合进行统一训练, 这一操作简化了模型训练流程, 提高了训练速度。在此之后, 为了解决候选框提取这一时间瓶颈, 在Fast R-CNN的主网络中附加了区域候选网络 (Region Proposal Network, RPN) 在高层特征图上进行候选框提取, RPN的引入真正实现了一个网络的端到端目标检测, 它在检测速度上获得了更进一步的提升, 同时结合各种训练策略, Faster R-CNN的检测准确率在各大数据集上也取得了当时最高的结果。</p>
                </div>
                <div class="p1">
                    <p id="98">上述系列方法进行目标检测时虽然整合在一个网络中实现了端到端训练和预测, 但网络结构实际是将区域提取和目标检测分成两阶段进行计算, 检测速度经过不断优化虽然有了大幅度提高, 但即时在GPU上进行运算, 最快速度也很难达到每秒10帧。为了使目标检测算法可以应用到视频中进行实时检测, 需要在保证准确率的前提下, 继续提高单张图片的目标检测速度, YOLO (You Only Look Once) <citation id="179" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、SSD<citation id="180" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等算法将区域提取和区域分类整合到单阶段进行计算。YOLO预先将图片分为若干栅格, 以这些栅格区域为候选区域进行框回归, 网络主干仍为CNN结构, 直接输出框回归以及对应框分类的结果, 而SSD则是在多个CNN的底层特征图上进行框回归和分类, 其检测精度要高于YOLO。单阶段网络减少了RPN的计算, 更接近于纯粹的图像分类网络, 在检测精度没有很大损失的前提下, 其检测速度提升到了每秒40帧以上, 已经可以满足视频检测的需求。</p>
                </div>
                <div class="p1">
                    <p id="99">目前, 单阶段和两阶段模型仍是目标检测领域的两大分支。对于单阶段模型, 在YOLO和SSD的基础上, 研究者们提出了一系列模型<citation id="182" type="reference"><link href="67" rel="bibliography" /><link href="69" rel="bibliography" /><link href="71" rel="bibliography" /><link href="73" rel="bibliography" /><sup>[<a class="sup">33</a>,<a class="sup">34</a>,<a class="sup">35</a>,<a class="sup">36</a>]</sup></citation>, 旨在提高检测精度。在Faster R-CNN的基础上, 原作者又对其进行细节优化, 并且将分割任务融合进模型中, 提出了Mask R-CNN<citation id="181" type="reference"><link href="75" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>。也有一些其他工作分别从特征图的前后关联和损失函数入手进行优化, 这些改进也可以与上述的主流模型进行融合提升检测效果。</p>
                </div>
                <h3 id="100" name="100" class="anchor-tag">2 算法设计</h3>
                <div class="p1">
                    <p id="101">本文算法将监控视频当成一帧帧图像进行图像中的车辆检测, 以Faster R-CNN和SSD这两类模型框架作为基础, 结合空洞卷积、特征金字塔、焦点损失进行改进, 下面对每部分进行介绍。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">2.1 Faster R-CNN<b>整体结构</b></h4>
                <div class="p1">
                    <p id="103">Faster R-CNN的整体结构是在一个主干的特征提取CNN中引入RPN结构进行候选区域提取, 筛选得到固定数量的候选框进行目标分类和框回归, 最后经过NMS进行框融合以及框位置精修得到最终的检测结果。图1展示了以文献<citation id="183" type="reference">[<a class="sup">38</a>]</citation>提出的VGG为骨干网络的Faster R-CNN的整体结构。</p>
                </div>
                <div class="p1">
                    <p id="104">对于一张输入图片, 首先经过特定骨干网络VGG头部的部分层计算得到某一层的高层特征图, RPN在特征图上进行滑窗计算, 通过预先设置不同面积及尺寸目标框的方式实现候选框位置的预估, 同时对每个预估框进行分类和框回归, 这里的分类是判断框范围内的图像是前景还是背景的二分类, 框坐标回归是对包含前景的框的位置进行修正。不同大小的目标框经过ROI池化层调整成相同长度的特征向量, 最后经过全连接层连接进行多分类和框回归。多分类是指目标框前景物体的准确分类的各类别得分, 框回归是对框位置的再次修正。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903015_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 以VGG为骨干网络的Faster R-CNN结构示意图" src="Detail/GetImg?filename=images/JSJY201903015_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 以VGG为骨干网络的Faster R-CNN结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903015_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure schematic diagram of Faster R-CNN with VGG as backbone network</p>

                </div>
                <div class="p1">
                    <p id="106">在本文改进的Faster R-CNN结构中, 为了获得更好的特征提取效果, 本文算法使用不同层数的文献<citation id="184" type="reference">[<a class="sup">39</a>]</citation>提出的Resnet代替原始Faster R-CNN中的VGG结构。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">2.2 SSD<b>整体结构</b></h4>
                <div class="p1">
                    <p id="108">SSD的整体结构是在一个主干的特征提取CNN中的多个高层特征图上直接回归候选区域的位置, 并对每个位置框进行分类, 其中预先设置的候选框区域依旧与Faster R-CNN的设置方法类似, 以不同面积及尺寸的锚点在特征图上的每个像素点上密布不同大小的多个目标框。图2展示了以VGG为骨干特征提取网络的SSD的整体结构。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903015_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 以VGG为骨干网络的SSD结构示意图" src="Detail/GetImg?filename=images/JSJY201903015_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 以VGG为骨干网络的SSD结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903015_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Structure schematic diagram of SSD with VGG as backbone network</p>

                </div>
                <div class="p1">
                    <p id="110">输入图片首先经过VGG头部的若干卷积层和池化层进行前向计算, 之后分别连接全连接层、卷积层、池化层, 取编号为Conv6、Conv7、Conv8_2、Conv9_2、Conv10_2、Conv11_2层的特征图。在这些特征图上直接进行框回归和多分类, 框回归依旧是对预设框坐标的修正, 多分类将背景单独算为附加的一类与前景<i>k</i>个类别一同计算, 得到 (<i>k</i>+1) 个类别的得分。最后利用NMS算法进行框融合。</p>
                </div>
                <div class="p1">
                    <p id="111">在本文改进的SSD结构中, 为了获得更好的特征提取效果, 并保证运算速度, 使用相对轻量的Inception<citation id="185" type="reference"><link href="81" rel="bibliography" /><sup>[<a class="sup">40</a>]</sup></citation>代替原始SSD中的VGG结构。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">2.3 <b>改进方法</b></h4>
                <div class="p1">
                    <p id="113">除了改进Faster R-CNN和SSD的骨干网络以提高其特征提取的能力外, 本文在基础的Faster R-CNN中加入空洞卷积进行优化, 在基础的SSD中加入特征金字塔和焦点损失进行优化。下面对每一项方法进行介绍。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">2.3.1 空洞卷积</h4>
                <div class="p1">
                    <p id="115">空洞卷积本质上是对一般卷积运算的推广。这里定义<i>F</i>是ℤ<sup>2</sup>→ℝ的离散函数, <i>Ω</i><sub><i>r</i></sub>=[-<i>r</i>, <i>r</i>]<sup>2</sup>∩ℤ<sup>2</sup>, <i>k</i>是<i>Ω</i><sub><i>r</i></sub>→ℝ的大小为 (2<i>r</i>+1) <sup>2</sup>的离散滤波器, 定义*为一般卷积运算, 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>F</mi><mo>*</mo><mi>k</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>+</mo><mi>t</mi><mo>=</mo><mi>p</mi></mrow></munder><mi>F</mi></mstyle><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mi>k</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">定义<i>l</i>为空洞卷积间隔因子, *<sub><i>l</i></sub>为空洞卷积运算, 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>F</mi><mo>*</mo><msub><mrow></mrow><mi>l</mi></msub><mspace width="0.25em" /><mi>k</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>+</mo><mi>l</mi><mi>t</mi><mo>=</mo><mi>p</mi></mrow></munder><mi>F</mi></mstyle><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mi>k</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">具体而言, 卷积核在进行一般卷积运算时, 是与特征图的相邻像素点作乘法运算, 而空洞卷积运算允许卷积核与固定间隔<i>l</i>的像素点作乘法运算, 这样在不增加额外运算量的同时, 增大感受野。而对于分辨率较高的图像, 相邻像素间的冗余信息相对较多, 可以利用空洞卷积进行优化。为此, 本文主要将空洞卷积引入到Faster R-CNN中的RPN对特征图的卷积运算中。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">2.3.2 特征金字塔</h4>
                <div class="p1">
                    <p id="121">最简单的特征金字塔结构可以依靠堆叠多个经过缩放的不同大小的同一张图片实现, 而在各种CNN网络结构中, 其前向传播的计算过程将原始图片逐步变成更小的特征图, 即自底向上的结构, 这是一种CNN都具备的原生金字塔。本文在基础SSD结构中加入自顶向下结构和横向连接, 使其同时利用低层特征高分辨率和高层特征强语义信息, 更兼顾位置信息和语义信息, 提高SSD的检测能力。自顶向下结构通过上采样对高层特征图逐层进行放大, 相当于自底向上结构的逆过程运算, 横向连接将两次计算中相同大小的特征图逐像素相加进行融合。</p>
                </div>
                <div class="p1">
                    <p id="122">图3展示了在图2基础上添加的金字塔结构, Conv6层到Conv11_2层是CNN的前向计算过程, 即原始的自底向上结构, 从Conv11_2通过一次次上采样计算生成对应大小相同的特征图, 直到Up_Conv6, 这个结构即自顶向下结构。Conv10_2层会与Conv11_2上采样得到的特征图进行融合得到Up_Conv10_2, 其他层也依次通过这一操作进行融合, 这个计算过程即横向连接。最后再对融合后的各层进行框坐标回归和分类预测。本文将其应用在网络的最后3个特征图上。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903015_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 SSD中的特征金字塔结构示意图" src="Detail/GetImg?filename=images/JSJY201903015_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 SSD中的特征金字塔结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903015_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structure schematic diagram of feature pyramid in SSD</p>

                </div>
                <h4 class="anchor-tag" id="124" name="124">2.3.3 焦点损失</h4>
                <div class="p1">
                    <p id="125">多分类任务常用的目标损失函数是交叉熵损失。假设任务中有<i>n</i>个样本, 分类目标有<i>C</i>类, 交叉熵<i>CE</i>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mo>-</mo></mstyle></mrow></mstyle><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mrow><mi>f</mi><mo>^</mo></mrow><mo stretchy="false"> (</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">其中: <i>f</i>^ (<i>x</i>) 表示预测类别概率, <b><i>y</i></b>是实际类别的one-hot向量。交叉熵函数本身将所有类别的物体均等对待, 在遇到类别不平衡现象时容易造成预测偏移, 而且无法对难分样本进行加强训练。</p>
                </div>
                <div class="p1">
                    <p id="128">针对类别不平衡现象, 可以针对不同类别引入一个权重因子<i>α</i>削弱大数量类别对损失值的影响:</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mo>-</mo></mstyle></mrow></mstyle><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mrow><mi>f</mi><mo>^</mo></mrow><mo stretchy="false"> (</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>j</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">针对难分样本问题, 一个样本的预测概率越高, 模型对该样本的识别力越强, 该样本成为易分样本, 反之则为难分样本。可以以预测概率为基础, 引入一个权重因子<i>β</i>削弱易分样本对损失值的影响, <i>β</i>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="131"><i>β</i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>= (1-<i>y</i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) <sup><i>γ</i></sup>      (5) </p>
                </div>
                <div class="p1">
                    <p id="134">其中<i>γ</i>是一个可调节的超参数。焦点损失<i>FL</i>定义为:</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mo>-</mo></mstyle></mrow></mstyle><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>β</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mrow><mi>f</mi><mo>^</mo></mrow><mo stretchy="false"> (</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136">本文将上述定义的多类别焦点损失应用到SSD模型中, 并对不同的<i>α</i>和<i>γ</i>取值进行实验。</p>
                </div>
                <h3 id="137" name="137" class="anchor-tag">3 实验</h3>
                <h4 class="anchor-tag" id="138" name="138">3.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="139">实验数据集由从监控视频中截取的不同时间的图像组成, 经过人为筛选剔除掉前后变化较小的图像, 总计图像851张, 划分训练集681张, 验证集170张图片。由于各段视频的分辨率不同, 获取的图像大小有1 080×720和1 920×1 080两种。根据实际应用场景, 本文将车辆类型分为四类:汽车 (car) 、公交车 (bus) 、出租车 (taxi) 、卡车 (truck) , 各种类目标标注数量如表1所示。</p>
                </div>
                <div class="area_img" id="140">
                    <p class="img_tit"><b>表</b>1 <b>本文数据集中各类别数量</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Number of every category in dataset</p>
                    <p class="img_note"></p>
                    <table id="140" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="4"><br />类别</td></tr><tr><td><br />bus</td><td>car</td><td>taxi</td><td>truck</td></tr><tr><td><br />训练集</td><td>452</td><td>3 388</td><td>694</td><td>182</td></tr><tr><td><br />验证集</td><td>91</td><td>810</td><td>158</td><td>36</td></tr><tr><td><br />合计</td><td>543</td><td>4 198</td><td>852</td><td>218</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="141" name="141">3.2 <b>模型训练</b></h4>
                <div class="p1">
                    <p id="142">在训练前, 对所有图片进行水平翻转、对比度增强、饱和度增强、色彩变换等操作进行图像增强。所有模型使用Tensorflow框架实现, 在Nvidia1080显卡上训练300个周期。对于Faster R-CNN类模型, 图片统一缩放到1 280×720输入网络, 初始化学习率为0.001, 每隔100个周期下降到之前的1/10, 梯度更新方法采用带有动量的小批量随机梯度下降, 动量因子为0.9。对于SSD类模型, 图片统一缩放到500×500输入网络, 初始化学习率为0.001, 每隔10个周期下降到之前的0.95倍, 梯度更新采用RMSProp优化器, 动量因子为0.9。</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143">3.3 <b>实验结果与分析</b></h4>
                <div class="p1">
                    <p id="144">本文使用平均准确率 (Mean Average Precision, MAP) 作为检测准确性的评估指标, 框匹配阈值设为0.5, 该指标综合考虑了定位精度与分类准确率。同时为了比较模型进行实时处理的能力, 本文在接入视频流的条件下, 对每个模型处理单张图片的速度进行了测试, 结果如表2所示。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表</b>2 <b>不同模型在本文数据集上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Test results of different models on the proposed dataset</p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td rowspan="2"><br />模型名称</td><td colspan="4"><br />各类别准确率</td><td rowspan="2">MAP</td><td rowspan="2">每张处理<br />时间/s</td></tr><tr><td><br />bus</td><td>car</td><td>taxi</td><td>truck</td></tr><tr><td><br />Faster_rcnn_resnet50</td><td>0.956</td><td>0.977</td><td>0.917</td><td>0.833</td><td>0.921</td><td>0.166</td></tr><tr><td><br />Faster_rcnn_resnet50_dilated_convolution</td><td>0.955</td><td>0.980</td><td>0.921</td><td>0.849</td><td>0.926</td><td>0.167</td></tr><tr><td><br />Faster_rcnn_resnet101</td><td>0.946</td><td>0.973</td><td>0.915</td><td>0.838</td><td>0.918</td><td>0.198</td></tr><tr><td><br />Faster_rcnn_resnet101_dilated_convolution</td><td>0.949</td><td>0.980</td><td>0.918</td><td>0.875</td><td>0.930</td><td>0.199</td></tr><tr><td><br />Faster_rcnn_resnet152</td><td>0.955</td><td>0.978</td><td>0.917</td><td>0.849</td><td>0.925</td><td>0.233</td></tr><tr><td><br />Faster_rcnn_resnet152_dilated_convolution</td><td>0.961</td><td>0.976</td><td>0.904</td><td>0.883</td><td>0.931</td><td>0.233</td></tr><tr><td><br />SSD_inceptionv2</td><td>0.918</td><td>0.906</td><td>0.843</td><td>0.773</td><td>0.860</td><td>0.024</td></tr><tr><td><br />SSD_inceptionv2_feature_pyramid_focal_loss</td><td>0.929</td><td>0.921</td><td>0.858</td><td>0.808</td><td>0.879</td><td>0.025</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="146">实验结果显示本文改进的网络在不影响检测速度的情况下, 提高了检测精度。其中, 使用空洞卷积的不同骨干网络的Faster RCNN模型的MAP值提高了0.5个百分点～1.2个百分点, 平均提高0.8个百分点, 而使用特征金字塔和焦点损失改进的SSD模型的MAP值提高了1.9个百分点。本文也对焦点损失中不同的<i>α</i>和<i>β</i>取值对模型的影响作了测试, 其中<i>α</i>=0.75, <i>β</i>=0.75时, MAP值最高, 全部结果如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="147">对比每一类的检测结果可以发现, 由于总类别数不是很大, 且前三类样本数足够多, 所以Faster R-CNN类模型对前三类的识别已经达到很高的水平, 改进后的模型在前三类的准确率比改进前模型略有提高。MAP值的提高主要在于卡车这一类的准确率提高, 这是因为该类的样本在数据集中相对较少, 相比其他三类, 模型对于这一类的学习难度是较高的, 所以其准确率相对较低。改进后的模型在卡车类的识别上平均提高2.9个百分点, 因为卡车样本数相对较大, 空洞卷积减少了相邻冗余像素对特征的干扰, 从而提高了检测准确性。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit">图3 不同α和β值的测试结果 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Test results for different <i>α</i> and <i>β</i> values</p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td><i>α</i></td><td><i>β</i></td><td>MAP</td><td></td><td><i>α</i></td><td><i>β</i></td><td>MAP</td><td></td><td><i>α</i></td><td><i>β</i></td><td>MAP</td></tr><tr><td>0.25</td><td>0.25</td><td>0.776 3</td><td></td><td>0.5</td><td>0.25</td><td>0.862 7</td><td></td><td>0.75</td><td>0.25</td><td>0.872 7</td></tr><tr><td><br />0.25</td><td>0.50</td><td>0.772 5</td><td></td><td>0.5</td><td>0.50</td><td>0.866 0</td><td></td><td>0.75</td><td>0.50</td><td>0.876 3</td></tr><tr><td><br />0.25</td><td>0.75</td><td>0.744 5</td><td></td><td>0.5</td><td>0.75</td><td>0.859 0</td><td></td><td>0.75</td><td>0.75</td><td>0.879 0</td></tr><tr><td><br />0.25</td><td>1.00</td><td>0.711 7</td><td></td><td>0.5</td><td>1.00</td><td>0.866 9</td><td></td><td>0.75</td><td>1.00</td><td>0.849 4</td></tr><tr><td><br />0.25</td><td>2.00</td><td>0.673 8</td><td></td><td>0.5</td><td>2.00</td><td>0.784 5</td><td></td><td>0.75</td><td>2.00</td><td>0.813 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="150">改进后的SSD模型MAP值提升了1.9个百分点, 每一类的准确率都提升明显, 其中特征金字塔结构融合了更多高层信息, 提升了模型的特征提取能力, 焦点损失增强了模型对难分的、准确率较低的样本的识别度, 结果显示这些改进针对每一类都有明显效果。</p>
                </div>
                <div class="p1">
                    <p id="151">此外, 本文使用开源的MSCOCO目标检测数据集<citation id="186" type="reference"><link href="83" rel="bibliography" /><sup>[<a class="sup">41</a>]</sup></citation>对改进后的模型进行评估, 全部结果如表4所示。结果显示, 不同的改进后的Faster R-CNN模型的MAP平均提高0.8个百分点, 改进后的SSD模型的MAP值提高1.5个百分点。</p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表</b>4 <b>不同模型在</b>MSCOCO<b>数据集上的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Test results of different models on MSCOCO dataset</p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td><br />模型名称</td><td>MAP</td></tr><tr><td><br />Faster_rcnn_resnet50</td><td>0.262</td></tr><tr><td><br />Faster_rcnn_resnet50_dilated_convolution</td><td>0.271</td></tr><tr><td><br />Faster_rcnn_resnet101</td><td>0.269</td></tr><tr><td><br />Faster_rcnn_resnet101_dilated_convolution</td><td>0.276</td></tr><tr><td><br />Faster_rcnn_resnet152</td><td>0.282</td></tr><tr><td><br />Faster_rcnn_resnet152_dilated_convolution</td><td>0.291</td></tr><tr><td><br />SSD_inceptionv2</td><td>0.204</td></tr><tr><td><br />SSD_inceptionv2_feature_pyramid_focal_loss</td><td>0.219</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="153">对比各模型的处理速度可以发现本文的改进措施基本没有引入过多的运算量, 其中, 空洞卷积和焦点损失属于计算的变化, 与原模型相比并未有多余计算, 而特征金字塔结构属于附加结构, 虽引入了多余运算, 但本质上只是若干次上采样和矩阵求和运算, 也并不会引起运算速度的大幅降低。</p>
                </div>
                <div class="p1">
                    <p id="154">实现结果也显示, 两阶段的Faster R-CNN模型运算准确率高于单阶段的SSD模型, 但速度明显慢于SSD模型。在实验运行环境中, 最快的Faster R-CNN模型每秒最多检测6帧图像, 而SSD模型每秒可以检测40帧图像。常见的视频流一般是每秒25帧图像, 所以SSD类模型完全可以应用在交通视频的实时检测中, 若想将Faster R-CNN接入实时视频流检测, 则需要每隔几帧图像检测一帧, 所以由于Faster R-CNN精度更高, 其更适用于离线处理。</p>
                </div>
                <h3 id="155" name="155" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="156">本文将深度学习模型引入交通监控视频的车辆检测中, 并对常用的Faster R-CNN和SSD两种模型进行改进, 实验结果显示改进后的模型在不影响检测速度的情况下提高了检测准确性, 取得了很好的效果。</p>
                </div>
                <div class="p1">
                    <p id="157">本文的改进模型可以在更大型的数据集上进行更深入的实验, 对于每项改进措施在模型中的应用, 也可以进行更多的实验。同时, 本文的工作也为后续车辆跟踪、车流统计等更具体的应用奠定了基础。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Agent-based control for networked traffic management systems">

                                <b>[1]</b> WANG F-Y. Agent-based control for networked traffic management systems [J]. IEEE Intelligent Systems, 2005, 20 (5) : 92-96.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards an artificial traffic control system">

                                <b>[2]</b> ROSSETTI R J F, FERREIRA P A F, BRAGA R A M, et al. Towards an artificial traffic control system [C]// Proceedings of the 2008 11th International IEEE Conference on Intelligent Transportation Systems. Piscataway, NJ: IEEE, 2008: 14-19.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201411003&amp;v=MDY1MTh0R0ZyQ1VSN3FmWnVacEZpRGxXN3JQTHo3QmI3RzRIOVhOcm85Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 赵娜, 袁家斌, 徐晗.智能交通系统综述[J].计算机科学, 2014, 41 (11) :7-11. (ZHAO N, YUAN J B, XU H. Survey on intelligent transport system [J]. Computer Science, 2014, 41 (11) : 7-11.) 
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDBN201501023&amp;v=MzI0MzdCdEdGckNVUjdxZlp1WnBGaURsVzdyUFB5bkpZTEc0SDlUTXJvOUhaNFFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 刘小明, 何忠贺.城市智能交通系统技术发展现状及趋势[J].自动化博览, 2015 (1) :58-60. (LIU X M, HE Z H. Development and tendency of intelligent transportation systems in China [J]. Automation Panorama, 2015 (1) : 58-60.) 
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vehicle detection video through image processing: the autoscope system">

                                <b>[5]</b> MICHALOPOULOS P G. Vehicle detection video through image processing: the autoscope system [J]. IEEE Transactions on Vehicular Technology, 1991, 40 (1) : 21-29.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On-road vehicle detection using gabor filters and support vector machines">

                                <b>[6]</b> SUN Z, BEBIS G, MILLER R. On-road vehicle detection using Gabor filters and support vector machines [C]// Proceedings of the 2002 14th International Conference on Digital Signal Processing. Piscataway, NJ: IEEE, 2002: 1019-1022.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vehicle detection in traffic scenes using shadows">

                                <b>[7]</b> TZOMAKAS C, von SEELEN W. Vehicle detection in traffic scenes using shadows [EB/OL]. [2018- 07- 02].http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=EB25161C6B0FFE3581F4DF3532E6DE28 doi=10.1.1.45.3234&amp;rep=rep1&amp;type=pdf.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vehicle detection using normalized color and edge map">

                                <b>[8]</b> TSAI L-W, HSIEH J-W, FAN K-C. Vehicle detection using normalized color and edge map [J]. IEEE Transactions on Image Processing, 2007, 16 (3) : 850-864.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZIY201509014&amp;v=MDY0OThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3clBJVGZDZDdHNEg5VE1wbzlFWUk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 宋晓琳, 邬紫阳, 张伟伟.基于阴影和类Haar特征的动态车辆检测[J].电子测量与仪器学报, 2015, 29 (9) :1340-1347. (SONG X L, WU Z Y, ZHANG W W. Dynamic vehicle detection based on shadow and Haar-like feature[J]. Journal of Electronic Measurement and Instrumentation, 2015, 29 (9) : 1340-1347.) 
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 LeCUN Y, BENGIO Y, HINTON G. Deep learning [J]. Nature, 2015, 521 (7553) : 436-444.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">

                                <b>[11]</b> REDMON J, DIVVALA S, GIRSHICK R, et al. You only look once: unified, real-time object detection [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2016: 779-788.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 LIU W, ANGUELOV D, ERHAN D, et al. SSD: single shot multibox detector [C]// Proceedings of the 2016 European Conference on Computer Vision. Berlin: Springer, 2016: 21-37.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 REN S, HE K, GIRSHICK R, et al. Faster R-CNN: towards real-time object detection with region proposal networks [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) : 1137-1149.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">

                                <b>[14]</b> YU F, KOLTUN V. Multi-scale context aggregation by dilated convolutions [EB/OL]. (2016- 04- 30) [2018- 07- 29]. https://arxiv.org/pdf/1511.07122v3.pdf.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature Pyramid Networks for Object Detection[C/OL]">

                                <b>[15]</b> LIN T-Y, DOLLÁR P, GIRSHICK R, et al. Feature pyramid networks for object detection [EB/OL]. [2018- 07- 11]. https://arxiv.org/pdf/1612.03144.pdf.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focal loss for dense object detection">

                                <b>[16]</b> LIN T-Y, GOYALP, GIRSHICK R, et al. Focal loss for dense object detection [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 2017: 2999-3007.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Improved Moving Object Detection Algorithm Based on Frame Difference and Edge Detection">

                                <b>[17]</b> ZHAN C, DUAN X, XU S, et al. An improved moving object detection algorithm based on frame difference and edge detection [C]// Proceedings of the 4th International Conference on Image and Graphics. Washington, DC: IEEE Computer Society, 2007: 519-523.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Determining optical flow">

                                <b>[18]</b> HORN B K P, SCHUNCK B G. Determining optical flow [J]. Artificial Intelligence, 1981, 17 (1/2/3) : 185-203.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=System and method for video detection and tracking">

                                <b>[19]</b> HAN X, ZHANG D Q, YU H H. System and method for video detection and tracking: U.S. Patent Application 13/720, 653 [P]. 2014- 06- 19.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MzAwMzFGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rmkva1c3L0FKVms9Tmo3QmFyTzRIdEhPcDR4&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> LOWE D G. Distinctive image features from scale-invariant keypoints [J]. International Journal of Computer Vision, 2004, 60 (2) : 91-110.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A general framework for object detection">

                                <b>[21]</b> PAPAGEORGIOU C P, OREN M, POGGIO T. A general framework for object detection [C]// Proceedings of the 6th International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 1998: 555-562.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">

                                <b>[22]</b> DALAL N, TRIGGS B. Histograms of oriented gradients for human detection [C]// Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2005, 1: 886-893
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600742093&amp;v=MzE3NzA5RlkrOE5ESFU2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpKS0Y0Y2J4VT1OaWZPZmJLN0h0RE5xWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> OJALA T, PIETIKINEN M, HARWOOD D. A comparative study of texture measures with classification based on featured distribution [J]. Pattern Recognition, 1996, 29 (1) : 51-59.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SIFT: Predicting amino acid changes that affect protein function">

                                <b>[24]</b> NG P C, HENIKOFF S. SIFT: predicting amino acid changes that affect protein function [J]. Nucleic Acids Research, 2003, 31 (13) : 3812-3814.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340003&amp;v=MzE5MjRGaS9rVzcvQUpWaz1OajdCYXJPNEh0SE5ySXRGWk9zTVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> SCHAPIRE R E, SINGER Y. Improved boosting algorithms using confidence-rated predictions [J]. Machine Learning, 1999, 37 (3) : 297-336.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A tutorial on v-support vector machines">

                                <b>[26]</b> CHEN P-H, LIN C-J, SCHÖLKOPF B. A tutorial on <i>v</i>-support vector machines [J]. Applied Stochastic Models in Business and Industry, 2005, 21 (2) : 111-136.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH201508010&amp;v=MTk5MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURsVzdyUE1pWElackc0SDlUTXA0OUVaSVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> 刘操, 郑宏, 黎曦, 等.基于多通道融合HOG特征的全天候运动车辆检测方法[J].武汉大学学报 (信息科学版) , 2015, 40 (8) :1048-1053. (LIU C, ZHENG H, LI X, et al. A method of moving vehicle detection in all-weather based on melted multi-channel HOG feature [J]. Geomatics and Information Science of Wuhan University, 2015, 40 (8) : 1048-1053.) 
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[28]</b> KRIZHEVSKY A, SUTSKEVER I, HINTON G E. ImageNet classification with deep convolutional neural networks [C]// Proceedings of the 2012 Advances in Neural Information Processing Systems. Piscataway, NJ: IEEE, 2012: 1097-1105.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Over Feat:integrated recognition,localization and detection using convolutional networks">

                                <b>[29]</b> SERMANET P, EIGEN D, ZHANG X, et al. OverFeat: integrated recognition, localization and detection using convolutional networks [EB/OL]. (2014- 02- 24) [2018- 07- 28]. https://arxiv.org/pdf/1312.6229v4.pdf.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[30]</b> GIRSHICK R, DONAHUE J, DARRELL T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation [C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2014: 580-587.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_31" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=Mjc1Mzd4VT1OajdCYXJLN0h0bk1yWTlGWk9vTUNuODlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyakpLRjRjYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[31]</b> UIJLINGS J R R, van de SANDE K E A, GEVERS T, et al. Selective search for object recognition [J]. International Journal of Computer Vision, 2013, 104 (2) : 154-171.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_32" >
                                    <b>[32]</b>
                                 GIRSHICK R. Fast R-CNN [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision. Piscataway, NJ: IEEE, 2015: 1440-1448.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhancement of SSD by concatenating feature maps for object detection">

                                <b>[33]</b> JEONG J, PARK H, KWAK N. Enhancement of SSD by concatenating feature maps for object detection [EB/OL]. (2017- 05- 26) [2018- 07- 29]. https://arxiv.org/pdf/1705.09587v1.pdf.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DSSD:Deconvolutional Single Shot Detector[C/OL]">

                                <b>[34]</b> FU C-Y, LIU W, RANGA A, et al. DSSD: deconvolutional single shot detector [EB/OL]. (2017- 01- 23) [2018- 07- 28]. https://arxiv.org/pdf/1701.06659v1.pdf.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">

                                <b>[35]</b> REDMON J, FARHADI A. YOLO9000: better, faster, stronger [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2017: 6517-6525.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An Incremental Improvement[C/OL]">

                                <b>[36]</b> REDMON J, FARHADI A. YOLOv3: an incremental improvement [EB/OL]. (2018- 04- 08) [2018- 07- 30]. https://arxiv.org/pdf/1804.02767v1.pdf.
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[37]</b> HE K, GKIOXARI G, DOLLAR P, et al. Mask R-CNN [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision. Washington, DC: IEEE Computer Society, 2017: 2980-2988.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[38]</b> SIMONYAN K, ZISSERMAN A. Very deep convolutional networks for large-scale image recognition [EB/OL]. (2015- 04- 10) [2018- 07- 25]. https://arxiv.org/pdf/1409.1556v6.pdf.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[39]</b> HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2016: 770-778.
                            </a>
                        </p>
                        <p id="81">
                            <a id="bibliography_40" >
                                    <b>[40]</b>
                                 SZEGEDY C, VANHOUCKE V, IOFFE S, et al. Rethinking the inception architecture for computer vision [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ:IEEE, 2016: 2818-2826.
                            </a>
                        </p>
                        <p id="83">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:common objects in context">

                                <b>[41]</b> LIN T-Y, MAIRE M, BELONGIE S, et al. Microsoft COCO: common objects in context [C]// Proceedings of the 2014 European Conference on Computer Vision. Berlin: Springer, 2014: 740-755.This work is partially supported by the Shanghai Science and Technology Commission Fund Project (17511104502) .XU Zihao, born in 1995, M. S. candidate. His research interests include deep learning, computer vision.HUANG Weiquan, born in 1994, M. S. candidate. His research interests include deep learning, computer vision.WANG Yin, born in 1979, Ph. D., professor. His research interests include deep learning, artificial intelligence.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201903015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903015&amp;v=MjQ0NTlHRnJDVVI3cWZadVpwRmlEbFc3clBMejdCZDdHNEg5ak1ySTlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
