<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136444778877500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911012%26RESULT%3d1%26SIGN%3dUeTiw0g2MXaEz8pNRUF50ZwPoaE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911012&amp;v=MDIzNjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcjNLTHo3QmQ3RzRIOWpOcm85RVpvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#38" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="1.1 &lt;b&gt;卷积神经网络&lt;/b&gt;">1.1 <b>卷积神经网络</b></a></li>
                                                <li><a href="#41" data-title="1.2 &lt;b&gt;特征提取机制&lt;/b&gt;">1.2 <b>特征提取机制</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="2 基于背景色的数据增强算法 ">2 基于背景色的数据增强算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="2.1 Mnist&lt;b&gt;数据集转换与处理&lt;/b&gt;">2.1 Mnist<b>数据集转换与处理</b></a></li>
                                                <li><a href="#51" data-title="2.2 &lt;b&gt;数据增强算法&lt;/b&gt;">2.2 <b>数据增强算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="3.1 &lt;b&gt;模型及参数&lt;/b&gt;">3.1 <b>模型及参数</b></a></li>
                                                <li><a href="#67" data-title="3.2 &lt;b&gt;基于&lt;/b&gt;Mnist&lt;b&gt;数据集的实验分析&lt;/b&gt;">3.2 <b>基于</b>Mnist<b>数据集的实验分析</b></a></li>
                                                <li><a href="#92" data-title="3.3 &lt;b&gt;数据增强实验分析&lt;/b&gt;">3.3 <b>数据增强实验分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#37" data-title="图1 误分类图片样本">图1 误分类图片样本</a></li>
                                                <li><a href="#46" data-title="图2 Mnist数据集样本">图2 Mnist数据集样本</a></li>
                                                <li><a href="#50" data-title="图3 Mnist数据集转换方式">图3 Mnist数据集转换方式</a></li>
                                                <li><a href="#59" data-title="图4 RGB数值对应颜色效果">图4 RGB数值对应颜色效果</a></li>
                                                <li><a href="#63" data-title="图5 基于背景色的数据增强算法框架">图5 基于背景色的数据增强算法框架</a></li>
                                                <li><a href="#70" data-title="图6 训练集&lt;i&gt;A&lt;/i&gt;与测试集&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;背景色信息">图6 训练集<i>A</i>与测试集<i>X</i><sub>1</sub>背景色信息</a></li>
                                                <li><a href="#72" data-title="图7 测试集&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;背景色信息">图7 测试集<i>X</i><sub>2</sub>背景色信息</a></li>
                                                <li><a href="#73" data-title="图8 使用训练集&lt;i&gt;A&lt;/i&gt;训练时 测试集&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;、&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;、&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;3&lt;/sub&gt;的准确率">图8 使用训练集<i>A</i>训练时 测试集<i>X</i><sub>1</sub>、<i>X</i><sub>2</sub>、<i>X</i><sub>3</sub>的准确率</a></li>
                                                <li><a href="#75" data-title="图9 测试集&lt;i&gt;X&lt;/i&gt;&lt;sub&gt;3&lt;/sub&gt;图片与对应标签">图9 测试集<i>X</i><sub>3</sub>图片与对应标签</a></li>
                                                <li><a href="#79" data-title="图10 训练集&lt;i&gt;B&lt;/i&gt;和测试集&lt;i&gt;Y&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;背景色信息">图10 训练集<i>B</i>和测试集<i>Y</i><sub>1</sub>背景色信息</a></li>
                                                <li><a href="#81" data-title="图11 测试集&lt;i&gt;Y&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;背景色信息">图11 测试集<i>Y</i><sub>2</sub>背景色信息</a></li>
                                                <li><a href="#84" data-title="图12 使用训练集&lt;i&gt;B&lt;/i&gt;训练时 测试集&lt;i&gt;Y&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;、&lt;i&gt;Y&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;的准确率">图12 使用训练集<i>B</i>训练时 测试集<i>Y</i><sub>1</sub>、<i>Y</i><sub>2</sub>的准确率</a></li>
                                                <li><a href="#85" data-title="图13 不同训练集可学习到的特征">图13 不同训练集可学习到的特征</a></li>
                                                <li><a href="#88" data-title="图14 &lt;i&gt;k&lt;/i&gt;取不同值时的背景色信息">图14 <i>k</i>取不同值时的背景色信息</a></li>
                                                <li><a href="#90" data-title="图15 不同&lt;i&gt;k&lt;/i&gt;值对应的测试集&lt;i&gt;Z&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;和&lt;i&gt;Z&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;的准确率">图15 不同<i>k</i>值对应的测试集<i>Z</i><sub>1</sub>和<i>Z</i><sub>2</sub>的准确率</a></li>
                                                <li><a href="#95" data-title="图16 训练集与测试集&lt;i&gt;T&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;样本">图16 训练集与测试集<i>T</i><sub>1</sub>样本</a></li>
                                                <li><a href="#97" data-title="图17 测试集&lt;i&gt;T&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;样本">图17 测试集<i>T</i><sub>2</sub>样本</a></li>
                                                <li><a href="#98" data-title="图18 测试集&lt;i&gt;T&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;、&lt;i&gt;T&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;准确率">图18 测试集<i>T</i><sub>1</sub>、<i>T</i><sub>2</sub>准确率</a></li>
                                                <li><a href="#101" data-title="图19 数据增强后的训练集样本">图19 数据增强后的训练集样本</a></li>
                                                <li><a href="#102" data-title="图20 数据增强后测试集 &lt;i&gt;T&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;、&lt;i&gt;T&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;准确率">图20 数据增强后测试集 <i>T</i><sub>1</sub>、<i>T</i><sub>2</sub>准确率</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="121">


                                    <a id="bibliography_1" title=" 唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报,2018,44(5):855-864.(TANG X L,DU Y M,LIU Y W,et al.Recognition with conditional deep convolutional generative adversarial networks[J].Acta Automatica Sinica,2018,44(5):855-864.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MjY1NzlLS0NMZlliRzRIOW5NcW85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcjM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报,2018,44(5):855-864.(TANG X L,DU Y M,LIU Y W,et al.Recognition with conditional deep convolutional generative adversarial networks[J].Acta Automatica Sinica,2018,44(5):855-864.)
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_2" title=" PINHEIRO P O,COLLOBERT R,DOLLAR P.Learning to segment object candidates[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.New York:ACM,2015:1990-1998." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to segment object candidates">
                                        <b>[2]</b>
                                         PINHEIRO P O,COLLOBERT R,DOLLAR P.Learning to segment object candidates[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.New York:ACM,2015:1990-1998.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_3" title=" REN S,HE K,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,39(6):1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">
                                        <b>[3]</b>
                                         REN S,HE K,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,39(6):1137-1149.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_4" title=" GONG Y,JIA Y,LEUNG T,et al.Deep convolutional ranking for multilabel image annotation[EB/OL].[2018- 04- 14].https://pdfs.semanticscholar.org/3b04/9d8cfea6c3bed377090e0e7fa677d2 82a361.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep convolutional ranking for multilabel image annotation">
                                        <b>[4]</b>
                                         GONG Y,JIA Y,LEUNG T,et al.Deep convolutional ranking for multilabel image annotation[EB/OL].[2018- 04- 14].https://pdfs.semanticscholar.org/3b04/9d8cfea6c3bed377090e0e7fa677d2 82a361.pdf.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_5" title=" RIBEIRO M T,SINGH S,GUESTRIN C.“Why should I trust you?”:Explaining the predictions of any classifier[C]// Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,2016:1135-1144." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Explaining the predictions of any classifier">
                                        <b>[5]</b>
                                         RIBEIRO M T,SINGH S,GUESTRIN C.“Why should I trust you?”:Explaining the predictions of any classifier[C]// Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,2016:1135-1144.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_6" title=" SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018- 04- 10].http://www.cs.virginia.edu/～vicente/recognition/slides/lecture07/iclr2015.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[6]</b>
                                         SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018- 04- 10].http://www.cs.virginia.edu/～vicente/recognition/slides/lecture07/iclr2015.pdf.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1-9.</a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_8" title=" HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[8]</b>
                                         HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_9" title=" HUANG G,LIU Z,LAURENS V D M,et al.Densely connected convolutional networks[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely Connected Convolutional Networks">
                                        <b>[9]</b>
                                         HUANG G,LIU Z,LAURENS V D M,et al.Densely connected convolutional networks[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:2261-2269.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_10" title=" ERHAN D,BENGIO Y,COURVILLE A,et al.Visualizing higher-layer features of a deep network[C]// Proceedings of the 26th Annual International Conference on Machine Learning.New York:ACM,2009:1341-1349." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing higher-layer features of a deep network">
                                        <b>[10]</b>
                                         ERHAN D,BENGIO Y,COURVILLE A,et al.Visualizing higher-layer features of a deep network[C]// Proceedings of the 26th Annual International Conference on Machine Learning.New York:ACM,2009:1341-1349.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_11" title=" NGIAM J,CHEN Z,CHIA D,et al.Tiled convolutional neural networks[C]// Proceedings of the 2010 Conference on Natural Information Processing System.Columbia:MIT Press,2010:1279-1287." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tiled convolutional neural networks">
                                        <b>[11]</b>
                                         NGIAM J,CHEN Z,CHIA D,et al.Tiled convolutional neural networks[C]// Proceedings of the 2010 Conference on Natural Information Processing System.Columbia:MIT Press,2010:1279-1287.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_12" title=" BERKES P,WISKOTT L.On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields[J].Neural Computation,2006,18(8):1868-1895." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012549&amp;v=MzAxMTNyM0lJVndSYUJBPU5pZkpaYks5SHRqTXFvOUZaT29OQ1hnd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         BERKES P,WISKOTT L.On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields[J].Neural Computation,2006,18(8):1868-1895.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_13" title=" DONAHUE J,JIA Y,VINYALS O,et al.DeCAF:a deep convolutional activation feature for generic visual recognition[C]// Proceedings of the 31st International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2014:I-647-I-655." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=De CAF:A Deep Convolutional Activation Feature for Generic Visual Recognition">
                                        <b>[13]</b>
                                         DONAHUE J,JIA Y,VINYALS O,et al.DeCAF:a deep convolutional activation feature for generic visual recognition[C]// Proceedings of the 31st International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2014:I-647-I-655.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_14" title=" 俞海宝,沈琦,冯国灿.在反卷积网络中引入数值解可视化卷积神经网络[J].计算机科学,2017,44(S1):146-150.(YU H B,SHEN Q,FENG G C.Introduce numerical solution to visualize convolutional neuron networks based on numerical solution[J].Computer Science,2017,44(S1):146-150.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S1034&amp;v=MDU3ODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublZyM0tMejdCYjdHNEg5YXZybzlHWUlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         俞海宝,沈琦,冯国灿.在反卷积网络中引入数值解可视化卷积神经网络[J].计算机科学,2017,44(S1):146-150.(YU H B,SHEN Q,FENG G C.Introduce numerical solution to visualize convolutional neuron networks based on numerical solution[J].Computer Science,2017,44(S1):146-150.)
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_15" title=" WANG X,HAMILTON H J.DBRS:a density-based spatial clustering method with random sampling[C]// Proceedings of the 7th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.Berlin:Springer-Verlag,2003:563-575." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DBRS:A Density-Based Spatial Clustering Method with Random Sampling">
                                        <b>[15]</b>
                                         WANG X,HAMILTON H J.DBRS:a density-based spatial clustering method with random sampling[C]// Proceedings of the 7th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.Berlin:Springer-Verlag,2003:563-575.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-09-16 10:38</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3172-3177 DOI:10.11772/j.issn.1001-9081.2019051140            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于特征提取偏好与背景色相关性的数据增强算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%99%E9%B9%B0&amp;code=32439971&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">余鹰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%B9%90%E4%B8%BA&amp;code=39350180&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王乐为</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%BA%94%E9%BE%99&amp;code=32097626&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张应龙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%9C%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E8%BD%AF%E4%BB%B6%E5%AD%A6%E9%99%A2&amp;code=0133870&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华东交通大学软件学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>深度神经网络具有强大的特征自学习能力,可以通过多层逐步提取的方式获取不同层次的粒度特征,但当图片目标本体与背景色具有强相关性时,特征提取会存在“惰性”,所提取特征的抽象层次较低,判别性不足。针对此问题,通过实验对深度神经网络特征提取的内在规律进行研究,发现特征提取偏好与图片背景色之间具有相关性,消除该相关性可以帮助深度神经网络忽略背景的干扰,直接学习目标本体的特征,由此提出了数据增强算法,并在自主构建的数据集上进行实验。实验结果表明,所提算法可以降低背景色对目标本体特征提取的干扰,减少过拟合,提高分类效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据增强;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%83%8C%E6%99%AF%E8%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">背景色;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *余鹰(1979—),女,江西广丰人,副教授,博士,CCF会员,主要研究方向:机器学习、计算机视觉,电子邮箱,yuyingjx@163.com;
                                </span>
                                <span>
                                    王乐为(1993—),男,江苏淮安人,硕士研究生,主要研究方向:深度学习、计算机视觉;;
                                </span>
                                <span>
                                    张应龙(1979—),男,陕西绥德人,副教授,博士,CCF会员,主要研究方向:数据挖掘、网络分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61563016,61762036);</span>
                                <span>江西省自然科学基金资助项目(20181BAB202023,20171BAB202012);</span>
                    </p>
            </div>
                    <h1><b>Data enhancement algorithm based on feature extraction preference and background color correlation</b></h1>
                    <h2>
                    <span>YU Ying</span>
                    <span>WANG Lewei</span>
                    <span>ZHANG Yinglong</span>
            </h2>
                    <h2>
                    <span>College of Software Engineering, East China Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Deep neural network has powerful feature self-learning ability, which can obtain the granularity features of different levels by multi-layer stepwise feature extraction. However, when the target subject of an image has strong correlation with the background color, the feature extraction will be “lazy”, the extracted features are difficult to be discriminated with low abstraction level. To solve this problem, the intrinsic law of feature extraction of deep neural network was studied by experiments. It was found that there was correlation between feature extraction preference and background color of the image. Eliminating this correlation was able to help deep neural network ignore background interference and extract the features of the target subject directly. Therefore, a data enhancement algorithm was proposed and experiments were carried out on the self-built dataset. The experimental results show that the proposed algorithm can reduce the interference of background color on the extraction of target features, reduce over-fitting and improve classification effect.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20enhancement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data enhancement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=background%20color&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">background color;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YU Ying, born in 1979, Ph. D., associate professor. Her research interests include machine learning, computer vision. ;
                                </span>
                                <span>
                                    WANG Lewei, born in 1993, M. S. candidate. His research interests include deep learning, computer vision. ;
                                </span>
                                <span>
                                    ZHANG Yinglong, born in 1979, Ph. D., associate professor. His research interests include data mining, network analysis.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China(61563016,61762036);</span>
                                <span>the Natural Science Foundation of Jiangxi Province(20181BAB202023,20171BAB202012);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="34">近年来,深度学习在目标分类<citation id="151" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、分割<citation id="152" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>和检测<citation id="153" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等计算机视觉领域取得了突破性进展,成为最有效的图像特征提取方法之一。在深度学习之前,常用的特征提取方法多为人工设计,如方向梯度直方图(Histogram of Oriented Gradient, HOG)、尺度不变特征变换(Scale-Invariant Feature Transform, SIFT)等,这些特征在特定类型对象中能够达到较好的识别效果,但所提取特征往往层次较低、抽象程度不高、判别力不足。文献<citation id="154" type="reference">[<a class="sup">4</a>]</citation>将传统的人工设计的特征提取方法与深度学习的方法进行了比较,发现后者提取的特征可以获得更好的图像分类效果。深度学习通过监督或非监督的方式,从大量的数据中逐层自动地学习目标的特征表示,将原始数据经过一系列非线性变换,从中提取由低层到高层、由具体到抽象、由一般到特定语义的特征,生成高层次的抽象表示,避免了手工设计特征的繁琐低效。</p>
                </div>
                <div class="p1">
                    <p id="35">虽然深度学习在图像特征自动提取方面效果很好,但与传统的特征提取算法相比,过于依赖大规模的训练数据,主要是因为当前主流深度网络模型含有的参数一般都是数以百万计,为了保证模型可以正确工作需要大量的数据进行训练,以便不断修正模型的参数,但现实世界中,所获取的数据可能是在有限条件下拍摄的。当训练数据有限,无法表现所有情况时,所提取的特征可能不具备普适性。同时,由于深度学习模型缺乏良好的可解释性,很难理解模型内部的行为,所提取的特征到底来自图片的哪块区域是无法确定的,导致特征提取可能存在偏差。例如,文献<citation id="155" type="reference">[<a class="sup">5</a>]</citation>曾指出,如图1所示,在样本的刻意选取下,因为图片背景是雪地,哈士奇被识别成了狼,分类模型利用了图片的白色背景,完全忽略了动物本体的特征。此时,需要通过数据增强的方法对输入进行干涉,用变换过的数据来训练深度神经网络,帮助模型学习到本体的特征,以提高泛化能力。</p>
                </div>
                <div class="p1">
                    <p id="36">因此,本文对深度神经网络的特征提取偏好进行研究,寻找特征提取偏好与数据集背景色的相关性,并在此基础上提出了相应的数据增强算法,减少过拟合。通过对训练图片进行背景色变换得到泛化能力更强的网络,使得模型在面对目标本体与背景色具有强相关性的数据集时,特征学习能够不受背景色的干扰,能真正地学习到目标本体的特征,提高分类的性能。</p>
                </div>
                <div class="area_img" id="37">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 误分类图片样本" src="Detail/GetImg?filename=images/JSJY201911012_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 误分类图片样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Misclassified samples</p>

                </div>
                <h3 id="38" name="38" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="39" name="39">1.1 <b>卷积神经网络</b></h4>
                <div class="p1">
                    <p id="40">卷积神经网络(Convolutional Neural Network, CNN)是一种深度神经网络模型,主要由卷积层、池化层和全连接层构成。近年来,CNN得到不断的发展,被广泛应用于图像处理领域中。AlexNet采用ReLU(Rectified Linear Unit)、Dropout等技术,降低了模型的计算复杂度,训练速度也提升了几倍,使模型更具有鲁棒性,并减少了全连接层的过拟合。VGG(Visual Geometry Group)<citation id="156" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>模型采用具有小卷积核的多个卷积层替换一个具有较大卷积核的卷积层,这种替换方式减少了参数的数量,而且也能够使决策函数更具有判别性。随后GoogLeNet<citation id="157" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>模型提出了Inception结构,加入了并行传播,使用了3种类型的卷积操作,提升了计算资源的利用率,但是模型的参数仅是AlexNet的1/12。为了利用网络深度对特征提取的影响,He等<citation id="158" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了ResNet网络模型,引入了残差结构,使得深度网络可达到1 000多层,从而提取出更加精确的特征。由于ResNet直接通过“Summation”操作将特征相加,一定程度上阻碍了网络中的信息流,继而又出现了DenseNet<citation id="159" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。该结构通过连接操作来结合feature map,每一层都与其他层相关,使得信息流可以最大化,提升了网络的鲁棒性并且加快了学习速度。</p>
                </div>
                <h4 class="anchor-tag" id="41" name="41">1.2 <b>特征提取机制</b></h4>
                <div class="p1">
                    <p id="42">目前已有一些学者对深度神经网络的特征提取机制进行研究,主要是通过可视化的方式,大多是针对第一层,主要是因为第一层比较容易提取到像素级特征,而较高的网络层则难以处理,但仍有一些方法从不同角度进行了尝试,比如:文献<citation id="160" type="reference">[<a class="sup">10</a>]</citation>通过在图像空间做梯度下降得到每个节点的最大响应,由此推断出节点的活跃性,但没有给出关于节点某种恒定属性的描述。文献<citation id="161" type="reference">[<a class="sup">11</a>]</citation>受此启发,对文献<citation id="162" type="reference">[<a class="sup">12</a>]</citation>提出的方法进行改进,通过计算一个节点的Hessian矩阵来观测节点的一些稳定的属性,但对于高层的网络节点,这些属性变量过于复杂。文献<citation id="163" type="reference">[<a class="sup">13</a>]</citation>通过可视化确定模型中高层节点究竟是被哪一块区域激活,但没有对节点属性进行描述,而是看图像的哪一部分激活了特征。与此类似的是,文献<citation id="164" type="reference">[<a class="sup">14</a>]</citation>通过在反卷积网络中引入数值解可视化CNN,探究CNN运行良好的机制。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">2 基于背景色的数据增强算法</h3>
                <h4 class="anchor-tag" id="44" name="44">2.1 Mnist<b>数据集转换与处理</b></h4>
                <div class="p1">
                    <p id="45">由于Mnist数据集的目标本体与背景信息易于区分,如图2所示,有利于分析深度神经网络的特征提取机制,故本文选择对Mnist数据集进行处理与转化,其中主要进行了三种情况的处理。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Mnist数据集样本" src="Detail/GetImg?filename=images/JSJY201911012_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Mnist数据集样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Mnist dataset samples</p>

                </div>
                <div class="p1">
                    <p id="47">第一种情况是将原Mnist训练集的图片转换成一种数字对应一种背景色的训练集<i>A</i>,例如将数字1的背景色转换成蓝色、将数字2的背景色转换成红色。对原Mnist测试集进行相同的转换,得到测试集<i>X</i><sub>1</sub>;再将原Mnist测试集的每种数字背景色转换成除训练集<i>A</i>对应颜色外的9种颜色,如数字1的背景色除了蓝色可为任意其他9种颜色,数字2除了红色可为任意其他9种颜色,将满足该条件的测试集称作测试集<i>X</i><sub>2</sub>;另外将测试集<i>X</i><sub>2</sub>的每张图片的标签更换成其背景色在训练集<i>A</i>上所对应的数字类别,得到测试集<i>X</i><sub>3</sub>,例如背景色为蓝色的数字标签均为1。使用训练集<i>A</i>训练,分别在测试集<i>X</i><sub>1</sub>、<i>X</i><sub>2</sub>、<i>X</i><sub>3</sub>上进行测试,可以分析模型是否只学习到背景颜色特征。</p>
                </div>
                <div class="p1">
                    <p id="48">第二种情况将原Mnist训练集的图片转换成每种数字可对应10种背景色的训练集<i>B</i>,按照同样的转换方法对原测试集进行转换得到测试集<i>Y</i><sub>1</sub>。重新搜集10种颜色,这10种颜色与训练集<i>B</i>的10种颜色无交集,根据这10种颜色,按照同样的转换方法得到测试集<i>Y</i><sub>2</sub>。通过在训练集<i>B</i>上进行训练,在测试集<i>Y</i><sub>1</sub>和<i>Y</i><sub>2</sub>上进行测试,可以分析当每种数字背景色复杂时,模型是否会学习到数字的自身特征。</p>
                </div>
                <div class="p1">
                    <p id="49">为了便于不同颜色的区分,第三种情况只选择两种数字类别,将原Mnist训练集上的数字0和数字1进行背景色转换得到训练集<i>C</i>,每种数字独享<i>k</i>种背景色,如当<i>k</i>=1时,数字0的背景色为天蓝色,数字1的背景色为粉色,当<i>k</i>=2时,数字0的背景色为天蓝色或者黄色,数字1的背景色为粉色或紫色,按照同样的转换方法对原测试集进行转换得到测试集<i>Z</i><sub>1</sub>。交换两类数字的背景色得到测试集<i>Z</i><sub>2</sub>,如当<i>k</i>=2时,训练集中数字1的背景色为粉色或紫色,而在测试集<i>Z</i><sub>2</sub>中数字0的背景色为粉色或紫色。通过在训练集<i>C</i>上进行训练,在测试集<i>Z</i><sub>1</sub>和测试集<i>Z</i><sub>2</sub>上进行测试,可以分析模型是否会因为某一类数字对应一定集合范围内的颜色,从而通过“蛮力”的统计方式去记住每种数字对应的颜色信息,而不去学习到数字的形状特征。以上处理方式如图3所示。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Mnist数据集转换方式" src="Detail/GetImg?filename=images/JSJY201911012_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Mnist数据集转换方式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Conversion methods of Mnist dataset</p>

                </div>
                <h4 class="anchor-tag" id="51" name="51">2.2 <b>数据增强算法</b></h4>
                <div class="p1">
                    <p id="52">设有数据集<i>D</i>,其中图片<i>X</i><sub><i>i</i></sub>∈<i>D</i>,<i>r</i><sub><i>i</i></sub><sub>1</sub>,<i>r</i><sub><i>i</i></sub><sub>2</sub>,…,<i>r</i><sub><i>i</i></sub><sub>100</sub>是从图片<i>X</i><sub><i>i</i></sub>中随机挑选的100个像素点的RGB值,使用DBSCAN(Density-Based Spatial Clustering of Applications with Noise)<citation id="165" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>算法对这100个像素点进行基于密度的聚类,<i>Center</i><sub><i>i</i></sub><sub>1</sub>,<i>Center</i><sub><i>i</i></sub><sub>2</sub>,…,<i>Center</i><sub><i>im</i></sub>分别为图片<i>X</i><sub><i>i</i></sub>经密度聚类后对应的<i>m</i>个簇中心点的RGB值,<i>Distacnce</i><sub><i>i</i></sub><sub>1</sub>,<i>Distacne</i><sub><i>i</i></sub><sub>2</sub>,…,<i>Distacne</i><sub><i>im</i></sub>分别为每个簇中心点到其他<i>m</i>-1个簇中心点的距离和,如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>E</mi></mstyle><mi>u</mi><mi>c</mi><mi>l</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo stretchy="false">(</mo><mi>C</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mi>C</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">为了避免随机挑选的点过多地出现在目标本体上,且由于目标本体与背景色的RGB值差异较大,故将最大值去掉,剩下的<i>m</i>-1个簇中心点的RGB值作为图片<i>X</i><sub><i>i</i></sub>的背景色RGB代表值,分别为:<i>BG</i><sub><i>i</i></sub><sub>1</sub>,<i>BG</i><sub><i>i</i></sub><sub>2</sub>,…,<i>BG</i><sub><i>i</i></sub><sub>(</sub><sub><i>m</i></sub><sub>-1)</sub>。</p>
                </div>
                <div class="p1">
                    <p id="55"><i>P</i><sub><i>ie</i></sub>为图片<i>X</i><sub><i>i</i></sub>的像素点RGB值,计算像素点<i>P</i><sub><i>ie</i></sub>与图片<i>X</i><sub><i>i</i></sub>中每个背景色代表值<i>BG</i><sub><i>iq</i></sub>的RGB差异度<i>Difference</i><sub><i>ieq</i></sub>,如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="56"><i>Difference</i><sub><i>ieq</i></sub>=(<i>RP</i><sub><i>ie</i></sub>-<i>RBG</i><sub><i>iq</i></sub>)<sup>2</sup>+(<i>GP</i><sub><i>ie</i></sub>-<i>GBG</i><sub><i>ie</i></sub>)<sup>2</sup>+(<i>BP</i><sub><i>ie</i></sub>-<i>BBG</i><sub><i>iq</i></sub>)<sup>2</sup>; 1≤<i>q</i>≤<i>m</i>-1      (2)</p>
                </div>
                <div class="p1">
                    <p id="58">其中:<i>RP</i><sub><i>ie</i></sub>、<i>GP</i><sub><i>ie</i></sub>、<i>BP</i><sub><i>ie</i></sub>分别表示图片<i>X</i><sub><i>i</i></sub>的像素点<i>P</i><sub><i>ie</i></sub>的三个通道值,<i>RBG</i><sub><i>iq</i></sub>、<i>GBG</i><sub><i>iq</i></sub>、<i>BBG</i><sub><i>iq</i></sub>分别表示图片<i>X</i><sub><i>i</i></sub>背景色代表值<i>BG</i><sub><i>iq</i></sub>的三个通道值。采用平方项的方式是由于改变某一通道数值产生的颜色变化效果明显于将改变的数值分布在三个通道产生变化的效果,如图4所示,故使用平方项可将在一个通道改变过多所导致的差异值放大。</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 RGB数值对应颜色效果" src="Detail/GetImg?filename=images/JSJY201911012_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 RGB数值对应颜色效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 RGB values and corresponding colors</p>

                </div>
                <div class="p1">
                    <p id="60"><i>C</i>是一组RGB差异性较大的颜色集合,在对每张图片<i>X</i><sub><i>i</i></sub>进行转换前,先从集合<i>C</i>中随机选择一种颜色<i>c</i><sub><i>g</i></sub>,对图片<i>X</i><sub><i>i</i></sub>中每个像素点<i>P</i><sub><i>ie</i></sub>与<i>m</i>-1个RGB差异值进行比较,若存在一个差异值<i>Difference</i><sub><i>ieq</i></sub>小于阈值1 000,则将该像素点RGB值用<i>c</i><sub><i>g</i></sub>进行替代,若像素点<i>P</i><sub><i>ie</i></sub>中<i>m</i>-1个差异值都大于1 000将不做变换,如式(3)所示:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>e</mi></mrow></msub><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>c</mi><msub><mrow></mrow><mi>g</mi></msub><mo>,</mo><mtext> </mtext><mo>∃</mo><mi>q</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>m</mi><mo>-</mo><mn>1</mn><mo stretchy="false">}</mo><mo>,</mo><mi>d</mi><mi>i</mi><mi>f</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>e</mi><mi>q</mi></mrow></msub><mo>≤</mo><mn>1</mn><mtext> </mtext><mn>0</mn><mn>0</mn><mn>0</mn></mtd></mtr><mtr><mtd><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>e</mi></mrow></msub><mo>,</mo><mtext> </mtext><mo>∀</mo><mi>q</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>m</mi><mo>-</mo><mn>1</mn><mo stretchy="false">}</mo><mo>,</mo><mi>d</mi><mi>i</mi><mi>f</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>e</mi><mi>q</mi></mrow></msub><mo>&gt;</mo><mn>1</mn><mtext> </mtext><mn>0</mn><mn>0</mn><mn>0</mn></mtd></mtr></mtable></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">整体算法框架如图5所示。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于背景色的数据增强算法框架" src="Detail/GetImg?filename=images/JSJY201911012_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基于背景色的数据增强算法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Framework of data enhancement algorithm based on background color</p>

                </div>
                <h3 id="64" name="64" class="anchor-tag">3 实验与分析</h3>
                <h4 class="anchor-tag" id="65" name="65">3.1 <b>模型及参数</b></h4>
                <div class="p1">
                    <p id="66">本文所采用的CNN包含三层卷积池化层和三层全连接层,每层的卷积核大小均为3×3,卷积核的个数分别为64、128、256,池化层均采用最大池化且池化核的大小为2×2,每次池化后都进行比例为0.5的Dropout,三层全连接层的大小分别为128、64、32,最后一层为Softmax层。此外,批大小(batch size)为64,学习率(learning rate)设置为10<sup>-4</sup>,优化算法选择了随机梯度下降算法。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">3.2 <b>基于</b>Mnist<b>数据集的实验分析</b></h4>
                <h4 class="anchor-tag" id="68" name="68">3.2.1 第一种情况分析</h4>
                <div class="p1">
                    <p id="69">图6为第一种情况的训练集<i>A</i>和测试集<i>X</i><sub>1</sub>背景色信息,图中数字为相应颜色的RGB值。使用训练集<i>A</i>进行训练,并在测试集<i>A</i>进行测试。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 训练集A与测试集X1背景色信息" src="Detail/GetImg?filename=images/JSJY201911012_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 训练集<i>A</i>与测试集<i>X</i><sub>1</sub>背景色信息  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Background color information of training set <i>A</i> and test set <i>X</i><sub>1</sub></p>

                </div>
                <div class="p1">
                    <p id="71">图7为测试集<i>X</i><sub>2</sub>的背景色信息,每个数字的背景色不使用训练集<i>A</i>中相应数字对应的背景色,如数字0的背景色除RGB值为(230, 189, 128)外,可为其他任意背景色。使用训练集<i>A</i>进行训练,测试集<i>B</i>进行测试,测试准确率如图8所示。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 测试集X2背景色信息" src="Detail/GetImg?filename=images/JSJY201911012_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 测试集<i>X</i><sub>2</sub>背景色信息  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Background color information of test set <i>X</i><sub>2</sub></p>

                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 使用训练集A训练时 测试集X1、X2、X3的准确率" src="Detail/GetImg?filename=images/JSJY201911012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 使用训练集<i>A</i>训练时 测试集<i>X</i><sub>1</sub>、<i>X</i><sub>2</sub>、<i>X</i><sub>3</sub>的准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Accuracy of test set <i>X</i><sub>1</sub>、<i>X</i><sub>2</sub>、<i>X</i><sub>3</sub> trained by training set <i>A</i></p>

                </div>
                <div class="p1">
                    <p id="74">为了进一步验证训练集<i>A</i>是否使模型只学习到背景色特征,而忽略数字的形状等自身特征,构建测试集<i>X</i><sub>3</sub>,测试集<i>X</i><sub>3</sub>与测试集<i>X</i><sub>2</sub>图片相同,但将标签进行更改,每张图片更改后的标签为其背景色在训练集<i>A</i>上所对应的数字,如将背景色RGB值为(21, 182, 18)的数字1的标签视为3,如图9所示。同样使用训练集<i>A</i>进行训练,测试集<i>X</i><sub>3</sub>进行测试,测试准确率如图8所示。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 测试集X3图片与对应标签" src="Detail/GetImg?filename=images/JSJY201911012_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 测试集<i>X</i><sub>3</sub>图片与对应标签  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Images of test set <i>X</i><sub>3</sub> and corresponding labels</p>

                </div>
                <div class="p1">
                    <p id="76">从图8中可以看出,测试集<i>X</i><sub>2</sub>的准确率始终较低,而测试集<i>X</i><sub>1</sub>和测试集<i>X</i><sub>3</sub>都在较短的时间内完成收敛并达到较高的准确率。可见使用训练集<i>A</i>并不能让模型学习到数字自身的特征,而是将图片背景色作为分类的依据。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">3.2.2 第二种情况分析</h4>
                <div class="p1">
                    <p id="78">图10为第二种情况下的训练集<i>B</i>和测试集<i>Y</i><sub>1</sub>背景色信息,其中每个数字的背景色可为10种颜色。使用训练集<i>B</i>进行训练,测试集<i>Y</i><sub>1</sub>进行测试。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 训练集B和测试集Y1背景色信息" src="Detail/GetImg?filename=images/JSJY201911012_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 训练集<i>B</i>和测试集<i>Y</i><sub>1</sub>背景色信息  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Background color information of training set <i>B</i> and test set <i>Y</i><sub>1</sub></p>

                </div>
                <div class="p1">
                    <p id="80">图11为测试集<i>Y</i><sub>2</sub>的背景色信息,每个数字背景色可为10种颜色,但与训练集<i>B</i>中的10种背景色无重复。使用训练集<i>B</i>进行训练,测试集<i>Y</i><sub>2</sub>进行测试,准确率如图12所示。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 测试集Y2背景色信息" src="Detail/GetImg?filename=images/JSJY201911012_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 测试集<i>Y</i><sub>2</sub>背景色信息  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Background color information of test set <i>Y</i><sub>2</sub></p>

                </div>
                <div class="p1">
                    <p id="82">从图12中可以看出,模型经训练集<i>B</i>训练到一定程度时,其在测试集<i>Y</i><sub>1</sub>和<i>Y</i><sub>2</sub>上准确率较接近,达到了较好的识别效果。可见,当训练集每个数字的背景色变得复杂时,模型能够学习到数字的自身特征。</p>
                </div>
                <div class="p1">
                    <p id="83">以上实验表明,使用训练集<i>A</i>可使模型学到背景的颜色特征,使用训练集<i>B</i>可学习到数字的自身特征,其可能原因在于当一种背景色对应一种数字时,背景色可作为数字的主要特征,而当使用训练集<i>B</i>或原Mnist训练集时,每种颜色可对应任意数字,颜色不能作为分类的主要依据,需进一步学习其他特征。但如果每类数字对应一定集合范围内的背景色,且集合无交集时,使用满足这种条件的训练集是否能让模型学习到数字的特征,还是仅能学到每类数字对应的各种颜色信息,具体如图13所示。</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 使用训练集B训练时 测试集Y1、Y2的准确率" src="Detail/GetImg?filename=images/JSJY201911012_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 使用训练集<i>B</i>训练时 测试集<i>Y</i><sub>1</sub>、<i>Y</i><sub>2</sub>的准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Accuracy of test set <i>Y</i><sub>1</sub>、<i>Y</i><sub>2</sub> trained by training set <i>B</i></p>

                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 不同训练集可学习到的特征" src="Detail/GetImg?filename=images/JSJY201911012_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 不同训练集可学习到的特征  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Features learned from different training sets</p>

                </div>
                <h4 class="anchor-tag" id="86" name="86">3.2.3 第三种情况分析</h4>
                <div class="p1">
                    <p id="87">为了进一步分析模型所学特征与数据集背景色的关系,进行了第三种情况的实验,让每种数字对应一个背景色集合,且集合间无交集。目的是验证此种情况下是否能让模型学习到数字的特征,而不是每种数字对应的背景色信息,如图14所示,分别给出了<i>k</i>=1和<i>k</i>=10时,训练集<i>C</i>和测试集<i>Z</i><sub>1</sub>以及测试集<i>Z</i><sub>2</sub>所对应的背景色信息。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 k取不同值时的背景色信息" src="Detail/GetImg?filename=images/JSJY201911012_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 <i>k</i>取不同值时的背景色信息  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 14 Background color information with different <i>k</i></p>

                </div>
                <div class="p1">
                    <p id="89">当<i>k</i>取值从1逐渐增大到10时,使用训练集<i>C</i>进行训练,并在测试集<i>Z</i><sub>1</sub>和<i>Z</i><sub>2</sub>上测试,<i>k</i>取值和对应的准确率情况如图15(a)至(j)所示。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 不同k值对应的测试集Z1和Z2的准确率" src="Detail/GetImg?filename=images/JSJY201911012_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图15 不同<i>k</i>值对应的测试集<i>Z</i><sub>1</sub>和<i>Z</i><sub>2</sub>的准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 15 Accuracy of test set <i>Z</i><sub>1</sub>、<i>Z</i><sub>2</sub> corresponding to different <i>k</i> value</p>

                </div>
                <div class="p1">
                    <p id="91">由图15可见,<i>Z</i><sub>1</sub>由于训练集和测试集颜色一致,所以准确率始终保持良好。当<i>k</i>的值较小时,<i>Z</i><sub>2</sub>的准确率很低,这主要是因为这此时将背景色作为判断依据,但是由于训练集和测试集背景色不一致,导致分类性能很差。随着<i>k</i>的增大,<i>Z</i><sub>2</sub>的准确率在逐渐提高,说明随着背景色越来越多,区分能力越来越弱,分类模型已经开始学习数字本体的特征进行判别。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">3.3 <b>数据增强实验分析</b></h4>
                <div class="p1">
                    <p id="93">通过以上的实验,本文发现了深度学习模型的特征提取能力与数据集的背景色具有一定的关系,故本文搜集了猫头鹰与海鸥这一类背景色有特点的图片构建数据集,由于猫头鹰大多在晚上出现,海鸥多数出现在海上或蓝天上飞行,所以猫头鹰的图片的背景大多以黑色系为主,海鸥主要以蓝色系为主。</p>
                </div>
                <div class="p1">
                    <p id="94">如图16所示,训练集共有600张,背景色信息主要为猫头鹰对应黑色、海鸥对应蓝色。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图16 训练集与测试集T1样本" src="Detail/GetImg?filename=images/JSJY201911012_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图16 训练集与测试集<i>T</i><sub>1</sub>样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 16 Samples of training set and test set <i>T</i><sub>1</sub></p>

                </div>
                <div class="p1">
                    <p id="96">如图17所示,测试集分为<i>T</i><sub>1</sub>和<i>T</i><sub>2</sub>两种情况,分别为200张与100张,<i>T</i><sub>1</sub>与训练集背景色信息相同,<i>T</i><sub>2</sub>则与其相反,即海鸥对应黑色背景和猫头鹰对应蓝色背景。由于满足<i>T</i><sub>2</sub>条件的图片较少,故<i>T</i><sub>1</sub>与<i>T</i><sub>2</sub>图片数量分别为200和100张。使用训练集进行训练,分别对测试集<i>T</i><sub>1</sub>和<i>T</i><sub>2</sub>进行测试,准确率如图18所示。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图17 测试集T2样本" src="Detail/GetImg?filename=images/JSJY201911012_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图17 测试集<i>T</i><sub>2</sub>样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 17 Samples of test set <i>T</i><sub>2</sub></p>

                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图18 测试集T1、T2准确率" src="Detail/GetImg?filename=images/JSJY201911012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图18 测试集<i>T</i><sub>1</sub>、<i>T</i><sub>2</sub>准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 18 Accuracy of test set <i>T</i><sub>1</sub>、<i>T</i><sub>2</sub></p>

                </div>
                <div class="p1">
                    <p id="99">由图18可以看出,模型在测试集<i>T</i><sub>1</sub>上很快完成收敛,并达到较高准确率,而在测试集<i>T</i><sub>2</sub>上则表现较差,且在<i>T</i><sub>2</sub>上准确率随着在<i>T</i><sub>1</sub>上准确率的升高而下降,可以看出模型并没有学习到猫头鹰和海鸥的自身特征信息,而只是简单地将背景色作为分类依据。</p>
                </div>
                <div class="p1">
                    <p id="100">使用上文基于背景色的数据增强算法,对训练集数据进行增强,增强后的训练集部分样本如图19所示。用数据增强后的训练集进行训练,并在测试集<i>T</i><sub>1</sub>、<i>T</i><sub>2</sub>上进行测试,准确率如图20所示。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图19 数据增强后的训练集样本" src="Detail/GetImg?filename=images/JSJY201911012_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图19 数据增强后的训练集样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 19 Data enhanced training set samples</p>

                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图20 数据增强后测试集 T1、T2准确率" src="Detail/GetImg?filename=images/JSJY201911012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图20 数据增强后测试集 <i>T</i><sub>1</sub>、<i>T</i><sub>2</sub>准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 20 Accuracy of test set <i>T</i><sub>1</sub>、<i>T</i><sub>2</sub> after data enhancement</p>

                </div>
                <div class="p1">
                    <p id="103">从图20中可以看出,经过对训练集使用基于背景色的数据增强,可在测试集<i>T</i><sub>1</sub>和<i>T</i><sub>2</sub>上得到较好的效果,模型不再受背景色信息的干扰,从而学习更高层次的特征信息,最终能对物体进行较好识别。通过该实验,可以发现通过基于背景色的数据增强,可以有效避免数据集中因某一背景色大量出现从而导致模型只学习背景颜色特征的“惰性”现象。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="105">本文通过将Mnist灰度图数据集转换成具有背景色的数据集,发现了深度学习模型特征提取偏好与背景色之间的关系,并在此基础上提出了基于背景色的数据增强算法,在猫头鹰与海鸥这类背景色有特点的数据集上进行实验,表明了本文的方法在一些目标本体与背景色具有强相关性的图像处理任务上具有一定的适用性。在未来工作中,将继续研究深度学习模型特征提取机制的内在规律,例如轮廓、纹理之间的关系,以及数据集大小对模型性能的影响,进一步分析其特征提取机制对分类性能的影响。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="121">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MTYzMjFDTGZZYkc0SDluTXFvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVnIzS0s=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报,2018,44(5):855-864.(TANG X L,DU Y M,LIU Y W,et al.Recognition with conditional deep convolutional generative adversarial networks[J].Acta Automatica Sinica,2018,44(5):855-864.)
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to segment object candidates">

                                <b>[2]</b> PINHEIRO P O,COLLOBERT R,DOLLAR P.Learning to segment object candidates[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.New York:ACM,2015:1990-1998.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">

                                <b>[3]</b> REN S,HE K,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep convolutional ranking for multilabel image annotation">

                                <b>[4]</b> GONG Y,JIA Y,LEUNG T,et al.Deep convolutional ranking for multilabel image annotation[EB/OL].[2018- 04- 14].https://pdfs.semanticscholar.org/3b04/9d8cfea6c3bed377090e0e7fa677d2 82a361.pdf.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Explaining the predictions of any classifier">

                                <b>[5]</b> RIBEIRO M T,SINGH S,GUESTRIN C.“Why should I trust you?”:Explaining the predictions of any classifier[C]// Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,2016:1135-1144.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[6]</b> SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018- 04- 10].http://www.cs.virginia.edu/～vicente/recognition/slides/lecture07/iclr2015.pdf.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1-9.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[8]</b> HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely Connected Convolutional Networks">

                                <b>[9]</b> HUANG G,LIU Z,LAURENS V D M,et al.Densely connected convolutional networks[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:2261-2269.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing higher-layer features of a deep network">

                                <b>[10]</b> ERHAN D,BENGIO Y,COURVILLE A,et al.Visualizing higher-layer features of a deep network[C]// Proceedings of the 26th Annual International Conference on Machine Learning.New York:ACM,2009:1341-1349.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tiled convolutional neural networks">

                                <b>[11]</b> NGIAM J,CHEN Z,CHIA D,et al.Tiled convolutional neural networks[C]// Proceedings of the 2010 Conference on Natural Information Processing System.Columbia:MIT Press,2010:1279-1287.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012549&amp;v=MDc4MjJmSlpiSzlIdGpNcW85RlpPb05DWGd3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSVZ3UmFCQT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> BERKES P,WISKOTT L.On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields[J].Neural Computation,2006,18(8):1868-1895.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=De CAF:A Deep Convolutional Activation Feature for Generic Visual Recognition">

                                <b>[13]</b> DONAHUE J,JIA Y,VINYALS O,et al.DeCAF:a deep convolutional activation feature for generic visual recognition[C]// Proceedings of the 31st International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2014:I-647-I-655.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S1034&amp;v=MTQzMjl1WnNGeW5uVnIzS0x6N0JiN0c0SDlhdnJvOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 俞海宝,沈琦,冯国灿.在反卷积网络中引入数值解可视化卷积神经网络[J].计算机科学,2017,44(S1):146-150.(YU H B,SHEN Q,FENG G C.Introduce numerical solution to visualize convolutional neuron networks based on numerical solution[J].Computer Science,2017,44(S1):146-150.)
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DBRS:A Density-Based Spatial Clustering Method with Random Sampling">

                                <b>[15]</b> WANG X,HAMILTON H J.DBRS:a density-based spatial clustering method with random sampling[C]// Proceedings of the 7th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.Berlin:Springer-Verlag,2003:563-575.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911012&amp;v=MDIzNjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WcjNLTHo3QmQ3RzRIOWpOcm85RVpvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
