<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637138991511353750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201903005%26RESULT%3d1%26SIGN%3dkBsjbJyihB0sf%252b4Vx5zBWytTig0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903005&amp;v=MTUyMTMzenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3L0lMejdCZDdHNEg5ak1ySTlGWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="1.1 &lt;b&gt;建议挖掘&lt;/b&gt;">1.1 <b>建议挖掘</b></a></li>
                                                <li><a href="#57" data-title="1.2 PU&lt;b&gt;学习方法&lt;/b&gt;">1.2 PU<b>学习方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="2 建议语句分类模型 ">2 建议语句分类模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="2.1 &lt;b&gt;使用规则选取正例&lt;/b&gt;">2.1 <b>使用规则选取正例</b></a></li>
                                                <li><a href="#63" data-title="2.2 &lt;b&gt;构建&lt;/b&gt;Autoencoder&lt;b&gt;特征向量&lt;/b&gt;">2.2 <b>构建</b>Autoencoder<b>特征向量</b></a></li>
                                                <li><a href="#73" data-title="2.3 Spy&lt;b&gt;划分可信反例&lt;/b&gt;">2.3 Spy<b>划分可信反例</b></a></li>
                                                <li><a href="#106" data-title="2.4 &lt;b&gt;多层感知机分类&lt;/b&gt;">2.4 <b>多层感知机分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="3 实验与结果 ">3 实验与结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#128" data-title="3.1 &lt;b&gt;数据集&lt;/b&gt;">3.1 <b>数据集</b></a></li>
                                                <li><a href="#130" data-title="3.2 &lt;b&gt;实验设置&lt;/b&gt;">3.2 <b>实验设置</b></a></li>
                                                <li><a href="#132" data-title="3.3 &lt;b&gt;实验结果和分析&lt;/b&gt;">3.3 <b>实验结果和分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#156" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="图1 建议语句分类模型">图1 建议语句分类模型</a></li>
                                                <li><a href="#72" data-title="图2 Autoencoder网络结构">图2 Autoencoder网络结构</a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;表&lt;/b&gt;1 PU-MLP&lt;b&gt;与基线方法的实验结果&lt;/b&gt;"><b>表</b>1 PU-MLP<b>与基线方法的实验结果</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;本文方法使用不同特征的实验结果&lt;/b&gt;"><b>表</b>2 <b>本文方法使用不同特征的实验结果</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;本文方法使用&lt;/b&gt;MLP&lt;b&gt;与不同迭代方法的实验结果&lt;/b&gt;"><b>表</b>3 <b>本文方法使用</b>MLP<b>与不同迭代方法的实验结果</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同正例集合&lt;/b&gt;&lt;i&gt;P&lt;/i&gt;&lt;b&gt;的实验结果对比表&lt;/b&gt;"><b>表</b>4 <b>不同正例集合</b><i>P</i><b>的实验结果对比表</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="267">


                                    <a id="bibliography_1" title="NEGI S.Suggestion mining from opinionated text[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop.Stroudsburg, PA:Association for Computational Linguistics, 2016:7-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Suggestion mining from opinionated text">
                                        <b>[1]</b>
                                        NEGI S.Suggestion mining from opinionated text[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop.Stroudsburg, PA:Association for Computational Linguistics, 2016:7-12.
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_2" title="BRUN C, HAGEGE C.Suggestion mining:detecting suggestions for improvement in users&#39;comments[J].Research in Computing Science, 2013, 70:171-181." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Suggestion Mining:Detecting Suggestions for Improvement in Users&amp;#39;&amp;#39;Comments">
                                        <b>[2]</b>
                                        BRUN C, HAGEGE C.Suggestion mining:detecting suggestions for improvement in users&#39;comments[J].Research in Computing Science, 2013, 70:171-181.
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_3" title="RAMANAND J, BHAVSAR K, PEDANEKAR N.Wishful thinking:finding suggestions and &#39;buy&#39; wishes from product reviews[C]//Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text.Stroudsburg, PA:Association for Computational Linguistics, 2010:54-61." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wishful thinking: findingsuggestions and&amp;#39;&amp;#39;buy&amp;#39;&amp;#39;wishes from product reviews">
                                        <b>[3]</b>
                                        RAMANAND J, BHAVSAR K, PEDANEKAR N.Wishful thinking:finding suggestions and &#39;buy&#39; wishes from product reviews[C]//Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text.Stroudsburg, PA:Association for Computational Linguistics, 2010:54-61.
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_4" title="WICAKSONO A F, MYAENG S-H.Automatic extraction of advicerevealing sentences for advice mining from online forums[C]//Proceedings of the 7th International Conference on Knowledge Capture.New York:ACM, 2013:97-104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic extraction of advice-revealing sentences for advice mining from online forums">
                                        <b>[4]</b>
                                        WICAKSONO A F, MYAENG S-H.Automatic extraction of advicerevealing sentences for advice mining from online forums[C]//Proceedings of the 7th International Conference on Knowledge Capture.New York:ACM, 2013:97-104.
                                    </a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_5" title="DONG L, WEI F, DUAN Y, et al.The automated acquisition of suggestions from tweets[C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2013:239-245." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The automated acquisition of suggestions from tweets">
                                        <b>[5]</b>
                                        DONG L, WEI F, DUAN Y, et al.The automated acquisition of suggestions from tweets[C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2013:239-245.
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_6" title="NEGI S, ASOOJA K, MEHROTRA S, et al.A study of suggestions in opinionated texts and their automatic detection[C]//Proceedings of the 5th Joint Conference on Lexical and Computational Semantics.Stroudsburg, PA:Association for Computational Linguistics, 2016:170-178." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A study of suggestions in opinionated texts and their automatic detection">
                                        <b>[6]</b>
                                        NEGI S, ASOOJA K, MEHROTRA S, et al.A study of suggestions in opinionated texts and their automatic detection[C]//Proceedings of the 5th Joint Conference on Lexical and Computational Semantics.Stroudsburg, PA:Association for Computational Linguistics, 2016:170-178.
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_7" title="张璞, 刘畅, 王永.基于特征融合和集成学习的建议语句分类模型[J].山东大学学报 (工学版) , 2018, 48 (5) :47-54. (ZHANGP, LIU C, WANG Y.Suggestion sentence classification model based on feature fusion and ensemble learning[J].Journal of Shandong University (Engineering Science) , 2018, 48 (5) :47-54.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SDGY201805007&amp;v=MjY5NzMzenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3L0lOaW5NZDdHNEg5bk1xbzlGWTRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        张璞, 刘畅, 王永.基于特征融合和集成学习的建议语句分类模型[J].山东大学学报 (工学版) , 2018, 48 (5) :47-54. (ZHANGP, LIU C, WANG Y.Suggestion sentence classification model based on feature fusion and ensemble learning[J].Journal of Shandong University (Engineering Science) , 2018, 48 (5) :47-54.) 
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_8" title="ZHANG Q C, YANG L T, CHEN Z K.Deep computation model for unsupervised feature learning on big data[J].IEEE Transactions on Services Computing, 2016, 9 (1) :161-171." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Computation Model for Unsupervised Feature Learningon Big Data">
                                        <b>[8]</b>
                                        ZHANG Q C, YANG L T, CHEN Z K.Deep computation model for unsupervised feature learning on big data[J].IEEE Transactions on Services Computing, 2016, 9 (1) :161-171.
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_9" title="LIU B, LEE W S, YU P S, et al.Partially supervised classification of text documents[C]//Proceedings of the 19th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann Publishers, 2002:387-394." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Partially supervised classification of text documents">
                                        <b>[9]</b>
                                        LIU B, LEE W S, YU P S, et al.Partially supervised classification of text documents[C]//Proceedings of the 19th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann Publishers, 2002:387-394.
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_10" title="任亚峰, 姬东鸿, 张红斌, 等.基于PU学习算法的虚假评论识别研究[J].计算机研究与发展, 2015, 52 (3) :639-648. (RENY F, JI D H, ZHANG H B, et al.Deceptive reviews detection based on positive and unlabeled learning[J].Journal of Computer Research and Development, 2015, 52 (3) :639-648.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201503012&amp;v=MTY3NzR6cXFCdEdGckNVUjdxZlp1WnBGaURsVzcvSUx5dlNkTEc0SDlUTXJJOUVab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        任亚峰, 姬东鸿, 张红斌, 等.基于PU学习算法的虚假评论识别研究[J].计算机研究与发展, 2015, 52 (3) :639-648. (RENY F, JI D H, ZHANG H B, et al.Deceptive reviews detection based on positive and unlabeled learning[J].Journal of Computer Research and Development, 2015, 52 (3) :639-648.) 
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_11" title="刘露, 彭涛, 左万利, 等.一种基于聚类的PU主动文本分类方法[J].软件学报, 2013, 24 (11) :2571-2583. (LIU L, PENG T, ZUO W L, et al.Clustering-based PU active text classification method[J].Journal of Software, 2013, 24 (11) :2571-2583.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201311010&amp;v=MTQxNTREbFc3L0lOeWZUYkxHNEg5TE5ybzlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        刘露, 彭涛, 左万利, 等.一种基于聚类的PU主动文本分类方法[J].软件学报, 2013, 24 (11) :2571-2583. (LIU L, PENG T, ZUO W L, et al.Clustering-based PU active text classification method[J].Journal of Software, 2013, 24 (11) :2571-2583.) 
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_12" title="王宗尧, 刘金岭.基于支持向量机的PU中文文本分类器构建[J].南京邮电大学学报 (自然科学版) , 2015, 35 (6) :100-105. (WANG Z Y, LIU J L.PU Chinese text classifier based on support vector machine construction[J].Journal of Nanjing University of Posts and Telecommunications (Natural Science Edition) , 2015, 35 (6) :100-105.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJYD201506017&amp;v=MTE5NTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXNy9JS3lmU2FyRzRIOVRNcVk5RVk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        王宗尧, 刘金岭.基于支持向量机的PU中文文本分类器构建[J].南京邮电大学学报 (自然科学版) , 2015, 35 (6) :100-105. (WANG Z Y, LIU J L.PU Chinese text classifier based on support vector machine construction[J].Journal of Nanjing University of Posts and Telecommunications (Natural Science Edition) , 2015, 35 (6) :100-105.) 
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_13" title="LIU B, DAI Y, LI X, et al.Building text classifiers using positive and unlabeled examples[C]//Proceedings of the 3rd IEEE International Conference on Data Mining.Washington, DC:IEEEComputer Society, 2003:179-188." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Building Text Classifiers Using Positive and Unlabeled Examples">
                                        <b>[13]</b>
                                        LIU B, DAI Y, LI X, et al.Building text classifiers using positive and unlabeled examples[C]//Proceedings of the 3rd IEEE International Conference on Data Mining.Washington, DC:IEEEComputer Society, 2003:179-188.
                                    </a>
                                </li>
                                <li id="293">


                                    <a id="bibliography_14" title="WLODARCZAK P, SOAR J, ALLY M.Multimedia data mining using deep learning[C]//Proceedings of the 2015 5th International Conference on Digital Information Processing and Communications.Piscataway, NJ:IEEE, 2015:190-196." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimedia data mining using deep learning">
                                        <b>[14]</b>
                                        WLODARCZAK P, SOAR J, ALLY M.Multimedia data mining using deep learning[C]//Proceedings of the 2015 5th International Conference on Digital Information Processing and Communications.Piscataway, NJ:IEEE, 2015:190-196.
                                    </a>
                                </li>
                                <li id="295">


                                    <a id="bibliography_15" title="ZHOU H, CHEN L, HUANG D.Cross-lingual sentiment classification based on denoising autoencoder[M]//ZONG C, NIE J Y, ZHAO D, et al.Natural Language Processing and Chinese Computing, CCIS 496.Berlin:Springer, 2014:181-192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-Lingual Sentiment Classification Based on Denoising Autoencoder">
                                        <b>[15]</b>
                                        ZHOU H, CHEN L, HUANG D.Cross-lingual sentiment classification based on denoising autoencoder[M]//ZONG C, NIE J Y, ZHAO D, et al.Natural Language Processing and Chinese Computing, CCIS 496.Berlin:Springer, 2014:181-192.
                                    </a>
                                </li>
                                <li id="297">


                                    <a id="bibliography_16" title="魏超, 罗森林, 张竞, 等.自编码网络短文本流形表示方法[J].浙江大学学报 (工学版) , 2015, 49 (8) :1591-1599. (WEI C, LUO S L, ZHANG J, et al.Short text manifold representation based on AutoEncoder network[J].Journal of Zhejiang University (Engineering Science) , 2015, 49 (8) :1591-1599.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC201508027&amp;v=MDEwMjZHNEg5VE1wNDlIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3L0lQeW5SYmI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        魏超, 罗森林, 张竞, 等.自编码网络短文本流形表示方法[J].浙江大学学报 (工学版) , 2015, 49 (8) :1591-1599. (WEI C, LUO S L, ZHANG J, et al.Short text manifold representation based on AutoEncoder network[J].Journal of Zhejiang University (Engineering Science) , 2015, 49 (8) :1591-1599.) 
                                    </a>
                                </li>
                                <li id="299">


                                    <a id="bibliography_17" title="高妮, 高岭, 贺毅岳, 等.基于自编码网络特征降维的轻量级入侵检测模型[J].电子学报, 2017, 45 (3) :730-739. (GAO N, GAO L, HE Y Y, et al.A lightweight intrusion detection model based on autoencoder network with feature reduction[J].Acta Electronica Sinica, 2017, 45 (3) :730-739.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201703033&amp;v=MTY0Nzh0R0ZyQ1VSN3FmWnVacEZpRGxXNy9JSVRmVGU3RzRIOWJNckk5R1o0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        高妮, 高岭, 贺毅岳, 等.基于自编码网络特征降维的轻量级入侵检测模型[J].电子学报, 2017, 45 (3) :730-739. (GAO N, GAO L, HE Y Y, et al.A lightweight intrusion detection model based on autoencoder network with feature reduction[J].Acta Electronica Sinica, 2017, 45 (3) :730-739.) 
                                    </a>
                                </li>
                                <li id="301">


                                    <a id="bibliography_18" title="ZHU Z, WANG X, BAI S, et al.Deep learning representation using autoencoder for 3D shape retrieval[J].Neurocomputing, 2016, 204 (C) :41-50." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning representation using autoencoder for 3D shape retrieval">
                                        <b>[18]</b>
                                        ZHU Z, WANG X, BAI S, et al.Deep learning representation using autoencoder for 3D shape retrieval[J].Neurocomputing, 2016, 204 (C) :41-50.
                                    </a>
                                </li>
                                <li id="303">


                                    <a id="bibliography_19" title="LE Q, MKOLOV T.Distributed representations of sentences and documents[EB/OL].[2018-06-20].https://arxiv.org/pdf/1405.4053v2.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed representations of sentences and documents">
                                        <b>[19]</b>
                                        LE Q, MKOLOV T.Distributed representations of sentences and documents[EB/OL].[2018-06-20].https://arxiv.org/pdf/1405.4053v2.pdf.
                                    </a>
                                </li>
                                <li id="305">


                                    <a id="bibliography_20" title="LI X, LIU B.Learning to classify texts using positive and unlabeled data[C]//Proceedings of the 18th International Joint Conference on Artificial Intelligence.San Francisco, CA:Morgan Kaufmann Publishers, 2003:587-592." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to Classify Texts Using Positive and Unlabeled Data">
                                        <b>[20]</b>
                                        LI X, LIU B.Learning to classify texts using positive and unlabeled data[C]//Proceedings of the 18th International Joint Conference on Artificial Intelligence.San Francisco, CA:Morgan Kaufmann Publishers, 2003:587-592.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-31 16:01</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(03),639-643 DOI:10.11772/j.issn.1001-9081.2018081759            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于PU学习的建议语句分类方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%92%9E&amp;code=11611162&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张璞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E7%95%85&amp;code=30226437&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘畅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%80%8D&amp;code=40287370&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李逍</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>建议挖掘作为一项新兴研究任务, 具有重要的应用价值。针对传统建议语句分类方法所存在的规则复杂、标注工作量大、特征维度高、数据稀疏等问题, 提出一种基于PU学习的建议语句分类方法。首先, 使用简单规则从无标注评论集合中选择建议语句的正例集合;然后, 为了降低特征维度, 缓解数据稀疏性, 在自编码神经网络 (Autoencoder) 特征空间中使用Spy技术划分可靠反例集合;最后, 利用正例集合和可靠反例集合来训练多层感知机 (MLP) 对剩余的无标注样例进行分类。该方法在中文数据集上的F1值和准确率值分别达到81.98%和82.67%, 实验结果表明, 该方法能够有效地对建议语句进行分类, 且不需要对数据进行人工标注。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BB%BA%E8%AE%AE%E6%8C%96%E6%8E%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">建议挖掘;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BB%BA%E8%AE%AE%E8%AF%AD%E5%8F%A5%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">建议语句分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=PU%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">PU学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自编码器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多层感知机;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *张璞 (1977—) , 男, 云南昭通人, 副教授, 博士, CCF会员, 主要研究方向:文本挖掘、情感分析;电子邮箱zhangpu@cqupt.edu.cn;
                                </span>
                                <span>
                                    刘畅 (1993—) , 男, 湖北孝感人, 硕士研究生, 主要研究方向:文本挖掘、情感分析;;
                                </span>
                                <span>
                                    李逍 (1994—) , 男, 湖北孝感人, 硕士研究生, 主要研究方向:文本挖掘、情感分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>教育部人文社会科学研究青年基金资助项目 (17YJCZH247);</span>
                                <span>重庆市教委人文社会科学研究项目 (17SKG055);</span>
                    </p>
            </div>
                    <h1><b>Suggestion sentence classification method based on PU learning</b></h1>
                    <h2>
                    <span>ZHANG Pu</span>
                    <span>LIU Chang</span>
                    <span>LI Xiao</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Technology, Chongqing University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>As a new research task, suggestion mining has important application value. Since traditional suggestion sentence classification methods have problems like complex rules, large labeling workload, high feature dimension and data sparsity, a PU (Positive and Unlabeled) -based suggestion sentence classification method was proposed. Firstly, some suggestion sentences were selected from an unlabeled review set by using a simple rule to form a positive example set; then a reliable negative example set was constructed by Spy technique in the feature space of autoencoder neural network to reduce the feature dimension and alleviate data sparsity; finally, Multi-Layer Perceptron (MLP) was trained by the positive example set and the reliable negative example set to classify the remaining unlabeled samples. On a Chinese dataset, the F1 value and the accuracy of the proposed method, reached 81.98% and 82.67% respectively. The experimental results show that the proposed method can classify suggestion sentences effectively without manually labelling the data.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=suggestion%20mining&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">suggestion mining;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=suggestion%20sentence%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">suggestion sentence classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=PU%20(Positive%20and%20Unlabeled)%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">PU (Positive and Unlabeled) learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=autoencoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">autoencoder;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-Layer%20Perceptron%20(MLP)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-Layer Perceptron (MLP) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Pu, born in 1977, Ph. D. , associate professor. His research interests include text mining, sentiment analysis.;
                                </span>
                                <span>
                                    LIU Chang, born in 1993, M. S. candidate. His research interests include text mining, sentiment analysis.;
                                </span>
                                <span>
                                    LI Xiao, born in 1994, M. S. candidate. His research interests include text mining, sentiment analysis.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-23</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Youth Program of Humanities and Social Science Foundation of the Ministry of Education in China (17YJCZH247);</span>
                                <span>the Humanities and Social Science Foundation of the Chongqing Municipal Education Commission (17SKG055);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="44">随着互联网的快速发展, 网络上出现了海量的用户评论, 人们不仅会在评论中传达积极或消极的情绪, 有时也会对产品、服务等提出相应的建议。例如, 在“希望三星手机能支持谷歌应用商店”这条评论中虽然并未包含情感极性, 但明确提出了对产品功能的改进建议。这类建议信息可以帮助厂家有效地提升产品质量, 也有助于商家有针对性地制定销售策略, 具有重要的应用价值, 建议挖掘<citation id="307" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>研究因此应运而生。</p>
                </div>
                <div class="p1">
                    <p id="45">进行建议挖掘, 首先需要对建议语句进行分类, 即将评论语句分为建议语句或非建议语句。但由于人们对建议的判定存在比较大的主观性, 导致建议语句的定义难以取得一致, 这给语料标注和问题定义带来了很多困难<citation id="308" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。本文采用和文献<citation id="309" type="reference">[<a class="sup">1</a>]</citation>类似的方案, 将明确表达了期望或提出改进意见的语句定义为建议语句。目前, 建议挖掘研究<citation id="310" type="reference"><link href="269" rel="bibliography" /><link href="271" rel="bibliography" /><link href="273" rel="bibliography" /><link href="275" rel="bibliography" /><link href="277" rel="bibliography" /><link href="279" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>虽然已经取得了一定的进展, 但还存在以下问题:一方面, 已有研究大多是在英文语料上开展的, 在中文语料上的相关研究很少, 而中文环境和英语环境中的网络文化和建议语句的表达方式存在较大差异, 因此需要深入研究中文环境下的建议语句分类方法。另一方面, 在已有研究中, 用于建议语句分类的方法主要有规则方法<citation id="311" type="reference"><link href="269" rel="bibliography" /><link href="271" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>和有监督机器学习方法<citation id="312" type="reference"><link href="273" rel="bibliography" /><link href="275" rel="bibliography" /><link href="277" rel="bibliography" /><link href="279" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。规则方法通过手工制定的规则来进行建议挖掘, 需要提前建立复杂的规则模板, 人工干预较多。而有监督机器学习方法虽然模型的精度较高, 但模型训练过程中需要大量人工标注语料, 标注工作量大, 代价昂贵, 并面临特征维度高、数据稀疏等问题。</p>
                </div>
                <div class="p1">
                    <p id="46">针对以上问题, 本文提出了一种综合利用简单语言规则和自编码器 (Autoencoder) 特征提取<citation id="313" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>能力的PU (Positive and Unlabeled) 学习<citation id="314" type="reference"><link href="283" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>方法来对中文建议语句进行分类。PU学习方法是一类利用少量正例和无标注样本进行学习的方法。该方法首先从无标注样本中获得部分可信反例, 然后通过正例和可信反例训练分类器, 并对剩余无标注样本进行分类, 整个过程只需要少量标注的正例样本, 节省了收集和标注反例的工作, 适用于缺乏标注数据集的建议挖掘任务。在PU学习框架下, 本文结合中文建议语句的表达特点, 通过制定简单的语言规则, 将满足规则的语句划分为正例集合, 即建议语句集合, 从而使得整个过程都不需要对数据进行手工标注。进一步地, 本文使用自编码神经网络训练文档的Autoencoder特征向量并用在之后的学习过程中, 与传统PU学习方法使用的词袋特征相比, Autoencoder特征包含了更深层次的语义信息, 同时降低了特征维度, 缓解了数据稀疏问题。在中文数据集上的实验结果表明了本文方法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="47">本文的主要贡献有以下几点:</p>
                </div>
                <div class="p1">
                    <p id="48">1) 在中文语料上开展建议挖掘研究, 并提出一种适用于建议语句分类的无监督机器学习方法。就目前国内外文献查阅情况而言, 已有研究绝大多数为监督学习方法, 且主要在英文语料上开展研究。</p>
                </div>
                <div class="p1">
                    <p id="49">2) 首次将PU学习方法应用于建议语句分类问题中, 实验结果验证了本文方法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="50">3) 在PU学习框架下, 综合利用简单语言规则和自编码神经网络特征提取能力, 使整个过程不需要人工标注, 并降低了特征维度并缓解了特征稀疏问题。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="52" name="52">1.1 <b>建议挖掘</b></h4>
                <div class="p1">
                    <p id="53">建议挖掘近年来刚起步, 相关的文献还较少, 主要方法分为基于规则的方法和有监督机器学习方法。基于规则的方法中, 文献<citation id="315" type="reference">[<a class="sup">2</a>]</citation>利用语法、句法及情感极性词典制定规则, 对产品评论中提出改进建议的语句进行挖掘;文献<citation id="316" type="reference">[<a class="sup">3</a>]</citation>基于“would”“should”等情态动词以及“hope”“wish”“needs to”等特殊搭配制定了一系列规则, 对用户的购买意图和用户提出的改进建议同时进行挖掘, 并将这两类语句统称为用户希望语句。有监督机器学习方法中, 文献<citation id="317" type="reference">[<a class="sup">4</a>]</citation>发现在网络论坛中, 通常由几条连续的语句来共同表达建议, 这意味着连续的句子之间存在很强的依赖关系, 所以将用户建议挖掘视为序列标注问题, 并提出了一种改进的隐马尔可夫模型来进行序列标注。文献<citation id="318" type="reference">[<a class="sup">5</a>]</citation>在Twitter语料上使用词袋特征、建议语句表达模板和因子分解机 (Factorization Machine, FM) 进行建议语句分类, 缓解了Twitter语料中建议语句类别不平衡的问题。近年来, 深度学习方法在自然语言处理领域得到了广泛应用, 文献<citation id="319" type="reference">[<a class="sup">6</a>]</citation>使用预先训练好的COMPOSES词向量和LSTM模型进行建议挖掘, 与支持向量机 (Support Vector Machine, SVM) 和卷积神经网络 (Convolutional Neural Network, CNN) 相比, 分类结果得到了进一步提升。文献<citation id="320" type="reference">[<a class="sup">7</a>]</citation> 标注了用于建议挖掘研究的中文数据集, 并提出一种基于特征融合和集成学习的建议语句分类模型。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903005_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 建议语句分类模型" src="Detail/GetImg?filename=images/JSJY201903005_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 建议语句分类模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903005_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Suggestion sentence classification model</p>

                </div>
                <div class="p1">
                    <p id="56">总体而言, 基于规则的方法需要的人工干预较多, 而有监督机器学习方法则面临标注工作量大、特征维度高、数据稀疏等问题。与已有研究不同, 本文方法通过利用简单语言规则, 无需进行语料的手工标注, 并在低维特征空间上进行建议语句分类, 可看作一种无监督机器学习方法。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57">1.2 PU<b>学习方法</b></h4>
                <div class="p1">
                    <p id="58">在使用有监督学习算法时, 每个类别都需要足量的标注数据, 标注通常需要手工完成, 会耗费大量的人力和时间。因此, 研究者们开始重视未标注数据来进行学习, 而PU学习就是一种只使用少量正例样本和大量未标注样本进行学习的方法<citation id="321" type="reference"><link href="283" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。如今, PU学习方法已得到广泛应用。文献<citation id="322" type="reference">[<a class="sup">10</a>]</citation>中提出了一种整合LDA (Latent Dirichlet Allocation) 、<i>K</i>-means和隐狄利克雷过程的PU学习框架来识别虚假评论, 减小了错误标注造成的影响。文献<citation id="323" type="reference">[<a class="sup">11</a>]</citation>着重研究了PU学习中划分可信反例的方法, 提出了一种基于聚类的半监督主动分类方法, 通过使正反例的共享特征尽可能少, 从未标注数据集中尽可能多地移除正例, 从而获得更多的可信反例。文献<citation id="324" type="reference">[<a class="sup">12</a>]</citation>中使用改进的特征词权值提取方法和OB_PCZ算法划分可信反例, 再结合支持向量机和改进的Rocchio算法来构建文本分类器。尽管PU学习方法已被用于上述的诸多任务中, 但是目前尚未见到有将其用于建议挖掘任务的相关研究。</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag">2 建议语句分类模型</h3>
                <div class="p1">
                    <p id="60">考虑到PU学习方法需要的标注数据少, 在只给定标注正例的情况下仍然能有效分类等特点<citation id="325" type="reference"><link href="283" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 本文将其引入建议语句分类任务。在传统的PU学习方法中, 一般是使用人工标注方式来标注正例集合<i>P</i>。不同于传统方法, 本文首先结合网络上建议语句的表达特点, 通过应用简单的语言规则来确定正例集合<i>P</i>, 降低了手工标注正例的成本, 再在使用自编码神经网络训练得到文档的特征空间上, 使用Spy技术<citation id="326" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>划分出可信的反例集合<i>RN</i>, 剩余的无标注集合记为<i>Q</i>。最后通过正例集合<i>P</i>、反例集合<i>RN</i>训练多层感知机MLP并对<i>Q</i>进行分类, 从而得到<i>Q</i>中的正例集合<i>PQ</i>、反例集合<i>NQ</i>, 最终得到建议语句集合<i>P</i>+<i>PQ</i>, 模型的整体框架见图1。</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61">2.1 <b>使用规则选取正例</b></h4>
                <div class="p1">
                    <p id="62">在传统的PU学习方法中, 使用手工标注的样本作为正例集合, 必然会耗费一定的人力和时间。由于目前缺乏可以用于建议挖掘的中文语料, 本文通过编写网络爬虫来采集网络论坛中的评论数据, 构建数据集。通过对数据的观察, 发现建议语句通常会使用到“建议”“希望”等词语。因此, 本文将评论中包含“建议”或者“希望”的评论选择出来, 作为正例集合<i>P</i>, 剩下的文档作为无标注集合<i>U</i>, <i>U</i>中既包含建议语句也包含非建议语句。</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63">2.2 <b>构建</b>Autoencoder<b>特征向量</b></h4>
                <div class="p1">
                    <p id="64">传统PU方法使用词袋模型来构建文档的特征向量, 而词袋模型采用的文本特征大多以词语特征为主, 只是文本语义的表层形式, 缺乏文本的深层语义信息, 会面临特征稀疏、维度灾难等一系列问题, 导致分类性能降低。</p>
                </div>
                <div class="p1">
                    <p id="65">自编码网络作为一种非监督学习方法<citation id="330" type="reference"><link href="281" rel="bibliography" /><link href="293" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">14</a>]</sup></citation>, 通过设计编码和解码过程使输入和输出越来越接近, 能通过具有隐藏层的神经网络的逐层特征变换获得原始数据的低维表示, 从而显著降低数据的维数, 并被广泛应用于跨语言情感分类<citation id="327" type="reference"><link href="295" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、短文本分类<citation id="328" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、入侵检测<citation id="329" type="reference"><link href="299" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等任务中, 因此, 本文引入自编码神经网络来进行数据表示。</p>
                </div>
                <div class="p1">
                    <p id="66">自编码神经网络的网络结构如图2所示, 原始数据<b><i>x</i></b>经过加权映射之后得到低维隐藏层<b><i>h</i></b>=<i>f</i> (<b><i>x</i></b>) =<i>s</i> (<b><i>w</i></b><sub>1</sub><b><i>x</i></b>+<b><i>b</i></b><sub>1</sub>) , 再经反向加权映射到<b><i>y</i></b>=<i>g</i> (<b><i>h</i></b>) =<i>s</i> (<b><i>w</i></b><sub>2</sub><b><i>h</i></b>+<b><i>b</i></b><sub>2</sub>) , 其中<b><i>w</i></b><sub>2</sub>=<b><i>w</i></b><sub>1</sub>, 经过反复训练使得误差函数最小, 即尽可能保证<b><i>y</i></b>近似于<b><i>x</i></b>, 实现对<b><i>x</i></b>的重构, 当误差在限定范围内时, 可认为编码过程是对原始数据的有效降维表达。图2中的“+1”表示加入偏置。</p>
                </div>
                <div class="p1">
                    <p id="67">Autoencoder的目标函数如式 (1) 所示, <i>M</i>为样本数, <i>N</i>为输入维度, <i>x</i><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup></mrow></math></mathml>表示输入样本<i>i</i>的第<i>j</i>个分量, <i>y</i><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup></mrow></math></mathml>表示样本<i>i</i>对应的输出向量的第<i>j</i>个分量。</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>、</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>、</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>、</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>、</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>、</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>、</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munder><mfrac><mn>1</mn><mi>Μ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>L</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>g</mi><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mspace width="0.25em" /><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo>=</mo><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup><mo>, </mo><mi>g</mi><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup><mo>-</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">训练过程中通过反向传播梯度下降的方法更新参数<b><i>w</i></b><sub>1</sub>、<b><i>w</i></b><sub>2</sub>、<b><i>b</i></b><sub>1</sub>、<b><i>b</i></b><sub>2</sub>, 使得目标函数减小。当输出误差<i>L</i>足够小时, 表明输入样本数据可以通过隐藏层重构表达, 此时, 隐藏层输出即为提取的样本特征<citation id="331" type="reference"><link href="301" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。利用全部文档集合<i>D</i>训练自编码神经网络, 得到相应的正例集合<i>P</i>的Autoencoder特征向量集合<i>PA</i>, 无标注集合<i>U</i>的Autoencoder特征向量集合<i>UA</i>。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903005_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Autoencoder网络结构" src="Detail/GetImg?filename=images/JSJY201903005_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Autoencoder网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903005_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Autoencoder network structure</p>

                </div>
                <h4 class="anchor-tag" id="73" name="73">2.3 Spy<b>划分可信反例</b></h4>
                <div class="p1">
                    <p id="74">由于样本空间中只包含正例样本和未标注的样本, 模型需要从中抽取一些可信的反例。本文利用Spy技术从<i>UA</i>中划分出可信反例<i>RN</i>, 具体过程如下:首先从<i>PA</i>中随机选出一定比例的样本集合<i>SP</i>, 这些选出来的样本被称为间谍样本“spies”。然后将<i>PA</i>-<i>SP</i>作为正例, <i>UA</i>+<i>SP</i>作为反例, 利用Autoencoder特征向量训练逻辑回归 (Logistic Regression, LR) 分类器LR, 并使用LR计算出<i>UA</i>+<i>SP</i>中每个样本属于正类的概率<i>p</i> (1|<i>x</i>) 。然后根据<i>SP</i>中样本的<i>p</i> (1|<i>x</i>) 确定一个阈值<i>t</i>, 将<i>UA</i>中<i>p</i> (1|<i>x</i>) 值小于<i>t</i>的样本作为可信的反例划入<i>RN</i>中, 最后将<i>UA</i>中剩余的无标注样本集合记为<i>Q</i>。</p>
                </div>
                <div class="p1">
                    <p id="75">算法1 Spy选取可信反例。</p>
                </div>
                <div class="p1">
                    <p id="76">输入:正例集合<i>P</i>的Autoencoder特征向量集合<i>PA</i>, 无标注集合<i>U</i>的Autoencoder特征向量集合<i>UA</i>;</p>
                </div>
                <div class="p1">
                    <p id="77">输出:可信反例集合<i>RN</i>和剩余的无标注样本集合<i>Q</i>。</p>
                </div>
                <div class="area_img" id="265">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201903005_26500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="106" name="106">2.4 <b>多层感知机分类</b></h4>
                <div class="p1">
                    <p id="107">多层感知机 (Multi-Layer Perceptron, MLP) 是一种常见的神经网络模型, 其前馈网络结构能映射一组输入向量到一组输出向量, 其中每一个神经元都带有一个非线性激活函数, 并使用反向传播算法来训练MLP。将<i>P</i>作为正例集合, <i>RN</i>作为反例集合训练MLP, 然后使用训练好的MLP对<i>Q</i>中的每个样本分类, 得到<i>Q</i>中的正例集合<i>PQ</i>、反例集合<i>NQ</i>, 最终得到整个文档集合<i>D</i>中的建议语句集合<i>P</i>+<i>PQ</i>。</p>
                </div>
                <div class="p1">
                    <p id="108">算法2 MLP分类。</p>
                </div>
                <div class="p1">
                    <p id="109">输入:可信反例集合<i>RN</i>, 正例集合<i>P</i>, 剩余的无标注样本集合<i>Q</i>;</p>
                </div>
                <div class="p1">
                    <p id="110">输出:<i>PQ</i>, <i>NQ</i>。</p>
                </div>
                <div class="area_img" id="266">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201903005_26600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="127" name="127" class="anchor-tag">3 实验与结果</h3>
                <h4 class="anchor-tag" id="128" name="128">3.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="129">已有研究中, 只有文献<citation id="332" type="reference">[<a class="sup">5</a>]</citation>公开了数据集, 该数据集由3 000条英文Tweet构成, 其中属于建议语句的只有238条, 不足8%。文献<citation id="333" type="reference">[<a class="sup">7</a>]</citation>中, 本文作者发现在一般网络平台上的用户评论中, 建议语句的比例是比较低的, 想要获取一定量的建议语句所需要的数据量及标注工作量均比较大, 而在部分网站专门设立的产品建议模块中的建议语句比例会比较高, 因而从三星盖乐世社区 (http://www.galaxyclub.cn) 的产品建议模块中爬取数据并标注了中文数据集。本文采用文献<citation id="334" type="reference">[<a class="sup">7</a>]</citation>中的数据集, 该数据集由15 695条手机评论帖子构成, 人工标注数据包括9 000条评论, 其中建议语句4 513条, 非建议语句4 487条。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130">3.2 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="131">使用scikit-learn (http://scikit-learn.org/stable) 工具包调用朴素贝叶斯 (Naïve Bayes, NB) 、LR、SVM等传统机器学习模型, 使用gensim工具包 (https://radimrehurek.com/gensim) 调用Word2Vec和段落向量 (Paragraph Vector, PV) <citation id="335" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>模型来训练对比实验的段落向量及词向量, 训练过程中, PV模型及Word2Vec模型均使用默认参数, 在15 695条评论数据上使用Word2Vec训练得到128维词向量, 并使用段落向量的分布记忆模型 (Distributed Memory model of Paragraph Vector, PV-DM) 方式训练得到128维的段落向量。自编码神经网络特征向量的维度为128维。实验结果的评价指标采用文本分类中常用的精确率 (Precision) 、召回率 (Recall) 、F1值和准确率 (Accuracy, Acc) 。</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132">3.3 <b>实验结果和分析</b></h4>
                <div class="p1">
                    <p id="133">本节进行一系列实验来验证本文提出的建议语句分类方法的可行性和有效性。第1个实验将本文提出的方法 (表1中的PU-MLP) 和以下四个基线方法进行比较。本文方法不需要标注数据, 可看成一种无监督学习方法, 下述实验结果均指在整个数据集上进行一次实验的结果。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">1) 规则:</h4>
                <div class="p1">
                    <p id="135">使用2.1节中的简单规则来进行分类, 即将含有“希望”或“建议”的评论作为建议语句, 其余语句作为非建议语句。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136">2) S-EM:</h4>
                <div class="p1">
                    <p id="137">文献<citation id="336" type="reference">[<a class="sup">9</a>]</citation>中提出的PU分类方法, 该方法第一步中使用Spy技术划分<i>RN</i>, 第二步使用朴素贝叶斯分类器和期望最大化 (Expectation-Maximum, EM) 算法来进行分类。</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138">3) I-EM:</h4>
                <div class="p1">
                    <p id="139">文献<citation id="337" type="reference">[<a class="sup">9</a>]</citation>中提出的PU分类方法, 该方法第一步使用朴素贝叶斯分类器从未标注集合<i>U</i>中划分出<i>RN</i>, 第二步使用EM算法来进行分类。</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140">4) Roc-SVM:</h4>
                <div class="p1">
                    <p id="141">文献<citation id="338" type="reference">[<a class="sup">20</a>]</citation>中提出的PU分类方法, 该方法第一步使用Rocchio算法划分出<i>RN</i>, 第二步迭代使用SVM来进行分类。</p>
                </div>
                <div class="p1">
                    <p id="142">为了与本文方法进行对比, 其中, S-EM、I-EM、Roc-SVM等方法均使用规则方法来划分正例集合<i>P</i>, 并用单词作为特征。本文方法与其他基线方法的实验结果见表1。</p>
                </div>
                <div class="area_img" id="143">
                    <p class="img_tit"><b>表</b>1 PU-MLP<b>与基线方法的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Experimental results of PU-MLP and baseline methods </p>
                    <p class="img_note">%</p>
                    <table id="143" border="1"><tr><td><br />Method</td><td>Precision</td><td>Recall</td><td>F1</td><td>Acc</td></tr><tr><td>规则</td><td>91.62</td><td>67.62</td><td>77.81</td><td>80.66</td></tr><tr><td><br />S-EM</td><td>62.54</td><td>94.08</td><td>75.13</td><td>68.77</td></tr><tr><td><br />I-EM</td><td>49.97</td><td>90.27</td><td>64.33</td><td>49.80</td></tr><tr><td><br />Roc-SVM</td><td>50.77</td><td>95.39</td><td>66.26</td><td>51.30</td></tr><tr><td><br />PU-MLP</td><td>85.11</td><td>79.06</td><td>81.98</td><td>82.67</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="144">表1中的实验结果中, 本文方法PU-MLP的F1值和准确率取得了最好效果, 比S-EM的 F1值和准确率分别提升了6.85个百分点、13.9个百分点, 比规则方法的 F1值和准确率则分别提升了4.17个百分点、2.01个百分点, 验证了本文方法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="145">在S-EM 、I-EM、Roc-SVM等几种方法中, S-EM方法的效果最好, 其原因在于S-EM方法使用了Spy技术来划分<i>RN</i>, 因而结果明显好于没有使用Spy技术的I-EM和Roc-SVM方法。另一方面, S-EM、I-EM、Roc-SVM三个方法和本文方法相比而言, 分类结果有较大差距, 原因有如下两点:一方面, S-EM、I-EM、Roc-SVM这三个方法均使用词袋模型作为文本表示, 特征维度高, 数据稀疏问题严重。另一方面, 上述三个方法在使用规则划分正例集合<i>P</i>时, <i>P</i>中包含了一定量的噪声数据, 在仅使用浅层的单词特征进行分类时, 可能受到噪声的影响较大, 导致之后的分类效果变差。</p>
                </div>
                <div class="p1">
                    <p id="146">为了验证Autoencoder特征的有效性, 第2个实验对本文方法使用不同的文本特征来进行对比实验, 包括词的Unigram特征、PV段落向量特征以及Word2Vec词向量特征, 实验结果见表2。表2中Unigram指传统的单词特征, 特征维度是20 189维, PV指120维的段落向量特征, Word2Vec是指120维的词向量求平均得到的文档特征向量。</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表</b>2 <b>本文方法使用不同特征的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Experimental results of the proposed method using different features </p>
                    <p class="img_note">%</p>
                    <table id="147" border="1"><tr><td><br />Feature</td><td>Precision</td><td>Recall</td><td>F1</td><td>Acc</td></tr><tr><td>Unigram</td><td>91.44</td><td>67.98</td><td>77.98</td><td>80.75</td></tr><tr><td><br />PV</td><td>65.59</td><td>92.84</td><td>76.87</td><td>71.99</td></tr><tr><td><br />Word2Vec</td><td>76.27</td><td>87.75</td><td>81.60</td><td>80.16</td></tr><tr><td><br />Autoencoder</td><td>85.11</td><td>79.06</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>8</mn><mn>1</mn><mo>.</mo><mn>9</mn><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>8</mn><mn>2</mn><mo>.</mo><mn>6</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="148">表2的实验结果表明, 使用Autoencoder特征比使用另外三种特征向量的效果有明显的提升, F1值比使用Unigram、PV、Word2Vec特征分别提升了4个百分点、5.11个百分点、0.38个百分点, Accuracy值则分别提升了1.92个百分点、10.68个百分点、2.51个百分点, 这说明利用自编码网络方法对高维、非线性的原始数据进行特征降维后, 能有效地利用学习过程中获得最优的低维特征向量进行建议语句识别。</p>
                </div>
                <div class="p1">
                    <p id="149">在传统PU学习方法的第二步中, 一般会选择EM算法或者选择其他分类器进行迭代, 而本文方法在第二步选择使用MLP的原因是由于EM算法对输入要求严格, 需要标称型数据, 不适用于Autoencoder特征向量。第3个实验将本文方法中的MLP分类器与其余迭代方法进行对比, 其余步骤保持不变, 实验结果见表3。</p>
                </div>
                <div class="p1">
                    <p id="150">表3中SVM、LG是指在本文方法第二步中直接利用<i>RN</i>和<i>P</i>来分别训练SVM和LR分类器并对<i>Q</i>分类;SVM-I则指迭代地训练SVM分类器, 每次训练完成后对<i>Q</i>进行分类, 并将<i>Q</i>中的反例划分到<i>RN</i>中, 然后进行下一次迭代, 直至<i>Q</i>中分不出反例。LG-I则将迭代步骤中的分类器替换成逻辑回归分类器, 其余步骤与SVM-I相同。</p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表</b>3 <b>本文方法使用</b>MLP<b>与不同迭代方法的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Experimental results of the proposed method using MLP and different iteration methods </p>
                    <p class="img_note">%</p>
                    <table id="151" border="1"><tr><td><br />Method</td><td>Precision</td><td>Recall</td><td>F1</td><td>Acc</td></tr><tr><td><br />SVM</td><td>91.60</td><td>68.40</td><td>78.32</td><td>81.01</td></tr><tr><td><br />LG</td><td>76.85</td><td>87.32</td><td>81.76</td><td>80.46</td></tr><tr><td><br />SVM-I</td><td>91.61</td><td>67.96</td><td>78.03</td><td>80.81</td></tr><tr><td><br />LG-I</td><td>84.07</td><td>79.54</td><td>81.74</td><td>82.18</td></tr><tr><td><br />PU-MLP</td><td>85.11</td><td>79.06</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>8</mn><mn>1</mn><mo>.</mo><mn>9</mn><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>8</mn><mn>2</mn><mo>.</mo><mn>6</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="152">对比表3中SVM与SVM-I的结果可以发现, 迭代的分类效果并不一定比只分类一次的效果好, 因为使用规则划分正例集合<i>P</i>, 导致<i>P</i>和<i>RN</i>中存在一些噪声, 如果在迭代过程中误将<i>Q</i>中的正例划分到<i>RN</i>中, 会导致不断迭代后的分类效果越来越差, 而且很难确定迭代过程中表现最好的分类器, 这也是使用迭代方法的一个缺点。MLP通过自身的网络结构能够实现对信息的记忆, 而所记忆的信息存储在神经元之间的权值中, 这种分布式的信息存储方式使得网络具有良好的容错性, 根据表3中的分类结果, PU-MLP分类一次的效果比迭代使用SVM、LG和仅使用一次SVM、LG都要好, 表明MLP更适用于本文任务。</p>
                </div>
                <div class="p1">
                    <p id="153">文献<citation id="339" type="reference">[<a class="sup">13</a>]</citation>指出正例集合<i>P</i>的大小对PU学习方法的效果影响较大, 为了分析<i>P</i>的大小对本文方法的影响, 因此进行第4个实验, 通过修改规则来改变<i>P</i>集合的大小, 并与使用不同数量标注正例的实验结果进行对比, 实验结果见表4。表4中, 20%～80%表示使用相应比例的手工标注正例作为<i>P</i>, Pos in P指<i>P</i>中包含的正例数量, Neg in U指<i>U</i>中包含的反例数量。</p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit"><b>表</b>4 <b>不同正例集合</b><i>P</i><b>的实验结果对比表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Experimental results comparison of different positive example sets <i>P</i></p>
                    <p class="img_note"></p>
                    <table id="154" border="1"><tr><td><br /><i>P</i></td><td>P size</td><td>Pos in P</td><td>U size</td><td>Neg in U</td><td>F1/%</td><td>Acc/%</td></tr><tr><td><br />PU-MLP</td><td>3 331</td><td>3 052</td><td>5 669</td><td>4 208</td><td>81.98</td><td>82.67</td></tr><tr><td><br />建议</td><td>1 751</td><td>1 601</td><td>7 249</td><td>2 912</td><td>66.64</td><td>72.08</td></tr><tr><td><br />希望</td><td>2 058</td><td>1 905</td><td>6 942</td><td>2 608</td><td>64.23</td><td>72.05</td></tr><tr><td><br />20%</td><td>902</td><td>902</td><td>8 098</td><td>8 098</td><td>77.51</td><td>78.66</td></tr><tr><td><br />40%</td><td>1 805</td><td>1 805</td><td>7 195</td><td>7 195</td><td>83.70</td><td>83.97</td></tr><tr><td><br />60%</td><td>2 707</td><td>2 707</td><td>6 293</td><td>6 293</td><td>86.06</td><td>84.92</td></tr><tr><td><br />80%</td><td>3 610</td><td>3 610</td><td>5 390</td><td>5 390</td><td>87.35</td><td>85.93</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="155">观察表4中使用标注正例的实验结果, 可以发现使用标注正例样本 (从20%变化到80%) 时, 随着<i>P</i>集合的增大, 分类效果逐步提升, 而随着<i>P</i>集合的减小, 分类效果逐步降低。在单独使用“建议”或者“希望”关键词来选择正例集合<i>P</i>时, <i>P</i>的规模明显减小, F1值和准确率下降明显, 均下降了10%以上, 与使用标注正例集合实验结果的变化趋势相同。此外, 使用规则时的PU-MLP模型的分类效果介于使用20%～40%手工标注正例之间, 其原因在于规则方法所选取的正例样本中存在噪声数据, 例如, “我的9350升级后2天, 屏幕右有条红线怎么处理, 希望大神回复。”, “三星都有那几款, 准备花4 000～5 500之间买个三星, 求建议!”这两条评论虽然符合本文所提的简单语言规则, 但却是噪声数据, 这些噪声数据会对后续分类过程产生不利影响。</p>
                </div>
                <h3 id="156" name="156" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="157">本文在中文数据集上开展建议挖掘研究, 提出了一种基于PU学习方法的无监督建议语句分类方法。首先通过规则来选择正例集合, 接着使用Spy技术划分出可信反例, 然后利用选择出来的正例集合和可信反例训练多层感知机MLP, 最后使用MLP进行分类。实验结果表明本文方法具有良好的分类性能, 能有效地进行建议语句分类任务。</p>
                </div>
                <div class="p1">
                    <p id="158">未来工作中, 将探索新的可靠反例划分方法, 将其用于建议语句分类中。此外, 本文实验只是在手机评论领域下进行的探索研究, 今后将在其他领域的语料上来分析本文方法的有效性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="267">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Suggestion mining from opinionated text">

                                <b>[1]</b>NEGI S.Suggestion mining from opinionated text[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop.Stroudsburg, PA:Association for Computational Linguistics, 2016:7-12.
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Suggestion Mining:Detecting Suggestions for Improvement in Users&amp;#39;&amp;#39;Comments">

                                <b>[2]</b>BRUN C, HAGEGE C.Suggestion mining:detecting suggestions for improvement in users'comments[J].Research in Computing Science, 2013, 70:171-181.
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wishful thinking: findingsuggestions and&amp;#39;&amp;#39;buy&amp;#39;&amp;#39;wishes from product reviews">

                                <b>[3]</b>RAMANAND J, BHAVSAR K, PEDANEKAR N.Wishful thinking:finding suggestions and 'buy' wishes from product reviews[C]//Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text.Stroudsburg, PA:Association for Computational Linguistics, 2010:54-61.
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic extraction of advice-revealing sentences for advice mining from online forums">

                                <b>[4]</b>WICAKSONO A F, MYAENG S-H.Automatic extraction of advicerevealing sentences for advice mining from online forums[C]//Proceedings of the 7th International Conference on Knowledge Capture.New York:ACM, 2013:97-104.
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The automated acquisition of suggestions from tweets">

                                <b>[5]</b>DONG L, WEI F, DUAN Y, et al.The automated acquisition of suggestions from tweets[C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2013:239-245.
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A study of suggestions in opinionated texts and their automatic detection">

                                <b>[6]</b>NEGI S, ASOOJA K, MEHROTRA S, et al.A study of suggestions in opinionated texts and their automatic detection[C]//Proceedings of the 5th Joint Conference on Lexical and Computational Semantics.Stroudsburg, PA:Association for Computational Linguistics, 2016:170-178.
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SDGY201805007&amp;v=MTg4NjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXNy9JTmluTWQ3RzRIOW5NcW85Rlk0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>张璞, 刘畅, 王永.基于特征融合和集成学习的建议语句分类模型[J].山东大学学报 (工学版) , 2018, 48 (5) :47-54. (ZHANGP, LIU C, WANG Y.Suggestion sentence classification model based on feature fusion and ensemble learning[J].Journal of Shandong University (Engineering Science) , 2018, 48 (5) :47-54.) 
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Computation Model for Unsupervised Feature Learningon Big Data">

                                <b>[8]</b>ZHANG Q C, YANG L T, CHEN Z K.Deep computation model for unsupervised feature learning on big data[J].IEEE Transactions on Services Computing, 2016, 9 (1) :161-171.
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Partially supervised classification of text documents">

                                <b>[9]</b>LIU B, LEE W S, YU P S, et al.Partially supervised classification of text documents[C]//Proceedings of the 19th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann Publishers, 2002:387-394.
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201503012&amp;v=MjI3NjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3L0lMeXZTZExHNEg5VE1ySTlFWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>任亚峰, 姬东鸿, 张红斌, 等.基于PU学习算法的虚假评论识别研究[J].计算机研究与发展, 2015, 52 (3) :639-648. (RENY F, JI D H, ZHANG H B, et al.Deceptive reviews detection based on positive and unlabeled learning[J].Journal of Computer Research and Development, 2015, 52 (3) :639-648.) 
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201311010&amp;v=MTM4NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURsVzcvSU55ZlRiTEc0SDlMTnJvOUVaSVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>刘露, 彭涛, 左万利, 等.一种基于聚类的PU主动文本分类方法[J].软件学报, 2013, 24 (11) :2571-2583. (LIU L, PENG T, ZUO W L, et al.Clustering-based PU active text classification method[J].Journal of Software, 2013, 24 (11) :2571-2583.) 
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJYD201506017&amp;v=MDU5NzVxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXNy9JS3lmU2FyRzRIOVRNcVk5RVk0UUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>王宗尧, 刘金岭.基于支持向量机的PU中文文本分类器构建[J].南京邮电大学学报 (自然科学版) , 2015, 35 (6) :100-105. (WANG Z Y, LIU J L.PU Chinese text classifier based on support vector machine construction[J].Journal of Nanjing University of Posts and Telecommunications (Natural Science Edition) , 2015, 35 (6) :100-105.) 
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Building Text Classifiers Using Positive and Unlabeled Examples">

                                <b>[13]</b>LIU B, DAI Y, LI X, et al.Building text classifiers using positive and unlabeled examples[C]//Proceedings of the 3rd IEEE International Conference on Data Mining.Washington, DC:IEEEComputer Society, 2003:179-188.
                            </a>
                        </p>
                        <p id="293">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimedia data mining using deep learning">

                                <b>[14]</b>WLODARCZAK P, SOAR J, ALLY M.Multimedia data mining using deep learning[C]//Proceedings of the 2015 5th International Conference on Digital Information Processing and Communications.Piscataway, NJ:IEEE, 2015:190-196.
                            </a>
                        </p>
                        <p id="295">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-Lingual Sentiment Classification Based on Denoising Autoencoder">

                                <b>[15]</b>ZHOU H, CHEN L, HUANG D.Cross-lingual sentiment classification based on denoising autoencoder[M]//ZONG C, NIE J Y, ZHAO D, et al.Natural Language Processing and Chinese Computing, CCIS 496.Berlin:Springer, 2014:181-192.
                            </a>
                        </p>
                        <p id="297">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC201508027&amp;v=MzE1NTNpRGxXNy9JUHluUmJiRzRIOVRNcDQ5SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>魏超, 罗森林, 张竞, 等.自编码网络短文本流形表示方法[J].浙江大学学报 (工学版) , 2015, 49 (8) :1591-1599. (WEI C, LUO S L, ZHANG J, et al.Short text manifold representation based on AutoEncoder network[J].Journal of Zhejiang University (Engineering Science) , 2015, 49 (8) :1591-1599.) 
                            </a>
                        </p>
                        <p id="299">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201703033&amp;v=MDg5Nzh0R0ZyQ1VSN3FmWnVacEZpRGxXNy9JSVRmVGU3RzRIOWJNckk5R1o0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>高妮, 高岭, 贺毅岳, 等.基于自编码网络特征降维的轻量级入侵检测模型[J].电子学报, 2017, 45 (3) :730-739. (GAO N, GAO L, HE Y Y, et al.A lightweight intrusion detection model based on autoencoder network with feature reduction[J].Acta Electronica Sinica, 2017, 45 (3) :730-739.) 
                            </a>
                        </p>
                        <p id="301">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning representation using autoencoder for 3D shape retrieval">

                                <b>[18]</b>ZHU Z, WANG X, BAI S, et al.Deep learning representation using autoencoder for 3D shape retrieval[J].Neurocomputing, 2016, 204 (C) :41-50.
                            </a>
                        </p>
                        <p id="303">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed representations of sentences and documents">

                                <b>[19]</b>LE Q, MKOLOV T.Distributed representations of sentences and documents[EB/OL].[2018-06-20].https://arxiv.org/pdf/1405.4053v2.pdf.
                            </a>
                        </p>
                        <p id="305">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to Classify Texts Using Positive and Unlabeled Data">

                                <b>[20]</b>LI X, LIU B.Learning to classify texts using positive and unlabeled data[C]//Proceedings of the 18th International Joint Conference on Artificial Intelligence.San Francisco, CA:Morgan Kaufmann Publishers, 2003:587-592.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201903005" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903005&amp;v=MTUyMTMzenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3L0lMejdCZDdHNEg5ak1ySTlGWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
