<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136665013752500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907040%26RESULT%3d1%26SIGN%3d9WVjIDuLBJV%252f2a%252fMGPAF0z%252bCpZE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907040&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907040&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907040&amp;v=MjY0MjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1U3clBMejdCZDdHNEg5ak1xSTlCWklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 风格迁移网络 ">1 风格迁移网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="1.1 &lt;b&gt;前馈转换网络&lt;/b&gt;">1.1 <b>前馈转换网络</b></a></li>
                                                <li><a href="#69" data-title="1.2 &lt;b&gt;损失网络&lt;/b&gt;">1.2 <b>损失网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#100" data-title="2 聚焦卷积神经网络 ">2 聚焦卷积神经网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="2.1 &lt;b&gt;聚焦结构&lt;/b&gt;">2.1 <b>聚焦结构</b></a></li>
                                                <li><a href="#112" data-title="2.2 &lt;b&gt;网络结构&lt;/b&gt;">2.2 <b>网络结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="3 实验结果及分析 ">3 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#117" data-title="3.1 &lt;b&gt;实验设置&lt;/b&gt;">3.1 <b>实验设置</b></a></li>
                                                <li><a href="#125" data-title="3.2 &lt;b&gt;本文方法与人工设计特征方法的对比实验&lt;/b&gt;">3.2 <b>本文方法与人工设计特征方法的对比实验</b></a></li>
                                                <li><a href="#129" data-title="3.3 &lt;b&gt;风格迁移网络效果论证实验&lt;/b&gt;">3.3 <b>风格迁移网络效果论证实验</b></a></li>
                                                <li><a href="#136" data-title="3.4 &lt;b&gt;本文方法与其他卷积神经网络方法的对比实验&lt;/b&gt;">3.4 <b>本文方法与其他卷积神经网络方法的对比实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#139" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 风格迁移网络结构">图1 风格迁移网络结构</a></li>
                                                <li><a href="#102" data-title="图2 短长度碎屑">图2 短长度碎屑</a></li>
                                                <li><a href="#108" data-title="图3 空洞卷积">图3 空洞卷积</a></li>
                                                <li><a href="#110" data-title="图4 聚焦结构">图4 聚焦结构</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;聚焦卷积神经网络参数&lt;/b&gt;"><b>表</b>1 <b>聚焦卷积神经网络参数</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;与传统人工设计特征方法的对比结果&lt;/b&gt;"><b>表</b>2 <b>与传统人工设计特征方法的对比结果</b></a></li>
                                                <li><a href="#132" data-title="图5 风格迁移网络处理效果">图5 风格迁移网络处理效果</a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同方法的实验结果对比&lt;/b&gt;"><b>表</b>3 <b>不同方法的实验结果对比</b></a></li>
                                                <li><a href="#135" data-title="图6 不同网络模型的训练损失">图6 不同网络模型的训练损失</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同卷积神经网络的对比结果&lt;/b&gt;"><b>表</b>4 <b>不同卷积神经网络的对比结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="170">


                                    <a id="bibliography_1" title=" 宋迪, 张东波, 刘霞.基于Gabor和纹理抑制的手机配件划痕检测[J].计算机工程, 2014, 40 (9) :1-5. (SONG D, ZHANG D B, LIU X.Scratch detection for mobile phone accessories based on Gabor and texture suppression[J].Computer Engineering, 2014, 40 (9) :1-5.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201409002&amp;v=MDc3OTQvZ1U3clBMejdCYmJHNEg5WE1wbzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         宋迪, 张东波, 刘霞.基于Gabor和纹理抑制的手机配件划痕检测[J].计算机工程, 2014, 40 (9) :1-5. (SONG D, ZHANG D B, LIU X.Scratch detection for mobile phone accessories based on Gabor and texture suppression[J].Computer Engineering, 2014, 40 (9) :1-5.) 
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_2" title=" 韩芳芳, 段发阶, 张宝峰, 等.单线阵CCD系统的表面凹坑缺陷检测方法[J].哈尔滨工业大学学报, 2012, 44 (4) :116-120. (HAN F F, DUAN F J, ZHANG B F, et al.Study and modeling for surface pit defect detection based on linear array CCD system[J].Journal of Harbin Institute of Technology, 2012, 44 (4) :116-120.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBX201204024&amp;v=MjEzODFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1U3clBMU2pKZHJHNEg5UE1xNDlIWUlRS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         韩芳芳, 段发阶, 张宝峰, 等.单线阵CCD系统的表面凹坑缺陷检测方法[J].哈尔滨工业大学学报, 2012, 44 (4) :116-120. (HAN F F, DUAN F J, ZHANG B F, et al.Study and modeling for surface pit defect detection based on linear array CCD system[J].Journal of Harbin Institute of Technology, 2012, 44 (4) :116-120.) 
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_3" title=" 崔炽标, 李阳, 毛霆, 等.基于预处理与卷积神经网络的塑件划痕检测[J].模具工业, 2017, 43 (9) :1-6. (CUI Z B, LI Y, MAO T, et al.Scratch detection of plastics based on preprocessing and convolutional neural network[J].Die and Mould Industry, 2017, 43 (9) :1-6.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MJGY201709004&amp;v=MjM5ODhadVpzRnkvZ1U3clBLQ2ZNZDdHNEg5Yk1wbzlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         崔炽标, 李阳, 毛霆, 等.基于预处理与卷积神经网络的塑件划痕检测[J].模具工业, 2017, 43 (9) :1-6. (CUI Z B, LI Y, MAO T, et al.Scratch detection of plastics based on preprocessing and convolutional neural network[J].Die and Mould Industry, 2017, 43 (9) :1-6.) 
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_4" title=" 李克斌, 余厚云, 周申江.基于形态学特征的机械零件表面划痕检测[J].光学学报, 2018, 38 (8) :815027- 1-815027- 7. (LI K B, YU H Y, ZHOU S J.Scratch detection for the surface of mechanical parts based on morphological features[J].Acta Optica Sinica, 2018, 38 (8) :815027- 1-815027- 7.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808028&amp;v=MjE5ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTdyUElqWFRiTEc0SDluTXA0OUhiSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         李克斌, 余厚云, 周申江.基于形态学特征的机械零件表面划痕检测[J].光学学报, 2018, 38 (8) :815027- 1-815027- 7. (LI K B, YU H Y, ZHOU S J.Scratch detection for the surface of mechanical parts based on morphological features[J].Acta Optica Sinica, 2018, 38 (8) :815027- 1-815027- 7.) 
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_5" title=" 汤勃, 孔建益, 伍世虔.机器视觉表面缺陷检测综述[J].中国图象图形学报, 2017, 22 (12) :1640-1663. (TANG B, KONG J Y, WU S Q.Review of surface defect detection based on machine vision[J].Journal of Image and Graphics, 2017, 22 (12) :1640-1663.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201712002&amp;v=MTIzOTJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dVN3JQUHlyZmJMRzRIOWJOclk5RlpvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         汤勃, 孔建益, 伍世虔.机器视觉表面缺陷检测综述[J].中国图象图形学报, 2017, 22 (12) :1640-1663. (TANG B, KONG J Y, WU S Q.Review of surface defect detection based on machine vision[J].Journal of Image and Graphics, 2017, 22 (12) :1640-1663.) 
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_6" title=" 胡文瑾, 李战明, 刘仲民.一种基于小波分析的唐卡图像划痕检测[J].光学技术, 2012, 38 (6) :751-755. (HU W J, LI Z M, LIU Z M.Scratch detection algorithm based on wavelet analysis for Thangka image[J].Optical Technique, 2012, 38 (6) :751-755.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJS201206024&amp;v=MDA1NDc0SDlQTXFZOUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTdyUElqWEJmYkc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         胡文瑾, 李战明, 刘仲民.一种基于小波分析的唐卡图像划痕检测[J].光学技术, 2012, 38 (6) :751-755. (HU W J, LI Z M, LIU Z M.Scratch detection algorithm based on wavelet analysis for Thangka image[J].Optical Technique, 2012, 38 (6) :751-755.) 
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_7" title=" 周鹏, 徐科, 刘顺华.基于剪切波和小波特征融合的金属表面缺陷识别方法[J].机械工程学报, 2015, 51 (6) :98-103. (ZHOU P, XU K, LIU S H.Surface defect recognition for metals based on feature fusion of shearlets and wavelets[J].Journal of Mechanical Engineering, 2015, 51 (6) :98-103.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JXXB201506019&amp;v=MTA2NDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dVN3JQTHpYVGJMRzRIOVRNcVk5RWJZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         周鹏, 徐科, 刘顺华.基于剪切波和小波特征融合的金属表面缺陷识别方法[J].机械工程学报, 2015, 51 (6) :98-103. (ZHOU P, XU K, LIU S H.Surface defect recognition for metals based on feature fusion of shearlets and wavelets[J].Journal of Mechanical Engineering, 2015, 51 (6) :98-103.) 
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_8" title=" 马云鹏, 李庆武, 何飞佳, 等.金属表面缺陷自适应分割算法[J].仪器仪表学报, 2017, 38 (1) :245-251. (MA Y P, LI Q W, HE F J, et al.Adaptive segmentation algorithm for metal surface defects[J].Chinese Journal of Scientific Instrument, 2017, 38 (1) :245-251.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201701032&amp;v=MjMzNDk5Yk1ybzlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1U3clBQRHpUYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         马云鹏, 李庆武, 何飞佳, 等.金属表面缺陷自适应分割算法[J].仪器仪表学报, 2017, 38 (1) :245-251. (MA Y P, LI Q W, HE F J, et al.Adaptive segmentation algorithm for metal surface defects[J].Chinese Journal of Scientific Instrument, 2017, 38 (1) :245-251.) 
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_9" title=" 郭皓然, 邵伟, 周阿维, 等.全局阈值自适应的高亮金属表面缺陷识别新方法[J].仪器仪表学报, 2017, 38 (11) :2797-2804. (GUO H R, SHAO W, ZHOU A W, et al.Novel defect recognition method based on adaptive global threshold for highlight metal surface[J].Chinese Journal of Scientific Instrument, 2017, 38 (11) :2797-2804.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201711022&amp;v=MDUyNzhyUFBEelRiTEc0SDliTnJvOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         郭皓然, 邵伟, 周阿维, 等.全局阈值自适应的高亮金属表面缺陷识别新方法[J].仪器仪表学报, 2017, 38 (11) :2797-2804. (GUO H R, SHAO W, ZHOU A W, et al.Novel defect recognition method based on adaptive global threshold for highlight metal surface[J].Chinese Journal of Scientific Instrument, 2017, 38 (11) :2797-2804.) 
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     JOHNSON J, ALAHI A, LI F F.Perceptual losses for real-time style transfer and super-resolution[C]// Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:694-711.</a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_11" title=" RUMELHART D E, HINTON G E, WILLIAMS R J.Learning representations by back-propagating errors[J].Nature, 1986, 323 (6088) :533-536." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning representations by back propagating errors">
                                        <b>[11]</b>
                                         RUMELHART D E, HINTON G E, WILLIAMS R J.Learning representations by back-propagating errors[J].Nature, 1986, 323 (6088) :533-536.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_12" title=" HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[12]</b>
                                         HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_13" title=" BOTTOU, CURTIS F E, NOCEDA J, et al.Optimization methods for large-scale machine learning[J].SIAM Review, 2016, 60 (2) :223-311." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimization methods for large-scale machine learning">
                                        <b>[13]</b>
                                         BOTTOU, CURTIS F E, NOCEDA J, et al.Optimization methods for large-scale machine learning[J].SIAM Review, 2016, 60 (2) :223-311.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_14" title=" RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].ArXiv Preprint, 2016, 2016:1511.06434." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">
                                        <b>[14]</b>
                                         RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].ArXiv Preprint, 2016, 2016:1511.06434.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_15" title=" LIM B, LEE K M.Deep recurrent ResNet for video super-resolution[C]// Proceedings of the 2018 IEEE Conference on Asia-pacific Signal and Information Processing Association Summit.Washington, DC:IEEE Conputer Society, 2018:643-648." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep recurrent ResNet for video super-resolution">
                                        <b>[15]</b>
                                         LIM B, LEE K M.Deep recurrent ResNet for video super-resolution[C]// Proceedings of the 2018 IEEE Conference on Asia-pacific Signal and Information Processing Association Summit.Washington, DC:IEEE Conputer Society, 2018:643-648.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_16" title=" IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift">
                                        <b>[16]</b>
                                         IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:448-456.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_17" title=" NAIR V, HINTON G E.Rectified linear units improve restricted boltzmann machines[C]// Proceedings of the 2010 International Conference on Machine Learning.New York:ACM, 2010:807-814." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted Boltzmann machines">
                                        <b>[17]</b>
                                         NAIR V, HINTON G E.Rectified linear units improve restricted boltzmann machines[C]// Proceedings of the 2010 International Conference on Machine Learning.New York:ACM, 2010:807-814.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_18" title=" SIMONVAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[C]// Proceedings of the 2015 International Conference on Learning.Washington, DC:IEEE Computer Society, 2015:687-699." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[18]</b>
                                         SIMONVAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[C]// Proceedings of the 2015 International Conference on Learning.Washington, DC:IEEE Computer Society, 2015:687-699.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_19" title=" YU F, KOLTUN V.Multi-scale context aggregation by dilated convolutions[C]// Proceedings of the 2016 International Conference on Learning.Washington, DC:IEEE Computer Society, 2016:511-524." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">
                                        <b>[19]</b>
                                         YU F, KOLTUN V.Multi-scale context aggregation by dilated convolutions[C]// Proceedings of the 2016 International Conference on Learning.Washington, DC:IEEE Computer Society, 2016:511-524.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_20" title=" KINGMA D, BA J.Adam:a method for stochastic optimization[C]// Proceedings of the 2014 International Conference on Learning.Washington, DC:IEEE Computer Society, 2014:248-263." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">
                                        <b>[20]</b>
                                         KINGMA D, BA J.Adam:a method for stochastic optimization[C]// Proceedings of the 2014 International Conference on Learning.Washington, DC:IEEE Computer Society, 2014:248-263.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_21" title=" HOWARD A G, ZHU M, CHEN B, et al.MobileNets:efficient convolutional neural networks for mobile vision applications[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:1056-1065." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MobileNets:Efficient Convolutional Neural Networks for Mobile Vision Applications">
                                        <b>[21]</b>
                                         HOWARD A G, ZHU M, CHEN B, et al.MobileNets:efficient convolutional neural networks for mobile vision applications[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:1056-1065.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_22" title=" ZHANG X, ZHOU X, LIN M, et al.ShuffleNet:an extremely efficient convolutional neural network for mobile devices[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:563-572." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ShuffleNet:an extremely efficient convolutional neural network for mobile devices">
                                        <b>[22]</b>
                                         ZHANG X, ZHOU X, LIN M, et al.ShuffleNet:an extremely efficient convolutional neural network for mobile devices[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:563-572.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-02-20 16:24</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),2103-2108 DOI:10.11772/j.issn.1001-9081.2018112247            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度神经网络的表面划痕识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%96%87%E4%BF%8A&amp;code=38429762&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李文俊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%96%8C&amp;code=33601829&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BB%BA%E6%98%8E&amp;code=42202204&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李建明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%B1%E5%9F%BA%E5%BE%B7&amp;code=28625442&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钱基德</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E6%88%90%E9%83%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0211714&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院成都计算机应用研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%B9%BF%E5%B7%9E%E7%94%B5%E5%AD%90%E6%8A%80%E6%9C%AF%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=0101523&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院广州电子技术有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为实现亮度不均的复杂纹理背景下表面划痕的鲁棒、精确、实时识别, 提出一种基于深度神经网络的表面划痕识别方法。用于表面划痕识别的深度神经网络由风格迁移网络和聚焦卷积神经网络 (CNN) 构成, 其中风格迁移网络针对亮度不均的复杂背景下的表面划痕进行预处理, 风格迁移网络包括前馈转换网络和损失网络, 首先通过损失网络提取亮度均匀模板的风格特征和检测图像的知觉特征, 对前馈转换网络进行离线训练, 获取网络最优参数值, 最终使风格迁移网络生成亮度均匀且风格一致的图像, 然后, 利用所提出的基于聚焦结构的聚焦卷积神经网络对生成图像中的划痕特征进行提取并识别。以光照变化的金属表面为例, 进行划痕识别实验, 实验结果表明:与需要人工设计特征的传统图像处理方法及传统深度卷积神经网络相比, 划痕漏报率低至8.54%, 并且收敛速度更快, 收敛曲线更加平滑, 在不同的深度模型下均可取得较好的检测效果, 准确率提升2%左右。风格迁移网络能够保留完整划痕特征的同时有效解决亮度不均的问题, 从而提高划痕识别精度;同时聚焦卷积神经网络能够实现对划痕的鲁棒、精确、实时识别, 大幅度降低划痕漏报率和误报率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%AE%E5%BA%A6%E4%B8%8D%E5%9D%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">亮度不均;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%8D%E6%9D%82%E7%BA%B9%E7%90%86%E8%83%8C%E6%99%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">复杂纹理背景;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%A8%E9%9D%A2%E5%88%92%E7%97%95%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">表面划痕识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">风格迁移网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李文俊 (1994—) , 男, 湖南岳阳人, 硕士研究生, CCF会员, 主要研究方向:计算机视觉、深度学习;;
                                </span>
                                <span>
                                    *陈斌 (1970—) , 男, 四川广汉人, 研究员, 博士生导师, 博士, 主要研究方向:实时工业图像分析、字符识别、智能视觉控制;电子邮箱chenbin306@sohu.com;
                                </span>
                                <span>
                                    李建明 (1989—) , 男, 四川南部人, 博士研究生, 主要研究方向:目标识别、深度学习;;
                                </span>
                                <span>
                                    钱基德 (1988—) , 男, 四川大英人, 博士, 主要研究方向:机器视觉、深度学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-09</p>

                    <p>

                            <b>基金：</b>
                                                        <span>广东省重大科技专项 (2017B030306017);</span>
                                <span>广东省产学研合作项目 (2017B090901040);</span>
                    </p>
            </div>
                    <h1><b>Surface scratch recognition method based on deep neural network</b></h1>
                    <h2>
                    <span>LI Wenjun</span>
                    <span>CHEN Bin</span>
                    <span>LI Jianming</span>
                    <span>QIAN Jide</span>
            </h2>
                    <h2>
                    <span>Chengdu Institute of Computer Application, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
                    <span>Guangzhou Electronic Technology Company Limited of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to achieve robust, accurate and real-time recognition of surface scratches under complex texture background with uneven brightness, a surface scratch recognition method based on deep neural network was proposed. The deep neural network for surface scratch recognition consisted of a style transfer network and a focus Convolutional Neural Network (CNN) . The style transfer network was used to preprocess surface scratches under complex background with uneven brightness. The style transfer networks included a feedforward conversion network and a loss network. Firstly, the style features of uniform brightness template and the perceptual features of the detected image were extracted through the loss network, and the feedforward conversion network was trained offline to obtain the optimal parameter values of network. Then, the images with uniform brightness and uniform style were generated by style transfer network. Finally, the proposed focus convolutional neural network based on focus structure was used to extract and recognize scratch features in the generated image. Taking metal surface with light change as an example, the scratch recognition experiment was carried out. The experimental results show that compared with traditional image processing methods requiring artificial designed features and traditional deep convolutional neural network, the false negative rate of scratch detection is as low as 8.54% with faster convergence speed and smoother convergence curve, and the better detection results can be obtained under different depth models with accuracy increased of about 2%. The style transfer network can retain complete scratch features with the problem of uneven brightness solved, thus improving the accuracy of scratch recognition, while the focus convolutional neural network can achieve robust, accurate and real-time recognition of scratches, which greatly reduces false negative rate and false positive rate of scratches.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=uneven%20brightness&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">uneven brightness;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=complex%20texture%20background&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">complex texture background;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=surface%20scratch%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">surface scratch recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=style%20transfer%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">style transfer network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Wenjun, born in 1994, M. S. candidate. His research interests include computer vision, deep learning. ;
                                </span>
                                <span>
                                    CHEN Bin, born in 1970, Ph. D. , research fellow. His research interests include real time industrial image analysis, character recognition, intelligent visual control. ;
                                </span>
                                <span>
                                    LI Jianming, born in 1989, Ph. D. candidate. His research interests include object recognition, deep learning. ;
                                </span>
                                <span>
                                    QIAN Jide, born in 1988, Ph. D. His research interests include machine vision, deep learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-09</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Major Scientific and Technological Project of Guangdong Province (2017B030306017);</span>
                                <span>the IndustryUniversity-Research Cooperation Project of Guangdong Province (2017B090901040);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">工业产品从配料到最终成型的整个过程中, 由于运输、生产工艺等意外情况, 表面常存在磕伤、划伤、擦伤等造成的损伤性划痕缺陷, 因此, 各种表面的划痕识别方法一直都是学者和工业界研究的热点, 如表面凹坑缺陷检测<citation id="214" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、玻璃屏幕表面划痕检测<citation id="215" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、塑件划痕检测<citation id="216" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、机械零件表面划痕检测<citation id="217" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等;但是表面划痕识别检测技术受环境、光照、生产工艺和噪声等多重因素的影响, 检测系统的信噪比一般较低, 微弱信号难以检出或不能与噪声有效区分, 以及由于检测对象多样、表面划痕缺陷种类繁多、形态多样、背景复杂, 致使对划痕缺陷的描述不充分, 对缺陷的特征提取有效性不高<citation id="218" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。2012年, 胡文瑾等<citation id="219" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>针对复杂的藏族唐卡图像上的垂直划痕利用小波变换实现划痕检测, 首先通过计算小波模极大值来描述图像中目标的多尺度边界, 然后通过投影变换增强划痕中心亮度的极值特性并采用多尺度检测划痕的可能位置, 再根据划痕的宽度、高度以及连通分量筛选出垂直划痕;但是该方法无法适应各种形态、方向下的划痕。2015年, 周鹏等<citation id="220" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>针对复杂金属表面提出了将剪切波和小波特征融合的划痕识别方法, 剪切波变换可以提取图像在不同尺度、不同方向上的信息, 因此对方向性的缺陷识别效果好。2017年, 马云鹏等<citation id="221" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>首先从多个方向对金属表面的灰度图像进行转换, 然后对多幅图像分别进行灰度波动分析, 自适应地改变阈值与步长对图像进行邻域灰度差分割处理, 最后利用主成分分析 (Principal Component Analysis, PCA) 算法将多幅图像压缩至单幅图像实现划痕分割, 但是灰度变化不明显的金属表面图像容易出现细节信息丢失等问题以及分割需要时间太长, 无法满足工业实时检测要求。郭皓然等<citation id="222" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>针对高光噪声缺陷图像, 利用高斯函数的一阶导数构建Canny最优边缘检测器, 结合全局阈值的最大类间方差法和形态学图像分割法, 对亮度不均的表面划痕图像进行分割从而实现识别;但该算法通用性不强, 并且无法保留划痕的完整特征。因此, 如何构建准确、鲁棒、实时的表面划痕检测系统, 以适应光照变化、背景噪声以及其他外界不良环境的干扰, 仍然是目前需要解决的难题。</p>
                </div>
                <div class="p1">
                    <p id="49">2016年, Li等<citation id="223" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>针对图像转换中的图像风格迁移问题, 提出了由前馈转换网络和损失网络构成的风格迁移网络, 使用基于损失网络得到的特征重建损失以及风格重建损失来训练前馈转换网络, 生成与模板图像风格一致, 同时能够保留原始图像主体特征的转换图像。</p>
                </div>
                <div class="p1">
                    <p id="50">为了克服上述利用人工设计特征的划痕识别方法的不足, 本文提出基于风格迁移网络将亮度均匀、背景单一的模板图像风格迁移到待检图像中进行预处理得到输出图像, 再利用聚焦卷积神经网络对输出图像进行表面划痕识别。待检图像通过前馈转换网络生成图像, 利用损失网络对图像特征进行特征提取, 计算生成图像与待检图像以及生成图像与模板图像语义特征之间的均方差损失, 然后传回前馈转换网络进行反向传播<citation id="224" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 迭代更新权重参数。风格迁移网络能够融合亮度均匀模板图像风格特征以及待检检测图像的知觉特征生成亮度均匀保留完整划痕特征的图像, 然后利用基于聚焦结构的多尺度卷积神经网络对生成图像中划痕特征进行识别, 由于训练集的亮度均匀且风格特征一致, 因此网络能更好地针对不同形态、尺寸的划痕特征进行提取进而识别。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 风格迁移网络</h3>
                <div class="p1">
                    <p id="52">本文基于Li等<citation id="225" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出的风格迁移网络对亮度不均的复杂背景纹理进行预处理。如图1所示, 风格迁移网络包括前馈转换网络<i>fw</i>以及损失网络ϕ。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907040_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 风格迁移网络结构" src="Detail/GetImg?filename=images/JSJY201907040_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 风格迁移网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907040_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure of style transfer network</p>

                </div>
                <div class="p1">
                    <p id="54">前馈转换网络为一个深度残差网络<citation id="226" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 参数是权重<b><i>w</i></b>, 网络将输入图像<b><i>x</i></b>通过映射<mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo>=</mo><mi>f</mi><mi>w</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></math></mathml>转换成输出图像<mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>, 每一个损失函数计算一个标量值<i>l</i><sub><i>i</i></sub> (<mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>, <i>y</i><sub><i>i</i></sub>) 用来衡量输出图像<mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>以及一个目标图像之间的差异。前馈转换网络使用随机梯度下降 (Stochastic Gradient Descent, SGD) <citation id="227" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>优化方法来最小化一组加权为<i>λ</i><sub><i>i</i></sub>的损失函数<i>W</i><sup>*</sup>, 可以写作:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>W</mi></munder><mspace width="0.25em" /><mi>E</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>, </mo><mo stretchy="false">{</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></msub><mo stretchy="false">[</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>λ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>f</mi><mi>w</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">相比逐像素计算损失函数的方式, 损失网络ϕ定义了一个特征重建损失<i>l</i><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>f</mtext><mtext>e</mtext><mtext>a</mtext><mtext>t</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>以及一个风格重建损失<i>l</i><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>y</mtext><mtext>l</mtext><mtext>e</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>, 分别用来衡量输出图像<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>与内容图像<b><i>y</i></b><sub><i>c</i></sub>以及模板图像<b><i>y</i></b><sub><i>s</i></sub>之间的差异。在训练时, 对于每一张输入图像<b><i>x</i></b>, 都对应有一个内容图像<b><i>y</i></b><sub><i>c</i></sub>以及一张模板图像<b><i>y</i></b><sub><i>s</i></sub>。对于风格迁移而言, 内容图像<b><i>y</i></b><sub><i>c</i></sub>就是输入图像<b><i>x</i></b>, 而输出图像<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>应该是内容图像<b><i>y</i></b><sub><i>c</i></sub>与模板图像<b><i>y</i></b><sub><i>s</i></sub>的结合。对于一张模板图像, 可以训练得到一个风格迁移网络。在测试时, 输入图像<b><i>x</i></b>直接通过前馈转换网络得到输出图像<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.1 <b>前馈转换网络</b></h4>
                <div class="p1">
                    <p id="67">前馈转换网络为一个深度残差网络, 网络设计基本遵循<i>Radford</i>等<citation id="228" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的指导性意见, 没有使用任何的池化层, 而是利用步幅卷积在网络中进行上采样或者下采样操作。前馈转换网络由五个残差块组成, 使用<i>ResNet</i>网络结构<citation id="229" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 所有的非残差卷积层之后都使用了一个空间性的批标准化 (<i>Batch Normalization</i>, <i>BN</i>) <citation id="230" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>以及非线性激活函数线性修正单元 (<i>Rectified Linear Unit</i>, <i>ReLU</i>) <citation id="231" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 最后一层除外, 最后一层使用一个缩放的<i>tanh</i>非线性函数来确保输出图像的像素值在[0, 255]区间。除了第一层和最后一层使用9×9大小的卷积核之外, 其余卷积层都使用3×3大小的卷积核。</p>
                </div>
                <div class="p1">
                    <p id="68">输入和输出:对于风格迁移而言, 输入和输出都是灰度图, 大小为200×200。网络使用了两个步长为2的卷积对输入进行下采样, 紧接着是几个残差块, 然后是两个步长为1/2的卷积层来进行上采样。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">1.2 <b>损失网络</b></h4>
                <div class="p1">
                    <p id="70">损失网络采用的网络模型为利用<i>ImageNet</i>数据集预训练的<i>VGG</i>16<citation id="232" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 并且定义了两个损失函数用来衡量两张图像在高层语义以及感知上的差别。</p>
                </div>
                <div class="p1">
                    <p id="71">1) 特征重建损失<i>l</i><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>f</mtext><mtext>e</mtext><mtext>a</mtext><mtext>t</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>。没有用到传统的逐像素对比损失, 而是用VGG16网络模型来得到高层特征来表征图像内容, 公式可以写作:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>l</mi><mtext> </mtext></mrow><msubsup><mrow></mrow><mrow><mtext>f</mtext><mtext>e</mtext><mtext>a</mtext><mtext>t</mtext></mrow><mrow><mtext>ϕ</mtext><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msubsup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo>, </mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mi>Η</mi><msub><mrow></mrow><mi>j</mi></msub><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo stretchy="false">∥</mo><mtext>ϕ</mtext><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>-</mo><mtext>ϕ</mtext><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中, <i>C</i>、<i>H</i>、<i>W</i>分别为特征图的通道数及尺寸大小, 下述相同。式 (2) 寻找一个输出图像<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>与内容图像<b><i>y</i></b><sub><i>c</i></sub>对应的第<i>j</i>层的特征图ϕ<sub><i>j</i></sub> (<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>) 和ϕ<sub><i>j</i></sub> (<b><i>y</i></b><sub><i>c</i></sub>) 使得低维语义层之间特征损失最小, 使得输出图像能够产生与内容图像<b><i>y</i></b><sub><i>c</i></sub>视觉上不太能区分的图像, 如果用高维语义层来进行特征重建, 内容和全局结构会被保留下来, 但是颜色纹理以及精确的形状信息将丢失, 因此使用一个特征重建损失<i>l</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>f</mtext><mtext>e</mtext><mtext>a</mtext><mtext>t</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>来训练前馈转换网络让输出图像<mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>非常接近内容图像<b><i>y</i></b><sub><i>c</i></sub>, 但并不是让它们完全匹配。</p>
                </div>
                <div class="p1">
                    <p id="79">2) 风格重建损失<i>l</i><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>y</mtext><mtext>l</mtext><mtext>e</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>。特征重建损失<i>l</i><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>f</mtext><mtext>e</mtext><mtext>a</mtext><mtext>t</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>使输出图像<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>的内容更加接近内容图像<b><i>y</i></b><sub><i>c</i></sub>, 同时, 也希望通过风格重建损失来让输出图像<mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>与模板图像<b><i>y</i></b><sub><i>s</i></sub>在颜色、纹理、通用模式等方面更加接近。为了实现这一点, 定义了Gram矩阵来表征上述方面特性, 令ϕ<sub><i>j</i></sub> (<b><i>x</i></b>) 表示对于输入图像<b><i>x</i></b>在损失网络ϕ中第<i>j</i>层的激活响应, 特征图的形状为<i>C</i><sub><i>j</i></sub>×<i>H</i><sub><i>j</i></sub>×<i>W</i><sub><i>j</i></sub>, 大小为<i>C</i><sub><i>j</i></sub>×<i>C</i><sub><i>j</i></sub>的Gram矩阵<b><i>G</i></b><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mtext>ϕ</mtext></msubsup></mrow></math></mathml> (<b><i>x</i></b>) 中的元素定义可以写作:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">G</mi><msubsup><mrow></mrow><mi>j</mi><mtext>ϕ</mtext></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mi>c</mi><mo>, </mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><mo>=</mo><mfrac><mn>1</mn><mrow><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mo>×</mo><mi>Η</mi><msub><mrow></mrow><mi>j</mi></msub><mo>×</mo><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Η</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munderover><mtext>ϕ</mtext></mstyle></mrow></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>h</mi><mo>, </mo><mi>w</mi><mo>, </mo><mi>c</mi></mrow></msub><mtext>ϕ</mtext><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>h</mi><mo>, </mo><mi>w</mi><mo>, </mo><mover accent="true"><mi>c</mi><mo>^</mo></mover></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中<i>c</i>, <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>c</mi><mo>^</mo></mover></math></mathml>分别代表特征图中任意两个通道, 如果将对应特征图ϕ<sub><i>j</i></sub> (<b><i>x</i></b>) 看成一个<i>C</i><sub><i>j</i></sub>维度的特征, 每个特征的形状为<i>H</i><sub><i>j</i></sub>×<i>W</i><sub><i>j</i></sub>, 式 (3) 中<b><i>G</i></b><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mtext>ϕ</mtext></msubsup></mrow></math></mathml> (<b><i>x</i></b>) 与<i>C</i><sub><i>j</i></sub>维度的特征的非中心的协方差成正比, 每一个网格位置都可以看作一个独立的样本, 从而获取能够影响其他信息的特征, Gram矩阵能够通过调整ϕ<sub><i>j</i></sub> (<b><i>x</i></b>) 的维度为<i>C</i><sub><i>j</i></sub>×<i>H</i><sub><i>j</i></sub>×<i>W</i><sub><i>j</i></sub>的矩阵<i>ψ</i>, 那么<b><i>G</i></b><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mtext>ϕ</mtext></msubsup></mrow></math></mathml> (<b><i>x</i></b>) =<i>ψ</i><i>ψ</i><sup>T</sup>/<i>C</i><sub><i>j</i></sub><i>H</i><sub><i>j</i></sub><i>W</i><sub><i>j</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="90">3) 风格重建损失<i>l</i><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>y</mtext><mtext>l</mtext><mtext>e</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>。输出图像<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>与模板图像<b><i>y</i></b><sub><i>s</i></sub>的Gram矩阵的弗罗贝尼乌斯范数 (Frobenius norm) 的差的平方, 可以写作:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>y</mtext><mtext>l</mtext><mtext>e</mtext></mrow><mrow><mtext>ϕ</mtext><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msubsup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo>, </mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">G</mi><msubsup><mrow></mrow><mi>j</mi><mtext>ϕ</mtext></msubsup><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">G</mi><msubsup><mrow></mrow><mi>j</mi><mtext>ϕ</mtext></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">即使<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>与<b><i>y</i></b><sub><i>s</i></sub>的尺寸不同, 风格重建损失<i>l</i><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>y</mtext><mtext>l</mtext><mtext>e</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>仍然能够被很好地定义, 因为它们的Gram矩阵的形状都是相同的。优化风格重建损失<i>l</i><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>y</mtext><mtext>l</mtext><mtext>e</mtext></mrow><mtext>ϕ</mtext></msubsup></mrow></math></mathml>能够保留模板图像<b><i>y</i></b><sub><i>s</i></sub>的风格化特征, 但是没有保留它的空间结构信息, 为了在一系列层<i>J</i>中而不是单层<i>j</i>中执行风格重建, 定义<i>l</i><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>y</mtext><mtext>l</mtext><mtext>e</mtext></mrow><mrow><mtext>ϕ</mtext><mo>, </mo><mi>J</mi></mrow></msubsup></mrow></math></mathml> (<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>, <b><i>y</i></b><sub><i>s</i></sub>) 为每一层<i>j</i>∈<i>J</i>的损失之和。</p>
                </div>
                <h3 id="100" name="100" class="anchor-tag">2 聚焦卷积神经网络</h3>
                <div class="p1">
                    <p id="101">由于划痕种类、长度、宽度、形状等的多样性使得传统划痕检测方法适应性比较差, 因此利用卷积神经网络来进行识别, 因为卷积神经网络是一种学习的方式, 在训练样本数量足够大、质量足够高、种类足够多的情况下, 能够设计足够多层数的带有大量参数的卷积神经网络来自动学习到具有代表性的划痕的本质特征, 凭借网络中大量激活函数的使用, 也提升了泛化能力, 使得算法对于划痕识别的适应性大幅度增强。本文在8层卷积核大小都为3×3的卷积层以及3层全连接层的<i>VGG</i>11<citation id="233" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>网络模型上进行改进, 提出了基于聚焦结构的聚焦卷积神经网络来进一步解决如图2工业环境下产生的短长度碎屑对于划痕识别的误报问题。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907040_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 短长度碎屑" src="Detail/GetImg?filename=images/JSJY201907040_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 短长度碎屑  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907040_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Debris of short length</i></p>

                </div>
                <h4 class="anchor-tag" id="103" name="103">2.1 <b>聚焦结构</b></h4>
                <div class="p1">
                    <p id="104">图像聚焦表现为焦点区域分辨率增大变得更加清晰, 非焦点区域分辨率减小变得模糊。因为聚焦卷积神经网络能更好地提取识别形态、大小、对比度各异的划痕特征, 基于聚焦结构<i>VGG</i>11网络结构进行改进提出了聚焦卷积神经网络。</p>
                </div>
                <div class="p1">
                    <p id="105">聚焦结构中的卷积核使用如图3所示的空洞卷积<citation id="234" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, <i>d</i>代表扩张率, <i>F</i><sub><i>d</i></sub>表示利用卷积核内部实值的间距, 空洞卷积的感受野范围与扩张率<i>d</i>的关系可以写作:</p>
                </div>
                <div class="p1">
                    <p id="106"><i>F</i><sub><i>d</i></sub>= (2<sup><i>d</i>+1</sup>-1) × (2<sup><i>d</i>+1</sup>-1)      (5) </p>
                </div>
                <div class="p1">
                    <p id="107">如图3 (a) 为标准卷积核, 其扩张率为1, 感受野范围为3×3, 图3 (c) 中扩张率为3, 其感受野范围为15×15。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907040_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 空洞卷积" src="Detail/GetImg?filename=images/JSJY201907040_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 空洞卷积  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907040_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Dilated convolution</p>

                </div>
                <div class="p1">
                    <p id="109">聚焦结构由如图4所示的聚焦块和对焦块组成。聚焦结构的主要设计思想在于利用不同扩张率的空洞稀疏卷积结构来聚焦到一个多尺度高度相关的单元中来近似表达一个优化的局部稀疏结构, 好处在于计算复杂度未达到计算爆炸时, 能够增加网络的宽度和深度, 增加网络的表达能力, 即类似于在图像尺寸大小不变的情况下, 图像中非焦点区域的分辨率减小转移至焦点区域, 使焦点区域内分辨率增加进而提高目标特征显著度及辨识度。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907040_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 聚焦结构" src="Detail/GetImg?filename=images/JSJY201907040_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 聚焦结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907040_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Focus structure</p>

                </div>
                <div class="p1">
                    <p id="111">聚焦块由扩张率分别为1、2、3的三种空洞卷积对上层相关统计信息卷积得到多尺度聚焦信息, 以便于下一个阶段能够同时提取不同尺寸下的特征, 然后在通过对焦块, 进一步聚焦上一阶段提取到的多尺度聚焦处理结果, 使用扩张率分别为1、2的两种空洞卷积来最终表达一个稠密且压缩的信息形式。为了提升聚焦结构内部以及多层之间的梯度传播能力, 在聚焦块、对焦块以及整个聚焦结构中都加入了与ResNet- 18中典型残差结构相同的快捷连接方式。在每一个卷积结束之后, 都使用了标准化以及非线性激活函数ReLU, 标准化能够去除特征之间的相关性, 使所有特征具有相同的均值和方差, 能有效提高反向传播的效率, 还具有参数正则化的效果;同时结合非线性激活函数ReLU, 能够大幅度提升网络的泛化性能。聚焦卷积神经网络中使用的聚焦结构中的标准化为批标准化 (BN) 。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">2.2 <b>网络结构</b></h4>
                <div class="p1">
                    <p id="113">网络结构主要基于<i>VGG</i>11模型进行构建, 这是一个对于工业环境中产生的短长度碎屑也能很好地识别, 与划痕特征区别开的满足工业条件实时处理的网络框架。</p>
                </div>
                <div class="p1">
                    <p id="114">网络输入为200×200的灰度图像, 对<i>VGG</i>11网络结构, 保留第一层网络结构, 倒数第二层全连接层替换为全局均值池化层 (<i>Global Average Pooling</i>) <citation id="235" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 剩下层都替换为聚焦结构。网络参数如表1所示, 其中<i>Focus</i>1等为聚焦结构, 包含聚焦块 (<i>a</i>) 及对焦块 (<i>b</i>) , <i>Global</i>_<i>avg</i>表示全局均值池化层, 在每一层卷积层之后都使用批标准化 (<i>BN</i>) 以及<i>ReLU</i>非线性激活函数来提升网络的泛化能力以及表达能力, 网络优化方式使用自适应矩估计 (<i>Adaptive moment estimation</i>, <i>Adam</i>) <citation id="236" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>梯度下降优化算法, 设置初始学习率为0.001, 权重衰减为0.000 5, 一阶矩估计的指数衰减率为0.9, 二阶矩估计的指数衰减率为0.999。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表</b>1 <b>聚焦卷积神经网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Parameters of focus convolutional neural network</i></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td rowspan="2"><br />卷积层</td><td rowspan="2">核尺寸</td><td colspan="3"><br />不同扩张率卷积核特征维度</td></tr><tr><td><br />d=1</td><td>d=2</td><td>d=3</td></tr><tr><td><br /><i>Conv</i>1</td><td>3</td><td>96</td><td>/</td><td>/</td></tr><tr><td><br /><i>Focus</i>1 (<i>a</i>) </td><td>3</td><td>64</td><td>16</td><td>16</td></tr><tr><td><br /><i>Focus</i>1 (<i>b</i>) </td><td>3</td><td>96</td><td>64</td><td>/</td></tr><tr><td><br /><i>Focus</i>2 (<i>a</i>) </td><td>3</td><td>96</td><td>32</td><td>32</td></tr><tr><td><br /><i>Focus</i>2 (<i>b</i>) </td><td>3</td><td>160</td><td>96</td><td>/</td></tr><tr><td><br /><i>Focus</i>3 (<i>a</i>) </td><td>3</td><td>128</td><td>64</td><td>64</td></tr><tr><td><br /><i>Focus</i>3 (<i>b</i>) </td><td>3</td><td>384</td><td>128</td><td>/</td></tr><tr><td><br /><i>Focus</i>4 (<i>a</i>) </td><td>3</td><td>256</td><td>128</td><td>128</td></tr><tr><td><br /><i>Focus</i>4 (<i>b</i>) </td><td>3</td><td>512</td><td>256</td><td>/</td></tr><tr><td><br /><i>Global</i>_<i>avg</i></td><td>13</td><td>768</td><td>/</td><td>/</td></tr><tr><td><br /><i>Fc</i><br /> (<i>Full connection</i>) </td><td>1</td><td>2</td><td>/ </td><td>/ </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="116" name="116" class="anchor-tag">3 实验结果及分析</h3>
                <h4 class="anchor-tag" id="117" name="117">3.1 <b>实验设置</b></h4>
                <h4 class="anchor-tag" id="118" name="118">1) 数据集。</h4>
                <div class="p1">
                    <p id="119">使用来自43套不同型号、不同尺寸、不同涂层工艺的服务器金属表面提取的划痕及背景图像作为数据集, 将200×200大小的表面划痕图像作为正样本, 同样大小的背景图像作为负样本。训练集包含11套不同服务器的5 246张金属表面划痕图像和5 502张背景图像, 验证集包含不同于训练集的11套不同服务器的5 134张划痕图像和5 133张背景图像, 测试集包含剩下21套服务器表面随机采样的10 312张划痕图像和10 298张背景图像。对训练集用随机旋转、翻转和裁剪来增加划痕图像数据并随机采样背景图像数据, 经过数据增强后, 训练集包含35 006张划痕图像和35 020张背景图像。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">2) 评估和实施。</h4>
                <div class="p1">
                    <p id="121">对于评价性能指标, 采用准确率 (<i>Accuracy</i>, <i>Acc</i>) 、单幅图像耗时 (<i>Time</i>) 、模型参数 (<i>Size</i>) 、灵敏度 (<i>Sensitivity</i>, <i>Sen</i>) 、特异性 (<i>Specificity</i>, <i>Spec</i>) , 其中划痕漏报率 (<i>Missing Report Rate</i>, <i>MRR</i>) 为灵敏度, 背景误报率 (<i>False Alarm Rate</i>, <i>FAR</i>) 为特异性。</p>
                </div>
                <div class="p1">
                    <p id="122">对于不同卷积神经网络模型, 未经过风格迁移网络进行预处理, 在测试集下得到的分类结果分别表示为<i>VGG</i>11<citation id="237" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、<i>MobileNetV</i>1<citation id="238" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、<i>ShuffleNet</i><citation id="239" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、<i>ResNet</i>18<citation id="240" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>和<i>FocusNet</i>, 而经过风格迁移网络进行预处理, 在测试集下得到的分类结果分别表示为<i>VGG</i>11-<i>Pre</i>、<i>MobileNetV</i>1-<i>Pre</i>、<i>ShuffleNet</i>-<i>Pre</i>、<i>ResNet</i>18-<i>Pre</i>、<i>FocusNet</i>-<i>Pre</i>。</p>
                </div>
                <div class="p1">
                    <p id="123">所有神经网络训练的实验参数都设置为利用<i>Adam</i>梯度下降优化算法, 初始学习率为0.001, 权重衰减为0.000 5, 一阶矩估计的指数衰减率为0.9, 二阶矩估计的指数衰减率为0.999, 批数量为50张, 最大迭代次数为40 000。</p>
                </div>
                <div class="p1">
                    <p id="124">本实验主要是在开源框架<i>PyTorch</i>下实现, 所有实验都在<i>CPU Inter Core i</i>7 8700<i>K</i>、<i>GPU NVIDIA GeForce GTX</i> 1080<i>Ti</i>和16 <i>GB RAM</i>的计算机上进行。</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125">3.2 <b>本文方法与人工设计特征方法的对比实验</b></h4>
                <div class="p1">
                    <p id="126">表2列出了上述传统人工设计特征的方法<citation id="241" type="reference"><link href="180" rel="bibliography" /><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><link href="186" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>, 基于小波分析 (<i>Wavelet Analysis</i>, <i>WA</i>) 的方法、基于剪切波与小波 (<i>Shear Wave and Wavelet</i>, <i>SWW</i>) 的方法、自适应分割 (<i>Adaptive Segmentation</i>, <i>AS</i>) 及全局阈值自适应 (<i>Global Threshold Adaptive</i>, <i>GTA</i>) 的方法, 与本文提出的方法 (<i>FocusNet</i>-<i>Pre</i>) 在测试集上进行比较。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表</b>2 <b>与传统人工设计特征方法的对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Comparison with traditional artificial design feature methods</i></p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br />方法</td><td><i>FAR</i>/%</td><td><i>MRR</i>/%</td><td><i>Acc</i>/%</td><td>运行时间/<i>ms</i></td></tr><tr><td><br /><i>WA</i></td><td>41.67</td><td>20.78</td><td>68.71</td><td>24.30</td></tr><tr><td><br /><i>SWW</i></td><td>28.43</td><td>20.13</td><td>75.74</td><td>62.50</td></tr><tr><td><br /><i>AS</i></td><td>22.08</td><td>15.26</td><td>82.79</td><td>34.70</td></tr><tr><td><br /><i>GTA</i></td><td>9.88</td><td>14.71</td><td>89.63</td><td>695.32</td></tr><tr><td><br /><i>FocusNet</i>-<i>Pre</i></td><td>0.55</td><td>8.54</td><td>96.66</td><td>1.90</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="128">可以看出本文方法比传统人工设计特征方法在各类评价指标上的表现都要更好, 具体来说, 本文方法相比人工设计特征方法中的最好结果, 误报率降低了9.33个百分点, 漏报率降低了6.17个百分点, 准确率提升了7.03个百分点, 并且运行时间大幅度降低, 单张图像效率高了12倍左右, 仅需要1.90 <i>ms</i>。这表明深度卷积特征比低级的人工特征描述符具有更加强大的表达能力、泛化性能, 各方面大幅度超越传统人工设计特征方法, 能够很好地提取并识别复杂背景下亮度不均的形态、大小、种类各异的表面划痕。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">3.3 <b>风格迁移网络效果论证实验</b></h4>
                <div class="p1">
                    <p id="130">为了验证风格迁移网络能够有效解决划痕识别中遇到的亮度不均、复杂背景的情况, 因此在不同深度神经网络模型下进行是否利用风格迁移网络对待检输入图像进行预处理的实验, 来观察风格迁移网络的作用。图5中展示风格迁移网络对于输入图像利用单一背景且亮度均匀的模板图像进行预处理的效果。</p>
                </div>
                <div class="p1">
                    <p id="131">表3列出了本文提出聚焦卷积神经网络 (<i>FocusNet</i>) 及其他神经网络模型在是否利用风格迁移网络进行预处理, 并在测试集得到的分类对比结果。可以看出, 在不同的网络架构中, 没有经过风格迁移网络进行预处理的分类表现比经过风格迁移网络进行预处理的分类结果表现都要差。具体来说, 在<i>VGG</i>11前经过风格迁移网络预处理后, 漏报率降低了5.46个百分点, 准确率提高了2.24个百分点;在<i>MobileNetV</i>1中, 漏报率降低了4.83个百分点, 准确率提高了2.21个百分点;在<i>ShuffleNet</i>中, 漏报率降低了5.80个百分点, 准确率提高了2.20个百分点;在<i>ResNet</i>18中, 漏报率降低了5.20个百分点, 准确率提高了2.10个百分点;在本文方法<i>FocusNet</i>中, 漏报率降低了5.03个百分点, 准确率提高了2.45个百分点。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907040_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 风格迁移网络处理效果" src="Detail/GetImg?filename=images/JSJY201907040_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 风格迁移网络处理效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907040_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Style transfer network effect</i></p>

                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表</b>3 <b>不同方法的实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Experimental results comparison of different methods</i></p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td><br />方法</td><td><i>FAR</i>/%</td><td><i>MRR</i>/%</td><td><i>Acc</i>/%</td></tr><tr><td><br /><i>VGG</i>11</td><td>0.71</td><td>16.67</td><td>92.34</td></tr><tr><td><br /><i>MobileNetV</i>1</td><td>0.62</td><td>15.46</td><td>93.02</td></tr><tr><td><br /><i>ShuffleNet</i></td><td>0.53</td><td>15.02</td><td>93.35</td></tr><tr><td><br /><i>ResNet</i>18</td><td>0.51</td><td>13.89</td><td>93.93</td></tr><tr><td><br /><i>FocusNet</i></td><td>0.46</td><td>13.57</td><td>94.21</td></tr><tr><td><br /><i>VGG</i>11-<i>Pre</i></td><td>0.69</td><td>11.21</td><td>94.58</td></tr><tr><td><br /><i>MobileNetV</i>1-<i>Pre</i></td><td>0.61</td><td>10.63</td><td>95.23</td></tr><tr><td><br /><i>ShuffleNet</i>-<i>Pre</i></td><td>0.51</td><td>9.22</td><td>95.55</td></tr><tr><td><br /><i>ResNet</i>18-<i>Pre</i></td><td>0.49</td><td>8.69</td><td>96.03</td></tr><tr><td><br /><i>FocusNet</i>-<i>Pre</i></td><td>0.44</td><td>8.54</td><td>96.66</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="134">图6表示不同卷积神经网络模型在训练过程中的损失对比, 图中<i>VGG</i>11、<i>MobileNetV</i>1、<i>ShuffleNet</i>、<i>ResNet</i>18、<i>FocusNet</i>代表没有经过风格迁移网络预处理的表现结果, <i>VGG</i>11-<i>Pre</i>、<i>MobileNetV</i>1-<i>Pre</i>、<i>ShuffleNet</i>-<i>Pre</i>、<i>ResNet</i>18-<i>Pre</i>、<i>FocusNet</i>-<i>Pre</i>表示经过风格迁移网络进行预处理的表现效果。可以看出各类卷积神经网络的表现差异不大, 但是相比没有经过风格迁移网络进行预处理的训练收敛曲线, 经过风格迁移网络预处理后的训练收敛速度更快, 收敛曲线更加平滑。由此说明, 综上所示, 不同的神经网络模型, 经过风格迁移网络预处理后能够有效增加分类结果的准确性, 并且减小过拟合程度, 提升网络模型的泛化性能。</p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907040_135.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同网络模型的训练损失" src="Detail/GetImg?filename=images/JSJY201907040_135.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同网络模型的训练损失  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907040_135.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Training loss of different network models</i></p>

                </div>
                <h4 class="anchor-tag" id="136" name="136">3.4 <b>本文方法与其他卷积神经网络方法的对比实验</b></h4>
                <div class="p1">
                    <p id="137">为了验证本文提出的聚焦卷积神经网络 (<i>FocusNet</i>) 能够更加有效地提取待检图像中划痕特征以及相比其他神经网络模型对于表面划痕检测更加有效, 将不同的神经网络模型在相同训练集下经过相同风格迁移网络预处理后进行训练, 并将测试集得到的各种评价分类结果中的指标进行对比。从表4实验结果可以看到, <i>FocueNet</i>-<i>Pre</i>提取到的深层特征优于<i>VGG</i>11-<i>Pre</i>、<i>MobileNetV</i>1-<i>Pre</i>和<i>ShuffleNet</i>-<i>Pre</i>中提取的深层特征, 与<i>ResNet</i>18-<i>Pre</i>差异不大, 但模型参数减少了67%左右, 单幅图像耗时减少了63.46%, 漏报率仅为8.54%, 误报率为0.44%, 同时准确率高达96.66%, 说明聚焦卷积神经网络不仅能够识别不同种类、长度、宽度、形状等的划痕, 并且具有较好的鲁棒性和泛化性能;同时误报率也有所降低并且模型运行时间短, 单张图像耗时仅仅需要1.90 <i>ms</i>, 完全满足工业生产线上的实时性的要求。</p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表</b>4 <b>不同卷积神经网络的对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 4 <i>Comparison results of different convolutional neural networks</i></p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td><br />网络</td><td><i>FAR</i>/%</td><td><i>MRR</i>/%</td><td><i>Acc</i>/%</td><td>耗时/<i>ms</i></td><td><i>Size</i>/<i>MB</i></td></tr><tr><td><i>VGG</i>11-<i>Pre</i></td><td>0.69</td><td>11.21</td><td>94.58</td><td>1.90</td><td>74.10</td></tr><tr><td><br /><i>MobileNetV</i>1-<i>Pre</i></td><td>0.61</td><td>10.63</td><td>95.23</td><td>2.40</td><td>25.80</td></tr><tr><td><br /><i>ShuffleNet</i>-<i>Pre</i></td><td>0.51</td><td>9.22</td><td>95.55</td><td>4.00</td><td>7.20</td></tr><tr><td><br /><i>ResNet</i>18-<i>Pre</i></td><td>0.49</td><td>8.69</td><td>96.03</td><td>5.20</td><td>89.40</td></tr><tr><td><br /><i>FocusNet</i>-<i>Pre</i></td><td>0.44</td><td>8.54</td><td>96.66</td><td>1.90</td><td>29.50</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="139" name="139" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="140">1) 提出基于深度神经网络的表面划痕识别方法, 该方法一方面将亮度不均、复杂背景的待检图像通过风格迁移网络进行亮度均匀化及背景单一化, 同时保留完整的划痕特征;另一方面利用本文提出的聚焦卷积神经网络实现表面划痕深度特征提取并识别, 能够鲁棒地识别形态、大小、种类各异的表面划痕。</p>
                </div>
                <div class="p1">
                    <p id="141">2) 通过实验证明了所提算法在解决亮度不均问题同时生成保留完整的划痕特征图像和有效地识别不同形态、大小的表面划痕方面具有更好的效果。另外, 实验数据表明, 基于所提算法在不同深度卷积神经网络模型对复杂背景下的划痕均有不错的识别效果, 进一步证明了该算法的有效性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="170">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201409002&amp;v=Mjc0MjBMejdCYmJHNEg5WE1wbzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1U3clA=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 宋迪, 张东波, 刘霞.基于Gabor和纹理抑制的手机配件划痕检测[J].计算机工程, 2014, 40 (9) :1-5. (SONG D, ZHANG D B, LIU X.Scratch detection for mobile phone accessories based on Gabor and texture suppression[J].Computer Engineering, 2014, 40 (9) :1-5.) 
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBX201204024&amp;v=MjIzMDZxZlp1WnNGeS9nVTdyUExTakpkckc0SDlQTXE0OUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 韩芳芳, 段发阶, 张宝峰, 等.单线阵CCD系统的表面凹坑缺陷检测方法[J].哈尔滨工业大学学报, 2012, 44 (4) :116-120. (HAN F F, DUAN F J, ZHANG B F, et al.Study and modeling for surface pit defect detection based on linear array CCD system[J].Journal of Harbin Institute of Technology, 2012, 44 (4) :116-120.) 
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MJGY201709004&amp;v=MDc1NDl1WnNGeS9nVTdyUEtDZk1kN0c0SDliTXBvOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 崔炽标, 李阳, 毛霆, 等.基于预处理与卷积神经网络的塑件划痕检测[J].模具工业, 2017, 43 (9) :1-6. (CUI Z B, LI Y, MAO T, et al.Scratch detection of plastics based on preprocessing and convolutional neural network[J].Die and Mould Industry, 2017, 43 (9) :1-6.) 
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201808028&amp;v=MjQ2NjZHNEg5bk1wNDlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1U3clBJalhUYkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 李克斌, 余厚云, 周申江.基于形态学特征的机械零件表面划痕检测[J].光学学报, 2018, 38 (8) :815027- 1-815027- 7. (LI K B, YU H Y, ZHOU S J.Scratch detection for the surface of mechanical parts based on morphological features[J].Acta Optica Sinica, 2018, 38 (8) :815027- 1-815027- 7.) 
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201712002&amp;v=MzAzMTNZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTdyUFB5cmZiTEc0SDliTnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 汤勃, 孔建益, 伍世虔.机器视觉表面缺陷检测综述[J].中国图象图形学报, 2017, 22 (12) :1640-1663. (TANG B, KONG J Y, WU S Q.Review of surface defect detection based on machine vision[J].Journal of Image and Graphics, 2017, 22 (12) :1640-1663.) 
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJS201206024&amp;v=MzA0NTR6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTdyUElqWEJmYkc0SDlQTXFZOUhZSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 胡文瑾, 李战明, 刘仲民.一种基于小波分析的唐卡图像划痕检测[J].光学技术, 2012, 38 (6) :751-755. (HU W J, LI Z M, LIU Z M.Scratch detection algorithm based on wavelet analysis for Thangka image[J].Optical Technique, 2012, 38 (6) :751-755.) 
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JXXB201506019&amp;v=MjA3NThyUEx6WFRiTEc0SDlUTXFZOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 周鹏, 徐科, 刘顺华.基于剪切波和小波特征融合的金属表面缺陷识别方法[J].机械工程学报, 2015, 51 (6) :98-103. (ZHOU P, XU K, LIU S H.Surface defect recognition for metals based on feature fusion of shearlets and wavelets[J].Journal of Mechanical Engineering, 2015, 51 (6) :98-103.) 
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201701032&amp;v=MDA5MTBiTXJvOUdab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTdyUFBEelRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 马云鹏, 李庆武, 何飞佳, 等.金属表面缺陷自适应分割算法[J].仪器仪表学报, 2017, 38 (1) :245-251. (MA Y P, LI Q W, HE F J, et al.Adaptive segmentation algorithm for metal surface defects[J].Chinese Journal of Scientific Instrument, 2017, 38 (1) :245-251.) 
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201711022&amp;v=Mjc5MTNvOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTdyUFBEelRiTEc0SDliTnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 郭皓然, 邵伟, 周阿维, 等.全局阈值自适应的高亮金属表面缺陷识别新方法[J].仪器仪表学报, 2017, 38 (11) :2797-2804. (GUO H R, SHAO W, ZHOU A W, et al.Novel defect recognition method based on adaptive global threshold for highlight metal surface[J].Chinese Journal of Scientific Instrument, 2017, 38 (11) :2797-2804.) 
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 JOHNSON J, ALAHI A, LI F F.Perceptual losses for real-time style transfer and super-resolution[C]// Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:694-711.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning representations by back propagating errors">

                                <b>[11]</b> RUMELHART D E, HINTON G E, WILLIAMS R J.Learning representations by back-propagating errors[J].Nature, 1986, 323 (6088) :533-536.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[12]</b> HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimization methods for large-scale machine learning">

                                <b>[13]</b> BOTTOU, CURTIS F E, NOCEDA J, et al.Optimization methods for large-scale machine learning[J].SIAM Review, 2016, 60 (2) :223-311.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">

                                <b>[14]</b> RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].ArXiv Preprint, 2016, 2016:1511.06434.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep recurrent ResNet for video super-resolution">

                                <b>[15]</b> LIM B, LEE K M.Deep recurrent ResNet for video super-resolution[C]// Proceedings of the 2018 IEEE Conference on Asia-pacific Signal and Information Processing Association Summit.Washington, DC:IEEE Conputer Society, 2018:643-648.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift">

                                <b>[16]</b> IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:448-456.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted Boltzmann machines">

                                <b>[17]</b> NAIR V, HINTON G E.Rectified linear units improve restricted boltzmann machines[C]// Proceedings of the 2010 International Conference on Machine Learning.New York:ACM, 2010:807-814.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[18]</b> SIMONVAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[C]// Proceedings of the 2015 International Conference on Learning.Washington, DC:IEEE Computer Society, 2015:687-699.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">

                                <b>[19]</b> YU F, KOLTUN V.Multi-scale context aggregation by dilated convolutions[C]// Proceedings of the 2016 International Conference on Learning.Washington, DC:IEEE Computer Society, 2016:511-524.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">

                                <b>[20]</b> KINGMA D, BA J.Adam:a method for stochastic optimization[C]// Proceedings of the 2014 International Conference on Learning.Washington, DC:IEEE Computer Society, 2014:248-263.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MobileNets:Efficient Convolutional Neural Networks for Mobile Vision Applications">

                                <b>[21]</b> HOWARD A G, ZHU M, CHEN B, et al.MobileNets:efficient convolutional neural networks for mobile vision applications[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:1056-1065.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ShuffleNet:an extremely efficient convolutional neural network for mobile devices">

                                <b>[22]</b> ZHANG X, ZHOU X, LIN M, et al.ShuffleNet:an extremely efficient convolutional neural network for mobile devices[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:563-572.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907040" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907040&amp;v=MjY0MjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1U3clBMejdCZDdHNEg5ak1xSTlCWklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
