<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136459339658750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910022%26RESULT%3d1%26SIGN%3dfrqkSA5oE0s2aqYiK151PcOBjPk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910022&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910022&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910022&amp;v=MjQ3NjdmWnVac0Z5bmhXcnJBTHo3QmQ3RzRIOWpOcjQ5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="1 基于RT的DA方法 ">1 基于RT的DA方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="1.1 RT&lt;b&gt;的数学原理&lt;/b&gt;">1.1 RT<b>的数学原理</b></a></li>
                                                <li><a href="#70" data-title="1.2 &lt;b&gt;参数变化对&lt;/b&gt;RT&lt;b&gt;的影响&lt;/b&gt;">1.2 <b>参数变化对</b>RT<b>的影响</b></a></li>
                                                <li><a href="#85" data-title="1.3 RT&lt;b&gt;的采样特点&lt;/b&gt;">1.3 RT<b>的采样特点</b></a></li>
                                                <li><a href="#89" data-title="1.4 &lt;b&gt;基于&lt;/b&gt;RT&lt;b&gt;的&lt;/b&gt;DA&lt;b&gt;策略&lt;/b&gt;">1.4 <b>基于</b>RT<b>的</b>DA<b>策略</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="2 &lt;i&gt;AlexNet&lt;/i&gt;模型改进 ">2 <i>AlexNet</i>模型改进</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="2.1 &lt;b&gt;激活函数改进&lt;/b&gt;">2.1 <b>激活函数改进</b></a></li>
                                                <li><a href="#102" data-title="2.2 &lt;b&gt;归一化层改进&lt;/b&gt;">2.2 <b>归一化层改进</b></a></li>
                                                <li><a href="#110" data-title="2.3 &lt;b&gt;改进&lt;/b&gt;AlexNet&lt;b&gt;的模型结构&lt;/b&gt;">2.3 <b>改进</b>AlexNet<b>的模型结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="3 实验过程 ">3 实验过程</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#114" data-title="3.1 &lt;b&gt;实验环境及超参数设置&lt;/b&gt;">3.1 <b>实验环境及超参数设置</b></a></li>
                                                <li><a href="#117" data-title="3.2 &lt;b&gt;数据集处理&lt;/b&gt;">3.2 <b>数据集处理</b></a></li>
                                                <li><a href="#128" data-title="3.3 &lt;i&gt;AlexNet&lt;/i&gt;&lt;b&gt;改进效果&lt;/b&gt;">3.3 <i>AlexNet</i><b>改进效果</b></a></li>
                                                <li><a href="#136" data-title="3.4 &lt;i&gt;RT&lt;/i&gt;&lt;b&gt;的参数设置&lt;/b&gt;">3.4 <i>RT</i><b>的参数设置</b></a></li>
                                                <li><a href="#142" data-title="3.5 &lt;b&gt;实验结果分析&lt;/b&gt;">3.5 <b>实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#152" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="图1 不同&lt;i&gt;maxRadius&lt;/i&gt;值的RT结果">图1 不同<i>maxRadius</i>值的RT结果</a></li>
                                                <li><a href="#82" data-title="图2 不同极点坐标的RT结果">图2 不同极点坐标的RT结果</a></li>
                                                <li><a href="#100" data-title="图3 &lt;i&gt;ReLU&lt;/i&gt;和&lt;i&gt;Leaky ReLU&lt;/i&gt;的函数图像">图3 <i>ReLU</i>和<i>Leaky ReLU</i>的函数图像</a></li>
                                                <li><a href="#112" data-title="图4 改进&lt;i&gt;AlexNet&lt;/i&gt;模型结构">图4 改进<i>AlexNet</i>模型结构</a></li>
                                                <li><a href="#131" data-title="图5 原始&lt;i&gt;AlexNet&lt;/i&gt;及改进&lt;i&gt;AlexNet&lt;/i&gt;的训练损失变化">图5 原始<i>AlexNet</i>及改进<i>AlexNet</i>的训练损失变化</a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;原始&lt;/b&gt;&lt;i&gt;AlexNet&lt;/i&gt;&lt;b&gt;和改进&lt;/b&gt;&lt;i&gt;AlexNet&lt;/i&gt;&lt;b&gt;的准确率对比&lt;/b&gt;"><b>表</b>1 <b>原始</b><i>AlexNet</i><b>和改进</b><i>AlexNet</i><b>的准确率对比</b></a></li>
                                                <li><a href="#141" data-title="图6 极点和&lt;i&gt;maxRadius&lt;/i&gt;对测试集准确率的影响">图6 极点和<i>maxRadius</i>对测试集准确率的影响</a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;两种&lt;/b&gt;&lt;i&gt;DA&lt;/i&gt;&lt;b&gt;方法的准确率对比&lt;/b&gt;"><b>表</b>2 <b>两种</b><i>DA</i><b>方法的准确率对比</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各类别的精确率、召回率和&lt;/b&gt;&lt;i&gt;F&lt;/i&gt;1&lt;b&gt;值&lt;/b&gt;"><b>表</b>3 <b>各类别的精确率、召回率和</b><i>F</i>1<b>值</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;本文方法与其他方法使用数据集及准确率对比&lt;/b&gt;"><b>表</b>4 <b>本文方法与其他方法使用数据集及准确率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="184">


                                    <a id="bibliography_1" title="SHIMIZU D,KANDA M,KODERA Y.Review of recent molecular landscape knowledge of gastric cancer[J].Histology and Histopathology,2018,33(1):11-26." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Review of recent molecular landscape knowledge of gastric cancer">
                                        <b>[1]</b>
                                        SHIMIZU D,KANDA M,KODERA Y.Review of recent molecular landscape knowledge of gastric cancer[J].Histology and Histopathology,2018,33(1):11-26.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_2" title="HIRASAWA T,AOYAMA K,TANIMOTO T,et al.Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images[J].Gastric Cancer,2018,21(4):653-660." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images">
                                        <b>[2]</b>
                                        HIRASAWA T,AOYAMA K,TANIMOTO T,et al.Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images[J].Gastric Cancer,2018,21(4):653-660.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_3" title="NAWAZ W,AHMED S,TAHIR A,et al.Classification of breast cancer histology images using ALEXNET[C]//Proceedings of the15th International Conference Image Analysis and Recognition,LNCS10882.Cham:Springer,2018:869-876." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification of breast cancer histology images using ALEXNET">
                                        <b>[3]</b>
                                        NAWAZ W,AHMED S,TAHIR A,et al.Classification of breast cancer histology images using ALEXNET[C]//Proceedings of the15th International Conference Image Analysis and Recognition,LNCS10882.Cham:Springer,2018:869-876.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_4" title="WANG S,XIE S,CHEN X,et al.Alcoholism identification based on an Alex Net transfer learning model[J].Frontiers in Psychiatry,2019,10:No.205." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Alcoholism identification based on an Alex Net transfer learning model">
                                        <b>[4]</b>
                                        WANG S,XIE S,CHEN X,et al.Alcoholism identification based on an Alex Net transfer learning model[J].Frontiers in Psychiatry,2019,10:No.205.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_5" title="刘飞,张俊然,杨豪.基于深度学习的糖尿病患者的分类识别[J].计算机应用,2018,38(S1):39-43.(LIU F,ZHANG JR,YANG H.Classification and recognition of diabetes mellitus based on deep learning[J].Journal of Computer Applications,2018,28(S1):39-43.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2018S1011&amp;v=MjM2NTVFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFdyckFMejdCZDdHNEg5bXZybzk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        刘飞,张俊然,杨豪.基于深度学习的糖尿病患者的分类识别[J].计算机应用,2018,38(S1):39-43.(LIU F,ZHANG JR,YANG H.Classification and recognition of diabetes mellitus based on deep learning[J].Journal of Computer Applications,2018,28(S1):39-43.)
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_6" title="胡光亮,王艳,罗勇,等.基于卷积神经网络的鼻咽肿瘤MR图像分割[J].计算机应用,2018,38(S1):208-212.(HU GL,WANG Y,LUO Y,et al.Segmentation of nasopharyngeal nneoplasm MR images based on convolutional neural network[J].Journal of Computer Applications,2018,38(S1):208-212.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2018S1049&amp;v=MTU1NzR6cXFCdEdGckNVUjdxZlp1WnNGeW5oV3JyQUx6N0JkN0c0SDltdnJvOUJiWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        胡光亮,王艳,罗勇,等.基于卷积神经网络的鼻咽肿瘤MR图像分割[J].计算机应用,2018,38(S1):208-212.(HU GL,WANG Y,LUO Y,et al.Segmentation of nasopharyngeal nneoplasm MR images based on convolutional neural network[J].Journal of Computer Applications,2018,38(S1):208-212.)
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_7" title="杜剑,胡炳樑,张周锋.基于卷积神经网络与显微高光谱的胃癌组织分类方法研究[J].光学学报,2018,38(6):267-273.(DU J,HU B L,ZHANG Z F.Gastric carcinoma classification based on convolutional neural network and micro-hyperspectral imaging[J].Acta Optica Sinica,2018,38(6):267-273.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806036&amp;v=MDQwMzFNcVk5R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhXcnJBSWpYVGJMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        杜剑,胡炳樑,张周锋.基于卷积神经网络与显微高光谱的胃癌组织分类方法研究[J].光学学报,2018,38(6):267-273.(DU J,HU B L,ZHANG Z F.Gastric carcinoma classification based on convolutional neural network and micro-hyperspectral imaging[J].Acta Optica Sinica,2018,38(6):267-273.)
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_8" title="张泽中,高敬阳,吕纲,等.基于深度学习的胃癌病理图像分类方法[J].计算机科学,2018,45(S2):263-268.(ZHANGZ Z,GAO J Y,LYU G,et al.Pathological image classification of gastric cancer based on depth learning[J].Computer Science,2018,45(S2):263-268.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2018S2054&amp;v=MTIwNTJGeW5oV3JyQUx6N0JiN0c0SDltdnJZOUFZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        张泽中,高敬阳,吕纲,等.基于深度学习的胃癌病理图像分类方法[J].计算机科学,2018,45(S2):263-268.(ZHANGZ Z,GAO J Y,LYU G,et al.Pathological image classification of gastric cancer based on depth learning[J].Computer Science,2018,45(S2):263-268.)
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_9" title="ALOM M Z,TAHA T M,YAKOPCIC C,et al.The history began from Alex Net:a comprehensive survey on deep learning approaches[EB/OL].[2019-02-08].https://arxiv.org/ftp/arxiv/papers/1803/1803.01164.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The history began from Alex Net:a comprehensive survey on deep learning approaches">
                                        <b>[9]</b>
                                        ALOM M Z,TAHA T M,YAKOPCIC C,et al.The history began from Alex Net:a comprehensive survey on deep learning approaches[EB/OL].[2019-02-08].https://arxiv.org/ftp/arxiv/papers/1803/1803.01164.pdf.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_10" title="PEREZ L,WANG J.The effectiveness of data augmentation in image classification using deep learning[EB/OL].[2019-02-08].https://arxiv.org/pdf/1712.04621.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The effectiveness of data augmentation in image classification using deep learning">
                                        <b>[10]</b>
                                        PEREZ L,WANG J.The effectiveness of data augmentation in image classification using deep learning[EB/OL].[2019-02-08].https://arxiv.org/pdf/1712.04621.pdf.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_11" title="SALEHINEJAD H,VALAEE S,DOWDELL T,et al.Image augmentation using radial transform for training deep neural networks[C]//Proceedings of the 2018 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2018:3016-3020." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image augmentation using radial transform for training deep neural networks">
                                        <b>[11]</b>
                                        SALEHINEJAD H,VALAEE S,DOWDELL T,et al.Image augmentation using radial transform for training deep neural networks[C]//Proceedings of the 2018 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2018:3016-3020.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_12" title="吕鸿蒙,赵地,迟学斌.基于增强Alex Net的深度学习的阿尔茨海默病的早期诊断[J].计算机科学,2017,4(S1):60-70.(LYU H M,ZHAO D,CHI X B.Deep learning for early diagnosis of Alzheimer&#39;s disease based on intensive Alex Net[J].Computer Science,2017,44(S1):60-70.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S1011&amp;v=MjczNTBac0Z5bmhXcnJBTHo3QmI3RzRIOWF2cm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        吕鸿蒙,赵地,迟学斌.基于增强Alex Net的深度学习的阿尔茨海默病的早期诊断[J].计算机科学,2017,4(S1):60-70.(LYU H M,ZHAO D,CHI X B.Deep learning for early diagnosis of Alzheimer&#39;s disease based on intensive Alex Net[J].Computer Science,2017,44(S1):60-70.)
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_13" title="丁蓬莉,李清勇,张振,等.糖尿病性视网膜图像的深度神经网络分类方法[J].计算机应用,2017,37(3):699-704.(DING P L,LI Q Y,ZHANG Z,et al.Diabetic retinal image classification method based on deep neural network[J].Journal of Computer Applications,2017,37(3):699-704.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201703015&amp;v=MDExNTBiTXJJOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oV3JyQUx6N0JkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        丁蓬莉,李清勇,张振,等.糖尿病性视网膜图像的深度神经网络分类方法[J].计算机应用,2017,37(3):699-704.(DING P L,LI Q Y,ZHANG Z,et al.Diabetic retinal image classification method based on deep neural network[J].Journal of Computer Applications,2017,37(3):699-704.)
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_14" title="BJORCK N,GOMES C,SELMAN B,et al.Understanding batch normalization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.02375.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding batch normalization">
                                        <b>[14]</b>
                                        BJORCK N,GOMES C,SELMAN B,et al.Understanding batch normalization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.02375.pdf.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                    WANG S,PHILLIPS P,SUI Y,et al.Classification of Alzheimer&#39;s disease based on eight-layer convolutional neural network with leaky rectified linear unit and max pooling[J].Journal of Medical Systems,2018,42(5):No.85.</a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_16" title="余博,郭雷,赵天云.基于对数极坐标变换的灰度投影稳像算法[J].计算机应用,2008,28(12):3126-3128.(YU B,GUO L,ZHAO T Y.Gray projection image stabilizing algorithm based on log-polar image transform[J].Journal of Computer Applications,2008,28(12):3126-3128.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200812042&amp;v=MTY5NDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFdyckFMejdCZDdHNEh0bk5yWTlCWm9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        余博,郭雷,赵天云.基于对数极坐标变换的灰度投影稳像算法[J].计算机应用,2008,28(12):3126-3128.(YU B,GUO L,ZHAO T Y.Gray projection image stabilizing algorithm based on log-polar image transform[J].Journal of Computer Applications,2008,28(12):3126-3128.)
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_17" title="FU H,CHENG J,XU Y,et al.Joint optic disc and cup segmentation based on multi-label deep network and polar transformation[J].IEEE Transactions on Medical Imaging,2018,37(7):1597-1605." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint optic disc and cup segmentation based on multi-label deep network and polar transformation">
                                        <b>[17]</b>
                                        FU H,CHENG J,XU Y,et al.Joint optic disc and cup segmentation based on multi-label deep network and polar transformation[J].IEEE Transactions on Medical Imaging,2018,37(7):1597-1605.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_18" title="KUMAR D,WONG A,CLAUSI D A.Lung nodule classification using deep features in CT images[C]//Proceedings of the 12th Conference on Computer and Robot Vision.Piscataway:IEEE,2015:133-138." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lung Nodule Classification Using Deep Features in CT Images">
                                        <b>[18]</b>
                                        KUMAR D,WONG A,CLAUSI D A.Lung nodule classification using deep features in CT images[C]//Proceedings of the 12th Conference on Computer and Robot Vision.Piscataway:IEEE,2015:133-138.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_19" title="KRIZHEYSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York:Curran Associates Inc,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[19]</b>
                                        KRIZHEYSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York:Curran Associates Inc,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_20" title="MHASKAR H N,MICCHELLI C A.How to choose an activation function[C]//Proceedings of the 6th International Conference on Neural Information Processing Systems.San Francisco:Morgan Kaufmann Publishers Inc,1993:319-326." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=How to choose an activation function">
                                        <b>[20]</b>
                                        MHASKAR H N,MICCHELLI C A.How to choose an activation function[C]//Proceedings of the 6th International Conference on Neural Information Processing Systems.San Francisco:Morgan Kaufmann Publishers Inc,1993:319-326.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_21" title="XU B,WANG N,CHEN T,et al.Empirical evaluation of rectified activations in convolutional network[EB/OL].[2019-02-08].https://arxiv.org/pdf/1505.00853.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of rectified activations in convolutional network">
                                        <b>[21]</b>
                                        XU B,WANG N,CHEN T,et al.Empirical evaluation of rectified activations in convolutional network[EB/OL].[2019-02-08].https://arxiv.org/pdf/1505.00853.pdf.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_22" title="SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-02-08].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[22]</b>
                                        SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-02-08].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_23" title="JING J,DONG A,LI P,et al.Yarn-dyed fabric defect classification based on convolutional neural network[J].Optical Engineering,2017,56(9):093104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Yarn-dyed fabric defect classification based on convolutional neural network">
                                        <b>[23]</b>
                                        JING J,DONG A,LI P,et al.Yarn-dyed fabric defect classification based on convolutional neural network[J].Optical Engineering,2017,56(9):093104.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                    IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[EB/OL].[2019-01-10].https://arxiv.org/pdf/1502.03167.pdf.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-19 09:03</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2923-2929 DOI:10.11772/j.issn.1001-9081.2019040709            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于径向变换和改进AlexNet的胃肿瘤细胞图像识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%98%E5%B2%9A&amp;code=07526818&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">甘岚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E5%AD%90%E6%B6%B5&amp;code=42897035&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭子涵</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%91%B6&amp;code=30567767&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王瑶</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%9C%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0133870&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华东交通大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>使用AlexNet实现胃肿瘤细胞图像分类时,存在数据集过小和模型收敛速度慢、识别率低的问题。针对上述问题,提出基于径向变换(RT)的数据增强(DA)和改进AlexNet的方法。将原始数据集划分为测试集和训练集,测试集采用剪裁方式增加数据,训练集首先采用剪裁、旋转、翻转和亮度变换得到增强图片集;然后选取其中一部分进行RT处理达到增强效果。此外,采用替换激活函数和归一化层的方式提高AlexNet的收敛速度并提高其泛化性能。实验结果表明,所提方法能以较快的收敛速度和较高的识别准确率实现胃肿瘤细胞图像的识别,在测试集中最高准确率为99.50%,平均准确率为96.69%,癌变、正常和增生三个类别的F1值分别为0.980、0.954和0.958,表明该方法较好地实现了胃肿瘤细胞图像的识别。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%8F%E6%A0%B7%E6%9C%AC%E6%95%B0%E6%8D%AE%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">小样本数据集;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据增强;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BE%84%E5%90%91%E5%8F%98%E6%8D%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">径向变换;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%83%83%E8%82%BF%E7%98%A4%E7%BB%86%E8%83%9E%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胃肿瘤细胞图像识别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    甘岚(1964—),女,江西南昌人,教授,硕士,主要研究方向:模式识别、图像处理;;
                                </span>
                                <span>
                                    *郭子涵(1995—),男,陕西渭南人,硕士研究生,主要研究方向:图像处理、图像识别;电子邮箱bradleyg@foxmail.com;
                                </span>
                                <span>
                                    王瑶(1994—),女,安徽淮南人,硕士研究生,主要研究方向:图像识别。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61861016);</span>
                                <span>江西省教育厅科学技术研究项目(GJJ180317);</span>
                    </p>
            </div>
                    <h1><b>Gastric tumor cell image recognition method based on radial transformation and improved AlexNet</b></h1>
                    <h2>
                    <span>GAN Lan</span>
                    <span>GUO Zihan</span>
                    <span>WANG Yao</span>
            </h2>
                    <h2>
                    <span>School of Information Engineering, East China Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>When using AlexNet to implement image classification of gastric tumor cells, there are problems of small dataset, slow model convergence and low recognition rate. Aiming at the above problems, a Data Augmentation(DA) method based on Radial Transformation(RT) and improved AlexNet was proposed. The original dataset was divided into test set and training set. In the test set, cropping was used to increase the data. In the training set, cropping, rotation, flipping and brightness conversion were employed to obtain the enhanced image set, and then some of them were selected for RT processing to achieve the enhanced effect. In addition, the replacement activation of functions and normalization layers was used to speed up the convergence and improve the generalization performance of AlexNet. Experimental results show that the proposed method can implement the recognition of gastric tumor cell images with faster convergence and higher recognition accuracy. On the test set, the highest accuracy is 99.50% and the average accuracy is 96.69%, and the F1 scores of categories: canceration, normal and hyperplasia are 0.980, 0.954 and 0.958 respectively, indicating that the proposed method can implement the recognition of gastric tumor cell images well.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=small%20dataset&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">small dataset;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Data%20Augmentation(DA)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Data Augmentation(DA);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Radial%20Transformation(RT)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Radial Transformation(RT);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=gastric%20tumor%20cell%20image%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">gastric tumor cell image recognition;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    GAN Lan,born in 1964,M.S.,professor.Her research interests include pattern recognition,image processing.;
                                </span>
                                <span>
                                    GUO Zihan,born in 1995,M.S.candidate.His research interests include image processing,image recognition.;
                                </span>
                                <span>
                                    WANG Yao,born in 1994,M.S.candidate.Her research interests include image recognition.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61861016);</span>
                                <span>the Science and Technology Research Project of Jiangxi Education Department(GJJ180317);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="52">胃癌是世界范围内最常被诊断出的癌症之一,具有预后不良的特点,一个重要原因是大多数患者在癌症已经发展时才被诊断出来<citation id="232" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,及时而精确地判断出胃肿瘤细胞能够改善胃癌的预后,为患者争取宝贵的治疗时间。随着近年来人工智能的快速发展,自动识别胃肿瘤细胞图像的需求日益迫切。</p>
                </div>
                <div class="p1">
                    <p id="53">目前,以卷积神经网络(Convolutional Neural Network, CNN)为代表的深度学习(Deep Learning, DL)算法快速发展,常用来识别特征维数高、背景复杂的医学图像<citation id="235" type="reference"><link href="186" rel="bibliography" /><link href="188" rel="bibliography" /><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><link href="194" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。在胃肿瘤细胞图像的应用上,杜剑等<citation id="233" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>使用CNN,结合高光谱成像与显微系统,实现了癌变和正常组织的精确分类;但是该方法的数据获取较为困难,需要一定的硬件条件才可实现。张泽中等<citation id="234" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>将AlexNet与GoogLeNet结合,使用专业标签过滤后的胃癌病理图像进行训练取得了良好效果,但现实中难以对胃肿瘤细胞图像进行实时过滤。可见,CNN是实现胃肿瘤图像识别的一个有效方法。</p>
                </div>
                <div class="p1">
                    <p id="54">由于DL是数据驱动技术,所以当数据不足时,即使是更深的网络也难以取得更好的效果<citation id="236" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。胃肿瘤细胞图像是典型的小样本数据集,不适合直接输入CNN进行训练。Perez等<citation id="237" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>证明通过剪裁、旋转、翻转等方法可以达到数据增强(Data Augmentation, DA)的目的,将这些传统DA方法组合起来可以进一步提高数据集的多样性,但并不能满足胃肿瘤细胞图像的数据集要求。Salehinejad等<citation id="238" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>针对超小型多模态医学数据集,提出使用径向变换(Radial Transform, RT)进行数据扩增,表明RT在多模态医学数据集上的可行性,由于胃肿瘤细胞图像具有旋转不变性,因此将传统DA方法与RT结合实现其数据扩展不失为一种好方法。但文献<citation id="239" type="reference">[<a class="sup">11</a>]</citation>中并未介绍使用RT的详细DA方法,且多模态医学数据的类与类之间差异明显,而胃肿瘤细胞数据的类与类之间差异较小,将RT应用于胃肿瘤细胞图像的可行性有待进一步研究,并且RT的使用包含两个重要参数,这两个参数的设置如何影响DA的效果也有待分析。</p>
                </div>
                <div class="p1">
                    <p id="55">AlexNet作为经典的CNN,与GoogLeNet、ResNet等更先进的模型相比,具有更简单的网络结构和更少的网络参数。对于不同的医学数据集,AlexNet更方便做出不同的改进,使之具有更好的训练效果<citation id="242" type="reference"><link href="206" rel="bibliography" /><link href="208" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。因此,本文采用AlexNet进行胃肿瘤细胞图像的识别。由于原始AlexNet存在收敛慢和识别率低的问题,而使用批归一化(Batch Normalization, BN)可以有效提高CNN的收敛速度和泛化性能<citation id="240" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,替换激活函数也可以改善CNN的性能表现<citation id="241" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。因此,本文借鉴上述方法改进AlexNet以提高其收敛速度和识别率。</p>
                </div>
                <div class="p1">
                    <p id="56">本文采用AlexNet实现胃肿瘤细胞图像的识别。为解决数据集过小的问题,采用RT与传统DA方法结合的数据扩增方法,并结合理论与实验,确定了RT的最佳参数。此外,采用改变激活函数和归一化层的方法改善了原始AlexNet收敛速度慢的问题。实验结果表明,使用上述方法实现胃肿瘤细胞图像识别时具有良好的分类效果和较快的收敛速度。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">1 基于RT的DA方法</h3>
                <div class="p1">
                    <p id="58">图像的RT也称线性极坐标变换,是指将图像中每个像素的笛卡儿坐标通过极坐标变换映射到新图像中对应像素的笛卡儿坐标的过程。在图像分割任务中,RT被用来消除旋转运动的影响<citation id="243" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。能够消除旋转运动影响的特性令RT十分适用于胃肿瘤细胞图像这类对旋转不敏感的数据,但是本文使用RT的目的并非消除旋转运动的影响,而是利用RT改变图像像素的原始排列,在不改变图像信息的前提下,使新图像具有与原始图像不同的表现形式,通过增加图像的多样性达到数据扩增目的。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.1 RT<b>的数学原理</b></h4>
                <div class="p1">
                    <p id="60">RT本质上是图像点的采样,即通过映射函数使得采样点重新排列形成新图像的过程。</p>
                </div>
                <div class="p1">
                    <p id="61">假设有某二维图像<i>X</i><sub><i>M</i></sub><sub>×</sub><sub><i>N</i></sub>,其内任意一点的极坐标表示为(<i>r</i>,<i>θ</i>)∈<i>P</i><sup>2</sup>,其中径向坐标<i>r</i>为该点到极点的直线距离,<i>θ</i>为该点到极点的连线和水平极轴的逆时针夹角,满足<i>r</i>∈<b>Z</b><sub>+</sub>,<i>θ</i>∈[0,2π],其中<i>P</i><sup>2</sup>表示二维极坐标系。</p>
                </div>
                <div class="p1">
                    <p id="62">在图像<i>X</i>中以笛卡儿坐标系任意取一点<i>O</i>(<i>u</i>,<i>v</i>)为极点,其中<i>u</i>,<i>v</i>∈<b>Z</b><sub>+</sub>。以极点<i>O</i>为起点,从极轴开始作<i>M</i>条射线,第<i>m</i>条射线到图像边缘的长度为<i>R</i><sub><i>m</i></sub>∈<b>R</b><sub>+</sub>,每条相邻射线具有相同的夹角Δ<i>θ</i>=|<i>θ</i><sub><i>m</i></sub><sub>+1</sub>-<i>θ</i><sub><i>m</i></sub>|,然后生成一个以<i>O</i>(<i>u</i>,<i>v</i>)为极点的空间极坐标集合<i>K</i>={(<i>r</i>,<i>θ</i><sub><i>m</i></sub>)<sub><i>k</i></sub>}。对于生成点(<i>r</i>,<i>θ</i><sub><i>m</i></sub>)<sub><i>k</i></sub>∈<i>P</i><sup>2</sup>,<i>r</i>∈[0,<i>R</i><sub><i>m</i></sub>],有:</p>
                </div>
                <div class="p1">
                    <p id="63"><i>θ</i><sub><i>m</i></sub>=2π·<i>m</i>/<i>M</i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="64">其中:<i>m</i>∈[0,<i>M</i>-1]。通过映射<mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><mover><mo stretchy="true" symmetric="false">→</mo><mrow><mi>φ</mi><mo stretchy="false">(</mo><mi>r</mi><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">)</mo><msub><mrow></mrow><mi>k</mi></msub></mrow></mover><mrow><mtext> </mtext><mi>X</mi><mo>^</mo><mspace width="0.25em" /></mrow></mrow></math></mathml>,可以使用映射函数<i>φ</i>(·)将<i>X</i><sub><i>M</i></sub><sub>×</sub><sub><i>N</i></sub>中每个点映射到 <i>X</i>^ <sub><i>M</i></sub><sub>×</sub><sub><i>N</i></sub>中。映射函数<i>φ</i>(·)为:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>α</mi><mo>=</mo><mtext>r</mtext><mtext>o</mtext><mtext>u</mtext><mtext>n</mtext><mtext>d</mtext><mo stretchy="false">(</mo><mi>r</mi><mo>⋅</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>β</mi><mo>=</mo><mtext>r</mtext><mtext>o</mtext><mtext>u</mtext><mtext>n</mtext><mtext>d</mtext><mo stretchy="false">(</mo><mi>r</mi><mo>⋅</mo><mi>sin</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">其中:round()表示取整,且必须满足<i>r</i>∈[0,<i>R</i><sub><i>m</i></sub>],<i>m</i>∈[0,<i>M</i>-1],使得0≤<i>u</i>+<i>α</i>≤<i>M</i>且0≤<i>v</i>+<i>β</i>≤<i>N</i>,从而保证映射点一定在 <i>X</i>^ 内。经过式(2)的变换后,重构图像 <i>X</i>^ 中像素(<i>p</i>,<i>q</i>)定义为  <i><b>f</b></i>^ <sub><i>p</i></sub><sub>,</sub><sub><i>q</i></sub>=<i><b>f</b></i><sub><i>u</i></sub><sub>+</sub><sub><i>α</i></sub><sub>,</sub><sub><i>v</i></sub><sub>+</sub><sub><i>β</i></sub>。图像 <i>X</i>^ 就是图像<i>X</i>相对于极点<i>O</i>(<i>u</i>,<i>v</i>)∈<i>X</i>的RT结果。</p>
                </div>
                <div class="p1">
                    <p id="67">在实际的计算机图像处理算法设计中,RT的转换算法为<i>dst</i>(<i>p</i>,<i>q</i>)=<i>T</i>(<i>src</i>(<i>a</i>, <i>b</i>)),假设使用的极点为<i>O</i>(<i>u</i>,<i>v</i>)∈<i>X</i>,则转换<i>T</i>(·)可由式(3)表示:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ι</mi><mo>=</mo><mo stretchy="false">(</mo><mi>a</mi><mo>-</mo><mi>u</mi><mo>,</mo><mspace width="0.25em" /><mi>b</mi><mo>-</mo><mi>v</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>p</mi><mo>=</mo><mo stretchy="false">(</mo><mi>s</mi><mi>r</mi><mi>c</mi><mo>.</mo><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi><mo>/</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi>R</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>m</mi><mi>a</mi><mi>g</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>u</mi><mi>d</mi><mi>e</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>q</mi><mo>=</mo><mo stretchy="false">(</mo><mi>s</mi><mi>r</mi><mi>c</mi><mo>.</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>/</mo><mo stretchy="false">(</mo><mn>2</mn><mtext>π</mtext><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>⋅</mo><mi>a</mi><mi>n</mi><mi>g</mi><mi>l</mi><mi>e</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中:<i>src</i>.<i>width</i>和<i>src</i>.<i>height</i>分别表示原图的宽度和高度;<i>magnitude</i>()表示计算矢量的幅值;<i>angle</i>()表示计算矢量与极轴的夹角。需要说明的是,<i>maxRadius</i>对应式(2)中的<i>r</i>,由于计算<i>R</i><sub><i>m</i></sub>需要大量资源,所以通常将式(2)中的条件<i>r</i>∈[0,<i>R</i><sub><i>m</i></sub>]改为<i>r</i>∈[0,<i>maxRadius</i>],其中<i>maxRadius</i>是一个可由人工设置的定值。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">1.2 <b>参数变化对</b>RT<b>的影响</b></h4>
                <div class="p1">
                    <p id="71">根据1.1节介绍,<i>RT</i>涉及两个主要参数:极点坐标和<i>maxRadius</i>。两个参数的组成分为两种情况:1)极点固定时<i>maxRadius</i>发生变化;2)<i>maxRadius</i>固定时极点发生变化。极点与<i>maxRadius</i>变化时可能会令采样点超出图像边界,导致映射图片出现像素缺失,由于缺失像素的默认值为0,因此在重构图像中出现会黑色区域,本文将这一现象称为超界缺失。超界缺失越严重黑色区域越多,这代表重构图像中含有原图特征的图像面积越小。下面将结合超界缺失分析两个参数变化对RT结果的影响。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">1.2.1 <i>maxRadius</i>对RT的影响</h4>
                <div class="p1">
                    <p id="73">在本节中,默认图像大小为220×220,为了方便研究maxRadius对<i>RT</i>的影响,采用控制变量法,使极点坐标固定在图像的中心点(110, 110),原图如图1(<i>a</i>)所示。</p>
                </div>
                <div class="p1">
                    <p id="74">当<i>maxRadius</i>=110时,根据1.1节的分析,普遍存在<i>R</i><sub><i>m</i></sub>≥<i>maxRadius</i>,此时射线上的部分点无法被全部采样,RT会丢失小部分的图像信息。根据式(3),因为<i>maxRadius</i>=110,使得<i>src</i>.<i>width</i>/<i>maxRadius</i>=2,此时原图的采样点刚好能够填充重构图像,不会出现超界缺失。因此以<i>maxRadius</i>=110为基准,设其为标准采样,如图1(c)所示。</p>
                </div>
                <div class="p1">
                    <p id="75">当<i>maxRadius</i>&lt;110时,根据1.1节的分析,普遍存在<i>R</i><sub><i>m</i></sub>&gt;<i>maxRadius</i>,此时射线上的点均无法被全部采样,从而丢失部分图像信息。根据式(3),因为<i>maxRadius</i>&lt;110,会使得<i>src</i>.<i>width</i>/<i>maxRadius</i>&gt;2,与标准采样相比,此时有效的采样点将被上采样,即图像被整体放大,如图1(b)所示。</p>
                </div>
                <div class="p1">
                    <p id="76">当<i>maxRadius</i>&gt;110时,根据1.1节的分析,普遍存在<i>R</i><sub><i>m</i></sub>&lt;<i>maxRadius</i>,此时对于<i>r</i>∈[0,<i>R</i><sub><i>m</i></sub>]的采样点为图像内的所有点,图像信息被全部提取,而对于<i>r</i>∈[<i>R</i><sub><i>m</i></sub>,<i>maxRadius</i>]的采样点,其超出了图像边界,导致超界缺失。根据式(3),因为<i>maxRadius</i>&gt;110,使得<i>src</i>.<i>width</i>/<i>maxRadius</i>&lt;2,与标准采样相比,有效的采样点被下采样,即图像被整体缩小,如图1(d)所示。</p>
                </div>
                <div class="p1">
                    <p id="77">综上所述,若想提取全部的图像信息,必须有<i>R</i><sub><i>m</i></sub>≤<i>maxRadius</i>,但是此时一定会出现超界缺失;若想不出现超界缺失,必须有<i>R</i><sub><i>m</i></sub>≥<i>maxRadius</i>,但是此时一定会出现信息丢失。因此,在使用固定极点的RT时,在超界缺失和信息丢失之间必须有所权衡。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910022_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同maxRadius值的RT结果" src="Detail/GetImg?filename=images/JSJY201910022_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 不同<i>maxRadius</i>值的RT结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910022_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Results of RT with different <i>maxRadius</i> values</p>

                </div>
                <h4 class="anchor-tag" id="79" name="79">1.2.2 极点位置对RT的影响</h4>
                <div class="p1">
                    <p id="80">与1.2.1节相似,本节令<i>maxRadius</i>恒为110,以研究极点位置对RT的影响,原图如图2(a)所示。</p>
                </div>
                <div class="p1">
                    <p id="81">当极点位于图像中心,即坐标为(110, 110)时,根据1.2.1节的分析,此时刚好不会出现超界缺失,但会丢失小部分的图像信息,如图2(b)所示。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910022_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同极点坐标的RT结果" src="Detail/GetImg?filename=images/JSJY201910022_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同极点坐标的RT结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910022_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Results of RT with different polar coordinates</p>

                </div>
                <div class="p1">
                    <p id="83">当极点偏离图像中心,接近图像边缘时,相对极点位于图像边缘区域的射线,即<i>R</i><sub><i>m</i></sub>较小的射线,存在<i>R</i><sub><i>m</i></sub>&lt;<i>maxRadius</i>,此时对于<i>r</i>∈[<i>R</i><sub><i>m</i></sub>,<i>maxRadius</i>]的采样点,其超出了图像边界,导致超界缺失。而当极点越接近图像边缘,满足<i>R</i><sub><i>m</i></sub>&lt;<i>maxRadius</i>的射线就越多,超界缺失就越严重,如图2(c)、(d)所示。此外,当极点接近图像边缘时,由于<i>maxRadius</i>固定,RT能覆盖的采样点也越来越少,这会导致更多的图像信息丢失,极点越接近图像边缘,信息丢失越严重,由图2(d)可看出明显的信息丢失。</p>
                </div>
                <div class="p1">
                    <p id="84">综上所述,极点坐标如果偏离图像中心,超界缺失及信息丢失会同时出现,且随着偏离程度的增大,超界缺失及信息丢失的程度也越严重,因此,在使用RT进行数据扩增时,应尽量使极点位于图像的中心点。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">1.3 RT<b>的采样特点</b></h4>
                <div class="p1">
                    <p id="86">根据式(1)易知,<i>θ</i><sub><i>m</i></sub>变化的范围是[0,2π]。根据式(3),当<i>θ</i><sub><i>m</i></sub>从0变化至2π时,<i>q</i>值也将从0变化至<i>src</i>.<i>height</i>,而<i>p</i>值保持不变。因此,在原图像上以极点<i>O</i>(<i>u</i>,<i>v</i>)∈<i><b>X</b></i>为圆心的圆,经过RT后,在新图像上会映射为一条竖直的直线。由于<i>src</i>.<i>height</i>是定值,而圆的周长会随着<i>r</i>的变化而变化,易知当<i>r</i>较小时,圆的周长也较小,经过RT其长度会被拉伸到<i>src</i>.<i>height</i>;当<i>r</i>较大时,圆的周长也较长,经过RT其长度会被压缩到<i>src</i>.<i>height</i>。这样的变化会使得极点的近邻点被扩展,极点的远邻点被压缩,由此可得RT的不均匀采样特点:对于极点<i>O</i>(<i>u</i>,<i>v</i>)∈<i>X</i>,其近邻点将会被上采样,远邻点将会被下采样。</p>
                </div>
                <div class="p1">
                    <p id="87">由于这种不均匀采样的性质,原图中的细胞组织经过RT会产生形变,使得原来的细胞组织具有不同的表现形式,而这一过程并未产生新的图像特征,对比图2(a)(b)可以看出这一变化。同样地,因为不均匀采样的特点,这些不同的表现形式仅能作用于与极点有一定距离的环状区域。也即是说,经过RT处理的图像仅能准确地反映原图中一部分的特征信息,本文认为,在训练过程中,这一效果与图像放大、剪裁的DA方式相似。</p>
                </div>
                <div class="p1">
                    <p id="88">此外,结合本节的分析可知,在原图中需要用非线性阈值划分的区域可以在RT后使用线性阈值划分,这表明RT不仅可以帮助增强图像,还可以降低图像中分类问题的复杂性,因此RT也常用于实现图像分割任务<citation id="244" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,本文仅讨论RT的DA能力。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">1.4 <b>基于</b>RT<b>的</b>DA<b>策略</b></h4>
                <div class="p1">
                    <p id="90">根据1.3节的分析,<i>RT</i>的不均匀采样特点使其具有<i>DA</i>能力,但是下采样造成的信息丢失会导致<i>CNN</i>的分类能力下降<citation id="245" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,所以单独使用<i>RT</i>实现<i>DA</i>的效果较差。而传统的<i>DA</i>方法如剪裁、旋转、亮度变换不会因为采样不均匀导致信息丢失。因此使用<i>RT</i>实现<i>DA</i>时,应该采用<i>RT</i>和传统<i>DA</i>方法同时使用的策略。</p>
                </div>
                <div class="p1">
                    <p id="91">更详细地,对于训练集,某张图片的增强图片集应分为两个部分:一部分为使用<i>RT</i>处理的数据,该部分保证图像表现的多样性;另一部分为仅使用传统<i>DA</i>方法处理的数据,该部分在增加多样性的基础上保证图像信息的完整性,以弥补<i>RT</i>带来的信息丢失问题。</p>
                </div>
                <div class="p1">
                    <p id="92">对于测试集,不宜使用<i>RT</i>进行处理。在1.3节的分析中,<i>RT</i>处理后的图像仅能准确地反映原图中一部分的特征信息,因为单张<i>RT</i>图像包含的信息相当有限,而胃肿瘤细胞图像的类与类之间差距很小,这些有限的信息可能不包含分类胃肿瘤细胞图像所需的关键特征信息,所以在测试集中使用<i>RT</i>进行处理难以取得好的测试结果,前期实验也表明对测试集进行<i>RT</i>处理会降低模型的识别准确率。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">2 <i>AlexNet</i>模型改进</h3>
                <div class="p1">
                    <p id="94">2012年<i>Alex</i>等<citation id="246" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了<i>AlexNet</i>,并因为它出色的分类能力获得了2012年<i>ImageNet LSVRC</i>比赛的冠军,具有结构简单、参数少的特点。虽然目前已有如<i>GoogLeNet</i>、<i>ResNet</i>等更先进的网络模型,但是有实验结果表明,对于小型的多模态医学图像数据集,采用<i>RT</i>作为<i>DA</i>手段时,<i>GoogLeNet</i>与<i>AlexNet</i>的分类效果十分相近,对于某些类别,<i>AlexNet</i>的识别率甚至优于<i>GoogLeNet</i><citation id="247" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。而对于<i>ResNet</i>等更复杂的网络结构,数据集过小时,其海量参数更容易导致过拟合。本文使用的胃肿瘤细胞图像数据集是典型的小样本数据集,<i>AlexNet</i>足以胜任该分类任务,因此选取<i>AlexNet</i>作为参考模型。</p>
                </div>
                <div class="p1">
                    <p id="95">为了使<i>AlexNet</i>有更好的训练效果,尝试在使用胃肿瘤细胞图像数据时改进其网络结构。有研究指出,原始<i>AlexNet</i>的第三层和第四层卷积层对<i>MRI</i>等医学图像的特征提取能力最强<citation id="248" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,但是在这两层的改动效果并不理想,对其他层的改动同样会导致网络的泛化能力下降,所以本文转而在<i>AlexNet</i>的各个模块中寻求改进方法。在前期实验中,发现原始<i>AlexNet</i>的收敛速度很慢,训练非常耗时;同时原始<i>AlexNet</i>在胃肿瘤细胞图像数据上的识别率很低。因此,对<i>AlexNet</i>进行改进的主要目的是提高收敛速度、提高识别率。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">2.1 <b>激活函数改进</b></h4>
                <div class="p1">
                    <p id="97">激活函数是神经网络的重要组成部分,通过激活函数,神经网络才具备了分层的非线性映射能力。除了非线性映射的能力以外,更加平滑的激活函数还具有更快的逼近速率<citation id="249" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。因此,使用更好的激活函数能够提高模型的收敛速度,同时提高模型的泛化性能。</p>
                </div>
                <div class="p1">
                    <p id="98">原始<i>AlexNet</i>使用修正线性单元(<i>Rectified Linear Unit</i>, <i>ReLU</i>)作为激活函数,当<i>ReLU</i>被激活至0以上时,其偏导恒为1,这解决了<i>Sigmoid</i>激活函数输入过大时带来的梯度消失问题,然而<i>ReLU</i>在输入为负值时其偏导恒为0,此时出现完全饱和,这可能导致神经元永远不会被激活,在使用基于梯度的优化算法时,这一缺点会使网络的训练速度变慢,甚至降低网络的泛化性能,<i>ReLU</i>的函数图像如图3(<i>a</i>)所示。</p>
                </div>
                <div class="p1">
                    <p id="99"><i>Leaky ReLU</i>与<i>ReLU</i>激活函数的不同之处在于输入为负值时其偏导不为0,这解决了<i>ReLU</i>可能出现神经元不被激活的情况,使得网络的训练速度更快、效果更好,所以使用<i>Leaky ReLU</i>代替<i>ReLU</i>激活函数,<i>Leaky ReLU</i>的函数图像如图3(<i>b</i>)所示。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910022_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 ReLU和Leaky ReLU的函数图像" src="Detail/GetImg?filename=images/JSJY201910022_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>ReLU</i>和<i>Leaky ReLU</i>的函数图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910022_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.3 <i>Function graphs of ReLU and Leaky ReLU</i></p>

                </div>
                <div class="p1">
                    <p id="101">根据图3(<i>b</i>),<i>Leaky ReLU</i>输入为负值时其函数表示为<i><b>y</b></i><sub><i>i</i></sub>=<i>a</i><sub><i>i</i></sub><i><b>x</b></i><sub><i>i</i></sub>,而<i>a</i><sub><i>i</i></sub>的值与激活函数的表现有较大关联,当<i>a</i><sub><i>i</i></sub>过小时,Leaky ReLU和ReLU的效果几乎相等<citation id="250" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>;可以预见地,当<i>a</i><sub><i>i</i></sub>过大时,对于负值输入,较大的梯度将使权重结果偏向正值,这将使模型训练不稳定,增加模型的训练时间。在TensorFlow 1.10.0版本中,<i>a</i><sub><i>i</i></sub>的默认值为0.2,使用该值时效果良好,无需改动,所以本文实验中<i>a</i><sub><i>i</i></sub>的值均设置为0.2。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">2.2 <b>归一化层改进</b></h4>
                <div class="p1">
                    <p id="103">在神经网络中,通常要对输入数据进行归一化处理,在隐含层输入之前的归一化操作称之为归一化层。归一化层可以将不同参量去量纲化,缩小数值差别,并使网络快速收敛。因此,通过改进归一化层可以达到提高网络收敛速度的目的。</p>
                </div>
                <div class="p1">
                    <p id="104">原始<i>AlexNet</i>中使用了局部响应归一化(<i>Local Response Normalization</i>, <i>LRN</i>)层对第一、二层进行归一化操作,以增强模型的泛化性能,但是<i>LRN</i>层对模型的实际改善效果有限<citation id="251" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>,还会极大增加模型的训练时间。使用<i>BN</i>层替换<i>LRN</i>层可以提高计算效率的同时提高分类准确性<citation id="252" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>,此外,使用<i>BN</i>层可以放宽网络的初始化条件,主要体现在可以使用更大的初始学习率<citation id="253" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="105"><i>BN</i>层与<i>LRN</i>层相似,也是一种归一化算法。对于深度学习这类包含很多隐含层的网络结构,训练过程中各层参数都在不停变化,各层的输入分布经过处理也在不停变化,由于训练期间网络参数变化导致的网络激活分布的变化称为协变量偏移<citation id="254" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。<i>BN</i>层通过将每层的输入分布变换到均值为0、方差为1的正态分布来解决协变量偏移的问题。对于小批量梯度下降(<i>Mini</i>-<i>Batch Gradient Descent</i>, <i>MBGD</i>),设每层输入为<i><b>x</b></i>=(<i><b>x</b></i><sup>(1)</sup>,<i><b>x</b></i><sup>(2)</sup>,…,<i><b>x</b></i><sup>(</sup><sup><i>d</i></sup><sup>)</sup>),BN的具体操作就是对输入<i><b>x</b></i>的每一维<i><b>x</b></i><sup>(</sup><sup><i>k</i></sup><sup>)</sup>进行式(4)的变换:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula"><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>E</mi><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow></math></mathml>)<mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>/</mo><msqrt><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>〗</mo></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="107">式(4)中的期望及方差由训练集计算得来。经过式(4)的变换,某层的输入就变成了均值为0、方差为1的正态分布,从而限制分布,增大导数值,增强反向传播信息流动性,提高训练的收敛速度,但是这样会导致网络的表达能力下降。为了改进这一缺陷,BN对每个神经元增加两个可以学习的调节参数<i>γ</i><sup>(</sup><sup><i>k</i></sup><sup>)</sup>和<i>β</i><sup>(</sup><sup><i>k</i></sup><sup>)</sup>,用来对经过式(4)变换后的激活值进行反变换,从而增强网络的表达能力,反变换操作为<mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>γ</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>β</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow></math></mathml>,其中<i><b>y</b></i><sup>(</sup><sup><i>k</i></sup><sup>)</sup>表示反变换结果。</p>
                </div>
                <div class="p1">
                    <p id="108">BN层提出时,原文将BN层设置在卷积层和激活函数中间,但是目前在实践上倾向于把BN层放置在激活函数后面。BN层既然作为输入的归一化操作,而激活函数处理过后的数据就是下一层的输入,所以本文更倾向于将BN层放在激活函数之后,但是前期实验结果表明,BN层不论在哪个位置都对模型的训练结果没有明显影响。</p>
                </div>
                <div class="p1">
                    <p id="109">需要说明的是,在加入BN层后,需要删除原AlexNet中的随机失活(Dropout)层并且不能使用L1、L2正则化项。有研究表明在使用BN层的网络中可以移除Dropout层<citation id="255" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>,而L1、L2正则化项与BN层不能同时使用的原因尚不清楚。实验发现,当BN层与Dropout层或正则化项同时使用时会导致模型的训练过程不稳定,主要表现在损失函数出现较大波动,而删除Dropout层和正则化项后,损失函数趋于稳定,且模型的泛化能力没有出现明显下降。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">2.3 <b>改进</b>AlexNet<b>的模型结构</b></h4>
                <div class="p1">
                    <p id="111">原始的<i>AlexNet</i>输入为227×227×3大小的图像,其中:227代表图像的宽和高,3代表图像的颜色通道数。因为对于胃肿瘤细胞图像,灰度图就足以判断其类别,所以将模型的输入调整为227×227×1,这样做不仅利于避免颜色噪声,还能一定程度上增加模型的训练速度。除2.1、2.2节的改进之外,对于原始<i>AlexNet</i>卷积层、池化层和全连接层的参数不作改变,改进后的<i>AlexNet</i>网络结构如图4所示。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910022_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 改进AlexNet模型结构" src="Detail/GetImg?filename=images/JSJY201910022_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 改进<i>AlexNet</i>模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910022_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.4 <i>Diagram of improved AlexNet model structure</i></p>

                </div>
                <h3 id="113" name="113" class="anchor-tag">3 实验过程</h3>
                <h4 class="anchor-tag" id="114" name="114">3.1 <b>实验环境及超参数设置</b></h4>
                <div class="p1">
                    <p id="115">实验通过1.10.0版本<i>TensorFlow</i>实现模型代码,通过<i>OpenCV</i>实现图像数据的预处理;训练模型时使用<i>GPU</i>加速,显卡使用单个<i>RTX</i>2070(8 <i>GB</i>),<i>CPU</i>使用英特尔<i>i</i>5-9400。</p>
                </div>
                <div class="p1">
                    <p id="116">模型训练使用<i>MBGD</i>方法,一个<i>Batch</i>大小设置为200,共训练10 000轮,使用滑动平均模型以减小过拟合,使用<i>Adam</i>优化器,以0.000 1为初始学习率,将训练集图片在输入前充分打乱,以减少图片顺序对模型的影响。数据集处理的时间与模型训练的时间一共约120 <i>min</i>。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">3.2 <b>数据集处理</b></h4>
                <div class="p1">
                    <p id="118">本文使用的实验数据是由南昌大学第一附属医院提供的胃粘膜细胞切片图像, 原始数据集共有387张彩色图片,每张大小为320×240像素。其中,胃肿瘤细胞图片180张,胃正常细胞图片134张,胃增生细胞图片73张。按比例随机选取其中307张(80%)作为原始训练集,80张(20%)作为原始测试集,将原始训练集和原始测试集分开处理。</p>
                </div>
                <div class="p1">
                    <p id="119">对于原始训练集中的每张图片,处理步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="120">1)首先对图像进行剪裁:如果是胃肿瘤细胞图片或胃正常细胞图片,从左上角开始,横坐标每隔5个像素,纵坐标每隔4个像素,截取130张220×220大小的局部图片。如果是胃增生细胞图片,从左上角开始,横纵坐标每隔4个像素,截取156张220×220大小的局部图片。</p>
                </div>
                <div class="p1">
                    <p id="121">2)对剪裁后的每张图片依次进行随机亮度变换、随机翻转和随机旋转处理,然后加入训练集,该部分为训练集中不使用<i>RT</i>处理的数据。</p>
                </div>
                <div class="p1">
                    <p id="122">3)重新对原始训练集进行剪裁:如果是胃肿瘤细胞图片或胃正常细胞图片,从左上角开始,横坐标隔4个像素,纵坐标隔2个像素,截取306张220×220大小的局部图片;如果是胃增生细胞图片,从左上角开始,横坐标每隔3个像素,纵坐标每隔2个像素,截取357张220×220大小的局部图片。</p>
                </div>
                <div class="p1">
                    <p id="123">4)对第3)步得到的每张图片依次进行随机亮度变换、随机翻转、随机旋转及<i>RT</i>处理,然后加入训练集,该部分为训练集中使用<i>RT</i>处理的数据。</p>
                </div>
                <div class="p1">
                    <p id="124">5)对处理后训练集中所有图像进行灰度化处理,并将每张图片的尺寸通过双线性插值法调整为227×227。</p>
                </div>
                <div class="p1">
                    <p id="125">经过如上步骤,原始训练集的每张图片都可以处理为若干张大小为227×227的局部图片,局部图片的集合包含原图像的所有信息。处理后的训练集共有138 754张图片,其中经过<i>RT</i>的约占70%,该比例并未经过严格验证,仅为了证明<i>RT</i>在<i>DA</i>中的有效性。</p>
                </div>
                <div class="p1">
                    <p id="126">对于原始测试集中的每张图片,随机剪裁出350张220×220大小的图片,然后通过双线性插值法调整为227×227大小并进行灰度化处理,处理后的测试集共有28 000张图片。</p>
                </div>
                <div class="p1">
                    <p id="127">需要说明的是,在原始训练集处理的第1)步和第3)步中将图片剪裁成220×220大小是为了方便讨论<i>RT</i>参数的影响。而目前没有文献证明对图片进行极小范围的尺寸变化会影响图像信息,因此本文认为将220×220尺寸的图片调整为模型输入所需的227×227尺寸时,不会对实验结果产生影响。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128">3.3 <i>AlexNet</i><b>改进效果</b></h4>
                <div class="p1">
                    <p id="129">根据2.1、2.2节,对原始<i>AlexNet</i>的改进目的是提高模型的收敛速度同时提高模型的泛化性能,图5对比了使用<i>RT</i>和不使用<i>RT</i>时原始<i>AlexNet</i>和改进<i>AlexNet</i>在训练过程中的训练集损失变化。</p>
                </div>
                <div class="p1">
                    <p id="130">图5(<i>a</i>)中,原始<i>AlexNet</i>在10 000轮仍未收敛,改进的<i>AlexNet</i>在3 000轮左右收敛,且其在整体训练过程中的损失都要小于原始<i>AlexNet</i>;图5(<i>b</i>)中,原始<i>AlexNet</i>同样在10 000轮仍未收敛,改进的<i>AlexNet</i>在3 000轮左右收敛。在前期实验中,原始<i>AlexNet</i>约在100 000轮收敛,由图5可知,相比原始<i>AlexNet</i>,改进<i>AlexNet</i>具有更快的收敛速度,且不受<i>RT</i>影响。</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910022_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 原始AlexNet及改进AlexNet的训练损失变化" src="Detail/GetImg?filename=images/JSJY201910022_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 原始<i>AlexNet</i>及改进<i>AlexNet</i>的训练损失变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910022_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.5 <i>Changes of training losses of</i><i>original AlexNet and improved AlexNet</i></p>

                </div>
                <div class="p1">
                    <p id="132">由于训练损失仅能一定程度反映模型的训练情况,并不能反映模型的泛化性能,为了进一步验证本文<i>AlexNet</i>的改进效果,使用了两个不同的数据集,将原始<i>AlexNet</i>和改进<i>AlexNet</i>的测试集平均准确率进行了对比,对比结果如表1所示。表1中,胃肿瘤细胞数据使用的是未经<i>RT</i>处理的数据集。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表</b>1 <b>原始</b><i>AlexNet</i><b>和改进</b><i>AlexNet</i><b>的准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="2"><br />测试集平均准确率/%</td></tr><tr><td><br />原始<i>AlexNet</i></td><td>改进<i>AlexNet</i></td></tr><tr><td><br />胃肿瘤细胞数据</td><td>83.49</td><td>90.64</td></tr><tr><td><br /><i>Cifar</i>-10</td><td>77.94</td><td>87.42</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="134">由表1可以发现,改进<i>AlexNet</i>的测试集准确率在两个数据集上均有不同程度的提升。对于未经<i>RT</i>处理的胃肿瘤细胞图像数据集,改进的<i>AlexNet</i>相比原始<i>AlexNet</i>提高了7.15个百分点;对于<i>Cifar</i>-10数据集,改进的<i>AlexNet</i>相比原始<i>AlexNet</i>提高了9.48个百分点。表明本文的改进<i>AlexNet</i>相比原始<i>AlexNet</i>有更好的泛化性能。</p>
                </div>
                <div class="p1">
                    <p id="135">综上所述,本文对<i>AlexNet</i>的改进方法是有效的,相比原始<i>AlexNet</i>,具有更快的收敛速度和更好的泛化性能。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136">3.4 <i>RT</i><b>的参数设置</b></h4>
                <div class="p1">
                    <p id="137">为了寻找使用<i>RT</i>实现<i>DA</i>时的最佳参数设置,采用先确定最佳极点坐标,然后确定最佳maxRadius的策略。</p>
                </div>
                <div class="p1">
                    <p id="138">令maxRadius恒为110,取图像左上至右下对角线上的点至中心点(110, 110)的相对距离为横坐标,两点间的欧氏距离为相对距离乘以<mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mn>2</mn></msqrt></mrow></math></mathml>,测试准确率为纵坐标,极点距图像中心点的距离与测试集准确率的关系如图6(<i>a</i>)所示,其中中心点左侧的点取负距离,右侧的点取正距离。可以发现,当距离为0时,即极点位于图像中心点时测试准确率最高,当极点偏离图像中心点时,测试准确率下降,这说明极点应选取在图像的中心,验证了1.2.2节中极点应尽量接近图像的中心点的结论。</p>
                </div>
                <div class="p1">
                    <p id="139">令极点坐标恒为(110, 110),测试集的平均准确率随maxRadius变化的情况如图6(<i>b</i>)所示,可以发现,maxRadius=110时,准确率最高,其他情况准确率均有下降。因此对于极点(110, 110),最佳的maxRadius应为110,根据1.2.1节的分析,此时刚好不会出现超界缺失,图像信息的丢失也最少,从侧面印证了该参数设置的合理性。</p>
                </div>
                <div class="p1">
                    <p id="140">综上所述,使用<i>RT</i>作为<i>DA</i>方法时,应该采用不会出现超界缺失和信息丢失最少的<i>RT</i>参数。因此可以得到一个适用于胃肿瘤细胞图像数据的<i>RT</i>参数设置方法:对于大小为<i>N</i>×<i>N</i>的方形图像数据集,选取的极点坐标应为(<i>N</i>/2, <i>N</i>/2),<i>maxRadius</i>=<i>N</i>/2。对于本文使用的数据集,<i>N</i>=220,因此最佳的参数设置应为极点(110, 110),<i>maxRadius</i>=110。</p>
                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910022_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 极点和maxRadius对测试集准确率的影响" src="Detail/GetImg?filename=images/JSJY201910022_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 极点和<i>maxRadius</i>对测试集准确率的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910022_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Influence of pole and <i>maxRadius</i> on accuracy of test set</p>

                </div>
                <h4 class="anchor-tag" id="142" name="142">3.5 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="143">本文使用改进的<i>AlexNet</i>模型进行训练,采取极点(110, 110),maxRadius=110的<i>RT</i>方法对数据集进行<i>DA</i>处理,实现胃肿瘤细胞的图像识别。</p>
                </div>
                <div class="p1">
                    <p id="144">表2显示了使用<i>RT</i>和不使用<i>RT</i>时,<i>DA</i>对测试集准确率的影响。可以发现使用<i>RT</i>比不使用<i>RT</i>的效果更好,它在测试集上的最高准确率和平均准确率分别提高了3.00和6.05个百分点,说明了使用<i>RT</i>作为<i>DA</i>手段的有效性。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表</b>2 <b>两种</b><i>DA</i><b>方法的准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.2 <i>Comparison accuracy of two DA methods</i></p>
                    <p class="img_note">单位:%</p>
                    <table id="145" border="1"><tr><td><br />方法</td><td>最高准确率</td><td>平均准确率</td></tr><tr><td><br />不使用<i>RT</i></td><td>96.50</td><td>90.64</td></tr><tr><td><br />使用<i>RT</i></td><td>99.50</td><td>96.69</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="146">为验证本文方法在胃肿瘤细胞图像中各类别上分类的有效性,表3展示了在三个类别上分类的具体情况,其中:精确率指正确预测为正的占全部预测为正的比例;召回率指正确预测为正的占全部实际为正的比例;<i>F</i>1值可以看作是精确率和召回率的加权平均,<i>F</i>1值越接近1表示分类性能越好。由于这三项指标一般用于二分类问题,所以表3在计算其中一类的指标时,将该类看作正类,其余两类看作是负类。</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表</b>3 <b>各类别的精确率、召回率和</b><i>F</i>1<b>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.3 <i>Precision</i>, <i>recall and F</i>1 <i>score of each class</i></p>
                    <p class="img_note"></p>
                    <table id="147" border="1"><tr><td><br />指标</td><td>癌变</td><td>正常</td><td>增生</td></tr><tr><td><br />精确率</td><td>0.981</td><td>0.957</td><td>0.951</td></tr><tr><td><br />召回率</td><td>0.980</td><td>0.952</td><td>0.967</td></tr><tr><td><br /><i>F</i>1值</td><td>0.980</td><td>0.954</td><td>0.958</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="148">根据表3可知,本文方法对胃肿瘤细胞图像的三个类别都能够实现较精确的识别,其中癌变细胞图像的分类效果最好,正常和增生细胞的分类效果相当。</p>
                </div>
                <div class="p1">
                    <p id="149">为了进一步验证本文识别胃肿瘤细胞图像的有效性,将本文方法与其他方法进行了对比。参与对比的方法使用的是<i>AlexNet</i>与<i>GoogLeNet</i>的融合模型<citation id="256" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,该模型使用的训练集是经过过滤的胃肿瘤细胞图像,过滤后的图像仅有肿瘤细胞,背景经过滤会变成黑色。本文方法与该方法的对比结果如表4所示。</p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit"><b>表</b>4 <b>本文方法与其他方法使用数据集及准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.4 <i>Comparison of used dataset and accuracy with other methods</i></p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td><br />方法</td><td>是否过滤数据</td><td>数据数量</td><td>最高准确率/%</td></tr><tr><td><br />本文方法</td><td>否</td><td>387</td><td>99.50</td></tr><tr><td><br />融合网络+<br />传统<i>DA</i>方法<sup>[8]</sup></td><td>是</td><td>700</td><td>99.40</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="151">由表4可知,本文方法使用了更少且未过滤的原始数据和更简单的模型结构实现了胃肿瘤细胞的图像识别,且本文方法的最高准确率与对比方法相比提高了0.1个百分点,表明了本文方法识别胃肿瘤细胞图像的有效性。</p>
                </div>
                <h3 id="152" name="152" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="153">本文使用<i>AlexNet</i>实现胃肿瘤细胞图像的分类识别。采用<i>RT</i>和剪裁、旋转、翻转、亮度变换结合的<i>DA</i>方法解决了数据集过小的问题,并且总结了使用<i>RT</i>进行胃肿瘤细胞图像增强时的<i>RT</i>参数设置方法。此外,通过改进激活函数和归一化层提高了<i>AlexNet</i>的收敛速度。实验结果表明该<i>DA</i>方法和<i>AlexNet</i>改进方法均有较好的效果,与其他方法的对比结果也表明使用本文方法能够有效实现胃肿瘤细胞图像的识别。然而,本文仅实现了癌变、正常和增生细胞的三分类任务,增生细胞还可往下细分为轻度增生、中度增生和重度增生,由于原始数据太少,所以并未尝试对增生细胞的进一步分类,如何在分类癌变和正常细胞的基础上精确分类增生细胞图像是下一步的研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="184">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Review of recent molecular landscape knowledge of gastric cancer">

                                <b>[1]</b>SHIMIZU D,KANDA M,KODERA Y.Review of recent molecular landscape knowledge of gastric cancer[J].Histology and Histopathology,2018,33(1):11-26.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images">

                                <b>[2]</b>HIRASAWA T,AOYAMA K,TANIMOTO T,et al.Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images[J].Gastric Cancer,2018,21(4):653-660.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification of breast cancer histology images using ALEXNET">

                                <b>[3]</b>NAWAZ W,AHMED S,TAHIR A,et al.Classification of breast cancer histology images using ALEXNET[C]//Proceedings of the15th International Conference Image Analysis and Recognition,LNCS10882.Cham:Springer,2018:869-876.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Alcoholism identification based on an Alex Net transfer learning model">

                                <b>[4]</b>WANG S,XIE S,CHEN X,et al.Alcoholism identification based on an Alex Net transfer learning model[J].Frontiers in Psychiatry,2019,10:No.205.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2018S1011&amp;v=MjE3MjRkN0c0SDltdnJvOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oV3JyQUx6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>刘飞,张俊然,杨豪.基于深度学习的糖尿病患者的分类识别[J].计算机应用,2018,38(S1):39-43.(LIU F,ZHANG JR,YANG H.Classification and recognition of diabetes mellitus based on deep learning[J].Journal of Computer Applications,2018,28(S1):39-43.)
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2018S1049&amp;v=MTM5NjZxZlp1WnNGeW5oV3JyQUx6N0JkN0c0SDltdnJvOUJiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>胡光亮,王艳,罗勇,等.基于卷积神经网络的鼻咽肿瘤MR图像分割[J].计算机应用,2018,38(S1):208-212.(HU GL,WANG Y,LUO Y,et al.Segmentation of nasopharyngeal nneoplasm MR images based on convolutional neural network[J].Journal of Computer Applications,2018,38(S1):208-212.)
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201806036&amp;v=MDM0MzBuTXFZOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oV3JyQUlqWFRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>杜剑,胡炳樑,张周锋.基于卷积神经网络与显微高光谱的胃癌组织分类方法研究[J].光学学报,2018,38(6):267-273.(DU J,HU B L,ZHANG Z F.Gastric carcinoma classification based on convolutional neural network and micro-hyperspectral imaging[J].Acta Optica Sinica,2018,38(6):267-273.)
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2018S2054&amp;v=MzExMDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFdyckFMejdCYjdHNEg5bXZyWTlBWUlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>张泽中,高敬阳,吕纲,等.基于深度学习的胃癌病理图像分类方法[J].计算机科学,2018,45(S2):263-268.(ZHANGZ Z,GAO J Y,LYU G,et al.Pathological image classification of gastric cancer based on depth learning[J].Computer Science,2018,45(S2):263-268.)
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The history began from Alex Net:a comprehensive survey on deep learning approaches">

                                <b>[9]</b>ALOM M Z,TAHA T M,YAKOPCIC C,et al.The history began from Alex Net:a comprehensive survey on deep learning approaches[EB/OL].[2019-02-08].https://arxiv.org/ftp/arxiv/papers/1803/1803.01164.pdf.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The effectiveness of data augmentation in image classification using deep learning">

                                <b>[10]</b>PEREZ L,WANG J.The effectiveness of data augmentation in image classification using deep learning[EB/OL].[2019-02-08].https://arxiv.org/pdf/1712.04621.pdf.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image augmentation using radial transform for training deep neural networks">

                                <b>[11]</b>SALEHINEJAD H,VALAEE S,DOWDELL T,et al.Image augmentation using radial transform for training deep neural networks[C]//Proceedings of the 2018 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2018:3016-3020.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S1011&amp;v=MzAxOTF2cm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhXcnJBTHo3QmI3RzRIOWE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>吕鸿蒙,赵地,迟学斌.基于增强Alex Net的深度学习的阿尔茨海默病的早期诊断[J].计算机科学,2017,4(S1):60-70.(LYU H M,ZHAO D,CHI X B.Deep learning for early diagnosis of Alzheimer's disease based on intensive Alex Net[J].Computer Science,2017,44(S1):60-70.)
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201703015&amp;v=MTgwMDc0SDliTXJJOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oV3JyQUx6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>丁蓬莉,李清勇,张振,等.糖尿病性视网膜图像的深度神经网络分类方法[J].计算机应用,2017,37(3):699-704.(DING P L,LI Q Y,ZHANG Z,et al.Diabetic retinal image classification method based on deep neural network[J].Journal of Computer Applications,2017,37(3):699-704.)
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding batch normalization">

                                <b>[14]</b>BJORCK N,GOMES C,SELMAN B,et al.Understanding batch normalization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.02375.pdf.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                WANG S,PHILLIPS P,SUI Y,et al.Classification of Alzheimer's disease based on eight-layer convolutional neural network with leaky rectified linear unit and max pooling[J].Journal of Medical Systems,2018,42(5):No.85.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200812042&amp;v=MTEzMzdyckFMejdCZDdHNEh0bk5yWTlCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>余博,郭雷,赵天云.基于对数极坐标变换的灰度投影稳像算法[J].计算机应用,2008,28(12):3126-3128.(YU B,GUO L,ZHAO T Y.Gray projection image stabilizing algorithm based on log-polar image transform[J].Journal of Computer Applications,2008,28(12):3126-3128.)
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint optic disc and cup segmentation based on multi-label deep network and polar transformation">

                                <b>[17]</b>FU H,CHENG J,XU Y,et al.Joint optic disc and cup segmentation based on multi-label deep network and polar transformation[J].IEEE Transactions on Medical Imaging,2018,37(7):1597-1605.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lung Nodule Classification Using Deep Features in CT Images">

                                <b>[18]</b>KUMAR D,WONG A,CLAUSI D A.Lung nodule classification using deep features in CT images[C]//Proceedings of the 12th Conference on Computer and Robot Vision.Piscataway:IEEE,2015:133-138.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[19]</b>KRIZHEYSKY A,SUTSKEVER I,HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York:Curran Associates Inc,2012:1097-1105.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=How to choose an activation function">

                                <b>[20]</b>MHASKAR H N,MICCHELLI C A.How to choose an activation function[C]//Proceedings of the 6th International Conference on Neural Information Processing Systems.San Francisco:Morgan Kaufmann Publishers Inc,1993:319-326.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of rectified activations in convolutional network">

                                <b>[21]</b>XU B,WANG N,CHEN T,et al.Empirical evaluation of rectified activations in convolutional network[EB/OL].[2019-02-08].https://arxiv.org/pdf/1505.00853.pdf.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[22]</b>SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-02-08].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Yarn-dyed fabric defect classification based on convolutional neural network">

                                <b>[23]</b>JING J,DONG A,LI P,et al.Yarn-dyed fabric defect classification based on convolutional neural network[J].Optical Engineering,2017,56(9):093104.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[EB/OL].[2019-01-10].https://arxiv.org/pdf/1502.03167.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910022" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910022&amp;v=MjQ3NjdmWnVac0Z5bmhXcnJBTHo3QmQ3RzRIOWpOcjQ5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
