<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136766322783750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905041%26RESULT%3d1%26SIGN%3dyCfpTByZkbj7u%252fyrX52VHt5mu9s%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905041&amp;v=MjIwNTh0R0ZyQ1VSN3FmWnVac0Z5RG5VcnpPTHo3QmQ3RzRIOWpNcW85QlpZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="1.1 &lt;b&gt;空中多节点的无人机网络&lt;/b&gt;">1.1 <b>空中多节点的无人机网络</b></a></li>
                                                <li><a href="#49" data-title="1.2 &lt;b&gt;视频处理中的问题&lt;/b&gt;">1.2 <b>视频处理中的问题</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="2 运动视频到全景视频 ">2 运动视频到全景视频</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="2.1 &lt;b&gt;相机路径提取&lt;/b&gt;">2.1 <b>相机路径提取</b></a></li>
                                                <li><a href="#63" data-title="2.2 &lt;b&gt;视频拼接与稳像&lt;/b&gt;">2.2 <b>视频拼接与稳像</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="3 提出的算法 ">3 提出的算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="3.1 &lt;b&gt;特征点评分&lt;/b&gt;">3.1 <b>特征点评分</b></a></li>
                                                <li><a href="#83" data-title="3.2 &lt;b&gt;匹配模型的构建&lt;/b&gt;">3.2 <b>匹配模型的构建</b></a></li>
                                                <li><a href="#104" data-title="3.3 &lt;b&gt;运动视频的连续帧提取&lt;/b&gt;">3.3 <b>运动视频的连续帧提取</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#117" data-title="4 实验与分析 ">4 实验与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="图1 空中多节点的无人机网络">图1 空中多节点的无人机网络</a></li>
                                                <li><a href="#62" data-title="图2 运动视频中提取出的相机运动路径">图2 运动视频中提取出的相机运动路径</a></li>
                                                <li><a href="#72" data-title="图3 投影变换与网格约束">图3 投影变换与网格约束</a></li>
                                                <li><a href="#75" data-title="图4 视频中每个网格的运动路径">图4 视频中每个网格的运动路径</a></li>
                                                <li><a href="#88" data-title="图5 灰度塔">图5 灰度塔</a></li>
                                                <li><a href="#97" data-title="图6 锁定匹配块">图6 锁定匹配块</a></li>
                                                <li><a href="#116" data-title="图7 感兴趣区域计算">图7 感兴趣区域计算</a></li>
                                                <li><a href="#120" data-title="图8 两种算法匹配结果对比">图8 两种算法匹配结果对比</a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;i&gt;MMGT&lt;/i&gt;&lt;b&gt;与&lt;/b&gt;K&lt;i&gt;NN&lt;/i&gt;&lt;b&gt;匹配算法性能对比&lt;/b&gt;"><b>表</b>1 <i>MMGT</i><b>与</b>K<i>NN</i><b>匹配算法性能对比</b></a></li>
                                                <li><a href="#124" data-title="图9 两种算法在两个视频源下的匹配对数目对比">图9 两种算法在两个视频源下的匹配对数目对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="141">


                                    <a id="bibliography_1" title=" IVAN S K, OLEG V P.Spherical video panorama stitching from multiple cameras with intersecting fields of view and inertial measurement unit[C]// Proceedings of the 2016 International Siberian Conference on Control and Communications.Piscataway, NJ:IEEE, 2016:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spherical video panorama stitching from multiple cameras with intersecting fields of view and inertial measurement unit">
                                        <b>[1]</b>
                                         IVAN S K, OLEG V P.Spherical video panorama stitching from multiple cameras with intersecting fields of view and inertial measurement unit[C]// Proceedings of the 2016 International Siberian Conference on Control and Communications.Piscataway, NJ:IEEE, 2016:1-6.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_2" title=" ZHANG L, HE Z, LIU Y.Deep object recognition across domains based on adaptive extreme learning machine[J].Neurocomputing, 2017, 239:194-203." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDA7B9F8A7E11F5478699913E4C134A8C&amp;v=MDEyOTlGMlljMFk1NE9EUW84eXhFYjdEWjBRWDdoMlJaR2VMR1FOTExzQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekxxK3c2RT1OaWZPZmNmSkdhUA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         ZHANG L, HE Z, LIU Y.Deep object recognition across domains based on adaptive extreme learning machine[J].Neurocomputing, 2017, 239:194-203.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_3" title=" YANG B, DONG Z, LIANG F, et al.Automatic registration of large-scale urban scene point clouds based on semantic feature points[J].ISPRS Journal of Photogrammetry &amp;amp; Remote Sensing, 2016, 113:43-58." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic registration of large-scale urban scene point clouds based on semantic feature points">
                                        <b>[3]</b>
                                         YANG B, DONG Z, LIANG F, et al.Automatic registration of large-scale urban scene point clouds based on semantic feature points[J].ISPRS Journal of Photogrammetry &amp;amp; Remote Sensing, 2016, 113:43-58.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_4" title=" LOWE D G.Distinctive image features from scale-invariant key-points[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MzE5MDVYcVJyeG94Y01IN1I3cWRaK1p1RmlybFc3M0pJMWM9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlT&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         LOWE D G.Distinctive image features from scale-invariant key-points[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_5" title=" HSU W Y, LEE Y C.Rat brain registration using improved speeded up robust features[J].Journal of Medical and Biological Engineering, 2017, 37 (1) :45-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rat brain registration using improved speeded up robust features">
                                        <b>[5]</b>
                                         HSU W Y, LEE Y C.Rat brain registration using improved speeded up robust features[J].Journal of Medical and Biological Engineering, 2017, 37 (1) :45-42.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_6" title=" RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]// Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:2564-2571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB:an efficient alternative to SIFT or SURF">
                                        <b>[6]</b>
                                         RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]// Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:2564-2571.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_7" title=" LEUTENEGGER S, CHLI M, SIEGWART Y.BRISK:binary robust invariant scalable keypoints[C]// Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:2548-2555." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BRISK:Binary Robust invariant scalable keypoints">
                                        <b>[7]</b>
                                         LEUTENEGGER S, CHLI M, SIEGWART Y.BRISK:binary robust invariant scalable keypoints[C]// Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:2548-2555.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_8" title=" CALONDER M, LEPETIT V, STRECHA C, et al.BRIEF:binary robust independent elementary features[C]// Proceedings of the 11th European Conference on Computer Vision.Berlin:Springer, 2010:778-792." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BRIEF:Binary robust independent elementary features">
                                        <b>[8]</b>
                                         CALONDER M, LEPETIT V, STRECHA C, et al.BRIEF:binary robust independent elementary features[C]// Proceedings of the 11th European Conference on Computer Vision.Berlin:Springer, 2010:778-792.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_9" title=" MOR M, FRAENKEL A S.A hash code method for detecting and correcting spelling-errors[J].Communications of the ACM, 1982, 25 (12) :935-938." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000025185&amp;v=MjU5MzBROG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGd1ZhUnM9TmlmSVk3SzdIdGpOcjQ5RlpPa0tEWA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         MOR M, FRAENKEL A S.A hash code method for detecting and correcting spelling-errors[J].Communications of the ACM, 1982, 25 (12) :935-938.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_10" title=" SANTHA T, MOHANA M B V.The significance of real-time, biomedical and satellite image processing in understanding the objects &amp;amp; application to computer vision[C]// Proceedings of the 2nd IEEE International Conference on Engineering &amp;amp; Technology.Piscataway, NJ:IEEE, 2016:661-670." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The significance of real-time,biomedical and satellite image processing in understanding the objects &amp;amp; application to computer vision">
                                        <b>[10]</b>
                                         SANTHA T, MOHANA M B V.The significance of real-time, biomedical and satellite image processing in understanding the objects &amp;amp; application to computer vision[C]// Proceedings of the 2nd IEEE International Conference on Engineering &amp;amp; Technology.Piscataway, NJ:IEEE, 2016:661-670.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_11" title=" BROWN M, LOWE D G.Automatic panoramic image stitching using invariant features[J].International Journal of Computer Vision, 2007, 74 (1) :59-73." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831131&amp;v=MjY4MTdqN0Jhck80SHRIT3A0eEVaZWdPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJsVzczSkkxYz1O&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         BROWN M, LOWE D G.Automatic panoramic image stitching using invariant features[J].International Journal of Computer Vision, 2007, 74 (1) :59-73.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_12" title=" GUO H, LIU S, HE T, et al.Joint video stitching and stabilization from moving cameras[J].IEEE Transactions on Image Processing, 2016, 25 (11) :5491-5503." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint video stitching and stabilization from moving cameras">
                                        <b>[12]</b>
                                         GUO H, LIU S, HE T, et al.Joint video stitching and stabilization from moving cameras[J].IEEE Transactions on Image Processing, 2016, 25 (11) :5491-5503.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_13" title=" 倪国强, 刘琼.多源图像配准技术分析与展望[J].光电工程, 2004, 31 (9) :1-6. (NI G Q, LIU Q.Analysis and prospect of multi-source image registration techniques[J].Opto-Electronic Engineering, 2004, 31 (9) :1-6.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC200409000&amp;v=MjU1MjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyekJJaW5NYmJHNEh0WE1wbzlGWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         倪国强, 刘琼.多源图像配准技术分析与展望[J].光电工程, 2004, 31 (9) :1-6. (NI G Q, LIU Q.Analysis and prospect of multi-source image registration techniques[J].Opto-Electronic Engineering, 2004, 31 (9) :1-6.) 
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_14" title=" ZARAGOZA J, CHIN T J, BROWN M S, et al.As-projective-as-possible image stitching with moving DLT[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (7) :1285-1298." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=As-projective-as-possible image stitching with moving DLT">
                                        <b>[14]</b>
                                         ZARAGOZA J, CHIN T J, BROWN M S, et al.As-projective-as-possible image stitching with moving DLT[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (7) :1285-1298.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_15" title=" 朱云芳, 叶秀清, 顾伟康.视频序列的全景图拼接技术[J].中国图象图形学报, 2006, 11 (8) :1150-1155. (ZHU Y F, YE X Q, GU W K.Mosaic panorama technique for videos[J].Journal of Image and Graphics, 2006, 11 (8) :1150-1155.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200608015&amp;v=MjcwOTA1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyekJQeXJmYkxHNEh0Zk1wNDlFWVlRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         朱云芳, 叶秀清, 顾伟康.视频序列的全景图拼接技术[J].中国图象图形学报, 2006, 11 (8) :1150-1155. (ZHU Y F, YE X Q, GU W K.Mosaic panorama technique for videos[J].Journal of Image and Graphics, 2006, 11 (8) :1150-1155.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-24 11:19</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1480-1484 DOI:10.11772/j.issn.1001-9081.2018092034            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于灰度塔评分的匹配模型构建在无人机网络视频拼接中的应用</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%8D%97%E4%BA%91&amp;code=41747332&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李南云</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%97%AD%E5%85%89&amp;code=35883436&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王旭光</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E5%8D%8E%E5%BC%BA&amp;code=29499212&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴华强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E9%9D%92%E6%9E%97&amp;code=41747333&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何青林</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E8%8B%8F%E5%B7%9E%E7%BA%B3%E7%B1%B3%E6%8A%80%E6%9C%AF%E4%B8%8E%E7%BA%B3%E7%B1%B3%E4%BB%BF%E7%94%9F%E5%AD%A6%E9%99%A2&amp;code=0002522&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学技术大学苏州纳米技术与纳米仿生学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%8B%8F%E5%B7%9E%E7%BA%B3%E7%B1%B3%E6%8A%80%E6%9C%AF%E4%B8%8E%E7%BA%B3%E7%B1%B3%E4%BB%BF%E7%94%9F%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1503885&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院苏州纳米技术与纳米仿生研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E5%BE%AE%E7%94%B5%E5%AD%90%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0187103&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">清华大学微电子学研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>对于复杂非配合情况下, 视频拼接中特征匹配对的数目和特征匹配准确率无法同时达到后续稳像和拼接的要求这一问题, 提出一种基于灰度塔对特征点进行评分后构建匹配模型来进行精准特征匹配的方法。首先, 利用灰度级压缩后相近灰度级合并这一现象, 建立灰度塔来实现对特征点的评分;而后, 选取评分高的特征点建立基于位置信息的匹配模型;最后, 依据匹配模型的定位进行区域分块匹配来避免全局特征点的干扰和大误差噪点匹配, 选择误差最小的特征匹配对作为最终结果匹配对。另外, 在运动的视频流中, 可通过前后帧信息建立掩模进行区域特征提取, 匹配模型也可选择性遗传给后帧以节约算法时间。实验结果表明, 在运用了基于灰度塔评分的匹配模型后, 特征匹配对准确率在95%左右。相同帧特征匹配对的数目相较于随机采样一致性有近10倍的提升, 在兼顾匹配数目和匹配准确率的同时且无大误差匹配结果, 对于环境和光照有较好的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E6%8B%BC%E6%8E%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频拼接;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%81%B0%E5%BA%A6%E5%A1%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">灰度塔;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">匹配模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E5%9D%97%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分块匹配;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李南云 (1997—) , 女, 安徽宿州人, 硕士研究生, 主要研究方向:图像处理、视频拼接;;
                                </span>
                                <span>
                                    *王旭光 (1976—) , 男, 吉林长春人, 研究员, 博士, 主要研究方向:人脸识别、半导体存储;电子邮箱xgwang2009@sinano.ac.cn;
                                </span>
                                <span>
                                    吴华强 (1978—) , 男, 安徽黄山人, 教授, 博士, 主要研究方向:新型半导体存储器、基于新型器件的类脑计算;;
                                </span>
                                <span>
                                    何青林 (1993—) , 男, 重庆人, 硕士研究生, 主要研究方向:视频处理、视频拼接。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-09</p>

            </div>
                    <h1><b>Application of matching model based on grayscale tower score in unmanned aerial vehicle network video stitching</b></h1>
                    <h2>
                    <span>LI Nanyun</span>
                    <span>WANG Xuguang</span>
                    <span>WU Huaqiang</span>
                    <span>HE Qinglin</span>
            </h2>
                    <h2>
                    <span>Suzhou Nanotechnology and Nano-Bionics College, University of Science and Technology of China</span>
                    <span>Suzhou Institute of Nano-Tech and Nano-Bionics (SINANO) , Chinese Academy of Sciences</span>
                    <span>Institute of Microelectronics, Tsinghua University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Concerning the problem that in complex and non-cooperative situations the number of matching feature pairs and the accuracy of feature matching results in video stitching can not meet the requirements of subsequent image stabilization and stitching at the same time, a method of constructing matching model to match features accurately after feature points being scored by grayscale tower was proposed. Firstly, the phenomenon that the similiar grayscales would merged together after grayscale compression was used to establish a grayscale tower to realize the scoring of feature points. Then, the feature points with high score were selected to establish the matching model based on position information. Finally, according to the positioning of the matching model, regional block matching was performed to avoid the influence of global feature point interference and large error noise matching, and the feature matching pair with the smallest error was selected as the final result of matching pair. In addition, in a motion video stream, regional feature extraction could be performed by using the information of previous and next frames to establish a mask, and the matching model could be selectively passed on to the next frame to save the computation time. The simulation results show that after using this matching model based on grayscale tower score, the feature matching accuracy is about 95% and the number of matching feature pairs of the same frame is nearly 10 times higher than that of the traditional method. The proposed method has good robustness to environment and illumination while guaranteeing the matching number and the matching accuracy without large error matching result.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20stitching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video stitching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=grayscale%20tower&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">grayscale tower;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=matching%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">matching model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=block%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">block matching;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Nanyun, born in 1997, M. S. candidate. Her research interests include image processing, video stitching. ;
                                </span>
                                <span>
                                    WANG Xuguang, born in 1976, Ph. D. , research fellow. His research interests include face recognition, semiconductor storage. ;
                                </span>
                                <span>
                                    WU Huaqiang, born in 1978, Ph. D. , professor. His research interests include new semiconductor memory, brain-like computing based on new devices. ;
                                </span>
                                <span>
                                    HE Qinglin, born in 1993, M. S. candidate. His research interests include video prossing, video stitching.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-09</p>
                            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="34">随着近年图像处理领域的飞速发展, 图像处理以及视频处理在社会上的应用也更加广泛。单纯的静态图像拼接已经难以满足目前社会的需求, 静态图像难以让人们获取真实场景中的每个物体在时间流上的信息, 而时间流上的信息在社会以及国防中是非常重要的一部分, 因此近年来人们在视频拼接领域中探讨也愈发深入。</p>
                </div>
                <div class="p1">
                    <p id="35">特征提取和匹配作为图像拼接<citation id="171" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>中非常重要的一部分, 也广泛应用于图像识别<citation id="172" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、三维模型构建<citation id="173" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等领域。提取出来的特征点的质量直接影响着特征匹配的结果;而特征匹配的结果又直接关系到后续拼接、识别和模型构建等最终的准确率。特征匹配对作为图像处理这个大厦中的基石, 目前主要的难点为匹配对的数目以及匹配准确率和匹配时间。</p>
                </div>
                <div class="p1">
                    <p id="36">基于传统图像算法的特征提取方式主要有尺度不变特征变换 (Scale Invariant Feature Transform, SIFT) <citation id="174" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、加速稳健特征 (Speeded Up Robust Feature, SURF) <citation id="175" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、ORB (Oriented FAST and Rotated BRIEF) <citation id="176" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、BRISK (Binary Robust Invariant Scalable Keypoints) <citation id="177" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、BRIEF (Binary Robust Independent Element Feature) <citation id="178" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、FAST (Features form Accelerated Segment Test) , 其中SIFT和SURF的描述符具有尺度不变性和旋转不变性, 可以获得较高的匹配准确度, 但是计算量较大, 效率较低。而ORB、BRISK、FAST则是通过比较特征像素点周围像素的差异来形成二值描述子, 通过计算两个特征描述子之间的Hamming距离<citation id="179" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>来比较两个特征点之间的相关性。</p>
                </div>
                <div class="p1">
                    <p id="37">特征匹配的筛选方式主要有<i>K</i>最近邻 (<i>K</i>-Nearest Neighbors, <i>K</i>NN) 匹配和暴力匹配, 然后加以随机采样一致性 (RANdom Sample Consensus, RANSAC) <citation id="180" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>筛选出内点集。基于全局单应矩阵的RANSAC从图像全局进行考虑, 保留图像主平面的匹配结果, 去除次平面以及视差平面的匹配对。也就是当存在视差时, 相机获取的成像图片中依据景物深度可以分为多个平面, RANSAC的筛选方式就是保留占比最大的深度平面的匹配结果, 从原理上来说这种匹配方式会丢失一些平面的信息, 在应对存在视差较大情况下配准能力不足。<i>K</i>NN匹配方式可以有效避免由于视差所造成的匹配对之间的差异, 直接获取匹配结果, 但是由于不同成像图片的不同光照信息和图像信息, <i>K</i>NN的阈值一成不变会造成较大的匹配误差, 无法直接应用于视频拼接。上面两种匹配方式无法避免由于噪声影响造成的无规律错误匹配。</p>
                </div>
                <div class="p1">
                    <p id="38">本文通过在视频拼接中寻找特征匹配对数目较多且均匀分布的特征匹配方式, 来解决视频拼接中特征点数目不够均匀无法考虑全局深度的问题。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="40">视频拼接主要可以分为两部分, 即固定摄像机视频的拼接和移动摄像机的视频拼接:固定摄像机的视频拼接一般为摄像机静止固定在某个位置上进行视频的捕获和后续的拼接工作;移动摄像机的视频拼接则是移动的摄像机对动态的场景进行捕获并且拼接。</p>
                </div>
                <h4 class="anchor-tag" id="41" name="41">1.1 <b>空中多节点的无人机网络</b></h4>
                <div class="p1">
                    <p id="42">视频拼接目前已经有了很多的应用, 如:</p>
                </div>
                <div class="p1">
                    <p id="43">2013年, 美国国防高级研究计划署 (Defense Advanced Research Projects Agency, DARPA) 和英国航空航天系统公司共同研发的自主实时地面监控影响系统 (Autonomous Real-time Ground Ubiquitous Surveillance-Imaging System, ARGUS) , 安置到军用无人机上, 18亿像素的传感器可以监视地面15 cm的任何物体, 监视高度为6 km, 系统造价为1 850万美元。</p>
                </div>
                <div class="p1">
                    <p id="44">2015年, 国家海洋局第二海洋研究所建成国内首套自主实时的地表全方位监视成像系统, 由6台摄像机组成, 覆盖范围为180°, 可获得视野覆盖范围内的图像实时数据。</p>
                </div>
                <div class="p1">
                    <p id="45">2017年, 电子科技大学手持相机的抖动视频拼接, 在具有视差存在的情况下对视频进行拼接处理后, 较好地去除了抖动和避免了鬼影现象, 但是处理方式为线下处理。</p>
                </div>
                <div class="p1">
                    <p id="46">以上的几种系统由于系统造价过高, 以及摄像机位置的固定导致无法具有足够的机动性或者不能实时处理等原因, 无法在民用和社会治安中有很好的应用。</p>
                </div>
                <div class="p1">
                    <p id="47">而由多台无人机所搭建的空中多节点的无人机 (Unmanned Aerial Vehicle, UAV) 网络解决了造价和机动性方面上的问题, 使得这个系统在民用以及社会治安、森林救火等紧急情况中具有良好的应用。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 空中多节点的无人机网络" src="Detail/GetImg?filename=images/JSJY201905041_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 空中多节点的无人机网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Multi-node unmanned aerial vehicle network</p>

                </div>
                <h4 class="anchor-tag" id="49" name="49">1.2 <b>视频处理中的问题</b></h4>
                <div class="p1">
                    <p id="50">在运动的摄像机拼接处理过程中, 相机和摄像机中物体的双重运动, 还有多个摄像机之间的相互运动, 这些情况对视频的拼接造成了非常严重的影响, 另外高空中无人机受到气流和风速影响, 本身采集的视频就具有低频抖动的情况, 这在一定程度上又增加了视频的拼接难度。</p>
                </div>
                <div class="p1">
                    <p id="51">因为每个时间对应帧之间投影变换矩阵具有独立性, 单纯地由图像拼接转换到视频拼接后, 所拼接完成的每一帧图像大小各不相同, 同一物体拉伸情况也不相同, 在视差存在的情况下进行尺寸统一 (resize) 后合成的视频具有严重的抖动情况。如果为了解决抖动引入强帧间关系, 在相机相对运动剧烈的情况下, 强帧间约束会使得视频的对齐出现问题。面对视频拼接中时间稳定和空间对齐方面的问题, 解决这些问题的基础就是在获取匹配对时获取足够多正确率较高的特征匹配对。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">2 运动视频到全景视频</h3>
                <div class="p1">
                    <p id="53">运动视频更加方便人们去观察现实的世界, 但是单一相机的视角往往较为局限, 而由此衍生的运动视频的拼接算法可以提供实时的全景视角。无人机搭载相机所获得的运动视频在获得更加广阔视角的同时具有非常好的机动性, 可以满足位置的实时更改、后续追踪等各种要求。</p>
                </div>
                <div class="p1">
                    <p id="54">传统图像拼接直接过渡到视频拼接存在很多问题<citation id="181" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>:每一帧独立的拼接和前后帧之间的关联较弱会导致时间上的不连续性, 拼接完成的视频存在严重的抖动;单纯的帧间约束能在一定程度上降低抖动, 但是会出现每一帧的匹配越来越不齐, 避免抖动同时保证对齐是当前需要考虑克服的一个难点, 而这个问题可以通过计算相机路径, 并且在时间和空间上同时优化相机路径来解决<citation id="182" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">2.1 <b>相机路径提取</b></h4>
                <div class="p1">
                    <p id="56">在图像处理中, 具有一定重叠面积的图片之间的对应关系可以用一个投影变换矩阵来表示, 相同的, 在同一个相机中第<i>t</i>帧的图片与第<i>t</i>+1帧的图片的对应关系可以用一个投影变换矩阵来表示:</p>
                </div>
                <div class="p1">
                    <p id="57"><b><i>I</i></b> (<i>t</i>) =<b><i>H</i></b> (<i>t</i>) *<b><i>I</i></b> (<i>t</i>-1)      (1) </p>
                </div>
                <div class="p1">
                    <p id="58">当从初始帧开始计算时, 可以得到以下公式:</p>
                </div>
                <div class="p1">
                    <p id="59"><b><i>I</i></b> (<i>t</i>) =<b><i>H</i></b> (<i>t</i>) *<b><i>H</i></b> (<i>t</i>-1) *…*<b><i>H</i></b> (1) *<b><i>I</i></b> (1)      (2) </p>
                </div>
                <div class="p1">
                    <p id="60">式 (2) 是初始帧到第<i>t</i>帧的对应关系, 在算法中把这个对应关系称之为相机运动路径。</p>
                </div>
                <div class="p1">
                    <p id="61">为了处理在实际应用中存在的视差情况, 以及相机在高空遇到的气流抖动造成视频源存在的抖动情况, 可以采取网格优化的方式, 将图像划分为<i>m</i><sub>1</sub>×<i>m</i><sub>2</sub>的网格, 每个网格有独立的运动路径。如图2所示。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 运动视频中提取出的相机运动路径" src="Detail/GetImg?filename=images/JSJY201905041_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 运动视频中提取出的相机运动路径  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Camera moving path extracted from motion video</p>

                </div>
                <h4 class="anchor-tag" id="63" name="63">2.2 <b>视频拼接与稳像</b></h4>
                <div class="p1">
                    <p id="64">在进行视频拼接时, 除了需要解决图像空间域的问题, 还需要解决视频时间域的连贯问题, 根据一系列的实验证明, 时间域和空间域的问题独立解决的结果并不理想, 而视频拼接与稳定同时进行时得到的结果往往更令人满意。</p>
                </div>
                <div class="p1">
                    <p id="65">对此视频拼接一般采取的解决方式是:在图像划分网格的情况下, 时间域平滑和空间域的拼接一起完成。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">2.2.1 空间对齐</h4>
                <div class="p1">
                    <p id="67">重叠区域的对齐<citation id="183" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>是拼接中最重要的一步。图3中黑色的点是不同相机<i>C</i>1和<i>C</i>2在同一时间<i>t</i>时获得的对应的特征点对, 这个特征点对分布在不同的网格, 对应帧之间的网格约束就是:网格<i>r</i>内所有的特征匹配点都可以用该网格的4个角点来表示:</p>
                </div>
                <div class="p1">
                    <p id="68"><i>P</i><sub><i>tr</i></sub>=<i>W</i><sub>1</sub>*<i>r</i><sub>00</sub>+<i>W</i><sub>2</sub>*<i>r</i><sub>01</sub>+<i>W</i><sub>3</sub>*<i>r</i><sub>10</sub>+<i>W</i><sub>4</sub>*<i>r</i><sub>11</sub>      (3) </p>
                </div>
                <div class="p1">
                    <p id="69">根据匹配点对之间的对应关系可以得到相应网格<i>r</i>与<i>r</i>′的对应关系:</p>
                </div>
                <div class="p1">
                    <p id="70"><i>r</i>′=<b><i>H</i></b><sub><i>r</i></sub>*<i>r</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="71">进一步可以获得不同摄像机C1和C2在<i>t</i>时刻分别获得的图像A和图像B中网格的对应关系。此时因为每个网格并不是独立的, 每个网格与其相邻的5或者8个网格之间都存在一个约束关系, 如图3中阴影面积的部分, 在具有这个约束关系的同时, 每个网格也都用式 (2) 计算出了一个变换后的矩阵, 由于它们相邻, 在变换后也应该是相邻的, 但是计算后发现并不完全相邻, 最终可以获得一个误差<b><i>E</i></b><sub><i>r</i></sub>。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 投影变换与网格约束" src="Detail/GetImg?filename=images/JSJY201905041_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 投影变换与网格约束  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Projection transformation and grid constraints</p>

                </div>
                <h4 class="anchor-tag" id="73" name="73">2.2.2 时域空间的稳定</h4>
                <div class="p1">
                    <p id="74">一方面, 通过相机路径获取的方式获取了每个网格的运动路径, 如图4所示为每个网格对应的运动路径, 抖动较为强烈的路径是计算出的相机真实运动路径, 较为平滑的路径是研究中希望获得的理想运动路径, 理论上来说对初始的相机路径进行低通滤波后可以获得相对平稳一些的相机运动路径。另一方面, 根据空间对齐中网格的约束关系同样可以获得同个相机<i>C</i>在不同时刻所拍摄的图片中的网格对应误差关系<b><i>E</i></b><sub><i>t</i></sub>。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 视频中每个网格的运动路径" src="Detail/GetImg?filename=images/JSJY201905041_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 视频中每个网格的运动路径  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Moving path of each grid in the video</p>

                </div>
                <div class="p1">
                    <p id="76">在两项误差<b><i>E</i></b><sub><i>r</i></sub>和<b><i>E</i></b><sub><i>t</i></sub>优化下降到最低时, 所获得的相机路径就是最终投影变换后的理想相机路径。</p>
                </div>
                <h3 id="77" name="77" class="anchor-tag">3 提出的算法</h3>
                <div class="p1">
                    <p id="78">由前文的视频拼接介绍可知:想要获得相机路径和网格变换关系, 就要一定程度上保证特征点对匹配的数目较多且尽量均匀分布, 还要保证特征点对匹配的正确率, 传统的特征匹配方法难以达到这个要求, 所以为了达到这个目的, 本文提出了基于特征点评分的匹配模型 (<i>Matching Model based on Grayscale Tower score</i>, <i>MMGT</i>) 构建的特征匹配方式。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">3.1 <b>特征点评分</b></h4>
                <div class="p1">
                    <p id="80">对输入图像进行特征提取时, 优秀的特征点提取方式提取的特征点具有尺度不变性和旋转不变性, 但是提取出的特征点包含噪点, 数目巨大, 因为光照影响并没有完整的评分机制, 从而对最终的匹配结果可信度和正确率造成影响。如果可以对特征点进行评分, 那么评分较高的特征点匹配结果的可信度上升, 在后续构建匹配模型时, 匹配模型的可信度也会提高, 从而作用到最终的匹配结果上。引入灰度级下降的特征点的评分方式可以在一定程度上解决光照和错误特征点的影响。</p>
                </div>
                <div class="p1">
                    <p id="81">灰度图像中一般具有256个灰度级, 在传统的特征点提取过程中, 首先构建高斯金字塔, 在高斯差分图像中与其周围相邻的8个特征点和上下相邻空间中的18个特征点比较灰度值, 极值点就是最终获得的特征点, 此时的特征点具有尺度不变性。特征点定义为图像中特殊的点, 理想的特征点不会随着尺度和光照的变换而消失, 也就是说当压缩灰度级时, 例如整体图像的灰度级由250变成128, 邻近的灰度级会合并, 但是优秀的特征点仍然存在。</p>
                </div>
                <div class="p1">
                    <p id="82">首先构建高斯差分金字塔, 比较其不同尺度空间的像素值后可以获取到初始提取的特征点集<b><i>P</i></b> (<i>n</i>) 和<b><i>P</i></b>′ (<i>n</i>) 。然后在这个特征点集中的每个特征点周围取一个半径为<i>R</i>的圆, 圆内的像素称之为光照关联像素, 此时需要构建一个尺度不变的光照金字塔:1) 获取特征点周围光照信息关联区域的直方图;2) 模拟光照, 在不同的倍数下压缩光照信息关联区域的整体灰度级数目;3) 比较不同灰度级金字塔上每一层的关联区域的灰度下降梯度信息, 当梯度变换低于一定阈值后认为该特征点消失;4) 特征点<b><i>P</i></b>存在在金字塔上的层数越多, 则认为这个特征点的评分越高, 同样该特征点的质量越高。如图5所示。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">3.2 <b>匹配模型的构建</b></h4>
                <div class="p1">
                    <p id="84">经过上述评分的步骤可以获得带有评分的特征点集<b><i>P</i></b> (<i>n</i>) 和<b><i>P</i></b>′ (<i>n</i>) , 评分越高代表特征点越可靠。下一步需要依据这个评分来构建一个匹配模型, 增加特征匹配对的数目和正确率, 避免由于噪点引起的大误差匹配。</p>
                </div>
                <div class="p1">
                    <p id="85">首先对特征点的评分进行排序, 确定一个评分阈值, 选取评分高于这个阈值的特征点归类到集合<b><i>P</i></b><sub><i>h</i></sub> (<i>i</i>) (<i>i</i>=1, 2, …, <i>n</i>;<i>n</i>≥4) , 这个集合是筛选出的高评分特征点集合, 集合内特征点的准确性非常适合用于构建匹配模型。同样对于图B也可计算出一个高评分的特征点集<b><i>P</i></b><sub><i>h</i></sub>′ (<i>i</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="86">其次需要构建匹配模型, 匹配模型可以定义为一个函数Match, 使得:输入两个图像的待匹配点集<b><i>P</i></b> (<i>n</i>) 和<b><i>P</i></b>′ (<i>n</i>) 就可以得到最终的输出匹配对点集<i>O</i> (<i>n</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="87"><i>O</i> (<i>n</i>) =Match (<b><i>P</i></b> (<i>n</i>) , <b><i>P</i></b>′ (<i>n</i>) )      (5) </p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 灰度塔" src="Detail/GetImg?filename=images/JSJY201905041_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 灰度塔  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Grayscale tower</p>

                </div>
                <h4 class="anchor-tag" id="89" name="89">3.2.1 锁定匹配块</h4>
                <div class="p1">
                    <p id="90">匹配块为图<i>A</i>中每个点在图<i>B</i>中的待匹配块, 该匹配块是一个半径为<i>r</i><sub><i>d</i></sub>的圆。匹配块的构建过程为:首先依据两张图像中获得的高评分匹配集合<b><i>P</i></b><sub><i>h</i></sub>和<b><i>P</i></b><sub><i>h</i></sub>′通过RANSAC的方式计算出一个匹配对集和<b><i>P</i></b><sub><i>m</i></sub>和投影变换矩阵<b><i>H</i></b><sub><i>c</i></sub>, 满足式 (6) , <b><i>P</i></b><sub><i>h</i></sub>′ (<i>i</i>) 和<b><i>P</i></b><sub><i>h</i></sub> (<i>i</i>′) 为匹配对集合<b><i>P</i></b><sub><i>m</i></sub>中的对应匹配对。</p>
                </div>
                <div class="p1">
                    <p id="91"><b><i>P</i></b><sub><i>h</i></sub>′ (<i>i</i>) =<b><i>H</i></b><sub><i>c</i></sub>*<b><i>P</i></b><sub><i>h</i></sub> (<i>i</i>′)      (6) </p>
                </div>
                <div class="p1">
                    <p id="92">此时获得了两张待拼接图片中特征点的大概对应关系, 也就是说通过这个对应关系的变换能够获取图<i>A</i>中的某一个特征点在图<i>B</i>中对应特征点的大概位置:</p>
                </div>
                <div class="p1">
                    <p id="93"><b><i>D</i></b><sub><i>i</i></sub> (<i>j</i>) =<b><i>D</i></b><sub><i>o</i></sub> (<b><i>P</i></b>′ (<i>j</i>) , <b><i>H</i></b><sub><i>c</i></sub>*<b><i>P</i></b> (<i>i</i>) ) ;</p>
                </div>
                <div class="p1">
                    <p id="94"><i>i</i>=1, 2, …, <i>n</i><sub>1</sub>, <i>j</i>=1, 2, …, <i>n</i><sub>2</sub>       (7) </p>
                </div>
                <div class="p1">
                    <p id="95"><b><i>D</i></b><sub><i>o</i></sub>为两个特征点之间的欧氏距离, <b><i>D</i></b><sub><i>i</i></sub> (<i>n</i>) 中距离小于最终半径<i>r</i><sub><i>d</i></sub>所对应的初始特征点集<b><i>P</i></b>′ (<i>n</i>) 中的特征点, 构成最终锁定的匹配块<b><i>P</i></b><sub><i>i</i></sub>′ (<i>x</i>) , 注意, 当点集<b><i>P</i></b><sub><i>i</i></sub>′ (<i>x</i>) 中特征点的数目大于5时, 才认为<b><i>P</i></b><sub><i>i</i></sub>′ (<i>x</i>) 是特征点<b><i>P</i></b> (<i>i</i>) 对应的匹配块。</p>
                </div>
                <div class="p1">
                    <p id="96">锁定匹配块的部分参考图6所示, 本方法避免了每个特征点需要跟对应待匹配图全图的特征点一一匹配, 加快了匹配速度, 减少全局特征点的影响, 并且避免了噪点引起的大误差匹配。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 锁定匹配块" src="Detail/GetImg?filename=images/JSJY201905041_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 锁定匹配块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Locking matched blocks</p>

                </div>
                <h4 class="anchor-tag" id="98" name="98">3.2.2 精细匹配</h4>
                <div class="p1">
                    <p id="99">在锁定匹配块之后, 图<i>A</i>中每个特征点<b><i>P</i></b> (<i>i</i>) 对应图<i>B</i>中匹配块点集<b><i>P</i></b><sub><i>i</i></sub>′ (<i>x</i>) , <b><i>P</i></b><sub><i>i</i></sub>′ (<i>x</i>) 是<b><i>P</i></b>′ (<i>n</i>) 的一个子集。此时只需要将特征点<b><i>P</i></b> (<i>i</i>) 和<b><i>P</i></b><sub><i>i</i></sub>′ (<i>x</i>) 进行匹配即可, 匹配过程采取<i>K</i>NN的匹配方式获得每个特征点的描述子<b><i>D</i></b><sub><i>c</i></sub> (<i>point</i>) , 计算描述子之间的欧氏距离:</p>
                </div>
                <div class="p1">
                    <p id="100"><b><i>D</i></b><sub><i>c</i></sub> (<i>i</i>, <i>j</i>) = <b><i>D</i></b><sub><i>o</i></sub> (<b><i>D</i></b><sub><i>c</i></sub> (<b><i>P</i></b> (<i>i</i>) ) , <b><i>D</i></b><sub><i>c</i></sub> (<b><i>P</i></b><sub><i>i</i></sub>′ (<i>j</i>) ) )      (8) </p>
                </div>
                <div class="p1">
                    <p id="101">对每个特征点<b><i>P</i></b> (<i>i</i>) 对应的<b><i>D</i></b><sub><i>c</i></sub> (<i>i</i>, <i>j</i>) 中的每个距离值从小到大排序, 只保留最大距离和次大距离, 然后进行非极大值抑制:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munderover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mtext>f</mtext><mtext>i</mtext><mtext>r</mtext><mtext>s</mtext><mtext>t</mtext></mrow><mspace width="0.25em" /></munderover><mspace width="0.25em" /><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mo>≤</mo><mn>0</mn><mo>.</mo><mn>8</mn><mo>*</mo><munderover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mtext>s</mtext><mtext>e</mtext><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>d</mtext></mrow><mspace width="0.25em" /></munderover><mspace width="0.25em" /><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">如果两个特征点满足式 (9) 就可以认为特征点<b><i>P</i></b> (<i>i</i>) 和<b><i>P</i></b><sub><i>i</i></sub>′ (<i>j</i>) 是一对特征匹配对。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">3.3 <b>运动视频的连续帧提取</b></h4>
                <div class="p1">
                    <p id="105">当匹配模型应用到运动视频的特征匹配中时, 本文方法可以依据运动视频的前后帧对应关系来减少部分运算。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">3.3.1 感兴趣区域的遗传</h4>
                <div class="p1">
                    <p id="107">在第<i>t</i>帧计算出的特征匹配对点集为<b><i>M</i></b><sub><i>c</i></sub> (<i>n</i>) , 可以找到图<i>A</i>和图<i>B</i>中位于4个顶点的特征匹配点, 得到每张图特征匹配点中横坐标和纵坐标的极值:<i>x</i><sub>max</sub>、<i>x</i><sub>min</sub>、<i>y</i><sub>max</sub>、<i>y</i><sub>min</sub>。从极值中可以计算两张图像的重叠区域, 构建出一个掩模。在<i>t</i>+1帧中只对掩模内的区域进行特征提取和特征匹配, 这种方式在一定程度上加快计算速度, 避免计算空间的浪费。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">3.3.2 匹配模型的遗传</h4>
                <div class="p1">
                    <p id="109">由于视频中相邻帧的信息更新并不大, 在<i>t</i>帧中获得的匹配模型<i>M</i>, 可以将<i>t</i>帧中计算出的匹配模型<i>M</i>经过相机路径的计算遗传给<i>t</i>+1帧, 已知对应关系:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>*</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>A</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msub></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>A</mi><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>A</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>A</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msub></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msub></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">计算可以获得如下对应关系:</p>
                </div>
                <div class="p1">
                    <p id="112"><b><i>I</i></b><sub><i>B</i> (<i>t</i>+1) </sub>=<b><i>H</i></b><sub><i>B</i> (<i>t</i>) </sub>*<b><i>M</i></b><sub><i>t</i></sub>*<b><i>H</i></b><sub><i>A</i> (<i>t</i>) </sub><sup>-1</sup>*<b><i>I</i></b><sub><i>A</i> (<i>t</i>+1) </sub>      (11) </p>
                </div>
                <div class="p1">
                    <p id="113">则计算出第<i>t</i>+1帧的匹配模型:</p>
                </div>
                <div class="p1">
                    <p id="114"><b><i>M</i></b><sub><i>t</i></sub>=<b><i>H</i></b><sub><i>B</i> (<i>t</i>) </sub>*<b><i>M</i></b><sub><i>t</i></sub>*<b><i>H</i></b><sub><i>A</i> (<i>t</i>) </sub><sup>-1</sup>      (12) </p>
                </div>
                <div class="p1">
                    <p id="115">同理, 根据相机路径匹配模型可以继续遗传。但是, 前帧信息并不是一直能遗传下去, 每一帧之间的误差在累积数帧之后会发生误差爆炸, 所以匹配模型建议3～5帧更新一次。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 感兴趣区域计算" src="Detail/GetImg?filename=images/JSJY201905041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 感兴趣区域计算  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Region of interest calculation</p>

                </div>
                <h3 id="117" name="117" class="anchor-tag">4 实验与分析</h3>
                <div class="p1">
                    <p id="118">为了在户外运动视频拼接系统中提供一种鲁棒的方法, 本文对匹配模型的性能进行了评估, 利用采集的视频源, 在对视频源中的每一帧图像用<i>SURF</i>算法进行特征提取的基础上, 将本文算法与K<i>NN</i>匹配算法进行了比较。</p>
                </div>
                <div class="p1">
                    <p id="119">图8 (<i>a</i>) 表示用<i>SURF</i>算法提取和K<i>NN</i>匹配结果中取10%的最终结果, K<i>NN</i>匹配参数设置为0.8, 图8 (<i>b</i>) 表示通过匹配模型进行匹配的结果。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 两种算法匹配结果对比" src="Detail/GetImg?filename=images/JSJY201905041_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 两种算法匹配结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Matching results comparison of two algorithm</i></p>

                </div>
                <div class="p1">
                    <p id="121">数据设定 灰度塔构建层数为4, 分别为256个灰度级、192个灰度级、128个灰度级和64个灰度级。匹配模型中块的大小定义为一个半径为20像素的圆, 块内特征点数目大于5则认为这个块是有效匹配块, 为了更具有鲁棒性, 感兴趣区域的边缘选择扩展20像素, 匹配模型4帧更新一次, 最终取匹配结果中的40%作为最终匹配结果。</p>
                </div>
                <div class="p1">
                    <p id="122">表1为在同一视频源下连续五帧对匹配对数目、匹配正确率以及运算时间的对比。图9为在不同的两个视频源下的实验结果, 其中视频源1为以上图片中相同的视频源, 视频源2为清华大学物理楼的视频源, 在视频源2中无人机运动速度较快且两个无人机的相对运动速度也较快。以下结果均为在<i>CPU</i>上单核运算的结果。</p>
                </div>
                <div class="area_img" id="123">
                    <p class="img_tit"><b>表</b>1 <i>MMGT</i><b>与</b>K<i>NN</i><b>匹配算法性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Performance comparison between</i><i>MMGT and</i> K<i>NN matching method</i></p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td><br />帧号</td><td>算法</td><td>匹配对数目</td><td>匹配正确率/%</td><td>运算时间/<i>s</i></td></tr><tr><td rowspan="2"><br />1</td><td><br />K<i>NN</i></td><td>31</td><td>75</td><td>0.63</td></tr><tr><td><br /><i>MMGT</i></td><td>357</td><td>95</td><td>1.27</td></tr><tr><td rowspan="2"><br />2</td><td><br />K<i>NN</i></td><td>22</td><td>82</td><td>0.24</td></tr><tr><td><br /><i>MMGT</i></td><td>245</td><td>97</td><td>0.16</td></tr><tr><td rowspan="2"><br />3</td><td><br />K<i>NN</i></td><td>24</td><td>83</td><td>0.32</td></tr><tr><td><br /><i>MMGT</i></td><td>260</td><td>92</td><td>0.11</td></tr><tr><td rowspan="2"><br />4</td><td><br />K<i>NN</i></td><td>23</td><td>84</td><td>0.19</td></tr><tr><td><br /><i>MMGT</i></td><td>234</td><td>99</td><td>0.09</td></tr><tr><td rowspan="2"><br />5</td><td><br />K<i>NN</i></td><td>25</td><td>80</td><td>0.23</td></tr><tr><td><br /><i>MMGT</i></td><td>223</td><td>96</td><td>0.49</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 两种算法在两个视频源下的匹配对数目对比" src="Detail/GetImg?filename=images/JSJY201905041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 两种算法在两个视频源下的匹配对数目对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 9 <i>Comparison of matching pairs by</i><i>two algorithms in different video cases</i></p>

                </div>
                <div class="p1">
                    <p id="125">从实验结果可以看出, 本文方法可以适用于不同环境下的特征匹配, 具有匹配率高、正确率也比较高的特点, 提取的时间也相应地增加, 但是由于在视频流中的应用中, 信息的遗传机制, 可以在视频流中减少运算时间, 弥补时间方面上的限制。</p>
                </div>
                <h3 id="126" name="126" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="127">本文对多节点的<i>UAV</i>网络采取高空视频并进行实时拼接成全景视频特征匹配进行了深入的研究, 提出了基于灰度塔进行特征点评分, 并构建匹配模型的特征匹配方法, 很好地解决了特征点提取的数目不够且分布不够均匀的问题, 并利用视频流中信息的遗传机制, 将匹配模型和感兴趣区域进行遗传, 取得了很好的效果。但是该方法在实时性上还有一些不足, 今后的研究应该是如何对匹配模型匹配过程中的一些互相不相关的过程进行多线程并行处理加快匹配速度。另外在灰度塔的应用上还可以进行扩展, 但是要注意在图像分割中伪轮廓的处理。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="141">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spherical video panorama stitching from multiple cameras with intersecting fields of view and inertial measurement unit">

                                <b>[1]</b> IVAN S K, OLEG V P.Spherical video panorama stitching from multiple cameras with intersecting fields of view and inertial measurement unit[C]// Proceedings of the 2016 International Siberian Conference on Control and Communications.Piscataway, NJ:IEEE, 2016:1-6.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDA7B9F8A7E11F5478699913E4C134A8C&amp;v=MDc2MjlMcSt3NkU9TmlmT2ZjZkpHYVBGMlljMFk1NE9EUW84eXhFYjdEWjBRWDdoMlJaR2VMR1FOTExzQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> ZHANG L, HE Z, LIU Y.Deep object recognition across domains based on adaptive extreme learning machine[J].Neurocomputing, 2017, 239:194-203.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic registration of large-scale urban scene point clouds based on semantic feature points">

                                <b>[3]</b> YANG B, DONG Z, LIANG F, et al.Automatic registration of large-scale urban scene point clouds based on semantic feature points[J].ISPRS Journal of Photogrammetry &amp; Remote Sensing, 2016, 113:43-58.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=Mjc5NTlsVzczSkkxYz1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rmly&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> LOWE D G.Distinctive image features from scale-invariant key-points[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rat brain registration using improved speeded up robust features">

                                <b>[5]</b> HSU W Y, LEE Y C.Rat brain registration using improved speeded up robust features[J].Journal of Medical and Biological Engineering, 2017, 37 (1) :45-42.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB:an efficient alternative to SIFT or SURF">

                                <b>[6]</b> RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]// Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:2564-2571.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BRISK:Binary Robust invariant scalable keypoints">

                                <b>[7]</b> LEUTENEGGER S, CHLI M, SIEGWART Y.BRISK:binary robust invariant scalable keypoints[C]// Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:2548-2555.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BRIEF:Binary robust independent elementary features">

                                <b>[8]</b> CALONDER M, LEPETIT V, STRECHA C, et al.BRIEF:binary robust independent elementary features[C]// Proceedings of the 11th European Conference on Computer Vision.Berlin:Springer, 2010:778-792.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000025185&amp;v=MTY3OTU9TmlmSVk3SzdIdGpOcjQ5RlpPa0tEWFE4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3VmFScw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> MOR M, FRAENKEL A S.A hash code method for detecting and correcting spelling-errors[J].Communications of the ACM, 1982, 25 (12) :935-938.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The significance of real-time,biomedical and satellite image processing in understanding the objects &amp;amp; application to computer vision">

                                <b>[10]</b> SANTHA T, MOHANA M B V.The significance of real-time, biomedical and satellite image processing in understanding the objects &amp; application to computer vision[C]// Proceedings of the 2nd IEEE International Conference on Engineering &amp; Technology.Piscataway, NJ:IEEE, 2016:661-670.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831131&amp;v=MTE4OTF1RmlybFc3M0pJMWM9Tmo3QmFyTzRIdEhPcDR4RVplZ09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWita&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> BROWN M, LOWE D G.Automatic panoramic image stitching using invariant features[J].International Journal of Computer Vision, 2007, 74 (1) :59-73.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint video stitching and stabilization from moving cameras">

                                <b>[12]</b> GUO H, LIU S, HE T, et al.Joint video stitching and stabilization from moving cameras[J].IEEE Transactions on Image Processing, 2016, 25 (11) :5491-5503.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC200409000&amp;v=MjE1MTdCdEdGckNVUjdxZlp1WnNGeURuVXJ6Qklpbk1iYkc0SHRYTXBvOUZaSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 倪国强, 刘琼.多源图像配准技术分析与展望[J].光电工程, 2004, 31 (9) :1-6. (NI G Q, LIU Q.Analysis and prospect of multi-source image registration techniques[J].Opto-Electronic Engineering, 2004, 31 (9) :1-6.) 
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=As-projective-as-possible image stitching with moving DLT">

                                <b>[14]</b> ZARAGOZA J, CHIN T J, BROWN M S, et al.As-projective-as-possible image stitching with moving DLT[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 36 (7) :1285-1298.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200608015&amp;v=MjQ2NTFzRnlEblVyekJQeXJmYkxHNEh0Zk1wNDlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 朱云芳, 叶秀清, 顾伟康.视频序列的全景图拼接技术[J].中国图象图形学报, 2006, 11 (8) :1150-1155. (ZHU Y F, YE X Q, GU W K.Mosaic panorama technique for videos[J].Journal of Image and Graphics, 2006, 11 (8) :1150-1155.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905041" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905041&amp;v=MjIwNTh0R0ZyQ1VSN3FmWnVac0Z5RG5VcnpPTHo3QmQ3RzRIOWpNcW85QlpZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
