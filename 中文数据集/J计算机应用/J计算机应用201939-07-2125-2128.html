<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136667083440000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907044%26RESULT%3d1%26SIGN%3dFEzY2GaCxURua2Mc15lQEQjcSP0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907044&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907044&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907044&amp;v=MjM4MzNJOUJZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVjcvS0x6N0JkN0c0SDlqTXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="1 深度卷积神经网络 ">1 深度卷积神经网络</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="2 基于深度学习的织物缺陷检测算法 ">2 基于深度学习的织物缺陷检测算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="2.1 &lt;b&gt;基于深度学习的织物缺陷检测算法流程&lt;/b&gt;">2.1 <b>基于深度学习的织物缺陷检测算法流程</b></a></li>
                                                <li><a href="#49" data-title="2.2 &lt;b&gt;缺陷分类模型&lt;/b&gt;">2.2 <b>缺陷分类模型</b></a></li>
                                                <li><a href="#52" data-title="2.3 &lt;b&gt;数据增强&lt;/b&gt;">2.3 <b>数据增强</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="3.1 &lt;b&gt;实验环境&lt;/b&gt;">3.1 <b>实验环境</b></a></li>
                                                <li><a href="#59" data-title="3.2 &lt;b&gt;训练&lt;/b&gt;">3.2 <b>训练</b></a></li>
                                                <li><a href="#72" data-title="3.3 &lt;b&gt;结果与分析&lt;/b&gt;">3.3 <b>结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;三个经典分类网络对比&lt;/b&gt;"><b>表</b>1 <b>三个经典分类网络对比</b></a></li>
                                                <li><a href="#46" data-title="图1 Inception模块">图1 Inception模块</a></li>
                                                <li><a href="#47" data-title="图2 残差模块">图2 残差模块</a></li>
                                                <li><a href="#48" data-title="图3 基于深度学习的织物缺陷检测算法流程">图3 基于深度学习的织物缺陷检测算法流程</a></li>
                                                <li><a href="#54" data-title="图4 缺陷分类模型">图4 缺陷分类模型</a></li>
                                                <li><a href="#55" data-title="图5 原图的预处理">图5 原图的预处理</a></li>
                                                <li><a href="#71" data-title="图6 学习率随迭代次数变化情况">图6 学习率随迭代次数变化情况</a></li>
                                                <li><a href="#76" data-title="图7 不同模型的准确率对比">图7 不同模型的准确率对比</a></li>
                                                <li><a href="#77" data-title="图8 不同模型损失随时间变化情况对比">图8 不同模型损失随时间变化情况对比</a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;测试时间对比分析&lt;/b&gt;"><b>表</b>2 <b>测试时间对比分析</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="99">


                                    <a id="bibliography_1" title=" HARALICK R M, SHANGMUGAN K, DINSTEIN I.Textural features for image classification[J].IEEE Transactions on Systems, Man, and Cybernetics, 1973, 3 (6) :610-621." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Textural features for image classification">
                                        <b>[1]</b>
                                         HARALICK R M, SHANGMUGAN K, DINSTEIN I.Textural features for image classification[J].IEEE Transactions on Systems, Man, and Cybernetics, 1973, 3 (6) :610-621.
                                    </a>
                                </li>
                                <li id="101">


                                    <a id="bibliography_2" title=" NGAN H Y T, PANG G K H, YUNG S P, et al.Wavelet based methods on patterned fabric defect detection[J].Pattern Recognition, 2005, 38 (4) :559-576." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740203&amp;v=MDQxMDM2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSjFzUWFoQT1OaWZPZmJLN0h0RE5xWTlGWSs4UERudw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         NGAN H Y T, PANG G K H, YUNG S P, et al.Wavelet based methods on patterned fabric defect detection[J].Pattern Recognition, 2005, 38 (4) :559-576.
                                    </a>
                                </li>
                                <li id="103">


                                    <a id="bibliography_3" title=" 张杨, 蒋高明, 姚君洲, 等.基于MRF层次模型的贾失经编针织物图像分割技术[J].纺织学报, 2012, 33 (12) :102-106. (ZHANG Y, JIANG G M, YAO J Z, et al.Segmentation of jacquard warp-knitted fabric image based on hierarchical Markov random field model[J].Journal of Textile Research, 2013, 33 (12) :102-106.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FZXB201212022&amp;v=MDI0NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVjcvS0l6ZlRiTEc0SDlQTnJZOUhab1E=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         张杨, 蒋高明, 姚君洲, 等.基于MRF层次模型的贾失经编针织物图像分割技术[J].纺织学报, 2012, 33 (12) :102-106. (ZHANG Y, JIANG G M, YAO J Z, et al.Segmentation of jacquard warp-knitted fabric image based on hierarchical Markov random field model[J].Journal of Textile Research, 2013, 33 (12) :102-106.) 
                                    </a>
                                </li>
                                <li id="105">


                                    <a id="bibliography_4" title=" SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, Inception-ResNet and the impact of residual connections on learning [EB/OL] (2016- 08- 23) [2018- 10- 15].https://arxiv.org/pdf/1602.07261.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning[C/OL]">
                                        <b>[4]</b>
                                         SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, Inception-ResNet and the impact of residual connections on learning [EB/OL] (2016- 08- 23) [2018- 10- 15].https://arxiv.org/pdf/1602.07261.pdf.
                                    </a>
                                </li>
                                <li id="107">


                                    <a id="bibliography_5" title=" LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector [EB/OL].[2018- 10- 15] https://arxiv.org/pdf/1512.02325.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">
                                        <b>[5]</b>
                                         LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector [EB/OL].[2018- 10- 15] https://arxiv.org/pdf/1512.02325.pdf.
                                    </a>
                                </li>
                                <li id="109">


                                    <a id="bibliography_6" title=" CHEN L C, PAPANDREOU G, KOKKINOS I, et al.DeepLab:semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 40 (4) :834-848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">
                                        <b>[6]</b>
                                         CHEN L C, PAPANDREOU G, KOKKINOS I, et al.DeepLab:semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 40 (4) :834-848.
                                    </a>
                                </li>
                                <li id="111">


                                    <a id="bibliography_7" title=" KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]// Proceedings of the 9th International Conference on Neural Information Processing Systems.Berlin:Springer, 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with DeepConvolutional Neural Networks">
                                        <b>[7]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]// Proceedings of the 9th International Conference on Neural Information Processing Systems.Berlin:Springer, 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="113">


                                    <a id="bibliography_8" title=" SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL]. (2015- 04- 10) [2018- 10- 15].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[8]</b>
                                         SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL]. (2015- 04- 10) [2018- 10- 15].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="115">


                                    <a id="bibliography_9" title=" SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions [EB/OL]. (2014- 07- 17) [2016- 12- 10].https://arxiv.org/abs/1409.4842." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">
                                        <b>[9]</b>
                                         SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions [EB/OL]. (2014- 07- 17) [2016- 12- 10].https://arxiv.org/abs/1409.4842.
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_10" title=" HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[EB/OL]. (2015- 12- 10) [2018- 10- 15].https://arxiv.org/abs/1512.03- 385." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[10]</b>
                                         HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[EB/OL]. (2015- 12- 10) [2018- 10- 15].https://arxiv.org/abs/1512.03- 385.
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_11" title=" LeCUN Y, BOTTOU I, BENGIO Y, et al.Gradient-based learning applied do document recognition [J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradient-based learning applied to document recognition">
                                        <b>[11]</b>
                                         LeCUN Y, BOTTOU I, BENGIO Y, et al.Gradient-based learning applied do document recognition [J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_12" title=" JARRETT K, KAVUKCUOGLU K, RANZATO M, et al.What is the best multi-stage architecture for object recognition?[EB/OL] [2018- 10- 15].http://yann.lecun.com/exdb/publis/orig/jarrett-iccv- 09.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What is the best multi-stage architecture for object recognition?">
                                        <b>[12]</b>
                                         JARRETT K, KAVUKCUOGLU K, RANZATO M, et al.What is the best multi-stage architecture for object recognition?[EB/OL] [2018- 10- 15].http://yann.lecun.com/exdb/publis/orig/jarrett-iccv- 09.pdf.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_13" title=" LIN M, CHEN Q, YAN S.Network in network[EB/OL]. (2014- 03- 04) [2018- 10- 15].https://arxiv.org/abs/1312.4400." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network in network">
                                        <b>[13]</b>
                                         LIN M, CHEN Q, YAN S.Network in network[EB/OL]. (2014- 03- 04) [2018- 10- 15].https://arxiv.org/abs/1312.4400.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_14" title=" HE K, ZHANG X, REN S, et al.Identity mappings in deep residual networks [EB/OL]. (2016- 07- 25) [2018- 10- 15].http://arxiv.org/obs/1603.05027." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">
                                        <b>[14]</b>
                                         HE K, ZHANG X, REN S, et al.Identity mappings in deep residual networks [EB/OL]. (2016- 07- 25) [2018- 10- 15].http://arxiv.org/obs/1603.05027.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_15" title=" IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [EB/OL].[2018- 10- 15].http//arxiv.org/pdf/1502.03167.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">
                                        <b>[15]</b>
                                         IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [EB/OL].[2018- 10- 15].http//arxiv.org/pdf/1502.03167.pdf.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_16" title=" BOTTOU L.Large-scale machine learning with stochastic gradient descent [C]// Proceedings of the 19th International Conference on Computational Statistics.Berlin:Springer, 2010:177-186." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale machine learning with stochastic gradient descent">
                                        <b>[16]</b>
                                         BOTTOU L.Large-scale machine learning with stochastic gradient descent [C]// Proceedings of the 19th International Conference on Computational Statistics.Berlin:Springer, 2010:177-186.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_17" title=" RUDER S.An overview of gradient descent optimization algorithms [EB/OL]. (2017- 06- 15) [2018- 10- 15].https://arxiv.org/abs/1609.04747." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An overview of gradient descent optimization algorithms">
                                        <b>[17]</b>
                                         RUDER S.An overview of gradient descent optimization algorithms [EB/OL]. (2017- 06- 15) [2018- 10- 15].https://arxiv.org/abs/1609.04747.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-28 15:21</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),2125-2128 DOI:10.11772/j.issn.1001-9081.2019010110            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的织物缺陷在线检测算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%90%86%E9%A1%BA&amp;code=42202208&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王理顺</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%9F%E5%8B%87&amp;code=09598229&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钟勇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%8C%AF%E4%B8%9C&amp;code=36173882&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李振东</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BA%E5%AE%9C%E9%BE%99&amp;code=42202210&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贺宜龙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E6%88%90%E9%83%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0211714&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院成都计算机应用研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>织物缺陷在线检测是纺织行业面临的重大难题, 针对当前织物缺陷检测中存在的误检率高、漏检率高、实时性不强等问题, 提出了一种基于深度学习的织物缺陷在线检测算法。首先基于GoogLeNet网络架构, 并参考其他分类模型的经典算法, 搭建出适用于实际生产环境的织物缺陷分类模型;其次利用质检人员标注的不同种类织物图片组建织物缺陷数据库, 并用该数据库对织物缺陷分类模型进行训练;最后对高清相机在织物验布机上采集的图片进行分割, 并将分割后的小图以批量的方式传入训练好的分类模型, 实现对每张小图的分类, 以此来检测缺陷并确定其位置。对该模型在织物缺陷数据库上进行了验证。实验结果表明:织物缺陷分类模型平均每张小图的测试时间为0.37 ms, 平均测试时间比GoogLeNet减少了67%, 比ResNet-50减少了93%;同时模型在测试集上的正确率达到99.99%。说明其准确率与实时性均满足实际工业需求。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">缺陷检测;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王理顺 (1992—) , 男, 重庆奉节人, 硕士研究生, 主要研究方向:大数据智能分析、机器视觉、人工智能;电子邮箱690332121@qq.com;
                                </span>
                                <span>
                                    钟勇 (1966—) , 男, 四川岳池人, 研究员, 博士, 主要研究方向:大数据、云计算、数据库;;
                                </span>
                                <span>
                                    李振东 (1990—) , 男, 宁夏吴忠人, 博士研究生, 主要研究方向:机器学习、图像处理;;
                                </span>
                                <span>
                                    贺宜龙 (1992—) , 男, 山东济宁人, 硕士研究生, 主要研究方向:数据挖掘、图像处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>四川省科技厅项目 (2018GZ0231);</span>
                    </p>
            </div>
                    <h1><b>On-line fabric defect recognition algorithm based on deep learning</b></h1>
                    <h2>
                    <span>WANG Lishun</span>
                    <span>ZHONG Yong</span>
                    <span>LI Zhendong</span>
                    <span>HE Yilong</span>
            </h2>
                    <h2>
                    <span>Chengdu Institute of Computer Applications, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>On-line detection of fabric defects is a major problem faced by textile industry. Aiming at the problems such as high false positive rate, high false negative rate and low real-time in the existing detection of fabric defects, an on-line detection algorithm for fabric defects based on deep learning was proposed. Firstly, based on GoogLeNet network architecture, and referring to classical algorithm of other classification models, a fabric defect classification model suitable for actual production environment was constructed. Secondly, a fabric defect database was set up by using different kinds of fabric pictures marked by quality inspectors, and the database was used to train the fabric defect classification model. Finally, the images collected by high-definition camera on fabric inspection machine were segmented, and the segmented small images were sent to the trained classification model in batches to realize the classification of each small image. Thereby the defects were detected and their positions were determined. The model was validated on a fabric defect database. The experimental results show that the average test time of each small picture is 0.37 ms by this proposed model, which is 67% lower than that by GoogLeNet, 93% lower than that by ResNet-50, and the accuracy of the proposed model is 99.99% on test set, which shows that its accuracy and real-time performance meet actual industrial demands.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=defect%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">defect detection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Lishun, born in 1992, M. S. candidate. His research interests include intelligent analysis of big data, machine vision, artificial intelligence. ;
                                </span>
                                <span>
                                    ZHONG Yong, born in 1966, Ph. D. , research fellow. His research interests include big data, cloud computing, database. ;
                                </span>
                                <span>
                                    LI Zhendong, born in 1990, Ph. D. candidate. His research interest is machine learning, image processing. ;
                                </span>
                                <span>
                                    HE Yilong, born in 1992, M. S. candidate. His research interests include data mining, image processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-17</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Sichuan Science and Technology Department Project (2018GZ0231);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="38">实现织物缺陷在线实时检测对于加速纺织行业自动化改造、节约生产成本、提高生产效率、实现产业升级具有重要意义。目前织物缺陷检测主要有人工检测和基于纹理分析方式检测。其中人工检测方式存在高成本、低效率且高误检率、高漏检率等一系列问题。基于纹理分析方式检测的方法主要有Haralick等<citation id="133" type="reference"><link href="99" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出的基于二阶灰度统计特征的共生矩阵方法、在频率域提取特征的小波变换法<citation id="134" type="reference"><link href="101" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、基于模型算法的马尔可夫随机场法<citation id="135" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 这些方法为实现织物缺陷自动检测作出重要贡献, 但由于实际生产环境中的织物种类多, 缺陷类型变化大, 基于纹理分析检测缺陷的方法由人工设计的特征不能完全覆盖所有缺陷的特点, 因此, 这些模型的准确率和泛化能力较弱, 很难满足实际工业要求。</p>
                </div>
                <div class="p1">
                    <p id="39">近年来随着深度学习的发展, 卷积神经网络 (Convolutional Neural Network, CNN) 在图像识别<citation id="136" type="reference"><link href="105" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、目标检测<citation id="137" type="reference"><link href="107" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、目标分割<citation id="138" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>上取得了一系列重大进展, 其中有代表性的图像识别网络主要有Alex网络 (Alex Networking, AlexNet) <citation id="139" type="reference"><link href="111" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、视觉几何组网络 (Visual Geometry Group Networking, VGGNet) <citation id="140" type="reference"><link href="113" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Google网络 (GoogLe Networking, GoogLeNet) <citation id="141" type="reference"><link href="115" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、残差网络 (Residential Networking, ResNet) <citation id="142" type="reference"><link href="117" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。这些经典分类模型在ImageNet大规模视觉识别挑战赛 (ImageNet Large Scale Visual Recognition Challenge, ILSVRC) 上获得极大成功, 其中ResNet- 152<citation id="143" type="reference"><link href="117" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>网络更是在该挑战赛上获得了4.49%的Top5错误率。由于神经网络无需手工设计特征, 且具有特征提取能力强、分类准确率高等特点, 因此本文提出了一种基于深度学习的织物缺陷在线检测算法。传统的经典分类网络算法虽然有着较高的分类准确率, 但其模型复杂度高、参数量大, 网络的实时性较差, 很难满足实际工业需求。本文基于GoogLeNet网络架构, 并综合其他分类模型的一些经典思想算法, 不断对原模型进行改进与优化, 搭建出一个适用于实际工业需求的织物缺陷分类模型。最后进行工程优化, 以批量测试的方法对图片进行分类预测, 模型的准确率与实时性均达到实际工业需求。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag">1 深度卷积神经网络</h3>
                <div class="p1">
                    <p id="41">卷积神经网络最早由LeCun在1989年提出, 即LeNet网络<citation id="144" type="reference"><link href="119" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的最初版本, 后对该网络经过不断改进与优化, 诞生了LeNet- 5网络, 成功应用于手写数字集MNIST (Mixed National Institute of Standards and Technology) 识别上。该网络包含两个卷积层、两个池化层和三个全链接层, 激活函数为Sigmoid函数, 通过反向传播对模型进行训练, 通过卷积层实现局部连接和权值共享, 通过池化层实现了局部平移不变性。通过这些方法, 有效降低了模型复杂度, 提高了模型的准确率与泛化能力。目前经典的目标识别网络, 如VGGNet系列、GoogLeNet系列、ResNet系列均借鉴这一基本思想, 同时这几个系列的网络也常常作为目标检测、目标分割的基础网络。表1对比了目前几个经典分类网络, 其中VGGNet网络结构用的是特别小的3×3的卷积核, 首次将卷积层提升到卷积块, 同时多次使用线性整流函数 (Rectified Linear Unit, ReLU) <citation id="145" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>作为激活函数替代Sigmoid函数, 一定程度上防止了梯度消失, 但其参数量巨大, 需要很大的存储空间。GoogLeNet提出了Inception模块, 如图1所示, 能够在一个网络层提取出不同尺度特征, 在增加网络宽度与深度的同时, 减少了模型参数, 降低了模型复杂度, 充分利用了计算资源, 同时加入局部响应归一化层 (Local Response Normalization, LRN) 和Dropout层, 有效避免了模型过拟合现象, 另外GoogLeNet还借鉴了NIN (Network in Network) 网络<citation id="146" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>中bottleneck思想, 并用全局平均池化层代替全链接层, 进一步降低了模型复杂度, 减少了模型参数。ResNet提出残差结构, 如图2所示, 使得训练超级深的网络成为可能, 避免了网络加深到一定程度准确率达到饱和的现象, 网络层数可以增加到1 000层<citation id="147" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>以上, 同时整个网络只需要学习残差, 降低了学习难度。</p>
                </div>
                <div class="area_img" id="42">
                    <p class="img_tit"><b>表</b>1 <b>三个经典分类网络对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Comparison of three classical classification networks</p>
                    <p class="img_note"></p>
                    <table id="42" border="1"><tr><td>网络名</td><td>输入<br />大小</td><td>卷积层<br />数量</td><td>全链接层<br />数量</td><td>有无跨层<br />连接</td><td>有无<br />Inception结构</td></tr><tr><td>VGG- 16</td><td>224×224</td><td>13</td><td>3</td><td>无</td><td>无</td></tr><tr><td><br />GoogLeNet</td><td>224×224</td><td>18</td><td>1</td><td>无</td><td>有</td></tr><tr><td><br />ResNet- 50</td><td>224×224</td><td>49</td><td>1</td><td>有</td><td>无</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">2 基于深度学习的织物缺陷检测算法</h3>
                <h4 class="anchor-tag" id="44" name="44">2.1 <b>基于深度学习的织物缺陷检测算法流程</b></h4>
                <div class="p1">
                    <p id="45">本文所提出的基于深度学习的织物缺陷检测算法由三部分构成, 如图3所示。首先由于在织物验布机上采集的图片大小为4 032×4 928, 而缺陷分类网络输入大小为224×224, 为了实现全覆盖检测, 本文将每张采集图片分割出396张小图, 并记好每张分割图片在原图中相对位置;然后将分割后的图片传入训练好的缺陷分类模型, 对所有图片进行分类;最后将有缺陷的图片依据相对位置在原图上进行定位, 以此来找出缺陷并确定其位置。其中分割部分仅仅是将采集大图的内存数据按分类网络输入大小进行逻辑切分, 然后以多线程的方式读入分类网络, 由于大图数据位于内存, 且采用多线程方式进行读取, 分割部分耗时相对于分类部分耗时可忽略不计, 本文主要对缺陷分类网络进行优化。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907044_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Inception模块" src="Detail/GetImg?filename=images/JSJY201907044_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Inception模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907044_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Inception module</p>

                </div>
                <div class="area_img" id="47">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907044_047.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 残差模块" src="Detail/GetImg?filename=images/JSJY201907044_047.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 残差模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907044_047.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 ResNet module</p>

                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907044_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于深度学习的织物缺陷检测算法流程" src="Detail/GetImg?filename=images/JSJY201907044_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于深度学习的织物缺陷检测算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907044_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Flow chart of fabric defect detection algorithm based on deep learning</p>

                </div>
                <h4 class="anchor-tag" id="49" name="49">2.2 <b>缺陷分类模型</b></h4>
                <div class="p1">
                    <p id="50">本文模型主要参考GoogLeNet网络架构, 考虑到工业对于实时性与准确率的要求, 本文综合其他分类模型的一些经典思想算法, 对模型进行了改进与优化, 使模型在保证准确率的同时, 实时性能达到工业要求。改进后的模型如图4所示。</p>
                </div>
                <div class="p1">
                    <p id="51">网络由两个卷积层、三个池化层、一个Inception层和一个全链接层组成, 最后用Softmax函数进行分类。输入图像大小为224×224的灰度图像, 第一层有32个7×7的卷积核, 滑动步长为2, padding为3, 输出32个大小为112×112的特征图, 后接一个滑动步长为2、核大小为2×2最大池化层, 特征图大小变为56×56;第二层有32×64个3×3的卷积核, 滑动步长为1, padding为1, 输出64个大小为56×56的特征图, 后接一个滑动步长为2、核大小为2×2的最大池化层, 特征图大小变为28×28;第三层为一个Inception层, 最后输出224个大小为28×28的特征图, 后接一个滑动步长为1、核大小为28×28的全局平均池化层, 输出224个1×1的特征图。由于只需要找出缺陷相对位置, 本文为降低模型复杂度, 提高模型准确率, 将所有缺陷样本归为一类。最后通过Softmax函数对图像进行正负样本划分。除池化层外, 每层网络后面都添加一个BatchNorm层<citation id="148" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>作归一化处理, 以加速收敛, 提高模型泛化能力。激活函数为ReLU函数, 同时去掉了GoogLeNet网络中的LRN层与dropout层, 并直接减少网络层数和每层的卷积核数, 降低了模型的复杂度, 提高了模型的处理速度。</p>
                </div>
                <h4 class="anchor-tag" id="52" name="52">2.3 <b>数据增强</b></h4>
                <div class="p1">
                    <p id="53">更多的数据有利于提高模型的泛化能力。本文所有数据均来源于织物验布机上的实时高清图, 所有缺陷均由经验丰富的质检人员进行标注。但由于每种缺陷采集的图片相对较少, 为了不让模型出现过拟合现象, 本文使用数据增强技术来扩充现有数据集, 以此来提高模型的泛化能力。数据增强的方式有很多, 通过对数据集的分析, 本研究发现部分缺陷相对较小, 图像缩放很有可能导致这些缺陷丢失, 因此在扩充数据时没有对图片进行缩放。最终扩充数据集的方式主要有:随机左右翻转, 随机旋转90°、180°, 随机亮度变化。部分缺陷预处理效果如图5所示。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907044_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 缺陷分类模型" src="Detail/GetImg?filename=images/JSJY201907044_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 缺陷分类模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907044_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Defect classification model</p>

                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907044_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 原图的预处理" src="Detail/GetImg?filename=images/JSJY201907044_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 原图的预处理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907044_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Preprocessing of original images</p>

                </div>
                <h3 id="56" name="56" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="57" name="57">3.1 <b>实验环境</b></h4>
                <div class="p1">
                    <p id="58">本实验数据集共有30 000张大小为224×224的灰度图片, 数据集基本包含所有纹理织物正样本及其缺陷样本。其中正样本20 000张, 负样本10 000张, 正负样本各按4∶1的比例将图片随机划分为训练集、测试集。本实验环境为Windows 7操作系统, 16 GB内存, NVIDIA GeForce GTX 1080ti显卡, 最终在Visual Studio 2013上用Caffe深度学习框架进行训练。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">3.2 <b>训练</b></h4>
                <div class="p1">
                    <p id="60">本实验采用随机梯度下降 (Stochastic Gradient Descent, SGD) <citation id="149" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>方法进行训练, 每次随机选取一个batch的图片对模型进行训练。最后的分类结果采用Softmax函数进行计算:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>exp</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中:<i>a</i><sub><i>i</i></sub>为网络输出结果为类别<i>i</i>的概率, <i>x</i><sub><i>i</i></sub>为网络最后一层第<i>i</i>个节点的输出值, <i>n</i>为划分类别总数。</p>
                </div>
                <div class="p1">
                    <p id="63">损失函数为交叉熵损失函数:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>y</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">其中<i>y</i><sub><i>i</i></sub>为类别<i>i</i>的真实标签。当类别为<i>i</i>时, <i>y</i><sub><i>i</i></sub>等于1;否则<i>y</i><sub><i>i</i></sub>等于0。</p>
                </div>
                <div class="p1">
                    <p id="66">为加速收敛, 减少模型收敛过程中的震荡, 本文算法加入了动量 (<i>momentum</i>) 因子<citation id="150" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 最终模型权重更新策略为:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>w</i><sub><i>t</i>+1</sub>=<i>w</i><sub><i>t</i></sub>+<i>v</i><sub><i>t</i></sub>      (3) </p>
                </div>
                <div class="p1">
                    <p id="68"><i>v</i><sub><i>t</i></sub>=-<i>ηdw</i>+<i>v</i><sub><i>t</i></sub>*<i>momentum</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="69"><i>dw</i>=ᐁ<i>Loss</i>      (5) </p>
                </div>
                <div class="p1">
                    <p id="70">其中:<i>w</i><sub><i>t</i></sub>为权重;<i>v</i><sub><i>t</i></sub>为每次权重更新量;<i>dw</i>为梯度方向;<i>momentum</i>为动量因子, 本实验设置为0.9;<i>η</i>为初始化学习率, 本实验设为0.000 1, 为使模型收敛, 每迭代400次, 就对学习率进行一定程度的衰减, 学习率衰减过程如图6所示。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907044_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 学习率随迭代次数变化情况" src="Detail/GetImg?filename=images/JSJY201907044_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 学习率随迭代次数变化情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907044_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Change of learning rate with number of iterations</p>

                </div>
                <h4 class="anchor-tag" id="72" name="72">3.3 <b>结果与分析</b></h4>
                <div class="p1">
                    <p id="73">最终本研究在数据集上分别对GoogLeNet、ResNet- 50和本文模型 (LocalModel) 进行训练, 各个网络的收敛速度和准确率如图7、图8所示。从图7、图8可以看出600 s时本文模型损失率下降到0.032, 准确率达到99.99%。同时准确率有随着损失率的下降而升高的趋势, 说明模型并没有出现过拟合现象, 泛化能力较强。另外, 从图8可以看出, 本文模型收敛速度明显高于GoogLeNet和ResNet- 50这两个经典分类模型;从图7可以看出, 在较短训练时间内, 本文模型的准确率也明显高于其他两个经典分类模型。</p>
                </div>
                <div class="p1">
                    <p id="74">为提高预测速度, 充分利用显存资源。本文采用批量 (batch) 测试的方法。如表2所示, 由于每个模型所需内存大小不同, 各个模型的batch大小也有所差异。实验表明, LocalModel、GoogLeNet、ResNet- 50的最佳batch大小分别为100, 30, 20。通过批量测试的方法, 本文模型每张小图的平均测试时间为0.37 ms, 每张小图的平均测试时间比GoogLeNet减少了67%, 比ResNet- 50减少了93%。</p>
                </div>
                <div class="p1">
                    <p id="75">实验结果表明, 本文所提出的基于深度学习的织物缺陷检测模型, 通过融合几个经典分类网络思想方法, 不断优化网络, 在保证分类准确率的同时, 提高了模型的实时性, 最终达到工业对准确率和实时性的要求。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907044_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同模型的准确率对比" src="Detail/GetImg?filename=images/JSJY201907044_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同模型的准确率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907044_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Accuracy comparison of different models</p>

                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907044_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同模型损失随时间变化情况对比" src="Detail/GetImg?filename=images/JSJY201907044_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同模型损失随时间变化情况对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907044_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Loss change with time of different models</p>

                </div>
                <div class="area_img" id="78">
                    <p class="img_tit"><b>表</b>2 <b>测试时间对比分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Contrastive analysis of test time</p>
                    <p class="img_note"></p>
                    <table id="78" border="1"><tr><td><br />模型名</td><td>所需内存/MB</td><td>batch大小 (test) </td><td>平均每张测试时间/ms</td></tr><tr><td>LocalModel</td><td>18.95</td><td>100</td><td>0.37</td></tr><tr><td><br />GoogLeNet</td><td>68.21</td><td>30</td><td>1.19</td></tr><tr><td><br />ResNet- 50</td><td>288.28</td><td>20</td><td>5.01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="79" name="79" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="80">本文通过对验布机上获取的织物样本进行分析, 对几个经典的分类神经网络进行改进融合, 最终提出了一种能满足实际工业需求的神经网络分类模型, 经过实验对比, 模型的时间复杂度、空间复杂度均低于传统的几个经典分类模型;同时模型具有收敛速度快、泛化能力强等优点。最后通过工程优化, 充分利用显卡资源, 实现批量测试, 极大提高了模型的单张图片平均测试时间, 准确率和实时性均达到实际工业要求, 为传统纺织行业实现工业升级提供坚实技术保障。虽然模型将传统的经典分类网络作了极大优化, 但模型的计算复杂度仍然很高, 需要在高速显卡上才能完成快速实时计算, 以此来保证实时性。未来还可以通过模型压缩等方法进一步优化模型, 减少模型占用空间, 提高模型处理速度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="99">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Textural features for image classification">

                                <b>[1]</b> HARALICK R M, SHANGMUGAN K, DINSTEIN I.Textural features for image classification[J].IEEE Transactions on Systems, Man, and Cybernetics, 1973, 3 (6) :610-621.
                            </a>
                        </p>
                        <p id="101">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740203&amp;v=MTk2NjJETnFZOUZZKzhQRG53Nm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxc1FhaEE9TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> NGAN H Y T, PANG G K H, YUNG S P, et al.Wavelet based methods on patterned fabric defect detection[J].Pattern Recognition, 2005, 38 (4) :559-576.
                            </a>
                        </p>
                        <p id="103">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FZXB201212022&amp;v=MzE5NjU3cWZadVpzRnkvZ1Y3L0tJemZUYkxHNEg5UE5yWTlIWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 张杨, 蒋高明, 姚君洲, 等.基于MRF层次模型的贾失经编针织物图像分割技术[J].纺织学报, 2012, 33 (12) :102-106. (ZHANG Y, JIANG G M, YAO J Z, et al.Segmentation of jacquard warp-knitted fabric image based on hierarchical Markov random field model[J].Journal of Textile Research, 2013, 33 (12) :102-106.) 
                            </a>
                        </p>
                        <p id="105">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning[C/OL]">

                                <b>[4]</b> SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, Inception-ResNet and the impact of residual connections on learning [EB/OL] (2016- 08- 23) [2018- 10- 15].https://arxiv.org/pdf/1602.07261.pdf.
                            </a>
                        </p>
                        <p id="107">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">

                                <b>[5]</b> LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector [EB/OL].[2018- 10- 15] https://arxiv.org/pdf/1512.02325.pdf.
                            </a>
                        </p>
                        <p id="109">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">

                                <b>[6]</b> CHEN L C, PAPANDREOU G, KOKKINOS I, et al.DeepLab:semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 40 (4) :834-848.
                            </a>
                        </p>
                        <p id="111">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with DeepConvolutional Neural Networks">

                                <b>[7]</b> KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]// Proceedings of the 9th International Conference on Neural Information Processing Systems.Berlin:Springer, 2012:1097-1105.
                            </a>
                        </p>
                        <p id="113">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[8]</b> SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL]. (2015- 04- 10) [2018- 10- 15].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="115">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">

                                <b>[9]</b> SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions [EB/OL]. (2014- 07- 17) [2016- 12- 10].https://arxiv.org/abs/1409.4842.
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[10]</b> HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[EB/OL]. (2015- 12- 10) [2018- 10- 15].https://arxiv.org/abs/1512.03- 385.
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradient-based learning applied to document recognition">

                                <b>[11]</b> LeCUN Y, BOTTOU I, BENGIO Y, et al.Gradient-based learning applied do document recognition [J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What is the best multi-stage architecture for object recognition?">

                                <b>[12]</b> JARRETT K, KAVUKCUOGLU K, RANZATO M, et al.What is the best multi-stage architecture for object recognition?[EB/OL] [2018- 10- 15].http://yann.lecun.com/exdb/publis/orig/jarrett-iccv- 09.pdf.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network in network">

                                <b>[13]</b> LIN M, CHEN Q, YAN S.Network in network[EB/OL]. (2014- 03- 04) [2018- 10- 15].https://arxiv.org/abs/1312.4400.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">

                                <b>[14]</b> HE K, ZHANG X, REN S, et al.Identity mappings in deep residual networks [EB/OL]. (2016- 07- 25) [2018- 10- 15].http://arxiv.org/obs/1603.05027.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">

                                <b>[15]</b> IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [EB/OL].[2018- 10- 15].http//arxiv.org/pdf/1502.03167.pdf.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale machine learning with stochastic gradient descent">

                                <b>[16]</b> BOTTOU L.Large-scale machine learning with stochastic gradient descent [C]// Proceedings of the 19th International Conference on Computational Statistics.Berlin:Springer, 2010:177-186.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An overview of gradient descent optimization algorithms">

                                <b>[17]</b> RUDER S.An overview of gradient descent optimization algorithms [EB/OL]. (2017- 06- 15) [2018- 10- 15].https://arxiv.org/abs/1609.04747.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907044" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907044&amp;v=MjM4MzNJOUJZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVjcvS0x6N0JkN0c0SDlqTXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
