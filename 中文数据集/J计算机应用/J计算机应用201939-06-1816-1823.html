<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136753702158750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906044%26RESULT%3d1%26SIGN%3d66MfX%252ffL9i1PWCwWn2pZ3a5AOM8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906044&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906044&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906044&amp;v=MDAwNDFyQ1VSN3FmWnVac0Z5RGtXN3pNTHo3QmQ3RzRIOWpNcVk5QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#61" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="1 相关理论 ">1 相关理论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="1.1 &lt;b&gt;卷积神经网络&lt;/b&gt;">1.1 <b>卷积神经网络</b></a></li>
                                                <li><a href="#76" data-title="1.2 &lt;b&gt;跨层级连接思想&lt;/b&gt;">1.2 <b>跨层级连接思想</b></a></li>
                                                <li><a href="#82" data-title="1.3 &lt;b&gt;密集神经网络&lt;/b&gt;">1.3 <b>密集神经网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#88" data-title="2 本文算法 ">2 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="2.1 &lt;b&gt;设计网络结构&lt;/b&gt;">2.1 <b>设计网络结构</b></a></li>
                                                <li><a href="#113" data-title="2.2 &lt;b&gt;构造损失函数&lt;/b&gt;">2.2 <b>构造损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#135" data-title="3.1 &lt;b&gt;实验数据集及环境&lt;/b&gt;">3.1 <b>实验数据集及环境</b></a></li>
                                                <li><a href="#138" data-title="3.2 &lt;b&gt;评价指标&lt;/b&gt;">3.2 <b>评价指标</b></a></li>
                                                <li><a href="#147" data-title="3.3 &lt;b&gt;实验结果&lt;/b&gt;">3.3 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#167" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="图1 残差单元结构">图1 残差单元结构</a></li>
                                                <li><a href="#84" data-title="图2 密集网络主体结构">图2 密集网络主体结构</a></li>
                                                <li><a href="#94" data-title="图3 本文算法网络结构">图3 本文算法网络结构</a></li>
                                                <li><a href="#99" data-title="图4 密集块内部结构">图4 密集块内部结构</a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;密集连接子网络&lt;/b&gt;"><b>表</b>1 <b>密集连接子网络</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;分类特征子网络&lt;/b&gt;"><b>表</b>2 <b>分类特征子网络</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;融合及输出网络&lt;/b&gt;"><b>表</b>3 <b>融合及输出网络</b></a></li>
                                                <li><a href="#155" data-title="图5 不同算法的着色效果对比">图5 不同算法的着色效果对比</a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;图&lt;/b&gt;5&lt;b&gt;中着色效果的图像熵及峰值信噪比对比&lt;/b&gt;"><b>表</b>4 <b>图</b>5<b>中着色效果的图像熵及峰值信噪比对比</b></a></li>
                                                <li><a href="#157" data-title="图6 着色效果细节对比">图6 着色效果细节对比</a></li>
                                                <li><a href="#165" data-title="图7 黑白老照片着色效果">图7 黑白老照片着色效果</a></li>
                                                <li><a href="#166" data-title="图8 不同算法信息损失对比">图8 不同算法信息损失对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="226">


                                    <a id="bibliography_1" title="冈萨雷斯, 伍兹.数字图像处理[M].3版.阮秋琦, 译.北京:电子工业出版社, 2007:484-486. (GONZALEZ R C, WOODS RE.Digital Image Processing[M].3rd ed.RUAN Q Q, translated.Beijing:Publishing House of Electronics Industry, 2007:484-486.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121043970001&amp;v=MTg0NjduM3hFOWZidm5LcmlmWmVadkZ5bm5VN2ZKS0YwVFhGcXpHYks2SDlISXJJWkNaT3NQRFJNOHp4VVNtRGQ5U0g3&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        冈萨雷斯, 伍兹.数字图像处理[M].3版.阮秋琦, 译.北京:电子工业出版社, 2007:484-486. (GONZALEZ R C, WOODS RE.Digital Image Processing[M].3rd ed.RUAN Q Q, translated.Beijing:Publishing House of Electronics Industry, 2007:484-486.) 
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_2" title="LEVIN A, LISCHINSKI D, WEISS Y.Colorization using optimization[J].ACM Transactions on Graphics, 2004, 23 (3) :689-694." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098145&amp;v=MDYzMzRqTnI0OUZaT0lIRFhnOG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGOGNhUlU9TmlmSVk3SzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        LEVIN A, LISCHINSKI D, WEISS Y.Colorization using optimization[J].ACM Transactions on Graphics, 2004, 23 (3) :689-694.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_3" title="LAGODZINSKI P, SMOLKA B.Medical image colorization[J].Journal of Medical Informatics&amp;amp;Technologies, 2007 (11) :47-57." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Medical image colorization">
                                        <b>[3]</b>
                                        LAGODZINSKI P, SMOLKA B.Medical image colorization[J].Journal of Medical Informatics&amp;amp;Technologies, 2007 (11) :47-57.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_4" title="LAGODZINSKI P, SMOLKA B.Colorization of medical images[J].China Healthcare Innovation, 2009, 15 (4) :13-23." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Colorization of medical images">
                                        <b>[4]</b>
                                        LAGODZINSKI P, SMOLKA B.Colorization of medical images[J].China Healthcare Innovation, 2009, 15 (4) :13-23.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_5" title="SHAH A A, MIKITA G, SHAH K M.Medical image colorization using optimization technique[J].Acta Medica Okayama, 2013, 62 (141) :235-248." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Medical Image Colorization using Optimization Technique">
                                        <b>[5]</b>
                                        SHAH A A, MIKITA G, SHAH K M.Medical image colorization using optimization technique[J].Acta Medica Okayama, 2013, 62 (141) :235-248.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_6" title="LI F, ZHU L, ZHANG L, et al.Pseudo-colorization of medical images based on two-stage transfer model[J].Chinese Journal of Stereology and Image Analysis, 2013, 18 (2) :135-144." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZTSX201302008&amp;v=MjY1MDBGckNVUjdxZlp1WnNGeURrVzd6UFB6bllkckc0SDlMTXJZOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        LI F, ZHU L, ZHANG L, et al.Pseudo-colorization of medical images based on two-stage transfer model[J].Chinese Journal of Stereology and Image Analysis, 2013, 18 (2) :135-144.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_7" title="WELSH T, ASHIKHMIN M, MUELLER K.Transferring color to greyscale images[J].ACM Transactions on Graphics, 2002, 21 (3) :277-280." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000097914&amp;v=MTkxNjlGOGNhUlU9TmlmSVk3SzdIdGpOcjQ5RlpPSUlCWDA5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        WELSH T, ASHIKHMIN M, MUELLER K.Transferring color to greyscale images[J].ACM Transactions on Graphics, 2002, 21 (3) :277-280.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_8" title="LIU X, WAN L, QU Y, et al.Intrinsic colorization[J].ACMTransactions on Graphics, 2008, 27 (5) :Article No.152." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098716&amp;v=MTU3MjRaT0lIQzMwL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGOGNhUlU9TmlmSVk3SzdIdGpOcjQ5Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        LIU X, WAN L, QU Y, et al.Intrinsic colorization[J].ACMTransactions on Graphics, 2008, 27 (5) :Article No.152.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_9" title="LIU Y, COHEN M, UYTTENDAELE M, et al.Auto Style:automatic style transfer from image collections to users&#39;images[J].Computer Graphics Forum, 2014, 33 (4) :21-31." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD14071600000756&amp;v=MTE0NzhubFVyM0lLRjhjYVJVPU5pZmNhcks4SHRiTnFZOUZaT3NQQzNrL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        LIU Y, COHEN M, UYTTENDAELE M, et al.Auto Style:automatic style transfer from image collections to users&#39;images[J].Computer Graphics Forum, 2014, 33 (4) :21-31.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_10" title="MORIMOTO Y, TAGUCHI Y, NAEMURA T.Automatic colorization of grayscale images using multiple images on the Web[C]//Proceedings of ACM SIGGRAPH 2009.New York:ACM, 2009:Article No.59." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic colorization of grayscale images using multiple images on the Web">
                                        <b>[10]</b>
                                        MORIMOTO Y, TAGUCHI Y, NAEMURA T.Automatic colorization of grayscale images using multiple images on the Web[C]//Proceedings of ACM SIGGRAPH 2009.New York:ACM, 2009:Article No.59.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_11" title="IRONY R, COHEN-OR D, LISCHINSKI D.Colorization by example[C]//Proceedings of the 16th Eurographics Conference on Rendering Techniques.Aire-la-Ville, Switzerland:Eurographics Association, 2005:201-210." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Colorization by example">
                                        <b>[11]</b>
                                        IRONY R, COHEN-OR D, LISCHINSKI D.Colorization by example[C]//Proceedings of the 16th Eurographics Conference on Rendering Techniques.Aire-la-Ville, Switzerland:Eurographics Association, 2005:201-210.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_12" title="CHENG Z, YANG Q, SHENG B.Deep colorization[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:415-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Colorization">
                                        <b>[12]</b>
                                        CHENG Z, YANG Q, SHENG B.Deep colorization[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:415-423.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_13" title="DESHPANDE A, ROCK J, FORSYTH D.Learning large-scale automatic image colorization[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:567-575." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Large-Scale Automatic Image Colorization">
                                        <b>[13]</b>
                                        DESHPANDE A, ROCK J, FORSYTH D.Learning large-scale automatic image colorization[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:567-575.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_14" title="ZHANG R, ISOLA P, EFROS A A.Colorful image colorization[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Amsterdam:Springer International Publishing, 2016:649-666." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Colorful Image Colorization">
                                        <b>[14]</b>
                                        ZHANG R, ISOLA P, EFROS A A.Colorful image colorization[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Amsterdam:Springer International Publishing, 2016:649-666.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_15" title="SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J/OL].ar Xiv Preprint, 2014, 2014:ar Xiv.1409.1556 (2014-09-04) [2018-08-10].http://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition.[OL]">
                                        <b>[15]</b>
                                        SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J/OL].ar Xiv Preprint, 2014, 2014:ar Xiv.1409.1556 (2014-09-04) [2018-08-10].http://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_16" title="RONNEBERGER O, FISCHER P, BROX T.U-Net:Convolutional networks for biomedical image segmentation[C]//MICCAI2015:Proceedings of the 2015 Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer International Publishing, 2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[16]</b>
                                        RONNEBERGER O, FISCHER P, BROX T.U-Net:Convolutional networks for biomedical image segmentation[C]//MICCAI2015:Proceedings of the 2015 Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer International Publishing, 2015:234-241.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_17" title="ZHANG R, ZHU J Y, ISOLA P, et al.Real-time user-guided image colorization with learned deep priors[J].ACM Transactions on Graphics, 2017, 36 (4) :Article No.119." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM789DD952ADDAB5927133A9860A753458&amp;v=MjkzNThXNHBvcEhGWjk3ZlE0OHhoUVU2engrT1hicXFoSkVmcmVYUWIrWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMbTN3Njg9TmlmSVk3U3dGNg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        ZHANG R, ZHU J Y, ISOLA P, et al.Real-time user-guided image colorization with learned deep priors[J].ACM Transactions on Graphics, 2017, 36 (4) :Article No.119.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_18" title="LIZUKA S, SIMOSERRA E, ISHIKAWA H.Let there be color!:joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification[J].ACM Transactions on Graphics, 2016, 35 (4) :Article No.110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMAFFF2D121F4EA546037EFF6E3BD86EAE&amp;v=MjY5NjJMT2FLZk8yNDVIWlowTGVRMDh5eEFUNlRnSVBnbmsyUkZIRGJxU01NdnFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6TG0zdzY4PU5pZklZOA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        LIZUKA S, SIMOSERRA E, ISHIKAWA H.Let there be color!:joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification[J].ACM Transactions on Graphics, 2016, 35 (4) :Article No.110.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_19" title="QIN P L, CHENG Z R, CUI Y H, et al.Research on image colorization algorithm based on residual neural network[C]//CCCV2017:Proceedings of the 2017 CCF Chinese Conference on Computer Vision, CCIS 771.Berlin:Springer, 2017:608-621." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on image colorization algorithm based on residual neural network">
                                        <b>[19]</b>
                                        QIN P L, CHENG Z R, CUI Y H, et al.Research on image colorization algorithm based on residual neural network[C]//CCCV2017:Proceedings of the 2017 CCF Chinese Conference on Computer Vision, CCIS 771.Berlin:Springer, 2017:608-621.
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_20" >
                                        <b>[20]</b>
                                    HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.</a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_21" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[21]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_22" title="SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[22]</b>
                                        SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_23" title="SRIVASTAVA R K, GREFF K, SCHMIDHUBER J.Highway networks[J/OL].ar Xiv Preprint, 2015, 2015:ar Xiv.1505.00387 (2015-03-03) [2018-08-10].https://arxiv.org/abs/1505.00387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Highway networks">
                                        <b>[23]</b>
                                        SRIVASTAVA R K, GREFF K, SCHMIDHUBER J.Highway networks[J/OL].ar Xiv Preprint, 2015, 2015:ar Xiv.1505.00387 (2015-03-03) [2018-08-10].https://arxiv.org/abs/1505.00387.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_24" title="HUANG G, SUN Y, LIU Z, et al.Deep networks with stochastic depth[C]//ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:646-661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep networks with stochastic depth">
                                        <b>[24]</b>
                                        HUANG G, SUN Y, LIU Z, et al.Deep networks with stochastic depth[C]//ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:646-661.
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_25" title="LARSSON G, MAIRE M, SHAKHNAROVICH G.Fractal Net:ultra-deep neural networks without residuals[J/OL].ar Xiv Preprint, 2016, 2016:ar Xiv.1605.07648 (2016-05-24) [2018-08-16].https://arxiv.org/abs/1605.07648." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fractal Net:ultra-deep neural networks without residuals">
                                        <b>[25]</b>
                                        LARSSON G, MAIRE M, SHAKHNAROVICH G.Fractal Net:ultra-deep neural networks without residuals[J/OL].ar Xiv Preprint, 2016, 2016:ar Xiv.1605.07648 (2016-05-24) [2018-08-16].https://arxiv.org/abs/1605.07648.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_26" title="HUANG G, LIU Z, LAURENS V D M, et al.Densely connected convolutional networks[C]//CVPR 2017:Proceedings of the2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DenselyConnected Convolutional Networks">
                                        <b>[26]</b>
                                        HUANG G, LIU Z, LAURENS V D M, et al.Densely connected convolutional networks[C]//CVPR 2017:Proceedings of the2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2261-2269.
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_27" title="ZHOU B, LAPEDRIZA A, XIAO J, et al.Learning deep features for scene recognition using places database[C]//NIPS 2014:Proceedings of the 2014 27th International Conference on Neural Information Processing Systems.Cambridge, CA:MIT Press, 2014:487-495." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep features for scene recognition using places database">
                                        <b>[27]</b>
                                        ZHOU B, LAPEDRIZA A, XIAO J, et al.Learning deep features for scene recognition using places database[C]//NIPS 2014:Proceedings of the 2014 27th International Conference on Neural Information Processing Systems.Cambridge, CA:MIT Press, 2014:487-495.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_28" title="DENG J, DONG W, SOCHER R, et al.Image Net:a large-scale hierarchical image database[C]//CVPR 2009:Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:A large-scale hierarchical image database">
                                        <b>[28]</b>
                                        DENG J, DONG W, SOCHER R, et al.Image Net:a large-scale hierarchical image database[C]//CVPR 2009:Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2009:248-255.
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_29" title="ABADI M, AGARWAL A, BARHAM P, et al.Tensor Flow:large-scale machine learning on heterogeneous distributed systems[J/OL].ar Xiv Preprint, 2016, 2016:ar Xiv.1603.04467[2018-08-14].https://arxiv.org/abs/1603.04467." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:Largescale machine learning on heterogeneous distributed Systems[OL]">
                                        <b>[29]</b>
                                        ABADI M, AGARWAL A, BARHAM P, et al.Tensor Flow:large-scale machine learning on heterogeneous distributed systems[J/OL].ar Xiv Preprint, 2016, 2016:ar Xiv.1603.04467[2018-08-14].https://arxiv.org/abs/1603.04467.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 09:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1816-1823 DOI:10.11772/j.issn.1001-9081.2018102100            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于密集神经网络的灰度图像着色算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%A8%9C&amp;code=22159890&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张娜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A7%A6%E5%93%81%E4%B9%90&amp;code=25670970&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">秦品乐</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%BE%E5%BB%BA%E6%BD%AE&amp;code=34838180&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曾建潮</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%90%AF&amp;code=41133373&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李启</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%8C%97%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E9%99%A2&amp;code=0036109&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中北大学大数据学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对在灰度图像着色领域中, 传统算法信息提取率不高、着色效果不理想的问题, 提出了基于密集神经网络的灰度图像着色算法, 以实现改善着色效果, 让人眼更好地观察图片信息的目的。利用密集神经网络的信息提取高效性, 构建并训练了一个端到端的深度学习模型, 对图像中的各类信息及特征进行提取。训练网络时与原图像进行对比, 以逐渐减小网络输出结果的信息、分类等各类型的损失。训练完成后, 只需向网络输入一张灰度图片, 即可生成一张颜色饱满、鲜明逼真的彩色图片。实验结果表明, 引入密集网络后, 可有效改善着色过程中的漏色、细节信息损失、对比度低等问题, 所提算法着色效果较基于VGG网络及U-Net、双流网络结构、残差网络 (ResNet) 等性能优异的先进着色算法而言取得了显著的改进。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E7%9D%80%E8%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像着色;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%86%E9%9B%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">密集神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">灰度图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%88%A9%E7%94%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征利用;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BF%A1%E6%81%AF%E6%8D%9F%E5%A4%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息损失;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张娜 (1995—) , 女, 山西临汾人, 硕士研究生, 主要研究方向:机器学习、计算机视觉、数字图像处理;;
                                </span>
                                <span>
                                    *秦品乐 (1978—) , 男, 山西太原人, 副教授, 博士, CCF会员, 主要研究方向:大数据、机器视觉、三维重建;qpl@ nuc. edu. cn;
                                </span>
                                <span>
                                    曾建潮 (1963—) , 男, 山西太原人, 教授, 博士生导师, 博士, 主要研究方向:复杂系统的维护决策和健康管理;;
                                </span>
                                <span>
                                    李启 (1991—) , 男, 山西大同人, 硕士研究生, 主要研究方向:机器学习、计算机视觉、数字图像处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-18</p>

            </div>
                    <h1><b>Grayscale image colorization algorithm based on dense neural network</b></h1>
                    <h2>
                    <span>ZHANG Na</span>
                    <span>QIN Pinle</span>
                    <span>ZENG Jianchao</span>
                    <span>LI Qi</span>
            </h2>
                    <h2>
                    <span>School of Data Science And Technology, North University of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of low information extraction rate of traditional methods and the unideal coloring effect in the grayscale image colorization field, a grayscale image colorization algorithm based on dense neural network was proposed to improve the colorization effect and make the information of image be better observed by human eyes. With making full use of the high information extraction efficiency of dense neural network, an end-to-end deep learning model was built and trained to extract multiple types of information and features in the image. During the training, the loss of the network output result (such as information loss and classification loss) was gradually reduced by comparing with the original image. After the training, with only a grayscale image input into the trained network, a full and vibrant vivid color image was able to be obtained. The experimental results show that the introduction of dense network can effectively alleviate the problems such as color leakage, loss of detail information and low contrast, during the colorization process. The coloring effect has achieved significant improvement compared with the current advanced coloring methods based on Visual Geometry Group (VGG) -net, U-Net, dual stream network structure, Residual Network (ResNet) , etc.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20coloring&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image coloring;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dense%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dense neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=grayscale%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">grayscale image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20utilization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature utilization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=information%20loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">information loss;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Na, born in 1995, M. S. candidate. Her research interests include machine learning, computer vision, digital image processing. ;
                                </span>
                                <span>
                                    QIN Pinle, born in 1978, Ph. D. , associate professor. His research interests include big data, machine vision, three-dimensional reconstruction. ;
                                </span>
                                <span>
                                    ZENG Jianchao, born in 1963, Ph. D. , professor. His research interests include maintenance decision and health management of complex system. ;
                                </span>
                                <span>
                                    LI Qi, born in 1991, M. S. candidate. His research interests include machine learning, computer vision, digital image processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-18</p>
                            </div>


        <!--brief start-->
                        <h3 id="61" name="61" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="62">色彩信息是图像所包含的一种重要信息, 能够结合图中场景的语义及物体表面纹理信息, 共同展现丰富的层次感。研究表明, 人眼对彩色强度及其变换具有很高的敏感度, 彩色图像较灰度图像而言, 更便于人眼观察信息;且从人的心理层面而言, 彩色图像能给与观察者以更加愉悦、明快的感受, 从而有助于理解图像的内容, 从中获取更加全面、丰富的信息, 提高图像使用价值。因此, 将灰度图像通过一定算法转换为彩色图像, 获得更好的观察效果是非常有意义的。灰度图像着色 (即伪彩色处理) 技术, 即是在上述需求下产生的, 通过一种指定的规则, 对灰度值赋以颜色<citation id="284" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 实现还原、增强或改变图像的色彩信息。</p>
                </div>
                <div class="p1">
                    <p id="63">目前, 主要存在着三类图像着色的方法。</p>
                </div>
                <div class="p1">
                    <p id="64">1) 基于用户引导下的颜色传播类算法。在灰度图像着色领域出现较早, 由用户进行关键部位或区域的指导性着色, 并按照设定的算法或转换规范进行颜色扩展。其中, Levin等<citation id="285" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出了全局优化着色算法, 支持图像先验定义, 用户在着色后, 生成与着色笔迹匹配的彩色图像;Lagodzinski等<citation id="286" type="reference"><link href="230" rel="bibliography" /><link href="232" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>提出了一种新颖的着色方法, 利用形态距离变换和图像结构自动传播用户在灰度图像内所写的颜色。上述方法可取得不错的效果, 但由于不同的颜色区域需要明确指示, 通常需要密集的用户交互, 且对与色度深浅等属性无法较好地进行表征与实现, 也容易出现由于标注不当、灰度值过于相近等原因产生的颜色渗漏问题。</p>
                </div>
                <div class="p1">
                    <p id="65">2) 基于指定函数或参数的颜色映射算法。通过设定的着色函数, 将灰度值与彩色值之间建立某种映射关系, 实现由灰色向彩色的变换。Shah等<citation id="287" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>在基于优化的着色方法基础上, 使用三个相关系数来评估其在信息损失方面的性能;Li等<citation id="288" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>也使用了基于阈值应用着色技术, 该类方法对阈值选取依赖性较高, 且得到的着色效果颜色数目有限, 效果较为生硬。</p>
                </div>
                <div class="p1">
                    <p id="66">3) 基于数据驱动的图像着色方法。此类方法在早期主要有基于实例图像参考法及类比法。Welsh等<citation id="289" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出可将灰度图像的亮度及纹理信息与实例参考图像进行对比, 实现灰度图像的着色;Liu等<citation id="290" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出可直接从互联网中搜索与目标灰度图相关的彩色参考实例图像进行着色;Liu等<citation id="291" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>及Morimoto等<citation id="292" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>通过颜色转换和图像分析, 实现对目标灰度图像着色。此类方法在目标灰度图与参考图像中内容相似的较高时, 效果非常不错, 但查找参考图像及匹配过程非常耗时, 当着色目标或场景非常复杂或罕见时, 着色效果就更难以得到保证。Irony等<citation id="293" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>则利用纹理特征匹配, 首先对参考图像和灰度图像进行图像分割处理, 参照参考图像, 对灰度图像中具有相似纹理的部分赋以相同的色彩, 虽然也可取得不错效果, 但分割处理操作也增加了图像处理负担。</p>
                </div>
                <div class="p1">
                    <p id="67">深度学习方法的发展及高性能图形处理器 (Graphics Processing Unit, GPU) 的出现, 为基于数据驱动的图像着色方法开辟了新的方向。该类方法利用神经网络, 搭建不同的网络架构, 通过卷积操作对图像的内容和特征进行提取及分析, 寻找灰度图像到彩色图像之间的映射关系, 从而训练出相应的模型, 实现着色。Cheng等<citation id="294" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过为大规模数据建模, 采用基于联合双边滤波的后处理方式, 利用自适应图像聚类技术来整合图像全局信息;Deshpande等<citation id="295" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>通过训练色度图中的二次目标函数, 通过最小化目标函数实现图像着色。此类网络结构较为简单, 其着色效果比较有限。Zhang等<citation id="296" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出通过VGG (Visual Geometry Group) 卷积神经网络<citation id="297" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>来提取图像特征, 预测每个像素的颜色直方图来为图像着色, 后又提出了新的思路, 利用U-Net网络<citation id="298" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>进行信息提取, 并结合用户交互进行着色<citation id="299" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>;Lizuka等<citation id="300" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>构建了双流结构网络, 同时提取图像的全局分类信息及局部特征信息, 将两类信息进行融合, 实现对像素颜色的预测。此三类方法较之前的方法已取得了较大改善, 但由于其网络均在图像处理过程中均进行了下采样及上采样操作, 存在一定程度的信息丢失。Qin等<citation id="301" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>采用残差网络<citation id="302" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>进行细节特征的提取, 结合分类信息指导, 在一定程度上改善了信息损失, 但仍存在细节着色不完善、漏色等问题。</p>
                </div>
                <div class="p1">
                    <p id="68">通过对现有灰度图像着色算法进行广泛研究分析, 可以看出, 现有的灰度图像着色已经可以实现给定一幅灰度图像, 通过一定算法得出一幅彩色图像, 但基本都存在如下共性问题:</p>
                </div>
                <div class="p1">
                    <p id="69">1) 细节信息还原度不高。由于对应映射关系效率有限, 特征提取的过程中存在着一定程度的信息损失, 导致图像中的部分内容 (尤其是较小的物体) 不能被赋予适当的颜色。</p>
                </div>
                <div class="p1">
                    <p id="70">2) 物体边界清晰度不高。在一定程度上存在着“漏色”的问题, 在物体边界处, 容易存在颜色渗漏。</p>
                </div>
                <div class="p1">
                    <p id="71">3) 用户交互的依赖性较强。需要借由用户做出大量辅助操作, 往往容易引入较多的随机性误差, 且不利于将用户从繁杂的参数调整工作中解放出来。</p>
                </div>
                <div class="p1">
                    <p id="72">综合考虑上述因素, 为了充分利用图像细节信息、轮廓信息等低阶语义信息, 本文采用自适应性强、用户依赖低的密集神经网络, 搭建着色网络, 并且构造了着色网络损失函数以及评价指标。经实验验证和理论分析, 本文算法与传统方法相比, 可以明显改善细节信息损失、边界不清晰的问题, 同时不需要用户干预, 得到的着色模型细节更加完善、丰富。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">1 相关理论</h3>
                <h4 class="anchor-tag" id="74" name="74">1.1 <b>卷积神经网络</b></h4>
                <div class="p1">
                    <p id="75">卷积神经网络在图像识别、语音分析、自然语言处理等领域已经成为研究热点, 该网络具有特征共享性, 可有效降低网络的复杂性, 在解决特征提取及特征映射问题时可发挥非常有效的作用。特别是在图像分析与处理应用中, 可直接将图像输入网络, 避免了特征提取和分类过程中数据重建的复杂度。目前已经有AlexNet (Alex Network) <citation id="303" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、VGG、GoogleNet<citation id="304" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、残差网络 (Residual Network, ResNet) 等不同结构的基于卷积神经网络的优秀网络, 在图像分类任务中已经将Top5错误率降到5%以下。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">1.2 <b>跨层级连接思想</b></h4>
                <div class="p1">
                    <p id="77">在深度学习网络中, 随着网络深度的加深, 梯度消失问题会愈加明显, 因此产生了较大的信息损失。目前, 很多研究学者都针对此问题提出了解决方案, 如ResNet、Highway Networks<citation id="305" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、Stochastic depth<citation id="306" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、FractalNets<citation id="307" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>等, 此类算法的网络结构各有差别, 但其核心都在于——创建早期层级到后期层级之间的短连接路径, 在较小的代价下, 利用较早层级提取到的信息, 提高整体信息利用率。其中, ResNet由于其较好的性能和结构的简单性, 为较多研究者所采用。该网络通过在残差块的输出和输入之间引入一个短连接, 而不是简单地堆叠网络, 实际映射关系可表示为<i>F</i> (<i>x</i>) +<i>x</i>, 如图 1所示。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906044_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 残差单元结构" src="Detail/GetImg?filename=images/JSJY201906044_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 残差单元结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906044_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Residual unit structure</p>

                </div>
                <div class="p1">
                    <p id="79">在一个<i>L</i>层的卷积神经网络中, 每一层<i>l</i>都对应一个非线性映射<i>H</i><sub><i>l</i></sub> (·) , 每一个映射通常包含了批标准化 (Batch Normalization, BN) 操作、ReLU激活函数、池化 (Pooling) 及卷积操作。当一幅图像<b><i>X</i></b><sub>0</sub>进入网络, 经过第<i>l</i>层后, 将得到<b><i>X</i></b><sub><i>l</i></sub>的输出。传统的前馈网络仅仅将第<i>l</i>-1层的输出直接连接至第<i>l</i>层, 即<b><i>X</i></b><sub><i>l</i></sub>=<i>H</i><sub><i>l</i></sub> (<b><i>X</i></b><sub><i>l</i>-1</sub>) , ResNet添加了一个跳层连接, 如式 (1) :</p>
                </div>
                <div class="p1">
                    <p id="80"><b><i>X</i></b><sub><i>l</i></sub>=<i>H</i><sub><i>l</i></sub> (<b><i>X</i></b><sub><i>l</i>-1</sub>) +<b><i>X</i></b><sub><i>l</i>-1</sub>       (1) </p>
                </div>
                <div class="p1">
                    <p id="81">ResNet的这一操作, 在一定程度上可以解决由于网络加深出现梯度消失的问题, 从而可以在一定程度上继续加深网络, 获取更好的性能, 但由于是直接将映射操作与前一次的输出相加, 这可能不利于网络中的信息流动。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">1.3 <b>密集神经网络</b></h4>
                <div class="p1">
                    <p id="83">与传统的网络结构不同, 密集神经网络<i>DenseNet</i> (<i>Densely connected convolutional Network</i>) <citation id="308" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>不是通过极其深或者宽的网络来获得更好的性能, 而是通过特征重用来提升网络的性能及潜力, 产生易于训练和参数效率高的压缩模型。残差神经网络 (<i>ResNet</i>) 虽然也利用了跳层连接的思想, 但其仅利用了上一层输入作为信息补充, 并未充分利用早期层级特征。而密集网络通过连接不同层级的特征图, 将传统模型中未曾利用或充分利用的前期层级的特征均引入新的层级中, 充分利用了低层级卷积层对位置信息、形状信息的敏感性, 增加后续层输入的变化, 将更加有效地提高效率, 这也是<i>DenseNet</i>和<i>ResNet</i>之间的主要区别。<i>DenseNet</i>相较于<i>ResNet</i>及早期其他类型网络, 具备更高的信息利用率。密集网络主体结构如图2所示。</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906044_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 密集网络主体结构" src="Detail/GetImg?filename=images/JSJY201906044_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 密集网络主体结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906044_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Main structure of DenseNet</i></p>

                </div>
                <div class="p1">
                    <p id="85">在一个<i>L</i>层的密集神经网络中, 网络直接将卷积层与所有后续层级连接起来, 即第<i>l</i>层接收了来自所有前面卷积层的特征图作为新的输入, 如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="86"><b><i>X</i></b><sub><i>l</i></sub>=<i>H</i><sub><i>l</i></sub> ([<b><i>X</i></b><sub>1</sub>, <b><i>X</i></b><sub>2</sub>, …, <b><i>X</i></b><sub><i>l</i>-1</sub>])       (2) </p>
                </div>
                <div class="p1">
                    <p id="87">其中[<b><i>X</i></b><sub>1</sub>, <b><i>X</i></b><sub>2</sub>, …, <b><i>X</i></b><sub><i>l</i>-1</sub>]即是将来自0, 1, …, <i>l</i>-1层的特征图进行连接。相较于ResNet及更早的网络模型, DenseNet在没有新增过多参数的情况下, 充分利用了早先层级中的特征信息, 有效提高了网络的性能。</p>
                </div>
                <h3 id="88" name="88" class="anchor-tag">2 本文算法</h3>
                <div class="p1">
                    <p id="89">本文算法引入密集神经网络, 利用其信息提取率和特征利用率高的特性, 结合分类指导及损失优化, 使输出彩色图像的细节特征更为丰富、轮廓更为清晰, 进而达到更好的着色效果。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">2.1 <b>设计网络结构</b></h4>
                <h4 class="anchor-tag" id="91" name="91">2.1.1 总体网络结构</h4>
                <div class="p1">
                    <p id="92">现有的基于深度学习的灰度图像着色网络主要是通过构建卷积神经网络, 对图像的细节纹理特征进行提取, 着色效果尚可, 但因没有适当的方式来学习正确的图像全局上下文信息 (如场景是属于室内还是室外等) , 着色网络可能出现明显的错误。<i>Lizuka</i>等<citation id="309" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>将图片的类别信息也融入网络, 用图片的类别信息来协同训练模型, 对整个着色网络起到了分类指导作用。<i>Qin</i>等<citation id="310" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>在此原理基础上进行网络设计, 也获得了不错的效果, 也证实了双流结构的有效性。</p>
                </div>
                <div class="p1">
                    <p id="93">本文汲取<i>Lizuka</i>等<citation id="311" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、<i>Qin</i>等<citation id="312" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>算法的优点, 总体采用双流架构, 主要由分类子网络和特征提取子网络构成, 通过整合特征信息和分类信息, 实现由灰度图像到彩色图像的转换。分类子网络采用<i>VGG</i>网络获取图像的分类信息;在设计特征提取子网络时, 为了解决传统深度学习算法容易出现的梯度消失及底层特征利用率不足的问题, 采用了密集神经网络。网络将纹理细节信息及分类信息融合后进行特征再提取, 根据得到的综合特征进行色彩预计, 并与彩色图像进行对比, 计算色彩、信息量等损失, 经过多次优化训练后得到最终着色模型。网络结构如图 3所示。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906044_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法网络结构" src="Detail/GetImg?filename=images/JSJY201906044_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906044_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Network structure of the proposed method</i></p>

                </div>
                <div class="p1">
                    <p id="95">在设计网络时, 考虑到<i>CIE Lab</i>颜色空间接近人眼视觉且色域比显示器甚至人眼的色域都要大, 其颜色表示方式与设备无关, 且弥补了<i>RGB</i>色彩模型色彩分布不均的不足, 故本文使用此颜色空间。图像在输入网络前, 首先从<i>RGB</i>颜色空间转换为<i>Lab</i>颜色空间, 标注分类信息后制作为训练数据集, 利用该数据集进行网络训练。</p>
                </div>
                <div class="p1">
                    <p id="96">整个网络模型为有监督类学习模型, 其中, 原图的<i>L</i>通道作为网络的输入 (<i>input</i>) , 分别进入特征提取部分及分类网络部分, 得到预测分类及对应于<i>ab</i>通道的输出;而原图的分类标签及<i>ab</i>通道信息作为监督信息 (<i>label</i>) , 供网络输出进行对比以计算各类损失, 将所有损失反馈给网络, 按照损失梯度对权重进行调整, 从而训练整个网络, 实现着色。</p>
                </div>
                <div class="p1">
                    <p id="97">整个着色网络由以下部分组成:基于密集网络的特征提取部分 (<i>fusion</i>部分左侧所有<i>Dense</i>-<i>block</i> (密集块) 和<i>Transition</i>块) 、基于<i>VGG</i>的分类指导部分 (<i>fusion</i>层以下的所有<i>Conv</i>层级<i>fc</i>层) 、融合及输出部分 (<i>fusion</i>层级右侧部分) 。</p>
                </div>
                <div class="p1">
                    <p id="98">第一部分中的密集块详细内部结构如图4所示。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906044_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 密集块内部结构" src="Detail/GetImg?filename=images/JSJY201906044_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 密集块内部结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906044_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Internal structure of dense block</i></p>

                </div>
                <div class="p1">
                    <p id="100">网络信息提取子网络 (即密集连接子网络) 结构如表1所示, 网络分类特征子网络如表2所示。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表</b>1 <b>密集连接子网络</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Densely connected sub</i>-<i>networks</i></p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td><br />层级名称</td><td>结构</td><td>输出</td></tr><tr><td><br /><i>conv</i>1</td><td>7×7 <i>conv</i>, <i>stride</i> 2</td><td>16</td></tr><tr><td><br /><i>Dense</i>-<i>block</i>1</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>×</mo><mn>1</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>6</mn></mrow></math></td><td>88</td></tr><tr><td><br /><i>Transition</i>- 1</td><td>1×1 <i>conv</i></td><td>44</td></tr><tr><td><br /><i>Dense</i>-<i>block</i>2</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>×</mo><mn>1</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>1</mn><mn>2</mn></mrow></math></td><td>188</td></tr><tr><td><br /><i>Transition</i>- 2</td><td>1×1 <i>conv</i></td><td>94</td></tr><tr><td><br /><i>Dense</i>-<i>block</i>3</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>×</mo><mn>1</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>2</mn><mn>4</mn></mrow></math></td><td>382</td></tr><tr><td><br /><i>Transition</i>- 3</td><td>1×1 <i>conv</i></td><td>191</td></tr><tr><td><br /><i>Dense</i>-<i>block</i>4</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>×</mo><mn>1</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>1</mn><mn>6</mn></mrow></math></td><td>383</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表</b>2 <b>分类特征子网络</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Classification feature sub</i>-<i>networks</i></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td><br />层级名称</td><td>卷积核</td><td>步长</td><td>输出</td></tr><tr><td><br /><i>Conv</i>_1</td><td>3×3</td><td>2×2</td><td>64</td></tr><tr><td><br /><i>Conv</i>_2</td><td>3×3</td><td>1×1</td><td>128</td></tr><tr><td><br /><i>Conv</i>_3</td><td>3×3</td><td>2×2</td><td>128</td></tr><tr><td><br /><i>Conv</i>_4</td><td>3×3</td><td>1×1</td><td>256</td></tr><tr><td><br /><i>Conv</i>_5</td><td>3×3</td><td>2×2</td><td>256</td></tr><tr><td><br /><i>Conv</i>_6</td><td>3×3</td><td>1×1</td><td>512</td></tr><tr><td><br /><i>Conv</i>_7</td><td>3×3</td><td>2×2</td><td>512</td></tr><tr><td><br /><i>Conv</i>_8</td><td>3×3</td><td>1×1</td><td>512</td></tr><tr><td><br /><i>Conv</i>_9</td><td>3×3</td><td>2×2</td><td>512</td></tr><tr><td><br /><i>Conv</i>_10</td><td>3×3</td><td>1×1</td><td>512</td></tr><tr><td><br /><i>fc</i>1</td><td>—</td><td>—</td><td>4 096</td></tr><tr><td><br /><i>fc</i>2</td><td>—</td><td>—</td><td>256</td></tr><tr><td><br /><i>fc</i>3</td><td>—</td><td>—</td><td>64</td></tr><tr><td><br /><i>fc</i>4</td><td>—</td><td>—</td><td>1 024</td></tr><tr><td><br /><i>fc</i>5</td><td>—</td><td>—</td><td>205</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="103">网络特征融合层结构如表3所示。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表</b>3 <b>融合及输出网络</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Fusion and output networks</i></p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td><br />层级名称</td><td>卷积核</td><td>输出</td></tr><tr><td><br /><i>fusion</i>_<i>out</i></td><td>—</td><td>447</td></tr><tr><td><br /><i>fusion</i></td><td>1×1</td><td>128</td></tr><tr><td><br /><i>Dense</i>-<i>block</i>5</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>×</mo><mn>1</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mspace width="0.25em" /><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>4</mn></mrow></math></td><td>64</td></tr><tr><td><br /><i>output</i></td><td>3×3</td><td>2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">2.1.2 特征提取部分</h4>
                <div class="p1">
                    <p id="106">输入图像的<i>L</i>通道 (此部分即为灰度图, 大小为<i>H</i> ×<i>W</i>×1) 进入特征提取部分, 经过一层卷积后, 将依次进入4个密集块。每一个密集块中的不同卷积层与其后续的卷积层进行密集连接 (块内每层均为<i>k</i>个feature map, 本文设置<i>k</i>=12) 。鉴于密集网络的稠密性, 每一个3×3卷积前设置了一个1×1卷积 (结构如表1中的Dense-block部分) , 此操作可减少输入的feature map数量, 可实现降维效果, 减少计算量, 同时还可融合各个通道的特征。</p>
                </div>
                <div class="p1">
                    <p id="107">在每两个Dense-block之间, 增加了1×1的卷积操作 (即图3中的Transition层) , 该操作可减少上一个Dense-block输出的feature map数量 (本文网络设置为减少到一半) , 这将有效避免网络过于庞大, 减少进入下一个Dense-block后的计算负担。</p>
                </div>
                <div class="p1">
                    <p id="108">图像在经过具有上述特征的网络后, 大量细节特征及纹理信息将被提取出, 由于密集块中的卷积层都与前面的每一层保持连接, 低级的信息也将被有效利用, 有效减少了信息损失, 并改善了梯度弥散的问题。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">2.1.3 分类指导部分</h4>
                <div class="p1">
                    <p id="110">图像进入分类指导网络后, 网络将逐步通过卷积操作提取图像的分类信息, 全连接层<i>fc</i>1将提取到的特征重构为1×4 096的特征向量, 再经由<i>fc</i>2、<i>fc</i>3整合后得到维度为1×64的特征向量, 作为辅助信息进入<i>fusion</i>层, 帮助判别图像内容的类别。<i>fc</i>4和<i>fc</i>5层和输入图像的标签进行对比, 经过损失优化, 训练分类网络</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">2.1.4 融合及输出部分</h4>
                <div class="p1">
                    <p id="112">特征提取网络及分类指导网络均完成信息提取后, 可将二者进行融合, 进一步充分利用分类特征及细节纹理特征。由于特征提取部分和分类提取部分所得的特征图维度不同 (前者即<i>Dense</i>-<i>block</i>4, 为由密集块组成的特征提取网络产生的特征图, 尺寸仍为输入网络时的<i>H</i>×<i>W</i> (<i>H</i>=<i>W</i>=256) , 通道数为383;而后者即fc3, 为VGG分类子网络经卷积操作、全连接操作及重构整合后形成的一维特征向量, 尺寸为1×64) , 在融合时需要进行统一维度, 将两部分信息将重构为具有相同维度的feature map, 即将fc3层扩展重构为与Dense-block4特征图相同的大小<i>H</i>×<i>W</i>, 通道数为64。二者尺寸统一后完成通道连接融合, 形成尺寸为<i>H</i>×<i>W</i>, 通道数为447的特征图Fusion_out, 经卷积操作后成为fusion层 (维度为<i>H</i>×<i>W</i>×128) , 随后进入Dense-block5进行特征再提取, 最后经卷积操作后成为得到<i>H</i>×<i>W</i>×2的输出output, 此部分即为网络所给出的色彩部分预测值 (ab通道) , 与黑白通道 (L通道) 进行融合, 转化为RGB颜色空间, 就形成了一副彩色图像。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113">2.2 <b>构造损失函数</b></h4>
                <div class="p1">
                    <p id="114">网络的损失将作为调整权重的重要参考内容。为更好地调节网络的特征提取能力及分类性能, 本文综合了特征提取子网络的特征提取损失 (<i>L</i>1) 及分类指导子网络的分类损失 (<i>L</i>2) , 共同构成总网络的损失 (<i>Loss</i>) 。两部分损失均独立反馈给网络, 彼此不交互影响。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">2.2.1 特征提取损失<i>L</i>1</h4>
                <div class="p1">
                    <p id="116">网络训练时, 每一批次读取<i>n</i>幅图像。得到输出预测后, 将颜色预测值与原图进行对比, 计算特征损失。</p>
                </div>
                <div class="p1">
                    <p id="117">特征损失中主要组成部分为像素损失<i>L</i><sub>pixel</sub>, 如式 (3) ～ (4) 所示, 可用于衡量网络输出与真值之间的损失。</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>i</mtext><mtext>x</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>Μ</mi></mstyle><mi>S</mi><mi>E</mi><msub><mrow></mrow><mi>k</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>w</mi><mo>×</mo><mi>h</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>h</mi></munderover><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mi>Y</mi><mi>p</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mi>X</mi><mi>p</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">其中:<i>w</i>为样本的宽, <i>h</i>为高;<i>Yp</i>为原图像的ab通道色彩值, <i>Xp</i>为网络预测输出的ab通道色彩值;<i>n</i>为一个训练批次所包含的图像数量;<i>MSE</i>即为均方误差 (Mean Square Error) 。</p>
                </div>
                <div class="p1">
                    <p id="121">为防止网络模型陷入过拟合, 本文还引入了正则化损失<i>L</i><sub>regularization</sub>, 如式 (5) 所示:</p>
                </div>
                <div class="p1">
                    <p id="122"><i>L</i><sub>regularization</sub> = <i>α</i>*‖<i>ω</i>‖<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="124">其中:<i>α</i>为正则化系数;<i>ω</i>为网络中的权重。</p>
                </div>
                <div class="p1">
                    <p id="125">即特征提取部分的最终损失为式 (6) :</p>
                </div>
                <div class="p1">
                    <p id="126"><i>L</i><sub>1</sub> = <i>L</i><sub>pixel</sub> + <i>L</i><sub>regularization</sub>      (6) </p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">2.2.2 分类损失L2</h4>
                <div class="p1">
                    <p id="128">分类指导部分, 输入图像的分类信息作为指导标签<i>y</i><sup>label</sup>, 指导网络的预测结果为<i>y</i><sup>out</sup>, 采用交叉熵 (Cross-Entropy) 来衡量网络预测的分类与真实分类的损失, 即如式 (7) 所示:</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">[</mo></mstyle><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>y</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>l</mtext><mtext>a</mtext><mtext>b</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msubsup><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">在对<i>y</i><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup></mrow></math></mathml>求log函数值时, 如果<i>y</i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup></mrow></math></mathml>的值为0, 会出现log (<i>y</i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup></mrow></math></mathml>) 值为无穷, 本文在计算时, 令小于1E-10的数都等于1E-10。</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="135" name="135">3.1 <b>实验数据集及环境</b></h4>
                <div class="p1">
                    <p id="136">作为有监督的着色网络, 本文提出的网络需要大量有分类标签的彩色图像作为训练数据集, 故采用<i>MIT Places Database</i><citation id="313" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation> (含205个场景分类、250多万张图片) 、<i>ImageNet</i><citation id="314" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation> (含1 000个场景分类、120多万张图片) 两种数据集对网络进行训练。采用<i>HDF</i>5对数据集进行处理, 生成一个“.<i>h</i>5”类型的<i>data</i>文件, 不再需要依次读取大量单幅图片, 方便运行与维护。</p>
                </div>
                <div class="p1">
                    <p id="137">本文提出的着色网络需要进行大量的矩阵计算, 故为提高训练效率, 采用<i>GPU</i>进行训练, <i>GPU</i>型号为<i>NVIDIA Tesla M</i>40。在方法实现时, 采用<i>Python</i>编程环境, 基于<i>TensorFlow</i><citation id="315" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>架构进行网络搭建。</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138">3.2 <b>评价指标</b></h4>
                <div class="p1">
                    <p id="139">对灰度图像进行着色的目的, 主要是希望从着色结果中获得较灰度图像而言更丰富的信息, 那么, 着色结果是否清晰, 包含的信息量是否充分, 即可视为衡量着色算法优劣的重要指标。</p>
                </div>
                <div class="p1">
                    <p id="140">传统的图像客观评价指标采用的是峰值信噪比 (<i>Peak Signal</i>-<i>to</i>-<i>Noise Ratio</i>, <i>PSNR</i>) , 分值越高即认为若质量越好, 如式 (8) 所示, 其中<i>MSE</i>计算方式如式 (4) 所示。</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>S</mi><mi>Ν</mi><mi>R</mi><mo>=</mo><mn>1</mn><mn>0</mn><mo>×</mo><mrow><mi>lg</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mo stretchy="false"> (</mo><mn>2</mn><msup><mrow></mrow><mi>n</mi></msup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">此标准主要针对的是新图与原图像素之间的像素差异性, 分数无法和人眼看到的视觉品质完全一致, 有可能PSNR较高者看起来反而比PSNR较低者差, 不能较好地描述信息量丰富程度。为此本文引入了评价图像所含信息充分程度的广为采用的量化指标——图像熵 (Image Entropy) 。熵指的是某一特定体系的混乱的程度, 对图像而言, 图像熵越大, 图像包含的信息更丰富。通过计算整张图片彩色通道的信息熵, 判断图像包含的信息量。利用图像熵, 可从客观角度评价着色结果与人眼主观感受是否一致。其计算方式如式 (9) 所示:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>n</mi><mi>E</mi><mi>n</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>c</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">其中:<i>InEn</i>表示图像熵值;<i>P</i> (<i>i</i>) 表示值为<i>i</i>的颜色在整幅图像中出现的概率。</p>
                </div>
                <div class="p1">
                    <p id="145">本文构造了基于信息熵的评价标准, 通过计算着色图像的信息熵, 结合主观观察结果, 从而判断信息丰富程度, 同时, 主观观察结果也将验证指标的有效性。</p>
                </div>
                <div class="p1">
                    <p id="146">由于本文提出的网络主要是基于Lab空间对图像ab通道的色值进行预估, 不对图像的L通道 (反映图像的灰度信息) 进行重复的计算及处理, 故为提高效率, 本文在评价着色效果时, 仅考虑彩色通道所包含的信息。</p>
                </div>
                <h4 class="anchor-tag" id="147" name="147">3.3 <b>实验结果</b></h4>
                <h4 class="anchor-tag" id="148" name="148">3.3.1 有原图参考下的着色效果对比</h4>
                <div class="p1">
                    <p id="149">为了验证本文所提算法的有效性及优异性, 现选取了部分代表性的图片, 与现有表现优异的算法 (如<i>Zhang</i>等<citation id="316" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的基于<i>VGG</i>的着色算法, <i>Lizuka</i>等<citation id="317" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出的双流结构算法, <i>Qin</i>等<citation id="318" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出的基于残差网络算法) 进行比较, 从是否漏色、色彩对比度、细节信息损失程度等方面对上述算法着色效果进行比较, 具体如图5所示。着色效果对比中, <i>Zhang</i>等<citation id="319" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法、<i>Lizuka</i>等<citation id="320" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>算法的着色效果均来自其对外公示网站的着色结果, <i>Qin</i>等<citation id="321" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>算法的着色效果采用其最终版着色模型。</p>
                </div>
                <div class="p1">
                    <p id="150">是否漏色 对图像中的物体着色时, 需要准确地识别物体的边界, 否则将会发生颜色漏色, 如图5中组 (<i>e</i>) 箭头指出部分, <i>Zhang</i>等<citation id="322" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法、<i>Lizuka</i>等<citation id="323" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>算法将天空的色彩渗漏到地面上, 使得着色效果不太理想, 本文算法在此方面表现较好。</p>
                </div>
                <div class="p1">
                    <p id="151">色彩对比度 在对灰度图像着色时要注意彩色图像中物体的颜色是各不相同的, 图5中组 (<i>a</i>) ～ (<i>d</i>) 中<i>Zhang</i>等<citation id="324" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法、<i>Lizuka</i>等<citation id="325" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>算法的着色结果倾向于整体赋予暖黄色色调, 各个物体之间的颜色没有较好地区分开来, 特别是天花板的颜色也混入主色调。 <i>Qin</i>等<citation id="326" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>算法着色效果物体区分度有了一定改善, 但不如本文算法着色结果色彩对比度更鲜明, 本文算法所得图像的色彩对比度就饱和度更高, 各个物体的颜色彼此独立, 不受整体色调蔓延影响, 如本文算法组 (<i>a</i>) 及组 (<i>c</i>) 中的地板更接近于真实的木地板色泽纹理;组 (<i>b</i>) 中的床上用品颜色更为鲜艳。</p>
                </div>
                <div class="p1">
                    <p id="152">细节信息损失程度 在整体着色能达到一定效果时, 细节信息可否良好还原即成为了重要衡量指标。为对比细节效果, 将图5中组 (<i>b</i>) 和组 (<i>d</i>) 的细节进行放大对比, 详见图6。图6 (<i>a</i>) 中 (即图5组 (<i>b</i>) 中虚线框圈出的床头挂画中的绿色植物) , 本文算法准确地赋予了应有的绿色, 但其他三种并没有合理地着色, 与主色调融为一体;图6 (<i>b</i>) 中本文算法相对于其他算法而言, 较好地为盆栽植物及挂画中的景物赋予了绿色。从对比效果可看出, 本文算法在实现整体着色时, 不会忽略细节部分, 具备更好的细节处理能力, 能较好地对细节部位予以着色。</p>
                </div>
                <div class="p1">
                    <p id="153">此外, 为客观评价各个算法的性能优劣, 本文采用了3.2节中所述的图像熵<i>InEn</i>作为评价指标, 计算方式如式 (9) 所示, 同时列出了采用式 (8) 中PSNR值, 即峰值信噪比计算的结果, 对比结果详见表4。表4中图像熵指标采用加粗的方式表示性能最好的算法, 由图像熵的定义可知, 熵值更大的, 表示信息量更丰富;峰值信噪比指标也采用加粗的方式表示性能最好的算法, 按照定义, 峰值信噪比越高, 图像质量越好。</p>
                </div>
                <div class="p1">
                    <p id="154">从图5性能对比可以看出, 采用本文所用评价指标图像熵<i>InEn</i>, 其评价结果与人眼所观察的效果基本一致, 这在客观上说明了本文算法的优异性, 同时也印证了本文采用评价指标的有效性, 而采用PSNR方式进行客观评价, 按此指标最高的为图像质量最好的, 但实际效果并非如此。</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906044_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同算法的着色效果对比" src="Detail/GetImg?filename=images/JSJY201906044_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同算法的着色效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906044_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Comparison of colorization effects of different algorithms</p>

                </div>
                <div class="area_img" id="156">
                    <p class="img_tit"><b>表</b>4 <b>图</b>5<b>中着色效果的图像熵及峰值信噪比对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Comparison of image entropy and PSNR of colorization results in Fig. 5</p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td rowspan="2">分组</td><td colspan="2"><br />Zhang等<sup>[14]</sup>算法</td><td rowspan="2"></td><td colspan="2"><br />Lizuka等<sup>[18]</sup>算法</td><td rowspan="2"></td><td colspan="2"><br />Qin等<sup>[19]</sup>算法</td><td rowspan="2"></td><td colspan="2"><br />本文算法</td></tr><tr><td><br />图像熵</td><td>峰值信<br />噪比/dB</td><td><br />图像熵</td><td>峰值信<br />噪比/dB</td><td><br />图像熵</td><td>峰值信<br />噪比/dB</td><td><br />图像熵</td><td>峰值信<br />噪比/dB</td></tr><tr><td> (a) </td><td>9.187 97</td><td>29.432 47</td><td></td><td>7.824 58</td><td>30.440 04</td><td></td><td>8.952 01</td><td>28.564 13</td><td></td><td>9.673 21</td><td>28.808 04</td></tr><tr><td><br /> (b) </td><td>9.928 10</td><td>29.210 01</td><td></td><td>10.460 60</td><td>29.326 72</td><td></td><td>9.289 61</td><td>28.296 39</td><td></td><td>11.823 41</td><td>28.235 19</td></tr><tr><td><br /> (c) </td><td>9.035 74</td><td>30.360 73</td><td></td><td>8.413 42</td><td>30.460 23</td><td></td><td>9.177 65</td><td>28.135 57</td><td></td><td>10.088 46</td><td>28.488 87</td></tr><tr><td><br /> (d) </td><td>9.844 41</td><td>29.445 68</td><td></td><td>8.774 61</td><td>29.557 82</td><td></td><td>8.504 54</td><td>27.960 64</td><td></td><td>9.626 60</td><td>27.869 34</td></tr><tr><td><br /> (e) </td><td>9.087 64</td><td>29.709 13</td><td></td><td>9.761 14</td><td>30.575 04</td><td></td><td>11.028 95</td><td>29.278 57</td><td></td><td>11.145 08</td><td>29.801 92</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="157">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906044_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 着色效果细节对比" src="Detail/GetImg?filename=images/JSJY201906044_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 着色效果细节对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906044_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of details of colorization effects</p>

                </div>
                <h4 class="anchor-tag" id="158" name="158">3.3.2 老照片及黑白图像着色效果对比</h4>
                <div class="p1">
                    <p id="159">为了验证本文算法的普适性, 现选取了部分老照片及黑白图像进行着色效果比较, 对比如图7所示。</p>
                </div>
                <div class="p1">
                    <p id="160"><i>Zhang</i>等<citation id="327" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法、<i>Lizuka</i>等<citation id="328" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>算法存在些许着色不均匀、漏色的问题, 如图7 (<i>a</i>) 的门柱和地面;图7 (<i>b</i>) 、 (<i>c</i>) 中天空在湖水中的倒影应为蓝色色调, <i>Zhang</i>等<citation id="329" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法着色为绿色;图7 (<i>c</i>) 中山体在水中的倒影, 只有本文算法没有被旁边绿色植物的倒影影响变为绿色;图7 (<i>d</i>) 中, <i>Zhang</i>等<citation id="330" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法、<i>Lizuka</i>等<citation id="331" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>算法、<i>Qin</i>等<citation id="332" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>算法的文字牌匾及屋顶均存在一定程度的漏色、色调暗沉等问题, 本文算法则表现良好。</p>
                </div>
                <div class="p1">
                    <p id="161">从上述老照片及黑白照片着色效果来看, 本文提出的算法较<i>Zhang</i>等<citation id="333" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法、<i>Lizuka</i>等<citation id="334" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>算法、<i>Qin</i>等<citation id="335" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>算法而言, 较少出现漏色现象, 细节更为丰富, 对比度也更好。</p>
                </div>
                <h4 class="anchor-tag" id="162" name="162">3.3.3 信息损失对比</h4>
                <div class="p1">
                    <p id="163">从图5和图7的着色效果对比中可以得知, <i>Qin</i>等<citation id="336" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出的基于残差神经网络的着色算法较之前的表现优秀的深度学习着色算法而言, 在一定程度上减少了信息损失, 着色效果也有所改善。将本文算法与其进行对比, 通过随机选取5 000张图片, 按照本文3.2节中的评价指标, 比较两种着色方法的信息熵, 即<i>InEn</i>值, Qin等<citation id="337" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>算法的算法均值为8.60, 而本文算法均值为10.34, 图8为数据对比。</p>
                </div>
                <div class="p1">
                    <p id="164">上述的实验结果可以客观地反映出, 本文着色算法较Qin等<citation id="338" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>的算法取得了明显改善, 信息量更丰富, 着色效果更好。</p>
                </div>
                <div class="area_img" id="165">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906044_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 黑白老照片着色效果" src="Detail/GetImg?filename=images/JSJY201906044_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 黑白老照片着色效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906044_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Colorization effects of grayscale old pictures</p>

                </div>
                <div class="area_img" id="166">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906044_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同算法信息损失对比" src="Detail/GetImg?filename=images/JSJY201906044_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同算法信息损失对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906044_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Information loss comparison of different algorithms</p>

                </div>
                <h3 id="167" name="167" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="168">本文提出了一种基于密集神经网络的灰度图像着色算法, 该算法通过密集模块构成的子网络和<i>VGG</i>分类子网络分别对图像的细节特征纹理及分类信息进行提取, 二者融合后实现对彩色结果的预测与输出。通过实验证明, 本文算法较现有的优秀灰度图像着色算法而言, 信息量可提高1%～20%, 色泽和对比度等方面也取得了较大的改善, 漏色程度也明显减小, 运用于老照片及黑白照片等方面也可取得不错的效果。</p>
                </div>
                <div class="p1">
                    <p id="169">本文将密集神经网络引入图像着色任务中, 尚未针对性地研究引入后续层级的低级层级特征中, 哪些层级对后期层级及网络输出有较大的影响, 是否有必要全部加入后期层级, 无差别地将前期层级提取的特征融入到后期层级中, 可能存在一定的“冗余连接”, 造成了该网络的稠密性, 从而对于运行设备性能要求较高, 网络训练时间也较长。同时, 由于采用的数据集并未涵盖所有图像类别, 本文所提的算法对于未学习过的图像着色效果还不太理想。</p>
                </div>
                <div class="p1">
                    <p id="170">在下一阶段的研究中, 首先可考虑通过优化密集网络部分及整体网络架构, 尝试通过引入自适应学习特征权重类的方式, 或自适应剪枝算法, 判别早期特征的重要程度, 降低网络稠密性, 提高整体效率;同时, 可尽量多地尝试更多种图像类别, 进一步强化算法的普适性与实用性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="226">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121043970001&amp;v=MjQ0NjRaT3NQRFJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5uVTdmSktGMFRYRnF6R2JLNkg5SElySVpD&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>冈萨雷斯, 伍兹.数字图像处理[M].3版.阮秋琦, 译.北京:电子工业出版社, 2007:484-486. (GONZALEZ R C, WOODS RE.Digital Image Processing[M].3rd ed.RUAN Q Q, translated.Beijing:Publishing House of Electronics Industry, 2007:484-486.) 
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098145&amp;v=MDU3MDVGOGNhUlU9TmlmSVk3SzdIdGpOcjQ5RlpPSUhEWGc4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>LEVIN A, LISCHINSKI D, WEISS Y.Colorization using optimization[J].ACM Transactions on Graphics, 2004, 23 (3) :689-694.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Medical image colorization">

                                <b>[3]</b>LAGODZINSKI P, SMOLKA B.Medical image colorization[J].Journal of Medical Informatics&amp;Technologies, 2007 (11) :47-57.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Colorization of medical images">

                                <b>[4]</b>LAGODZINSKI P, SMOLKA B.Colorization of medical images[J].China Healthcare Innovation, 2009, 15 (4) :13-23.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Medical Image Colorization using Optimization Technique">

                                <b>[5]</b>SHAH A A, MIKITA G, SHAH K M.Medical image colorization using optimization technique[J].Acta Medica Okayama, 2013, 62 (141) :235-248.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZTSX201302008&amp;v=MjgxNDVyRzRIOUxNclk5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGtXN3pQUHpuWWQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>LI F, ZHU L, ZHANG L, et al.Pseudo-colorization of medical images based on two-stage transfer model[J].Chinese Journal of Stereology and Image Analysis, 2013, 18 (2) :135-144.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000097914&amp;v=MjMyODFCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGOGNhUlU9TmlmSVk3SzdIdGpOcjQ5RlpPSUlCWDA5bw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>WELSH T, ASHIKHMIN M, MUELLER K.Transferring color to greyscale images[J].ACM Transactions on Graphics, 2002, 21 (3) :277-280.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098716&amp;v=MTY4MjdQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGOGNhUlU9TmlmSVk3SzdIdGpOcjQ5RlpPSUhDMzAvb0JNVDZUNA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>LIU X, WAN L, QU Y, et al.Intrinsic colorization[J].ACMTransactions on Graphics, 2008, 27 (5) :Article No.152.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD14071600000756&amp;v=MDk5NjFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRjhjYVJVPU5pZmNhcks4SHRiTnFZOUZaT3NQQzNrL29CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>LIU Y, COHEN M, UYTTENDAELE M, et al.Auto Style:automatic style transfer from image collections to users'images[J].Computer Graphics Forum, 2014, 33 (4) :21-31.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic colorization of grayscale images using multiple images on the Web">

                                <b>[10]</b>MORIMOTO Y, TAGUCHI Y, NAEMURA T.Automatic colorization of grayscale images using multiple images on the Web[C]//Proceedings of ACM SIGGRAPH 2009.New York:ACM, 2009:Article No.59.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Colorization by example">

                                <b>[11]</b>IRONY R, COHEN-OR D, LISCHINSKI D.Colorization by example[C]//Proceedings of the 16th Eurographics Conference on Rendering Techniques.Aire-la-Ville, Switzerland:Eurographics Association, 2005:201-210.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Colorization">

                                <b>[12]</b>CHENG Z, YANG Q, SHENG B.Deep colorization[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:415-423.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Large-Scale Automatic Image Colorization">

                                <b>[13]</b>DESHPANDE A, ROCK J, FORSYTH D.Learning large-scale automatic image colorization[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:567-575.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Colorful Image Colorization">

                                <b>[14]</b>ZHANG R, ISOLA P, EFROS A A.Colorful image colorization[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Amsterdam:Springer International Publishing, 2016:649-666.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition.[OL]">

                                <b>[15]</b>SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J/OL].ar Xiv Preprint, 2014, 2014:ar Xiv.1409.1556 (2014-09-04) [2018-08-10].http://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[16]</b>RONNEBERGER O, FISCHER P, BROX T.U-Net:Convolutional networks for biomedical image segmentation[C]//MICCAI2015:Proceedings of the 2015 Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer International Publishing, 2015:234-241.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM789DD952ADDAB5927133A9860A753458&amp;v=MDA0OTErT1hicXFoSkVmcmVYUWIrWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMbTN3Njg9TmlmSVk3U3dGNlc0cG9wSEZaOTdmUTQ4eGhRVTZ6eA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>ZHANG R, ZHU J Y, ISOLA P, et al.Real-time user-guided image colorization with learned deep priors[J].ACM Transactions on Graphics, 2017, 36 (4) :Article No.119.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMAFFF2D121F4EA546037EFF6E3BD86EAE&amp;v=MDI5OTV2RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6TG0zdzY4PU5pZklZOExPYUtmTzI0NUhaWjBMZVEwOHl4QVQ2VGdJUGduazJSRkhEYnFTTU12cUNPTg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>LIZUKA S, SIMOSERRA E, ISHIKAWA H.Let there be color!:joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification[J].ACM Transactions on Graphics, 2016, 35 (4) :Article No.110.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on image colorization algorithm based on residual neural network">

                                <b>[19]</b>QIN P L, CHENG Z R, CUI Y H, et al.Research on image colorization algorithm based on residual neural network[C]//CCCV2017:Proceedings of the 2017 CCF Chinese Conference on Computer Vision, CCIS 771.Berlin:Springer, 2017:608-621.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_20" >
                                    <b>[20]</b>
                                HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[21]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[22]</b>SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Highway networks">

                                <b>[23]</b>SRIVASTAVA R K, GREFF K, SCHMIDHUBER J.Highway networks[J/OL].ar Xiv Preprint, 2015, 2015:ar Xiv.1505.00387 (2015-03-03) [2018-08-10].https://arxiv.org/abs/1505.00387.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep networks with stochastic depth">

                                <b>[24]</b>HUANG G, SUN Y, LIU Z, et al.Deep networks with stochastic depth[C]//ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:646-661.
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fractal Net:ultra-deep neural networks without residuals">

                                <b>[25]</b>LARSSON G, MAIRE M, SHAKHNAROVICH G.Fractal Net:ultra-deep neural networks without residuals[J/OL].ar Xiv Preprint, 2016, 2016:ar Xiv.1605.07648 (2016-05-24) [2018-08-16].https://arxiv.org/abs/1605.07648.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DenselyConnected Convolutional Networks">

                                <b>[26]</b>HUANG G, LIU Z, LAURENS V D M, et al.Densely connected convolutional networks[C]//CVPR 2017:Proceedings of the2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2261-2269.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep features for scene recognition using places database">

                                <b>[27]</b>ZHOU B, LAPEDRIZA A, XIAO J, et al.Learning deep features for scene recognition using places database[C]//NIPS 2014:Proceedings of the 2014 27th International Conference on Neural Information Processing Systems.Cambridge, CA:MIT Press, 2014:487-495.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:A large-scale hierarchical image database">

                                <b>[28]</b>DENG J, DONG W, SOCHER R, et al.Image Net:a large-scale hierarchical image database[C]//CVPR 2009:Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2009:248-255.
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:Largescale machine learning on heterogeneous distributed Systems[OL]">

                                <b>[29]</b>ABADI M, AGARWAL A, BARHAM P, et al.Tensor Flow:large-scale machine learning on heterogeneous distributed systems[J/OL].ar Xiv Preprint, 2016, 2016:ar Xiv.1603.04467[2018-08-14].https://arxiv.org/abs/1603.04467.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906044" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906044&amp;v=MDAwNDFyQ1VSN3FmWnVac0Z5RGtXN3pNTHo3QmQ3RzRIOWpNcVk5QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
