<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136456469815000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910018%26RESULT%3d1%26SIGN%3dUkAW7ZRPJVcCrN3mOxNgE5mTGU4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910018&amp;v=MzE3Mzg3TEx6N0JkN0c0SDlqTnI0OUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="1 理论基础 ">1 理论基础</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="1.1 &lt;b&gt;局部性约束字典学习方法&lt;/b&gt;">1.1 <b>局部性约束字典学习方法</b></a></li>
                                                <li><a href="#62" data-title="1.2 &lt;b&gt;结构化遮挡编码&lt;/b&gt;">1.2 <b>结构化遮挡编码</b></a></li>
                                                <li><a href="#77" data-title="1.3 &lt;b&gt;极限学习机&lt;/b&gt;">1.3 <b>极限学习机</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="2 SOC-ELM模型结构和算法 ">2 SOC-ELM模型结构和算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="2.1 &lt;i&gt;SOC&lt;/i&gt;-&lt;i&gt;ELM&lt;/i&gt;&lt;b&gt;模型结构&lt;/b&gt;">2.1 <i>SOC</i>-<i>ELM</i><b>模型结构</b></a></li>
                                                <li><a href="#97" data-title="2.2 &lt;i&gt;SOC&lt;/i&gt;-&lt;i&gt;ELM&lt;/i&gt;&lt;b&gt;算法&lt;/b&gt;">2.2 <i>SOC</i>-<i>ELM</i><b>算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="3 实验 ">3 实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="3.1 &lt;b&gt;实验数据集&lt;/b&gt;">3.1 <b>实验数据集</b></a></li>
                                                <li><a href="#120" data-title="3.2 &lt;b&gt;实验结果分析&lt;/b&gt;">3.2 <b>实验结果分析</b></a></li>
                                                <li><a href="#134" data-title="3.3 &lt;b&gt;运行时间&lt;/b&gt;">3.3 <b>运行时间</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="图1 现实世界中的各种遮挡图像">图1 现实世界中的各种遮挡图像</a></li>
                                                <li><a href="#76" data-title="图2 SOC分离遮挡实例">图2 SOC分离遮挡实例</a></li>
                                                <li><a href="#79" data-title="图3 &lt;i&gt;ELM&lt;/i&gt;原理">图3 <i>ELM</i>原理</a></li>
                                                <li><a href="#96" data-title="图4 &lt;i&gt;SOC&lt;/i&gt;-&lt;i&gt;ELM&lt;/i&gt;模型结构">图4 <i>SOC</i>-<i>ELM</i>模型结构</a></li>
                                                <li><a href="#115" data-title="图5 &lt;i&gt;AR&lt;/i&gt;人脸数据库局部样本图像">图5 <i>AR</i>人脸数据库局部样本图像</a></li>
                                                <li><a href="#117" data-title="图6 &lt;i&gt;CelebA&lt;/i&gt;人脸数据库局部样本图像">图6 <i>CelebA</i>人脸数据库局部样本图像</a></li>
                                                <li><a href="#125" data-title="图7 墨镜遮挡三种方法结果对比">图7 墨镜遮挡三种方法结果对比</a></li>
                                                <li><a href="#126" data-title="图8 围巾遮挡三种方法结果对比">图8 围巾遮挡三种方法结果对比</a></li>
                                                <li><a href="#127" data-title="图9 在&lt;i&gt;CelebA&lt;/i&gt;数据上修复的结果">图9 在<i>CelebA</i>数据上修复的结果</a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;表&lt;/b&gt;1 9&lt;b&gt;种不同迭代次数下三种模型的&lt;/b&gt;&lt;i&gt;SSIM&lt;/i&gt;&lt;b&gt;比较&lt;/b&gt;"><b>表</b>1 9<b>种不同迭代次数下三种模型的</b><i>SSIM</i><b>比较</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表&lt;/b&gt;2 9&lt;b&gt;种不同迭代次数下三种模型的&lt;/b&gt;&lt;i&gt;PSNR&lt;/i&gt;&lt;b&gt;比较&lt;/b&gt;"><b>表</b>2 9<b>种不同迭代次数下三种模型的</b><i>PSNR</i><b>比较</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;三种算法的平均运行时间&lt;/b&gt;"><b>表</b>3 <b>三种算法的平均运行时间</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="176">


                                    <a id="bibliography_1" title="ILIADIS M,WANG H,MOLINA R,et al.Robust and low-rank representation for fast face identification with occlusions[J].IEEETransactions on Image Processing,2017,26(5):2203-2218." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust and Low-Rank Representation for Fast Face Identification with Occlusions">
                                        <b>[1]</b>
                                        ILIADIS M,WANG H,MOLINA R,et al.Robust and low-rank representation for fast face identification with occlusions[J].IEEETransactions on Image Processing,2017,26(5):2203-2218.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_2" title="WEN Y,LIU W,YANG M,et al.Structured occlusion coding for robust face recognition[J].Neurocomputing,2016,178(C):11-24." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC5D3B1558757D67027416415FEAAF3EA&amp;v=MjY5MTdTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeGJ5Nndhcz1OaWZPZmNDOWF0Sytyb3BBYk93S0N3Zy95QllSN1R0OFRudmpxV1JBQ01QaVJzL3VDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        WEN Y,LIU W,YANG M,et al.Structured occlusion coding for robust face recognition[J].Neurocomputing,2016,178(C):11-24.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_3" title="HE R,ZHENG W,TAN T,et al.Half-quadratic-based iterative minimization for robust sparse representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,36(2):261-275." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Half-quadratic-based iterative minimization for robust sparse representation">
                                        <b>[3]</b>
                                        HE R,ZHENG W,TAN T,et al.Half-quadratic-based iterative minimization for robust sparse representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,36(2):261-275.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_4" title="ZHAO F,FENG J,ZHAO J,et al.Robust LSTM-autoencoders for face de-occlusion in the wild[J].IEEE Transactions on Image Processing,2016,27(2):778-790." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust LSTM-Autoencod-ers for Face De-Occlusion in the Wild">
                                        <b>[4]</b>
                                        ZHAO F,FENG J,ZHAO J,et al.Robust LSTM-autoencoders for face de-occlusion in the wild[J].IEEE Transactions on Image Processing,2016,27(2):778-790.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_5" title="MOHAMMAD O A,BOUBAKEUR B.Occlusion handling based on sub-blobbing in automated video surveillance system[C]//Proceedings of the 4th International Conference on Computer Science and Software Engineering.New York:ACM,2011:139-143." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Occlusion handling based on sub-blobbing in automated video surveillance system">
                                        <b>[5]</b>
                                        MOHAMMAD O A,BOUBAKEUR B.Occlusion handling based on sub-blobbing in automated video surveillance system[C]//Proceedings of the 4th International Conference on Computer Science and Software Engineering.New York:ACM,2011:139-143.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_6" title="CUCCHIARA R,GRANA C,TARDINI G.Track-based and object-based occlusion for people tracking refinement in indoor surveillance[C]//Proceedings of the ACM 2nd International Workshop on Video Surveillance&amp;amp;Sensor Networks.New York:ACM,2004:81-87." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Track-based and object-based occlusion forpeople tracking refinement in indoor surveillance">
                                        <b>[6]</b>
                                        CUCCHIARA R,GRANA C,TARDINI G.Track-based and object-based occlusion for people tracking refinement in indoor surveillance[C]//Proceedings of the ACM 2nd International Workshop on Video Surveillance&amp;amp;Sensor Networks.New York:ACM,2004:81-87.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_7" title="ONG F,LUSTIG M.Beyond low rank+sparse:multiscale low rank matrix decomposition[J].IEEE Journal of Selected Topics in Signal Processing,2016,10(4):672-687." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond Low Rank+Sparse:Multiscale Low Rank Matrix Decomposition">
                                        <b>[7]</b>
                                        ONG F,LUSTIG M.Beyond low rank+sparse:multiscale low rank matrix decomposition[J].IEEE Journal of Selected Topics in Signal Processing,2016,10(4):672-687.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_8" title="OROUGHI H,SHAKERI M,RAY N,et al.Face recognition using multi-modal low-rank dictionary learning[C]//Proceedings of the2017 IEEE International Conference on Image Processing.Piscataway:IEEE,2017:1081-1086." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition using multi-modal low-rank dictionary learning">
                                        <b>[8]</b>
                                        OROUGHI H,SHAKERI M,RAY N,et al.Face recognition using multi-modal low-rank dictionary learning[C]//Proceedings of the2017 IEEE International Conference on Image Processing.Piscataway:IEEE,2017:1081-1086.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_9" title="唐娴,黄军伟.低秩鲁棒性主成分分析的遮挡人脸识别[J].南京理工大学学报,2017,41(4):460-465.(TANG W,HUANG JW.Occlusion face recognition based on low rank robust principal component analysis[J].Journal of Nanjing University of Science and Technology,2017,41(4):460-465.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJLG201704010&amp;v=MTk4NTRuaFZyN0xLeWZIYWJHNEg5Yk1xNDlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        唐娴,黄军伟.低秩鲁棒性主成分分析的遮挡人脸识别[J].南京理工大学学报,2017,41(4):460-465.(TANG W,HUANG JW.Occlusion face recognition based on low rank robust principal component analysis[J].Journal of Nanjing University of Science and Technology,2017,41(4):460-465.)
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_10" title="李晋江,张彩明,范辉,等.基于分形的图像修复算法[J].电子学报,2010,38(10):2430-2435.(LI J J,ZHANG C M,FAN H,et al.Fractal-based image restoration algorithm[J].Acta Electronica Sinica,2010,38(10):2430-2435.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201010038&amp;v=MTY5MDFyQ1VSN3FmWnVac0Z5bmhWcjdMSVRmVGU3RzRIOUhOcjQ5R2JJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        李晋江,张彩明,范辉,等.基于分形的图像修复算法[J].电子学报,2010,38(10):2430-2435.(LI J J,ZHANG C M,FAN H,et al.Fractal-based image restoration algorithm[J].Acta Electronica Sinica,2010,38(10):2430-2435.)
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_11" title="DING Z M,SUH S,HAN J,et al.Discriminative low-rank metric learning for face recognition[C]//Proceedings of the 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Piscataway:IEEE,2015:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative low-rank metric learning for face recognition">
                                        <b>[11]</b>
                                        DING Z M,SUH S,HAN J,et al.Discriminative low-rank metric learning for face recognition[C]//Proceedings of the 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Piscataway:IEEE,2015:1-6.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_12" title="DING X,LIU X,XU L.An optimization method of extreme learning machine for regression[C]//Proceedings of the 31st Annual ACM Symposium on Applied Computing.New York:ACM,2016:891-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An optimization method of extreme learning machine for regression">
                                        <b>[12]</b>
                                        DING X,LIU X,XU L.An optimization method of extreme learning machine for regression[C]//Proceedings of the 31st Annual ACM Symposium on Applied Computing.New York:ACM,2016:891-893.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_13" title="YANG M,ZHANG L,SHIU S,et al.Gabor feature based robust representation and classification for face recognition with Gabor occlusion dictionary[J].Pattern Recognition,2014,46(7):1559-1572." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739928&amp;v=MjAyODhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJVm9SYXhFPU5pZk9mYks3SHRETnFZOUZZK2dHQlg0eA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        YANG M,ZHANG L,SHIU S,et al.Gabor feature based robust representation and classification for face recognition with Gabor occlusion dictionary[J].Pattern Recognition,2014,46(7):1559-1572.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_14" title="HE R,ZHENG W,HU B,et al.Two-stage nonnegative sparse representation for large-scale face recognition[J].IEEE Transactions on Neural Networks and Learning Systems,2013,24(1):35-46." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two-Stage Nonnegative Sparse Representation for Large-Scale Face Recognition">
                                        <b>[14]</b>
                                        HE R,ZHENG W,HU B,et al.Two-stage nonnegative sparse representation for large-scale face recognition[J].IEEE Transactions on Neural Networks and Learning Systems,2013,24(1):35-46.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_15" title="朱明旱,李树涛,叶华.稀疏表示分类中遮挡字典构造方法的改进[J].计算机辅助设计与图形学报,2014,26(11):2064-2078.(ZHU M H,LI S T,YE H.Improvement of the construction method of occlusion dictionary in sparse representation classification[J].Journal of Computer-Aided Design&amp;amp;Computer Graphics,2014,26(11):2064-2078.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201411019&amp;v=MTQ0OTZxQnRHRnJDVVI3cWZadVpzRnluaFZyN0xMejdCYUxHNEg5WE5ybzlFYllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        朱明旱,李树涛,叶华.稀疏表示分类中遮挡字典构造方法的改进[J].计算机辅助设计与图形学报,2014,26(11):2064-2078.(ZHU M H,LI S T,YE H.Improvement of the construction method of occlusion dictionary in sparse representation classification[J].Journal of Computer-Aided Design&amp;amp;Computer Graphics,2014,26(11):2064-2078.)
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_16" title="SING Y,CHENG Y.Noise-resistant network:a deep-learning method for face recognition under noise[J].EURASIP Journal on Image and Video Processing,2017,2017:Article number 43." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD4E05A58FA6C51E70A80FD6CAD268F96D&amp;v=MDU3NTV2RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4Ynk2d2FzPU5qN0JhcmZOSHRTOXFvY3pGZTE4Q1gxTXlCWmk0ajhMUEhtUjNXWTNmN3JpVEx6ckNPTg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        SING Y,CHENG Y.Noise-resistant network:a deep-learning method for face recognition under noise[J].EURASIP Journal on Image and Video Processing,2017,2017:Article number 43.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_17" title="ZHOU Y,BARNER K.Locality constrained dictionary learning for nonlinear dimensionality reduction[J].IEEE Signal Processing Letters,2013,20(4):335-338." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locality constrained dictionary learning for nonlinear dimensionality reduction">
                                        <b>[17]</b>
                                        ZHOU Y,BARNER K.Locality constrained dictionary learning for nonlinear dimensionality reduction[J].IEEE Signal Processing Letters,2013,20(4):335-338.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_18" title="ZHOU Z,WAGNER A,MOBAHI H,et al.Face recognition with contiguous occlusion using Markov random fields[C]//Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway:IEEE,2009:1050-1057." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition with contiguous occlusion using markov random fields">
                                        <b>[18]</b>
                                        ZHOU Z,WAGNER A,MOBAHI H,et al.Face recognition with contiguous occlusion using Markov random fields[C]//Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway:IEEE,2009:1050-1057.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_19" title="刘丽娜,马世伟,温加睿.基于局部约束字典学习的数据降维和重构方法[J].仪表仪器学报,2016,37(1):99-108.(LIUL N,MA S W,WEN J R.Data dimension reduction and reconstruction method based on local constraint dictionary learning[J].Journal of Instrument and Instrument,2016,37(1):99-108.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201601015&amp;v=MDE4OTZxQnRHRnJDVVI3cWZadVpzRnluaFZyN0xQRHpUYkxHNEg5Zk1ybzlFWVlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        刘丽娜,马世伟,温加睿.基于局部约束字典学习的数据降维和重构方法[J].仪表仪器学报,2016,37(1):99-108.(LIUL N,MA S W,WEN J R.Data dimension reduction and reconstruction method based on local constraint dictionary learning[J].Journal of Instrument and Instrument,2016,37(1):99-108.)
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_20" title="SASTRAWAHA S,HORATA P.Ensemble extreme learning machine for multi-instance learning[J]//Proceedings of the 9th International Conference on Machine Learning and Computing.New York:ACM,2017:56-60." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble extreme learning machine for multi-instance learning">
                                        <b>[20]</b>
                                        SASTRAWAHA S,HORATA P.Ensemble extreme learning machine for multi-instance learning[J]//Proceedings of the 9th International Conference on Machine Learning and Computing.New York:ACM,2017:56-60.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_21" title="彭双.神经网络隐层节点的稀疏化[D].大连:大连理工大学,2017:1-48.(PENG S.Sparseization of hidden nodes in neural networks[D].Dalian:Dalian University of Technology,2017:1-48.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017821936.nh&amp;v=MTk0MTR6cXFCdEdGckNVUjdxZlp1WnNGeW5oVnI3TFZGMjZHYnU2SDlqUHFaRWJQSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        彭双.神经网络隐层节点的稀疏化[D].大连:大连理工大学,2017:1-48.(PENG S.Sparseization of hidden nodes in neural networks[D].Dalian:Dalian University of Technology,2017:1-48.)
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_22" title="PATHAK D,DONAHUE P,DARRELL T,et al.Context encoders:feature learning by inpainting[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:2536-2544." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Context Encoders:Feature Learning by Inpainting">
                                        <b>[22]</b>
                                        PATHAK D,DONAHUE P,DARRELL T,et al.Context encoders:feature learning by inpainting[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:2536-2544.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_23" title="YEH R,CHEN C,LIMT Y,et al.Semantic image inpainting with perceptual and contextual losses[C]//Proceedings of the2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6882-6890." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic image inpainting with perceptual and contextual losses">
                                        <b>[23]</b>
                                        YEH R,CHEN C,LIMT Y,et al.Semantic image inpainting with perceptual and contextual losses[C]//Proceedings of the2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6882-6890.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-31 14:09</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2893-2898 DOI:10.11772/j.issn.1001-9081.2019051176            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于结构化遮挡编码和极限学习机的局部遮挡人脸识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E8%8A%B3%E8%89%B3&amp;code=42897029&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张芳艳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%96%B0&amp;code=09656957&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王新</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AE%B8%E6%96%B0%E5%BE%81&amp;code=11277633&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">许新征</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%9F%BF%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0041682&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国矿业大学计算机科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%85%89%E7%94%B5%E6%8A%80%E6%9C%AF%E4%B8%8E%E6%99%BA%E8%83%BD%E6%8E%A7%E5%88%B6%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%85%B0%E5%B7%9E%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6)&amp;code=0231149&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光电技术与智能控制教育部重点实验室(兰州交通大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出使用结构化遮挡编码(SOC)结合极限学习机(ELM)的算法来处理人脸识别中的遮挡问题。首先,使用SOC去除图像上的遮挡物,将遮挡物体与人脸分离开;同时,通过局部性约束字典(LCD)来估计遮挡物的位置,建立遮挡字典和人脸字典。然后,将建立好的人脸字典矩阵进行归一化处理,并利用ELM对归一化的数据进行分类识别。最后,在AR人脸库上进行的仿真实验结果表明,所提方法对不同遮挡物和不同区域遮挡的图像具有较好的识别率和鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%AE%E6%8C%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遮挡;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%93%E6%9E%84%E5%8C%96%E9%81%AE%E6%8C%A1%E7%BC%96%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">结构化遮挡编码;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%80%E9%83%A8%E6%80%A7%E7%BA%A6%E6%9D%9F%E5%AD%97%E5%85%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">局部性约束字典;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9E%81%E9%99%90%E5%AD%A6%E4%B9%A0%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">极限学习机;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张芳艳(1990—),女,甘肃宁县人,硕士研究生,主要研究方向:深度学习;;
                                </span>
                                <span>
                                    王新(1978—),女,山东蒙阴人,副教授,博士,CCF会员,主要研究方向:机器学习;;
                                </span>
                                <span>
                                    *许新征(1980—),男,安徽宿州人,副教授,博士,CCF高级会员,主要研究方向:机器学习、模式识别。电子邮箱xuxinzh@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61976217);</span>
                                <span>光电技术与智能控制教育部重点实验室开放课题(KFKT2018-3);</span>
                    </p>
            </div>
                    <h1><b>Partial occlusion face recognition based on structured occlusion coding and extreme learning machine</b></h1>
                    <h2>
                    <span>ZHANG Fangyan</span>
                    <span>WANG Xin</span>
                    <span>XU Xinzheng</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, China University of Mining and Technology</span>
                    <span>Key Laboratory of Opto-Technology and Intelligent Control,Ministry of Education (Lanzhou Jiaotong University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An algorithm combining Structured Occlusion Coding(SOC) with Extreme Learning Machine(ELM) was proposed to deal with the occlusion problem in face recognition. Firstly, the SOC was used to remove the occlusion from the image and separate the oclusion from the human face. At the same time, the position of the occlusion was estimated by the Local Constraint Dictionary(LCD), and an occlusion dictionary and a face dictionary were established. Then, the established face dictionary matrix was normalized, and the ELM was used to classify and identify the normalized data. Finally, the simulation results on the AR face database show that the proposed method has higher recognition rate and stronger robustness for different types of occlusions and images with different regions occluded.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=occlusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">occlusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Structured%20Occlusion%20Coding(SOC)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Structured Occlusion Coding(SOC);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=local%20constraint%20dictionary&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">local constraint dictionary;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Extreme%20Learning%20Machine(ELM)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Extreme Learning Machine(ELM);</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Fangyan,born in 1990,M.S.candidate.Her research interests include deep learning.;
                                </span>
                                <span>
                                    WANG Xin,born in 1978,Ph.D.,associate professor.Her research interests include machine learning.;
                                </span>
                                <span>
                                    XU Xinzheng,born in 1980,Ph.D.,associate professor.His research interests include machine learning,pattern recognition.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61976217);</span>
                                <span>the Opening Project of Key Laboratory of Opto-Technology and Intelligent Control,Ministry of Education(KFKT2018-3);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="50">近年来,人脸识别技术在理论进展和实际应用中取得了很大的突破,已成为模式识别领域的前沿研究方向。但遮挡人脸图像的识别问题在人脸处理过程中会经常出现,例如口罩、发型、墨镜和帽子遮挡是十分常见的。如图1所示(图例选自occluded CASIA-WebFace dataset),这些遮挡对人脸的正确识别具有极大的干扰。而低秩表示<citation id="222" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>可以快速解决遮挡问题,它运用了一种新型的迭代方法有效地提高了识别率和大面积图像遮挡识别的鲁棒性。文献<citation id="223" type="reference">[<a class="sup">2</a>]</citation>提出了结构性遮挡编码和稀疏表示(Sparse Representation-based Classifier, SRC)<citation id="224" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>相结合的方法,巧妙地利用结构化稀疏编码处理图像的遮挡问题。此外,长短期记忆网络自编码器<citation id="225" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>也常用来解决面部遮挡问题,较好地提高了图像降噪的鲁棒性。但遮挡问题还是没有得到完全解决。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 现实世界中的各种遮挡图像" src="Detail/GetImg?filename=images/JSJY201910018_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 现实世界中的各种遮挡图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Various images with occlusions in real world</p>

                </div>
                <div class="p1">
                    <p id="52">在人脸识别领域中,有遮挡的人脸识别问题引起了学术界的广泛关注。遮挡处理方法一般分为视频中的遮挡处理和图像中的遮挡处理方法。通常采用物体跟踪方法处理动态视频中的遮挡问题,例如:文献<citation id="226" type="reference">[<a class="sup">5</a>]</citation>提出了一种基于视频监控的跟踪方法,自动检测并处理遮挡物;文献<citation id="227" type="reference">[<a class="sup">6</a>]</citation>提出了一种新的物体跟踪技术,它可以跟踪人们的动态行为动作,即使在较大的遮挡下也保持像素跟踪分配,由于其鲁棒性,已被用于室内人们行为监督的不同实验。图像中的遮挡处理方法大致可分为五类:低秩表示法、图像修复法、模糊分析、鲁棒主成分分析法和结构性遮挡编码法。文献<citation id="236" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>]</citation>提出使用健壮低秩表示方法来解决带遮挡的人脸识别问题,该方法主要结合了健壮性表示和误差的低级估计。目前,文献<citation id="228" type="reference">[<a class="sup">9</a>]</citation>提出了多尺度的分形编码及重构的图像修复方法,对纹理图像和有较大孔洞的图像效果较好。文献<citation id="229" type="reference">[<a class="sup">10</a>]</citation>提出一种基于模糊主分量分析方法对遮挡区域进行检测并恢复人脸区域,但是,模糊主分量分析法的计算量较大,对大区域的遮挡处理效果不理想。低秩鲁棒主成分分析<citation id="230" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是一种主流的遮挡人脸特征提取方法,同时有用结构性遮挡编码结合稀疏表示分类<citation id="237" type="reference"><link href="198" rel="bibliography" /><link href="200" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>处理遮挡得到较好的效果。文献<citation id="231" type="reference">[<a class="sup">14</a>]</citation>提出了一种新的非负稀疏表示方法,用于大规模数据库的鲁棒人脸识别;但是该算法计算量较大,结构比较复杂。文献<citation id="232" type="reference">[<a class="sup">15</a>]</citation>提出的遮挡字典方法对人脸识别的作用越来越重要,能够有效地处理各种遮挡物,可区分非遮挡和遮挡区域的特征,并分别在字典的相应部分进行编码。文献<citation id="233" type="reference">[<a class="sup">16</a>]</citation>将遮挡字典连接到原始字典来执行遮挡编码,通过寻找稀疏字典的方法,使得遮挡图像成功地分解为面部图像和遮挡图像两部分。遮挡人脸识别问题<citation id="234" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>是人脸识别技术迈向实用的最为关键的一步。如果要构建一个完善的人脸识别系统就必须要解决遮挡问题。比如很常见的帽子、围巾和太阳镜等遮挡物。有时,大面积的遮挡会严重破坏原始图像的有关信息,导致图像识别产生很大的偏差。文献<citation id="235" type="reference">[<a class="sup">18</a>]</citation>提出了稀疏误差和图解模型的方法不断迭代遮挡像素,最终显示遮挡面具,利用马尔可夫随机场模型将空间连续性转化为对训练图像的稀疏表示的计算,从而准确地找到遮挡区域。因此,如何把有遮挡的人脸图像分离成人脸图像和遮挡图像,就具有重要的研究意义。</p>
                </div>
                <div class="p1">
                    <p id="53">本文以遮挡人脸识别问题为出发点,以解决实际场景中的遮挡人脸识别问题为目标,提出了基于结构性遮挡编码(Structured Occlusion Coding,SOC)的遮挡人脸图像识别方法,并利用极限学习机(Extreme Learning Machine, ELM)对分离后的人脸进行分类和识别,即用结构化遮挡编码和极限学习机(SOC-ELM)来处理局部遮挡问题。在该方法中,面部图像和遮挡图像可以分别由字典的相应部分表示,可以在复杂场景中实现更有效的分类效果。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag">1 理论基础</h3>
                <h4 class="anchor-tag" id="55" name="55">1.1 <b>局部性约束字典学习方法</b></h4>
                <div class="p1">
                    <p id="56">局部性约束字典学习(Local Constraint Dictionary, LCD)<citation id="238" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,即在给定字典<i><b>D</b></i><sub><i>H</i></sub>的情况下, <i><b>x</b></i><sub><i>i</i></sub>可由该字典中各原子<i><b>d</b></i><sub><i>j</i></sub>的线性组合近似表示,即<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>,则点<i><b>x</b></i><sub><i>i</i></sub>在<i>d</i>维空间的嵌入<i><b>y</b></i><sub><i>i</i></sub>=<i>g</i>(<i><b>x</b></i><sub><i>i</i></sub>)可以由<i><b>d</b></i><sub><i>j</i></sub>在低维空间中的嵌入<i>g</i>(<i><b>d</b></i><sub><i>i</i></sub>)近线性表示,即:<mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mi>g</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>, 根据<i>l</i><sub>2</sub>距离,为使上述两个线性表示的误差最小,关于<i><b>D</b></i><sub><i>H</i></sub>和<i><b>C</b></i>=,需要同时最小化如下两式:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>g</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mi>g</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">为保证选择不变性,约束<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∀</mo><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow></math></mathml>和‖<i><b>c</b></i><sub><i>i</i></sub>‖<sub>0</sub>=<i>τ</i>成立。进而,如果<i><b>d</b></i><sub><i>j</i></sub>不在<i><b>x</b></i><sub><i>i</i></sub>(<i>i</i>=1,2,…,<i>N</i>)的<i>τ</i>邻域内,则<i>c</i><sub><i>ij</i></sub>=0;如果<i><b>d</b></i><sub><i>j</i></sub>在<i><b>x</b></i><sub><i>i</i></sub>的<i>τ</i>邻域内,则<i>c</i><sub><i>ji</i></sub>≠0。相应的函数变为:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>C</mi><mo>,</mo><mi>D</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>Η</mi></msub><mi mathvariant="bold-italic">C</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>c</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow><mn>2</mn></msubsup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>μ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">C</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>0</mn><mo>,</mo><mi>c</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>=</mo><mn>0</mn></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">; ∀<sub><i>i</i></sub>=1,2,…,<i>N</i>;<i><b>d</b></i><sub><i>j</i></sub>∉<i>Ω</i><sub><i>T</i></sub>(<i><b>x</b></i><sub><i>i</i></sub>)</p>
                </div>
                <div class="p1">
                    <p id="61">其中: <i><b>I</b></i>是元素均为1的列向量;<i>Ω</i><sub><i>τ</i></sub>(<i><b>x</b></i><sub><i>i</i></sub>)∈<b>R</b><sup><i>n</i></sup><sup>×</sup><sup><i>r</i></sup>为<i><b>x</b></i><sub><i>i</i></sub>的<i>τ</i>邻域(包含了<i><b>x</b></i><sub><i>i</i></sub>的<i>τ</i>个最近邻元素);<i>λ</i>是参数,<i>λ</i>&gt;0。此目标函数的优化过程可参考文献<citation id="239" type="reference">[<a class="sup">8</a>]</citation>。</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62">1.2 <b>结构化遮挡编码</b></h4>
                <div class="p1">
                    <p id="63">一张人脸图像<i><b>y</b></i>被一个遮挡物<i><b>v</b></i>遮挡后的形式为<i><b>u</b></i>=<i><b>y</b></i>+<i><b>v</b></i>,违反了低维线性照明模型,导致SOC分类错误。真实场景中的遮挡类别是可预测的,可以提前收集。受到文献<citation id="240" type="reference">[<a class="sup">2</a>]</citation>启发本文算法组成一个遮挡物体的子字典<i><b>B</b></i>=[<i><b>B</b></i><sub>1</sub>,<i><b>B</b></i><sub>2</sub>,…,<i><b>B</b></i><sub><i>S</i></sub>](不同的下标表示不同的类别),并把它加入到原始字典中为<i><b>R</b></i>=[<i><b>D</b></i>,<i><b>B</b></i>]。那么属于第<i>t</i>类的遮档<i><b>v</b></i>就用相应的子字典<i>V</i><sub><i>t</i></sub>表示。</p>
                </div>
                <div class="p1">
                    <p id="64">具体来说,如果第<i>r</i>个物体的一个面被第<i>t</i>个类别的遮挡所遮挡,那么<i><b>u</b></i>就由<i><b>D</b></i><sub><i>r</i></sub>和<i><b>B</b></i><sub><i>t</i></sub>的线性表示为<i><b>u</b></i>=<i><b>y</b></i>+<i><b>v</b></i>=<i><b>D</b></i><sub><i>r</i></sub><i><b>X</b></i><sub><i>r</i></sub>+<i><b>B</b></i><sub><i>t</i></sub><i><b>C</b></i><sub><i>t</i></sub>。通过寻找适当的稀疏解,能够获得一系列系数,其非零项表示的是面部和遮挡物体。具体形式如下:</p>
                </div>
                <div class="p1">
                    <p id="65"><i><b>W</b></i>=[<i><b>x</b></i><sup>T</sup>,<i><b>c</b></i><sup>T</sup>]<sup>T</sup>=[0,…,0,<i>x</i><sub><i>r</i></sub><sub>,1</sub>,<i>x</i><sub><i>r</i></sub><sub>,2</sub>,…,<i>x</i><sub><i>r</i></sub><sub>,</sub><sub><i>n</i></sub>,</p>
                </div>
                <div class="p1">
                    <p id="66">0,…,0,<i>c</i><sub><i>r</i></sub><sub>,1</sub>,<i>c</i><sub><i>r</i></sub><sub>,2</sub>,…,<i>c</i><sub><i>r</i></sub><sub>,</sub><sub><i>nt</i></sub>,0,…,0]<sup>T</sup></p>
                </div>
                <div class="p1">
                    <p id="67">可以看出,构造出良好的字典能够有效地处理各种遮挡,在实际场景中具有高度鲁棒性。有遮挡的识别问题的公式如下:</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mn>0</mn></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">x</mi><mo>,</mo><mi mathvariant="bold-italic">c</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi></mrow></math></mathml>;<i><b>c</b></i>‖<sub>0</sub>      (1)</p>
                </div>
                <div class="p1">
                    <p id="69">s.t.</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo>-</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">D</mi><mo>,</mo><mi mathvariant="bold-italic">B</mi><mo stretchy="false">]</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">x</mi></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">c</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo>-</mo><mi mathvariant="bold-italic">R</mi><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mi>ε</mi></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">当数据逐渐增多时,解决问题(1)是不可行的,所以用<i>l</i><sub>1</sub>范式代替问题(1)中的<i>l</i><sub>0</sub>范式,则问题(1)变成如下问题:</p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">x</mi><mo>,</mo><mi mathvariant="bold-italic">c</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi></mrow></math></mathml>;<i><b>c</b></i>‖<sub>1</sub>      (2)</p>
                </div>
                <div class="p1">
                    <p id="73">s.t.</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo>-</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">D</mi><mo>,</mo><mi mathvariant="bold-italic">B</mi><mo stretchy="false">]</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">x</mi></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">c</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo>-</mo><mi mathvariant="bold-italic">R</mi><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mi>ε</mi></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">计算<mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>与<mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>之间的误差,此时<mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>D</mi><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></math></mathml>,即使有遮挡,也可以把最小残差分配给样本<i><b>u</b></i>, 组成了遮挡字典和干净的人脸字典,得到的两个字典对后续的去遮挡工作有相当大的作用,这样便把遮挡物和人脸分离,将分离后的人脸和原始人脸相比,像素值并没有减小,组成的人脸字典更有利于后续的分类,本文在后续的实验中展示了识别结果。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 SOC分离遮挡实例" src="Detail/GetImg?filename=images/JSJY201910018_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 SOC分离遮挡实例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 SOC occlusion separation examples</p>

                </div>
                <h4 class="anchor-tag" id="77" name="77">1.3 <b>极限学习机</b></h4>
                <div class="p1">
                    <p id="78">极限学习机(<i>ELM</i>)<citation id="241" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>是一种新型的快速的单隐层神经网络<citation id="242" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>学习算法,可以随机初始化输入权重和偏置并得到相应的输出权重。该算法的特点是在网络参数的确定过程中,<i>ELM</i>原理如图3所示。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 ELM原理" src="Detail/GetImg?filename=images/JSJY201910018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>ELM</i>原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>ELM schematic diagram</i></p>

                </div>
                <div class="p1">
                    <p id="80">针对训练样本(<i><b>x</b></i>,<i>t</i>)具有<i>L</i>个隐层神经元的单隐层前向神经网络输出函数表达式为:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>G</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<i><b>a</b></i><sub><i>i</i></sub>和<i><b>b</b></i><sub><i>i</i></sub>为隐层节点参数; <i>β</i><sub><i>i</i></sub>表示连接第<i>i</i>个隐层和网络输出之间的外权;<i>G</i>(<i><b>a</b></i><sub><i>i</i></sub>,<i><b>b</b></i><sub><i>i</i></sub>,<i><b>x</b></i>)表示第<i>i</i>个隐层对应于样本<i><b>x</b></i>的隐层节点输出。针对加法型的隐层节点,<i>G</i>(<i><b>a</b></i><sub><i>i</i></sub>,<i><b>b</b></i><sub><i>i</i></sub>,<i><b>x</b></i>)的表达式为:</p>
                </div>
                <div class="p1">
                    <p id="83"><i>G</i>(<i><b>a</b></i><sub><i>i</i></sub>,<i><b>b</b></i><sub><i>i</i></sub>,<i><b>x</b></i>)=<i>g</i>(<i><b>a</b></i><sub><i>i</i></sub>·<i><b>x</b></i>+<i><b>b</b></i><sub><i>i</i></sub>)</p>
                </div>
                <div class="p1">
                    <p id="84">其中:<i>g</i>(<i><b>x</b></i>)为激活函数;<i><b>a</b></i><sub><i>i</i></sub>·<i><b>x</b></i>代表内权向量<i><b>a</b></i><sub><i>i</i></sub>和样本<i><b>x</b></i>在<b>R</b><sup><i>n</i></sup>中的内积。针对径向基函数(Radical Basis Function, RBF)神经网络型的隐层节点,<i>G</i>(<i><b>a</b></i><sub><i>i</i></sub>,<i><b>b</b></i><sub><i>i</i></sub>,<i><b>x</b></i>)的表达式为:</p>
                </div>
                <div class="p1">
                    <p id="85"><i>G</i>(<i><b>a</b></i><sub><i>i</i></sub>,<i><b>b</b></i><sub><i>i</i></sub>,<i><b>x</b></i>)=<i>g</i>(<i><b>b</b></i><sub><i>i</i></sub>‖<i><b>x</b></i>-<i><b>a</b></i><sub><i>i</i></sub>‖)</p>
                </div>
                <div class="p1">
                    <p id="86">其中:<i>g</i>(<i><b>x</b></i>)为激活函数;<i><b>a</b></i><sub><i>i</i></sub>和<i><b>b</b></i><sub><i>i</i></sub>(<i><b>b</b></i><sub><i>i</i></sub>&gt;0)分别表示第<i>i</i>个径向基函数(RBF)节点的中心和影响因子。考虑<i>N</i>个互异的数据样本(<i><b>x</b></i><sub><i>i</i></sub>,<i><b>t</b></i><sub><i>i</i></sub>),其中:<i><b>x</b></i><sub><i>i</i></sub>=<sup>T</sup>∈<i><b>R</b></i><sup><i>n</i></sup>,<i><b>t</b></i><sub><i>i</i></sub>=<sup>T</sup>∈<i><b>R</b></i><sup><i>n</i></sup>,如果一个具有<i>L</i>个隐层神经元的单隐层神经网络可以零误差逼近这<i>N</i>个互异的数据样本。也就是说,存在<i><b>a</b></i><sub><i>i</i></sub>、<i><b>b</b></i><sub><i>i</i></sub>和<i>β</i><sub><i>i</i></sub>(<i>i</i>=1,2,…,<i>L</i>),使</p>
                </div>
                <div class="p1">
                    <p id="87"><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>G</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>E</mi></mrow></math></mathml>; <i>j</i>=1,2,…,<i>N</i></p>
                </div>
                <div class="p1">
                    <p id="88">公式可以写为:</p>
                </div>
                <div class="p1">
                    <p id="89"><i><b>H</b></i><i>β</i>=<i><b>T</b></i></p>
                </div>
                <div class="p1">
                    <p id="90">其中:<i><b>H</b></i>为隐层输出矩阵。网络参数的训练问题转化为最小平方损失函数的问题,也就是说,寻找最小<i>β</i>,使得:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>β</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>β</mi></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Η</mi><mi>β</mi><mo>-</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false">∥</mo><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mo>+</mo></msup><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">其中:<i><b>H</b></i><sup>+</sup>是矩阵<i><b>H</b></i>的Moore-Penrose广义逆,且可证明求得的解 <i>β</i>^ 的范数是最小的并且唯一。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">2 SOC-ELM模型结构和算法</h3>
                <h4 class="anchor-tag" id="94" name="94">2.1 <i>SOC</i>-<i>ELM</i><b>模型结构</b></h4>
                <div class="p1">
                    <p id="95">训练阶段不需在每个子字典上进行详尽的计算,就可以很容易地获得被遮挡的图像的身份并找出最好的遮挡估计。因文献<citation id="243" type="reference">[<a class="sup">18</a>]</citation>中被遮挡的标签是事先给出的,通过标签的关联就可以很容易地估计遮挡子字典。然而,在实际情况中,大多数遮挡图像的标签是未知的,则文献<citation id="244" type="reference">[<a class="sup">18</a>]</citation>算法就不可取。而本文算法对有标签和无标签图像都适用。<i>SOC</i>-<i>ELM</i>模型结构如图4所示。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 SOC-ELM模型结构" src="Detail/GetImg?filename=images/JSJY201910018_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 <i>SOC</i>-<i>ELM</i>模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Framework of SOC</i>-<i>ELM</i></p>

                </div>
                <h4 class="anchor-tag" id="97" name="97">2.2 <i>SOC</i>-<i>ELM</i><b>算法</b></h4>
                <div class="p1">
                    <p id="98">算法 <i>SOC</i>-<i>ELM</i>处理遮挡问题。</p>
                </div>
                <div class="p1">
                    <p id="99">输入 训练样本的矩阵<i><b>R</b></i>∈<i><b>F</b></i><sup><i>m</i></sup><sup>×</sup><sup><i>n</i></sup>和一个测试样本<i><b>u</b></i>∈<i><b>F</b></i><sup><i>m</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="100">输出 识别准确率。</p>
                </div>
                <div class="p1">
                    <p id="101">测量训练和测试样本的相似度<i>ψ</i><sub>1</sub>∈<i><b>R</b></i><sup>T</sup>·<i><b>u</b></i>,<i>ψ</i><sub>2</sub>∈<i><b>F</b></i><sup><i>n</i></sup>(<i>ψ</i>是训练与测试样本内积),选择最大时<i>M</i>构建限制性字典<i><b>R</b></i><sub>LCD</sub>。</p>
                </div>
                <div class="p1">
                    <p id="102">令<i>z</i><sup>(0)</sup>=1<sub><i>m</i></sub>;<i>t</i>=0;<i>t</i>=<i>t</i>+1;<i><b>R</b></i><sup>*</sup><sub><i>LCD</i></sub>=<i><b>R</b></i><sub>LCD</sub>[<i>Z</i><sup>(</sup><sup><i>t</i></sup><sup>-1)</sup>=1];<i><b>u</b></i><sup>*</sup>=<i><b>u</b></i>[<i>z</i><sup>(</sup><sup><i>t</i></sup><sup>-1)</sup>=1]。</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mo>,</mo><mover accent="true"><mi>e</mi><mo>^</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi>e</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></munder><mo stretchy="false">∥</mo><mi>e</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mtext> </mtext><mi mathvariant="bold-italic">u</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>D</mtext></mrow><mo>*</mo></msubsup><mi>x</mi><mo>+</mo><mi>e</mi><msup><mrow></mrow><mo>*</mo></msup></mtd></mtr><mtr><mtd><mover accent="true"><mi>e</mi><mo>^</mo></mover><mo>=</mo><mi mathvariant="bold-italic">u</mi><mo>-</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>C</mtext><mtext>D</mtext></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">通过图像分割更新误差:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>z</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi>Ζ</mi><mo>∈</mo><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><msup><mrow></mrow><mi>m</mi></msup></mrow></munder><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo>∈</mo><mi>E</mi></mrow></munder><mi>β</mi></mstyle><mi>z</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mi>z</mi><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>V</mi></mrow></munder><mrow><mi>log</mi></mrow></mstyle><mspace width="0.25em" /><mi>p</mi><mo stretchy="false">(</mo><mover accent="true"><mi>e</mi><mo>^</mo></mover><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mi>z</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">直到最大收敛点:<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>e</mi><mo>^</mo></mover><mo stretchy="false">[</mo><mi>z</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>1</mn><mo stretchy="false">]</mo><mo>=</mo><mn>0</mn><mo stretchy="false">(</mo><mi>z</mi><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mn>0</mn></mrow></math></mathml>或<i>z</i><sup><i>i</i></sup>=1被认为是遮挡或无遮挡);</p>
                </div>
                <div class="p1">
                    <p id="108">将上述输出<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>e</mi><mo>^</mo></mover></math></mathml>归一化放入ELM/SOFTMAX/支持向量机(Support Vector Machine, SVM)识别分类;</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi>e</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>G</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mover accent="true"><mi>e</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>G</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>E</mi><mo>;</mo><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Ν</mi><mo>;</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">在该算法中,将图像域视为一个图,误差<i>e</i>的支持向量被表示为<i><b>z</b></i>∈{0,1}<sup><i>m</i></sup>,<i>z</i><sup><i>i</i></sup>=0表示无遮挡,<i>z</i><sup><i>i</i></sup>=1表示有遮挡,图像经过SOC时,时间复杂度为<i>O</i>(<i>mn</i>),用ELM对归一化的图像数据做分类时,时间复杂度为<i>O</i>(<i>n</i><sup>3</sup>),则此算法的时间复杂度为<i>O</i>(<i>mn</i>+<i>n</i><sup>3</sup>)。</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag">3 实验</h3>
                <div class="p1">
                    <p id="112">实验在<i>AR</i>人脸数据集和<i>CelebA</i>数据集上进行。通过与<i>SOC</i>-<i>SVM</i>和<i>SOC</i>-<i>SOFTMAX</i>进行对比和分析,采用峰值信噪比(<i>Peak Signal</i>-<i>to</i>-<i>Noise Ratio</i>, <i>PSNR</i>)和结构相似性指数(<i>Structural SIMilarity index</i>, <i>SSIM</i>)两个常用评价指标来验证所提模型的有效性。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113">3.1 <b>实验数据集</b></h4>
                <div class="p1">
                    <p id="114"><i>AR</i>数据库被广泛用来做遮挡的处理,它包括126个人的4 000多幅正面图像。该数据库中的图像包括较多的面部表情变化和遮挡,例如光照变化、表情变化,墨镜和围巾遮挡,典型示例如图5所示。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 AR人脸数据库局部样本图像" src="Detail/GetImg?filename=images/JSJY201910018_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>AR</i>人脸数据库局部样本图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Sample images of AR face database</i></p>

                </div>
                <div class="p1">
                    <p id="116"><i>CelebA</i>是香港中文大学的公开数据,包含202 599张10 177个名人身份的图像,所有这些图像都有很好的标记,是一个非常完整的面部相关训练数据集。图片大小为(178,218,3),添加了相应的遮挡,来模拟真实的遮挡情况。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 CelebA人脸数据库局部样本图像" src="Detail/GetImg?filename=images/JSJY201910018_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 <i>CelebA</i>人脸数据库局部样本图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Sample images of CelebA face database</i></p>

                </div>
                <div class="p1">
                    <p id="118">在<i>AR</i>数据库中选择了50个男人和50个女人的图像(其中每个人有26张图片,14张普通图片和6张戴围巾的图片),每张图片大小为83×60。为每个人物选择7张没有任何伪装的图片,随机构造原始字典并用于测试。在戴墨镜和围巾的图片中,随机挑选10个人,共60张图片来获得遮挡模型,其他的则用于测试。最后,以识别率来评价算法的有效性,并与<i>SOC</i>-<i>SOFTMAX</i>和<i>SOC</i>-<i>SVM</i>模型进行对比。</p>
                </div>
                <div class="p1">
                    <p id="119">在<i>CelebA</i>数据库上,本文选择了有代表性的3 000张随机遮挡的图片,将这些图片放入三种模型进行实验。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">3.2 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="121">在<i>AR</i>人脸库中,采用540张遮挡图片作为训练人脸库。用稀疏遮挡编码分离其中戴墨镜和戴围巾的人脸。对分离后的人脸进行归一化,输入到三个不同的分类器中进行分类识别。首先测试了结构化遮挡编码(<i>SOC</i>),学习了60张墨镜图片的遮挡字典,并和原始图片共同组成训练集,剩余的540张戴墨镜的图片用于测试。将去遮挡的图像集归一化处理,利用<i>SVM</i>、<i>SOFTMAX</i>和<i>ELM</i>分类器进行识别,得到的结果如图7～9所示。</p>
                </div>
                <div class="p1">
                    <p id="122">此外,比较了9种不同迭代次数的识别准确率,<i>SOC</i>-<i>ELM</i>的识别效果是最好的。从表1～2的实验结果还可以看出,算法的最佳迭代次数为50 000。采用本文算法,使得分离人脸和遮挡物后识别人脸准确率得到有效的提高。另外,相对于文献<citation id="245" type="reference">[<a class="sup">2</a>]</citation>提出的<i>SOC</i>-<i>SRC</i>来说,本文方法大幅降低了计算时间,只有15～41 <i>s</i>;同时,可以看出在相同的迭代次数下,<i>SOC</i>-<i>ELM</i>表现出的效果明显好于其他两种算法。因而可以得出本文算法具有较好的稳定性和实时性。</p>
                </div>
                <div class="p1">
                    <p id="123">在<i>AR</i>人脸数据库中仿真模拟实验结果对比如图7～8所示。观察图7可知,在墨镜遮挡的条件下<i>SOC</i>-<i>ELM</i>和<i>SOC</i>-<i>SOFTMAX</i>在迭代次数为50 000时效果最佳,分别为91.15%和80.56%。<i>SOC</i>-<i>SOFTMAX</i>在迭代次数为52 500时识别准确率达到88.65%。观察图8可知,围巾遮挡条件下<i>SOC</i>-<i>ELM</i>同样在迭代次数为50 000时识别率最高。</p>
                </div>
                <div class="p1">
                    <p id="124">在<i>CelebA</i>数据库中,通过调整不同数量的样本子集的选择来调整<i>SOC</i>-<i>ELM</i>的测试准确率。如图9所示,迭代次数设置为9个不同数值,训练样本总数为3 000。图9中,使用了随机手动遮挡方法。将实验结果与<i>SOC</i>-<i>SOFTMAX</i>和<i>SOC</i>-<i>SVM</i>的结果进行了比较。本文方法生成的图像在恢复后更加清晰,与原始图像高度相似。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 墨镜遮挡三种方法结果对比" src="Detail/GetImg?filename=images/JSJY201910018_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 墨镜遮挡三种方法结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Results comparison of three methods for sunglass occlusion</i></p>

                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 围巾遮挡三种方法结果对比" src="Detail/GetImg?filename=images/JSJY201910018_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 围巾遮挡三种方法结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Results comparison of three methods for scarf occlusion</i></p>

                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910018_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 在CelebA数据上修复的结果" src="Detail/GetImg?filename=images/JSJY201910018_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 在<i>CelebA</i>数据上修复的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910018_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 9 <i>Results fixed on CelebA data</i></p>

                </div>
                <div class="p1">
                    <p id="128">训练阶段 在<i>AR</i>人脸库和<i>CelebA</i>数据库中分别选用了戴墨镜、随机遮挡、戴围巾的遮挡图像,以及不同光照强度的遮挡图像。通过<i>LCD</i>检测到遮挡所在面部的具体位置,利用结构化遮挡编(<i>SOC</i>)学习,得到遮挡物字典和干净人脸字典,并对干净人脸字典进行归一化等相关处理。</p>
                </div>
                <div class="p1">
                    <p id="129">测试阶段 对比了结构化遮挡编码结合三种分类器在遮挡人脸上的识别效果。用<i>SVM</i>、<i>SOFTMAX</i>和<i>ELM</i>分别来处理分类问题,通过实验可知,最终<i>ELM</i>的分类效果是最好。<i>ELM</i>对分离后的人脸和遮挡物进行识别分类,使得识别分离后人脸的识别率有效提升。</p>
                </div>
                <div class="p1">
                    <p id="130">表1显示了9种不同迭代次数下的三种模型的峰值信噪比(<i>PSNR</i>),它们直接测量像素值的差异。注意,在<i>SOC</i>-<i>ELM</i>模型下,获得了相对较高的<i>PSNR</i>值,但这并不意味着更好的定性结果,这两个指标只是偏向平滑和模糊的结果。</p>
                </div>
                <div class="area_img" id="131">
                    <p class="img_tit"><b>表</b>1 9<b>种不同迭代次数下三种模型的</b><i>SSIM</i><b>比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>SSIM comparison of three models with</i> 9 <i>different iterations</i></p>
                    <p class="img_note"></p>
                    <table id="131" border="1"><tr><td><br />迭代次数</td><td><i>CE</i><sup>[22]</sup></td><td><i>SI</i><sup>[23]</sup></td><td><i>SOC</i>-<i>ELM</i></td></tr><tr><td><br />40 000</td><td>0.674</td><td>0.665</td><td>0.712</td></tr><tr><td><br />42 500</td><td>0.629</td><td>0.658</td><td>0.728</td></tr><tr><td><br />45 000</td><td>0.716</td><td>0.697</td><td>0.715</td></tr><tr><td><br />47 500</td><td>0.706</td><td>0.674</td><td>0.756</td></tr><tr><td><br />50 000</td><td>0.698</td><td>0.705</td><td>0.778</td></tr><tr><td><br />52 500</td><td>0.718</td><td>0.711</td><td>0.783</td></tr><tr><td><br />55 000</td><td>0.722</td><td>0.693</td><td>0.802</td></tr><tr><td><br />57 500</td><td>0.725</td><td>0.709</td><td>0.762</td></tr><tr><td><br />60 000</td><td>0.706</td><td>0.701</td><td>0.771</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="132">表2是估计的结构相似性指数(<i>SSIM</i>)的两个图像之间的相似性,值越高越好。在迭代次数为50 000左右时<i>SOC</i>-<i>ELM</i>表现更好,但这并不意味着迭代次数越高越好。对比上下文编码器(<i>Context Encoders</i>, <i>CE</i>)<citation id="246" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和上下文损失(<i>Contextual Losses</i>, <i>CL</i>)<citation id="247" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>两个模型,本文的修复算法使得图像更加清晰逼真。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表</b>2 9<b>种不同迭代次数下三种模型的</b><i>PSNR</i><b>比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>PSNR comparison of three models with</i> 9 <i>different iterations</i></p>
                    <p class="img_note">单位:dB</p>
                    <table id="133" border="1"><tr><td><br />迭代次数</td><td><i>CE</i><sup>[22]</sup></td><td><i>CL</i><sup>[23]</sup></td><td><i>SOC</i>-<i>ELM</i></td></tr><tr><td><br />40 000</td><td>16.8</td><td>16.2</td><td>17.9</td></tr><tr><td><br />42 500</td><td>16.7</td><td>15.5</td><td>18.2</td></tr><tr><td><br />45 000</td><td>15.9</td><td>16.7</td><td>17.4</td></tr><tr><td><br />47 500</td><td>15.2</td><td>15.8</td><td>17.3</td></tr><tr><td><br />50 000</td><td>17.3</td><td>16.4</td><td>18.6</td></tr><tr><td><br />52 500</td><td>17.6</td><td>17.1</td><td>17.8</td></tr><tr><td><br />55 000</td><td>16.4</td><td>16.9</td><td>16.8</td></tr><tr><td><br />57 500</td><td>16.7</td><td>16.3</td><td>17.4</td></tr><tr><td><br />60 000</td><td>16.2</td><td>15.8</td><td>16.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">3.3 <b>运行时间</b></h4>
                <div class="p1">
                    <p id="135">在<i>AR</i>数据库上计算了遮挡字典收集训练和识别分类的时间,对于大小为83×60的图片,从原始图片中通过<i>SOC</i>记录遮挡模式和干净图像,此时间可忽略不计;当涉及到分类器时,会花费较长的时间。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表</b>3 <b>三种算法的平均运行时间</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Average running time of three algorithms</i></p>
                    <p class="img_note">单位:s</p>
                    <table id="136" border="1"><tr><td><br />识别算法</td><td>训练遮挡字典</td><td>收集遮挡样本</td></tr><tr><td><br /><i>SOC</i>-<i>SVM</i></td><td>1 562.32</td><td>0.03</td></tr><tr><td><br /><i>SOC</i>-<i>SOFTMAX</i></td><td>2 756.12</td><td>0.02</td></tr><tr><td><br /><i>SOC</i>-<i>ELM</i></td><td>1 198.10</td><td>0.02</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="137" name="137" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="138">遮挡图像处理一直是图像处理和视频处理研究中的重要课题,因此受到了人工智能领域的极大重视。在处理遮挡图像时,本文采用了<i>SOC</i>-<i>ELM</i>算法,先使用<i>LCD</i>对遮挡处定位,使得遮挡物和干净人脸可以分离开,进而对分离后的人脸进行分类识别,以此得到的识别有较好的鲁棒性和识别率。实验结果表明,本文算法取得了较好的识别结果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="176">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust and Low-Rank Representation for Fast Face Identification with Occlusions">

                                <b>[1]</b>ILIADIS M,WANG H,MOLINA R,et al.Robust and low-rank representation for fast face identification with occlusions[J].IEEETransactions on Image Processing,2017,26(5):2203-2218.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC5D3B1558757D67027416415FEAAF3EA&amp;v=MjE3MjJLK3JvcEFiT3dLQ3dnL3lCWVI3VHQ4VG52anFXUkFDTVBpUnMvdUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieTZ3YXM9TmlmT2ZjQzlhdA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>WEN Y,LIU W,YANG M,et al.Structured occlusion coding for robust face recognition[J].Neurocomputing,2016,178(C):11-24.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Half-quadratic-based iterative minimization for robust sparse representation">

                                <b>[3]</b>HE R,ZHENG W,TAN T,et al.Half-quadratic-based iterative minimization for robust sparse representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,36(2):261-275.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust LSTM-Autoencod-ers for Face De-Occlusion in the Wild">

                                <b>[4]</b>ZHAO F,FENG J,ZHAO J,et al.Robust LSTM-autoencoders for face de-occlusion in the wild[J].IEEE Transactions on Image Processing,2016,27(2):778-790.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Occlusion handling based on sub-blobbing in automated video surveillance system">

                                <b>[5]</b>MOHAMMAD O A,BOUBAKEUR B.Occlusion handling based on sub-blobbing in automated video surveillance system[C]//Proceedings of the 4th International Conference on Computer Science and Software Engineering.New York:ACM,2011:139-143.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Track-based and object-based occlusion forpeople tracking refinement in indoor surveillance">

                                <b>[6]</b>CUCCHIARA R,GRANA C,TARDINI G.Track-based and object-based occlusion for people tracking refinement in indoor surveillance[C]//Proceedings of the ACM 2nd International Workshop on Video Surveillance&amp;Sensor Networks.New York:ACM,2004:81-87.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond Low Rank+Sparse:Multiscale Low Rank Matrix Decomposition">

                                <b>[7]</b>ONG F,LUSTIG M.Beyond low rank+sparse:multiscale low rank matrix decomposition[J].IEEE Journal of Selected Topics in Signal Processing,2016,10(4):672-687.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition using multi-modal low-rank dictionary learning">

                                <b>[8]</b>OROUGHI H,SHAKERI M,RAY N,et al.Face recognition using multi-modal low-rank dictionary learning[C]//Proceedings of the2017 IEEE International Conference on Image Processing.Piscataway:IEEE,2017:1081-1086.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJLG201704010&amp;v=MDg5ODJDVVI3cWZadVpzRnluaFZyN0xLeWZIYWJHNEg5Yk1xNDlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>唐娴,黄军伟.低秩鲁棒性主成分分析的遮挡人脸识别[J].南京理工大学学报,2017,41(4):460-465.(TANG W,HUANG JW.Occlusion face recognition based on low rank robust principal component analysis[J].Journal of Nanjing University of Science and Technology,2017,41(4):460-465.)
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201010038&amp;v=MzA3MTRuaFZyN0xJVGZUZTdHNEg5SE5yNDlHYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>李晋江,张彩明,范辉,等.基于分形的图像修复算法[J].电子学报,2010,38(10):2430-2435.(LI J J,ZHANG C M,FAN H,et al.Fractal-based image restoration algorithm[J].Acta Electronica Sinica,2010,38(10):2430-2435.)
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative low-rank metric learning for face recognition">

                                <b>[11]</b>DING Z M,SUH S,HAN J,et al.Discriminative low-rank metric learning for face recognition[C]//Proceedings of the 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition.Piscataway:IEEE,2015:1-6.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An optimization method of extreme learning machine for regression">

                                <b>[12]</b>DING X,LIU X,XU L.An optimization method of extreme learning machine for regression[C]//Proceedings of the 31st Annual ACM Symposium on Applied Computing.New York:ACM,2016:891-893.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739928&amp;v=MDIxMDJvUmF4RT1OaWZPZmJLN0h0RE5xWTlGWStnR0JYNHhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJVg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>YANG M,ZHANG L,SHIU S,et al.Gabor feature based robust representation and classification for face recognition with Gabor occlusion dictionary[J].Pattern Recognition,2014,46(7):1559-1572.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two-Stage Nonnegative Sparse Representation for Large-Scale Face Recognition">

                                <b>[14]</b>HE R,ZHENG W,HU B,et al.Two-stage nonnegative sparse representation for large-scale face recognition[J].IEEE Transactions on Neural Networks and Learning Systems,2013,24(1):35-46.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201411019&amp;v=MDA0MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVnI3TEx6N0JhTEc0SDlYTnJvOUViWVE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>朱明旱,李树涛,叶华.稀疏表示分类中遮挡字典构造方法的改进[J].计算机辅助设计与图形学报,2014,26(11):2064-2078.(ZHU M H,LI S T,YE H.Improvement of the construction method of occlusion dictionary in sparse representation classification[J].Journal of Computer-Aided Design&amp;Computer Graphics,2014,26(11):2064-2078.)
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD4E05A58FA6C51E70A80FD6CAD268F96D&amp;v=MjA5MzNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieTZ3YXM9Tmo3QmFyZk5IdFM5cW9jekZlMThDWDFNeUJaaTRqOExQSG1SM1dZM2Y3cmlUTHpyQw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>SING Y,CHENG Y.Noise-resistant network:a deep-learning method for face recognition under noise[J].EURASIP Journal on Image and Video Processing,2017,2017:Article number 43.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locality constrained dictionary learning for nonlinear dimensionality reduction">

                                <b>[17]</b>ZHOU Y,BARNER K.Locality constrained dictionary learning for nonlinear dimensionality reduction[J].IEEE Signal Processing Letters,2013,20(4):335-338.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition with contiguous occlusion using markov random fields">

                                <b>[18]</b>ZHOU Z,WAGNER A,MOBAHI H,et al.Face recognition with contiguous occlusion using Markov random fields[C]//Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway:IEEE,2009:1050-1057.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201601015&amp;v=MTQ5MzZWcjdMUER6VGJMRzRIOWZNcm85RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>刘丽娜,马世伟,温加睿.基于局部约束字典学习的数据降维和重构方法[J].仪表仪器学报,2016,37(1):99-108.(LIUL N,MA S W,WEN J R.Data dimension reduction and reconstruction method based on local constraint dictionary learning[J].Journal of Instrument and Instrument,2016,37(1):99-108.)
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble extreme learning machine for multi-instance learning">

                                <b>[20]</b>SASTRAWAHA S,HORATA P.Ensemble extreme learning machine for multi-instance learning[J]//Proceedings of the 9th International Conference on Machine Learning and Computing.New York:ACM,2017:56-60.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017821936.nh&amp;v=MDI4NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhWcjdMVkYyNkdidTZIOWpQcVpFYlBJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>彭双.神经网络隐层节点的稀疏化[D].大连:大连理工大学,2017:1-48.(PENG S.Sparseization of hidden nodes in neural networks[D].Dalian:Dalian University of Technology,2017:1-48.)
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Context Encoders:Feature Learning by Inpainting">

                                <b>[22]</b>PATHAK D,DONAHUE P,DARRELL T,et al.Context encoders:feature learning by inpainting[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:2536-2544.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic image inpainting with perceptual and contextual losses">

                                <b>[23]</b>YEH R,CHEN C,LIMT Y,et al.Semantic image inpainting with perceptual and contextual losses[C]//Proceedings of the2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6882-6890.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910018&amp;v=MzE3Mzg3TEx6N0JkN0c0SDlqTnI0OUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
