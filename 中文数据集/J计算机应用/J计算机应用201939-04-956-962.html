<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775270752500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904005%26RESULT%3d1%26SIGN%3dVZiEC0Uo7d%252fhSRvu4Eoq9Mi133c%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904005&amp;v=MjkwMDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ekxMejdCZDdHNEg5ak1xNDlGWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 结合双特征和松弛边界的异常点检测 ">2 结合双特征和松弛边界的异常点检测</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="2.1 DFRB&lt;b&gt;算法原理&lt;/b&gt;">2.1 DFRB<b>算法原理</b></a></li>
                                                <li><a href="#73" data-title="2.2 &lt;b&gt;决策树节点的双特征值域&lt;/b&gt;">2.2 <b>决策树节点的双特征值域</b></a></li>
                                                <li><a href="#84" data-title="2.3 &lt;b&gt;结合双特征和松弛边界的异常点检测&lt;/b&gt;">2.3 <b>结合双特征和松弛边界的异常点检测</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#110" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="3.1 &lt;b&gt;实验数据和性能指标&lt;/b&gt;">3.1 <b>实验数据和性能指标</b></a></li>
                                                <li><a href="#120" data-title="3.2 &lt;b&gt;异常点识别性能对比&lt;/b&gt;">3.2 <b>异常点识别性能对比</b></a></li>
                                                <li><a href="#131" data-title="3.3 &lt;b&gt;双特征与单特征的对比分析&lt;/b&gt;">3.3 <b>双特征与单特征的对比分析</b></a></li>
                                                <li><a href="#134" data-title="3.4 &lt;b&gt;综合性能&lt;/b&gt;&lt;i&gt;F&lt;/i&gt;1&lt;b&gt;对比分析及参数讨论&lt;/b&gt;">3.4 <b>综合性能</b><i>F</i>1<b>对比分析及参数讨论</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#143" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="图1 决策树分割示意图">图1 决策树分割示意图</a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;实验数据集的相关信息&lt;/b&gt;"><b>表</b>1 <b>实验数据集的相关信息</b></a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同算法在&lt;/b&gt;seeds&lt;b&gt;数据集上的实验结果&lt;/b&gt;"><b>表</b>2 <b>不同算法在</b>seeds<b>数据集上的实验结果</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同算法在&lt;/b&gt;hepato&lt;b&gt;数据集上的实验结果&lt;/b&gt;"><b>表</b>3 <b>不同算法在</b>hepato<b>数据集上的实验结果</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同算法在&lt;/b&gt;wine&lt;b&gt;数据集上的实验结果&lt;/b&gt;"><b>表</b>4 <b>不同算法在</b>wine<b>数据集上的实验结果</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同算法在&lt;/b&gt;forestType&lt;b&gt;数据集上的实验结果&lt;/b&gt;"><b>表</b>5 <b>不同算法在</b>forestType<b>数据集上的实验结果</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;不同算法在&lt;/b&gt;dermatology&lt;b&gt;数据集上的实验结果&lt;/b&gt;"><b>表</b>6 <b>不同算法在</b>dermatology<b>数据集上的实验结果</b></a></li>
                                                <li><a href="#129" data-title="图2 单双特征值域的性能对比分析">图2 单双特征值域的性能对比分析</a></li>
                                                <li><a href="#130" data-title="图3 不同算法不同数据集在&lt;i&gt;F&lt;/i&gt;1指标上的比较">图3 不同算法不同数据集在<i>F</i>1指标上的比较</a></li>
                                                <li><a href="#139" data-title="图4 &lt;i&gt;ε&lt;/i&gt;对&lt;i&gt;F&lt;/i&gt;1的影响">图4 <i>ε</i>对<i>F</i>1的影响</a></li>
                                                <li><a href="#141" data-title="图5 &lt;i&gt;σ&lt;/i&gt;对召回率、 精度和&lt;i&gt;F&lt;/i&gt;1的影响">图5 <i>σ</i>对召回率、 精度和<i>F</i>1的影响</a></li>
                                                <li><a href="#142" data-title="图6 &lt;i&gt;ε&lt;/i&gt;和&lt;i&gt;σ&lt;/i&gt;同时变化对算法性能的影响">图6 <i>ε</i>和<i>σ</i>同时变化对算法性能的影响</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="180">


                                    <a id="bibliography_1" title=" HAWKINS D M. Identification of outliers[M]. London: Chapman and Hall, 1980: 1-2." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identification of Outliers">
                                        <b>[1]</b>
                                         HAWKINS D M. Identification of outliers[M]. London: Chapman and Hall, 1980: 1-2.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_2" title=" DOMINGUES R, FILIPPONE M, MICHIARDI P, et al. A comparative evaluation of outlier detection algorithms: experiments and analyses [J]. Pattern Recognition, 2018, 74: 406-421." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE4A0042AF521DCBBBC213AB7A9AB8065&amp;v=MjY2NzU3dzZzPU5pZk9mY2E4YjlITXE0MDBFdTRORFFoS3ZXUmhtVDE4U3c2UXEyTThDTUNjUmJ5YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMMg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         DOMINGUES R, FILIPPONE M, MICHIARDI P, et al. A comparative evaluation of outlier detection algorithms: experiments and analyses [J]. Pattern Recognition, 2018, 74: 406-421.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_3" title=" WANG Y, WONG J, MINER A. Anomaly intrusion detection using one class SVM[C]// Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop. Piscataway, NJ: IEEE, 2004: 358-364." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anomaly intrusion detection using one class SVM">
                                        <b>[3]</b>
                                         WANG Y, WONG J, MINER A. Anomaly intrusion detection using one class SVM[C]// Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop. Piscataway, NJ: IEEE, 2004: 358-364.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_4" title=" SCHOLKOPF B, WILLIAMSON R, SMOLA A, et al. Support vector method for novelty detection[J]. Advances in Neural Information Processing Systems, 2000, 12 (3) : 582-588." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support vector method for novelty detection">
                                        <b>[4]</b>
                                         SCHOLKOPF B, WILLIAMSON R, SMOLA A, et al. Support vector method for novelty detection[J]. Advances in Neural Information Processing Systems, 2000, 12 (3) : 582-588.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_5" title=" 张晓惠, 林柏钢. 基于特征选择和多分类支持向量机的异常检测[J]. 通信学报, 2009, 30 (增刊1) : 68-73. (ZHANG X H, LIN B G. Anomaly detection based on feature selection and multi-class support vector machines[J]. Journal on Communications, 2009, 30 (S1) : 68-73." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB2009S1014&amp;v=MTA3MDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ekxNVFhUYkxHNEh0aXZybzlFWUlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         张晓惠, 林柏钢. 基于特征选择和多分类支持向量机的异常检测[J]. 通信学报, 2009, 30 (增刊1) : 68-73. (ZHANG X H, LIN B G. Anomaly detection based on feature selection and multi-class support vector machines[J]. Journal on Communications, 2009, 30 (S1) : 68-73.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_6" title=" ERFANI S M, RAJASEGARAR S, KARUNASEKERA S, et al. High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning[J]. Pattern Recognition, 2016, 58: 121-134." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1E94FF704664CCB4133E539722914ECA&amp;v=MzIwNzhwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekwyN3c2cz1OaWZPZmJMTkY5VzYyWWhGWU8wSkNBOUt2UklTNlR3SVRYenJxeEEzY0xPUU1NbnVDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         ERFANI S M, RAJASEGARAR S, KARUNASEKERA S, et al. High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning[J]. Pattern Recognition, 2016, 58: 121-134.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_7" title=" PAULA E L, LADEIRA M, CARVALHO R N, et al. Deep learning anomaly detection as support fraud investigation in brazilian exports and anti-money laundering[C]// Proceedings of the 2016 IEEE International Conference on Machine Learning and Applications. Piscataway, NJ: IEEE, 2016: 954-960." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning anomaly detection as support fraud investigation in Brazilian exports and anti-money laundering">
                                        <b>[7]</b>
                                         PAULA E L, LADEIRA M, CARVALHO R N, et al. Deep learning anomaly detection as support fraud investigation in brazilian exports and anti-money laundering[C]// Proceedings of the 2016 IEEE International Conference on Machine Learning and Applications. Piscataway, NJ: IEEE, 2016: 954-960.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_8" title=" LIU F T, TING K M, ZHOU Z H. Isolation-based anomaly detection [J]. ACM Transactions on Knowledge Discovery from Data, 2012, 6 (1) : 1-39." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000013918&amp;v=MjI1OTdYMHhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRnNRYVJFPU5pZklZN0s3SHRqTnI0OUZaT29NQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         LIU F T, TING K M, ZHOU Z H. Isolation-based anomaly detection [J]. ACM Transactions on Knowledge Discovery from Data, 2012, 6 (1) : 1-39.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_9" title=" SHEN Y, LIU H, WANG Y, et al. A novel isolation-based outlier detection method[C]// PRICAI 2016: Proceedings of the 2016 Pacific Rim International Conference on Artificial Intelligence. Berlin: Springer, 2016: 446-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel isolation-based outlier detection method">
                                        <b>[9]</b>
                                         SHEN Y, LIU H, WANG Y, et al. A novel isolation-based outlier detection method[C]// PRICAI 2016: Proceedings of the 2016 Pacific Rim International Conference on Artificial Intelligence. Berlin: Springer, 2016: 446-456.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_10" title=" 邱一卉, 林成德. 基于随机森林方法的异常样本检测方法 [J]. 福建工程学院学报, 2007, 5 (4) : 392-396. (QIU Y H, LIN C D. Outlier detection based on random forest[J]. Journal of Fujian University of Technology, 2007, 5 (4) : 392-396.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZGZ200704021&amp;v=MTExOTR6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd6TEx6Zk1kTEc0SHRiTXE0OUhaWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         邱一卉, 林成德. 基于随机森林方法的异常样本检测方法 [J]. 福建工程学院学报, 2007, 5 (4) : 392-396. (QIU Y H, LIN C D. Outlier detection based on random forest[J]. Journal of Fujian University of Technology, 2007, 5 (4) : 392-396.) 
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_11" title=" ZHOU Q F, ZHOU H, NING Y P, et al. Two approaches for novelty detection using random forest [J]. Expert Systems with Applications, 2015, 42 (10) : 4840-4850." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES28DB9A6701469538A950FAA46A5D9ED9&amp;v=MDU2ODhyTFUwNXRwaHpMMjd3NnM9TmlmT2ZiR3dhcVBGM29sQ1pPb0xDblU4ekI1aTR6cDlQZzZUcUJSRWZNYWRNTTZXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         ZHOU Q F, ZHOU H, NING Y P, et al. Two approaches for novelty detection using random forest [J]. Expert Systems with Applications, 2015, 42 (10) : 4840-4850.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_12" title=" 李贞贵.随机森林改进的若干研究[D]. 厦门: 厦门大学, 2013: 28-30. (LI Z G. Several research on random forest improve[D]. Xiamen: Xiamen University, 2013: 28-30.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014224505.nh&amp;v=MTI4ODl1WnNGeURnVjd6TFZGMjZHckc2R3RUTXFwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         李贞贵.随机森林改进的若干研究[D]. 厦门: 厦门大学, 2013: 28-30. (LI Z G. Several research on random forest improve[D]. Xiamen: Xiamen University, 2013: 28-30.) 
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_13" title=" 胡淼, 王开军, 李海超, 等.模糊树节点的随机森林与异常点检测[J]. 南京大学学报 (自然科学版) , 2018, 54 (6) : 1141-1151. (HU M, WANG K J, LI H C, et al. A random forest algorithm based on fuzzy tree node for anomaly detection[J]. Journal of Nanjing University (Natural Science) , 2018, 54 (6) : 1141-1151.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=NJDZ201806010&amp;v=Mjc4MDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ekxLeWZQZExHNEg5bk1xWTlFWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         胡淼, 王开军, 李海超, 等.模糊树节点的随机森林与异常点检测[J]. 南京大学学报 (自然科学版) , 2018, 54 (6) : 1141-1151. (HU M, WANG K J, LI H C, et al. A random forest algorithm based on fuzzy tree node for anomaly detection[J]. Journal of Nanjing University (Natural Science) , 2018, 54 (6) : 1141-1151.) 
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_14" title=" BREIMAN L, FRIEDMAN J, OLSHEN R, et al. Classification and Regression Trees[M]. New York:Champman &amp;amp; Hall, 1984:18-55." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification and Regression Trees">
                                        <b>[14]</b>
                                         BREIMAN L, FRIEDMAN J, OLSHEN R, et al. Classification and Regression Trees[M]. New York:Champman &amp;amp; Hall, 1984:18-55.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_15" title=" 李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012: 67-71. (LI H. Statistical Learning Method[M]. Beijing: Tsinghua University Press, 2012: 67-71.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302275954001&amp;v=MjgwNjRTSDduM3hFOWZidm5LcmlmWmVadkZ5bm5VN2ZOSkYwWFhGcXpHYkM0SE5QTHFvWkFZT3NQRFJNOHp4VVNtRGQ5&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012: 67-71. (LI H. Statistical Learning Method[M]. Beijing: Tsinghua University Press, 2012: 67-71.) 
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_16" title=" BREIMAN L. Bagging predictors [J]. Machine Learning, 1996, 24 (2) : 123-140." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MDIwODhjTUg3UjdxZForWnVGaXJsVzdyTUkxMD1OajdCYXJPNEh0SE5ySXhNWU9NTlkzazV6QmRoNGo5OVNYcVJyeG94&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         BREIMAN L. Bagging predictors [J]. Machine Learning, 1996, 24 (2) : 123-140.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_17" title=" BREIMAN L. Random forest [J]. Machine Learning, 2001, 45 (1) : 5-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340271&amp;v=MDYzNzhIN1I3cWRaK1p1RmlybFc3ck1JMTA9Tmo3QmFyTzRIdEhOckl0Rlp1d09ZM2s1ekJkaDRqOTlTWHFScnhveGNN&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         BREIMAN L. Random forest [J]. Machine Learning, 2001, 45 (1) : 5-32.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_18" title=" 周志华.机器学习[M]. 北京: 清华大学出版社, 2016: 179-181. (ZHOU Z H. Machine Learning[M]. Beijing: Tsinghua University Press, 2016: 179-181.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MjMwNDZDNEhOWE9ySTFOWStzUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3Zk5KRjBYWEZxekdi&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         周志华.机器学习[M]. 北京: 清华大学出版社, 2016: 179-181. (ZHOU Z H. Machine Learning[M]. Beijing: Tsinghua University Press, 2016: 179-181.) 
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_19" title=" BLAKE C L, M C J. UCI repository of machine learning databases [EB/OL]. [2018- 05- 10]. http://mlearn.ics.uci.edu/MLRepository.html." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=UCI repository of machine learning databases">
                                        <b>[19]</b>
                                         BLAKE C L, M C J. UCI repository of machine learning databases [EB/OL]. [2018- 05- 10]. http://mlearn.ics.uci.edu/MLRepository.html.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_20" title=" CHANG C C, LIN C J. LIBSVM: a library for support vector machines [EB/OL]. [2018- 05- 10]. http://www.csie.ntu.edu.tw/～cjlin/libsvm/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LIBSVM: a library for support vector machines">
                                        <b>[20]</b>
                                         CHANG C C, LIN C J. LIBSVM: a library for support vector machines [EB/OL]. [2018- 05- 10]. http://www.csie.ntu.edu.tw/～cjlin/libsvm/.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_21" title=" LIU F T, TING K M, ZHOU Z H. Isolation-based anomaly detection [EB/OL]. [2018- 05- 10]. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Isolation-based anomaly detection">
                                        <b>[21]</b>
                                         LIU F T, TING K M, ZHOU Z H. Isolation-based anomaly detection [EB/OL]. [2018- 05- 10]. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_22" title=" HAN J W, KAMBER M. 数据挖掘: 概念与技术[M]. 范明, 孟小峰, 译.3版.北京: 机械工业出版社, 2012: 236-240. (HAN J W, KAMBER M. Data Mining: Concepts and Techniques [M]. FAN M, MENG X F, translated. 3rd ed. Beijing: China Machine Press, 2012: 236-240.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787111374312001&amp;v=MTYxOTFYWEZxekdiSzVIOUxMcTR4RVp1c1BEUk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bm5VN2ZOSkYw&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         HAN J W, KAMBER M. 数据挖掘: 概念与技术[M]. 范明, 孟小峰, 译.3版.北京: 机械工业出版社, 2012: 236-240. (HAN J W, KAMBER M. Data Mining: Concepts and Techniques [M]. FAN M, MENG X F, translated. 3rd ed. Beijing: China Machine Press, 2012: 236-240.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-11-26 13:57</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),956-962 DOI:10.11772/j.issn.1001-9081.2018091966            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于双特征和松弛边界的随机森林进行异常点检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E6%B7%BC&amp;code=38151244&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡淼</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%BC%80%E5%86%9B&amp;code=23950352&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王开军</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%BB%BA%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0222237&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福建师范大学数学与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%BB%BA%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%97%E7%A6%8F%E5%BB%BA%E7%8E%AF%E5%A2%83%E7%9B%91%E6%B5%8B%E7%89%A9%E8%81%94%E7%BD%91%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福建师范大学数字福建环境监测物联网实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有基于随机森林的异常检测算法性能不高的问题, 提出一种结合双特征和松弛边界的随机森林算法用于异常点检测。首先, 在只使用正常类数据构建随机森林的分类决策树过程中, 在二叉决策树的每个节点里记录两个特征的取值范围 (每个特征对应一个值域) , 以此双特征值域作为异常点判断的依据。然后, 在进行异常检测时, 当某样本不满足决策树节点中的双特征值域时, 该样本被标记为候选异常类;否则, 该样本进入决策树的下层树节点继续作特征值域的比较, 若无下层节点则被标记为候选正常类。最后, 由随机森林算法中的判别机制决定该样本的类别。在5个UCI数据集上进行的异常点检测实验结果表明, 所提方法比现有的异常检测随机森林算法性能更好, 其综合性能与孤立森林 (iForest) 和一类支持向量机 (OCSVM) 方法相当或更好, 且稳定于较高水平。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%82%E5%B8%B8%E7%82%B9%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">异常点检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">随机森林;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E7%89%B9%E5%BE%81%E8%BF%87%E6%BB%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双特征过滤;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%BE%E5%BC%9B%E8%BE%B9%E7%95%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">松弛边界;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    胡淼 (1994—) , 男, 安徽太和人, 硕士研究生, 主要研究方向:机器学习、数据挖掘;;
                                </span>
                                <span>
                                    *王开军 (1965—) , 男, 福建福州人, 副教授, 博士, 主要研究方向:机器学习、数据挖掘。电子邮箱wkjwang@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61672157);</span>
                                <span>福建省自然科学基金资助项目 (2018J01778);</span>
                    </p>
            </div>
                    <h1><b>Random forest based on double features and relaxation boundary for anomaly detection</b></h1>
                    <h2>
                    <span>HU Miao</span>
                    <span>WANG Kaijun</span>
            </h2>
                    <h2>
                    <span>College of Mathematics and Informatics, Fujian Normal University</span>
                    <span>Digit Fujian Internet-of-Things Laboratory of Environmental Monitoring, Fujian Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the low performance of existing anomaly detection algorithms based on random forest, a random forest algorithm combining double features and relaxation boundary was proposed for anomaly detection. Firstly, in the process of constructing binary decision tree of random forest with normal class data only, the range of two features (each feature had a corresponding eigenvalue range) were recorded in each node of the binary decision tree, and the double-feature eigenvalue ranges were used as the basis for abnormal point judgment. Secondly, during the anomaly detection, if a sample did not satisfy the double-feature eigenvalue range in the decision tree node, the sample would be marked as a candidate exception class; otherwise, the sample would enter the lower nodes of the decision tree and continue the comparision with the corresponding double-feature eigenvalue range. The sample would be marked as candidate normal class if there were no lower nodes. Finally, the discriminative mechanism in random forest algorithm was used to distinguish the class of the samples. Experimented results on five UCI datasets show that the proposed method has better performance than the existing random forest algorithms for anomaly detection, and its comprehensive performance is equivalent to or better than isolation Forest (iForest) and One-Class SVM (OCSVM) , and stable at a high level.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=anomaly%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">anomaly detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Random%20Forest%20(RF)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Random Forest (RF) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=double-feature%20filtering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">double-feature filtering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=relaxation%20boundary&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">relaxation boundary;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    HU Miao, born in 1994, M. S. candidate. His research interests include machine learning, data mining.;
                                </span>
                                <span>
                                    WANG Kaijun, born in 1965, Ph. D. , associate professor. His research interests include machine learning, data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-25</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61672157);</span>
                                <span>the Natural Science Foundation of Fujian Province (2018J01778);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">异常点检测问题是许多相关领域的研究热点。Hawkins的定义<citation id="224" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>揭示了异常点的本质:“异常点的表现与其他点如此不同, 不禁让人怀疑它是由不同机制产生的”。在数据挖掘的工作中开展异常检测, 其任务是发现与常规数据模式显著不同的数据模式, 亦可将异常检测认为是一种对新模式的发现。目前, 异常点检测已经在信用卡欺诈、电子商务中的犯罪行为探测、 网络入侵检测分析等领域得到了广泛应用。</p>
                </div>
                <div class="p1">
                    <p id="49">专家学者针对异常点检测问题提出了一系列方法<citation id="225" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>:文献<citation id="226" type="reference">[<a class="sup">3</a>]</citation>提出一类支持向量机 (One-Class SVM, OCSVM) 机器学习算法用于异常点检测, 用正常类数据训练OCSVM, 然后用训练好的模型对输入数据分类从而检测异常值;文献<citation id="236" type="reference">[<a class="sup">4</a>,<a class="sup">5</a>]</citation>介绍基于统计学习理论的OCSVM在网络入侵检测和财务异常检测中的应用;文献<citation id="237" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>]</citation>介绍了基于深度学习技术的异常点检测方法。近年来基于集成学习方法的异常点检测是一个研究热点。文献<citation id="227" type="reference">[<a class="sup">8</a>]</citation>提出孤立森林 (isolation Forest, iForest) 算法, 依据随机选择训练集<i>D</i>中的一个特征<i>q</i>及其最大值和最小值之间值<i>p</i>, 将<i>D</i>中在<i>q</i>特征上比<i>p</i>大的样本分至右子节点, 比<i>p</i>小的样本分至左子节点;递归该步骤直至节点中只包含一个样本或多个相同样本时停止分裂, 完成孤立树 (isolation Tree, iTree) 的构建;然后根据根节点到叶子节点的路径长度建立异常指数, 当该指数值趋于1时, 该叶节点的样本很可能是异常样本。文献<citation id="228" type="reference">[<a class="sup">9</a>]</citation>基于iTree<citation id="229" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出一种改进算法EGITree (Entropy-based Greedy Isolation Tree) , 该算法通过启发式搜索检测出<i>k</i>个异常度较高的数据点。文献<citation id="230" type="reference">[<a class="sup">10</a>]</citation>提出一种基于随机森林的异常点检测方法, 随机森林训练时, 得到所有样本的异常点尺度, 如果某一个样本的异常点尺度比较大, 则说明这个样本与其他样本的相似程度较小, 很有可能是异常样本, 把异常点尺度超过某个阈值的样本当作异常点。文献<citation id="231" type="reference">[<a class="sup">11</a>]</citation>提出基于随机森林的异常点检测算法RFV (Random Forest based on Votes for anomaly detection) 和RFP (Random Forest based on Proximity matrix for anomaly detection) 。RFV通过对正常类别数据构建随机森林模型, 得到每一个正常类的投票均值, 分类时通过样本的投票数是否达到正常类的投票均值, 来判断样本是否为异常点。RFP训练随机森林时, 得到样本的相似度矩阵, 然后计算得到每个类的类内相似度, 作为阈值;分类时, 计算新样本与每个类的相似度, 若小于各类的阈值, 则认为是异常点。对于算法RFV, 当树节点中最优切分点值大于或小于所有异常样本对应特征值时, 致使异常样本点全部落入同一叶子节点中, 其投票后的分类结果偏向于某一正常类, 异常点检测易失败。文献<citation id="232" type="reference">[<a class="sup">10</a>]</citation>方法和RFP算法<citation id="233" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>均采用样本的相似度矩阵计算相似度, 这种计算相似度的方式只考虑样本落在一棵树同一叶子节点的情况, 对落入不同叶子节点的样本间的相似度都统一视为0, 故不能全面而完整地度量样本之间的相似度<citation id="234" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。这些不足, 降低了RFV和RFP算法在异常检测方面的性能。文献<citation id="235" type="reference">[<a class="sup">13</a>]</citation>中提出一种结合模糊方法的随机森林进行异常点检测方法 (Random Forest algorithm based on Fuzzy Tree node for anomaly detection, FuzzyTRF) , 利用关键特征在树节点中构造模糊值域进行异常点判断。FuzzyTRF方法中设计的模糊隶属度函数对异常检测的成败至关重要, 而面对复杂应用时, 设计出最优的模糊隶属度函数较为困难。</p>
                </div>
                <div class="p1">
                    <p id="50">为了进一步提高基于决策树的随机森林方法在异常检测方面的性能, 本文将双关键特征过滤方法引入到随机森林的决策树模型中, 提出结合双特征和松弛边界的异常点检测 (Double Features and Relaxation Boundary for anomaly detection, DFRB) 算法。DFRB算法使用正常类样本构建随机森林模型过程中, 在决策树节点中设计两个关键特征的值域用于过滤样本, 异常点检测阶段利用构建好的双特征值域过滤出异常点。本文算法还可以避免距离度量和相似度矩阵的计算问题。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="52">本章主要介绍分类与回归树 (Classification And Regression Tree, CART) 算法<citation id="240" type="reference"><link href="206" rel="bibliography" /><link href="208" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>、Bagging分类器<citation id="238" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 以及随机森林算法<citation id="239" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="53">CART<citation id="241" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>由特征选择、树的生成以及剪枝组成, 既可用于分类也可用于回归, 具体实现可参见文献<citation id="242" type="reference">[<a class="sup">15</a>]</citation>。</p>
                </div>
                <div class="p1">
                    <p id="54">Bagging算法<citation id="243" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>是一种通过操作训练样本集来生成互异的子分类器的算法, 其基础是自助抽样法 (bootstrap sampling) , 给定包含<i>m</i>个样本的数据集<i>D</i>, 进行有放回的采样<i>m</i>次, 得到含有<i>m</i>个样本的集合<i>D</i><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>m</i>) , 使用<i>D</i><sub><i>i</i></sub>构建基分类器, <i>D</i><sub><i>i</i></sub>不完全包含<i>D</i>中样本, 从而保证了训练集的差异性。</p>
                </div>
                <div class="p1">
                    <p id="55">随机森林 (Random Forest, RF) <citation id="244" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>是Bagging的一个扩展变体, RF是以决策树为基分类器构建在Bagging基础上, 进一步在决策树构建过程中引入随机特征选择。</p>
                </div>
                <div class="p1">
                    <p id="56">算法1 Random Forest算法。</p>
                </div>
                <div class="p1">
                    <p id="57">输入 训练集<i>D</i>, 待测样本<i>X</i>。</p>
                </div>
                <div class="p1">
                    <p id="58">输出 树的集合{<i>tree</i><sub><i>i</i></sub>|<i>i</i>=1, 2, …, <i>N</i>};对待测样本<i>X</i>, 决策树<i>tree</i><sub><i>i</i></sub>输出<i>tree</i><sub><i>i</i></sub> (<i>X</i>) ;通过投票, 确定样本类别<i>f</i> (<i>X</i>) 。<i>f</i> (<i>X</i>) =<i>majority</i>{<i>tree</i><sub><i>i</i></sub> (<i>X</i>) }<mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="60">for <i>i</i>=1 in <i>N</i> do:</p>
                </div>
                <div class="p1">
                    <p id="61">对<i>D</i>做Boostrap抽样, 生成训练集<i>D</i><sub><i>i</i></sub></p>
                </div>
                <div class="p1">
                    <p id="62">使用<i>D</i><sub><i>i</i></sub>生成一棵不剪枝的CART:</p>
                </div>
                <div class="p1">
                    <p id="63">从<i>d</i>个特征中随机地选择<i>k</i>个特征</p>
                </div>
                <div class="p1">
                    <p id="64">在每个节点上从<i>k</i>个特征中依据Gini指数选择最优分裂特征</p>
                </div>
                <div class="p1">
                    <p id="65">分裂直至满足终止条件</p>
                </div>
                <div class="p1">
                    <p id="66">endfor</p>
                </div>
                <div class="p1">
                    <p id="67">随机森林算法原理简单、容易实现、计算开销小, 它在很多现实任务中展现出强大的性能, 被誉为“代表集成学习技术水平的方法”, 可看出RF对Bagging只是作了小改动, 但是与Bagging中的基分类器的“多样性”仅通过样本扰动是不同的, RF中的基分类器的多样性不仅来自样本扰动, 还来自特征的扰动, 这使得最终集成的泛化性能可通过个体分类器之间差异度的增加而进一步提升<citation id="245" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 结合双特征和松弛边界的异常点检测</h3>
                <h4 class="anchor-tag" id="69" name="69">2.1 DFRB<b>算法原理</b></h4>
                <div class="p1">
                    <p id="70">随机森林算法中的决策树具有计算简单, 既可处理离散数据也可处理连续型数据的优点;而且不需要计算样本相似度, 可避免高维样本的维数灾难问题, 且算法不易过拟合<citation id="246" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。同时, 鉴于异常点的关键特征值与正常类样本取值有较大差异的特性<citation id="247" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 即异常点与正常点在关键特征上取值不同, 故本文的DFRB以随机森林算法为基本框架, 将关键特征过滤机制结合到决策树中, 在树节点中记录关键特征的值域, 使用该值域进行异常点的过滤。</p>
                </div>
                <div class="p1">
                    <p id="71">本文DFRB算法设计为只使用正常类样本构建随机森林模型。决策树的分裂过程可描述为使用垂直于坐标轴的分割超平面进行递归划分, 直至满足停止条件。如图1所示, 使用2类样本训练一棵决策树, 在决策树分裂过程中设计双特征值域来刻画分裂区域, 决策树训练完成时, 样本空间被划分成多个不重叠区域;若某样本落入这些区域之外, 该决策树将其记为候选异常点。这个过程即是在决策树的每个节点中设计描述正常类样本取值范围的双特征值域, 该值域用于过滤出候选异常点;当某个类别未知的样本通过DFRB随机森林模型时, 每个决策树标记该样本为候选正常类或异常类, 最终按照随机森林算法的判别机制 (包括投票规则) , 识别该样本为正常类或异常类。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904005_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 决策树分割示意图" src="Detail/GetImg?filename=images/JSJY201904005_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 决策树分割示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904005_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic diagram of decision tree segmentation</p>

                </div>
                <h4 class="anchor-tag" id="73" name="73">2.2 <b>决策树节点的双特征值域</b></h4>
                <div class="p1">
                    <p id="74">基于CART树, 在树节点中设计双特征值域, 构造一种结合双特征值域的CART (CART with Double Features, DF-CART) 。构建CART时, 在二叉决策树的节点中依据Gini指数得到该节点的最优特征和最优分割点, 并且记录样本在两个特征上的取值范围 (称为特征值域) 。最优特征对决策树的分类贡献最大, 将其作为一个重要考量。树节点中设计的两个特征分别是父节点的最优特征和一个随机特征。</p>
                </div>
                <div class="p1">
                    <p id="75">算法2 决策树及双特征值域的生成。</p>
                </div>
                <div class="p1">
                    <p id="76">输入 训练数据集<i>D</i> (特征个数为<i>d</i>) ;停止计算条件。</p>
                </div>
                <div class="p1">
                    <p id="77">输出 DF-CART决策树 (二叉树) 。</p>
                </div>
                <div class="p1">
                    <p id="78">1) 在决策树训练过程中, 从特征集合中随机选取一个包含<i>k</i>个特征的候选特征集, 记为<i>F</i>。</p>
                </div>
                <div class="p1">
                    <p id="79">2) 在当前节点统计所有样本在父节点的最优分裂特征<i>f</i>上的取值, 得到取值范围 (值域) <i>R</i> ( <i>f</i>) ;然后, 随机选择一个特征<i>f</i><sub><i>r</i></sub>, 统计得到值域<i>R</i> ( <i>f</i><sub><i>r</i></sub>) 。</p>
                </div>
                <div class="p1">
                    <p id="80">根节点没有父节点, 随机从特征集合<i>F</i>中选择一个特征作为根节点的父节点的最优特征<i>f</i>。</p>
                </div>
                <div class="p1">
                    <p id="81">3) 计算<i>F</i>中每个候选特征对当前节点的基尼指数, 对应最小基尼指数得到最优分裂特征<i>f</i><sub><i>b</i></sub>和最优切分点<i>V</i><sub><i>b</i></sub>, 根据最优切分点生成左、右子节点。</p>
                </div>
                <div class="p1">
                    <p id="82">4) 对每个子节点递归重复步骤1) ～3) , 直至满足停止条件。</p>
                </div>
                <div class="p1">
                    <p id="83">停止条件:节点中的样本个数小于预定值, 或样本集的基尼指数小于预设值 (此时样本基本属于同一类) , 或者树的高度达到预设值。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">2.3 <b>结合双特征和松弛边界的异常点检测</b></h4>
                <div class="p1">
                    <p id="85">本文算法DFRB的实现分为两个阶段:</p>
                </div>
                <div class="p1">
                    <p id="86">1) 训练阶段。构建随机森林模型, 采用算法2的决策树作为随机森林模型的基分类器。</p>
                </div>
                <div class="p1">
                    <p id="87">训练完成后, 得到的随机森林模型与传统随机森林不同的是在每个决策树的节点中记录了双特征的值域, 用于分类阶段进行异常样本过滤。</p>
                </div>
                <div class="p1">
                    <p id="88">随机森林的决策树构建时, 训练数据是抽样得到, 抽样数据不包含所有数据的信息, 这就使得由样本得到的特征值域小于正常类数据的真实值域, 很容易将正常类样本误认为是异常点, 故需要对得到的特征值域作松弛处理。</p>
                </div>
                <div class="p1">
                    <p id="89">2) 检测阶段。引入松弛因子<i>σ</i>对特征值域作松弛操作。检测算法见算法3。</p>
                </div>
                <div class="p1">
                    <p id="90">算法3 基于随机森林的DFRB模型分类算法。</p>
                </div>
                <div class="p1">
                    <p id="91">输入 包含异常样本的测试集<i>T</i>;松弛因子<i>σ</i> (通常取值0～0.05) ;异常比例<i>ε</i> (通常取值0.1～0.5) 。</p>
                </div>
                <div class="p1">
                    <p id="92">输出 测试样本的类别<i>y</i>。</p>
                </div>
                <div class="p1">
                    <p id="93">for <i>i</i> in <i>T</i> do</p>
                </div>
                <div class="p1">
                    <p id="94">for <i>j</i> in {<i>Tree</i><sub><i>j</i></sub>| <i>j</i>=1, 2, …, <i>N</i>} do</p>
                </div>
                <div class="p1">
                    <p id="95">样本<i>x</i><sub><i>i</i></sub>从根节点开始, 递归地在树节点中做两次异常判别</p>
                </div>
                <div class="p1">
                    <p id="96">if 当前是叶子节点且叶子节点的样本数小于等于2</p>
                </div>
                <div class="p1">
                    <p id="97">不作判别, 将当前叶子节点代表的类别赋予<i>y</i><sub><i>ij</i></sub></p>
                </div>
                <div class="p1">
                    <p id="98">else</p>
                </div>
                <div class="p1">
                    <p id="99">if <i>x</i><sub><i>i</i><sub><i>f</i></sub></sub>∉[<i>a</i>-|<i>R</i> (<i>f</i>) |*<i>σ</i>, <i>b</i>+|<i>R</i> (<i>f</i>) |*<i>σ</i>]</p>
                </div>
                <div class="p1">
                    <p id="100">//<i>x</i><sub><i>i</i></sub>的<i>f</i>特征值不属于该特征值域</p>
                </div>
                <div class="p1">
                    <p id="101">样本<i>x</i><sub><i>i</i></sub>在<i>j</i>这棵树上输出<i>y</i><sub><i>ij</i></sub>=-1</p>
                </div>
                <div class="p1">
                    <p id="102">continue</p>
                </div>
                <div class="p1">
                    <p id="103">else</p>
                </div>
                <div class="p1">
                    <p id="104">样本<i>x</i><sub><i>i</i></sub>根据最优切分点进入孩子节点, 若节点不再分裂, 将当前叶子节点代表的类别赋予<i>y</i><sub><i>ij</i></sub></p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mrow><mo>{</mo><mrow><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false"> (</mo><mo stretchy="false">{</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">}</mo><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup><mrow><mo>=</mo><mo>=</mo></mrow><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow><mi>Ν</mi></mfrac></mrow><mo>}</mo></mrow><mo>≥</mo><mi>ε</mi></mtd></mtr><mtr><mtd><mi>y</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mo>-</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo>/</mo><mo>/</mo><mtext>即</mtext><mtext>该</mtext><mtext>样</mtext><mtext>本</mtext><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mtext>是</mtext><mtext>异</mtext><mtext>常</mtext><mtext>点</mtext></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">else:</p>
                </div>
                <div class="p1">
                    <p id="107"><i>y</i><sub><i>x</i><sub><i>i</i></sub></sub>=<i>majority</i>{<i>y</i><sub><i>ij</i></sub>}<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>//多数投票决定样本<i>x</i><sub><i>i</i></sub>的类别</p>
                </div>
                <div class="p1">
                    <p id="109">算法3中:<i>a</i>、<i>b</i>分别为特征值域的左右边界;|<i>R</i> ( <i>f</i>) |即是|<i>a</i>-<i>b</i>|;异常比例<i>ε</i>表示随机森林中判断样本是异常点的决策树数量与随机森林中决策树总量的比值。对于某样本, 统计随机森林的输出结果, 若判断样本为异常点的决策树所占的比例大于<i>ε</i>, 则认为该样本点是异常点;否则归为多数票决定的类别。因每棵决策树所使用的是部分特征, 而部分特征不能保证每棵决策树都能有效检测异常点, 则只有部分树可以有效检测出异常点, 故将<i>ε</i>设置在区间[0.1, 0.5]内。</p>
                </div>
                <h3 id="110" name="110" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="111" name="111">3.1 <b>实验数据和性能指标</b></h4>
                <div class="p1">
                    <p id="112">本文实验使用python3.6实现算法编码。选用UCI <citation id="248" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>中的5个数据集对算法进行测试, 数据集的相关信息如表1所示。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit"><b>表</b>1 <b>实验数据集的相关信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Information of experimental datasets</p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td><br />数据集</td><td>样本数</td><td>特征数</td><td>类别数</td><td>各类别样本数</td></tr><tr><td><br />seeds</td><td>210</td><td>7</td><td>3</td><td>70∶70∶70</td></tr><tr><td><br />hepato</td><td>536</td><td>9</td><td>4</td><td>116∶178∶124∶118</td></tr><tr><td><br />wine</td><td>178</td><td>13</td><td>3</td><td>59∶71∶48</td></tr><tr><td><br />forestType</td><td>523</td><td>27</td><td>4</td><td>159∶86∶83∶195</td></tr><tr><td><br />dermatology</td><td>366</td><td>34</td><td>6</td><td>112∶61∶72∶49∶52∶20</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="114">本文选择OCSVM<citation id="249" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、RFV<citation id="250" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、RFP<citation id="251" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、iForest<citation id="252" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>四种方法作性能对比:OCSVM的程序来源于libSVM<citation id="253" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>;iForest算法速度快、精度高, 在工业开发中得到广泛应用, 被多个机器学习库集成, 本文的测试的程序来源于sklearn库提供的iForest算法<citation id="254" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>;RFV和RFP算法的程序由作者依据文献<citation id="255" type="reference">[<a class="sup">11</a>]</citation>编写。</p>
                </div>
                <div class="p1">
                    <p id="115">本文实验采用召回率 (Recall, R) 、精度 (Precision, P) 和<i>F</i>1作为评价指标<citation id="256" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。召回率是完全性的度量 (即正元组标记为正的百分比) , 本文实验指异常点标记为异常的百分比;精度是精确性的度量 (即标记为正类的元组, 实际为正类的百分比) , 本文实验指被标记为异常的元组中, 实际为异常点的百分比。</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mi>r</mi></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>n</mtext><mtext>o</mtext><mtext>m</mtext><mtext>a</mtext><mtext>l</mtext><mtext>y</mtext></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mi>r</mi></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>e</mtext><mtext>t</mtext><mtext>e</mtext><mtext>c</mtext><mtext>t</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">采用<i>F</i>1指标综合评价算法性能, 它是<i>R</i>和<i>P</i>的调和平均, 采用以下定义:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mn>1</mn><mo stretchy="false"> (</mo><mi>R</mi><mo>, </mo><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>R</mi><mo>×</mo><mi>Ρ</mi></mrow><mrow><mi>R</mi><mo>+</mo><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">其中:<i>N</i><sub><i>r</i></sub>表示系统正确识别的异常样本数;<i>N</i><sub>anomaly</sub>表示测试集中异常样本总数;<i>N</i><sub>detection</sub>表示系统检测出的异常样本总数。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">3.2 <b>异常点识别性能对比</b></h4>
                <div class="p1">
                    <p id="121">实验时, 首先对数据进行归一化处理;然后采用文献<citation id="257" type="reference">[<a class="sup">11</a>]</citation>中的仿真模式, 对一个数据集轮流选择一个类C<sub>i</sub> (i=1, 2, 3, …, k) 作为异常类数据 (<i>anomaly</i>_<i>data</i>) , 其余K-1类作为正常类数据。采用10折交叉验证方法评估分类器的性能:将正常类数据按顺序分成10份, 第1次实验取第1份数据和异常类数据合并作为测试集 (test_data) , 剩余9份数据作为训练集 (train_data) 用于构建随机森林模型, 第2次实验取第2份数据和异常类数据合并作为测试集 (test_data) , 剩余9份数据作为训练集 (train_data) , 以此类推重复10次实验, 最后将10次实验的结果平均作为性能指标。</p>
                </div>
                <div class="p1">
                    <p id="122">对RFV、RFP、iForest和本文DFRB算法, 均设置随机森林中树的数目<i>m</i>=100, 决策树停止分裂的条件是:节点中的样本个数小于3, 或节点样本集的基尼指数小于10<sup>-7</sup>, 或者树的高度达到10层。OCSVM的参数取程序设置的默认值<i>nu</i>=0.5;依据文献<citation id="258" type="reference">[<a class="sup">8</a>]</citation>中所给的参数contamination (<i>c</i>) 范围为0.02～0.5, iForest的参数取实验效果最好时的<i>c</i>=0.5, DFRB的参数<i>ε</i>、<i>σ</i>分别取0.3和0.01。实验结果对比如表2～6所示。</p>
                </div>
                <div class="area_img" id="123">
                    <p class="img_tit"><b>表</b>2 <b>不同算法在</b>seeds<b>数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Experimental results of different algorithms on seeds dataset</p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td><br />异常类别</td><td>指标</td><td>RFV</td><td>RFP</td><td>iForest</td><td>OCSVM</td><td>DFRB</td></tr><tr><td rowspan="3"><br />Class1</td><td><br /><i>R</i></td><td>0.698 5</td><td>0.901 4</td><td><b>0.997</b><b>1</b></td><td>0.978 5</td><td>0.845 7</td></tr><tr><td><br /><i>P</i></td><td><b>0.978</b><b>6</b></td><td>0.945 4</td><td>0.894 1</td><td>0.907 0</td><td>0.957 4</td></tr><tr><td><br /><i>F</i>1</td><td>0.813 5</td><td>0.921 0</td><td><b>0.942</b><b>7</b></td><td>0.941 1</td><td>0.897 6</td></tr><tr><td rowspan="3"><br />Class2</td><td><br /><i>R</i></td><td>0.185 7</td><td>0.087 1</td><td><b>1.000</b><b>0</b></td><td><b>1.000</b><b>0</b></td><td>0.984 2</td></tr><tr><td><br /><i>P</i></td><td>0.636 1</td><td>0.504 7</td><td>0.894 2</td><td>0.910 8</td><td><b>0.921</b><b>9</b></td></tr><tr><td><br /><i>F</i>1</td><td>0.285 5</td><td>0.141 0</td><td>0.944 1</td><td><b>0.953</b><b>1</b></td><td>0.951 8</td></tr><tr><td rowspan="3"><br />Class3</td><td><br /><i>R</i></td><td>0.191 4</td><td>0.301 4</td><td><b>1.000</b><b>0</b></td><td>0.998 5</td><td>0.732 8</td></tr><tr><td><br /><i>P</i></td><td>0.667 2</td><td>0.522 9</td><td>0.895 5</td><td>0.907 6</td><td><b>0.925</b><b>7</b></td></tr><tr><td><br /><i>F</i>1</td><td>0.279 3</td><td>0.345 7</td><td>0.944 7</td><td>0.950 6</td><td>0.815 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表</b>3 <b>不同算法在</b>hepato<b>数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Experimental results of different algorithms on hepato dataset</p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td><br />异常类别</td><td>指标</td><td>RFV</td><td>RFP</td><td>iForest</td><td>OCSVM</td><td>DFRB</td></tr><tr><td rowspan="3"><br />Class1</td><td><br /><i>R</i></td><td><b>0.999</b><b>1</b></td><td>0.904 3</td><td>0.607 7</td><td>0.599 1</td><td>0.992 2</td></tr><tr><td><br /><i>P</i></td><td>0.786 8</td><td>0.808 4</td><td>0.796 5</td><td><b>0.819</b><b>2</b></td><td>0.812 0</td></tr><tr><td><br /><i>F</i>1</td><td>0.880 2</td><td>0.853 5</td><td>0.688 9</td><td>0.691 9</td><td><b>0.893</b><b>0</b></td></tr><tr><td rowspan="3"><br />Class2</td><td><br /><i>R</i></td><td><b>0.998</b><b>3</b></td><td>0.817 4</td><td>0.740 4</td><td>0.574 1</td><td>0.984 2</td></tr><tr><td><br /><i>P</i></td><td>0.876 1</td><td>0.873 2</td><td><b>0.892</b><b>8</b></td><td>0.886 1</td><td>0.884 6</td></tr><tr><td><br /><i>F</i>1</td><td><b>0.933</b><b>1</b></td><td>0.844 1</td><td>0.809 3</td><td>0.696 6</td><td>0.931 7</td></tr><tr><td rowspan="3"><br />Class3</td><td><br /><i>R</i></td><td><b>0.979</b><b>8</b></td><td>0.781 4</td><td>0.378 2</td><td>0.257 2</td><td>0.876 6</td></tr><tr><td><br /><i>P</i></td><td>0.807 6</td><td>0.804 2</td><td>0.738 5</td><td>0.678 1</td><td><b>0.813</b><b>5</b></td></tr><tr><td><br /><i>F</i>1</td><td><b>0.885</b><b>3</b></td><td>0.792 4</td><td>0.498 3</td><td>0.372 1</td><td>0.843 5</td></tr><tr><td rowspan="3"><br />Class4</td><td><br /><i>R</i></td><td><b>1.000</b><b>0</b></td><td>0.870 3</td><td>0.341 5</td><td>0.307 6</td><td>0.992 3</td></tr><tr><td><br /><i>P</i></td><td>0.748 3</td><td><b>0.787</b><b>0</b></td><td>0.681 4</td><td>0.690 3</td><td>0.760 9</td></tr><tr><td><br /><i>F</i>1</td><td>0.856 0</td><td>0.826 1</td><td>0.453 0</td><td>0.424 5</td><td><b>0.861</b><b>3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表</b>4 <b>不同算法在</b>wine<b>数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Experimental results of different algorithms on wine dataset</p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td><br />异常类别</td><td>指标</td><td>RFV</td><td>RFP</td><td>iForest</td><td>OCSVM</td><td>DFRB</td></tr><tr><td rowspan="3"><br />Class1</td><td><br /><i>R</i></td><td>0.977 5</td><td>0.962 7</td><td><b>1.000</b><b>0</b></td><td>0.949 1</td><td>0.940 6</td></tr><tr><td><br /><i>P</i></td><td>0.879 6</td><td>0.901 0</td><td>0.887 1</td><td>0.900 5</td><td><b>0.923</b><b>3</b></td></tr><tr><td><br /><i>F</i>1</td><td>0.925 3</td><td>0.929 8</td><td><b>0.939</b><b>8</b></td><td>0.922 9</td><td>0.931 0</td></tr><tr><td rowspan="3"><br />Class2</td><td><br /><i>R</i></td><td>0.981 6</td><td>0.977 4</td><td><b>1.000</b><b>0</b></td><td><b>1.000</b><b>0</b></td><td><b>1.000</b><b>0</b></td></tr><tr><td><br /><i>P</i></td><td>0.952 6</td><td>0.933 4</td><td>0.908 3</td><td>0.925 2</td><td><b>0.952</b><b>9</b></td></tr><tr><td><br /><i>F</i>1</td><td>0.966 7</td><td>0.954 7</td><td>0.951 8</td><td>0.960 9</td><td><b>0.975</b><b>5</b></td></tr><tr><td rowspan="3"><br />Class3</td><td><br /><i>R</i></td><td>0.985 4</td><td>0.995 8</td><td><b>1.000</b><b>0</b></td><td><b>1.000</b><b>0</b></td><td><b>1.000</b><b>0</b></td></tr><tr><td><br /><i>P</i></td><td>0.861 5</td><td>0.886 8</td><td>0.858 8</td><td>0.878 7</td><td><b>0.900</b><b>2</b></td></tr><tr><td><br /><i>F</i>1</td><td>0.918 9</td><td>0.937 7</td><td>0.923 6</td><td>0.935 1</td><td><b>0.947</b><b>1</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="126">
                    <p class="img_tit"><b>表</b>5 <b>不同算法在</b>forestType<b>数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Experimental results of different algorithms on forestType dataset</p>
                    <p class="img_note"></p>
                    <table id="126" border="1"><tr><td><br />异常类别</td><td>指标</td><td>RFV</td><td>RFP</td><td>iForest</td><td>OCSVM</td><td>DFRB</td></tr><tr><td rowspan="3"><br />Class1</td><td><br /><i>R</i></td><td><b>0.933</b><b>5</b></td><td>0.874 8</td><td>0.871 6</td><td>0.888 6</td><td>0.886 7</td></tr><tr><td><br />P</td><td>0.867 0</td><td><b>0.906</b><b>0</b></td><td>0.869 5</td><td>0.889 6</td><td>0.891 6</td></tr><tr><td><br />F1</td><td>0.898 8</td><td><b>0.889</b><b>9</b></td><td>0.870 1</td><td>0.886 6</td><td>0.889 0</td></tr><tr><td rowspan="3"><br />Class2</td><td><br />R</td><td><b>0.974</b><b>4</b></td><td>0.923 2</td><td>0.952 3</td><td>0.956 9</td><td>0.895 3</td></tr><tr><td><br />P</td><td>0.697 9</td><td>0.792 7</td><td>0.779 6</td><td>0.803 8</td><td><b>0.808</b><b>3</b></td></tr><tr><td><br />F1</td><td>0.813 1</td><td>0.852 0</td><td>0.856 9</td><td><b>0.873</b><b>0</b></td><td>0.849 4</td></tr><tr><td rowspan="3"><br />Class3</td><td><br />R</td><td>0.669 8</td><td>0.318 0</td><td><b>0.962</b><b>6</b></td><td>0.951 8</td><td>0.880 7</td></tr><tr><td><br />P</td><td>0.610 8</td><td>0.569 1</td><td>0.766 9</td><td><b>0.797</b><b>8</b></td><td>0.796 4</td></tr><tr><td><br />F1</td><td>0.636 3</td><td>0.406 0</td><td>0.853 3</td><td><b>0.866</b><b>9</b></td><td>0.835 8</td></tr><tr><td rowspan="3"><br />Class4</td><td><br />R</td><td><b>0.985</b><b>6</b></td><td>0.962 0</td><td>0.945 1</td><td>0.905 6</td><td>0.942 5</td></tr><tr><td><br />P</td><td>0.885 7</td><td>0.926 5</td><td>0.917 9</td><td>0.921 1</td><td><b>0.930</b><b>4</b></td></tr><tr><td><br />F1</td><td>0.933 0</td><td><b>0.943</b><b>9</b></td><td>0.931 0</td><td>0.913 1</td><td>0.936 3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表</b>6 <b>不同算法在</b>dermatology<b>数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 6 Experimental results of different algorithms on dermatology dataset</p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br />异常类别</td><td>指标</td><td>RFV</td><td>RFP</td><td>iForest</td><td>OCSVM</td><td>DFRB</td></tr><tr><td rowspan="3"><br />Class1</td><td><br /><i>R</i></td><td>1.000 0</td><td>0.874 1</td><td><b>1.000</b><b>0</b></td><td><b>1.000</b><b>0</b></td><td><b>1.000</b><b>0</b></td></tr><tr><td><br /><i>P</i></td><td>0.869 6</td><td>0.882 5</td><td>0.884 5</td><td>0.893 2</td><td><b>0.978</b><b>8</b></td></tr><tr><td><br /><i>F</i>1</td><td>0.930 0</td><td>0.877 2</td><td>0.938 4</td><td>0.943 2</td><td><b>0.989</b><b>1</b></td></tr><tr><td rowspan="3"><br />Class3</td><td><br /><i>R</i></td><td><b>1.000</b><b>0</b></td><td>0.768 0</td><td>0.986 1</td><td><b>1.000</b><b>0</b></td><td><b>1.000</b><b>0</b></td></tr><tr><td><br /><i>P</i></td><td>0.779 2</td><td>0.765 7</td><td>0.799 7</td><td>0.818 5</td><td><b>0.969</b><b>8</b></td></tr><tr><td><br /><i>F</i>1</td><td>0.875 4</td><td>0.763 1</td><td>0.882 6</td><td>0.899 6</td><td><b>0.984</b><b>5</b></td></tr><tr><td rowspan="3"><br />Class5<br />Class6</td><td><br /><i>R</i></td><td><b>1.000</b><b>0</b></td><td>0.513 8</td><td>0.972 2</td><td><b>1.000</b><b>0</b></td><td>0.987 5</td></tr><tr><td><br /><i>P</i></td><td>0.810 8</td><td>0.678 9</td><td>0.804 6</td><td>0.818 4</td><td><b>0.973</b><b>9</b></td></tr><tr><td><br /><i>F</i>1</td><td>0.894 7</td><td>0.580 0</td><td>0.879 4</td><td>0.899 3</td><td><b>0.980</b><b>2</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="128">对于seeds数据集, class1作为异常类进行测试时, 本文算法的检测效果稍低于RFP、iForest和OCSVM, 明显优于RFV;class2作为异常类进行测试时, 本文算法的检测效果与iForest和OCSVM相当, 明显高于RFV和RFP;class3作为异常类进行测试时, 本文算法的检测效果低于iForest和OCSVM, 明显高于RFV和RFP, 可发现iForest算法获得较高召回率的同时牺牲了检测精度。对于hepato数据集, class3作为异常类进行测试时, 本文算法检测效果略低于RFV, 其余类作为异常类进行测试时, 本文提出的算法与RFV的检测性能相当, 优于RFP, 检测效果明显高于经典的iForest和OCSVM方法;class3、class4分布松散, 且与其他类别数据交叉严重, 可分性较差, 使用class3、class4作为异常类进行测试时, iForest和OCSVM效果不理想, 异常点检测效果较差。wine数据集上, class2、class3作为异常点时, 本文算法优于其他算法;class1类作为异常点时, 本文算法的检测效率略低于iForest, 与同类的RFV、RFP方法相当, 不难发现iForest和OCSVM以牺牲精度获得较高的召回率, 精度低于DFRB。对于forestType数据集, 经典方法iForest和OCSVM在class3作为异常点时, 表现较好, 召回率较高;其余类别作为异常类进行测试时, RFV算法较优;本文算法在召回率方面低于其他方法, 而精度高, 即说明本文算法的检测准确度优于其他方法。对于dermatology数据集, 本文算法整体性能优于其他方法, 由实验结果可知, 其他方法能获得较高的召回率, 但是牺牲了精度。通过以上结果分析表明, 本文算法整体表现优越, 获得较高召回率的同时, 也能保证检测准确度。</p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904005_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 单双特征值域的性能对比分析" src="Detail/GetImg?filename=images/JSJY201904005_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 单双特征值域的性能对比分析  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904005_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Performance comparative analysis of single and double feature range</p>

                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904005_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同算法不同数据集在F1指标上的比较" src="Detail/GetImg?filename=images/JSJY201904005_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同算法不同数据集在<i>F</i>1指标上的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904005_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 <i>F</i>1 indicator comparison of different algorithms on different datasets</p>

                </div>
                <h4 class="anchor-tag" id="131" name="131">3.3 <b>双特征与单特征的对比分析</b></h4>
                <div class="p1">
                    <p id="132">本文算法利用异常点与正常点在数值上的取值不同, 提出特征值域过滤机制, 即在树节点中设计两个特征, 训练过程中统计得到两个特征值域, 并以此为依据, 进行异常点的过滤。</p>
                </div>
                <div class="p1">
                    <p id="133">图2是对比单个随机特征作为过滤条件时, 不同数据集在召回率、精度、<i>F</i>1的表现。由实验可得, 在召回率和<i>F</i>1上双值域对异常点的检测效果优于单值域, 在精度上低于单值域;即随着树节点中设计的过滤条件越多, 更多的异常点被检测出来, 召回率增加, 但是增加了正常点误认为是异常点的可能性, 导致精度降低。因此, 本文算法采用双特征值域进行异常点检测, 在获取较高召回率的同时, 保证综合性能<i>F</i>1较高。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">3.4 <b>综合性能</b><i>F</i>1<b>对比分析及参数讨论</b></h4>
                <div class="p1">
                    <p id="135">每次实验使用数据集K个类别中的一类作为异常样本 (异常点) 进行异常点检测, 采用F1指标来反映算法综合性能, 结果如图3所示, 其中横轴异常点百分比表示随机选取的异常点占这类异常测试样本的比例。</p>
                </div>
                <div class="p1">
                    <p id="136">从图3可看出, 本文算法综合性能<i>F</i>1与对比方法相当或更优。例如, 对于hepato数据集, class3作为异常类进行测试, 本文算法的综合性能<i>F</i>1略低于RFV;其余各类作为异常类进行测试时, 与RFV相当, 高于iForest和OCSVM。对于forestType数据集, 本文算法表现出色, 与一类模型方法iForest和OCSVM相当, 综合性能明显优于RFV和RFP。对于seeds数据集, class3作为异常类进行测试时, 本文算法的综合性能低于一类模型方法iForest和OCSVM, 其余类作为异常类进行测试, 本文算法的综合性能与iForest和OCSVM相当, 高于RFV和RFP。对于wine数据集, class1作为异常类进行测试, 本文算法综合性能略低于iForest, 其他类作为异常类进行测试时, 本文算法性能较好, 高于其他方法。对于dermatology数据集, 本文算法综合性能明显优于其他方法。</p>
                </div>
                <div class="p1">
                    <p id="137">本文算法中引入异常比例<i>ε</i>和松弛变量<i>σ</i>两个参数, 通过设置不同的<i>ε</i>和<i>σ</i>, 观察不同参数对算法性能的影响。图4～6的仿真实验分别分析了参数<i>ε</i>和<i>σ</i>对召回率、精度、<i>F</i>1的影响。首先, 分析不同异常比例<i>ε</i>对<i>F</i>1的影响。固定松弛因子<i>σ</i>=0.01, 对于每个数据集, 每次实验轮流选取其中一个类作为异常点进行异常检测, 得到在不同异常比例<i>ε</i>下的<i>F</i>1指标, 各次实验的<i>F</i>1均值情况如图4所示。由图4可知, <i>ε</i>的范围在0.1～0.3, <i>F</i>1指标高, 异常检测性能最好;除seeds数据集外, 随着<i>ε</i>的增大, <i>F</i>1先上升后下降, <i>ε</i>≥0.5时, <i>F</i>1趋于稳定。随机森林中, 不是所有的决策树都包含有关键特征, 即不是所有的树都能够识别出异常点, 当<i>ε</i>过大时, 不能识别异常点的树会增多, 但这些树不影响检测性能, 故<i>F</i>1趋于稳定。</p>
                </div>
                <div class="p1">
                    <p id="138">其次, 分析松弛因子<i>σ</i>对召回率、 精度和<i>F</i>1的影响。固定异常比例<i>ε</i>=0.3, 观察在不同<i>σ</i>条件下的召回率、精度和<i>F</i>1。观察图5可得随着<i>σ</i>的增大, 召回率呈下降趋势, 精度逐渐升高, 表示随着<i>σ</i>的增大, 异常的召回率降低, 识别正确率升高, 容易理解当树节点中特征值域较大时, 将异常点误认为是正常点, 导致召回率下降, 同时将正常点误认为是异常点的概率降低, 提高了异常点检测精度。为满足综合性能<i>F</i>1较高的要求, 选择<i>σ</i>=0.01时, 异常点的检测效果最佳。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904005_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 ε对F1的影响" src="Detail/GetImg?filename=images/JSJY201904005_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 <i>ε</i>对<i>F</i>1的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904005_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Effect of <i>ε</i> on <i>F</i>1</p>

                </div>
                <div class="p1">
                    <p id="140">最后, 分析异常比例<i>ε</i>和松弛变量<i>σ</i>两个参数同时变化, 对算法性能的影响。松弛因子<i>σ</i> 从0 渐变至0.07, 异常比例<i>ε</i>从0.1渐变至0.7, 观察在不同参数下的召回率、精度和<i>F</i>1。观察图6, 随着<i>ε</i>和<i>σ</i>同时增大, 召回率逐渐降低, 精度逐渐提高, 即随着过滤条件的放宽, 异常点被检测出的可能性降低, 而检测的正确率提高。考虑算法的综合性能, 算法的预设参数<i>σ</i>=[0, 0.05], <i>ε</i>=[0.1, 0.5]是合理的。</p>
                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904005_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 σ对召回率、 精度和F1的影响" src="Detail/GetImg?filename=images/JSJY201904005_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>σ</i>对召回率、 精度和<i>F</i>1的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904005_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Effect of <i>σ</i> on <i>recall</i>, <i>precision</i> and <i>F</i>1</p>

                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904005_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 ε和σ同时变化对算法性能的影响" src="Detail/GetImg?filename=images/JSJY201904005_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 <i>ε</i>和<i>σ</i>同时变化对算法性能的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904005_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Effect of simultaneous change of <i>ε</i> and <i>σ</i> on algorithm performance</p>

                </div>
                <h3 id="143" name="143" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="144">针对异常点检测问题, 设计了DFRB算法进行异常点检测, 本文算法结合异常点特性, 在决策树节点中引入特征过滤, 通过对关键特征的阈值比较达到检测异常点的目的。实验结果表明, 本文算法相对于传统随机森林算法RFV、RFP和iForest在多个数据集上有较高的综合性能, 同时本文算法避免了传统的距离度量以及相似度矩阵计算问题。本文可进一步改进的工作包括选择更合适的双特征组合和来自特殊应用的数据集参数, 以进一步提高异常点检测的精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="180">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identification of Outliers">

                                <b>[1]</b> HAWKINS D M. Identification of outliers[M]. London: Chapman and Hall, 1980: 1-2.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE4A0042AF521DCBBBC213AB7A9AB8065&amp;v=MTk5MzFKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekwyN3c2cz1OaWZPZmNhOGI5SE1xNDAwRXU0TkRRaEt2V1JobVQxOFN3NlFxMk04Q01DY1JieWFDT052RlNpV1dyNw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> DOMINGUES R, FILIPPONE M, MICHIARDI P, et al. A comparative evaluation of outlier detection algorithms: experiments and analyses [J]. Pattern Recognition, 2018, 74: 406-421.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anomaly intrusion detection using one class SVM">

                                <b>[3]</b> WANG Y, WONG J, MINER A. Anomaly intrusion detection using one class SVM[C]// Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop. Piscataway, NJ: IEEE, 2004: 358-364.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support vector method for novelty detection">

                                <b>[4]</b> SCHOLKOPF B, WILLIAMSON R, SMOLA A, et al. Support vector method for novelty detection[J]. Advances in Neural Information Processing Systems, 2000, 12 (3) : 582-588.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB2009S1014&amp;v=MzI2MDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ekxNVFhUYkxHNEh0aXZybzlFWUlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 张晓惠, 林柏钢. 基于特征选择和多分类支持向量机的异常检测[J]. 通信学报, 2009, 30 (增刊1) : 68-73. (ZHANG X H, LIN B G. Anomaly detection based on feature selection and multi-class support vector machines[J]. Journal on Communications, 2009, 30 (S1) : 68-73.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1E94FF704664CCB4133E539722914ECA&amp;v=MTkwMDZpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6TDI3dzZzPU5pZk9mYkxORjlXNjJZaEZZTzBKQ0E5S3ZSSVM2VHdJVFh6cnF4QTNjTE9RTU1udUNPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> ERFANI S M, RAJASEGARAR S, KARUNASEKERA S, et al. High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning[J]. Pattern Recognition, 2016, 58: 121-134.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning anomaly detection as support fraud investigation in Brazilian exports and anti-money laundering">

                                <b>[7]</b> PAULA E L, LADEIRA M, CARVALHO R N, et al. Deep learning anomaly detection as support fraud investigation in brazilian exports and anti-money laundering[C]// Proceedings of the 2016 IEEE International Conference on Machine Learning and Applications. Piscataway, NJ: IEEE, 2016: 954-960.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000013918&amp;v=MDk0ODdpclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0ZzUWFSRT1OaWZJWTdLN0h0ak5yNDlGWk9vTUJYMHhvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> LIU F T, TING K M, ZHOU Z H. Isolation-based anomaly detection [J]. ACM Transactions on Knowledge Discovery from Data, 2012, 6 (1) : 1-39.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel isolation-based outlier detection method">

                                <b>[9]</b> SHEN Y, LIU H, WANG Y, et al. A novel isolation-based outlier detection method[C]// PRICAI 2016: Proceedings of the 2016 Pacific Rim International Conference on Artificial Intelligence. Berlin: Springer, 2016: 446-456.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZGZ200704021&amp;v=MDU0MzdCdEdGckNVUjdxZlp1WnNGeURnVjd6TEx6Zk1kTEc0SHRiTXE0OUhaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 邱一卉, 林成德. 基于随机森林方法的异常样本检测方法 [J]. 福建工程学院学报, 2007, 5 (4) : 392-396. (QIU Y H, LIN C D. Outlier detection based on random forest[J]. Journal of Fujian University of Technology, 2007, 5 (4) : 392-396.) 
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES28DB9A6701469538A950FAA46A5D9ED9&amp;v=MjcxMzk9TmlmT2ZiR3dhcVBGM29sQ1pPb0xDblU4ekI1aTR6cDlQZzZUcUJSRWZNYWRNTTZXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekwyN3c2cw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> ZHOU Q F, ZHOU H, NING Y P, et al. Two approaches for novelty detection using random forest [J]. Expert Systems with Applications, 2015, 42 (10) : 4840-4850.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014224505.nh&amp;v=MTc5NzA1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ekxWRjI2R3JHNkd0VE1xcEViUElRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 李贞贵.随机森林改进的若干研究[D]. 厦门: 厦门大学, 2013: 28-30. (LI Z G. Several research on random forest improve[D]. Xiamen: Xiamen University, 2013: 28-30.) 
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=NJDZ201806010&amp;v=MDA3MjRkTEc0SDluTXFZOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjd6TEt5ZlA=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 胡淼, 王开军, 李海超, 等.模糊树节点的随机森林与异常点检测[J]. 南京大学学报 (自然科学版) , 2018, 54 (6) : 1141-1151. (HU M, WANG K J, LI H C, et al. A random forest algorithm based on fuzzy tree node for anomaly detection[J]. Journal of Nanjing University (Natural Science) , 2018, 54 (6) : 1141-1151.) 
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification and Regression Trees">

                                <b>[14]</b> BREIMAN L, FRIEDMAN J, OLSHEN R, et al. Classification and Regression Trees[M]. New York:Champman &amp; Hall, 1984:18-55.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302275954001&amp;v=MzA1NDRZT3NQRFJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5uVTdmTkpGMFhYRnF6R2JDNEhOUExxb1pB&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012: 67-71. (LI H. Statistical Learning Method[M]. Beijing: Tsinghua University Press, 2012: 67-71.) 
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MDAyMjZxUnJ4b3hjTUg3UjdxZForWnVGaXJsVzdyTUkxMD1OajdCYXJPNEh0SE5ySXhNWU9NTlkzazV6QmRoNGo5OVNY&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> BREIMAN L. Bagging predictors [J]. Machine Learning, 1996, 24 (2) : 123-140.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340271&amp;v=MTU1MDFJdEZadXdPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJsVzdyTUkxMD1OajdCYXJPNEh0SE5y&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> BREIMAN L. Random forest [J]. Machine Learning, 2001, 45 (1) : 5-32.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MDYxNjZNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5uVTdmTkpGMFhYRnF6R2JDNEhOWE9ySTFOWStzUERC&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 周志华.机器学习[M]. 北京: 清华大学出版社, 2016: 179-181. (ZHOU Z H. Machine Learning[M]. Beijing: Tsinghua University Press, 2016: 179-181.) 
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=UCI repository of machine learning databases">

                                <b>[19]</b> BLAKE C L, M C J. UCI repository of machine learning databases [EB/OL]. [2018- 05- 10]. http://mlearn.ics.uci.edu/MLRepository.html.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LIBSVM: a library for support vector machines">

                                <b>[20]</b> CHANG C C, LIN C J. LIBSVM: a library for support vector machines [EB/OL]. [2018- 05- 10]. http://www.csie.ntu.edu.tw/～cjlin/libsvm/.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Isolation-based anomaly detection">

                                <b>[21]</b> LIU F T, TING K M, ZHOU Z H. Isolation-based anomaly detection [EB/OL]. [2018- 05- 10]. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787111374312001&amp;v=MjExOTh4RVp1c1BEUk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bm5VN2ZOSkYwWFhGcXpHYks1SDlMTHE0&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> HAN J W, KAMBER M. 数据挖掘: 概念与技术[M]. 范明, 孟小峰, 译.3版.北京: 机械工业出版社, 2012: 236-240. (HAN J W, KAMBER M. Data Mining: Concepts and Techniques [M]. FAN M, MENG X F, translated. 3rd ed. Beijing: China Machine Press, 2012: 236-240.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904005" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904005&amp;v=MjkwMDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ekxMejdCZDdHNEg5ak1xNDlGWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
