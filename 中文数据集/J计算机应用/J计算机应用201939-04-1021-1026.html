<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775631377500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904015%26RESULT%3d1%26SIGN%3dcyJv7UknuH7iPnxugUH1EWPC1pg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904015&amp;v=MjU0MTZxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3dkFMejdCZDdHNEg5ak1xNDlFWVlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="1.1 &lt;b&gt;非负矩阵分解&lt;/b&gt;">1.1 <b>非负矩阵分解</b></a></li>
                                                <li><a href="#63" data-title="1.2 &lt;b&gt;概念分解&lt;/b&gt;">1.2 <b>概念分解</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="2 本文算法 ">2 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="2.1 &lt;b&gt;多核概念分解&lt;/b&gt;">2.1 <b>多核概念分解</b></a></li>
                                                <li><a href="#95" data-title="2.2 &lt;b&gt;多核概念分解模型求解算法&lt;/b&gt;">2.2 <b>多核概念分解模型求解算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#194" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#197" data-title="3.1 &lt;b&gt;数据集的选择&lt;/b&gt;">3.1 <b>数据集的选择</b></a></li>
                                                <li><a href="#209" data-title="3.2 &lt;b&gt;对比方法&lt;/b&gt;">3.2 <b>对比方法</b></a></li>
                                                <li><a href="#214" data-title="3.3 &lt;b&gt;评价指标&lt;/b&gt;">3.3 <b>评价指标</b></a></li>
                                                <li><a href="#227" data-title="3.4 &lt;b&gt;结果与分析&lt;/b&gt;">3.4 <b>结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#238" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#203" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;实验中使用的数据集&lt;/b&gt;"><b>表</b>1 <b>实验中使用的数据集</b></a></li>
                                                <li><a href="#232" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;各聚类算法的聚类准确性&lt;/b&gt; (&lt;i&gt;ACC&lt;/i&gt;) &lt;b&gt;对比&lt;/b&gt;"><b>表</b>2 <b>各聚类算法的聚类准确性</b> (<i>ACC</i>) <b>对比</b></a></li>
                                                <li><a href="#233" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各聚类算法的归一化互信息&lt;/b&gt; (NMI) &lt;b&gt;对比&lt;/b&gt;"><b>表</b>3 <b>各聚类算法的归一化互信息</b> (NMI) <b>对比</b></a></li>
                                                <li><a href="#234" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;各聚类算法的聚类纯度&lt;/b&gt; (purity) &lt;b&gt;对比&lt;/b&gt;"><b>表</b>4 <b>各聚类算法的聚类纯度</b> (purity) <b>对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="274">


                                    <a id="bibliography_1" title="HAN J, KAMBER M, PEI J.Data Mining:Concepts and Techniques[M].3rd ed.San Francisco:Margan Kaufmann, 2011:525-527." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data Mining:Concepts and Techniques">
                                        <b>[1]</b>
                                        HAN J, KAMBER M, PEI J.Data Mining:Concepts and Techniques[M].3rd ed.San Francisco:Margan Kaufmann, 2011:525-527.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_2" title="LEE D D, HSEBASTIAN S S.Learning the parts of objects by nonnegative matrix factorization[J].Nature, 1999, 401:788-791." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning the parts of objects by non-negative matrix factorization">
                                        <b>[2]</b>
                                        LEE D D, HSEBASTIAN S S.Learning the parts of objects by nonnegative matrix factorization[J].Nature, 1999, 401:788-791.
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_3" title="CESARIO E, MANCO G, ORTALE R.Top-down parameter-free clustering of high-dimensional categorical data[J].IEEE Transactions on Knowledge and Data Engineering, 2007, 19 (12) :1607-1624." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Top-down parameter-free clustering of high-dimensional categorical data">
                                        <b>[3]</b>
                                        CESARIO E, MANCO G, ORTALE R.Top-down parameter-free clustering of high-dimensional categorical data[J].IEEE Transactions on Knowledge and Data Engineering, 2007, 19 (12) :1607-1624.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_4" title="SHI J, MALIK J.Normalized cuts and image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (8) :888-905." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Normalized Cuts and Image Segmentation">
                                        <b>[4]</b>
                                        SHI J, MALIK J.Normalized cuts and image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (8) :888-905.
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_5" title="FREY B J, DUECK D.Clustering by passing messages between data points[J].Science, 2007, 315 (5814) :972-976." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering by passing messages between data points">
                                        <b>[5]</b>
                                        FREY B J, DUECK D.Clustering by passing messages between data points[J].Science, 2007, 315 (5814) :972-976.
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_6" title="CAI D, HE X, HAN J, et al.Graph regularized nonnegative matrix factorization for data representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (8) :1548-1560." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph Regularized Nonnegative Matrix Factorization for Data Representation">
                                        <b>[6]</b>
                                        CAI D, HE X, HAN J, et al.Graph regularized nonnegative matrix factorization for data representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (8) :1548-1560.
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_7" title="BISHOP C M.Pattern Recognition and Machine Learning[M].2nd ed.New York:Springer, 2010:291-292." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pattern Recognition and Machine Learning">
                                        <b>[7]</b>
                                        BISHOP C M.Pattern Recognition and Machine Learning[M].2nd ed.New York:Springer, 2010:291-292.
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_8" title="HOYER P O.Non-negative matrix factorization with sparseness constraints[EB/OL].[2018-05-10].https://arxiv.org/abs/cs/0408058." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-negative matrix factorization with sparseness constraints">
                                        <b>[8]</b>
                                        HOYER P O.Non-negative matrix factorization with sparseness constraints[EB/OL].[2018-05-10].https://arxiv.org/abs/cs/0408058.
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_9" title="DU L, LI X, SHEN Y.Robust nonnegative matrix factorization via half-quadratic minimization[C]//Proceedings of the 2012 IEEE12th International Conference on Data Mining.Piscataway, NJ:IEEE, 2012:201-210." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Nonnegative Matrix Factorization via Half-Quadratic Minimization.">
                                        <b>[9]</b>
                                        DU L, LI X, SHEN Y.Robust nonnegative matrix factorization via half-quadratic minimization[C]//Proceedings of the 2012 IEEE12th International Conference on Data Mining.Piscataway, NJ:IEEE, 2012:201-210.
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_10" title="XU W, GONG Y.Document clustering by concept factorization[C]//SIGIR 2004:Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2004:202-209." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Document clustering by concept factorization">
                                        <b>[10]</b>
                                        XU W, GONG Y.Document clustering by concept factorization[C]//SIGIR 2004:Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2004:202-209.
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_11" title="CAI D, HE X, HAN J.Locally consistent concept factorization for document clustering[J].IEEE Transactions on Knowledge and Data Engineering, 2011, 23 (6) :902-913." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locally consistent concept factorization for document clustering">
                                        <b>[11]</b>
                                        CAI D, HE X, HAN J.Locally consistent concept factorization for document clustering[J].IEEE Transactions on Knowledge and Data Engineering, 2011, 23 (6) :902-913.
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_12" title="PEI X, CHEN C, GONG W.Concept factorization with adaptive neighbors for document clustering[J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (2) :343-352." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Concept factorization with adaptive neighbors for document clustering">
                                        <b>[12]</b>
                                        PEI X, CHEN C, GONG W.Concept factorization with adaptive neighbors for document clustering[J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (2) :343-352.
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_13" title="LEE D D, SEUNG H S.Algorithms for non-negative matrix factorization[EB/OL].[2018-05-10].http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Algorithms for non-negative matrix factorization">
                                        <b>[13]</b>
                                        LEE D D, SEUNG H S.Algorithms for non-negative matrix factorization[EB/OL].[2018-05-10].http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf.
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_14" title="KUMAR A, RAI P, DAUMH.Co-regularized multi-view spectral clustering[EB/OL].[2018-05-10].http://www.cs.utah.edu/~piyush/recent/spectral-nips11.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Co-regularized multi-view spectral clustering">
                                        <b>[14]</b>
                                        KUMAR A, RAI P, DAUMH.Co-regularized multi-view spectral clustering[EB/OL].[2018-05-10].http://www.cs.utah.edu/~piyush/recent/spectral-nips11.pdf.
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_15" title="DU L, ZHOU P, SHI L, et al.Robust multiple kernel k-means clustering using L&lt;sub&gt;21&lt;/sub&gt;-norm[C]//Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:3476-3482." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust multiple kernel k-means clustering using L21-norm">
                                        <b>[15]</b>
                                        DU L, ZHOU P, SHI L, et al.Robust multiple kernel k-means clustering using L&lt;sub&gt;21&lt;/sub&gt;-norm[C]//Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:3476-3482.
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_16" title="LI X, SHEEN X, SHU Z, et al.Graph regularized multilayer concept factorization for data representation[J].Neurocomputing, 2017, 238:139-151." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES52F772448489C3374B68EA1C66051721&amp;v=MzE3MDFTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekwyN3hLQT1OaWZPZmJhNmFOYkxyWXRCYk84SEJRODZ6QkVYbURsMVBRN2ozeFF6ZWJlVlFyaWVDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        LI X, SHEEN X, SHU Z, et al.Graph regularized multilayer concept factorization for data representation[J].Neurocomputing, 2017, 238:139-151.
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_17" title="ZHAN K, SHI J, WANG J, et al.Adaptive structure concept factorization for multiview clustering[J].Neural Computation, 2018, 30 (2) :1080-1103." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK1EA247530A433772299969F10549447E&amp;v=MzA1NDROaWZKWmJMTmI5UElxSXBHWkpvTEQzOCt5QlFSNHpaMFRuYVVyUkl3ZmJ1UVFiM3FDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6TDI3eEtBPQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        ZHAN K, SHI J, WANG J, et al.Adaptive structure concept factorization for multiview clustering[J].Neural Computation, 2018, 30 (2) :1080-1103.
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_18" title="SHU Z, WU X, HUANG P, et al.Multiple graph regularized concept factorization with adaptive weights[J].IEEE Access, 2018, 6:64938-64945." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiple graph regularized concept factorization with adaptive weights">
                                        <b>[18]</b>
                                        SHU Z, WU X, HUANG P, et al.Multiple graph regularized concept factorization with adaptive weights[J].IEEE Access, 2018, 6:64938-64945.
                                    </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_19" title="MA S, ZHANG L, HU E, et al.Self-representative manifold concept factorization with adaptive neighbors for clustering[C]//IJCAI 2018:Proceedings of the 27th International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2018:2539-2545." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-representative manifold concept factorization with adaptive neighbors for clustering">
                                        <b>[19]</b>
                                        MA S, ZHANG L, HU E, et al.Self-representative manifold concept factorization with adaptive neighbors for clustering[C]//IJCAI 2018:Proceedings of the 27th International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2018:2539-2545.
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_20" title="KUMAR A, RAI P, DAUMH, Ⅲ.Co-regularized multi-view spectral clustering[C]//NIPS 2011:Proceedings of the 24th International Conference on Neural Information Processing Systems.New York:ACM, 2011:1413-1421." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Co-regularized multiview spectral clustering">
                                        <b>[20]</b>
                                        KUMAR A, RAI P, DAUMH, Ⅲ.Co-regularized multi-view spectral clustering[C]//NIPS 2011:Proceedings of the 24th International Conference on Neural Information Processing Systems.New York:ACM, 2011:1413-1421.
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_21" title="YAN W, ZHANG B, MA S, et al.A novel regularized concept factorization for document clustering[J].Knowledge-based Systems, 2017, 135:147-158." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC6D959A9BB6AB30B941BEEDF6B94FA53&amp;v=Mjc3NTVtYUJ1SFlmT0dRbGZCckxVMDV0cGh6TDI3eEtBPU5pZk9mY0MrYXRqSnB2NU1GcGtKZlE0NnoyUWE3ajRQUFFxVzJoUkhjTGJpTkwrY0NPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        YAN W, ZHANG B, MA S, et al.A novel regularized concept factorization for document clustering[J].Knowledge-based Systems, 2017, 135:147-158.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-17 16:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),1021-1026 DOI:10.11772/j.issn.1001-9081.2018081817            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于全局融合的多核概念分解算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%A3%9E&amp;code=08407512&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%9C%E4%BA%AE&amp;code=33459857&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杜亮</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%BB%E8%B6%85%E5%AE%8F&amp;code=41473326&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">任超宏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0176514&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学计算机与信息技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E4%BA%A7%E4%B8%9A%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学大数据科学与产业研究院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E4%B8%8E%E4%B8%AD%E6%96%87%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算智能与中文信息处理教育部重点实验室(山西大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>非负矩阵分解 (NMF) 算法仅能用于对原始非负数据寻找低秩近似, 而概念分解 (CF) 算法将矩阵分解模型扩展到单个非线性核空间, 提升了矩阵分解算法的学习能力和普适性。针对无监督环境下概念分解面临的如何设计或选择合适核函数这一问题, 提出基于全局融合的多核概念分解 (GMKCF) 算法。同时输入多种候选核函数, 在概念分解框架下基于全局线性权重融合对它们进行学习, 以得出质量高稳定性好的聚类结果, 并解决概念分解模型面临核函数选择的问题。采用交替迭代的方法对新模型进行求解, 证明了算法的收敛性。将该算法与基于核的<i>K</i>-均值 (KKM) 、谱聚类 (SC) 、KCF (Kernel Concept Factorization) 、Coreg (Co-regularized multi-view spectral clustering) 、RMKKM (Robust Multiple KKM) 在多个真实数据库上的实验结果表明, 该算法在数据聚类方面优于对比算法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多核学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A6%82%E5%BF%B5%E5%88%86%E8%A7%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">概念分解;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">矩阵分解;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%B8%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多核聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%B1%80%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全局融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李飞 (1993—) , 男, 湖北黄冈人, 硕士研究生, CCF会员, 主要研究方向:数据挖掘、机器学习;;
                                </span>
                                <span>
                                    *杜亮 (1985—) , 男, 山西晋中人, 讲师, 博士, CCF会员, 主要研究方向:数据挖掘、机器学习;电子邮箱im.duliang@qq.com;
                                </span>
                                <span>
                                    任超宏 (1994—) , 男, 山西朔州人, 硕士研究生, 主要研究方向:数据挖掘、机器学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61502289);</span>
                    </p>
            </div>
                    <h1><b>Multiple kernel concept factorization algorithm based on global fusion</b></h1>
                    <h2>
                    <span>LI Fei</span>
                    <span>DU Liang</span>
                    <span>REN Chaohong</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information Technology, Shanxi University</span>
                    <span>Institute of Big Data Science and Industry, Shanxi University</span>
                    <span>Key Laboratory of Computational Intelligence and Chinese Information Processing, Ministry of Education (Shanxi University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Non-negative Matrix Factorization (NMF) algorithm can only be used to find low rank approximation of original non-negative data while Concept Factorization (CF) algorithm extends matrix factorization to single non-linear kernel space, improving learning ability and adaptability of matrix factorization. In unsupervised environment, to design or select proper kernel function for specific dataset, a new algorithm called Globalized Multiple Kernel CF (GMKCF) was proposed. Multiple candidate kernel functions were input in the same time and learned in the CF framework based on global linear fusion, obtaining a clustering result with high quality and stability and solving the problem of kernel function selection that the CF faced. The convergence of the proposed algorithm was verified by solving the model with alternate iteration. The experimental results on several real databases show that the proposed algorithm outperforms comparison algorithms in data clustering, such as Kernel <i>K</i>-Means (KKM) , Spectral Clustering (SC) , Kernel CF (KCF) , Co-regularized multi-view spectral clustering (Coreg) , and Robust Multiple KKM (RMKKM) .</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multiple%20kernel%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multiple kernel learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Concept%20Factorization%20(CF)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Concept Factorization (CF) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=matrix%20factorization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">matrix factorization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multiple%20kernel%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multiple kernel clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=global%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">global fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Fei, born in 1993, M. S. candidate. His research interests include data mining, machine learning.;
                                </span>
                                <span>
                                    DU Liang, born in 1985, Ph. D. , lecturer. His research interests include data mining, machine learning.;
                                </span>
                                <span>
                                    REN Chaohong, born in 1994, M. S. candidate. His research interests include data mining, machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-13</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61502289);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">数据挖掘从看似无序的数据中寻找有序、有价值的信息。聚类分析是数据挖掘、机器学习中的一项重要技术, 也是国内外学者研究的一个重点领域。聚类技术可用来探索数据的内部结构, 并就其某种相关关系进行挖掘, 因而在很多领域中得到广泛应用, 例如:在电子商务中, 应用聚类算法可以发现不同客户群体, 有利于寻找潜在市场;在生物学领域, 可以对基因、蛋白质等进行聚类研究, 从而获取对其结构的深入认识;在互联网上, 可以对微博、新闻中的文档进行聚类研究, 从而进行热点事件发现等。</p>
                </div>
                <div class="p1">
                    <p id="49">根据聚类算法的输入数据类型分类, 聚类算法可以分为数值型算法 (如<i>K</i>-means<citation id="316" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、非负矩阵分解 (Non-negative Matrix Factorization, NMF) <citation id="317" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等) 、离散型 (如AT-DC<citation id="318" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>) 、关系型 (如NCut<citation id="319" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、仿射传播AP<citation id="320" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>) 和混合型 (如图正则的NMF (Graph regularized NMF, GNMF) <citation id="321" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>) 算法等。根据输出结果, 聚类算法分为层次聚类和划分式聚类<citation id="322" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。根据簇的描述形式, 聚类算法可分为基于原型的方法 (也叫簇代表元, 代表性算法有<i>K</i>-means, <i>K</i>-medoids等) 和基于模型的方法 (代表性算法有高斯混合模型 (Gaussian Mixture Model, GMM) <citation id="323" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>) 。</p>
                </div>
                <div class="p1">
                    <p id="50">近年来研究人员提出许多方法进一步提高非负矩阵分解算法的效果。文献<citation id="324" type="reference">[<a class="sup">6</a>]</citation>提出利用数据流形结构提升聚类结果;文献<citation id="325" type="reference">[<a class="sup">8</a>]</citation>研究矩阵分解的稀疏性提高结果的可解释性;文献<citation id="326" type="reference">[<a class="sup">9</a>]</citation>研究噪声数据上的矩阵分解提高分解结果的鲁棒性。文献<citation id="327" type="reference">[<a class="sup">10</a>]</citation>提出概念分解 (Concept Factorization, CF) 方法将矩阵分解从线性原始空间扩展到非线性核空间并用于文本聚类, 文献<citation id="328" type="reference">[<a class="sup">11</a>]</citation>提出基于图正则的概念分解算法, 文献<citation id="329" type="reference">[<a class="sup">12</a>]</citation>进一步提出自适应邻居正则化的概念分解算法, 文献<citation id="330" type="reference">[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</citation>提出基于多图/多层/多视图的正则化的概念分解算法, 文献<citation id="331" type="reference">[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</citation>提出新型单视图数据的正则化概念分解算法。值得指出的是这类正则化方法通常需要引入额外的参数用于平衡概念分解目标函数和正则目标, 但实际应用中如何设置较为准确的参数是比较困难的。</p>
                </div>
                <div class="p1">
                    <p id="51">文献<citation id="332" type="reference">[<a class="sup">10</a>]</citation>提出的核概念分解方法在实际应用中面临的核心问题之一是针对特定任务和数据集如何设计和选择合适的核函数。需要进一步指出的是, 由于缺乏数据标签等监督信息, 核函数选择在无监督学习任务中变得更加困难。</p>
                </div>
                <div class="p1">
                    <p id="52">为了减轻核函数选择带来的困难, 本文提出在无监督多核学习框架中通过全局线性加权方法从一系列初始给定的核矩阵中学习聚类质量更高、稳定性更好的核函数。针对本文提出的多核概念分解模型, 推导和设计了对应的迭代式优化求解算法——基于全局融合的多核概念分解 (Globalized Multiple Kernel CF, GMKCF) 算法, 并证明该算法的收敛性以及算法的时间和空间复杂度。本文提出的多核概念分解模型没有引入额外的超参数, 降低了算法在实际应用中实施部署的难度。</p>
                </div>
                <div class="p1">
                    <p id="53">多个基准数据集上的聚类实验结果表明多核聚类方法明显优于单核平均结果, 验证了多核学习可以提升聚类算法性能。本文提出的多核概念分解在聚类准确性、归一化互信息和聚类纯度上的性能优于对比多核聚类方法。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="55" name="55">1.1 <b>非负矩阵分解</b></h4>
                <div class="p1">
                    <p id="56">针对非负值矩阵数据<b><i>X</i></b>∈<b>R</b><sup><i>d</i>×<i>n</i></sup>, Lee等<citation id="333" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>于1999年在《Nature》上正式提出了非负矩阵分解的基本概念。非负矩阵分解的认知基础是:对整体的感知由基于对组成整体的部分 (局部) 。NMF通过非负约束纯加性的感知过程刻画出数据的组成部分和数据如何由局部感知构成的本质。该方法用两个低秩非负矩的乘积阵<b><i>UV</i></b><sup><i>T</i></sup>近似原始非负数据, 其中<b><i>U</i></b>∈<b>R</b><sup><i>d</i>×<i>k</i></sup>, <b><i>V</i></b>∈<b>R</b><sup><i>n</i>×<i>k</i></sup>。非负矩阵分解方法对应的最优结果可以通过求解以下优化问题<citation id="334" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>获得:</p>
                </div>
                <div class="p1">
                    <p id="57"><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">U</mi><mo>, </mo><mi mathvariant="bold-italic">V</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>; <b><i>U</i></b>≥0, <b><i>V</i></b>≥0      (1) </p>
                </div>
                <div class="p1">
                    <p id="59">从式 (1) 可看出:每一个样本<b><i>x</i></b><sub><i>i</i></sub>可以通过<b><i>U</i></b>, <b><i>V</i></b>的线性合并得到, 即<mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mspace width="0.25em" /></mstyle><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>k</mi></msub><mi>v</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></math></mathml>。因此, 矩阵<b><i>U</i></b>可以看作是一组非负基向量, 而矩阵<b><i>V</i></b>可以看作数据在基矩阵<b><i>U</i></b>下新的表示。</p>
                </div>
                <div class="p1">
                    <p id="61">上述优化问题是关于联合 (<b><i>U</i></b>, <b><i>V</i></b>) 的非凸优化问题, 因此很难用非线性优化方法得到全局最优解。然而对于仅关于<b><i>U</i></b>或者仅关于<b><i>V</i></b>的子问题, 仍然是一个凸优化问题。其局部最优解可以通过分块坐标轮换法分别求解。通用的非负矩阵分解求解算法通过以下乘法更新公式获得:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mfrac><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mfrac><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="63" name="63">1.2 <b>概念分解</b></h4>
                <div class="p1">
                    <p id="64">Xu等在文献<citation id="335" type="reference">[<a class="sup">10</a>]</citation>中提出概念分解算法。在概念分解模型中, 分解后的基向量<b><i>u</i></b>要求通过对原始空间样本的非负线性组合得到, 其对应的优化问题可以写成:</p>
                </div>
                <div class="p1">
                    <p id="65"><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">U</mi><mo>, </mo><mi mathvariant="bold-italic">V</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>; <b><i>U</i></b>≥0, <b><i>V</i></b>≥0</p>
                </div>
                <div class="p1">
                    <p id="67">文献<citation id="336" type="reference">[<a class="sup">2</a>]</citation>中提出的非负矩阵分解方法仅适用于原始特征。对于高度非线性分布的数据集, 可以利用核方法来提阵矩阵分解结果。核函数是从低维空间到高维空间的一种映射函数对于输入空间<b><i>x</i></b>∈<b>R</b><sup><i>d</i>×1</sup>, 函数<i>φ</i> (<b><i>x</i></b>) 将输入空间映射到希尔伯特特征空间<b><i>H</i></b>。对于<b><i>x</i></b>∈<b>R</b><sup><i>d</i>×1</sup>, <b><i>y</i></b>∈<b>R</b><sup><i>d</i>×1</sup>, 函数<i>k</i> (<b><i>x</i></b>, <b><i>y</i></b>) 称之为核函数的条件是满足以下条件:</p>
                </div>
                <div class="p1">
                    <p id="68"><i>k</i> (<b><i>x</i></b>, <b><i>y</i></b>) =<i>φ</i> (<b><i>x</i></b>) <sup>T</sup><i>φ</i> (<b><i>y</i></b>) </p>
                </div>
                <div class="p1">
                    <p id="69">映射函数<i>φ</i> (<b><i>x</i></b>) 将输入投影到高维空间后引发难以计算的问题, 核方法的关键之一是核技巧 (kernel trick) 。核技巧将高维空间的计算问题转化为低维度核函数计算问题。通常满足Mercer定理<citation id="337" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 即定义任何半正定的函数都可以作为核函数。常用的核函数有:线性核、多项式核、高斯核等, 分别定义如下:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">y</mi></mtd></mtr><mtr><mtd><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mi>d</mi></msup></mtd></mtr><mtr><mtd><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mo>-</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>δ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">) </mo><msup><mrow></mrow><mi>d</mi></msup></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">为此, 文献<citation id="338" type="reference">[<a class="sup">10</a>]</citation>中进一步将概念分解模型扩展到核空间, 核概念分解对应的优化问题变为:</p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">U</mi><mo>, </mo><mi mathvariant="bold-italic">V</mi></mrow></munder><mspace width="0.25em" /><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Κ</mi><mo stretchy="false">) </mo><mo>-</mo><mn>2</mn><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><mo>+</mo><mtext>t</mtext><mtext>r</mtext></mrow></math></mathml> (<b><i>U</i></b><sup>T</sup><b><i>KUV</i></b><sup>T</sup><b><i>V</i></b>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="74">s.t. <b><i>U</i></b>≥0, <b><i>V</i></b>≥0</p>
                </div>
                <div class="p1">
                    <p id="75">对于非负核矩阵, 上述优化问题的最优解可以通过下面的乘法更新公式获得:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mfrac><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mfrac><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="77" name="77" class="anchor-tag">2 本文算法</h3>
                <div class="p1">
                    <p id="78">上述提到的核概念分解算法仅适用于单核数据聚类问题;然而, 核方法面临的核心问题之一是针对特定任务和数据集如何设计和选择合适的核函数。需要进一步指出的是, 由于缺乏数据标签等监督信息, 核函数选择在无监督学习任务中变得更加困难。</p>
                </div>
                <div class="p1">
                    <p id="79">为了减轻核函数选择带来的困难, 本文提出在无监督多核学习框架中通过全局线性加权方法从一系列初始给定的核矩阵中学习聚类质量更高、稳定性更好的核函数。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80">2.1 <b>多核概念分解</b></h4>
                <div class="p1">
                    <p id="81">假设一共给定<i>m</i>个不同的核关系数据用于聚类过程{<b><i>K</i></b><sup><i>i</i></sup>}<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml>, 与此对应的是<i>m</i>个不同的特征空间{<b><i>H</i></b><sup><i>i</i></sup>}<mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml>。为了合并这些核空间并且使得合并后的核矩阵仍然满足Mercer条件, 可以采用基于非负全局权重线性加权的方式, 即合并后的特征空间可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="84"><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>φ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>φ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></math></mathml>; <i>w</i><sub><i>i</i></sub>≥0      (5) </p>
                </div>
                <div class="p1">
                    <p id="86">然而需要指出的是上述方法并不可行, 原因在于不同的特征空间<i>φ</i><sub><i>i</i></sub>对应的维度并不相同。因此, 本文通过将不同的核函数对应的特征空间加权拼接起来得到一个扩张的高维映射<i>φ</i><sub><b><i>w</i></b></sub> (<b><i>x</i></b>) =[<i>w</i><sub>1</sub><i>φ</i><sub>1</sub> (<b><i>x</i></b>) ;<i>w</i><sub>2</sub><i>φ</i><sub>2</sub> (<b><i>x</i></b>) ;…;<i>w</i><sub><i>m</i></sub><i>φ</i><sub><i>m</i></sub> (<b><i>x</i></b>) ], 进而构造一个增广希尔伯特空间<b><i>H</i></b><sub><b><i>w</i></b></sub>。</p>
                </div>
                <div class="p1">
                    <p id="87">现有研究工作表明半正定核矩阵{<b><i>K</i></b><sup><i>i</i></sup>}<mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml>的非负凸组合</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi mathvariant="bold-italic">Κ</mi></mstyle><msup><mrow></mrow><mi>i</mi></msup></mrow></math></mathml>; <i>w</i><sub><i>i</i></sub>≥0      (6) </p>
                </div>
                <div class="p1">
                    <p id="91">仍然是一个半正定核矩阵。通过替换核概念分解中的核矩阵为新的多核矩阵, 就可以得到GMKCF算法, 对应的优化问题可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="92">min tr (<b><i>K</i></b><sub><b><i>w</i></b></sub>) -2tr (<b><i>V</i></b><sup>T</sup><b><i>K</i></b><sub><b><i>w</i></b></sub><b><i>U</i></b>) +tr (<b><i>U</i></b><sup>T</sup><b><i>K</i></b><sub><b><i>w</i></b></sub><b><i>UV</i></b><sup>T</sup><b><i>V</i></b>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="93"><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mi mathvariant="bold-italic">U</mi><mo>≥</mo><mn>0</mn><mo>, </mo><mi mathvariant="bold-italic">V</mi><mo>≥</mo><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></math></mathml>; <i>w</i><sub><i>i</i></sub>≥0</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">2.2 <b>多核概念分解模型求解算法</b></h4>
                <div class="p1">
                    <p id="96">首先需要指出的是, 上述多核概念分解模型整体对于所有待求变量仍然是一个非凸优化问题, 但是对于单个变量的各子优化问题都是凸优化问题。为此, 本文提出迭代式求解算法对整体问题进行求解, 并采用分块坐标轮换法分别对每个变量对应的子优化问题进行单独求解。最终通过求解一系列子优化问题, 可以获得对应的局部最优解。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97">2.2.1 固定<b><i>V</i></b>、<b><i>w</i></b>, 求解<b><i>U</i></b></h4>
                <div class="p1">
                    <p id="98">多核概念分解关于<b><i>U</i></b>的子优化问题变成:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">U</mi></munder><mspace width="0.25em" /><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><mo>-</mo><mn>2</mn><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mi mathvariant="bold-italic">U</mi><mo>≥</mo><mn>0</mn></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">容易看出, 上述优化问题等价于采用<b><i>K</i></b><sub><b><i>w</i></b></sub>的单核概念分解模型对应的子优化问题。</p>
                </div>
                <div class="p1">
                    <p id="101">因此, 上述问题的最优解同样可以通过下述乘法更新公式获得:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mfrac><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">对于<b><i>K</i></b><sub><b><i>w</i></b></sub>中有负数的情况, 对应的更新公式变为:</p>
                </div>
                <div class="area_img" id="267">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904015_26700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="268">P<sup>+</sup><sub>w</sub>、P<sup>-</sup><sub>w</sub>的计算公式为:</p>
                </div>
                <div class="area_img" id="269">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904015_26900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="106" name="106">2.2.2 固定<b><i>U</i></b>、<b><i>w</i></b>, 求解<b><i>V</i></b></h4>
                <div class="p1">
                    <p id="107">多核概念分解关于<b><i>V</i></b>的子优化问题变成:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">V</mi></munder><mspace width="0.25em" /><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><mo>-</mo><mn>2</mn><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mi mathvariant="bold-italic">V</mi><mo>≥</mo><mn>0</mn></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">容易看出, 上述优化问题等价于采用<b><i>K</i></b><sub><b><i>w</i></b></sub>的单核概念分解模型对应的子优化问题。</p>
                </div>
                <div class="p1">
                    <p id="110">因此, 上述问题的最优解同样可以通过下述乘法更新公式获得:</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mfrac><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">对于<b><i>K</i></b><sub><b><i>w</i></b></sub>中有负数的情况, 对应的更新公式变为:</p>
                </div>
                <div class="area_img" id="270">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904015_27000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="271">Q<sup>+</sup><sub>w</sub>、Q<sup>-</sup><sub>w</sub>的计算公式为:</p>
                </div>
                <div class="area_img" id="272">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904015_27200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="115" name="115">2.2.3 固定<b><i>U</i></b>、<b><i>V</i></b>, 求解<b><i>w</i></b></h4>
                <div class="p1">
                    <p id="273">首先定义e∈R<sup>m×1</sup>, 且令</p>
                </div>
                <div class="p1">
                    <p id="116"><i>e</i><sub><i>i</i></sub>=tr (<b><i>K</i></b><sup><i>i</i></sup>) -2tr (<b><i>V</i></b><sup>T</sup><b><i>K</i></b><sup><i>i</i></sup><b><i>U</i></b>) +tr (<b><i>U</i></b><sup>T</sup><b><i>K</i></b><sup><i>i</i></sup><b><i>UV</i></b><sup>T</sup><b><i>V</i></b>) </p>
                </div>
                <div class="p1">
                    <p id="117">多核概念分解关于<b><i>w</i></b>的子优化问题变成:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mn>2</mn></msup><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>;</mo><mspace width="0.25em" /><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≥</mo><mn>0</mn></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">上述问题关于变量<b><i>w</i></b>的拉格朗日扩展可以写为:</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mn>2</mn></msup><mspace width="0.25em" /><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>λ</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="121">该目标最优解对应的KKT条件<mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">w</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>, 且需要满足等式约束<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></math></mathml>。通过进一步求解, 可以获得关于变量<b><i>w</i></b>的闭式解:</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mfrac><mn>1</mn><mrow><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mspace width="0.25em" /></mstyle><mfrac><mn>1</mn><mrow><mi>e</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="125" name="125">2.2.4 多核概念算法</h4>
                <div class="p1">
                    <p id="126">算法1 全局多核概念分解算法。</p>
                </div>
                <div class="p1">
                    <p id="127">输入 样本-特征关系矩阵<b><i>X</i></b>∈<b>R</b><sup><i>d</i>×<i>n</i></sup>, 概念因子个数<i>k</i>。</p>
                </div>
                <div class="p1">
                    <p id="128">输出 多核全局意义下样本的低秩表示<b><i>V</i></b>∈<b>R</b><sup><i>n</i>×<i>k</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="129">1) 采用多个核函数 (<i>m</i>) 分别得到多个对应的核矩阵{<b><i>K</i></b><sup><i>i</i></sup>}<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml>;</p>
                </div>
                <div class="p1">
                    <p id="132">2) 初始化非负因子<b><i>U</i></b>和<b><i>V</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="134">3) 初始化核函数权重因子<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>;</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="137">4) 计算初始目标函数<i>obj</i><sub>old</sub>;</p>
                </div>
                <div class="p1">
                    <p id="139">5) While not converge do</p>
                </div>
                <div class="p1">
                    <p id="141">6) 根据式 (6) 计算<b><i>K</i></b><sub><b><i>w</i></b></sub>;</p>
                </div>
                <div class="p1">
                    <p id="143">7) 根据式 (9) 或式 (10) 计算<b><i>U</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="145">8) 根据式 (13) 或式 (14) 计算<b><i>V</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="147">9) 根据式 (17) 计算<b><i>w</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="149">10) 计算当前目标函数<i>obj</i><sub>new</sub>;</p>
                </div>
                <div class="p1">
                    <p id="151">11) 如果<mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>o</mi><mi>b</mi><mi>j</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>l</mtext><mtext>d</mtext></mrow></msub><mo>-</mo><mi>o</mi><mi>b</mi><mi>j</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub></mrow><mrow><mi>o</mi><mi>b</mi><mi>j</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub></mrow></mfrac><mo>≤</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>5</mn></mrow></msup></mrow></math></mathml>, 判定收敛, 终止迭代;</p>
                </div>
                <div class="p1">
                    <p id="154">12) End While</p>
                </div>
                <div class="p1">
                    <p id="156">后处理:利用<i>K</i>-means算法对多核低秩表示<b><i>V</i></b>进行二次聚类获得高质量的离散化聚类结果。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157">2.2.5 算法收敛性证明</h4>
                <div class="p1">
                    <p id="158">式 (7) 中的全局多核概念分解算法是一个关于联合 ({<b><i>U</i></b><sup><i>i</i></sup>}<mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml>, <b><i>V</i></b>, <b><i>w</i></b>) 的非凸优化问题, 因此很难用非线性优化方法得到全局最优解。然而对于仅关于{<b><i>U</i></b><sup><i>i</i></sup>}<mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml>或者仅关于<b><i>V</i></b> 或者仅关于<b><i>w</i></b>的子问题, 仍然是一个凸优化问题。通过分块坐标轮换法的迭代式求解可以使整体目标函数单调下降。并且可以很明显看出式 (7) 的目标函数是有下界的。因此, 整体求解算法的收敛性可以得到保障。</p>
                </div>
                <div class="p1">
                    <p id="161">具体来讲, 容易看出式 (7) 的目标函数是有下界的 (下界为0) , 并且式 (7) 的函数值随着算法迭代每一步都是非增的 (Non-increasing) 。本文引入非负矩阵分解 (NMF) 和概念分解 (CF) 模型乘法更新过程 (Multiplicative update rule) 中常见的辅助函数 (Auxiliary function) 定义<citation id="339" type="reference"><link href="292" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。因为非负因子<b><i>U</i></b>的更新过程和非负因子<b><i>V</i></b>更新类似, 本文仅给出求解非负因子<b><i>V</i></b>时的辅助函数证明。</p>
                </div>
                <div class="p1">
                    <p id="162"><b>定义</b>1 满足以下条件的函数<i>G</i> (<i>v</i>, <i>v</i>′) 是函数<i>F</i> (<i>v</i>) 的辅助函数:<i>G</i> (<i>v</i>, <i>v</i>′) ≥<i>F</i> (<i>v</i>) , <i>G</i> (<i>v</i>, <i>v</i>) =<i>F</i> (<i>v</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="163"><b>引理</b>1 如果函数<i>G</i>是函数<i>F</i>的辅助函数, 用<mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>v</mi><msup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo></mrow><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>v</mi></munder><mspace width="0.25em" /><mi>G</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>v</mi><msup><mrow></mrow><mi>t</mi></msup><mo stretchy="false">) </mo></mrow></mrow></math></mathml>更新公式进行更新时函数<i>F</i>是非增的。</p>
                </div>
                <div class="p1">
                    <p id="165">证明 <i>F</i> (<i>v</i><sup><i>t</i>+1</sup>) ≤<i>G</i> (<i>v</i><sup><i>t</i>+1</sup>, <i>v</i><sup><i>t</i></sup>) ≤<i>G</i> (<i>v</i><sup><i>t</i></sup>, <i>v</i><sup><i>t</i></sup>) =<i>F</i> (<i>v</i><sup><i>t</i></sup>) 。</p>
                </div>
                <div class="p1">
                    <p id="166">式 (12) 是关于整个非负因子<b><i>V</i></b>的优化问题。简单起见, 本文对元素<i>v</i><sub><i>ab</i></sub>对应的优化问题进行分析。令与元素<i>v</i><sub><i>ab</i></sub>相关的目标函数为<i>F</i><sub><i>ab</i></sub>, 则其导数和二阶导数如下:</p>
                </div>
                <div class="p1">
                    <p id="167"><i>F</i>′<sub><i>ab</i></sub>= (-2<b><i>K</i></b><sub><b><i>w</i></b></sub><b><i>U</i></b>+2<b><i>VU</i></b><sup>T</sup><b><i>K</i></b><sub><b><i>w</i></b></sub><b><i>U</i></b>) <sub><i>ab</i></sub></p>
                </div>
                <div class="p1">
                    <p id="168"><i>F</i>"<sub><i>ab</i></sub>=2 (<b><i>U</i></b><sup>T</sup><b><i>K</i></b><sub><b><i>w</i></b></sub><b><i>U</i></b>) <sub><i>ab</i></sub></p>
                </div>
                <div class="p1">
                    <p id="169"><b>引理</b>2 函数<i>F</i><sub><i>ab</i></sub> (<i>v</i>) 的辅助如下:</p>
                </div>
                <div class="p1">
                    <p id="170" class="code-formula">
                        <mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo><mo>+</mo><msup><mi>F</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>v</mi><mo>-</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">w</mi><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub></mrow><mrow><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup></mrow></mfrac><mo stretchy="false"> (</mo><mi>v</mi><mo>-</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="171">证明 显然<i>G</i> (<i>v</i>, <i>v</i>) =<i>F</i><sub><i>ab</i></sub> (<i>v</i>) 。<i>F</i><sub><i>ab</i></sub> (<i>v</i>) 的二阶泰勒展开如下:</p>
                </div>
                <div class="p1">
                    <p id="172"><i>F</i><sub><i>ab</i></sub> (<i>v</i>) =<i>F</i><sub><i>ab</i></sub> (<i>v</i><mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>) +<i>F</i>′<sub><i>ab</i></sub> (<i>v</i><mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>) (<i>v</i>-<i>v</i><mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>) + (<b><i>VUKwU</i></b>) <sub><i>ab</i></sub> (<i>v</i>-<i>v</i><mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>) <sup>2</sup></p>
                </div>
                <div class="p1">
                    <p id="178">此外, <mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">w</mi><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>v</mi></mstyle><msubsup><mrow></mrow><mrow><mi>a</mi><mi>l</mi></mrow><mi>t</mi></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">w</mi><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>l</mi><mi>b</mi></mrow></msub><mo>≥</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">w</mi><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub></mrow></math></mathml>。因此<i>G</i> (<i>v</i>, <i>v</i><mathml id="180"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>) ≥<i>F</i><sub><i>ab</i></sub> (<i>v</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="181">辅助函数<i>G</i> (<i>v</i>, <i>v</i><mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>) 是简单的一元二次函数, 将其引入定义1并求解<mathml id="183"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>v</mi></munder><mspace width="0.25em" /><mi>G</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo></mrow></math></mathml>可以得到:</p>
                </div>
                <div class="p1">
                    <p id="184" class="code-formula">
                        <mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo>-</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mfrac><mrow><msup><mi>F</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo></mrow><mrow><mn>2</mn><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub></mrow></mfrac><mo>=</mo><mi>v</mi><msubsup><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow><mi>t</mi></msubsup><mfrac><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi mathvariant="bold-italic">U</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>a</mi><mi>b</mi></mrow></msub></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="185">更新式 (13) 得证。</p>
                </div>
                <div class="p1">
                    <p id="186">此外, 式 (16) 中关于<b><i>w</i></b>的问题是凸优化问题, 式 (17) 可以获得最优解。</p>
                </div>
                <h4 class="anchor-tag" id="187" name="187">2.2.6 算法复杂性说明</h4>
                <div class="p1">
                    <p id="188">初始阶段, 本文算法需要计算<i>m</i>个核矩阵, 对应的计算复杂性是<i>O</i> (<i>mn</i><sup>2</sup><i>d</i>) , 其中<i>n</i>是样本个数, <i>d</i>是特征个数。每次迭代过程中的计算量分别为:</p>
                </div>
                <div class="p1">
                    <p id="189">1) 更新变量<b><i>U</i></b>, 其中需要计算<i>P</i><sup>+</sup>和<i>P</i><sup>-</sup>, 对应的计算复杂性为<i>O</i> (<i>n</i><sup>2</sup><i>k</i>+<i>k</i><sup>2</sup><i>n</i>) , 更新<b><i>U</i></b>的复杂性是<i>O</i> (<i>n</i><sup>2</sup><i>k</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="190">2) 更新变量<b><i>V</i></b>, 其中需要计算<i>Q</i><sup>+</sup>和<i>Q</i><sup>-</sup>, 对应的计算复杂性为<i>O</i> (<i>n</i><sup>2</sup><i>k</i>+<i>k</i><sup>2</sup><i>n</i>) , 更新<b><i>V</i></b>的复杂性是<i>O</i> (<i>n</i><sup>2</sup><i>k</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="191">3) 更新变量<b><i>w</i></b>, 对应的计算复杂性为<i>O</i> (<i>m</i> (<i>n</i><sup>2</sup><i>k</i>+<i>k</i><sup>2</sup><i>n</i>) ) 。</p>
                </div>
                <div class="p1">
                    <p id="192">4) 更新变量<b><i>K</i></b><sub><b><i>w</i></b></sub>, 对应的计算复杂性为<i>O</i> (<i>mn</i><sup>2</sup>) 。</p>
                </div>
                <div class="p1">
                    <p id="193">假设迭代算法在迭代<i>t</i>次后收敛, 多核概念分解的整体复杂度表示为<i>O</i> (<i>mn</i><sup>2</sup><i>d</i>+<i>n</i><sup>2</sup><i>t</i> (<i>k</i>+<i>m</i>) ) 。可以看出, 多核概念分解整体算法复杂性和单核概念分解在同一量级。</p>
                </div>
                <h3 id="194" name="194" class="anchor-tag">3 实验与结果分析</h3>
                <div class="p1">
                    <p id="195">本文实验通过基准测试数据集上的聚类结果对比来验证本文提出的多核方法在聚类问题上的有效性。</p>
                </div>
                <div class="p1">
                    <p id="196">实验平台的配置:<i>PC</i>为<i>Intel Core i</i>5处理器, 8 <i>GB</i>内存, 120 <i>GB</i>硬盘;操作系统为<i>Windows</i> 10;编程环境为<i>Matlab</i> 2015<i>a</i>。</p>
                </div>
                <h4 class="anchor-tag" id="197" name="197">3.1 <b>数据集的选择</b></h4>
                <div class="p1">
                    <p id="198">本文分别选择了<i>BBC</i>、<i>TR</i>31、<i>K</i>1<i>B</i>、<i>WebKB</i>四个数据集作为测试基准数据集。这些数据集经常被用于评估聚类算法的性能, 数据集的统计信息如表1所示。</p>
                </div>
                <div class="p1">
                    <p id="199"><i>BBC</i>数据集包含了来自<i>BBC</i>新闻网站提供的2 225份文件, 对应于2004—2005年5个主题领域的故事, 共有5类标签:商业、娱乐、政治、体育、科技。</p>
                </div>
                <div class="p1">
                    <p id="200"><i>TR</i>31数据集来自<i>TREC</i>收集的文本数据集, 包含927个文本, 分为7个类别。</p>
                </div>
                <div class="p1">
                    <p id="201"><i>K</i>1<i>B</i>数据集来自<i>WebACE</i>项目, 包括2 340篇文章, 这些文章来自于路透新闻的20个类别中, 其中每个文档对应于<i>Yahoo</i>!的主题层次结构中列出的网页。</p>
                </div>
                <div class="p1">
                    <p id="202"><i>WebKB</i>数据集包含了约6 000个从4所高校 (康奈尔大学、德克萨斯大学、华盛顿大学、威斯康星大学) 的计算机科学部门收集的网页。每个网页都标有一个标签:学生、教授、课程、项目、人员、部门, 以及其他。</p>
                </div>
                <div class="area_img" id="203">
                    <p class="img_tit"><b>表</b>1 <b>实验中使用的数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Datasets used in the experiment</i></p>
                    <p class="img_note"></p>
                    <table id="203" border="1"><tr><td><br />数据集名</td><td>类数</td><td>实例数</td><td>维数</td></tr><tr><td><br /><i>BBC</i></td><td>5</td><td>737</td><td>1 000</td></tr><tr><td><br /><i>TR</i>31</td><td>7</td><td>927</td><td>10 128</td></tr><tr><td><br /><i>K</i>1<i>B</i></td><td>6</td><td>2 340</td><td>21 839</td></tr><tr><td><br /><i>WebKB</i></td><td>4</td><td>4 199</td><td>1 000</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="204">和其他多核学习方法中的策略类似, 本文使用了12种不同的核函数作为多核学习的输入。这些核函数包括7个不同带宽的径向基函数 (Radial Basis Function, RBF) 核函数<mathml id="205"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mo>-</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>δ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">) </mo><msup><mrow></mrow><mi>d</mi></msup></mrow></math></mathml>, 其中令<i>δ</i>=<i>tD</i><sub>0</sub>, 且<i>D</i><sub>0</sub>是样本两两之间距离的平均值, 而<i>t</i>的变化范围包括{0.01, 0.05, 0.1, 1, 10, 50, 100};4个多项式核函数<i>k</i> (<b><i>x</i></b>, <b><i>y</i></b>) = (<i>a</i>+<b><i>x</i></b><mathml id="206"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><b><i>y</i></b>) <sup><i>b</i></sup>, 其中<i>a</i>的取值范围包括{0, 1}, <i>b</i>的取值范围包括{2, 4};1个余弦核函数<mathml id="207"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">y</mi></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">∥</mo><mo>⋅</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo></mrow></mfrac></mrow></math></mathml>。最后, 所有的核函数都又经过了标准化<mathml id="208"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo></mrow><mrow><msqrt><mrow><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo></mrow></msqrt></mrow></mfrac></mrow></math></mathml>, 并且被进一步缩放到区间[0, 1]内。</p>
                </div>
                <h4 class="anchor-tag" id="209" name="209">3.2 <b>对比方法</b></h4>
                <div class="p1">
                    <p id="210">本文实验是多核数据聚类实验, 实验中对比了单核方法和多核方法。采用的单核方法包括:基于核的K-均值 (<i>Kernel</i> K-<i>Means</i>, <i>KKM</i>) 、谱聚类 (<i>Spectual Clustering</i>, <i>SC</i>) 、<i>KCF</i> (<i>Kernel CF</i>) 。采用的多核方法包括:<i>Coreg</i> (<i>Co</i>-<i>regularized multi</i>-<i>view spectral clustering</i>) <citation id="340" type="reference"><link href="312" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、<i>RMKKM</i> (<i>Robust Multiple KKM</i>) <citation id="341" type="reference"><link href="314" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 以及本文<i>GMKCF</i>算法。</p>
                </div>
                <div class="p1">
                    <p id="211">针对多核实验数据, 单核方法可以获得多组实验结果, 为了准确刻画单核方法在不同核函数上的性能, 本文实验采用单核方法在多个核函数上聚类结果的平均值。</p>
                </div>
                <div class="p1">
                    <p id="212">根据文献<citation id="342" type="reference">[<a class="sup">20</a>,<a class="sup">21</a>]</citation>中的实验结果, <i>Coreg</i>在本文实验中的参数设置为0.1, <i>RMKKM</i>的实验参数设置为0.3。概念因子的个数设置为数据集中类的个数。</p>
                </div>
                <div class="p1">
                    <p id="213">聚类中簇的个数设置为数据集中真实类别的个数。<i>SC</i>和<i>Coreg</i>获得样本低维表示后都采用K-<i>means</i>算法最终得到离散化的聚类结果。针对聚类算法需要初始化的问题, 本文实验采用随机值对算法进行初始化, 重复实验20次并报告对应的平均值。</p>
                </div>
                <h4 class="anchor-tag" id="214" name="214">3.3 <b>评价指标</b></h4>
                <div class="p1">
                    <p id="215">因本文实验所采用的数据集类别标签已知, 本文选择三个外部评价指标来评估算法在聚类问题上的性能, 各评价指标介绍如下:</p>
                </div>
                <div class="p1">
                    <p id="216">1) 聚类准确性 (Clustering Accuracy, ACC) 。聚类准确性是基于类和簇的一一对应关系来评价聚类性能, 对于样本<b><i>x</i></b><sub><i>i</i></sub>, <i>p</i><sub><i>i</i></sub>和<i>q</i><sub><i>i</i></sub>分别为聚类结果和真实标签。<i>ACC</i>可以定义为:</p>
                </div>
                <div class="p1">
                    <p id="217" class="code-formula">
                        <mathml id="217"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>C</mi><mi>C</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>δ</mi></mstyle><mo stretchy="false"> (</mo><mi>q</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>m</mi><mi>a</mi><mi>p</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="218">其中:<i>n</i>是样本总数。如果<b><i>x</i></b>=<b><i>y</i></b>, 则<i>δ</i> (<b><i>x</i></b>, <b><i>y</i></b>) =1;如果<b><i>x</i></b>≠<b><i>y</i></b>, 则<i>δ</i> (<b><i>x</i></b>, <b><i>y</i></b>) =0。而<i>map</i> (·) 是置换映射函数, 它将簇标签映射到类标签。最佳映射可以通过Kuhn-Munkres算法获取。<i>ACC</i>是0～1的值, <i>ACC</i>的值越大说明聚类效果越好。</p>
                </div>
                <div class="p1">
                    <p id="219">2) 归一化互信息 (Normalized Mutual Information, NMI) 。<i>NMI</i>是一种外部评价标准, 它用来评价算法在一个数据集上的聚类结果与该数据集真实划分的相似程度。用<i>C</i>表示真实标签中类的集合, 用<i>C</i>′表示聚类算法获得的簇的集合。它们的互信息定义为:</p>
                </div>
                <div class="p1">
                    <p id="220" class="code-formula">
                        <mathml id="220"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>Ι</mi><mo stretchy="false"> (</mo><mi>C</mi><mo>, </mo><msup><mi>C</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>C</mi><mo>, </mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><msup><mi>C</mi><mo>′</mo></msup></mrow></munder><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>p</mi><mo stretchy="false"> (</mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="221">其中: <i>p</i> (<i>c</i><sub><i>i</i></sub>) 和<i>p</i> (<i>c</i>′<sub><i>j</i></sub>) 分别是样本属于类<i>c</i><sub><i>i</i></sub>和簇<i>c</i>′<sub><i>j</i></sub>的概率; <i>p</i> (<i>c</i><sub><i>i</i></sub>, <i>c</i>′<sub><i>j</i></sub>) 是样本同时属于类<i>c</i><sub><i>i</i></sub>和簇<i>c</i>′<sub><i>j</i></sub>的概率。实验中使用下面的归一化互信息:</p>
                </div>
                <div class="p1">
                    <p id="222" class="code-formula">
                        <mathml id="222"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><mi>Μ</mi><mi>Ι</mi><mo stretchy="false"> (</mo><mi>C</mi><mo>, </mo><msup><mi>C</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>Μ</mi><mi>Ι</mi><mo stretchy="false"> (</mo><mi>C</mi><mo>, </mo><msup><mi>C</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow><mrow><mi>max</mi><mo stretchy="false"> (</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>C</mi><mo stretchy="false">) </mo><mo>, </mo><mi>Η</mi><mo stretchy="false"> (</mo><msup><mi>C</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="223">其中:<i>H</i> (<i>C</i>) 和<i>H</i> (<i>C</i>′) 分别是类<i>C</i>和簇<i>C</i>′对应的信息熵。容易验证<i>NMI</i>位于0～1, 并且<i>NMI</i>的值越大说明聚类效果越好。</p>
                </div>
                <div class="p1">
                    <p id="224">3) 聚类纯度 (<i>Purity</i>) 是一种简单的聚类评价方法, 只需计算正确聚类的样本数占样本总数的比例, 其计算方法如下:</p>
                </div>
                <div class="p1">
                    <p id="225" class="code-formula">
                        <mathml id="225"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mi>u</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mrow><mi>max</mi></mrow></mstyle><mo stretchy="false"> (</mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="226">其中:用<i>C</i>={<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>k</i></sub>}表示真实标签中类的集合;用<i>C</i>′={<i>c</i>′<sub>1</sub>, <i>c</i>′<sub>2</sub>, …, <i>c</i>′<sub><i>k</i></sub>}表示聚类算法获得的簇的集合。<i>Purity</i>同样位于0～1, 并且<i>Purity</i>的值越大说明聚类效果越好。</p>
                </div>
                <h4 class="anchor-tag" id="227" name="227">3.4 <b>结果与分析</b></h4>
                <div class="p1">
                    <p id="228">表2～4分别列出了不同的聚类方法在这些数据集上聚类准确性、归一化互信息和聚类纯度的结果。</p>
                </div>
                <div class="p1">
                    <p id="229">实验结果表明多核方法 (<i>Coreg</i>、<i>RMKKM</i>和<i>GMKCF</i>) 普遍优于单核方法 (<i>KKM</i>、<i>SC</i>和<i>KCF</i>) 。从表2聚类准确性指标可看出多核方法在多个数据集上的平均结果达到0.580 9, 而单核方法的平均结果为0.491 5, 多核方法在聚类准确性上的平均提升达到了18.2%;从表3归一化互信息指标可看出多核方法在多个数据集上的平均结果达到0.374 1, 而单核方法的平均结果为0.246 3, 多核方法在归一化互信息上的平均提升达到了51.8%;从表4聚类纯度指标可看出多核方法在多个数据集上的平均结果达到0.659 9, 而单核方法的平均结果为0.576 6, 多核方法在归一化互信息上的平均提升达到了14.4%。</p>
                </div>
                <div class="p1">
                    <p id="230">实验结果表明本文提出的多核概念分解方法要优于其他单核方法和多核方法。三种不同指标上<i>GMKCF</i>在多个数据集上的平均结果明显高于其他方法。</p>
                </div>
                <div class="p1">
                    <p id="231">具体来看, <i>GMKCF</i>在聚类准确性上达到0.614 5, 而第二名的算法<i>Coreg</i>为0.566 4, 性能提升为8.5%。<i>GMKCF</i>在归一化互信息上达到0.434 4, 第二名为0.403 2, 性能提升为7.7%;<i>GMKCF</i>在聚类纯度上达到0.698 2, 第二名为0.675 6, 性能提升为3.3%。</p>
                </div>
                <div class="area_img" id="232">
                    <p class="img_tit"><b>表</b>2 <b>各聚类算法的聚类准确性</b> (<i>ACC</i>) <b>对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Comparison of ACC among different algorithms</i></p>
                    <p class="img_note"></p>
                    <table id="232" border="1"><tr><td><br />数据集</td><td><i>KKM</i></td><td><i>SC</i></td><td><i>KCF</i></td><td><i>Coreg</i></td><td><i>RMKKM</i></td><td><i>GMKCF</i></td></tr><tr><td><br /><i>BBC</i></td><td>0.456 7</td><td>0.406 2</td><td>0.470 1</td><td>0.560 4</td><td>0.470 7</td><td><b>0.613</b><b>2</b></td></tr><tr><td><br />TR31</td><td>0.437 6</td><td>0.478 9</td><td>0.423</td><td>0.436 3</td><td><b>0.496</b><b>9</b></td><td>0.475 9</td></tr><tr><td><br />K1B</td><td>0.642 2</td><td>0.554</td><td>0.614 5</td><td>0.696 2</td><td>0.694 7</td><td><b>0.769</b><b>4</b></td></tr><tr><td><br />WebKB</td><td>0.504 5</td><td>0.443 8</td><td>0.466 9</td><td>0.572 8</td><td>0.585 9</td><td><b>0.599</b><b>5</b></td></tr><tr><td><br />平均值</td><td>0.510 3</td><td>0.470 7</td><td>0.493 6</td><td>0.566 4</td><td>0.562 0</td><td><b>0.614</b><b>5</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="233">
                    <p class="img_tit"><b>表</b>3 <b>各聚类算法的归一化互信息</b> (NMI) <b>对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Comparison of NMI among different algorithms</p>
                    <p class="img_note"></p>
                    <table id="233" border="1"><tr><td><br />数据集</td><td>KKM</td><td>SC</td><td>KCF</td><td>Coreg</td><td>RMKKM</td><td>GMKCF</td></tr><tr><td><br />BBC</td><td>0.196 1</td><td>0.201 8</td><td>0.237 8</td><td>0.346 9</td><td>0.226 9</td><td><b>0.397</b><b>7</b></td></tr><tr><td><br />TR31</td><td>0.202 9</td><td>0.343 6</td><td>0.250 6</td><td>0.329 2</td><td>0.262 8</td><td><b>0.354</b><b>0</b></td></tr><tr><td><br />K1B</td><td>0.281 5</td><td>0.391 6</td><td>0.351 2</td><td>0.564</td><td>0.332 5</td><td><b>0.620</b><b>4</b></td></tr><tr><td><br />WebKB</td><td>0.198 1</td><td>0.113 2</td><td>0.187 2</td><td><b>0.372</b><b>7</b></td><td>0.316 9</td><td>0.365 6</td></tr><tr><td><br />平均值</td><td>0.219 7</td><td>0.262 6</td><td>0.256 7</td><td>0.403 2</td><td>0.284 8</td><td><b>0.434</b><b>4</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="234">
                    <p class="img_tit"><b>表</b>4 <b>各聚类算法的聚类纯度</b> (purity) <b>对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Comparison of purity among different algorithms</p>
                    <p class="img_note"></p>
                    <table id="234" border="1"><tr><td><br />数据集</td><td>KKM</td><td>SC</td><td>KCF</td><td>Coreg</td><td>RMKKM</td><td>GMKCF</td></tr><tr><td><br />BBC</td><td>0.484 0</td><td>0.467 5</td><td>0.519</td><td>0.586 1</td><td>0.504 6</td><td><b>0.643</b><b>6</b></td></tr><tr><td><br />TR31</td><td>0.520 4</td><td>0.628 4</td><td>0.552 2</td><td>0.608 3</td><td>0.562 2</td><td><b>0.630</b><b>9</b></td></tr><tr><td><br />K1B</td><td>0.716 4</td><td>0.770 2</td><td>0.745 7</td><td>0.829 8</td><td>0.733 6</td><td><b>0.853</b><b>9</b></td></tr><tr><td><br />WebKB</td><td>0.534 9</td><td>0.451 6</td><td>0.528 8</td><td><b>0.678</b><b>0</b></td><td>0.622 8</td><td>0.664 2</td></tr><tr><td><br />平均值</td><td>0.563 9</td><td>0.579 4</td><td>0.586 4</td><td>0.675 6</td><td>0.605 8</td><td>0.698 2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="235">需要指出的是多核方法Coreg和RMKKM都带有超参数, 无监督聚类问题中如何选择有效的超参数本身就是一个非常困难的问题。而本文提出的GMKCF算法无需设置其他特定参数, 极大提升了算法的实际可用性。</p>
                </div>
                <div class="p1">
                    <p id="236">此外, 本文提出的GMKCF算法在空间复杂度上和其他多核方法类似, 都是<i>O</i> (<i>n</i><sup>2</sup>) , 从时间复杂度看GMKCF和RMKKM都是<i>O</i> (<i>n</i><sup>2</sup>) , 而Coreg的时间复杂度为<i>O</i> (<i>n</i><sup>3</sup>) ;并且GMKCF和RMKKM中主要涉及矩阵和向量的基本操作, 可以借助MapReduce等框架容易实现分布式部署, 而Coreg由于需要计算特征空间导致分布式实现较为困难。</p>
                </div>
                <div class="p1">
                    <p id="237">实验结果表明, 本文提出的多核概念分解方法在多种聚类评价指标上的结果要优于其他单核和多核聚类方法, 无需设置超参数, 并且算法复杂度较低, 容易分布式部署。</p>
                </div>
                <h3 id="238" name="238" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="239">针对核概念分解模型在实际应用中面临的核函数选择问题, 本文提出基于多核全局融合的概念分解模型。与核概念分解模型类似, 本文推导出对应的迭代式乘法更新公式作为求解算法并且证明算法的收敛性。多个基准数据集上的实验结果表明, 本文算法在不引入额外超参数的情况下能够有效提升核分解模型在实际应用中的聚类性能。未来, 我们将进一步研究如何在分布式环境中部署实施多核概念分解算法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="274">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data Mining:Concepts and Techniques">

                                <b>[1]</b>HAN J, KAMBER M, PEI J.Data Mining:Concepts and Techniques[M].3rd ed.San Francisco:Margan Kaufmann, 2011:525-527.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning the parts of objects by non-negative matrix factorization">

                                <b>[2]</b>LEE D D, HSEBASTIAN S S.Learning the parts of objects by nonnegative matrix factorization[J].Nature, 1999, 401:788-791.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Top-down parameter-free clustering of high-dimensional categorical data">

                                <b>[3]</b>CESARIO E, MANCO G, ORTALE R.Top-down parameter-free clustering of high-dimensional categorical data[J].IEEE Transactions on Knowledge and Data Engineering, 2007, 19 (12) :1607-1624.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Normalized Cuts and Image Segmentation">

                                <b>[4]</b>SHI J, MALIK J.Normalized cuts and image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (8) :888-905.
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering by passing messages between data points">

                                <b>[5]</b>FREY B J, DUECK D.Clustering by passing messages between data points[J].Science, 2007, 315 (5814) :972-976.
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph Regularized Nonnegative Matrix Factorization for Data Representation">

                                <b>[6]</b>CAI D, HE X, HAN J, et al.Graph regularized nonnegative matrix factorization for data representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (8) :1548-1560.
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pattern Recognition and Machine Learning">

                                <b>[7]</b>BISHOP C M.Pattern Recognition and Machine Learning[M].2nd ed.New York:Springer, 2010:291-292.
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-negative matrix factorization with sparseness constraints">

                                <b>[8]</b>HOYER P O.Non-negative matrix factorization with sparseness constraints[EB/OL].[2018-05-10].https://arxiv.org/abs/cs/0408058.
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Nonnegative Matrix Factorization via Half-Quadratic Minimization.">

                                <b>[9]</b>DU L, LI X, SHEN Y.Robust nonnegative matrix factorization via half-quadratic minimization[C]//Proceedings of the 2012 IEEE12th International Conference on Data Mining.Piscataway, NJ:IEEE, 2012:201-210.
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Document clustering by concept factorization">

                                <b>[10]</b>XU W, GONG Y.Document clustering by concept factorization[C]//SIGIR 2004:Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York:ACM, 2004:202-209.
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locally consistent concept factorization for document clustering">

                                <b>[11]</b>CAI D, HE X, HAN J.Locally consistent concept factorization for document clustering[J].IEEE Transactions on Knowledge and Data Engineering, 2011, 23 (6) :902-913.
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Concept factorization with adaptive neighbors for document clustering">

                                <b>[12]</b>PEI X, CHEN C, GONG W.Concept factorization with adaptive neighbors for document clustering[J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (2) :343-352.
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Algorithms for non-negative matrix factorization">

                                <b>[13]</b>LEE D D, SEUNG H S.Algorithms for non-negative matrix factorization[EB/OL].[2018-05-10].http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf.
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Co-regularized multi-view spectral clustering">

                                <b>[14]</b>KUMAR A, RAI P, DAUMH.Co-regularized multi-view spectral clustering[EB/OL].[2018-05-10].http://www.cs.utah.edu/~piyush/recent/spectral-nips11.pdf.
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust multiple kernel k-means clustering using L21-norm">

                                <b>[15]</b>DU L, ZHOU P, SHI L, et al.Robust multiple kernel k-means clustering using L<sub>21</sub>-norm[C]//Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2015:3476-3482.
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES52F772448489C3374B68EA1C66051721&amp;v=MjkwOTJCUTg2ekJFWG1EbDFQUTdqM3hRemViZVZRcmllQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekwyN3hLQT1OaWZPZmJhNmFOYkxyWXRCYk84SA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>LI X, SHEEN X, SHU Z, et al.Graph regularized multilayer concept factorization for data representation[J].Neurocomputing, 2017, 238:139-151.
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK1EA247530A433772299969F10549447E&amp;v=MDg5MDlMVTA1dHBoekwyN3hLQT1OaWZKWmJMTmI5UElxSXBHWkpvTEQzOCt5QlFSNHpaMFRuYVVyUkl3ZmJ1UVFiM3FDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>ZHAN K, SHI J, WANG J, et al.Adaptive structure concept factorization for multiview clustering[J].Neural Computation, 2018, 30 (2) :1080-1103.
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiple graph regularized concept factorization with adaptive weights">

                                <b>[18]</b>SHU Z, WU X, HUANG P, et al.Multiple graph regularized concept factorization with adaptive weights[J].IEEE Access, 2018, 6:64938-64945.
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-representative manifold concept factorization with adaptive neighbors for clustering">

                                <b>[19]</b>MA S, ZHANG L, HU E, et al.Self-representative manifold concept factorization with adaptive neighbors for clustering[C]//IJCAI 2018:Proceedings of the 27th International Joint Conference on Artificial Intelligence.Menlo Park, CA:AAAI Press, 2018:2539-2545.
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Co-regularized multiview spectral clustering">

                                <b>[20]</b>KUMAR A, RAI P, DAUMH, Ⅲ.Co-regularized multi-view spectral clustering[C]//NIPS 2011:Proceedings of the 24th International Conference on Neural Information Processing Systems.New York:ACM, 2011:1413-1421.
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC6D959A9BB6AB30B941BEEDF6B94FA53&amp;v=MTY3NzBRYTdqNFBQUXFXMmhSSGNMYmlOTCtjQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekwyN3hLQT1OaWZPZmNDK2F0akpwdjVNRnBrSmZRNDZ6Mg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>YAN W, ZHANG B, MA S, et al.A novel regularized concept factorization for document clustering[J].Knowledge-based Systems, 2017, 135:147-158.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904015&amp;v=MjU0MTZxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3dkFMejdCZDdHNEg5ak1xNDlFWVlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
