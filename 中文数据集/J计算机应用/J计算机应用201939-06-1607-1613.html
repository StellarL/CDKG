<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136674642658750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906010%26RESULT%3d1%26SIGN%3dGKbfO9%252bySoW8zqnQcb3q6oidpA4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906010&amp;v=MjEzODlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hWYnJBTHo3QmQ3RzRIOWpNcVk5RVpJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="1 传统的卷积神经网络模型压缩方法 ">1 传统的卷积神经网络模型压缩方法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="2 特征复用单元 ">2 特征复用单元</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="2.1 &lt;b&gt;传统的卷积操作&lt;/b&gt;">2.1 <b>传统的卷积操作</b></a></li>
                                                <li><a href="#59" data-title="2.2 &lt;b&gt;特征复用的卷积操作&lt;/b&gt;">2.2 <b>特征复用的卷积操作</b></a></li>
                                                <li><a href="#73" data-title="2.3 &lt;b&gt;使用&lt;/b&gt;FR-unit&lt;b&gt;对网络的传统卷积层进行优化&lt;/b&gt;">2.3 <b>使用</b>FR-unit<b>对网络的传统卷积层进行优化</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="3.1 &lt;b&gt;本文实验配置&lt;/b&gt;">3.1 <b>本文实验配置</b></a></li>
                                                <li><a href="#85" data-title="3.2 &lt;b&gt;方法可行性实验&lt;/b&gt;">3.2 <b>方法可行性实验</b></a></li>
                                                <li><a href="#118" data-title="3.3 &lt;b&gt;对&lt;/b&gt;&lt;i&gt;VGG&lt;/i&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;Resnet&lt;/i&gt;&lt;b&gt;的优化实验&lt;/b&gt;">3.3 <b>对</b><i>VGG</i><b>和</b><i>Resnet</i><b>的优化实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#135" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="图1 传统的卷积操作">图1 传统的卷积操作</a></li>
                                                <li><a href="#64" data-title="图2 特征复用的卷积操作">图2 特征复用的卷积操作</a></li>
                                                <li><a href="#79" data-title="图3 使用FR-unit对单次卷积进行优化">图3 使用FR-unit对单次卷积进行优化</a></li>
                                                <li><a href="#80" data-title="图4 使用FR-unit对卷积块进行优化">图4 使用FR-unit对卷积块进行优化</a></li>
                                                <li><a href="#81" data-title="图5 使用FR-unit对通道数不变的卷积块进行优化">图5 使用FR-unit对通道数不变的卷积块进行优化</a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;实验&lt;/b&gt;1&lt;b&gt;模型及其参数&lt;/b&gt;"><b>表</b>1 <b>实验</b>1<b>模型及其参数</b></a></li>
                                                <li><a href="#93" data-title="图6 单次卷积结构优化实验准确率曲线">图6 单次卷积结构优化实验准确率曲线</a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;单次卷积结构优化实验结果对比&lt;/b&gt;"><b>表</b>2 <b>单次卷积结构优化实验结果对比</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;实验&lt;/b&gt;2&lt;b&gt;模型及其参数&lt;/b&gt;"><b>表</b>3 <b>实验</b>2<b>模型及其参数</b></a></li>
                                                <li><a href="#102" data-title="图7 卷积块结构优化实验准确率曲线">图7 卷积块结构优化实验准确率曲线</a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;卷积块结构优化实验结果对比&lt;/b&gt;"><b>表</b>4 <b>卷积块结构优化实验结果对比</b></a></li>
                                                <li><a href="#108" data-title="图8 数据增强后卷积块结构优化实验准确率曲线">图8 数据增强后卷积块结构优化实验准确率曲线</a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;数据增强后卷积块结构优化实验结果对比&lt;/b&gt;"><b>表</b>5 <b>数据增强后卷积块结构优化实验结果对比</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;实验&lt;/b&gt;3&lt;b&gt;模型参数&lt;/b&gt;"><b>表</b>6 <b>实验</b>3<b>模型参数</b></a></li>
                                                <li><a href="#115" data-title="图9 不同&lt;i&gt;FR&lt;/i&gt;-&lt;i&gt;unit&lt;/i&gt;叠加实验准确率曲线">图9 不同<i>FR</i>-<i>unit</i>叠加实验准确率曲线</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表&lt;/b&gt;7 &lt;b&gt;不同&lt;/b&gt;&lt;i&gt;FR&lt;/i&gt;-&lt;i&gt;unit&lt;/i&gt;&lt;b&gt;叠加实验结果对比&lt;/b&gt;"><b>表</b>7 <b>不同</b><i>FR</i>-<i>unit</i><b>叠加实验结果对比</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表&lt;/b&gt;8 &lt;i&gt;VGG&lt;/i&gt;&lt;b&gt;优化实验网络参数&lt;/b&gt;"><b>表</b>8 <i>VGG</i><b>优化实验网络参数</b></a></li>
                                                <li><a href="#123" data-title="图10 &lt;i&gt;VGG&lt;/i&gt;优化实验准确率曲线">图10 <i>VGG</i>优化实验准确率曲线</a></li>
                                                <li><a href="#124" data-title="图11 &lt;i&gt;VGG&lt;/i&gt;优化实验损失曲线">图11 <i>VGG</i>优化实验损失曲线</a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表&lt;/b&gt;9 &lt;i&gt;VGG&lt;/i&gt;&lt;b&gt;优化实验结果对比&lt;/b&gt;"><b>表</b>9 <i>VGG</i><b>优化实验结果对比</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表&lt;/b&gt;10 &lt;i&gt;Resnet&lt;/i&gt;&lt;b&gt;优化实验模型及其参数&lt;/b&gt;"><b>表</b>10 <i>Resnet</i><b>优化实验模型及其参数</b></a></li>
                                                <li><a href="#131" data-title="图12 &lt;i&gt;Resnet&lt;/i&gt;优化实验准确率曲线">图12 <i>Resnet</i>优化实验准确率曲线</a></li>
                                                <li><a href="#132" data-title="图13 &lt;i&gt;Resnet&lt;/i&gt;优化实验损失曲线">图13 <i>Resnet</i>优化实验损失曲线</a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表&lt;/b&gt;11 &lt;i&gt;Resnet&lt;/i&gt;&lt;b&gt;网络优化前后的实验结果对比&lt;/b&gt;"><b>表</b>11 <i>Resnet</i><b>网络优化前后的实验结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="161">


                                    <a id="bibliography_1" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.Piscataway, NJ:IEEE, 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[1]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.Piscataway, NJ:IEEE, 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_2" title="SERCU T, PUHRSCH C, KINGSBURY B, et al.Very deep multilingual convolutional neural networks for LVCSR[C]//Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2016:4955-4959." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep multilingual convolutional neural networks for LVCSR">
                                        <b>[2]</b>
                                        SERCU T, PUHRSCH C, KINGSBURY B, et al.Very deep multilingual convolutional neural networks for LVCSR[C]//Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2016:4955-4959.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_3" title="SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[3]</b>
                                        SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_4" title="HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[4]</b>
                                        HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:770-778.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                    LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436-444.</a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_6" title="ZEILER M D, FERGUS R.Visualizing and understanding convolutional networks[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:818-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">
                                        <b>[6]</b>
                                        ZEILER M D, FERGUS R.Visualizing and understanding convolutional networks[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:818-833.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_7" title="HARIHARAN B, ARBELAEZ P, ARBELAEZ R, et al.Simultaneous detection and segmentation[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:297-312." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous detection and segmentation">
                                        <b>[7]</b>
                                        HARIHARAN B, ARBELAEZ P, ARBELAEZ R, et al.Simultaneous detection and segmentation[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:297-312.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_8" title="GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[8]</b>
                                        GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_9" title="WANG N, YEUNG D Y.Learning a deep compact image representation for visual tracking[C]//Proceedings of the 26th International Conference on Neural Information Processing Systems.Piscataway, NJ:IEEE, 2013:809-817." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Compact Image Representation for Visual Tracking">
                                        <b>[9]</b>
                                        WANG N, YEUNG D Y.Learning a deep compact image representation for visual tracking[C]//Proceedings of the 26th International Conference on Neural Information Processing Systems.Piscataway, NJ:IEEE, 2013:809-817.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_10" title="周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251. (ZHOU F Y, JIN L P, DONG J.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MDQxMDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFZickFMejdCZHJHNEg5Yk1xWTlGWllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251. (ZHOU F Y, JIN L P, DONG J.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251.) 
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_11" title="雷杰, 高鑫, 宋杰, 等.深度网络模型压缩综述[J].软件学报, 2018, 29 (2) :251-266. (LEI J, GAO X, SONG J, et al.Survey of deep neural network model compression[J].Journal of Software, 2018, 29 (2) :251-266.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201802002&amp;v=MTkwNjBGckNVUjdxZlp1WnNGeS9oVmJyQU55ZlRiTEc0SDluTXJZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        雷杰, 高鑫, 宋杰, 等.深度网络模型压缩综述[J].软件学报, 2018, 29 (2) :251-266. (LEI J, GAO X, SONG J, et al.Survey of deep neural network model compression[J].Journal of Software, 2018, 29 (2) :251-266.) 
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_12" title="OBERMAN S F, FLYNN M J.Design issues in division and other floating-point operations[J].IEEE Transactions on Computers, 1997, 46 (2) :154-161." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design Issues in Division and Other Floating-Point Operations">
                                        <b>[12]</b>
                                        OBERMAN S F, FLYNN M J.Design issues in division and other floating-point operations[J].IEEE Transactions on Computers, 1997, 46 (2) :154-161.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_13" title="SETIONO R, LIU H.Neural-network feature selector[J].IEEETransactions on Neural Networks, 1997, 8 (3) :654-662." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural-network feature selector">
                                        <b>[13]</b>
                                        SETIONO R, LIU H.Neural-network feature selector[J].IEEETransactions on Neural Networks, 1997, 8 (3) :654-662.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_14" title="孔英会, 朱成诚, 车辚辚.复杂背景下基于Mobile Nets的花卉识别与模型剪枝[J].科学技术与工程, 2018, 18 (19) :84-88. (KONG Y H, ZHU C C, CHE L L.Flower recognition in complex background and model pruning based on Mobile Nets[J].Science Technology and Engineering, 2018, 18 (19) :84-88.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXJS201819013&amp;v=MDE5MjZHNEg5bk5wbzlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFZickFMalhCZmI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        孔英会, 朱成诚, 车辚辚.复杂背景下基于Mobile Nets的花卉识别与模型剪枝[J].科学技术与工程, 2018, 18 (19) :84-88. (KONG Y H, ZHU C C, CHE L L.Flower recognition in complex background and model pruning based on Mobile Nets[J].Science Technology and Engineering, 2018, 18 (19) :84-88.) 
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_15" title="HU H, PENG R, TAI Y W, et al.Network trimming:a datadriven neuron pruning approach towards efficient deep architectures[J].ar Xiv Preprint, 2016, 2016:ar Xiv.1607.03250." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network trimming:a datadriven neuron pruning approach towards efficient deep architectures">
                                        <b>[15]</b>
                                        HU H, PENG R, TAI Y W, et al.Network trimming:a datadriven neuron pruning approach towards efficient deep architectures[J].ar Xiv Preprint, 2016, 2016:ar Xiv.1607.03250.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_16" title="ROSENBLUETH E.Point estimates for probability moments[J].Proceedings of the National Academy of Sciences of the United States of America, 1975, 72 (10) :3812-3814." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Point estimates for probability moments">
                                        <b>[16]</b>
                                        ROSENBLUETH E.Point estimates for probability moments[J].Proceedings of the National Academy of Sciences of the United States of America, 1975, 72 (10) :3812-3814.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_17" title="GOLUB G H, REINSCH C.Singular value decomposition and least squares solutions[J].Numerische Mathematik, 1970, 14 (5) :403-420." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002469547&amp;v=MDYwMTU9Tmo3QmFyTzRIdEhPcTRsTVllOElZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpcmxWTHZPSlZZ&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        GOLUB G H, REINSCH C.Singular value decomposition and least squares solutions[J].Numerische Mathematik, 1970, 14 (5) :403-420.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_18" title="HUANG G, SUN Y, LIU Z, et al.Deep networks with stochastic depth[C]//Proceedings of the 2016 14th European Conference on Computer Vision.Berlin:Springer, 2016:646-661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep networks with stochastic depth">
                                        <b>[18]</b>
                                        HUANG G, SUN Y, LIU Z, et al.Deep networks with stochastic depth[C]//Proceedings of the 2016 14th European Conference on Computer Vision.Berlin:Springer, 2016:646-661.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                    HUANG G, LIU Z, van der MAATEN L, et al.Densely connected convolutional networks[C]//Proceedings of the 2017 IEEEConference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2261-2269.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-07 15:10</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1607-1613 DOI:10.11772/j.issn.1001-9081.2018091992            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于特征复用的卷积神经网络模型压缩方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%80%E6%A0%91%E4%BC%9F&amp;code=41987892&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冀树伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%96%9C%E6%97%BA&amp;code=09479456&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨喜旺</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E6%99%8B%E8%8B%B1&amp;code=09503389&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄晋英</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B0%B9%E5%AE%81&amp;code=41987893&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尹宁</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%8C%97%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E9%99%A2&amp;code=0036109&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中北大学大数据学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%8C%97%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0036109&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中北大学机械工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%8C%97%E5%A4%A7%E5%AD%A6%E8%BD%AF%E4%BB%B6%E5%AD%A6%E9%99%A2&amp;code=0036109&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中北大学软件学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了在不降低准确率的前提下, 减小卷积神经网络模型的体积与计算量, 提出一种基于特征复用的卷积神经网络压缩模块——特征复用单元 (FR-unit) 。首先, 针对不同类型的卷积神经网络结构, 提出不同的优化方法;然后, 在对输入特征图进行卷积操作后, 将输入特征与输出特征进行结合;最后, 将结合后的特征传递给下一层。通过对低层特征的重复使用, 使总的提取的特征数量不发生改变, 以保证优化后的网络的准确率不会发生改变。在CIFAR10数据集上进行验证, 实验结果表明, 优化后的VGG模型体积缩小为优化前的75.4%, 预测时间缩短为优化前的43.5%;优化后的Resnet模型体积缩小为优化前的53.1%, 预测时间缩短为优化前的60.9%, 且在测试集上的准确率均未降低。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%A4%8D%E7%94%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征复用;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E5%8A%A0%E9%80%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络加速;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型压缩;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    冀树伟 (1992—) , 男, 山西阳泉人, 硕士研究生, CCF会员, 主要研究方向:深度学习、机器视觉;;
                                </span>
                                <span>
                                    *杨喜旺 (1971—) , 男, 内蒙古包头人, 副教授, 博士, 主要研究方向:人工智能、精密仪器与机械、计算机仿真;383913197@ qq. com;
                                </span>
                                <span>
                                    黄晋英 (1971—) , 女, 山西代县人, 教授, 博士, 主要研究方向:结构动力学分析、检测与控制、故障诊断;;
                                </span>
                                <span>
                                    尹宁 (1992—) , 女, 山西忻州人, 硕士研究生, CCF会员, 主要研究方向:人工智能、深度学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-27</p>

            </div>
                    <h1><b>Model compression method of convolution neural network based on feature-reuse</b></h1>
                    <h2>
                    <span>JI Shuwei</span>
                    <span>YANG Xiwang</span>
                    <span>HUANG Jinying</span>
                    <span>YIN Ning</span>
            </h2>
                    <h2>
                    <span>School of Date Science And Technology, North University of China</span>
                    <span>School of Mechanical Engineering, North University of China</span>
                    <span>Software School, North University of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to reduce the volume and computational complexity of the convolutional neural network model without reducing the accuracy, a compression method of convolutional neural network model based on feature reuse unit called FR-unit (Feature-Reuse unit) was proposed. Firstly, different optimization methods were proposed for different types of convolution neural network structures. Then, after convoluting the input feature map, the input feature was combined with output feature. Finally, the combined feature was transferred to the next layer. Through the reuse of low-level features, the total number of extracted features would not change, so as to ensure that the accuracy of optimized network would not change. The experimental results on CIFAR10 dataset show that, the volume of Visual Geometry Group (VGG) model is reduced to 75.4% and the prediction time is reduced to 43.5% after optimization, the volume of Resnet model is reduced to 53.1% and the prediction time is reduced to 60.9% after optimization, without reducing the accuracy on the test set.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolution%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolution Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature-reuse&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature-reuse;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=net%20accelerate&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">net accelerate;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20compression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model compression;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JI Shuwei, born in 1992, M. S. candidate. His research interests include deep learning, machine vision. ;
                                </span>
                                <span>
                                    YANG Xiwang, born in 1971, Ph. D. , associate professor. His research interests include artificial intelligence, precision instruments and machinery, computer simulation. ;
                                </span>
                                <span>
                                    HUANG Jinying, born in 1971, Ph. D. , professor. Her research interests include structural dynamics analysis, detection and control, fault diagnosis. ;
                                </span>
                                <span>
                                    YIN Ning, born in 1992, M. S. candidate. Her research interests include artificial intelligence, deep learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-27</p>
                            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">在2012年, AlexNet<citation id="199" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>以Top- 5错误率16.4%的显著优势夺得了大规模视觉识别挑战赛 (Large Scale Visual Recognition Challenge, LSVRC) 的冠军, 比第二名错误率降低了12.2%的成绩, 使得卷积神经网络 (Convolution Neural Network, CNN) 受到了广泛的关注与研究。之后, 各种卷积神经网络模型被不断提出, 如牛津大学计算机视觉组 (Visual Geometry Group, VGG) 提出的VGGNet<citation id="200" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、Google提出的Inception Net<citation id="201" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>以及微软研究院提出的Resnet<citation id="202" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等, 都刷新了AlexNet的记录。深度学习<citation id="203" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的研究发展极其迅速, 从最初的图像分类<citation id="206" type="reference"><link href="161" rel="bibliography" /><link href="163" rel="bibliography" /><link href="165" rel="bibliography" /><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><link href="171" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>, 到之后的目标检测<citation id="207" type="reference"><link href="173" rel="bibliography" /><link href="175" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>、目标跟踪<citation id="204" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等诸多方面都取得了非常瞩目的成就<citation id="205" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="43">但是, 这些模型由于较大的计算量, 往往需要运行于GPU或高性能的CPU上。如拥有8层网络层数的AlexNet有着60×10<sup>6</sup>的参数量, 运行一次需要1.5 GFLOPS (每秒10亿次的浮点运算数) 的运算量, 需要249 MB的存储空间。VGG16有着138×10<sup>6</sup>的参数量, 需要15.5 GFLOPS的运算量, 需要528.7 MB的存储空间。Resnet152有19.4×10<sup>6</sup>的参数量, 11.3 GFLOPS的运算量, 需要230.34 MB的存储空间。VGG模型<citation id="208" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>通过叠加小卷积核代替大卷积核, 而Resnet<citation id="209" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>则通过1×1的卷积操作, 对特征图的通道数进行了降维。这些操作均有效地降低了模型的参数量和计算量<citation id="210" type="reference"><link href="161" rel="bibliography" /><link href="163" rel="bibliography" /><link href="167" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag">1 传统的卷积神经网络模型压缩方法</h3>
                <div class="p1">
                    <p id="45">传统的卷积神经网络大多是由卷积层、池化层、激活层、全连接层等结构反复堆叠而组成的。卷积层是卷积神经网络的核心部分, 具有局部连接、权重共享等特点, 这些特点使得卷积层的参数数量比全连接层少, 但是计算量较大。计算量大导致前向传播慢, 推断耗时长;参数量大则需要占用大量的存储空间。模型的压缩大多针对卷积层与全连接层来进行优化, 常用的模型压缩大多从以下三个方面入手<citation id="211" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="46">第一种方法为网络的裁剪。由于深度神经网络中的参数是经过多次训练得到的, 往往存在着一定程度的冗余, 所以通过某种方式裁剪掉冗余的参数可完成模型的压缩。模型的裁剪, 主要是针对已经训练完毕的模型, 定义一个模型参数重要性的判别依据, 将不重要的网络参数裁剪掉<citation id="212" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。网络剪枝<citation id="215" type="reference"><link href="185" rel="bibliography" /><link href="187" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>方法在卷积层与全连接层都比较适用, 但是不同的方法侧重点不一样。常用的方法有:1) 基于激活响应熵的裁剪标准<citation id="213" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 在验证数据集上, 统计卷积层每个通道的平均激活值分布, 计算分布熵, 将熵较小的通道所对应的滤波器裁剪掉;2) 将模型参数使用泰勒公式进行展开<citation id="214" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 将模型损失的变化以参数的函数来表示, 来实现对模型参数重要性的评估, 并将不重要的参数裁剪掉。</p>
                </div>
                <div class="p1">
                    <p id="47">第二种方法是重新设计网络结构。模型的运行速度受以下三方面的影响:模型的参数量、网络结构与网络的深度, 因此, 通过修改网络结构、调整模型参数量等, 可以有效降低模型运行时间。例如模型网络层的组合以及改变网络的连接关系等。常用的有深度可分离卷积 (depthwise separable convolution) 等。</p>
                </div>
                <div class="p1">
                    <p id="48">第三种方法为权重分解。通过将权重张量分解为两个或者多个张量, 来降低模型的计算量, 如奇异值分解 (Singular Value Decomposition, SVD) 方法<citation id="216" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 将矩阵分解为向量之间的乘积, 来达到降低模型计算量与参数量的目的。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag">2 特征复用单元</h3>
                <div class="p1">
                    <p id="50">本文提出一种针对卷积层的优化结构——特征复用 (Feature-Reuse) , 来降低模型的计算量并压缩模型的体积。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">2.1 <b>传统的卷积操作</b></h4>
                <div class="p1">
                    <p id="52">传统卷积操作如下:输入一个单通道或多通道的特征图后, 通过卷积核的卷积操作, 得到一个单通道或多通道的特征图。该次卷积操作结束后, 将得到的特征图传给下一个卷积层或池化层。</p>
                </div>
                <div class="p1">
                    <p id="53">因此, 假设输入通道数为<i>I</i>, 输出通道数为<i>O</i>, 卷积核用<b><i>C</i></b>表示, <i>n</i>是卷积核大小, 输入为<b><i>x</i></b>, <i>m</i>是输出的特征图大小, <b><i>B</i></b>为偏置矩阵。输出的单个神经元<i>y</i>的计算为:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mi>l</mi></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="bold-italic">x</mi></mstyle><msubsup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo></mrow><mi>l</mi></msubsup><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>i</mi><mo>, </mo><mi>n</mi><mo>-</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">B</mi><msup><mrow></mrow><mi>l</mi></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">一次卷积操作的计算量为:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><msup><mrow></mrow><mi>m</mi></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>*</mo><mi>m</mi></mrow></munderover><mi>y</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mi>l</mi></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>*</mo><mi>m</mi></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="bold-italic">x</mi></mstyle><msubsup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo></mrow><mi>l</mi></msubsup><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>i</mi><mo>, </mo><mi>n</mi><mo>-</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">B</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">总计算量为:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>Ι</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><mi>Ο</mi></munderover><mi>Y</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow><mi>m</mi></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">2.2 <b>特征复用的卷积操作</b></h4>
                <div class="p1">
                    <p id="60">传统的卷积神经网络的网络结构是一个递进的层级结构, 它通过用底层的卷积核提取基础特征, 用高层的卷积核提取抽象的特征来达到特征学习的目的。当该网络提取高阶特征的时候, 前面提取的比较基础的特征就会被遗弃掉。而随机深度网络 (<i>Deep networks with stochastic depth</i>) <citation id="217" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>与密集连通卷积网络 (<i>Densely Connected Convolution Network</i>, <i>DCCN</i>) <citation id="218" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>的出现, 则展示了一种新的方法。该方法指出, 网络中的某一层可以不仅仅依赖于上一层的特征, 也可以依赖于较浅层的基础特征, 同时该方法也说明卷积神经网络中的卷积层存在一定的冗余。</p>
                </div>
                <div class="p1">
                    <p id="61">本文的思路就是利用特征复用的思想, 将前一层特征重复使用, 以减少每次卷积操作所需提取的特征数量。该方法可以在不改变网络总体特征提取数量的同时, 来达到降低计算量和缩小模型体积的目的。</p>
                </div>
                <div class="p1">
                    <p id="62">如图1所示为传统的卷积操作结构, 图2为本文提出的特征复用的卷积操作结构。图中<i>CONV</i>是卷积层;<i>POOL</i>是池化层;<i>FC</i>是全连接层;<i>CAT</i>是连接操作, 在特征图通道这一维度上对特征图进行连接。图2中方框内为本文提出的一个基本的特征复用结构单元 (<i>Feature</i>-<i>Reuse unit</i>, <i>FR</i>-<i>unit</i>) 。本文将串联的多次卷积操作称为卷积块。<i>FR</i>-<i>unit</i>可进行反复叠加, 对卷积块进行优化。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 传统的卷积操作" src="Detail/GetImg?filename=images/JSJY201906010_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 传统的卷积操作  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 1 <i>Traditional convolution operation</i></p>

                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 特征复用的卷积操作" src="Detail/GetImg?filename=images/JSJY201906010_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 特征复用的卷积操作  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Feature</i>-<i>reuse convolution operation</i></p>

                </div>
                <div class="p1">
                    <p id="65">假设卷积块有<i>I</i>个输入通道和<i>O</i>个输出通道, 中间层的输出通道数为<i>I</i><sup>*</sup>, 输入特征图的大小为<i>M</i>*<i>M</i>, 卷积核大小为<i>N</i>*<i>N</i>。</p>
                </div>
                <div class="p1">
                    <p id="66">使用传统卷积操作时, 卷积块做一次前向传播需要的计算量为:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>cal</i>=<i>IM</i><sup>2</sup><i>N</i><sup>2</sup><i>I</i><sup>*</sup>+<i>I</i><sup>*</sup><i>M</i><sup>2</sup><i>N</i><sup>2</sup><i>O</i>=<i>M</i><sup>2</sup><i>N</i><sup>2</sup><i>I</i><sup>*</sup> (<i>I</i>+<i>O</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="68">使用特征复用结构时, 卷积块做一次前向传播的计算量为:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>cal</i>=[<i>IM</i><sup>2</sup><i>N</i><sup>2</sup> (<i>O</i>-<i>I</i>) ]/2+<i>M</i><sup>2</sup><i>N</i><sup>2</sup>[ (<i>O</i><sup>2</sup>-<i>I</i><sup>2</sup>) /4]=</p>
                </div>
                <div class="p1">
                    <p id="70">[2<i>I</i>+ (<i>O</i>-<i>I</i>) /2][ (<i>O</i>-<i>I</i>) <i>M</i><sup>2</sup><i>N</i><sup>2</sup>]/2      (5) </p>
                </div>
                <div class="p1">
                    <p id="71">以图1和图2结构为例。通过对比可知, 使用FR-unit对传统卷积结构进行优化后, 可以明显减少卷积层的计算量, 优化后的计算量约为原先的20%:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>c</mi><mi>a</mi><mi>l</mi><mn>2</mn></mrow><mrow><mi>c</mi><mi>a</mi><mi>l</mi><mn>1</mn></mrow></mfrac><mo>=</mo><mfrac><mrow><mo stretchy="false">[</mo><mn>2</mn><mi>Ι</mi><mo>+</mo><mo stretchy="false"> (</mo><mi>Ο</mi><mo>-</mo><mi>Ι</mi><mo stretchy="false">) </mo><mo>/</mo><mn>2</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi>Ο</mi><mo>-</mo><mi>Ι</mi><mo stretchy="false">) </mo><mo>/</mo><mn>2</mn><mo stretchy="false">]</mo></mrow><mrow><mi>Ι</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>Ι</mi><mo>+</mo><mi>Ο</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mn>0</mn><mo>.</mo><mn>2</mn><mn>0</mn><mn>8</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">2.3 <b>使用</b>FR-unit<b>对网络的传统卷积层进行优化</b></h4>
                <div class="p1">
                    <p id="74">对于卷积神经网络中不同情况的卷积层, 使用不同的优化方式。</p>
                </div>
                <div class="p1">
                    <p id="75">1) 针对卷积神经网络最底层基础特征的提取, 不使用<i>FR</i>-<i>unit</i>进行优化。</p>
                </div>
                <div class="p1">
                    <p id="76">2) 针对网络中的单次卷积, 使用一个<i>FR</i>-<i>unit</i>进行优化。<i>FR</i>-<i>unit</i>中卷积的输出通道数为 (<i>O</i>-<i>I</i>) /2, 其中, <i>I</i>、<i>O</i>为被优化的卷积层的输入通道数与输出通道数。如图3所示, 使用一个卷积操作输出为64的FR-unit对原卷积层进行优化。</p>
                </div>
                <div class="p1">
                    <p id="77">3) 针对网络中的卷积块, 叠加与其卷积层数量相等的FR-unit来进行优化。FR-unit中卷积的输出通道数为 (<i>O</i>-<i>I</i>) /<i>n</i>, 其中, <i>I</i>、<i>O</i>为被优化的卷积块的最初输入通道数与最终输出通道数, <i>n</i>为卷积块中卷积层的数量。如图4所示, 使用两个输出为32通道的FR-unit对含有两个卷积层的卷积块进行优化。</p>
                </div>
                <div class="p1">
                    <p id="78">4) 针对网络中的输入和输出通道数没有发生改变的卷积块。FR-unit中卷积的输出通道数为 (<i>O</i>-<i>I</i>) /<i>n</i>, 如图5所示, 使用两个输出为256通道的FR-unit对含有两个卷积层的卷积块进行优化, 然后使用1×1卷积对特征图进行通道降维。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 使用FR-unit对单次卷积进行优化" src="Detail/GetImg?filename=images/JSJY201906010_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 使用FR-unit对单次卷积进行优化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Using FR-unit to optimize single convolution</p>

                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 使用FR-unit对卷积块进行优化" src="Detail/GetImg?filename=images/JSJY201906010_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 使用FR-unit对卷积块进行优化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Using FR-unit to optimize convolution block</p>

                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 使用FR-unit对通道数不变的卷积块进行优化" src="Detail/GetImg?filename=images/JSJY201906010_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 使用FR-unit对通道数不变的卷积块进行优化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Using FR-unit to optimize convolution block with channel number unchanged</p>

                </div>
                <h3 id="82" name="82" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="83" name="83">3.1 <b>本文实验配置</b></h4>
                <div class="p1">
                    <p id="84">本文的实验环境为<i>CPU</i>:<i>i</i>5-4210<i>M</i>, <i>GPU</i>:<i>GTX</i>960<i>M</i>, 操作系统:<i>Ubuntu</i>16.04, 深度学习框架:<i>Pytorch</i>0.3.1。为了加速训练, 在训练时使用<i>GPU</i>训练。在测试时, 在<i>CPU</i>上进行预测时间的统计。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">3.2 <b>方法可行性实验</b></h4>
                <div class="p1">
                    <p id="86">为了验证本文所提出的<i>FR</i>-<i>unit</i>结构的有效性, 在<i>CIFAR</i>10数据集上分别进行特征复用单元针对单次卷积结构、卷积块与多个特征复用单元叠加等三方面的实验。<i>CIFAR</i>10数据集包含60 000张32×32的彩色图片, 共分为10个类别。每类含有训练图像5 000张, 测试图像1 000张。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">3.2.1 <i>FR</i>-<i>unit</i>对单次卷积结构的优化实验</h4>
                <div class="p1">
                    <p id="88">本实验使用一个标准的卷积神经网络作为本次实验的基础网络<i>Basenet</i>, 然后使用<i>FR</i>-<i>unit</i>对<i>Basenet</i>中的卷积结构进行优化, 形成本文的特征复用网络 (<i>Feature</i>-<i>Reuse net</i>, 以下简称<i>FRnet</i>) 。由于本文主要是对卷积层进行改进, 为了减少干扰, 故将<i>Basenet</i>全连接层设为一层。虽然此设定会降低模型的准确率与收敛的平稳性, 但是对网络的对比并不会产生影响。</p>
                </div>
                <div class="p1">
                    <p id="89">在<i>CIFAR</i>10数据集上, 分别对<i>Basenet</i>和<i>FRnet</i>进行训练, 对比训练完毕后模型在测试集上的预测准确率 (<i>acc</i>) , 以及预测运行时间 (<i>time</i>) 。如表1所示, 为本实验中所使用的基础网络<i>Basenet</i>_<i>A</i>与特征复用网络 <i>FRnet</i>_<i>A</i>的模型参数。</p>
                </div>
                <div class="p1">
                    <p id="90">其中<i>Conv</i>代表卷积层, 参数3、32、3×3、1分别表示输入通道、输出通道、卷积核大小以及边界扩展。<i>FRlayer</i>代表本文的特征复用层, 参数分别代表卷积操作的输入通道数、输出通道数、卷积核大小以及边界扩展。每个<i>FRlayer</i>层都会将卷积操作的输入与输出在通道维度上进行连接, 然后作为最后的输出。</p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit"><b>表</b>1 <b>实验</b>1<b>模型及其参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Experiment</i> 1 <i>model and its parameters</i></p>
                    <p class="img_note"></p>
                    <table id="91" border="1"><tr><td colspan="2"><br /><i>Basenet</i>_<i>A</i></td><td rowspan="2"></td><td colspan="2"><br /><i>FRnet</i>_<i>A</i></td></tr><tr><td><br />层名</td><td>参数</td><td><br />层名</td><td>参数</td></tr><tr><td><br /><i>Conv</i>1</td><td>3, 32, 3×3, 1</td><td></td><td><i>Conv</i>1</td><td>3, 32, 3×3, 1</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td></td><td><i>MaxPool</i></td><td>2×2, 2</td></tr><tr><td><br /><i>Conv</i>2</td><td>32, 64, 3×3, 1</td><td></td><td><i>FRlayer</i>1</td><td>32, 32, 3×3, 1</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td></td><td><i>MaxPool</i></td><td>2×2, 2</td></tr><tr><td><br /><i>Conv</i>3</td><td>64, 128, 3×3, 1</td><td></td><td><i>FRlayer</i>2</td><td>64, 64, 3×3, 1</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td></td><td><i>MaxPool</i></td><td>2×2, 2</td></tr><tr><td><br /><i>FC</i></td><td>2 048, 10</td><td></td><td><i>FC</i></td><td>2 048, 10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="92">本部分实验部分进行了25个<i>epoch</i>的实验, 分别以0.001和0.000 1的学习率进行了10个<i>epoch</i>的实验, 以0.000 01的学习率进行了5个<i>epoch</i>的实验 (当数据集完整地通过网络训练一次称为一个<i>epoch</i>) 。训练数据没做增强。实验结果如图6及表2所示。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 单次卷积结构优化实验准确率曲线" src="Detail/GetImg?filename=images/JSJY201906010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 单次卷积结构优化实验准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Accuracy curves of single convolution optimization experiment</i></p>

                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表</b>2 <b>单次卷积结构优化实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Experimental results comparison of single convolution optimization</i></p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td><br />模型</td><td>训练<br />时间/<i>s</i></td><td>模型体<br />积大小/<i>KB</i></td><td>每百张图<br />预测时间/<i>s</i></td><td>测试集最<br />大准确率/%</td></tr><tr><td><br /><i>Basenet</i>_<i>A</i></td><td>2 816</td><td>456.2</td><td>0.063 3</td><td>73.49</td></tr><tr><td><br /><i>FRnet</i>_<i>A</i></td><td>2 746</td><td>270.7</td><td>0.056 7</td><td>74.19</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="95">实验数据显示, 使用本文所提方法对单次卷积机构进行优化后, 运行时间缩短到优化前的89.6%, 模型的体积缩小到优化前的59.3%。该结果表明使用特征复用对原模型优化后, 模型的压缩效果明显, 且测试准确率保持不变。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">3.2.2 <i>FR</i>-<i>unit</i>对卷积块结构的优化实验</h4>
                <div class="p1">
                    <p id="97">本实验所使用网络如表3所示, 其中<i>Basenet</i>_<i>B</i>为一个标准的卷积神经网络, <i>FRnet</i>_<i>B</i>为其对应的优化后的特征复用网络。</p>
                </div>
                <div class="area_img" id="98">
                    <p class="img_tit"><b>表</b>3 <b>实验</b>2<b>模型及其参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Experiment</i> 2 <i>model and its parameters</i></p>
                    <p class="img_note"></p>
                    <table id="98" border="1"><tr><td colspan="2"><br /><i>Basenet</i>_<i>B</i></td><td rowspan="2"></td><td colspan="2"><br /><i>FRnet</i>_ <i>B</i></td></tr><tr><td><br />层名</td><td>参数</td><td><br />层名</td><td>参数</td></tr><tr><td><br /><i>Conv</i>1</td><td>3, 32, 3×3, 1</td><td></td><td><i>Conv</i>1</td><td>3, 32, 3×3, 1</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td></td><td><i>MaxPool</i></td><td>2×2, 2</td></tr><tr><td><br /><i>Conv</i>2</td><td>32, 64, 3×3, 1<br />64, 64, 3×3, 1</td><td></td><td><i>FRlayer</i>1</td><td>32, 16, 3×3, 1<br />48, 16, 3×3, 1</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td></td><td><i>MaxPool</i></td><td>2×2, 2</td></tr><tr><td><br /><i>Conv</i>3</td><td>64, 128, 3×3, 1<br />128, 128, 3×3, 1</td><td></td><td><i>FRlayer</i>2</td><td>64, 32, 3×3, 1<br />96, 32, 3×3, 1</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td></td><td><i>MaxPool</i></td><td>2×2, 2</td></tr><tr><td><br /><i>FC</i></td><td>2 048, 10</td><td></td><td><i>FC</i></td><td>2 048, 10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="99">其中<i>Conv</i>2为含有两个卷积层的卷积块, <i>FRlayer</i>1特征复用层为两个<i>FR</i>-<i>unit</i>的叠加。</p>
                </div>
                <div class="p1">
                    <p id="100">由于使用了较深层的网络, 而数据集并没有做增强, 所以模型出现了过拟合的现象 (本次实验的实验1) 部分) 。为了得出较合理的实验结果, 在对训练集进行数据增强后, 进行了本部分的第二次实验 (本次实验的实验2) 部分) 。</p>
                </div>
                <div class="p1">
                    <p id="101">1) 本次实验部分进行了25个<i>epoch</i>的实验, 分别以0.001和0.000 1学习率进行了10个<i>epoch</i>的实验, 以0.000 01的学习率进行了5个<i>epoch</i>的实验。训练数据未做数据增强处理。实验结果如图7及表4所示。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 卷积块结构优化实验准确率曲线" src="Detail/GetImg?filename=images/JSJY201906010_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 卷积块结构优化实验准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Acuracy curves of convolution block optimization experiment</i></p>

                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表</b>4 <b>卷积块结构优化实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 4 <i>Experimental results comparison of convolution block optimization</i></p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><br />模型</td><td>训练<br />时间/<i>s</i></td><td>模型体积<br />大小/<i>KB</i></td><td>每百张图<br />预测时间/<i>s</i></td><td>测试集最大<br />准确率/%</td></tr><tr><td><br /><i>Basenet</i>_<i>B</i></td><td>3 284</td><td>1 228.8</td><td>0.131 5</td><td>71.92</td></tr><tr><td><br /><i>FRnet</i>_<i>B</i></td><td>3 230</td><td>317.1</td><td>0.075 4</td><td>74.97</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="104">实验数据表明, <i>FR</i>-<i>unit</i>对卷积块进行优化后, 预测时间缩短到优化前的57%, 而模型体积更是缩小到优化前的25.8%。因此, 通过使用<i>FR</i>-<i>unit</i>对网络进行优化, 可以大幅度降低预测时间, 并且压缩模型的体积。</p>
                </div>
                <div class="p1">
                    <p id="105">加深层次后的<i>Basenet</i>_<i>B</i>准确率相较<i>Basenet</i>_<i>A</i>有所降低, 这是因为由于训练数据集样本数量少, 所以出现了过拟合现象。实验2) 部分在训练数据进行数据增强后, 对<i>Basenet</i>_<i>B</i>与优化后的<i>FRnet</i>_<i>B</i>进行重新的训练与对比。</p>
                </div>
                <div class="p1">
                    <p id="106">2) 对训练集做数据增强后的实验效果。</p>
                </div>
                <div class="p1">
                    <p id="107">本实验部分进行了25个<i>epoch</i>的实验, 分别以0.001和0.000 1的学习率进行了10个<i>epoch</i>的实验, 以0.000 01的学习率进行了5个<i>epoch</i>的实验。训练数据进行了翻转, 扩充等增强。实验结果如图8及表5所示。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 数据增强后卷积块结构优化实验准确率曲线" src="Detail/GetImg?filename=images/JSJY201906010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 数据增强后卷积块结构优化实验准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Accuracy curves of convolution block optimization</i><i>experiment after data enhancement</i></p>

                </div>
                <div class="p1">
                    <p id="109">实验结果表明, 对数据进行增强后, <i>Basenet</i>与<i>FRnet</i>准确率均有所提升。这两次实验中, <i>FRnet</i>的准确率一直略高于<i>Basenet</i>, 表明本文所提结构在对模型进行压缩的同时, 不会降低模型的准确率, 且具有良好的抗拟合性能。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit"><b>表</b>5 <b>数据增强后卷积块结构优化实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 5 <i>Experimental results comparison of convolution block</i><i>optimization after data enhancement</i></p>
                    <p class="img_note"></p>
                    <table id="110" border="1"><tr><td><br />模型</td><td>训练<br />时间/<i>s</i></td><td>模型体<br />积大小/<i>KB</i></td><td>每百张<br />预测时间/<i>s</i></td><td>测试集最<br />大准确率/%</td></tr><tr><td><br /><i>Basenet</i>_<i>B</i></td><td>3 313</td><td>1 228.8</td><td>0.131 1</td><td>77.68</td></tr><tr><td><br /><i>FRnet</i>_<i>B</i></td><td>3 225</td><td>317.1</td><td>0.075 9</td><td>78.69</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">3.2.3 不同数量<i>FR</i>-<i>unit</i>叠加的对比实验</h4>
                <div class="p1">
                    <p id="112">本次实验通过针对不同输出通道数的<i>FR</i>-<i>unit</i>结构进行比较, 测试<i>FR</i>-<i>unit</i>输出通道数与叠加次数对网络运行速度和预测准确率的影响。实验所用网络如表6所示。以<i>FRlayer</i>2层为例, <i>C</i>1网络为2个输出为32的<i>FR</i>-<i>unit</i>叠加, <i>C</i>2网络为4个输出为16的<i>FR</i>-<i>unit</i>叠加, <i>C</i>3网络为8个输出为8的<i>FR</i>-<i>unit</i>叠加。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit"><b>表</b>6 <b>实验</b>3<b>模型参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 6 <i>Model parameters in experiment</i> 3</p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td><br />层名</td><td><i>C</i>1网络参数</td><td><i>C</i>2网络参数</td><td><i>C</i>3网络参数</td></tr><tr><td><br /><i>Conv</i>1</td><td>3, 32, 3×3, 1</td><td>3, 32, 3×3, 1</td><td>3, 32, 3×3, 1</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td>2×2, 2</td><td>2×2, 2</td></tr><tr><td><br /><i>FRlayer</i>1</td><td>{16, 3×3, 1}×2</td><td>{8, 3×3, 1}×4</td><td>{4, 3×3, 1}×8</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td>2×2, 2</td><td>2×2, 2</td></tr><tr><td><br /><i>FRlayer</i>2</td><td>{32, 3×3, 1}×2</td><td>{16, 3×3, 1}×4</td><td>{8, 3×3, 1}×8</td></tr><tr><td><br /><i>MaxPool</i></td><td>2×2, 2</td><td>2×2, 2</td><td>2×2, 2</td></tr><tr><td><br /><i>FC</i></td><td>2 048, 10</td><td>2 048, 10</td><td>2 048, 10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="114">本实验部分进行了10个<i>epoch</i>的实验, 分别以0.001和0.000 1的学习率进行了5个<i>epoch</i>的实验。训练数据没做增强。实验结果如图9及表7所示。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同FR-unit叠加实验准确率曲线" src="Detail/GetImg?filename=images/JSJY201906010_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同<i>FR</i>-<i>unit</i>叠加实验准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 9 <i>Accuracy curves of superposition</i><i>experiments with different FR</i>-<i>unit number</i></p>

                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表</b>7 <b>不同</b><i>FR</i>-<i>unit</i><b>叠加实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 7 <i>Experimental results comparison of superposition</i><i>experiments with different FR</i>-<i>unit number</i></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />网络</td><td>训练<br />时间/<i>s</i></td><td>模型体积<br />大小/<i>KB</i></td><td>每百张图<br />预测时间/<i>s</i></td><td>测试集最大<br />准确率/%</td></tr><tr><td><br /><i>C</i>1</td><td>1 261</td><td>317.1</td><td>0.080 3</td><td>75.11</td></tr><tr><td><br /><i>C</i>2</td><td>1 516</td><td>340.8</td><td>0.147 0</td><td>74.90</td></tr><tr><td><br /><i>C</i>3</td><td>2 101</td><td>353.5</td><td>0.238 1</td><td>75.07</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">对实验结果数据分析可以看出, 即使使用多次较少输出通道的<i>FR</i>-<i>unit</i>叠加结构, 准确率也并没有显著的变化, 但是模型的训练时间与预测时间却更长, 模型的体积也更大, 所以在使用<i>FR</i>-<i>unit</i>对模型进行优化时, 还是建议使用等数量的<i>FR</i>-<i>unit</i>来优化等数量卷积层数的卷积块。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">3.3 <b>对</b><i>VGG</i><b>和</b><i>Resnet</i><b>的优化实验</b></h4>
                <h4 class="anchor-tag" id="119" name="119">3.3.1 使用<i>FR</i>-<i>unit</i>对<i>VGG</i>进行优化</h4>
                <div class="p1">
                    <p id="120">本次实验使用<i>FR</i>-<i>unit</i>对<i>VGG</i>16网络进行了优化实验。为适应<i>CIFAR</i>10数据集, 对<i>VGG</i>16模型结构作了些许调整, 去掉了原模型中的第五个池化层, 并将第一个全连接层的输入和输出均改为了2 048。<i>VGG</i>网络参数与其对应的<i>FR</i>_<i>VGG</i>网络参数如表8所示, 表中:<i>Conv</i>表示普通卷积操作;<i>FR</i>-<i>unit</i>表示本文提出的特征复用卷积单元;{}内分别表示输入通道、输出通道、卷积核大小、卷积步长与边界扩展。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表</b>8 <i>VGG</i><b>优化实验网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 8 <i>Network parameters in VGG optimization experiments</i></p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br />层名</td><td><i>VGG</i>网络</td><td><i>FR</i>_<i>VGG</i>网络</td></tr><tr><td rowspan="2"><br /><i>Block</i>1</td><td><br /><i>Conv</i>{3, 64, 3, 1, 1}</td><td><i>Conv</i>{3, 64, 3, 1, 1}</td></tr><tr><td><br /><i>Conv</i>{64, 64, 3, 1, 1}</td><td><i>Conv</i>{64, 64, 3, 1, 1}</td></tr><tr><td><br /><i>Pool</i>1</td><td><i>MaxPool</i></td><td><i>MaxPool</i></td></tr><tr><td rowspan="2"><br /><i>Block</i>2</td><td><br /><i>Conv</i>{128, 128, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{64, 32, 3, 1, 1}</td></tr><tr><td><br /><i>Conv</i>{128, 128, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{96, 32, 3, 1, 1}</td></tr><tr><td><br /><i>Pool</i>2</td><td><i>MaxPool</i></td><td><i>MaxPool</i></td></tr><tr><td rowspan="4"><br /><i>Block</i>3</td><td><br /><i>Conv</i>{128, 256, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{128, 32, 3, 1, 1}</td></tr><tr><td><br /><i>Conv</i>{256, 256, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{160, 32, 3, 1, 1}</td></tr><tr><td><br /><i>Conv</i>{256, 256, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{192, 32, 3, 1, 1}</td></tr><tr><td><br /></td><td><i>FR</i>-<i>unit</i>{224, 32, 3, 1, 1}</td></tr><tr><td><br /><i>Pool</i>3</td><td><i>MaxPool</i></td><td><i>MaxPool</i></td></tr><tr><td rowspan="4"><br /><i>Block</i>4</td><td><br /><i>Conv</i>{256, 512, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{256, 64, 3, 1, 1}</td></tr><tr><td><br /><i>Conv</i>{512, 512, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{320, 64, 3, 1, 1}</td></tr><tr><td><br /><i>Conv</i>{512, 512, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{384, 64, 3, 1, 1}</td></tr><tr><td><br /></td><td><i>FR</i>-<i>unit</i>{448, 64, 3, 1, 1}</td></tr><tr><td><br /><i>Pool</i>4</td><td><i>MaxPool</i></td><td><i>MaxPool</i></td></tr><tr><td rowspan="5"><br /><i>Block</i>5</td><td><br /><i>Conv</i>{512, 512, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{512, 128, 3, 1, 1}</td></tr><tr><td><br /><i>Conv</i>{512, 512, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{640, 128, 3, 1, 1}</td></tr><tr><td><br /><i>Conv</i>{512, 512, 3, 1, 1}</td><td><i>FR</i>-<i>unit</i>{768, 128, 3, 1, 1}</td></tr><tr><td><br /></td><td><i>FR</i>-<i>unit</i>{896, 128, 3, 1, 1}</td></tr><tr><td><br /></td><td><i>Conv</i> (1024, 512, 1, 1) </td></tr><tr><td rowspan="3"><br /><i>FC</i></td><td><br /><i>FC</i>- 2 048</td><td><i>FC</i>- 2 048</td></tr><tr><td><br /><i>FC</i>- 4 096</td><td><i>FC</i>- 4 096</td></tr><tr><td><br /><i>FC</i>- 10</td><td><i>FC</i>- 10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="122">本实验部分进行了150个<i>epoch</i>的实验, 以0.001的学习率进行了100个<i>epoch</i>的实验, 以0.000 1的学习率进行了50个<i>epoch</i>的实验。训练时进行了翻转、扩充等数据增强操作。实验结果如图10～11及表9所示。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_123.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 VGG优化实验准确率曲线" src="Detail/GetImg?filename=images/JSJY201906010_123.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 <i>VGG</i>优化实验准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_123.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 10 <i>Accuracy curves of VGG optimization experiments</i></p>

                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 VGG优化实验损失曲线" src="Detail/GetImg?filename=images/JSJY201906010_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 <i>VGG</i>优化实验损失曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 11 <i>Loss curves of VGG optimization experiments</i></p>

                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表</b>9 <i>VGG</i><b>优化实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 9 <i>Experimental results comparison of VGG optimization experiments</i></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td><br />网络</td><td>模型体积<br />大小/ <i>MB</i></td><td>卷积层<br />参数量</td><td>每百张图<br />预测时间/<i>s</i></td><td>测试集最大<br />准确率/%</td></tr><tr><td><br /><i>VGG</i></td><td>159.8</td><td>1.47<i>E</i>+07</td><td>1.624</td><td>90.61</td></tr><tr><td><br /><i>FR</i>_<i>VGG</i></td><td>120.5</td><td>4.30<i>E</i>+06</td><td>0.706</td><td>91.31</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="126">其中<i>VGG</i>网络参数量主要集中在全连接层, 因此使用<i>FR</i>-<i>unit</i>对卷积层优化后, 并没有对模型整体的体积有极大改善, 模型体积只缩小到原先的75.4%。但是, 计算量主要集中在卷积层, 优化后模型计算量显著减小, 模型的预测时间缩短为原先的43.5%。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.3.2 使用<i>FR</i>-<i>unit</i>对<i>Resnet</i>进行优化</h4>
                <div class="p1">
                    <p id="128">本次实验使用<i>FR</i>-<i>unit</i>对<i>Resnet</i>18网络进行了优化实验。为适应<i>CIFAR</i>10数据集, 对<i>Resnet</i>18结构作了些许调整。<i>Resnet</i>网络参数与其对应的<i>FR</i>_<i>Resnet</i>网络参数如表10所示。表中[]内表示一个<i>Resnet</i>的残差单元, <i>Conv</i>表示普通卷积操作。<i>FR</i>-<i>unit</i>表示本文提出的特征复用卷积单元。1×1、3×3表示卷积核大小, 64、128等表示输出通道数。</p>
                </div>
                <div class="area_img" id="129">
                    <p class="img_tit"><b>表</b>10 <i>Resnet</i><b>优化实验模型及其参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 10 <i>Resnet optimization experimental model and its parameters</i></p>
                    <p class="img_note"></p>
                    <table id="129" border="1"><tr><td><br />层名</td><td><i>Resnet</i></td><td><i>FR</i>_<i>Resnet</i></td></tr><tr><td><br /><i>Conv</i></td><td colspan="2"><i>Conv</i>3×3, 64</td></tr><tr><td><br /><i>Block</i>1</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>6</mn><mn>4</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>2</mn></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>3</mn><mn>2</mn></mtd></mtr><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>3</mn><mn>2</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>1</mn><mo>×</mo><mn>1</mn><mo>, </mo><mn>6</mn><mn>4</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>2</mn></mrow></math></td></tr><tr><td><br /><i>Block</i>2</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>3</mn><mn>2</mn></mtd></mtr><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>3</mn><mn>2</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>1</mn><mo>×</mo><mn>1</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable></math></td></tr><tr><td><br /><i>Block</i>3</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>6</mn><mn>4</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>1</mn><mo>×</mo><mn>1</mn><mo>, </mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable></math></td></tr><tr><td><br /><i>Block</i>4</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr><mtr><mtd><mtext>F</mtext><mtext>R</mtext><mo>-</mo><mtext>u</mtext><mtext>n</mtext><mtext>i</mtext><mtext>t</mtext><mn>3</mn><mo>×</mo><mn>3</mn><mo>, </mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr><mtr><mtd><mtext>C</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><mn>1</mn><mo>×</mo><mn>1</mn><mo>, </mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr></mtable><mo>]</mo></mrow></mtd></mtr></mtable></math></td></tr><tr><td rowspan="2"><br /><i>FC</i></td><td colspan="2"><br /><i>Average pool</i></td></tr><tr><td colspan="2"><br /><i>FC</i>- 512- 10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="130">本实验部分进行了250个<i>epoch</i>的实验, 以0.001的学习率进行了100个<i>epoch</i>的实验, 以0.000 1的学习率进行了100个<i>epoch</i>的实验, 以0.000 01的学习率进行了50个<i>epoch</i>的实验。训练时进行了翻转、扩充等数据增强操作, 实验结果如图12～13及表11所示。</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 Resnet优化实验准确率曲线" src="Detail/GetImg?filename=images/JSJY201906010_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 <i>Resnet</i>优化实验准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 12 <i>Accuracy curves of Resnet optimization experiments</i></p>

                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906010_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 Resnet优化实验损失曲线" src="Detail/GetImg?filename=images/JSJY201906010_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 <i>Resnet</i>优化实验损失曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906010_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 13 <i>Loss curves of Resnet optimization experiments</i></p>

                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表</b>11 <i>Resnet</i><b>网络优化前后的实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 11 <i>Comparison of experimental results of Resnet network</i><i>before and after optimization</i></p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td><br />网络</td><td>模型体积<br />大小/<i>MB</i></td><td>每百张图<br />预测时间/<i>s</i></td><td>测试集最<br />大准确率/%</td></tr><tr><td><br /><i>Resnet</i></td><td>44.8</td><td>2.768</td><td>93.06</td></tr><tr><td><br /><i>FR</i>_<i>Resnet</i></td><td>23.8</td><td>1.686</td><td>93.29</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="134">其中, <i>Resnet</i>网络为全卷积网络;使用<i>FR</i>-<i>unit</i>对网络优化后, 模型体积缩小为原先的53.1%, 预测时间缩短为原先的60.9%。</p>
                </div>
                <h3 id="135" name="135" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="136">本文提出了一种针对卷积层的特征复用的优化结构, 即:在不降低总特征提取数量的同时, 通过对已提取特征的重复使用, 降低卷积层的卷积操作数量, 来达到对卷积神经网络的加速和模型压缩的目的。通过实验得到如下结果:</p>
                </div>
                <div class="p1">
                    <p id="137">结论1 本文所提出的<i>FR</i>-<i>unit</i>结构对卷积神经网络卷积层有良好的压缩作用。优化后<i>VGG</i>与<i>Resnet</i>模型的体积分别缩小为优化前的75.4%和53.1%;预测时间分别缩短为优化前的43.5%和60.9%。</p>
                </div>
                <div class="p1">
                    <p id="138">结论2 在使用<i>FR</i>-<i>unit</i>对网络进行优化时, 若选用多次较小输出的<i>FR</i>-<i>unit</i>进行叠加虽然可以使网络模型更深, 但是对于模型的性能并没有明显的提升, 因此, 还是建议选用等量的<i>FR</i>-<i>unit</i>对模型的卷积层进行优化。</p>
                </div>
                <div class="p1">
                    <p id="139">结论3 使用<i>FR</i>-<i>unit</i>对网络进行优化后, 模型性能并没有降低。主要原因是没有改变模型整体的结构:第一, 虽然对网络中结构进行了替换, 但是提取的特征总数没有发生改变;第二, 该方法并未降低网络的深度。</p>
                </div>
                <div class="p1">
                    <p id="140">结论4 优化后的模型具有良好的抗拟合性能。这是由于<i>FR</i>-<i>unit</i>在训练时, 会结合低层的特征进行学习, 而不只依赖于最高阶的一层。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="161">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[1]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.Piscataway, NJ:IEEE, 2012:1097-1105.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep multilingual convolutional neural networks for LVCSR">

                                <b>[2]</b>SERCU T, PUHRSCH C, KINGSBURY B, et al.Very deep multilingual convolutional neural networks for LVCSR[C]//Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2016:4955-4959.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[3]</b>SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1-9.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[4]</b>HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:770-778.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436-444.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">

                                <b>[6]</b>ZEILER M D, FERGUS R.Visualizing and understanding convolutional networks[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:818-833.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous detection and segmentation">

                                <b>[7]</b>HARIHARAN B, ARBELAEZ P, ARBELAEZ R, et al.Simultaneous detection and segmentation[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:297-312.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[8]</b>GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Compact Image Representation for Visual Tracking">

                                <b>[9]</b>WANG N, YEUNG D Y.Learning a deep compact image representation for visual tracking[C]//Proceedings of the 26th International Conference on Neural Information Processing Systems.Piscataway, NJ:IEEE, 2013:809-817.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MTE2MzZWYnJBTHo3QmRyRzRIOWJNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2g=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251. (ZHOU F Y, JIN L P, DONG J.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251.) 
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201802002&amp;v=MDE5NTdCdEdGckNVUjdxZlp1WnNGeS9oVmJyQU55ZlRiTEc0SDluTXJZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>雷杰, 高鑫, 宋杰, 等.深度网络模型压缩综述[J].软件学报, 2018, 29 (2) :251-266. (LEI J, GAO X, SONG J, et al.Survey of deep neural network model compression[J].Journal of Software, 2018, 29 (2) :251-266.) 
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design Issues in Division and Other Floating-Point Operations">

                                <b>[12]</b>OBERMAN S F, FLYNN M J.Design issues in division and other floating-point operations[J].IEEE Transactions on Computers, 1997, 46 (2) :154-161.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural-network feature selector">

                                <b>[13]</b>SETIONO R, LIU H.Neural-network feature selector[J].IEEETransactions on Neural Networks, 1997, 8 (3) :654-662.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXJS201819013&amp;v=MDcxMjBGckNVUjdxZlp1WnNGeS9oVmJyQUxqWEJmYkc0SDluTnBvOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>孔英会, 朱成诚, 车辚辚.复杂背景下基于Mobile Nets的花卉识别与模型剪枝[J].科学技术与工程, 2018, 18 (19) :84-88. (KONG Y H, ZHU C C, CHE L L.Flower recognition in complex background and model pruning based on Mobile Nets[J].Science Technology and Engineering, 2018, 18 (19) :84-88.) 
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network trimming:a datadriven neuron pruning approach towards efficient deep architectures">

                                <b>[15]</b>HU H, PENG R, TAI Y W, et al.Network trimming:a datadriven neuron pruning approach towards efficient deep architectures[J].ar Xiv Preprint, 2016, 2016:ar Xiv.1607.03250.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Point estimates for probability moments">

                                <b>[16]</b>ROSENBLUETH E.Point estimates for probability moments[J].Proceedings of the National Academy of Sciences of the United States of America, 1975, 72 (10) :3812-3814.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002469547&amp;v=MjEwNjM0SHRIT3E0bE1ZZThJWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJsVkx2T0pWWT1OajdCYXJP&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>GOLUB G H, REINSCH C.Singular value decomposition and least squares solutions[J].Numerische Mathematik, 1970, 14 (5) :403-420.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep networks with stochastic depth">

                                <b>[18]</b>HUANG G, SUN Y, LIU Z, et al.Deep networks with stochastic depth[C]//Proceedings of the 2016 14th European Conference on Computer Vision.Berlin:Springer, 2016:646-661.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                HUANG G, LIU Z, van der MAATEN L, et al.Densely connected convolutional networks[C]//Proceedings of the 2017 IEEEConference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2261-2269.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906010" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906010&amp;v=MjEzODlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hWYnJBTHo3QmQ3RzRIOWpNcVk5RVpJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
