<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136766112315000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905035%26RESULT%3d1%26SIGN%3d96QMgDvzUBZpL%252bPig4KSNMG8w1g%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905035&amp;v=MDgwNzMzenFxQnRHRnJDVVI3cWZadVpzRnlEblVyN1BMejdCZDdHNEg5ak1xbzlHWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="1.1 SRCNN&lt;b&gt;算法&lt;/b&gt;">1.1 SRCNN<b>算法</b></a></li>
                                                <li><a href="#66" data-title="1.2 MC-SRCNN&lt;b&gt;算法&lt;/b&gt;">1.2 MC-SRCNN<b>算法</b></a></li>
                                                <li><a href="#72" data-title="1.3 &lt;i&gt;SRGAN&lt;/i&gt;&lt;b&gt;算法&lt;/b&gt;">1.3 <i>SRGAN</i><b>算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="2 改进的图像超分辨率重建算法 ">2 改进的图像超分辨率重建算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#76" data-title="2.1 &lt;b&gt;改进思想&lt;/b&gt;">2.1 <b>改进思想</b></a></li>
                                                <li><a href="#78" data-title="2.2 &lt;b&gt;网络结构&lt;/b&gt;">2.2 <b>网络结构</b></a></li>
                                                <li><a href="#93" data-title="2.3 &lt;b&gt;残差网络结构&lt;/b&gt;">2.3 <b>残差网络结构</b></a></li>
                                                <li><a href="#97" data-title="2.4 &lt;b&gt;亚像素卷积层&lt;/b&gt;">2.4 <b>亚像素卷积层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 SRCNN算法网络框架">图1 SRCNN算法网络框架</a></li>
                                                <li><a href="#68" data-title="图2 &lt;i&gt;MC&lt;/i&gt;-&lt;i&gt;SRCNN&lt;/i&gt;算法的网络框架">图2 <i>MC</i>-<i>SRCNN</i>算法的网络框架</a></li>
                                                <li><a href="#69" data-title="图3 本文算法搭建的卷积神经网络框架">图3 本文算法搭建的卷积神经网络框架</a></li>
                                                <li><a href="#96" data-title="图4 残差模块的结构">图4 残差模块的结构</a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同算法在&lt;/b&gt;Set5&lt;b&gt;数据集上的&lt;/b&gt;PSNR&lt;b&gt;和&lt;/b&gt;SSIM&lt;b&gt;结果&lt;/b&gt;"><b>表</b>1 <b>不同算法在</b>Set5<b>数据集上的</b>PSNR<b>和</b>SSIM<b>结果</b></a></li>
                                                <li><a href="#114" data-title="图5 不同算法在baboon和lenna图像上的重建结果">图5 不同算法在baboon和lenna图像上的重建结果</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同算法在&lt;/b&gt;Set14&lt;b&gt;数据集上的&lt;/b&gt;PSNR (&lt;b&gt;放大倍数为&lt;/b&gt;3) dB"><b>表</b>2 <b>不同算法在</b>Set14<b>数据集上的</b>PSNR (<b>放大倍数为</b>3) dB</a></li>
                                                <li><a href="#122" data-title="图6 图像预处理操作与改进的网络结构对图像重建结果的影响">图6 图像预处理操作与改进的网络结构对图像重建结果的影响</a></li>
                                                <li><a href="#125" data-title="图7 当放大倍数为3时各超分辨率算法在 Set14数据集上的PSNR值与运算时间的关系">图7 当放大倍数为3时各超分辨率算法在 Set14数据集上的PSNR值与运算时间的关系</a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同算法在不同数据集上的平均&lt;/b&gt;PSNR (&lt;b&gt;放大倍数为&lt;/b&gt;3) dB"><b>表</b>3 <b>不同算法在不同数据集上的平均</b>PSNR (<b>放大倍数为</b>3) dB</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="180">


                                    <a id="bibliography_1" title=" 孙旭, 李晓光, 李嘉锋, 等.基于深度学习的图像超分辨率复原研究进展[J].自动化学报, 2017, 43 (5) :697-709. (SUN X, LI X G, LI J F, et al.Review on deep learning based image super-resolution restoration algorithms [J].Acta Automatica Sinica, 2017, 43 (5) :697-709.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201705002&amp;v=MTIwMDJMZlliRzRIOWJNcW85RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VcjdPS0M=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         孙旭, 李晓光, 李嘉锋, 等.基于深度学习的图像超分辨率复原研究进展[J].自动化学报, 2017, 43 (5) :697-709. (SUN X, LI X G, LI J F, et al.Review on deep learning based image super-resolution restoration algorithms [J].Acta Automatica Sinica, 2017, 43 (5) :697-709.) 
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_2" title=" 苏衡, 周杰, 张志浩.超分辨率图像重建方法综述[J].自动化学报, 2013, 39 (8) :1202-1213. (SU H, ZHOU J, ZHANG Z H.Survey of super-resolution image reconstruction methods [J].Acta Automatica Sinica, 2013, 39 (8) :1202-1213.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201308005&amp;v=MDU1MzVGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyN09LQ0xmWWJHNEg5TE1wNDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         苏衡, 周杰, 张志浩.超分辨率图像重建方法综述[J].自动化学报, 2013, 39 (8) :1202-1213. (SU H, ZHOU J, ZHANG Z H.Survey of super-resolution image reconstruction methods [J].Acta Automatica Sinica, 2013, 39 (8) :1202-1213.) 
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_3" title=" 李浪宇, 苏卓, 石晓红, 等.图像超分辨率重建中的细节互补卷积模型[J].中国图象图形学报, 2018, 23 (4) :572-582. (LI L Y, SU Z, SHI X H, et al.Mutual-detail convolution model for image super-resolution reconstruction [J].Journal of Image and Graphics, 2018, 23 (4) :572-582.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201804012&amp;v=MDc4MDc0SDluTXE0OUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXI3T1B5cmZiTEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         李浪宇, 苏卓, 石晓红, 等.图像超分辨率重建中的细节互补卷积模型[J].中国图象图形学报, 2018, 23 (4) :572-582. (LI L Y, SU Z, SHI X H, et al.Mutual-detail convolution model for image super-resolution reconstruction [J].Journal of Image and Graphics, 2018, 23 (4) :572-582.) 
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_4" title=" KIM J, LEE J K, LEE K M.Accurate image super-resolution using very deep convolutional networks [C]// Proceedings of the 2016 IEEE Conference on Computer on Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1646-1654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate Image Super-Resolution Using Very Deep Convolutional Networks">
                                        <b>[4]</b>
                                         KIM J, LEE J K, LEE K M.Accurate image super-resolution using very deep convolutional networks [C]// Proceedings of the 2016 IEEE Conference on Computer on Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1646-1654.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_5" title=" TIAN J, MA K K.A survey on super-resolution imaging[J].Signal, Image and Video Processing, 2011, 5 (3) :329-342." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD120828002575&amp;v=MDY3Mjc2SHRuT3A0OUZadTRJQ1JNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5uVTdmS0lWOFNOajdCYXJL&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         TIAN J, MA K K.A survey on super-resolution imaging[J].Signal, Image and Video Processing, 2011, 5 (3) :329-342.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_6" title=" YANG C Y, MA C, YANG M H.Single-image super-resolution:a benchmark [C]// Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:372-386." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution:a benchmark">
                                        <b>[6]</b>
                                         YANG C Y, MA C, YANG M H.Single-image super-resolution:a benchmark [C]// Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:372-386.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_7" title=" YANG J C, WRIGHT J, HUANG T S, et al.Image super-resolution via sparse representation [J].IEEE Transactions on Image Processing, 2010, 19 (11) :2861-2873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">
                                        <b>[7]</b>
                                         YANG J C, WRIGHT J, HUANG T S, et al.Image super-resolution via sparse representation [J].IEEE Transactions on Image Processing, 2010, 19 (11) :2861-2873.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_8" title=" 李云飞, 符冉迪, 金炜, 等.多通道卷积的图像超分辨率方法[J].中国图象图形学报, 2017, 22 (12) :1690-1700. (LI Y F, FU R D, JIN W, et al.Image super-resolution using multi-channel convolution [J].Journal of Image and Graphics, 2017, 22 (12) :1690-1700.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201712005&amp;v=MTUwMTFOclk5RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VcjdPUHlyZmJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         李云飞, 符冉迪, 金炜, 等.多通道卷积的图像超分辨率方法[J].中国图象图形学报, 2017, 22 (12) :1690-1700. (LI Y F, FU R D, JIN W, et al.Image super-resolution using multi-channel convolution [J].Journal of Image and Graphics, 2017, 22 (12) :1690-1700.) 
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_9" title=" ZEYDE R, ELAD M, PROTTER M.On single image scale-up using sparse-representations [C]// Proceedings of the 2010 International Conference on Curves and Surfaces, LNCS 6920.Berlin:Springer, 2010:711-730." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On single image scale-up using sparse representations">
                                        <b>[9]</b>
                                         ZEYDE R, ELAD M, PROTTER M.On single image scale-up using sparse-representations [C]// Proceedings of the 2010 International Conference on Curves and Surfaces, LNCS 6920.Berlin:Springer, 2010:711-730.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_10" title=" TIMOFTE R, DE V, GOOL L V.Anchored neighborhood regression for fast example-based super-resolution [C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2013:1920-1927." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anchored neighborhood regression for fast example-based super-resolution">
                                        <b>[10]</b>
                                         TIMOFTE R, DE V, GOOL L V.Anchored neighborhood regression for fast example-based super-resolution [C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2013:1920-1927.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_11" title=" TIMOFTE R, ROTHE R, GOOL L V.A&lt;sup&gt;+&lt;/sup&gt;:adjusted anchored neighborhood regression for fast super-resolution [C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2015:111-126." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A+:adjusted anchored neighborhood regression for fast super-resolution">
                                        <b>[11]</b>
                                         TIMOFTE R, ROTHE R, GOOL L V.A&lt;sup&gt;+&lt;/sup&gt;:adjusted anchored neighborhood regression for fast super-resolution [C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2015:111-126.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_12" title=" CHANG H, XIONG Y, YEUNG D Y.Super-resolution through neighbor embedding [C]// Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2004:275-282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution throughneighbor embedding">
                                        <b>[12]</b>
                                         CHANG H, XIONG Y, YEUNG D Y.Super-resolution through neighbor embedding [C]// Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2004:275-282.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_13" title=" DONG C, CHEN C L, HE K, et al.Learning a deep convolutional network for image super-resolution [C]// ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:184-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Convolutional Network for Image Super-Resolution">
                                        <b>[13]</b>
                                         DONG C, CHEN C L, HE K, et al.Learning a deep convolutional network for image super-resolution [C]// ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:184-199.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_14" title=" 徐冉, 张俊格, 黄凯奇.利用双通道卷积神经网络的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (5) :556-564. (XU R, ZHANG J G, HUANG K Q.Image super-resolution using two-channel convolutional neural networks [J].Journal of Image and Graphics, 2016, 21 (5) :556-564.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201605003&amp;v=MjI5NDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXI3T1B5cmZiTEc0SDlmTXFvOUZaNFFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         徐冉, 张俊格, 黄凯奇.利用双通道卷积神经网络的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (5) :556-564. (XU R, ZHANG J G, HUANG K Q.Image super-resolution using two-channel convolutional neural networks [J].Journal of Image and Graphics, 2016, 21 (5) :556-564.) 
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_15" title=" 刘娜, 李翠华.基于多层卷积神经网络学习的单帧图像超分辨率重建方法[J].中国科技论文, 2015, 10 (2) :201-206. (LIU N, LI C H.Single image super-resolution reconstruction via deep convolutional neural network [J].China Sciencepaper, 2015, 10 (2) :201-206.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZKZX201502017&amp;v=MDI3ODlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VcjdPUHliUmRyRzRIOVRNclk5RVk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         刘娜, 李翠华.基于多层卷积神经网络学习的单帧图像超分辨率重建方法[J].中国科技论文, 2015, 10 (2) :201-206. (LIU N, LI C H.Single image super-resolution reconstruction via deep convolutional neural network [J].China Sciencepaper, 2015, 10 (2) :201-206.) 
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_16" title=" YOUM G Y, BAE S H, KIM M.Image super-resolution based on convolution neural networks using multi-channel input [C]// Proceedings of the 2006 IEEE 12th Image, Video, and Multidimensional Signal Processing Workshop.Piscataway, NJ:IEEE, 2016:1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution based on convolution neural networks using multi-channel input">
                                        <b>[16]</b>
                                         YOUM G Y, BAE S H, KIM M.Image super-resolution based on convolution neural networks using multi-channel input [C]// Proceedings of the 2006 IEEE 12th Image, Video, and Multidimensional Signal Processing Workshop.Piscataway, NJ:IEEE, 2016:1-5.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_17" title=" SHI W, CABALLERO J, HUSZAR F, et al.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1874-1883." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network.&amp;quot;">
                                        <b>[17]</b>
                                         SHI W, CABALLERO J, HUSZAR F, et al.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1874-1883.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_18" title=" LEDIG C, THEIS L, HUSZAR F, et al.Photo-realistic single image super-resolution using a generative adversarial network [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:105-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network">
                                        <b>[18]</b>
                                         LEDIG C, THEIS L, HUSZAR F, et al.Photo-realistic single image super-resolution using a generative adversarial network [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:105-114.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_19" title=" DONG C, CHEN C L, HE K, et al.Image super-resolution using deep convolutional networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">
                                        <b>[19]</b>
                                         DONG C, CHEN C L, HE K, et al.Image super-resolution using deep convolutional networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_20" >
                                        <b>[20]</b>
                                     JIA Y Q, SHELHAMER E, DONAHUE J, et al.Caffe:convolutional architecture for fast feature embedding [C]// Proceedings of the 22nd ACM International Conference on Multimedia.New York:ACM, 2014:675-678.</a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_21" title=" MEGHA G, YASHPAL L, VIVEK L.Analytical relation &amp;amp; comparison of PSNR and SSIM on babbon image and human eye perception using Matlab [J].International Journal of Advanced Research in Engineering and Applied Sciences, 2015, 4 (5) :108-119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analytical relation &amp;amp; comparison of PSNR and SSIM on babbon image and human eye perception using Matlab">
                                        <b>[21]</b>
                                         MEGHA G, YASHPAL L, VIVEK L.Analytical relation &amp;amp; comparison of PSNR and SSIM on babbon image and human eye perception using Matlab [J].International Journal of Advanced Research in Engineering and Applied Sciences, 2015, 4 (5) :108-119.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-11-28 11:01</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1440-1447 DOI:10.11772/j.issn.1001-9081.2018091887            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进卷积神经网络的单幅图像超分辨率重建方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%9C%88%E5%B3%B0&amp;code=10558594&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘月峰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%B6%B5%E6%99%B0&amp;code=40946711&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨涵晰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%94%A1%E7%88%BD&amp;code=40946710&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蔡爽</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%99%A8%E8%8D%A3&amp;code=40946712&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张晨荣</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%86%85%E8%92%99%E5%8F%A4%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0161306&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">内蒙古科技大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>对于重建图像存在的边缘失真和纹理细节信息模糊的问题, 提出一种基于改进卷积神经网络 (CNN) 的图像超分辨率重建方法。首先在底层特征提取层以三种插值方法和五种锐化方法进行多种预处理操作, 并将只进行一次插值操作的图像和先进行一次插值后进行一次锐化的图像合并排列成三维矩阵;然后在非线性映射层将预处理后构成的三维特征映射作为深层残差网络的多通道输入, 以获取更深层次的纹理细节信息;最后在重建层为减少图像重建时间在网络结构中引入亚像素卷积来完成图像重建操作。在多个常用数据集上的实验结果表明, 与经典方法相比, 所提方法重建图像的纹理细节信息和高频信息能得到更好的恢复, 峰值信噪比 (PSNR) 平均增加0.23 dB, 结构相似性 (SSIM) 平均增加0.006 6。在保证图像重建时间的前提下, 所提方法更好地保持重建图像的纹理细节并减少图像边缘失真, 提升重建图像的性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%95%E5%B9%85%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单幅图像超分辨率重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多通道卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%9A%E5%83%8F%E7%B4%A0%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">亚像素卷积;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘月峰 (1977—) , 男, 内蒙古包头人, 副教授, 博士, 主要研究方向:深度学习、图像处理;电子邮箱liuyuefeng01035@163.com;
                                </span>
                                <span>
                                    杨涵晰 (1994—) , 女, 内蒙古包头人, 硕士研究生, 主要研究方向:深度学习、图像处理;;
                                </span>
                                <span>
                                    蔡爽 (1993—) , 女, 山东菏泽人, 硕士研究生, 主要研究方向:深度学习、入侵检测;;
                                </span>
                                <span>
                                    张晨荣 (1993—) , 男, 内蒙古巴彦淖尔人, 硕士研究生, 主要研究方向:深度学习、知识图谱。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>内蒙古自然科学基金资助项目 (2018MS06019);</span>
                    </p>
            </div>
                    <h1><b>Single image super-resolution reconstruction method based on improved convolutional neural network</b></h1>
                    <h2>
                    <span>LIU Yuefeng</span>
                    <span>YANG Hanxi</span>
                    <span>CAI Shuang</span>
                    <span>ZHANG Chenrong</span>
            </h2>
                    <h2>
                    <span>School of Information Engineering, Inner Mongolia University of Science & Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of edge distortion and fuzzy texture detail information in reconstructed images, an image super-resolution reconstruction method based on improved Convolutional Neural Network (CNN) was proposed. Firstly, various preprocessing operations were performed on the underlying feature extraction layer by three interpolation methods and five sharpening methods, and the images which were only subjected to one interpolation operation and the images which were sharpened after interpolation operation were arranged into a 3 D matrix. Then, the 3 D feature map formed by the preprocessing was used as the multi-channel input of a deep residual network in the nonlinear mapping layer to obtain deeper texture detail information. Finally, for reducing image reconstruction time, sub-pixel convolution was introduced into the reconstruction layer to complete image reconstruction operation. Experimental results on several common datasets show that the proposed method achieves better restored texture detail information and high-frequency information in the reconstructed image compared with the classical methods. Furthermore, the Peak Signal-to-Noise Ratio (PSNR) was increased by 0.23 dB on average, and the structural similarity was increased by 0.006 6 on average. The proposed method can better maintain the texture details of the reconstructed image and reduce the image edge distortion under the premise of ensuring the image reconstruction time, improving the performance of image reconstruction.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=single%20image%20super-resolution%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">single image super-resolution reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-channel%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-channel convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sub-pixel%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sub-pixel convolution;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LIU Yuefeng, born in 1977, Ph. D. , associate professor. His research interests include deep learning, image processing. ;
                                </span>
                                <span>
                                    YANG Hanxi, born in 1994, M. S. candidate. Her research interests include deep learning, image processing. ;
                                </span>
                                <span>
                                    CAI Shuang, born in 1993, M. S. candidate. Her research interests include deep learning, intrusion detection. ;
                                </span>
                                <span>
                                    ZHANG Chenrong, born in 1993, M. S. candidate. His research interests include deep learning, knowledge graph.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-10</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Inner Mongolia Natural Science Foundation (2018MS06019);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="46">近年来, 图像作为获取信息最为直接的途径, 在遥感、医疗、军事、公共安全、计算机视觉等诸多领域都有着重要的应用<citation id="222" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。单幅图像超分辨率重建 (Simple Image Super-Resolution, SISR) 技术的方法主要有基于插值的方法、基于重建的方法和基于学习的方法<citation id="224" type="reference"><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。基于学习的重建方法保存了图像更多的高频信息和细节信息, 且算法适应性和鲁棒性更好, 因此成为近年来的单幅图像超分辨率重建技术研究的热点<citation id="223" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="47">基于学习的超分辨率 (Super-Resolution, SR) 方法的基本思路是通过大量样本训练得到低分辨率图像 (Low Resolution, LR) 与高分辨率图像 (High Resolution, HR) 之间的映射关系, 并以此作为先验知识进行重建<citation id="225" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。Yang等<citation id="229" type="reference"><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>提出基于稀疏编码的图像超分辨算法, 对高、低分辨率的图像块进行联合训练得到相应的一个过完备字典, 通过这个字典进行高低分辨率图像块的稀疏关联重建图像。在此基础上, 李云飞等<citation id="226" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和Zeyde等<citation id="227" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>应用<i>K</i>-奇异值分解 (<i>K</i>-Singular Value Decomposition, <i>K</i>-SVD) 算法对字典的训练过程进行改进, 虽然提高了字典训练速度, 使重建图像在主客观评价指标上均有所改善, 但重建过程的计算复杂度较高, HR图像生成时间过长。为了做到图像的实时处理, Timofte等<citation id="230" type="reference"><link href="198" rel="bibliography" /><link href="200" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>提出锚点邻域回归 (Anchored Neighborhood Regression, ANR) 算法, 在最近邻域嵌入算法<citation id="228" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>上引入稀疏编码思想, 降低了算法的计算复杂度, 但是图像的细节纹理恢复效果较差, 重建质量仍需提高。</p>
                </div>
                <div class="p1">
                    <p id="48">近几年, 随着深度学习的不断发展, 2014年Dong等<citation id="231" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>在基于样本学习的SR算法基础上提出了基于卷积神经网络 (Convolutional Neural Network, CNN) 的图像超分辨率重建 (Super-Resolution CNN, SRCNN) 算法。SRCNN首次将CNN引入到SR任务中, 将传统SR算法的分步处理整合到一个深度学习模型中, 大幅简化了SR工作流程<citation id="232" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。SRCNN的提出证明将深度学习, 特别是CNN应用到SR任务中是非常合适的, 发展前景广阔。同传统的SR算法相比, SRCNN在SR性能上有不错的提升, 但SRCNN重建的HR图像仍存在纹理模糊的问题, SR性能有待进一步提高<citation id="233" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。在2016年, Youm等<citation id="234" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出基于多通道输入卷积神经网络的图像超分辨率算法 (Super-Resolution method using Multi-Channel-input CNN, MC-SRCNN) , 其输入图像是多通道的, 即输入图像包含了18种低分辨率图像, 能更好地保留图像的高频信息。Shi等<citation id="235" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了一种直接在低分辨率图像上提取特征信息, 从而高效重建得到高分辨率图像算法, 即有效的亚像素卷积神经网络 (Efficient Sub-Pixel Convolutional Neural network, ESPCN) , 其核心思想是亚像素卷积层, 获得了较好的效率。在2017年, Ledig等<citation id="236" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出将生成对抗网络 (Generative Adversarial Network, GAN) 应用在SR问题中, 因为传统的SR算法在放大倍数较小时的图像恢复效果较好, 但当图像的放大倍数在4以上时, 传统方法重建图像的细节信息与边缘过于平滑, 缺少视觉真实感, 因此基于生成对抗网络的超分辨率 (Super-Resolution using Generative Adversarial Network, SRGAN) 算法以牺牲峰值信噪比 (Peak Signal-to-Noise Ratio, PSNR) 来用GAN生成图像中的更多细节, 增加真实感。</p>
                </div>
                <div class="p1">
                    <p id="49">综上所述, 考虑到图像的重建性能和网络的训练时间以及图像恢复的真实感三方面的因素, 本文提出一种基于改进卷积神经网络的单幅图像超分辨率重建方法。该方法以由单幅低分辨率图像构成的三维特征矩阵作为神经网络的多通道输入, 减少图像预处理过程中的高频信息的丢失;然后通过深层残差网络, 提取图像中的多层次的细节信息, 以提高图像重建的真实感;最后为保证图像重建的时间且尽可能地缩短重建时间, 通过亚像素卷积层输出得到高分辨率图像。实验结果表明本文算法在保证训练时间的前提下, 获得了较好的单幅图像重建性能, 图像的边缘和细节信息的恢复效果更好。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="51" name="51">1.1 SRCNN<b>算法</b></h4>
                <div class="p1">
                    <p id="52">SRCNN是在单幅图像超分辨率重建研究中较早跨越传统算法的深度学习方法, 将传统SR算法的分步处理整合到一个深度学习模型中, 简化了SR工作流程<citation id="237" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。应用卷积神经网络, 通过学习输入和输出之间的特征映射关系, 实现了图像由低分辨率到高分辨率的重建过程。SRCNN网络框架如图1所示, 首先用双立方插值, 依据预先设计好的尺寸对一幅低分辨率图像进行简单的画质提升处理, 此举作为预处理操作, 然后将预处理后的图像表示为<b><i>Y</i></b>, 原始的高分辨率图像为<b><i>X</i></b>, 但为了简化表述, 仍将<b><i>Y</i></b>称为低分辨率图像, 实际上<b><i>Y</i></b>与<b><i>X</i></b>具有相同的图像尺寸。网络训练的目标主要是学习特征映射<i>F</i>, 使得<b><i>Y</i></b>通过映射<i>F</i> (<b><i>Y</i></b>) 尽可能地恢复到<b><i>X</i></b>的性能。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905035_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SRCNN算法网络框架" src="Detail/GetImg?filename=images/JSJY201905035_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SRCNN算法网络框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905035_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Network framework of SRCNN algorithm</p>

                </div>
                <div class="p1">
                    <p id="54">SRCNN算法主要分为3个部分:</p>
                </div>
                <div class="p1">
                    <p id="55">1) 图像块的底层特征提取与特征表示。即从低分辨率图像<b><i>Y</i></b>中提取图像块, 可以进行有重叠的特征提取;然后将每个提取的图像块表示为一个高维度的矢量, 这些矢量被表示为一系列的特征映射, 且令特征映射的数量等于高纬度矢量的维度。其计算公式如式 (1) 所示:</p>
                </div>
                <div class="p1">
                    <p id="56"><i>F</i><sub>1</sub> (<b><i>Y</i></b>) =max (0, <b><i>W</i></b><sub>1</sub>*<b><i>Y</i></b>+<b><i>B</i></b><sub>1</sub>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="57">其中:<b><i>W</i></b><sub>1</sub>和<b><i>B</i></b><sub>1</sub>表示滤波器和偏差;“*”表示卷积运算;<b><i>W</i></b><sub>1</sub>表示包含<i>n</i><sub>1</sub>个<i>c</i>×<i>f</i><sub>1</sub>×<i>f</i><sub>1</sub>的滤波器, <i>c</i>为图像包含的通道数量, <i>f</i><sub>1</sub>为滤波器空域大小, 即<b><i>W</i></b><sub>1</sub>对图像进行了<i>n</i><sub>1</sub>次卷积, 所使用卷积核为<i>c</i>×<i>f</i><sub>1</sub>×<i>f</i><sub>1</sub>, 该层输出<i>n</i><sub>1</sub>个特征映射;<b><i>B</i></b><sub>1</sub>是一个<i>n</i><sub>1</sub>维的向量, 它的每个元素对应一个滤波器, 激活函数使用ReLU (Rectified Linear Unit) , 即max (0, <b><i>x</i></b>) 。算法的第1) 部分表示为图1网络结构中的第一层网络, 提取到预处理图像中的<i>n</i><sub>1</sub>维特征, 作为第二层网络的输入。</p>
                </div>
                <div class="p1">
                    <p id="58">2) 非线性映射。非线性地将每个高维度的矢量映射到另一个高维度的矢量上, 每个被映射的矢量被表示为一个高分辨率图像块, 这些被映射的矢量表示为另一系列的特征映射。其计算公式如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="59"><i>F</i><sub>2</sub>=max (0, <b><i>W</i></b><sub>2</sub>*<i>F</i><sub>1</sub> (<b><i>Y</i></b>) +<b><i>B</i></b><sub>2</sub>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="60">其中:<b><i>B</i></b><sub>2</sub>是<i>n</i><sub>2</sub>维的向量。<b><i>W</i></b><sub>2</sub>包含<i>n</i><sub>2</sub>个<i>n</i><sub>1</sub>×<i>f</i><sub>2</sub>×<i>f</i><sub>2</sub>的滤波器。</p>
                </div>
                <div class="p1">
                    <p id="61">3) 图像重建。将以上步骤形成的高分辨率图像块再进行卷积运算, 重建形成最终的尽可能接近真实的高分辨率图像。其公式如式 (3) 所示:</p>
                </div>
                <div class="p1">
                    <p id="62"><i>F</i> (<b><i>Y</i></b>) =<b><i>W</i></b><sub>3</sub>*<i>F</i><sub>2</sub> (<b><i>Y</i></b>) +<b><i>B</i></b><sub>3</sub>      (3) </p>
                </div>
                <div class="p1">
                    <p id="63">其中:<b><i>B</i></b><sub>3</sub>是<i>c</i>维的向量, <b><i>W</i></b><sub>3</sub>包含<i>c</i>个<i>n</i><sub>2</sub>×<i>f</i><sub>3</sub>×<i>f</i><sub>3</sub>的滤波器。</p>
                </div>
                <div class="p1">
                    <p id="64">SRCNN的主要贡献是由稀疏编码跨越为深度学习, 应用完整的神经网络处理图像超分辨率问题, 并取得了较好的实验效果。SRCNN理论中还指出, 将处理的单幅图像由单一颜色通道更换为三颜色通道, 即在图像的YCbCr或RGB空间中完成图像重建的效果应优于单一颜色通道, 可以保留更多的图像色彩信息<citation id="238" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。而且SRCNN算法最先是采用较小的数据集Set5和Set14, 网络训练后学习到的知识较少, 图像重建性能受限制, 而更换为相对较大的数据集BSD200后重建性能也明显得到提高, 可见数据集的大小对图像的重建性能影响也较大。</p>
                </div>
                <div class="p1">
                    <p id="65">SRCNN在重建HR图像时, 网络层数较少, 感受野也相对较小, 利用到的区域特征单一并且利用效率不高<citation id="239" type="reference"><link href="194" rel="bibliography" /><link href="214" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">18</a>]</sup></citation>, 最终导致重建的HR图像纹理有些模糊, 算法的适应性受一定限制, SR性能有待进一步提高。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.2 MC-SRCNN<b>算法</b></h4>
                <div class="p1">
                    <p id="67">在<i>MC</i>-<i>SRCNN</i><citation id="240" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>中, <i>SR</i>过程针对<i>SRCNN</i>中的网络输入的单通道作出改进。在<i>SRCNN</i>中, 卷积神经网络的输入由图像单一颜色通道形成的数据经双立方插值构成, 这种单一的预处理操作会使重建图像存在边界模糊和缺少高频信息的问题。所以在<i>MC</i>-<i>SRCNN</i>中, 神经网络的输入改为多通道, 即输入包含了18种低分辨率图像, 如图2给出<i>MC</i>-<i>SRCNN</i>算法的网络框架。这18种低分辨率图像由单幅图像在预处理过程中生成, 即结合不同锐化卷积核的优势和3种不同的插值方式 (3种插值方法为最近邻、双线性、双立方插值;5种图像锐化强度值为0.4、0.8、1.2、1.6、2的被锐化的低分辨率图像) , 令所有被锐化的低分辨率图像通过最近邻插值、双线性插值或双立方插值被插入到同样的一幅高分辨率图像中, 然后令所有被锐化的和被插值的低分辨率图像共同构成卷积神经网络的多通道输入。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905035_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 MC-SRCNN算法的网络框架" src="Detail/GetImg?filename=images/JSJY201905035_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>MC</i>-<i>SRCNN</i>算法的网络框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905035_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Network framework of MC</i>-<i>SRCNN algorithm</i></p>

                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905035_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法搭建的卷积神经网络框架" src="Detail/GetImg?filename=images/JSJY201905035_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法搭建的卷积神经网络框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905035_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Convolutional neural network framework built by the proposed algorithm</i></p>

                </div>
                <div class="p1">
                    <p id="70"><i>MC</i>-<i>SRCNN</i>在经过第一层网络后构建的特征映射比<i>SRCNN</i>的要好, 因为图像本身是一种时域的波形图, 在其时域波形图中只能观察到波形的突起, 但通过傅里叶公式可知, 图像是由低频部分和高频部分两部分组成, 图像中的低频部分表示图像的内容即灰度变化, 而高频部分则表示图像的边缘信息和细节信息即锐度变化。在<i>SRCNN</i>算法中重建的图像存在边缘模糊的问题, 也就是预处理过程中单一的双立方插值使得输入图像从进入神经网络时就丢失了部分的高频信息, 而<i>MC</i>-<i>SRCNN</i>中神经网络的多通道输入解决了这一问题, 并且该算法在卷积过程中对于多通道的输入采用相同大小的卷积核, 在提升重建效果的同时并未增添参数。</p>
                </div>
                <div class="p1">
                    <p id="71"><i>MC</i>-<i>SRCNN</i>虽然解决了重建过程中丢失高频信息的问题, 但它选用的数据集<i>Set</i>5和<i>Set</i>14较小, 为了提高神经网络的图像重建性能应选用较大的数据集。而且在<i>SRCNN</i>算法中提出, 虽然在实现单幅图像的超分辨率重建的过程中一般使用图像的单一颜色通道, 但使用图像的3个颜色通道的重建效果要优于单一颜色通道。所以, 为了更好地实现单幅图像的超分辨率重建, 应将两种算法中的优势结合起来, 实现更好的重建效果。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">1.3 <i>SRGAN</i><b>算法</b></h4>
                <div class="p1">
                    <p id="73">均方误差 (<i>Mean Squared Error</i>, <i>MSE</i>) 和<i>PSNR</i>在捕获与知觉相关的差异方面的能力 (比如高频纹理细节) 非常有限, 因为它们是根据图像的像素差异来定义的, <i>PSNR</i>值最高并不一定能更好地反映感知能力的结果, 图像的真实感未必是最好的<citation id="241" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="74">在以前的基于神经网络的<i>SR</i>算法中, 虽然能够获得很高的<i>PSNR</i>值, 但恢复出的单幅图像通常会丢失部分高频细节信息, 使人不能有较好的视觉感受, 忽略了细节纹理的恢复。而更深层次的网络结构已被证明可以提高<i>SISR</i>的重建性能, 传统的基于<i>CNN</i>的<i>SISR</i>网络模型的网络层次较浅, 虽然能够提升一定的图像重建性能, 但重建过程中网络的学习效果受限, 提取特征的过程丢失高频细节信息的问题较为严重, 因此<i>SRGAN</i>使用深层的残差网络来恢复单幅图像中更多的细节纹理信息, 增强单幅图像重建后的视觉真实感<citation id="242" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。从实验数据中可知, 虽然<i>SRGAN</i>的<i>PSNR</i>值不是最高, 但它较好地恢复了图像中的纹理与细节, 使重建图像的视觉效果更为逼真。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">2 改进的图像超分辨率重建算法</h3>
                <h4 class="anchor-tag" id="76" name="76">2.1 <b>改进思想</b></h4>
                <div class="p1">
                    <p id="77">为了解决以上提出的问题, 本文对需要进行重建的低分辨率图像保留三颜色通道, 首先对其进行预处理操作:先对低分辨率图像进行三种插值操作, 即最近邻插值、双线性插值和双立方插值;然后对进行不同插值处理后的图像分别进行五种不同强度值的图像锐化操作;最后将只进行一次插值操作的图像与先进行一次插值、后进行一次锐化的图像合并排列成三维矩阵, 即令预处理过程后的18幅图像作为神经网络的多通道输入。然后, 为学习更深的图像细节纹理信息和高频信息, 重建过程构建了更深层次的网络结构, 但考虑到网络的收敛问题和防止梯度消失的问题, 本文采用深层残差网络结构, 提高网络的准确率。最后在单幅图像的重建过程中采用亚像素卷积层来实现, 提升重建效率, 减少重建时间。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">2.2 <b>网络结构</b></h4>
                <div class="p1">
                    <p id="79">考虑到重建性能和重建时间等多方面的因素, 本文提出的基于改进<i>CNN</i>的图像超分辨率重建算法主要是构建了一种深层卷积神经网络, 该网络共有6层, 分别由3部分构成, 即底层特征提取、非线性映射和图像重建。该网络考虑到低分辨率图像与高分辨率图像之间的非线性映射关系, 能提升网络对高频信息和细节纹理信息的学习效率, 并在改进重建性能的同时缩短网络训练时间与图像重建时间。本文搭建的卷积神经网络框架如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="80">本文算法中, 网络首先完成特征提取操作如图3中所示, 即卷积层Conv1为特征提取层。预处理操作后构成的输入网络通道数为18的低分辨率图像<b><i>Y</i></b>进入网络后, Conv1使用32个大小5×5的卷积核对低分辨率图像完成三维卷积操作, 提取图像的特征信息, 高纬度的特征映射构成包含32幅特征图的C1层, 具体操作如式 (4) 所示:</p>
                </div>
                <div class="p1">
                    <p id="81"><i>F</i><sub>1</sub> (<b><i>Y</i></b>) =max (0, <b><i>W</i></b><sub>1</sub>*<b><i>Y</i></b>+<b><i>B</i></b><sub>1</sub>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="82">其中:“*”表示卷积操作;<b><i>W</i></b><sub>1</sub>表示滤波器, 其大小表示为<i>c</i>×<i>n</i><sub>1</sub>×<i>f</i><sub>1</sub>×<i>f</i><sub>1</sub>, <i>c</i>表示所处理图像的颜色通道数, 本文算法中取<i>c</i>=18, <i>n</i><sub>1</sub>表示为Conv1层的滤波器数量, 即<b><i>W</i></b><sub>1</sub>包含<i>n</i><sub>1</sub>个<i>c</i>×<i>f</i><sub>1</sub>×<i>f</i><sub>1</sub>的滤波器, <i>f</i><sub>1</sub>×<i>f</i><sub>1</sub>为该层卷积核的大小;<b><i>B</i></b><sub>1</sub>表示偏置向量, 维度为<i>n</i><sub>1</sub>维, max (0, <b><i>x</i></b>) 即为特征提取后使用的激活函数ReLU。</p>
                </div>
                <div class="p1">
                    <p id="83">Conv1层的输出C1为第一个残差模块的输入, Conv2、Conv3、Conv4、Conv5为网络中的非线性映射层。Conv2与Conv3分别使用32个大小为5×5的卷积核进行卷积操作, 分别形成包含32幅特征图的C2、C3层, 这两层的公式同Conv1相类似。然后将第一个残差模块的卷积分支的输出C3层的特征图和该残差模块的恒等映射分支C1层的特征图, 与32个大小为5×5的卷积核进行卷积操作, 形成含有32幅特征图的C4层, Conv4层操作如式 (5) 所示:</p>
                </div>
                <div class="p1">
                    <p id="84"><i>F</i><sub>4</sub> (<b><i>Y</i></b>) =max (0, <b><i>W</i></b><sub>4</sub>* (<i>F</i><sub>3</sub> (<b><i>Y</i></b>) +<i>F</i><sub>1</sub> (<b><i>Y</i></b>) ) +<b><i>B</i></b><sub>4</sub>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="85">其中:<b><i>B</i></b><sub>4</sub>表示Conv4层的偏置向量, 维度为<i>n</i><sub>4</sub>维;<b><i>W</i></b><sub>4</sub>表示滤波器, 其大小为<i>n</i><sub>3</sub>×<i>n</i><sub>4</sub>×<i>f</i><sub>4</sub>×<i>f</i><sub>4</sub>。C4层作为Conv5层卷积操作的输入, 使用32个大小为5×5的卷积核进行卷积操作, 形成包含32幅特征图的C5层, 公式同Conv1、Conv2、Conv3相类似。</p>
                </div>
                <div class="p1">
                    <p id="86">Conv6表示为亚像素卷积操作, C6层为通过<i>r</i><sup>2</sup>个大小为3×3的卷积核完成亚像素卷积操作而构成, Conv6输出的特征通道数为<i>r</i><sup>2</sup>, C6层包含<i>r</i><sup>2</sup>幅特征图, <i>r</i>为上采样倍数, 其具体操作如式 (6) 所示:</p>
                </div>
                <div class="p1">
                    <p id="87"><b><i>I</i></b><sup>SR</sup>=<i>F</i><sub>6</sub> (<b><i>Y</i></b>) =<i>PS</i> (<b><i>W</i></b><sub>6</sub>* (<i>F</i><sub>5</sub> (<b><i>Y</i></b>) +<i>F</i><sub>3</sub> (<b><i>Y</i></b>) ) +<b><i>B</i></b><sub>6</sub>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="88">其中:<b><i>W</i></b><sub>6</sub>与<b><i>B</i></b><sub>6</sub>分别表示Conv6亚像素卷积层的滤波器与偏置向量, <b><i>W</i></b><sub>6</sub>的大小为<i>n</i><sub>4</sub>×<i>n</i><sub>5</sub>×<i>f</i><sub>6</sub>×<i>f</i><sub>6</sub>;<i>F</i><sub>5</sub> (<b><i>Y</i></b>) 、<i>F</i><sub>3</sub> (<b><i>Y</i></b>) 分别表示第二个残差模块的卷积分支和恒等映射分支;<i>PS</i>表示对像素的重新排列 (Periodic Shuffling) 操作, 对图像特征进行整合;<b><i>I</i></b><sup>SR</sup>=<i>F</i><sub>6</sub> (<b><i>Y</i></b>) 表示重建的高分辨率图像。</p>
                </div>
                <div class="p1">
                    <p id="89">该网络结构的特点如下:</p>
                </div>
                <div class="p1">
                    <p id="90">1) 本文算法中采用图像的三颜色通道进行处理, 以便在预处理操作中尽可能地保留输入的原始低分辨率图像中的高频信息和细节纹理信息等特征。不同插值算法的计算复杂度不同, 且插值时考虑周围不同数目的相邻像素点灰度值影响也不同, 如最近邻插值计算量最小但仅考虑位置最近的像素的灰度值, 双线性插值在两水平方向与垂直方向均进行一阶线性插值, 双立方插值考虑周围四个相邻像素点灰度值影响。图像在插值处理后, 会出现边缘和轮廓模糊问题, 而图像边缘信息主要集中在高频部分, 使用锐化操作则可以有效减轻插值的不利影响。因此, 本文算法采用插值与锐化操作完成图像预处理, 并以18幅低分辨率图像作为神经网络的多通道输入, 相对于单一地使用双立方插值, 有利于学习到一种更好且更复杂的特征映射。这种以插值和锐化构成的18幅低分辨率图像具有丰富的可用于重建高分辨率图像过程中的补充信息, 能提高重建过程的准确性。</p>
                </div>
                <div class="p1">
                    <p id="91">2) 在非线性映射层加入残差网络结构, 通过恒等映射将后层信息直接传向前层, 实现了特征的重复利用, 减少了特征信息的丢失。通过增加非线性映射层来加深网络结构, 而且引入残差网络后, 解决了由于网络加深而易引起的梯度消失和梯度爆炸问题, 网络的每一层分别对应于提取不同层次的特征信息, 有效地加深网络结构, 提取到的不同层次的特征信息就会增多。</p>
                </div>
                <div class="p1">
                    <p id="92">3) 在图像重建过程中引入亚像素卷积层, 在网络的最后一层使用上采样操作, 能减少在图像重建中的重建步骤与时间, 为减少计算量使用尺寸较小的滤波器, 在保持特征信息前后关系的同时, 对高分辨率特征图中的像素进行重新排列, 整合相同的信息, 恢复成最终所需的高分辨率图像。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">2.3 <b>残差网络结构</b></h4>
                <div class="p1">
                    <p id="94">在非线性映射过程中引入残差网络, 因为残差网络更容易优化, 并且能够通过增加有效的深度来提高准确率。残差网络的优势是不仅解决了增加网络深度导致的梯度消失与梯度爆炸问题, 而且提高了网络性能。神经网络层数的不同, 提取的浅层、中层和深层的特征信息不同, 网络层数越多, 意味着能够提取到不同层次的特征信息越丰富, 并且, 网络越深提取的特征越抽象, 具有的高频信息越丰富<citation id="243" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。使用残差网络不仅很好地避免了退化问题, 而且其反向更新的特点引入快捷链接的概念, 使连加运算代替传统网络中的连乘运算, 大幅降低了计算量<citation id="244" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="95">图4中给出了残差网络中的一个残差模块的结构, 由卷积层、快捷连接和激活函数<i>ReLU</i>组成。假设将采取某种预处理操作后的图像<b><i>a</i></b>作为此残差模块的输入, 经过快捷连接后为<i>H</i> (<b><i>a</i></b>) =<i>F</i> (<b><i>a</i></b>) +<b><i>a</i></b>, 如果<i>F</i> (<b><i>a</i></b>) 和<b><i>a</i></b>的通道相同, 则可直接相加。残差模块的主要设计有两个部分:快捷连接和恒等映射。通过在一个浅层网络上令<i>F</i> (<b><i>a</i></b>) =0, 使得<i>H</i> (<b><i>a</i></b>) =<b><i>a</i></b>, 就构成一个恒等映射, 使网络在深度增加时而不退化, 因此快捷连接使残差变得可能, 而恒等映射使网络变深。在卷积层后增加激活函数, 能使网络的学习周期大幅缩短<citation id="245" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905035_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 残差模块的结构" src="Detail/GetImg?filename=images/JSJY201905035_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 残差模块的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905035_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Residual module structure</p>

                </div>
                <h4 class="anchor-tag" id="97" name="97">2.4 <b>亚像素卷积层</b></h4>
                <div class="p1">
                    <p id="98">亚像素卷积层是在网络输出层进行上采样的操作, 与在输出层进行卷积操作的<i>SRCNN</i>相比, 在训练和测试时都降低了复杂度, 减少了在输出层卷积上的时间消耗<citation id="246" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。在<i>SRCNN</i>中, 对图像进行下采样操作, <i>r</i>为下采样因子, 高分辨率图像和低分辨率图像都具有<i>c</i>个颜色通道, 因此它们可表示为大小为<i>H</i>×<i>W</i>×<i>c</i>和<i>rH</i>×<i>rW</i>×<i>c</i>的实数张量。而亚像素卷积层的输入为特征通道数为<i>r</i><sup>2</sup>的特征映射, <i>r</i>表示上采样因子, 将每个像素的<i>r</i><sup>2</sup>个通道重新排列成一个<i>r</i>×<i>r</i>的区域, 对应于高分辨率图像中的一个<i>r</i>×<i>r</i>大小的图像块, 从而大小为<i>H</i>×<i>W</i>×<i>cr</i><sup>2</sup>的特征图像被重新排列成大小为<i>rH</i>×<i>rW</i>×<i>c</i>的高分辨率图像, 这个变换就被称作亚像素卷积, 但它实际上并没有进行卷积操作<citation id="247" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。因此亚像素卷积的本质就是将维度为<i>r</i><sup>2</sup>的低分辨率特征向量按一定排列要求周期性地排列来构成高分辨率图像。</p>
                </div>
                <div class="p1">
                    <p id="99">在低分辨率空间中, 令大小为<i>f</i><sub><i>s</i></sub>的滤波器<b><i>W</i></b><sub><i>s</i></sub>与权重空间1/<i>r</i>进行卷积操作, 步幅为1/<i>r</i>, 其结果虽未激活和计算像素之间的权重, 但激活了滤波器<b><i>W</i></b><sub><i>s</i></sub>的不同部分。当激活模型为<i>r</i><sup>2</sup>个时, 每个激活模型依据其位置, 有最多[<i>f</i><sub><i>s</i></sub>/<i>r</i>]<sup>2</sup>个权重被激活。当进行卷积操作的滤波器依据不同的亚像素位置扫过图片时, 这些激活模型就会被周期性地激活:<b><i>x</i></b>、 <b><i>y</i></b>为高分辨率空间的输出像素, mod (<b><i>x</i></b>, <i>r</i>) 、mod (<b><i>y</i></b>, <i>r</i>) 为<b><i>x</i></b>、 <b><i>y</i></b>输出像素在高分辨率空间的位置坐标。亚像素卷积就是在mod (<i>f</i><sub><i>s</i></sub>, <i>r</i>) =0时进行上采样操作, 操作过程如式 (7) 、 (8) 所示:</p>
                </div>
                <div class="p1">
                    <p id="100"><b><i>I</i></b><sup>SR</sup>=<i>PS</i> (<b><i>W</i></b><sub><i>L</i></sub>*<i>F</i><sub><i>L</i>-1</sub> (<b><i>Y</i></b>) +<b><i>B</i></b><sub><i>L</i></sub>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="101"><i>PS</i> (<i>T</i>) <sub><b><i>x</i></b>, <b><i>y</i></b>, <i>c</i></sub>=<i>T</i><sub>⎣<b><i>x</i></b>/<i>r</i>」, ⎣<b><i>y</i></b>/<i>r</i>」, <i>c</i>·<i>r</i>·mod (<b><i>y</i></b>, <i>r</i>) +<i>c</i>·mod (<b><i>x</i></b>, <i>r</i>) </sub>      (8) </p>
                </div>
                <div class="p1">
                    <p id="102">卷积操作<b><i>W</i></b><sub><i>L</i></sub>的大小为<i>r</i><sup>2</sup><i>c</i>×<i>n</i><sub><i>L</i>-1</sub>×<i>f</i><sub><i>L</i></sub>×<i>f</i><sub><i>L</i></sub>, 在最后一层卷积操作中不再使用非线性操作, 当<i>f</i><sub><i>L</i></sub>=<i>f</i><sub><i>s</i></sub>/<i>r</i>和mod (<i>f</i><sub><i>s</i></sub>, <i>r</i>) =0两个等式成立时等价于在低分辨率空间中以滤波器<b><i>W</i></b><sub><i>s</i></sub>进行亚像素卷积操作。因此, 在本文算法的网络结构中, 由于采用多通道输出操作与较深的网络结构, 而造成了一定的重建速率损耗, 因此通过亚像素卷积将低分辨率特征映射直接构成所需的高分辨率图像, 尽可能地弥补之前损耗的时间。</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表</b>1 <b>不同算法在</b>Set5<b>数据集上的</b>PSNR<b>和</b>SSIM<b>结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 PSNR and SSIM of different algorithms on dataset Set5</p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td rowspan="2">图像</td><td rowspan="2">放大倍数</td><td colspan="2"><br />SRCNN算法<sup>[13]</sup></td><td rowspan="2"></td><td colspan="2"><br />MC-SRCNN算法<sup>[16]</sup></td><td rowspan="2"></td><td colspan="2"><br />本文算法</td></tr><tr><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td><td><br />PSNR/dB</td><td>SSIM</td></tr><tr><td rowspan="3"><br />baby</td><td>2</td><td>38.27</td><td>0.956 4</td><td></td><td>38.31</td><td>0.962 5</td><td></td><td><b>38.36</b></td><td><b>0.970</b><b>4</b></td></tr><tr><td><br />3</td><td>34.96</td><td>0.921 0</td><td></td><td>35.02</td><td>0.921 8</td><td></td><td><b>35.10</b></td><td><b>0.926</b><b>7</b></td></tr><tr><td><br />4</td><td><b>33.03</b></td><td>0.880 6</td><td></td><td>33.01</td><td>0.876 9</td><td></td><td>33.02</td><td><b>0.881</b><b>4</b></td></tr><tr><td rowspan="3"><br />bird</td><td><br />2</td><td>40.65</td><td>0.973 2</td><td></td><td>40.93</td><td>0.987 7</td><td></td><td><b>41.16</b></td><td><b>0.988</b><b>9</b></td></tr><tr><td><br />3</td><td>35.08</td><td>0.948 5</td><td></td><td>35.49</td><td>0.948 9</td><td></td><td><b>35.71</b></td><td><b>0.949</b><b>8</b></td></tr><tr><td><br />4</td><td>32.11</td><td>0.901 5</td><td></td><td>32.35</td><td>0.904 5</td><td></td><td><b>32.53</b></td><td><b>0.908</b><b>5</b></td></tr><tr><td rowspan="3"><br />butterfly</td><td><br />2</td><td>32.25</td><td>0.954 4</td><td></td><td>32.82</td><td>0.962 6</td><td></td><td><b>33.15</b></td><td><b>0.973</b><b>5</b></td></tr><tr><td><br />3</td><td>27.55</td><td>0.889 7</td><td></td><td>28.21</td><td>0.906 4</td><td></td><td><b>28.37</b></td><td><b>0.922</b><b>3</b></td></tr><tr><td><br />4</td><td>25.06</td><td>0.831 8</td><td></td><td>25.45</td><td>0.847 2</td><td></td><td><b>25.54</b></td><td><b>0.861</b><b>5</b></td></tr><tr><td rowspan="3"><br />head</td><td><br />2</td><td>35.62</td><td>0.884 3</td><td></td><td>35.62</td><td>0.884 9</td><td></td><td><b>35.68</b></td><td><b>0.885</b><b>9</b></td></tr><tr><td><br />3</td><td>33.53</td><td>0.823 1</td><td></td><td>33.62</td><td>0.825 3</td><td></td><td><b>33.75</b></td><td><b>0.832</b><b>2</b></td></tr><tr><td><br />4</td><td>32.15</td><td>0.769 8</td><td></td><td>32.21</td><td>0.773 8</td><td></td><td><b>32.33</b></td><td><b>0.777</b><b>8</b></td></tr><tr><td rowspan="3"><br />woman</td><td><br />2</td><td>35.08</td><td>0.962 5</td><td></td><td>35.27</td><td>0.965 8</td><td></td><td><b>35.41</b></td><td><b>0.971</b><b>5</b></td></tr><tr><td><br />3</td><td>30.86</td><td>0.922 6</td><td></td><td>31.48</td><td>0.927 9</td><td></td><td><b>31.61</b></td><td><b>0.934</b><b>3</b></td></tr><tr><td><br />4</td><td>28.52</td><td>0.871 4</td><td></td><td>28.73</td><td>0.878 3</td><td></td><td><b>28.85</b></td><td><b>0.886</b><b>5</b></td></tr><tr><td rowspan="3"><br />平均值</td><td><br />2</td><td>36.37</td><td>0.886 1</td><td></td><td>36.59</td><td>0.952 7</td><td></td><td><b>36.75</b></td><td><b>0.958</b><b>1</b></td></tr><tr><td><br />3</td><td>32.39</td><td>0.840 9</td><td></td><td>32.76</td><td>0.906 0</td><td></td><td><b>32.90</b></td><td><b>0.913</b><b>1</b></td></tr><tr><td><br />4</td><td>30.17</td><td>0.851 0</td><td></td><td>30.35</td><td>0.856 1</td><td></td><td><b>30.46</b></td><td><b>0.863</b><b>2</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:粗体表示为最优值。</p>
                    <p class="img_note"></p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">3 实验与结果分析</h3>
                <div class="p1">
                    <p id="105">本文采用的实验平台是使用NVIDIA显卡GeForce GTX TITAN-X、3.20 GHz Intel i5 CPU、32 GB RAM, 编译软件使用Matlab 2016a, 并使用Caffe深度学习工具箱进行神经网络模型的搭建和训练<citation id="248" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。由于本文改建的网络相对较深, 算法应使用更大的训练集, 以体现该网络的优势, 因此实验从ImageNet数据集中随机抽取约6万幅图像组成训练集, 抽取的图像尺寸最大不超过512×512, 原始高分辨率图像为<b><i>X</i></b>, 放大倍数取<i>s</i>=2, 3, 4, 预处理后的多幅图像为<b><i>Y</i></b>, 作为网络的多通道输入。虽然选用较大的学习率可以加快网络的收敛, 但可能出现局部最优问题, 因此根据网络模型训练的经验与更公平地进行对比实验和分析, 将前五层网络的学习率设为10<sup>-4</sup>, 最后一层网络的学习率设为10<sup>-5</sup>, 动量参数设为0.9, 权重衰减参数设为0.000 5。</p>
                </div>
                <div class="p1">
                    <p id="106">本文从主客观两种角度对重建图像的性能进行评价分析。主观评价主要是评价图像的人员在正常环境下依据图像在人眼的直接观察, 对于图像清晰度、图像呈现内容的直观的判断;而客观评价是通过一些定性的公式进行准确定量的计算分析, 客观评价指标选用峰值信噪比 (Peak Signal-to-Noise Ratio, PSNR) 和结构相似度 (Structural SIMilarity index, SSIM) 。<i>PSNR</i>是衡量图像失真或是噪声水平的客观标准, <i>PSNR</i>值越大, 则重建图像越接近原始高分辨率图像。其计算公式如式 (9) 、 (10) 所示:</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Η</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>W</mi><mo>-</mo><mn>1</mn></mrow></munderover><mo stretchy="false">[</mo></mstyle></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ι</mi><mtext> </mtext></mrow><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mi>S</mi><mi>Ν</mi><mi>R</mi><mo>=</mo><mn>1</mn><mn>0</mn><mspace width="0.25em" /><mrow><mi>lg</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mi>Μ</mi><mi>A</mi><mi>X</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Ι</mi></msub><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi></mrow></mfrac><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">其中:<i>H</i>、<i>W</i>为图像的尺寸;<i>MSE</i>为均方根误差;<i>MAX</i><sub><b><i>I</i></b></sub>为图像的最大像素值。<i>SSIM</i>是衡量两幅或者多幅图像之间的相似度指标, 计算公式如式 (11) 所示:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>S</mi><mi>Ι</mi><mi>Μ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo>, </mo><mrow><mi mathvariant="bold-italic">Ι</mi><mtext> </mtext></mrow><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>2</mn><mi>u</mi><msub><mrow></mrow><mi mathvariant="bold-italic">X</mi></msub><mi>u</mi><msub><mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">Ι</mi><mtext> </mtext></mrow><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup></mrow></msub><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mn>2</mn><mi>σ</mi><msub><mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">Ι</mi><mtext> </mtext></mrow><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup></mrow></msub><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">X</mi><mn>2</mn></msubsup><mo>+</mo><mi>u</mi><msub><mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">Ι</mi><mtext> </mtext></mrow><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup></mrow></msub><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">X</mi><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msub><mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">Ι</mi><mtext> </mtext></mrow><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>R</mtext></mrow></msup></mrow></msub><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中:<i>u</i><sub><b><i>X</i></b></sub>和<i>u</i><sub><b><i>I</i></b><sup>SR</sup></sub>表示原始图像<b><i>X</i></b>和重建图像<b><i>I</i></b><sup>SR</sup>的均值;<i>σ</i><sub><b><i>X</i></b></sub>和<i>σ</i><sub><b><i>I</i></b><sup>SR</sup></sub>表示原始图像<b><i>X</i></b>和重建图像<b><i>I</i></b><sup>SR</sup>的方差;<i>σ</i><sub><b><i>XI</i></b><sup>SR</sup></sub>表示两者的协方差;<i>C</i><sub>1</sub>和<i>C</i><sub>1</sub>为常数值。<i>SSIM</i>从亮度、对比度和结构三个角度对图像间进行评价, <i>SSIM</i>值越接近1则表示重建图像与原始高分辨率图像相似度越高, 重建性能越好<citation id="249" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="111">首先将本文算法分别与Bicubic算法、Yang算法<citation id="250" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、ANR算法<citation id="251" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、SRCNN算法<citation id="252" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、MC-SRCNN算法<citation id="253" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>等单幅图像超分辨率重建领域中具有代表性的一些算法进行对比实验分析。为验证本文算法的有效性和对比实验的公平性考虑, 在数据集Set5和Set14上完成对比实验, 并将图像重建的数据对比如表1～2所示, 视觉对比如图5所示。</p>
                </div>
                <div class="p1">
                    <p id="112">图5可从主观角度分析图像的超分辨率重建效果:</p>
                </div>
                <div class="p1">
                    <p id="113">从图5 (b) 可看出, Bicubic算法重建过程中丢失了较多的图像高频信息, 造成图像边缘存在过于平滑的现象, 边缘失真, 缺少纹理信息。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905035_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同算法在baboon和lenna图像上的重建结果" src="Detail/GetImg?filename=images/JSJY201905035_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同算法在baboon和lenna图像上的重建结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905035_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Reconstruction results of different algorithms on image baboon and lenna</p>

                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表</b>2 <b>不同算法在</b>Set14<b>数据集上的</b>PSNR (<b>放大倍数为</b>3) dB <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 PSNR of different algorithms on dataset Set14 (magnification=3) dB</p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />图像</td><td>Bicubic算法</td><td>Yang算法<sup>[7]</sup></td><td>ANR算法<sup>[11]</sup></td><td>SRCNN算法<sup>[13]</sup></td><td>MC-SRCNN算法<sup>[16]</sup></td><td>本文算法</td></tr><tr><td><br />baboon</td><td>23.20</td><td>23.49</td><td>23.59</td><td>23.59</td><td>23.60</td><td><b>23.72</b></td></tr><tr><td><br />barbara</td><td>26.19</td><td>26.40</td><td>26.71</td><td><b>26.52</b></td><td>26.51</td><td>26.51</td></tr><tr><td><br />bridge</td><td>24.37</td><td>24.80</td><td>24.98</td><td>25.11</td><td>25.21</td><td><b>25.26</b></td></tr><tr><td><br />coastguard</td><td>26.59</td><td>26.67</td><td>27.09</td><td>27.19</td><td>27.22</td><td><b>27.32</b></td></tr><tr><td><br />comic</td><td>23.10</td><td>23.89</td><td>24.00</td><td>24.24</td><td>24.40</td><td><b>24.61</b></td></tr><tr><td><br />face</td><td>32.79</td><td>33.07</td><td>33.59</td><td>33.49</td><td>33.62</td><td><b>33.73</b></td></tr><tr><td><br />flowers</td><td>27.18</td><td>28.20</td><td>28.48</td><td>28.84</td><td>29.11</td><td><b>29.20</b></td></tr><tr><td><br />foreman</td><td>31.20</td><td>31.99</td><td>33.17</td><td>34.46</td><td>34.60</td><td><b>34.71</b></td></tr><tr><td><br />lenna</td><td>31.69</td><td>32.58</td><td>33.10</td><td>33.32</td><td>33.46</td><td><b>33.67</b></td></tr><tr><td><br />man</td><td>27.01</td><td>27.77</td><td>27.89</td><td>28.15</td><td>28.30</td><td><b>28.49</b></td></tr><tr><td><br />monarch</td><td>29.38</td><td>30.69</td><td>31.09</td><td>32.35</td><td>32.84</td><td><b>32.92</b></td></tr><tr><td><br />pepper</td><td>32.40</td><td>33.29</td><td>33.78</td><td>34.35</td><td>34.66</td><td><b>34.89</b></td></tr><tr><td><br />ppt3</td><td>23.69</td><td>25.47</td><td>25.01</td><td>25.97</td><td><b>26.48</b></td><td>26.45</td></tr><tr><td><br />zebra</td><td>26.59</td><td>27.98</td><td>28.39</td><td>28.91</td><td>29.03</td><td><b>29.19</b></td></tr><tr><td><br />平均值</td><td>27.53</td><td>28.31</td><td>28.64</td><td>29.03</td><td>29.22</td><td><b>29.34</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:粗体表示为最优值。</p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">从图5 (c) 、 (d) 可看出, 通过字典训练来学习特征的Yang算法<citation id="254" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和ANR算法<citation id="255" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>对于图像的特征提取具有一定的局限性, 难以提取到图像不同层次的特征, 学习到的信息更多为浅层的特征, 因而观察到的重建图像纹理清晰度较差;但相对于Bicubic算法, 这两种算法恢复的边缘平滑现象有较好的改进。</p>
                </div>
                <div class="p1">
                    <p id="118">从图5 (e) 、 (f) 可看出, SRCNN算法<citation id="256" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>局部感受野较小, 利用的区域特征单一, 重建图像的边缘仍存在较为明显的模糊现象;MC-SRCNN算法<citation id="257" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的多通道输入, 虽然减少了不必要的特征丢失, 但仍然存在边缘模糊现象, 而且观察两幅重建图像的细节纹理可知, 由于提取的深层特征信息有限, 图像baboon的嘴周皮毛处和图像lenna的头发处的纹理恢复效果与原始高分辨率图像相比较差。</p>
                </div>
                <div class="p1">
                    <p id="119">从图5 (g) 可看出, 本文算法重建后的单幅图像在区分图像的边缘和改善纹理细节信息方面有较好的效果, 从重建的baboon图像和lenna图像, 狒狒的嘴周皮毛和lenna女士头发处的纹理细节的区分度和清晰度来看, 相对于其他算法都有较明显的提升。</p>
                </div>
                <div class="p1">
                    <p id="120">从表1～2可看出, 本文算法的重建效果普遍优于SRCNN算法<citation id="258" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和MC-SRCNN算法<citation id="259" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。虽然图像bird中包含的纹理信息和细节信息相对较少, 其定量计算的数值并没有较大的提升, 但其重建效果仍有一些提高。综合分析, 本文算法相对于SRCNN算法<citation id="260" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和MC-SRCNN算法<citation id="261" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, PSNR指数平均增加了0.23 dB, SSIM指数平均增加了0.006 6。</p>
                </div>
                <div class="p1">
                    <p id="121">为了进一步验证本文算法改进的有效性, 针对图像预处理操作与网络结构改进两者对图像重建效果的影响, 在放大倍数为3时在Set14数据集上设置三组对比实验, 实验结果如图6所示。</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905035_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 图像预处理操作与改进的网络结构对图像重建结果的影响" src="Detail/GetImg?filename=images/JSJY201905035_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 图像预处理操作与改进的网络结构对图像重建结果的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905035_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of effect of image preprocessing operation and improved network structure on image reconstruction results</p>

                </div>
                <div class="p1">
                    <p id="123">从图6可知:进行单一的双立方插值预处理操作的本文算法与SRCNN算法<citation id="262" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>相比较, 两种算法的预处理操作相同, 但网络结构的深度不同, 采用单一的双立方插值预处理操作的本文算法的网络更深。经过实验对比, 采用单一双立方插值预处理操作的本文算法相比SRCNN算法, 运算时间较少, PSNR值更高, 图像重建效果更好, 可见较深的网络结构可以更好地恢复图像的高频特征, 且亚像素卷积操作确实在一定程度上减少了图像重建时间。而单一的双立方插值预处理操作的本文算法与采取多通道输入的本文算法相比较, 两种算法的预处理操作不同, 一个采用单一的双立方插值进行预处理, 另一个采用多种预处理操作结合的算法。经过实验对比, 在时间损耗上, 虽然采用单一的双立方插值预处理操作的本文算法少于多通道的算法, 但在图像重建性能上却出现较多的损失, 由此可见, 多通道的预处理操作在恢复图像重建性能上具有较重要的作用。由以上两组对比实验可知, 对于提升图像重建性能而言, 多通道的预处理操作对图像性能的提升要稍大于网络结构的加深对图像重建性能的提升。单一的双立方插值预处理操作的本文算法与MC-SRCNN算法<citation id="263" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>相比, 两种算法采用的预处理操作和网络层数均不同。经过实验对比, 单一的双立方插值预处理操作的本文算法虽然PSNR值相对较低, 却在时间损耗上有较大幅度的降低, 由此可见, 虽然单一预处理操作的本文算法的网络结构较深, 但是采用亚像素卷积进行重建操作很好地减少了时间损耗。因此, 本文算法中多通道的预处理操作能有效地改善了图像的重建性能, 而对于网络结构的改进, 虽然在图像重建性能与重建时间上都有所改进, 但改进效果并不是十分显著, 更多的是弥补了多种预处理操作过程中的时间损耗, 所以在综合考虑后本文算法采取优势互补的方式作出了前文算法描述中的改进。</p>
                </div>
                <div class="p1">
                    <p id="124">将本文算法分别与Zeyde算法<citation id="264" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、ANR算法<citation id="265" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、SRCNN算法<citation id="266" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和MC-SRCNN算法<citation id="267" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>进行PSNR值与运算时间的对比实验分析。如图7所示, 本文算法相对于Zeyde算法<citation id="268" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>和ANR算法<citation id="269" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的图像重建相似性明显提高;且相对于SRCNN算法<citation id="270" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和MC-SRCNN算法<citation id="271" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 本文算法虽然加深了网络, 但在保证提升图像重建质量的基础上, 残差网络结构与亚像素卷积层的应用在一定程度上缩减了运算时间, 较好地提升了重建性能。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905035_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 当放大倍数为3时各超分辨率算法在 Set14数据集上的PSNR值与运算时间的关系" src="Detail/GetImg?filename=images/JSJY201905035_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 当放大倍数为3时各超分辨率算法在 Set14数据集上的PSNR值与运算时间的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905035_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Relationship between PSNR and operation time of each super-resolution algorithm on dataset Set14 when magnification is 3</p>

                </div>
                <div class="p1">
                    <p id="126">为了更好地验证本文算法的有效性和广泛适应性, 本文选用不同的数据集进行对比。表3给出在放大倍数为3时在数据集BSD200、BSD300、BSD500和Texture上的不同算法的PSNR平均值, 由定量数值分析可知, 本文算法的PSNR值普遍高于其他算法, 较好地验证了本文算法的有效性和适应性, 一定程度上增强了重建图像的纹理信息和高频细节信息, 提高图像的纹理清晰度和区别度, 较好地提升了单幅图像的超分辨率重建效果。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表</b>3 <b>不同算法在不同数据集上的平均</b>PSNR (<b>放大倍数为</b>3) dB <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Average PSNR of different algorithms on different datasets (magnification=3) dB</p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br />数据集</td><td>SRCNN算法<sup>[13]</sup></td><td>MC-SRCNN算法<sup>[16]</sup></td><td>本文算法</td></tr><tr><td><br />Set5</td><td>32.39</td><td>32.76</td><td>33.28</td></tr><tr><td><br />Set14</td><td>29.03</td><td>29.22</td><td>29.34</td></tr><tr><td><br />BSD200</td><td>28.27</td><td>28.30</td><td>28.32</td></tr><tr><td><br />BSD300</td><td>28.31</td><td>28.32</td><td>28.35</td></tr><tr><td><br />BSD500</td><td>28.36</td><td>28.39</td><td>28.47</td></tr><tr><td><br />Texture</td><td>26.35</td><td>26.41</td><td>26.59</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="128" name="128" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="129">本文主要进行的是深度学习在SISR技术领域的研究, 通过改进卷积神经网络来提升图像质量, 使用多种预处理手段构成网络的多通道输入, 以有效避免区域特征单一的问题;同时, 在非线性映射层采用残差网络结构不仅获取了图像的不同层次的特征信息, 而且有效解决了梯度消失和梯度爆炸问题;最后引入亚像素卷积完成图像重建操作, 提高了重建效率。本文算法使用更大的训练集, 经过实验和结果分析, 不仅避免了网络加深的过拟合现象, 而且获得了较好的重建效果, 图像纹理细节部分的区分度和清晰度得到有效提高, 但边缘重建效果与原始高分辨率图像间仍有差距, 需进一步改进算法, 解决图像边缘区域的模糊问题, 提升单幅图像的超分辨率重建效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="180">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201705002&amp;v=MjQ4NDhIOWJNcW85RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VcjdPS0NMZlliRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 孙旭, 李晓光, 李嘉锋, 等.基于深度学习的图像超分辨率复原研究进展[J].自动化学报, 2017, 43 (5) :697-709. (SUN X, LI X G, LI J F, et al.Review on deep learning based image super-resolution restoration algorithms [J].Acta Automatica Sinica, 2017, 43 (5) :697-709.) 
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201308005&amp;v=MDg4NTZxQnRHRnJDVVI3cWZadVpzRnlEblVyN09LQ0xmWWJHNEg5TE1wNDlGWVlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 苏衡, 周杰, 张志浩.超分辨率图像重建方法综述[J].自动化学报, 2013, 39 (8) :1202-1213. (SU H, ZHOU J, ZHANG Z H.Survey of super-resolution image reconstruction methods [J].Acta Automatica Sinica, 2013, 39 (8) :1202-1213.) 
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201804012&amp;v=MTk0Nzh0R0ZyQ1VSN3FmWnVac0Z5RG5VcjdPUHlyZmJMRzRIOW5NcTQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 李浪宇, 苏卓, 石晓红, 等.图像超分辨率重建中的细节互补卷积模型[J].中国图象图形学报, 2018, 23 (4) :572-582. (LI L Y, SU Z, SHI X H, et al.Mutual-detail convolution model for image super-resolution reconstruction [J].Journal of Image and Graphics, 2018, 23 (4) :572-582.) 
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate Image Super-Resolution Using Very Deep Convolutional Networks">

                                <b>[4]</b> KIM J, LEE J K, LEE K M.Accurate image super-resolution using very deep convolutional networks [C]// Proceedings of the 2016 IEEE Conference on Computer on Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1646-1654.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD120828002575&amp;v=MDU4MzZwNDlGWnU0SUNSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3ZktJVjhTTmo3QmFySzZIdG5P&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> TIAN J, MA K K.A survey on super-resolution imaging[J].Signal, Image and Video Processing, 2011, 5 (3) :329-342.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution:a benchmark">

                                <b>[6]</b> YANG C Y, MA C, YANG M H.Single-image super-resolution:a benchmark [C]// Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:372-386.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">

                                <b>[7]</b> YANG J C, WRIGHT J, HUANG T S, et al.Image super-resolution via sparse representation [J].IEEE Transactions on Image Processing, 2010, 19 (11) :2861-2873.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201712005&amp;v=Mjc4NTNZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXI3T1B5cmZiTEc0SDliTnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 李云飞, 符冉迪, 金炜, 等.多通道卷积的图像超分辨率方法[J].中国图象图形学报, 2017, 22 (12) :1690-1700. (LI Y F, FU R D, JIN W, et al.Image super-resolution using multi-channel convolution [J].Journal of Image and Graphics, 2017, 22 (12) :1690-1700.) 
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On single image scale-up using sparse representations">

                                <b>[9]</b> ZEYDE R, ELAD M, PROTTER M.On single image scale-up using sparse-representations [C]// Proceedings of the 2010 International Conference on Curves and Surfaces, LNCS 6920.Berlin:Springer, 2010:711-730.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anchored neighborhood regression for fast example-based super-resolution">

                                <b>[10]</b> TIMOFTE R, DE V, GOOL L V.Anchored neighborhood regression for fast example-based super-resolution [C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2013:1920-1927.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A+:adjusted anchored neighborhood regression for fast super-resolution">

                                <b>[11]</b> TIMOFTE R, ROTHE R, GOOL L V.A<sup>+</sup>:adjusted anchored neighborhood regression for fast super-resolution [C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2015:111-126.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution throughneighbor embedding">

                                <b>[12]</b> CHANG H, XIONG Y, YEUNG D Y.Super-resolution through neighbor embedding [C]// Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2004:275-282.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Convolutional Network for Image Super-Resolution">

                                <b>[13]</b> DONG C, CHEN C L, HE K, et al.Learning a deep convolutional network for image super-resolution [C]// ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:184-199.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201605003&amp;v=MjU5MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyN09QeXJmYkxHNEg5Zk1xbzlGWjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 徐冉, 张俊格, 黄凯奇.利用双通道卷积神经网络的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (5) :556-564. (XU R, ZHANG J G, HUANG K Q.Image super-resolution using two-channel convolutional neural networks [J].Journal of Image and Graphics, 2016, 21 (5) :556-564.) 
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZKZX201502017&amp;v=MzAwNDBGckNVUjdxZlp1WnNGeURuVXI3T1B5YlJkckc0SDlUTXJZOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 刘娜, 李翠华.基于多层卷积神经网络学习的单帧图像超分辨率重建方法[J].中国科技论文, 2015, 10 (2) :201-206. (LIU N, LI C H.Single image super-resolution reconstruction via deep convolutional neural network [J].China Sciencepaper, 2015, 10 (2) :201-206.) 
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution based on convolution neural networks using multi-channel input">

                                <b>[16]</b> YOUM G Y, BAE S H, KIM M.Image super-resolution based on convolution neural networks using multi-channel input [C]// Proceedings of the 2006 IEEE 12th Image, Video, and Multidimensional Signal Processing Workshop.Piscataway, NJ:IEEE, 2016:1-5.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network.&amp;quot;">

                                <b>[17]</b> SHI W, CABALLERO J, HUSZAR F, et al.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1874-1883.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network">

                                <b>[18]</b> LEDIG C, THEIS L, HUSZAR F, et al.Photo-realistic single image super-resolution using a generative adversarial network [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:105-114.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">

                                <b>[19]</b> DONG C, CHEN C L, HE K, et al.Image super-resolution using deep convolutional networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_20" >
                                    <b>[20]</b>
                                 JIA Y Q, SHELHAMER E, DONAHUE J, et al.Caffe:convolutional architecture for fast feature embedding [C]// Proceedings of the 22nd ACM International Conference on Multimedia.New York:ACM, 2014:675-678.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analytical relation &amp;amp; comparison of PSNR and SSIM on babbon image and human eye perception using Matlab">

                                <b>[21]</b> MEGHA G, YASHPAL L, VIVEK L.Analytical relation &amp; comparison of PSNR and SSIM on babbon image and human eye perception using Matlab [J].International Journal of Advanced Research in Engineering and Applied Sciences, 2015, 4 (5) :108-119.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905035" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905035&amp;v=MDgwNzMzenFxQnRHRnJDVVI3cWZadVpzRnlEblVyN1BMejdCZDdHNEg5ak1xbzlHWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
