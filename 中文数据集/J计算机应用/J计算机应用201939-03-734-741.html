<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637138992006197500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201903020%26RESULT%3d1%26SIGN%3dVmWFBsNz91VnQ0vONLsaH%252fW51ZY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903020&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903020&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903020&amp;v=MDAzMjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURtVXI3Skx6N0JkN0c0SDlqTXJJOUhaSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="2 互信息理论 ">2 互信息理论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="3 问题陈述与分析 ">3 问题陈述与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="4 MCJMI特征选择方法 ">4 MCJMI特征选择方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="4.1 &lt;b&gt;总体思路&lt;/b&gt;">4.1 <b>总体思路</b></a></li>
                                                <li><a href="#102" data-title="4.2 &lt;b&gt;最大最小互信息&lt;/b&gt;">4.2 <b>最大最小互信息</b></a></li>
                                                <li><a href="#116" data-title="4.3 &lt;b&gt;最大联合条件互信息&lt;/b&gt;">4.3 <b>最大联合条件互信息</b></a></li>
                                                <li><a href="#135" data-title="4.4 &lt;b&gt;方法步骤&lt;/b&gt;">4.4 <b>方法步骤</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#192" data-title="5 实验验证 ">5 实验验证</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#193" data-title="5.1 &lt;b&gt;实验方案&lt;/b&gt;">5.1 <b>实验方案</b></a></li>
                                                <li><a href="#199" data-title="5.2 &lt;b&gt;小样本数据预测精度分析&lt;/b&gt;">5.2 <b>小样本数据预测精度分析</b></a></li>
                                                <li><a href="#210" data-title="5.3 &lt;b&gt;大样本数据预测精度分析&lt;/b&gt;">5.3 <b>大样本数据预测精度分析</b></a></li>
                                                <li><a href="#218" data-title="5.4 &lt;b&gt;非平稳数据预测精度分析&lt;/b&gt;">5.4 <b>非平稳数据预测精度分析</b></a></li>
                                                <li><a href="#226" data-title="5.5 &lt;b&gt;稳定性分析&lt;/b&gt;">5.5 <b>稳定性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#232" data-title="6 结语 ">6 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;样本数据的特征及分类&lt;/b&gt;"><b>表</b>1 <b>样本数据的特征及分类</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;特征与分类之间互信息相关计算结果&lt;/b&gt;"><b>表</b>2 <b>特征与分类之间互信息相关计算结果</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;最小联合互信息计算示意表&lt;/b&gt;"><b>表</b>3 <b>最小联合互信息计算示意表</b></a></li>
                                                <li><a href="#195" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;实验数据集&lt;/b&gt;"><b>表</b>4 <b>实验数据集</b></a></li>
                                                <li><a href="#198" data-title="图1 实验流程">图1 实验流程</a></li>
                                                <li><a href="#205" data-title="图2 不同小样本数据集分类精度对比">图2 不同小样本数据集分类精度对比</a></li>
                                                <li><a href="#207" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同小样本数据集的平均预测精度&lt;/b&gt;"><b>表</b>5 <b>不同小样本数据集的平均预测精度</b></a></li>
                                                <li><a href="#209" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;达到最大精度时特征选择数/特征数&lt;/b&gt;"><b>表</b>6 <b>达到最大精度时特征选择数/特征数</b></a></li>
                                                <li><a href="#216" data-title="&lt;b&gt;表&lt;/b&gt;7 &lt;b&gt;不同大样本数据集的平均预测精度&lt;/b&gt;"><b>表</b>7 <b>不同大样本数据集的平均预测精度</b></a></li>
                                                <li><a href="#217" data-title="图3 不同大样本数据集分类精度对比">图3 不同大样本数据集分类精度对比</a></li>
                                                <li><a href="#224" data-title="图4 数据集secom分类精度对比">图4 数据集secom分类精度对比</a></li>
                                                <li><a href="#225" data-title="图5 特征选择过程时间消耗对比">图5 特征选择过程时间消耗对比</a></li>
                                                <li><a href="#230" data-title="&lt;b&gt;表&lt;/b&gt;8 &lt;b&gt;各方法稳定性比较&lt;/b&gt;"><b>表</b>8 <b>各方法稳定性比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="255">


                                    <a id="bibliography_1" title="GANDHI S S, PRABHUNE S S.Overview of feature subset selection algorithm for high dimensional data[C]//ICISC 2017:Proceedings of the 2017 IEEE International Conference on Inventive Systems and Control.Piscataway, NJ:IEEE, 2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overview of feature subset selection algorithm for high dimensional data">
                                        <b>[1]</b>
                                        GANDHI S S, PRABHUNE S S.Overview of feature subset selection algorithm for high dimensional data[C]//ICISC 2017:Proceedings of the 2017 IEEE International Conference on Inventive Systems and Control.Piscataway, NJ:IEEE, 2017:1-6.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_2" title="FLEURET F.Fast binary feature selection with conditional mutual information[J].Journal of Machine Learning Research, 2004, 5 (3) :1531-1555." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Binary feature selection Conditional Mutual Information">
                                        <b>[2]</b>
                                        FLEURET F.Fast binary feature selection with conditional mutual information[J].Journal of Machine Learning Research, 2004, 5 (3) :1531-1555.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_3" title="LIU H, DITZLER G.Speeding up joint mutual information feature selection with an optimization heuristic[C]//Proceedings of the2017 IEEE Symposium Series on Computational Intelligence.Piscataway, NJ:IEEE, 2018:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speeding up joint mutual information feature selection with an optimization heuristic">
                                        <b>[3]</b>
                                        LIU H, DITZLER G.Speeding up joint mutual information feature selection with an optimization heuristic[C]//Proceedings of the2017 IEEE Symposium Series on Computational Intelligence.Piscataway, NJ:IEEE, 2018:1-8.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_4" title="MIN F, XU J.Semi-greedy heuristics for feature selection with test cost constraints[J].Granular Computing, 2016, 1 (3) :199-211." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-greedy heuristics for feature selection with test cost constraints">
                                        <b>[4]</b>
                                        MIN F, XU J.Semi-greedy heuristics for feature selection with test cost constraints[J].Granular Computing, 2016, 1 (3) :199-211.
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_5" title="TSAGRIS M, LAGANI V, TSAMARDINOS I.Feature selection for high-dimensional temporal data[J].BMC Bioinformatics, 2018, 19:17." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature selection for high-dimensional temporal data">
                                        <b>[5]</b>
                                        TSAGRIS M, LAGANI V, TSAMARDINOS I.Feature selection for high-dimensional temporal data[J].BMC Bioinformatics, 2018, 19:17.
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_6" title="黄志艳.一种基于信息增益的特征选择方法[J].山东农业大学学报 (自然科学版) , 2013, 44 (2) :252-256. (HUANG Z Y.Based on the information gain text feature selection method[J].Journal of Shandong Agricultural University (Natural Science) , 2013, 44 (2) :252-256.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SCHO201302017&amp;v=MDc3MDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbVVyN0pOaTdEWWJHNEg5TE1yWTlFWTRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        黄志艳.一种基于信息增益的特征选择方法[J].山东农业大学学报 (自然科学版) , 2013, 44 (2) :252-256. (HUANG Z Y.Based on the information gain text feature selection method[J].Journal of Shandong Agricultural University (Natural Science) , 2013, 44 (2) :252-256.) 
                                    </a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_7" title="刘海峰, 刘守生, 宋阿羚.基于词频分布信息的优化IG特征选择方法[J].计算机工程与应用, 2017, 53 (4) :113-117. (LIU H F, LIU S S, SONG A L.Improved method of IG feature selection based on word frequency distribution[J].Computer Engineering and Applications, 2017, 53 (4) :113-117.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201704020&amp;v=MDc0Njg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURtVXI3Skx6N01hYkc0SDliTXE0OUhaSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        刘海峰, 刘守生, 宋阿羚.基于词频分布信息的优化IG特征选择方法[J].计算机工程与应用, 2017, 53 (4) :113-117. (LIU H F, LIU S S, SONG A L.Improved method of IG feature selection based on word frequency distribution[J].Computer Engineering and Applications, 2017, 53 (4) :113-117.) 
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_8" title="BATTITI R.Using mutual information for selecting features in supervised neural net learning[J].IEEE Transactions on Neural Networks, 1994, 5 (4) :537-550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using mutual information for selecting features in supervised neural net learning">
                                        <b>[8]</b>
                                        BATTITI R.Using mutual information for selecting features in supervised neural net learning[J].IEEE Transactions on Neural Networks, 1994, 5 (4) :537-550.
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_9" title="HOQUE N, BHATTACHARYYA D K, KALITA J K.MIFS-ND:a mutual information-based feature selection method[J].Expert Systems with Applications, 2014, 41 (14) :6371-6385." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700212104&amp;v=MDQ0NDJ3OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSktGMFZheE09TmlmT2ZiSzhIdGZOcUk5Rlp1b05EWA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        HOQUE N, BHATTACHARYYA D K, KALITA J K.MIFS-ND:a mutual information-based feature selection method[J].Expert Systems with Applications, 2014, 41 (14) :6371-6385.
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_10" title="CHO D, LEE B.Optimized automatic sleep stage classification using the Normalized Mutual Information Feature Selection (NMIFS) method[C]//Proceedings of the 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway, NJ:IEEE, 2017:3094-3097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimized automatic sleep stage classification using the Normalized Mutual Information Feature Selection (NMIFS)method">
                                        <b>[10]</b>
                                        CHO D, LEE B.Optimized automatic sleep stage classification using the Normalized Mutual Information Feature Selection (NMIFS) method[C]//Proceedings of the 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway, NJ:IEEE, 2017:3094-3097.
                                    </a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_11" title="PENG H, LONG F, DING C.Feature selection based on mutual information:criteria of max-dependency, max-relevance, and minredundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226-1238." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy">
                                        <b>[11]</b>
                                        PENG H, LONG F, DING C.Feature selection based on mutual information:criteria of max-dependency, max-relevance, and minredundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226-1238.
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_12" title="董泽民, 石强.基于归一化模糊联合互信息最大的特征选择[J].计算机工程与应用, 2017, 53 (22) :105-110. (DONG Z M, SHI Q.Feature selection using normalized fuzzy joint mutual information maximum[J].Computer Engineering and Applications, 2017, 53 (22) :105-110.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201722019&amp;v=MTg5NDNVUjdxZlp1WnBGaURtVXI3Skx6N01hYkc0SDliT3JZOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        董泽民, 石强.基于归一化模糊联合互信息最大的特征选择[J].计算机工程与应用, 2017, 53 (22) :105-110. (DONG Z M, SHI Q.Feature selection using normalized fuzzy joint mutual information maximum[J].Computer Engineering and Applications, 2017, 53 (22) :105-110.) 
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_13" title="BENNASAR M, HICKS Y, SETCHI R.Feature selection using joint mutual information maximisation[J].Expert Systems with Applications, 2015, 42 (22) :8520-8532." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES789EBD0C7F31EF04A5CF133023653E85&amp;v=MjU2NTUwNXQ5Z3pMdSt3YWs9TmlmT2ZiU3dGNlMrMjQ4Mlk1ME1EUWxQenhKaTcwd0xTWHpockJBMmY3ZVhNTEthQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        BENNASAR M, HICKS Y, SETCHI R.Feature selection using joint mutual information maximisation[J].Expert Systems with Applications, 2015, 42 (22) :8520-8532.
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_14" title="LI J, DONG W, MENG D.Grouped gene selection of cancer via adaptive sparse group lasso based on conditional mutual information[J].IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2018, 15 (6) :2028-2038." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Grouped gene selection of cancer via adaptive sparse group lasso based on conditional mutual information">
                                        <b>[14]</b>
                                        LI J, DONG W, MENG D.Grouped gene selection of cancer via adaptive sparse group lasso based on conditional mutual information[J].IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2018, 15 (6) :2028-2038.
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_15" title="LIU C, WANG W, ZHAO Q, et al.A new feature selection method based on a validity index of feature subset[J].Pattern Recognition Letters, 2017, 92:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new feature selection method based on a validity index of feature subset">
                                        <b>[15]</b>
                                        LIU C, WANG W, ZHAO Q, et al.A new feature selection method based on a validity index of feature subset[J].Pattern Recognition Letters, 2017, 92:1-8.
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_16" title="AMARATUNGA D, CABRERA J.High-dimensional data[J].Journal of the National Science Foundation of Sri Lanka, 2016, 44 (1) :3." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-dimensional data">
                                        <b>[16]</b>
                                        AMARATUNGA D, CABRERA J.High-dimensional data[J].Journal of the National Science Foundation of Sri Lanka, 2016, 44 (1) :3.
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_17" title="DUA, D.AND KARRA TANISKIDOU, E.UCI Machine Learning Repository[DB/OL].[2018-07-13].http://archive.ics.uci.edu/ml." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=UCI Machine Learning Repository">
                                        <b>[17]</b>
                                        DUA, D.AND KARRA TANISKIDOU, E.UCI Machine Learning Repository[DB/OL].[2018-07-13].http://archive.ics.uci.edu/ml.
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_18" title="ROSS B C.Mutual information between discrete and continuous data sets[J].PLo S One, 2014, 9 (2) :e87357." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mutual information between discrete and continuous data sets">
                                        <b>[18]</b>
                                        ROSS B C.Mutual information between discrete and continuous data sets[J].PLo S One, 2014, 9 (2) :e87357.
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_19" title="CHELVAN P M, PERUMAL K.A study on selection stability measures for various feature selection algorithms[C]//Proceedings of the 2016 IEEE International Conference on Computational Intelligence and Computing Research.Piscataway, NJ:IEEE, 2017:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A study on selection stability measures for various feature selection algorithms">
                                        <b>[19]</b>
                                        CHELVAN P M, PERUMAL K.A study on selection stability measures for various feature selection algorithms[C]//Proceedings of the 2016 IEEE International Conference on Computational Intelligence and Computing Research.Piscataway, NJ:IEEE, 2017:1-4.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-31 16:01</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(03),734-741 DOI:10.11772/j.issn.1001-9081.2018081694            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于最大联合条件互信息的特征选择</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AF%9B%E8%8E%BA%E6%B1%A0&amp;code=07172665&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">毛莺池</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E6%B5%B7&amp;code=41113347&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹海</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B9%B3%E8%90%8D&amp;code=27884582&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">平萍</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%99%93%E8%8A%B3&amp;code=30779749&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李晓芳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E6%B5%B7%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=1044544&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河海大学计算机与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B8%B8%E5%B7%9E%E5%B7%A5%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0168968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">常州工学院计算机信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%8B%8F%E9%AB%98%E6%A0%A1%E6%96%87%E5%8C%96%E5%88%9B%E6%84%8F%E5%8D%8F%E5%90%8C%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江苏高校文化创意协同创新中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在高维数据如图像数据、基因数据、文本数据等的分析过程中, 当样本存在冗余特征时会大大增加问题分析复杂难度, 因此在数据分析前从中剔除冗余特征尤为重要。基于互信息 (MI) 的特征选择方法能够有效地降低数据维数, 提高分析结果精度, 但是, 现有方法在特征选择过程中评判特征是否冗余的标准单一, 无法合理排除冗余特征, 最终影响分析结果。为此, 提出一种基于最大联合条件互信息的特征选择方法 (MCJMI) 。MCJMI选择特征时考虑整体联合互信息与条件互信息两个因素, 两个因素融合增强特征选择约束。在平均预测精度方面, MCJMI与信息增益 (IG) 、最小冗余度最大相关性 (mRMR) 特征选择相比提升了6个百分点;与联合互信息 (JMI) 、最大化联合互信息 (JMIM) 相比提升了2个百分点;与LW向前搜索方法 (SFS-LW) 相比提升了1个百分点。在稳定性方面, MCJMI稳定性达到了0.92, 优于JMI、JMIM、SFS-LW方法。实验结果表明MCJMI能够有效地提高特征选择的准确率与稳定性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BF%A1%E6%81%AF%E7%86%B5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息熵;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%92%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">互信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%A1%E4%BB%B6%E4%BA%92%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">条件互信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%94%E5%90%88%E4%BA%92%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">联合互信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征选择;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *毛莺池 (1976—) , 女, 上海人, 教授, 博士, CCF会员, 主要研究方向:分布式计算、并行处理、分布式数据管理;电子邮箱yingchimao@hhu.edu.cn;
                                </span>
                                <span>
                                    曹海 (1991—) , 男, 河南信阳人, 硕士研究生, 主要研究方向:分布式计算、数据挖掘;;
                                </span>
                                <span>
                                    平萍 (1982—) , 女, 江苏吴江人, 副教授, 博士, CCF会员, 主要研究方向:信息编码、数字图像处理;;
                                </span>
                                <span>
                                    李晓芳 (1971—) , 女, 内蒙赤峰人, 副教授, 博士, CCF会员, 主要研究方向:分布式数据管理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>“十三五”国家重点研发计划项目 (2018YFC0407105);</span>
                                <span>华能集团重点研发课题资助项目 (HNKJ17-21);</span>
                                <span>中央高校业务费课题资助项目 (2017B16814, 2017B20914);</span>
                    </p>
            </div>
                    <h1><b>Feature selection based on maximum conditional and joint mutual information</b></h1>
                    <h2>
                    <span>MAO Yingchi</span>
                    <span>CAO Hai</span>
                    <span>PING Ping</span>
                    <span>LI Xiaofang</span>
            </h2>
                    <h2>
                    <span>College of Computer and Information, Hohai University</span>
                    <span>College of Computer and Information Engineering, Changzhou Institute of Technology</span>
                    <span>Jiangsu Collaborative Innovation Center for Cultural Creativity</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the analysis process of high-dimensional data such as image data, genetic data and text data, when samples have redundant features, the complexity of the problem is greatly increased, so it is important to reduce redundant features before data analysis. The feature selection based on Mutual Information (MI) can reduce the data dimension and improve the accuracy of the analysis results, but the existing feature selection methods cannot reasonably eliminate the redundant features because of the single standard. To solve the problem, a feature selection method based on Maximum Conditional and Joint Mutual Information (MCJMI) was proposed. Joint mutual information and conditional mutual information were both considered when selecting features with MCJMI, improving the feature selection constraint. Exerimental results show that the detection accuracy is improved by 6% compared with Information Gain (IG) and minimum Redundancy Maximum Relevance (mRMR) feature selection; 2% compared with Joint Mutual Information (JMI) and Joint Mutual Information Maximisation (JMIM) ; and 1% compared with LW index with Sequence Forward Search algorithm (SFS-LW) . And the stability of MCJMI reaches 0.92, which is better than JMI, JMIM and SFS-LW. In summary the proposed method can effectively improve the accuracy and stability of feature selection.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=information%20entropy&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">information entropy;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Mutual%20Information%20(MI)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Mutual Information (MI) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=conditional%20mutual%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">conditional mutual information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=joint%20mutual%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">joint mutual information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature selection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    MAO Yingchi, born in 1976, Ph. D. , professor. Her research interests include distributed computation, parallel processing, distributed data management.;
                                </span>
                                <span>
                                    CAO Hai, born in 1991, M. S. candidate. His research interests include distributed computation, data mining.;
                                </span>
                                <span>
                                    PING Ping, born in 1982, Ph. D. , associate professor. Her research interests include information coding, digital image processing.;
                                </span>
                                <span>
                                    LI Xiaofang, born in 1982, Ph. D. , associate professor. Her research interests include distributed data management.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Technology Research and Development Program of the Ministry of Science and Technology of China (2018YFC0407905);</span>
                                <span>the Key Technology Project of China Huaneng Group (HNKJ17-21);</span>
                                <span>the Fundamental Research Funds for the Central Universities (2017B16814, 2017B20914);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="41">随着数据时代不断发展, 大数据应用越来越彰显出它的优势, 如图像数据分析、基因数据分析、文本数据分析等。高维数据能够详细记录事物的属性, 同时也存在着大量冗余数据, 冗余数据给数据分析带来了巨大难题。特征选择方法能从高维数据中分析抽取出相关特征, 减小数据维数, 降低分析复杂度。基于互信息的特征选择是Filter<citation id="293" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>类型特征选择方法一个重要研究方向。互信息在相关性分析上有计算简单、可解释性强特点, 因此基于互信息的特征选择方法被广泛应用于特征选择。如Fleuret等<citation id="294" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出通过考虑条件互信息降低数据维数的条件互信息特征选择方法。特征选择效率及策略方面也有相关研究, 一种贪婪向前搜索的联合互信息特征选择方法被提出, 用于解决互信息计算过程中效率问题<citation id="295" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。现有互信息特征选择方法主要从3个方面展开研究:信息增益、条件互信息和联合互信息。其中条件互信息及联合互信息在随着特征不断选择过程中计算变得复杂。针对条件互信息及联合互信息计算复杂问题, 一种启发式方法被应用在特征选择当中, 启发式计算方式大幅降低了互信息计算难度<citation id="296" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="42">基于启发式计算互信息评判特征重要性时, 不同的特征选择方法评判标准有所不同。如信息增益方法单独考虑每个子特征同目标分类之间相关性, 未考虑特征与特征之间关系。联合互信息考虑整体互信息大小, 未考虑单个特征同目标之间的相关性。基于最大联合互信息考虑互信息的整体稳定性, 未考虑联合互信息整体大小。针对现有联合互信息计算方法存在的不足, 本文提出基于最大联合条件互信息的特征选择方法 (feature selection method based on Maximum Conditional and Joint Mutual Information, MCJMI) 。MCJMI特性选择方法基于联合互信息整体稳定性的基础上, 利用条件互信息, 挑选出使整体互信息增长最显著的特征。MCJMI特征选择方法既保证了联合互信息在整体上的稳定性, 同时使所选特征与分类之间的整体互信息增量最大。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="44">随着特征选择领域研究不断发展, 产生了各种类型的特征选择方法。基于互信息理论的特征选择方法最终目的是从所有特征中挑选出指定个数最相关的特征降低高维分类问题复杂度<citation id="297" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="45">基于信息增益 (Information Gain, IG) 的特征选择最早应用于特征选择, 通过IG分析特征与分类之间相关性大小从而排除冗余特征<citation id="298" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。IG在特征选择过程中计算简单, 仅考虑每个特征与分类之间的互信息大小, 能在<i>o</i> (<i>n</i>) 时间复杂度内完成。由于IG选择条件简单, 为了增强冗余特征判断, Liu等<citation id="299" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了一种基于类别与类别之间特征分布改进的IG文本方法。</p>
                </div>
                <div class="p1">
                    <p id="46">Battiti等<citation id="300" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出基于互信息的特征选择方法 (Mutual Information Feature Selection, MIFS) , MIFS方法既考虑已选特征与分类之间的互信息, 同时考虑已选特征与未选特征之间的相关性, MIFS不再假设特征之间独立。其计算方式分析主要由两部分组成, 一部分为未选特征与分类之间的互信息, 另一部分为未选择特征与已选特征互信息求和。MIFS存在多个改进版本, 如Hoque等<citation id="301" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>研究的MIFS-ND方法, Cho等<citation id="302" type="reference"><link href="273" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出的归一化互信息特征选择 (Normalized Mutual Information Feature Selection, NMIFS) 方法其在表现上都优于MIFS。</p>
                </div>
                <div class="p1">
                    <p id="47">Peng等<citation id="303" type="reference"><link href="275" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>将最大依赖性、最大相关性和最小冗余度准则应用到特征选择当中, 提出了基于互信息的特征选择最大依赖性、最小冗余度最大相关性 (minimum Redundancy Maximum Relevance, mRMR) 准则的特征选择方法。mRMR特征选择将MIFS方法中参数处理成已选特征个数的倒数使选择标准一致。</p>
                </div>
                <div class="p1">
                    <p id="48">基于联合互信息的特征选择方法在子特征选择中也有广泛应用。董泽民等<citation id="304" type="reference"><link href="277" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>使用基于联合互信息 (Joint Mutual Information, JMI) 的特征选择方法。JMI加入了分类变量, 在特征选择时不仅需考虑所选特征同分类之间的关系, 同时, 考虑在有分类条件下子集特征与未选特征之间的互信息大小。Bennasar等<citation id="305" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一种基于最大化联合互信息 (Joint Mutual Information Maximisation, JMIM) 的特征选择方法, JMIM考虑联合互信息整体稳定性。</p>
                </div>
                <div class="p1">
                    <p id="49">基于条件互信息的特征选择同样也有着广泛的应用。Li等<citation id="306" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了一种通过条件互信息改进的自适应稀疏群套索方法, 改进了分块下降方法, 提高了分类选择的精度。在互信息发展的过程中, 也出现了类型相同的特征选择方法, 如Liu等<citation id="307" type="reference"><link href="283" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出的LW索引向前搜索方法 (LW index with Sequence Forward Search algorithm, SFS-LW) 。SFS-LW特征选择方法与上述JMIM方法在计算选择过程相似, 不同的是SFS-LW采用了基于类与类之间距离作为特征选择的标准。</p>
                </div>
                <div class="p1">
                    <p id="50">综上, 特征选择方法根据特征与分类之间的互信息大小, 作为特征选择评判标准。在评判特征是否冗余时, 考虑的标准单一, 如仅考虑联合互信息或仅考虑条件信息就造成了不同特征方法选择结果不同。本文采用联合互信息与条件互信息结合的方式, 分析特征之间的冗余性, 以提高对冗余特征的筛选效果。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">2 互信息理论</h3>
                <div class="p1">
                    <p id="52">1948年, 香农提出“信息熵”的概念, 对于数据序列<i>X</i>= (<i>x</i><sub>1</sub>, <i>x</i><sub><i>i</i></sub>, …, <i>x</i><sub><i>m</i></sub>) 其求熵公式如式 (1) 所示:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>X</mi></mrow></munder><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">式 (1) 中<i>p</i> (<i>x</i><sub><i>i</i></sub>) 表示<i>x</i><sub><i>i</i></sub>在<i>X</i>中的概率密度。两个变量联合熵大小表示两个变量在一起的不确定性度量。条件信息熵表示已知其中一个变量情况下求另一变量<i>C</i>= (<i>c</i><sub>1</sub>, <i>c</i><sub><i>i</i></sub>, …, <i>c</i><sub><i>m</i></sub>) 情况下信息熵大小, 两者公式如下:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Η</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>, </mo><mi>C</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>X</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>C</mi></mrow></munder><mi>p</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Η</mi><mo stretchy="false"> (</mo><mi>C</mi><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>X</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>C</mi></mrow></munder><mi>p</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">式 (2) 中<i>p</i> (<i>x</i><sub><i>i</i></sub>, <i>c</i><sub><i>j</i></sub>) 表示两个变量的组合概率密度函数, 式 (3) 中<i>p</i> (<i>x</i><sub><i>i</i></sub>|<i>c</i><sub><i>j</i></sub>) 表示在已知<i>c</i><sub><i>j</i></sub>情况下<i>p</i> (<i>x</i><sub><i>i</i></sub>) 的概率密度。由式 (1) 和式 (3) 可得式 (4) 、 (5) 表示如下:</p>
                </div>
                <div class="p1">
                    <p id="57"><i>H</i> (<i>X</i>, <i>C</i>) =<i>H</i> (<i>X</i>) +<i>H</i> (<i>C</i>|<i>X</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="58"><i>H</i> (<i>X</i>, <i>C</i>) =<i>H</i> (<i>C</i>) +<i>H</i> (<i>X</i>|<i>C</i>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="59">熵大小表示变量之间的稳定性, 而互信息大小能够表示变量之间的相似程度, 互信息定义如下式 (6) 表示:</p>
                </div>
                <div class="p1">
                    <p id="60"><i>I</i> (<i>X</i>;<mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>X</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>C</mi></mrow></munder><mi>p</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>p</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="62">其中<i>I</i> (<i>X</i>;<i>C</i>) 表示<i>X</i>, <i>C</i>两者之间的共享信息度量。<i>I</i> (<i>X</i>;<i>C</i>) 越大说明<i>X</i>, <i>C</i>相关性越强。式 (6) 通过式 (1) 、式 (2) 转换, 互信息可表示为熵的形式如式 (7) :</p>
                </div>
                <div class="p1">
                    <p id="63"><i>I</i> (<i>X</i>;<i>C</i>) =<i>H</i> (<i>X</i>) +<i>H</i> (<i>C</i>) -<i>H</i> (<i>X</i>, <i>C</i>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="64">加入条件<i>Y</i>= (<i>y</i><sub>1</sub>, <i>y</i><sub><i>i</i></sub>, …, <i>y</i><sub><i>m</i></sub>) 后条件互信息由信息熵公式定义如下:</p>
                </div>
                <div class="p1">
                    <p id="65"><i>I</i> (<i>X</i>;<i>C</i>|<i>Y</i>) =<i>H</i> (<i>X</i>|<i>C</i>) -<i>H</i> (<i>X</i>|<i>C</i>, <i>Y</i>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="66">其中<i>Y</i>为条件下<i>X</i>, <i>C</i>之间的互信息大小。联合互信息由式 (7) 、式 (8) 得出如下公式:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>I</i> (<i>X</i>, <i>Y</i>;<i>C</i>) =<i>I</i> (<i>X</i>;<i>C</i>|<i>Y</i>) +<i>I</i> (<i>C</i>;<i>Y</i>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="68">式 (9) 表示联合互信息, 即考虑<i>X</i>, <i>Y</i>整体同<i>C</i>之间的关系。同时根据式 (9) 能够得到联合互信息由条件互信息与互信息之和组成。根据互信息计算式 (6) 得出联合互信息求和公式如下:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>I</i> (<i>X</i>, <i>Y</i>;<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>X</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Y</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∈</mo><mi>C</mi></mrow></munder><mi>p</mi></mstyle></mrow></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mtext>l</mtext><mtext>b</mtext><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>p</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="71"><i>I</i> (<i>X</i>, <i>Y</i>;<i>C</i>) 值越大说明<i>X</i>, <i>Y</i>同<i>C</i>之间的相关性越强。</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag">3 问题陈述与分析</h3>
                <div class="p1">
                    <p id="73">研究高维多特征数据子集问题时通常对特征筛选, 剔除冗余特征, 利用较少的特征达到更好的分类效果。数学符号描述如下, 设<i>F</i>={<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>, …, <i>f</i><sub><i>i</i></sub>, …, <i>f</i><sub><i>n</i>-1</sub>, <i>f</i><sub><i>n</i></sub>}, <i>F</i>为全特征集合, <i>f</i><sub><i>i</i></sub>∈<i>F</i>, 1≤<i>i</i>≤<i>n</i>, <i>n</i>为特征总量。 <i>f</i><sub><i>i</i></sub>={<i>x</i><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>, <i>x</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>, …, <i>x</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>i</mi></msubsup></mrow></math></mathml>, …, <i>x</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>i</mi></msubsup></mrow></math></mathml>}表示特征<i>f</i><sub><i>i</i></sub>的样本集合, <i>x</i><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>i</mi></msubsup></mrow></math></mathml>为<i>f</i><sub><i>i</i></sub>的一个样本值, 1≤<i>k</i>≤<i>m</i>, <i>m</i>为样本记录数。<i>C</i>={<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>w</i></sub>}, <i>C</i>为类别集合。如果特征数量<i>n</i>较大且<i>m</i>≪<i>n</i>时分类问题变得复杂, 传统的分类模型对高维数据适应性较低<citation id="308" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="79">基于互信息的特征选择方法, 从全部特征集<i>F</i>中挑选出<i>w</i>数量特征构成子集记作<i>S</i>={<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>, …, <i>f</i><sub><i>i</i></sub>, …, <i>f</i><sub><i>w</i></sub>}, <i>S</i>⊆<i>F</i>, 1≤<i>s</i>≤<i>n</i>且满足<i>S</i>作输入预测分类时, 结果等效或优于<i>F</i>作输入时的分类结果。如第2章中所介绍的互信息选择方法mRMR、JMI、JMIM、SFS-LW等。由于现有特征在计算中所选的评判标准不同, 在特征选择时所得出的特征选择结果有所差异, 现有方法不能合理剔除冗余特征。</p>
                </div>
                <div class="p1">
                    <p id="80">下面给出mRMR、 JMI、 JMIM、SFS-LW计算过程对比说明, mRMR特征选择计算过程如式 (11) 所示:</p>
                </div>
                <div class="p1">
                    <p id="81"><i>α</i> (<i>f</i><sub><i>i</i></sub>) =<i>I</i> (<i>f</i><sub><i>i</i></sub>;<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">) </mo><mo>-</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>; <i>f</i><sub><i>s</i></sub>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="83">其中计算以<i>I</i> (<i>f</i><sub><i>i</i></sub>;<i>C</i>) 为基础值, 考虑未加入的特征与每个子特征的互信息。mRMR特征选择方法能够排除与已选特征具有直接相关的特征, 但无法筛选具有间接相关的特征。若存在<i>f</i><sub><i>i</i></sub>∈<i>F</i>-<i>S</i>, <i>f</i><sub><i>s</i>1</sub>, <i>f</i><sub><i>s</i>2</sub>∈<i>S</i>, 其中<i>f</i><sub><i>i</i></sub>分别与<i>f</i><sub><i>s</i>1</sub>, <i>f</i><sub><i>s</i>2</sub>独立, 当<i>f</i><sub><i>i</i></sub>与{<i>f</i><sub><i>s</i>1</sub>, <i>f</i><sub><i>s</i>2</sub>}存在较强的相关性时, mRMR方法无法判断加入特征是否冗余。JMI与mRMR有所不同, 考虑联合互信息选择特征, 其计算过程如式 (12) 所示:</p>
                </div>
                <div class="p1">
                    <p id="84"><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></math></mathml>;<i>C</i>)      (12) </p>
                </div>
                <div class="p1">
                    <p id="86">根据式 (12) JMI计算公式能够得出, JMI考虑每个<i>f</i><sub><i>i</i></sub>加入后的联合互信息总和。JMI筛选特征时关注整体互信息的大小从整体考虑特征相关性, 当<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></math></mathml>;<i>C</i>) 计算值较大即可。若<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) 中存在比较小值时, 当存在<i>f</i><sub><i>i</i></sub>∈<i>F</i>-<i>S</i>, <i>f</i><sub><i>s</i>1</sub>∈<i>S</i>, <i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i>1</sub>;<i>C</i>) 较小时, 表明<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i>1</sub>相似, JMI并未考虑。JMIM改进了JMI中存在的不足, JMIM具体计算过程如式 (13) 所示:</p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>F</mi><mo>-</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></math></mathml>;<i>C</i>) ) )      (13) </p>
                </div>
                <div class="p1">
                    <p id="90">根据式 (13) 中JMIM计算公式得出, JMIM与JMI有所不同, JMIM考虑每个<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) 的值。JMIM满足加入特征<i>f</i><sub><i>i</i></sub>后, 至少子集中存在特征<i>f</i><sub><i>s</i></sub>使得<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) 比加入其他特征时所求的联合互信息大。下面通过构造数据说明JMIM存在的不足, 构造数据如表1所示。</p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit"><b>表</b>1 <b>样本数据的特征及分类</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Features and classification for samples</p>
                    <p class="img_note"></p>
                    <table id="91" border="1"><tr><td><br />样本</td><td><i>f</i><sub>1</sub></td><td><i>f</i><sub>2</sub></td><td><i>f</i><sub>3</sub></td><td><i>f</i><sub>4</sub></td><td><i>f</i><sub>5</sub></td><td><i>C</i></td></tr><tr><td><br />1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td></tr><tr><td><br />2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td></tr><tr><td><br />3</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td><br />4</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td><br />5</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td><br />6</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="92">表1数据集中包含5个特征分别为<i>f</i><sub>1</sub>、 <i>f</i><sub>2</sub>、 <i>f</i><sub>3</sub>、 <i>f</i><sub>4</sub>、 <i>f</i><sub>5</sub>以及分类<i>C</i>。其中样本数量为6。根据互信息相关计算得出结果如表2所示。</p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表</b>2 <b>特征与分类之间互信息相关计算结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Mutual information calculation result</p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td><br />互信息</td><td>联合互信息</td></tr><tr><td><br /><i>I</i> (<i>f</i><sub>1</sub>;<i>C</i>) =0.318 3</td><td><i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>;<i>C</i>) =0.374 9</td></tr><tr><td><br /><i>I</i> (<i>f</i><sub>2</sub>;<i>C</i>) =0.056 6</td><td><i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>3</sub>;<i>C</i>) =0.374 9</td></tr><tr><td><br /><i>I</i> (<i>f</i><sub>3</sub>;<i>C</i>) =0.056 6</td><td><i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>4</sub>;<i>C</i>) =0.374 9</td></tr><tr><td><br /><i>I</i> (<i>f</i><sub>4</sub>;<i>C</i>) =0.056 6</td><td><i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>5</sub>;<i>C</i>) =0.318 3</td></tr><tr><td><br /><i>I</i> (<i>f</i><sub>5</sub>;<i>C</i>) =0.000 0</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="94">根据表2中得出, 当使用联合互信息选择特征时出现<i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>;<i>C</i>) , <i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>3</sub>;<i>C</i>) , <i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>4</sub>;<i>C</i>) , 计算所得结果为0.374 9。特征<i>f</i><sub>2</sub>, <i>f</i><sub>3</sub>, <i>f</i><sub>4</sub>三者都满足JMIM特征选择条件。JMIM并未考虑联合互信息相等或相似的情况。SFS-LW计算过程与JMIM计算过程相似, 其计算过程如式 (14) 所示:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>F</mi><mo>-</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow></munder><mrow><mi>min</mi></mrow></mstyle><mspace width="0.25em" /><mi>F</mi><mi>D</mi><msub><mrow></mrow><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中:<i>FD</i><sub><i>c</i><sub><i>i</i></sub>|<i>c</i><sub><i>j</i></sub></sub>, <i>c</i><sub><i>i</i></sub>≠<i>c</i><sub><i>j</i></sub>表示各个分类之最小距离。SFS-LW同JMIM在特征选择过程相似, 但所采用的评判标准有所不同, SFS-LW计算类与类之间距离进行特征选择, 基于距离方式的评判标准对数据格式适应性强, 无需考虑数据的连续或离散, 而基于互信息的特征选择则需要对离散-连续数据进行处理操作。当数据噪声较小的情况, SFS-LW能够很好地进行特征分类, 数据存在多个噪声数据时基于互信息的方式表现更好。SFS-LW在计算类之间距离时间复杂为<i>o</i> (|<i>C</i>|<sup>2</sup>) , SFS-LW在非平稳数据特征选择时, 由于计算时仅考虑距离, 每类数据样本多少对SFS-LW影响较小。</p>
                </div>
                <div class="p1">
                    <p id="97">通过对上述mRMR、JMI、JMIM、SFS-LW方法分析得出, mRMR在特征选择时考虑从单个特征加入后互信息大小情况, 无法排除两个及以上特征相关情况。JMI通过联合互信息的方式降低了两个以上特征相关性的情况, 忽略了单个特征相关性的情况, 而SFS-LW对数据类型的适应性强, 对非平衡数据适应较强, 但存在时间复杂度及噪声影响问题。JMIM是对JMI的一种改进, 考虑当加入不同特征时, 选择其中每组最小值, 选择最小值中使得<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) 最大的<i>f</i><sub><i>i</i></sub>。JMIM从所有已选特征联合互信息考虑, 保证所选的特征稳定性, JMIM未考虑到整体相关性大小。随着特征的增加{<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) }中存在<i>f</i><sub><i>s</i>1</sub>, <i>f</i><sub><i>s</i>2</sub>∈<i>S</i>, <i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>j</i></sub>∈<i>F</i>-<i>S</i>且<i>s</i>1≠<i>s</i>2, <i>i</i>≠<i>j</i>时, 有<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i>1</sub>;<i>C</i>) ≈<i>I</i> (<i>f</i><sub><i>j</i></sub>, <i>f</i><sub><i>s</i>2</sub>;<i>C</i>) 。使用JMIM方法选中虽然会选择出最小最大值如<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i>1</sub>;<i>C</i>) , 但JMIM忽略了相似值的<i>I</i> (<i>f</i><sub><i>j</i></sub>, <i>f</i><sub><i>s</i>2</sub>;<i>C</i>) , 显然当存在相似情况时, 需要考虑特征与已选特征的冗余关系。</p>
                </div>
                <div class="p1">
                    <p id="98">对于上述方法中存在的冗余特征筛选问题, 本文提出的方法考虑在特征选择计算联合互信息时考虑数据最小最大联合互信息相似情况, 当有多个相似值情况时, 通过条件互信息选择使整体互信息较大的<i>f</i><sub><i>i</i></sub>加入特征子集中, 同时使得<i>f</i><sub><i>i</i></sub>与<i>S</i>之间的冗余性达到最低。</p>
                </div>
                <h3 id="99" name="99" class="anchor-tag">4 MCJMI特征选择方法</h3>
                <h4 class="anchor-tag" id="100" name="100">4.1 <b>总体思路</b></h4>
                <div class="p1">
                    <p id="101">MCJMI方法从联合互信息与条件互信息两部分考虑所选择特征, 同时结合最小最大原则作特征选择选择过程中涉及两个部分:特征与分类之间的相关性分析, 已选特征与未选特征之间的冗余分析。提出的方法, 主要解决现有联合互信息方法在特征选择过程中出现的无法排除冗余及不相关特征选择问题。最终方法在指定子集大小情况下, 挑选出子集<i>S</i>使<i>I</i> (<i>S</i>;<i>C</i>) 最大。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">4.2 <b>最大最小互信息</b></h4>
                <div class="p1">
                    <p id="103">特征相关性:当已选特征子集<i>S</i>={<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>, …, <i>f</i><sub><i>s</i>-1</sub>, <i>f</i><sub><i>s</i></sub>}, <i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>j</i></sub>∈<i>F</i>-<i>S</i>, 如果<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>S</i>;<i>C</i>) &gt;<i>I</i> (<i>f</i><sub><i>j</i></sub>, <i>S</i>;<i>C</i>) , 则在子集<i>S</i>为前提条件下<i>f</i><sub><i>i</i></sub>与<i>C</i>相关性更强。</p>
                </div>
                <div class="p1">
                    <p id="104">特征冗余性:当<i>f</i><sub><i>i</i></sub>∈<i>F</i>-<i>S</i>, <i>S</i>={<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>, …, <i>f</i><sub><i>s</i>-1</sub>, <i>f</i><sub><i>s</i></sub>}若<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>S</i>;<i>C</i>) =<i>I</i> (<i>S</i>;<i>C</i>) , 则<i>f</i><sub><i>i</i></sub>对子集<i>S</i>属于冗余特征。</p>
                </div>
                <div class="p1">
                    <p id="105">最小联合互信息:已选特征子集<i>S</i>, <i>f</i><sub><i>i</i></sub>∈<i>F</i>-<i>S</i>, 当每个<i>f</i><sub><i>i</i></sub>加入集合<i>S</i>计算联合互信息时, 求得{<i>I</i> (<i>f</i><sub><i>s</i></sub>, <i>f</i><sub><i>i</i></sub>) }集合中的最小值记作最小联合互信息<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><mo stretchy="false">{</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo stretchy="false">) </mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="107">最小联合互信息集合:根据最小联合互信息计算方法, <i>F</i>-<i>S</i>中的每个<i>f</i><sub><i>i</i></sub>计算得到一个最小联合互信息<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><mo stretchy="false">{</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo stretchy="false">) </mo></mrow></math></mathml>, 最终得到集合为<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>C</i>) }, <i>f</i><sub><i>i</i></sub>∈<i>F</i>-<i>S</i>, 集合大小为|<i>F</i>-<i>S</i>|。具体计算过程解释如表3。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit"><b>表</b>3 <b>最小联合互信息计算示意表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Minimum joint mutual information calculation diagram</p>
                    <p class="img_note"></p>
                    <table id="110" border="1"><tr><td><br /><i>f</i><sub>1</sub></td><td><i>f</i><sub>2</sub></td><td><i>f</i><sub>3</sub></td><td><i>f</i><sub>4</sub></td><td><i>f</i><sub>5</sub></td></tr><tr><td><br />Null</td><td><i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>;<i>C</i>) </td><td><i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>3</sub>;<i>C</i>) </td><td><i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>4</sub>;<i>C</i>) </td><td><i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>5</sub>;<i>C</i>) </td></tr><tr><td><br />Null</td><td>Null</td><td><i>I</i> (<i>f</i><sub>2</sub>, <i>f</i><sub>3</sub>;<i>C</i>) </td><td><i>I</i> (<i>f</i><sub>2</sub>, <i>f</i><sub>4</sub>;<i>C</i>) </td><td><i>I</i> (<i>f</i><sub>2</sub>, <i>f</i><sub>5</sub>;<i>C</i>) </td></tr><tr><td><br />Null</td><td>Null</td><td>…</td><td>…</td><td>…</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="111">表3中当子集<i>S</i>={<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>}时, 表中Null表示已加入子集中的<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>不再与子集中其他特征作计算。通过<i>f</i><sub>3</sub>为例具体说明最小最大集合的选择过程, <i>f</i><sub>3</sub>∈<i>F</i>-<i>S</i>。当<i>f</i><sub>3</sub>与子集中<i>f</i><sub>1</sub>, <i>f</i><sub>2</sub>分别计算得到<i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>3</sub>;<i>C</i>) , <i>I</i> (<i>f</i><sub>2</sub>, <i>f</i><sub>3</sub>;<i>C</i>) 。如表3中<i>f</i><sub>3</sub>所在列所示, 当<i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>3</sub>;<i>C</i>) &gt;<i>I</i> (<i>f</i><sub>2</sub>, <i>f</i><sub>3</sub>;<i>C</i>) 将<i>I</i> (<i>f</i><sub>2</sub>, <i>f</i><sub>3</sub>;<i>C</i>) 加入最小最大集合中, 相反地将<i>I</i> (<i>f</i><sub>1</sub>, <i>f</i><sub>3</sub>;<i>C</i>) 加入集合中。同理<i>f</i><sub>4</sub>, <i>f</i><sub>5</sub>计算分别挑选出最小值加入到集合中。表3计算所得的集合大小为|<i>F</i>-<i>S</i>|=3。</p>
                </div>
                <div class="p1">
                    <p id="112">最小最大联合互信息相似集合:通过最小联合互信息计算得到加入不同未选特征的最小联合互信息集合。在最小联合互信息集合中找出最大值。若集合中存在与最大值相等或相似值时加入到最小最大联合互信息相似集合, 该集合公式表示如下:</p>
                </div>
                <div class="p1">
                    <p id="113"><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>i</mi><mi>m</mi><mo>=</mo><mo stretchy="false">{</mo><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>;<i>C</i>) ) ) }      (15) </p>
                </div>
                <div class="p1">
                    <p id="115">式 (15) 中<i>sim</i>表示与最小最大值相等或相似的值集合。其中, <i>f</i><sub><i>j</i></sub>∈<i>F</i>-<i>S</i>, <i>f</i><sub><i>s</i></sub>∈<i>S</i>, 集合相似值的个数为超参数且超参数的值小于|<i>F</i>-<i>S</i>|。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">4.3 <b>最大联合条件互信息</b></h4>
                <div class="p1">
                    <p id="117">特征选择过程中, 希望每次选择的特征能够最大限度地提升<i>S</i>与分类<i>C</i>之间的互信息值。根据特征选择的特点, 提出了MCJMI方法。最大联合条件互信息不仅考虑每次联合互信息是否最大, 同时考虑条件互信息是否满足要求。条件互信息排除与子集<i>S</i>冗余的特征, 增强了特征选择的约束。</p>
                </div>
                <div class="p1">
                    <p id="118">最大联合条件互信息定义:特征<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>j</i></sub>∈<i>F</i>-<i>S</i>, <i>f</i><sub><i>s</i>1</sub>, <i>f</i><sub><i>s</i>2</sub>∈<i>S</i>, <i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i>1</sub>;<i>C</i>) ≈<i>I</i> (<i>f</i><sub><i>j</i></sub>, <i>f</i><sub><i>s</i>2</sub>;<i>C</i>) 且<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i>1</sub>;<i>C</i>) , <i>I</i> (<i>f</i><sub><i>j</i></sub>, <i>f</i><sub><i>s</i>2</sub>;<i>C</i>) 属于式 (15) 所描述的集合。 <i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>j</i></sub>分别加入子集<i>S</i>计算条件互信息和, 若<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">|</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo><mo>&lt;</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>;<i>C</i>|<i>f</i><sub><i>s</i></sub>) 则有<i>f</i><sub><i>i</i></sub>相比<i>f</i><sub><i>j</i></sub>, 子集<i>S</i>与<i>f</i><sub><i>i</i></sub>特征之间的冗余性更小。</p>
                </div>
                <div class="p1">
                    <p id="121">最大联合条件互信息定义证明:由联合互信息计算公式<i>I</i> (<i>X</i>, <i>Y</i>;<i>C</i>) =<i>I</i> (<i>X</i>;<i>C</i>|<i>Y</i>) +<i>I</i> (<i>C</i>;<i>Y</i>) 知, 当<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>j</i></sub>∈<i>F</i>-<i>S</i>, <i>f</i><sub><i>i</i></sub>≠<i>f</i><sub><i>j</i></sub>, <i>f</i><sub><i>s</i></sub>∈<i>S</i>时可得到如下式 (16) 、 (17) :</p>
                </div>
                <div class="p1">
                    <p id="122"><i>I</i> (<i>f</i><sub><i>i</i></sub>;<i>C</i>|<i>f</i><sub><i>s</i></sub>) =<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) -<i>I</i> (<i>f</i><sub><i>s</i></sub>;<i>C</i>)      (16) </p>
                </div>
                <div class="p1">
                    <p id="123"><i>I</i> (<i>f</i><sub><i>j</i></sub>;<i>C</i>|<i>f</i><sub><i>s</i></sub>) =<i>I</i> (<i>f</i><sub><i>j</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) -<i>I</i> (<i>f</i><sub><i>s</i></sub>;<i>C</i>)      (17) </p>
                </div>
                <div class="p1">
                    <p id="124">式 (16) 、 (17) 中条件互信息与联合互信息成正比。对于相同的<i>f</i><sub><i>s</i></sub>, 如果联合互信息越大, 则有以<i>f</i><sub><i>s</i></sub>为前提的条件互信息越大。通过在子集<i>S</i>条件互信息求和可得到如下公式:</p>
                </div>
                <div class="p1">
                    <p id="125"><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">|</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></math></mathml>;<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">) </mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></math></mathml>;<i>C</i>)      (18) </p>
                </div>
                <div class="p1">
                    <p id="129"><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>;<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">|</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></math></mathml>;<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">) </mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></math></mathml>;<i>C</i>)      (19) </p>
                </div>
                <div class="p1">
                    <p id="133">根据最小最大相似集合知<i>f</i><sub><i>s</i></sub>∈<i>S</i>, 当最小值集合{<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) }, {<i>I</i> (<i>f</i><sub><i>j</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) }中最小值相同时, 从整体稳定性上<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>j</i></sub>加入后等效。</p>
                </div>
                <div class="p1">
                    <p id="134">假设当<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) -<i>I</i> (<i>f</i><sub><i>s</i></sub>;<i>C</i>) &lt;<i>I</i> (<i>f</i><sub><i>j</i></sub>, <i>f</i><sub><i>s</i></sub>;<i>C</i>) -<i>I</i> (<i>f</i><sub><i>s</i></sub>;<i>C</i>) 时, 加入<i>f</i><sub><i>j</i></sub>对互信息<i>I</i> (<i>f</i><sub><i>s</i></sub>;<i>C</i>) 增加较大, 由互信息一致性定义对于<i>f</i><sub><i>s</i></sub>来说<i>f</i><sub><i>j</i></sub>冗余程度小于<i>f</i><sub><i>i</i></sub>。因此当最小联合互信息相似或相同情况下, 根据条件互信息能够区分如何选择特征降低所选特征与子集<i>S</i>之间的冗余。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">4.4 <b>方法步骤</b></h4>
                <div class="p1">
                    <p id="136">根据4.3节特征之间相关性冗余性分析, 本文提出基于最小最大联合条件互信息的特征选择方法, 计算公式如式 (20) 、 (21) 所示:</p>
                </div>
                <div class="p1">
                    <p id="137"><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>C</i>|<i>f</i><sub><i>s</i></sub>) )      (20) </p>
                </div>
                <div class="p1">
                    <p id="139"><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">{</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>F</mi><mo>-</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>∈</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>C</i>) ) ) }      (21) </p>
                </div>
                <div class="p1">
                    <p id="141">式 (19) 中<mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>C</i>|<i>f</i><sub><i>s</i></sub>) 表示当<i>f</i><sub><i>i</i></sub>加入后, <i>f</i><sub><i>i</i></sub>与每个子特征<i>f</i><sub><i>s</i></sub>的条件互信息求和, 可理解为加入<i>f</i><sub><i>i</i></sub>后互信息的增量大小。其中<i>f</i><sub><i>i</i></sub>则约束在最小最大相似集合中式 (21) 表示的集合中。</p>
                </div>
                <div class="p1">
                    <p id="143">当<i>f</i><sub><i>i</i></sub>满足式 (21) 集合时, 由最小最大联合互信息知, 当加入<i>f</i><sub><i>i</i></sub>满足了在所有特征子集中, 至少存在一个特征使得联合互信息大于其他特征。当存在多个相似值时, 方法判断其对子集特征的整体增量, 通过整体互信息增量排除冗余性特征。通过增量大小方法确定最终要选择的特征, 其方法流程如下:</p>
                </div>
                <div class="p1">
                    <p id="144">MCJMI方法流程。</p>
                </div>
                <div class="p1">
                    <p id="145">输入:数据集<i>F</i>, 分类<i>C</i>, 特征数<i>n</i>, 要选择的特征数<i>num</i>, 相似集合大小<i>m</i>;</p>
                </div>
                <div class="p1">
                    <p id="146">输出:要选择的特征集合<i>S</i>。</p>
                </div>
                <div class="area_img" id="254">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201903020_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="254">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201903020_25401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="191">MCJMI方法流程中, <i>F</i>为数据样本所有特征, <i>n</i>表示特征量, <i>C</i>表示样本对应的分类。<i>num</i>、<i>m</i>分别表示最终要选择的特征数量与最大相似集合的大小。<i>P</i>表示特征输入时的下标, 方法流程中的<i>List</i>用来存储每次计算的联合互信息。方法循环计算, 每次挑选出最适合的特征子集, 当子集大小满足|<i>S</i>|=<i>num</i>方法结束。MCJMI在每次计算特征互信息时间复杂性为<i>o</i> (|<i>C</i>|) 。根据方法流程得出方法复杂度与需要选择的特征数<i>num</i>, 待选特征集合大小|<i>F</i>-<i>S</i>|相关, 同JMIM方法复杂度相同。</p>
                </div>
                <h3 id="192" name="192" class="anchor-tag">5 实验验证</h3>
                <h4 class="anchor-tag" id="193" name="193">5.1 <b>实验方案</b></h4>
                <div class="p1">
                    <p id="194">实验数据来自UCI公开数据集<citation id="309" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 如表4所示, 其中数据集Breast-cancer、Sonar、Parkinsons在文献<citation id="310" type="reference">[<a class="sup">10</a>]</citation>使用到, 使用相同数据集以达到验证实验结果的作用。剩余数据集则根据不同数据类型从UCI数据集挑选所得。实验中按照数据集的样本大小, 将数据分为两个部分具体见表4。表4中编号1～4数据集属于较少样本的数据集, 编号5～7数据集属于样本较多的数据集, 编号8数据为非平衡数据集。</p>
                </div>
                <div class="area_img" id="195">
                                            <p class="img_tit">
                                                <b>表</b>4 <b>实验数据集</b>
                                                    <br />
                                                Tab. 4 Experimental dataset
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903020_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201903020_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903020_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 实验数据集" src="Detail/GetImg?filename=images/JSJY201903020_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>
                                <p class="img_note">注:特征表示样本中属性的个数;类别表示样本总地被分为几类。</p>

                </div>
                <div class="p1">
                    <p id="196">每个数据集随机划分80%数据作训练集, 20%数据作测试集。实验数据中存在离散型数据、连续型数据、离散+连续型数据, 为保证特征选择模型能够适用计算连续及离散特征数据类型, 采用基于<i>K</i>-近邻 (<i>K</i> Nearest Neighbors, <i>K</i>NN) 互信息计算方法<citation id="311" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。所有数据集数据采用归一化方法处理到0～1区间以降低特征选择过程计算复杂度。</p>
                </div>
                <div class="p1">
                    <p id="197">实验中将MCJMI与IG、mRMR、JMI、JMIM、SFS-LW五种特征选择方法作比较。为验证特征选择的效果, 方法将已选的特征子集<i>S</i>输入到<i>K</i>NN (<i>n</i>=3) 及贝叶斯分类模型中, 通过分类的正确率评判选择特征的合理性。为避免出现偶然性的实验结果, 实验中分别对每个数据集进行5次实验, 预测结果取均值, 具体实验流程如图1所示。</p>
                </div>
                <div class="area_img" id="198">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903020_198.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 实验流程" src="Detail/GetImg?filename=images/JSJY201903020_198.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 实验流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903020_198.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Experimental flow chart</p>

                </div>
                <h4 class="anchor-tag" id="199" name="199">5.2 <b>小样本数据预测精度分析</b></h4>
                <div class="p1">
                    <p id="200">图2中横坐标表示数据集上选择的特征数量, 纵坐标表示对应特征数量下, <i>K</i>NN与贝叶斯分类平均预测精度。图2给出了样本较少数据集上各个方法在特征选择过程中预测精度变化情况。</p>
                </div>
                <div class="p1">
                    <p id="201">通过Flowmeters、wine、Sonar、Parkinsons数据集上不同特征方法下分类预测精度变化图可得出以下结论:</p>
                </div>
                <div class="p1">
                    <p id="202">1) 如图2所示MCJMI在样本较少数据集上预测结果。Flowmeters数据集上MCJMI、JMIM、JMI、mRMR、SFS-LW仅在特征选择数5左右预测精度已经达到最高值, 分别为83%、80%、80%、75%、80%。IG在特征数达20左右才到达最大精度75%。虽然JMIM、JMI、mRMR方法在特征数5左右也都达到了最大精度, 从预测精度上能够得出MCJMI比其他方法相比精度都要高。MCJMI达到最大预测精度后, 随着选择特征加入预测精度始终稳定, 而mRMR、IG则有较大的波动。在parkinson数据集上MCJMI也仅在特征选择数到10左右预测精度已经达到了92%, 并且其产生的波动也较小。其主要原因在于特征选择过程MCJMI通过条件互信息排除了冗余特征。在Sonar数据集上看出, 几种算法在特征数都无较好的稳定性, 当特征数达到30后, MCJMI预测精度达到了88%且趋于平稳。</p>
                </div>
                <div class="p1">
                    <p id="203">2) 从特征不断增加过程中预测精度变化趋势看, 特征数逐渐增加时, 所有特征选择方法在数据集上预测精度呈现先增加后减小的规律。这符合了随着新特征的加入, 信息量在不断增加, 冗余信息也在不断增加的规律。图2中MCJMI、JMI、JMIM选择的特征加入后预测精度不断增加, 达到一定特征数量后预测精度呈现下降趋势一致。从图2可以看出, MCJMI很好地反映这一规律, 而IG、mRMR这一特征表现并不明显, 而且出现预测精度上下跳跃的情况。主要原因在于IG, mRMR在特征选择时考虑的标准单一, 选择的冗余特征加入后为分类提供的信息量较少, 造成预测精度上升较慢。</p>
                </div>
                <div class="p1">
                    <p id="204">3) MCJMI与SFS-LW方法在预测精度上差异较小, 但根据MCJMI与SFS-LW在精度曲线变化上, 可以看出 MCJMI在精度变化过程中平稳性优于SFS-LW。主要原因, MCJMI在每次计算过程中基于前一特征计算互信息和。而SFS-LW每当加入特征后, 将会重新计算度量各个分类之间距离, 降低了已选特征之间的关联性。</p>
                </div>
                <div class="area_img" id="205">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903020_205.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同小样本数据集分类精度对比" src="Detail/GetImg?filename=images/JSJY201903020_205.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同小样本数据集分类精度对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903020_205.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Classification accuracy comparison of different datasets with small samples</p>

                </div>
                <div class="p1">
                    <p id="206">从表5中不同数据集上平均精度上看, 在wine数据集上平均精度MCJMI相对IG提升了6.1个百分点, 相对mRMR、JMI、JMIM、SFS-LW平均预测精度无明显大小变化;在Parkinsons数据集上MCJMI相对IG提升了3.8个百分点, 相对mRMR提升了3.5个百分点, 相对JMI、JMIM、SFS-LW平均预测精度无明显大小变化;在Flowmeters数据集上MCJMI相对IG提升了5.1个百分点, 相对mRMR提升了4.8个百分点, 相对JMI、JMIM平均预测精度提升了1.8和2个百分点, 相对SFS-LW提升了1.7个百分点;在Sonar数据集上MCJMI相对IG提升了7.8个百分点, 相对mRMR提升了5.4个百分点, 相对JMI、JMIM平均预测精度提升了1.5和2.5个百分点。</p>
                </div>
                <div class="area_img" id="207">
                    <p class="img_tit"><b>表</b>5 <b>不同小样本数据集的平均预测精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Average prediction accuracy of different datasets with small samples</p>
                    <p class="img_note"></p>
                    <table id="207" border="1"><tr><td><br />数据集</td><td>IG</td><td>mRMR</td><td>JMI</td><td>JMIM</td><td>MCJMI</td><td>SFS-LW</td></tr><tr><td><br />wine</td><td>0.874</td><td>0.933</td><td>0.936</td><td>0.929</td><td>0.935</td><td>0.916</td></tr><tr><td><br />Parkinsons</td><td>0.854</td><td>0.857</td><td>0.886</td><td>0.889</td><td>0.892</td><td>0.873</td></tr><tr><td><br />Flowmeters</td><td>0.702</td><td>0.705</td><td>0.735</td><td>0.733</td><td>0.753</td><td>0.736</td></tr><tr><td><br />Sonar</td><td>0.735</td><td>0.759</td><td>0.798</td><td>0.788</td><td>0.813</td><td>0.797</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="208">根据表6中达到最大精度所选特征数占比得出, IG所占总特征比例变化范围为13.64%～83.33%, 其波动区间大小为70%。同理得mRMR波动区间大小45%, JMI波动区间大小42%, JMIM波动区间大小15%, MCJMI波动区间大小38%, SFS-LW波动区间大小为38%与MCJMI相同。IG波动较大主要原因IG未考虑特征之间的相关性, 特征相关性较强时为达到高的预测精度, IG特征选择数量要多于其他方法。JMI, JMIM, MCJMI波动区间相似, 而每个数据集上达到最大精度时所选特征占比上得出, MCJMI与SFS-LW选择特征数低于JMI与JMIM。</p>
                </div>
                <div class="area_img" id="209">
                    <p class="img_tit"><b>表</b>6 <b>达到最大精度时特征选择数/特征数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 6 Feature selection number at maximum precision</p>
                    <p class="img_note"></p>
                    <table id="209" border="1"><tr><td><br />数据集</td><td>IG</td><td>mRMR</td><td>JMI</td><td>JMIM</td><td>MCJMI</td><td>SFS-LW</td></tr><tr><td><br />wine</td><td>0.833</td><td>0.500</td><td>0.500</td><td>0.500</td><td>0.500</td><td>0.500</td></tr><tr><td><br />Parkinsons</td><td>0.136</td><td>0.091</td><td>0.500</td><td>0.546</td><td>0.455</td><td>0.455</td></tr><tr><td><br />Flowmeters</td><td>0.535</td><td>0.326</td><td>0.209</td><td>0.256</td><td>0.116</td><td>0.110</td></tr><tr><td><br />Sonar</td><td>0.117</td><td>0.650</td><td>0.633</td><td>0.400</td><td>0.433</td><td>0.425</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="210" name="210">5.3 <b>大样本数据预测精度分析</b></h4>
                <div class="p1">
                    <p id="211">图3给出样本数量较多的数据集上分类预测精度。其中:break-cancer选择特征选择过程前20个特征, Isolet及semeion选取特征选取过程前50个最相关特征作分类预测。详细预测精度见图3。</p>
                </div>
                <div class="p1">
                    <p id="212">根据图3比较可得出以下结论:</p>
                </div>
                <div class="p1">
                    <p id="213">1) 数据样本量较多时, 几种特征选择方法在随着特征数量增加时分类精度也在不断增加, 而SFS-LW则出现了预测进度上下浮动较大的情况。Breast-cancer数据集上当特征量在4时MCJMI、JMIM、JMI预测精度达到96%, SFS-LW预测精度达到94%。在之后出现了精度下降, 主要原因在于SFS-LW每选择一次特征后都需重新计算分类间距离, 当样本数据存在较多噪声时, 即数据非平稳数据就存在如图3 (a) 所示情况。IG、mRMR在特征4时预测精度为93%出现明显下降。主要原因是IG、mRMR选择了冗余特征, 增加了数据噪声。在Isolet数据集上特性选择数量8左右出现JMIM预测精度短暂优于MCJMI情况, 在特征数量达到10以后MCJMI预测精度优于其他几种方法。</p>
                </div>
                <div class="p1">
                    <p id="214">2) 图3 (a) 、 (c) 数据集上特征选择预测精度曲线变化的斜率得出, MCJMI预测精度上升的速度要优于IG、mRMR、JMI、JMIM。SFS-LW方法同样预测精度优于IG、mRMR、JMI、JMIM。其主要原因MCMI方法在特征选择时考虑了条件互信息加入特征<i>f</i><sub><i>j</i></sub>后, 总能使<i>I</i> (<i>f</i><sub><i>i</i></sub>, <i>S</i>;<i>C</i>) 向增长速度最快的方向选择特征, SFS-LW每次加入特征最大限度的区分类, 忽略了特征选择稳定性。</p>
                </div>
                <div class="p1">
                    <p id="215">根据表7对不同数据集上预测的平均精度对比得出, 在Breast-cancers数据集上, JMI、JMIM、MCJMI最大预测精度相差不大, MCJMI相对IG提升了2.5个百分点, 相对mRMR精度提升了1.3个百分点, 相对SFS-LW提升了2.4个百分点;在Semeion数据集上, MCJMI相对IG提升了14个百分点, 相对mRMR提升了19.2个百分点, 相对JMI提升了3.7个百分点, 相对于SFS-LW提升了1.6个百分点;在Isolet数据集, MCJMI相对IG提升了23.6个百分点, 相对mRMR提升了23.6个百分点, 相对JMI提升了3.4个百分点, 相对JMIM提升了4个百分点。从平均预测精度上得出, MCJMI特征选择预测精度整体上高于其他方法。</p>
                </div>
                <div class="area_img" id="216">
                    <p class="img_tit"><b>表</b>7 <b>不同大样本数据集的平均预测精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 7 Average prediction accuracy of different datasets with large samples</p>
                    <p class="img_note"></p>
                    <table id="216" border="1"><tr><td>数据集</td><td>IG</td><td>mRMR</td><td>JMI</td><td>JMIM</td><td>MCJMI</td><td>SFS-LW</td></tr><tr><td>Breast-cancer</td><td>0.928</td><td>0.940</td><td>0.953</td><td>0.953</td><td>0.953</td><td>0.929</td></tr><tr><td><br />Semeion</td><td>0.530</td><td>0.442</td><td>0.597</td><td>0.634</td><td>0.634</td><td>0.618</td></tr><tr><td><br />Isolet</td><td>0.426</td><td>0.410</td><td>0.628</td><td>0.622</td><td>0.662</td><td>0.645</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="217">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903020_217.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同大样本数据集分类精度对比" src="Detail/GetImg?filename=images/JSJY201903020_217.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同大样本数据集分类精度对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903020_217.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Classification accuracy comparison of different datasets with large samples</p>

                </div>
                <h4 class="anchor-tag" id="218" name="218">5.4 <b>非平稳数据预测精度分析</b></h4>
                <div class="p1">
                    <p id="219">图4给出非平衡数据集secom不同特征选择方法的预测精度, 选取前100个特征预测结果。图5给出不同特征选择方法在特征选择过程时间复杂度情况。</p>
                </div>
                <div class="p1">
                    <p id="220">根据图4预测计算精度方面能够得出:在特征数到达30之前, 基于互信息的特征选择方法在精度预测方面优于SFS-LW;特征达到30之后, SFS-LW在预测精度明显高于互信息的特征选择方法。主要有以下原因:</p>
                </div>
                <div class="p1">
                    <p id="221">1) 基于互信息特征选择计算过程中, 互信息计算基于各类样本所占比例。在特征选择前期不均衡样本, 某一类样本占比例较大, 互信息所选择的特征倾向于占比重大的分类样本。</p>
                </div>
                <div class="p1">
                    <p id="222">2) SFS-LW计算类与类之间距离, 各个类之间计算距离, 对样本的比例大小敏感度不高, 但当选择特征较少时, 数据噪声对SFS-LW影响较大, 正如图4所示SFS-LW在特征到达30之前精度低于MCJIM方法。</p>
                </div>
                <div class="p1">
                    <p id="223">根据图5可以得出:在特征选择过程中SFS-LW时间复杂度最高, SFS-LW在每次计算类与类之间距离时为组合问题;而MCJMI在特征选择过程中, 计算互信息与|<i>C</i>|的大小有关且在计算相似集合时消耗了较多时间;JMI、JMIM计算时间复杂度相同;mRMR时间复杂度略低于JMI与JMIM, IG时间复杂度最低。</p>
                </div>
                <div class="area_img" id="224">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903020_224.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 数据集secom分类精度对比" src="Detail/GetImg?filename=images/JSJY201903020_224.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 数据集secom分类精度对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903020_224.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Classification accuracy comparison of secom dataset</p>

                </div>
                <div class="area_img" id="225">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903020_225.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 特征选择过程时间消耗对比" src="Detail/GetImg?filename=images/JSJY201903020_225.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 特征选择过程时间消耗对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903020_225.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Time cost comparison in feature selection</p>

                </div>
                <h4 class="anchor-tag" id="226" name="226">5.5 <b>稳定性分析</b></h4>
                <div class="p1">
                    <p id="227">稳定性是评判方法泛化能力的重要指标, 特征选择结果易受样本选择的影响。对于同一数据集中不同数据样本, 若所选出的最优特征子集越相似说明方法越稳定。Chelvan等<citation id="312" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出相似特征稳定性的评判标准。设<i>S</i><sub>1</sub>, <i>S</i><sub>2</sub>为同一数据集上不同样本上所选出的两个最优子集, 其中两者子集中的特征个数为|<i>S</i><sub>1</sub>|=|<i>S</i><sub>2</sub>|记作<i>m</i>, |<i>F</i>|为总特征数<i>n</i>, <i>S</i><sub>1</sub>, <i>S</i><sub>2</sub>相交的个数<i>r</i>=|<i>S</i><sub>1</sub>∩<i>S</i><sub>2</sub>|, 则稳定性计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="228"><i>θ</i>= (<i>r</i>*<i>n</i>-<i>m</i><sup>2</sup>) / (<i>m</i> (<i>n</i>-<i>m</i>) )      (22) </p>
                </div>
                <div class="p1">
                    <p id="229">由式 (22) 得出当所选特征数一定时, <i>r</i>=|<i>S</i><sub>1</sub>∩<i>S</i><sub>2</sub>|交集越大则说明方法在选择特征上越稳定。表8中给出各个方法平均精度、稳定性, 由于稳定性仅能够说明算法在特征选择上的稳定。通过平均精度与稳定性的比值, 能够标准化稳定性分析结果。本文给出平均精度与稳定性的比值, 作为算法最终稳定性评判指标, 具体如表8所示。</p>
                </div>
                <div class="area_img" id="230">
                    <p class="img_tit"><b>表</b>8 <b>各方法稳定性比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 8 Stability comparison of different methods</p>
                    <p class="img_note"></p>
                    <table id="230" border="1"><tr><td><br />方法</td><td>平均精度</td><td>稳定性 (<i>θ</i>) </td><td>平均精度/稳定性</td></tr><tr><td><br />IG</td><td>0.721 2</td><td>0.902 5</td><td>0.799 1</td></tr><tr><td><br />mRMR</td><td>0.720 6</td><td>0.880 3</td><td>0.818 5</td></tr><tr><td><br />JMI</td><td>0.790 5</td><td>0.860 6</td><td>0.918 5</td></tr><tr><td><br />JMIM</td><td>0.792 5</td><td>0.851 2</td><td>0.899 3</td></tr><tr><td><br />MCJMI</td><td>0.806 0</td><td>0.876 0</td><td>0.920 0</td></tr><tr><td><br />SFS-LW</td><td>0.790 0</td><td>0.880 0</td><td>0.897 0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="231">从表8中可以看出, IG的稳定性最高, 值为0.902 5, mRMR稳定性为0.880 3, MCJMI稳定性为0.876 0, SFS-LW稳定性为0.880 0。主要原因是IG所考虑的条件最少, 仅存在特征与分类之间的互信息大小。JMI、JMIM稳定性处于几个方法中较低的水平, 主要原因是JMI、JMIM在计算选择过程中考虑的因素要多于IG与mRMR, 受样本的影响较大。而MCJMI则相对于JMI与JMIM稳定性有所提升, MCJMI在选择时结合了两种方法的优点, 考虑因素相同的情况下, 增加了特征选择的约束条件。MCJMI与SFS-LW在稳定性方面较为一致, 但在平稳性一致的情况下, MCJMI的预测精度为0.806 0, 高于SFS-LW预测精度。表8采用平均精度与稳定性比值来标准化稳定性评判标准, 同时考虑精度与稳定性两个方面, 通过比值得出MCJMI稳定性最高达到0.92。</p>
                </div>
                <h3 id="232" name="232" class="anchor-tag">6 结语</h3>
                <div class="p1">
                    <p id="233">本文通过比较特征选择方法选择出的特征, 在数据集上预测的平均精度、最大预测精度、所需特征数以及稳定性方面比较得出实验结果。MCJMI综合考虑联合互信息与条件互信息, 增强了特征选择的约束性, 实验结果表明MCJMI能够减少冗余特征的选择。MCJMI也存在不足之处, MCJMI未考虑数据不均衡的情况, 未来研究可考虑非平衡数据情况下如何改进。特征选择不仅适用于数据冗余排除, 同样适用于因素之间的相关性分析, 如物体变形影响因素、城市空气质量影响因素等。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="255">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overview of feature subset selection algorithm for high dimensional data">

                                <b>[1]</b>GANDHI S S, PRABHUNE S S.Overview of feature subset selection algorithm for high dimensional data[C]//ICISC 2017:Proceedings of the 2017 IEEE International Conference on Inventive Systems and Control.Piscataway, NJ:IEEE, 2017:1-6.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Binary feature selection Conditional Mutual Information">

                                <b>[2]</b>FLEURET F.Fast binary feature selection with conditional mutual information[J].Journal of Machine Learning Research, 2004, 5 (3) :1531-1555.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speeding up joint mutual information feature selection with an optimization heuristic">

                                <b>[3]</b>LIU H, DITZLER G.Speeding up joint mutual information feature selection with an optimization heuristic[C]//Proceedings of the2017 IEEE Symposium Series on Computational Intelligence.Piscataway, NJ:IEEE, 2018:1-8.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-greedy heuristics for feature selection with test cost constraints">

                                <b>[4]</b>MIN F, XU J.Semi-greedy heuristics for feature selection with test cost constraints[J].Granular Computing, 2016, 1 (3) :199-211.
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature selection for high-dimensional temporal data">

                                <b>[5]</b>TSAGRIS M, LAGANI V, TSAMARDINOS I.Feature selection for high-dimensional temporal data[J].BMC Bioinformatics, 2018, 19:17.
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SCHO201302017&amp;v=MzIyNjJDVVI3cWZadVpwRmlEbVVyN0pOaTdEWWJHNEg5TE1yWTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>黄志艳.一种基于信息增益的特征选择方法[J].山东农业大学学报 (自然科学版) , 2013, 44 (2) :252-256. (HUANG Z Y.Based on the information gain text feature selection method[J].Journal of Shandong Agricultural University (Natural Science) , 2013, 44 (2) :252-256.) 
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201704020&amp;v=MDYyNDRhYkc0SDliTXE0OUhaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURtVXI3Skx6N00=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>刘海峰, 刘守生, 宋阿羚.基于词频分布信息的优化IG特征选择方法[J].计算机工程与应用, 2017, 53 (4) :113-117. (LIU H F, LIU S S, SONG A L.Improved method of IG feature selection based on word frequency distribution[J].Computer Engineering and Applications, 2017, 53 (4) :113-117.) 
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using mutual information for selecting features in supervised neural net learning">

                                <b>[8]</b>BATTITI R.Using mutual information for selecting features in supervised neural net learning[J].IEEE Transactions on Neural Networks, 1994, 5 (4) :537-550.
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700212104&amp;v=MjgyNDVCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSktGMFZheE09TmlmT2ZiSzhIdGZOcUk5Rlp1b05EWHc5bw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>HOQUE N, BHATTACHARYYA D K, KALITA J K.MIFS-ND:a mutual information-based feature selection method[J].Expert Systems with Applications, 2014, 41 (14) :6371-6385.
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimized automatic sleep stage classification using the Normalized Mutual Information Feature Selection (NMIFS)method">

                                <b>[10]</b>CHO D, LEE B.Optimized automatic sleep stage classification using the Normalized Mutual Information Feature Selection (NMIFS) method[C]//Proceedings of the 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway, NJ:IEEE, 2017:3094-3097.
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy">

                                <b>[11]</b>PENG H, LONG F, DING C.Feature selection based on mutual information:criteria of max-dependency, max-relevance, and minredundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226-1238.
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201722019&amp;v=MTAyNTg3Skx6N01hYkc0SDliT3JZOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURtVXI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>董泽民, 石强.基于归一化模糊联合互信息最大的特征选择[J].计算机工程与应用, 2017, 53 (22) :105-110. (DONG Z M, SHI Q.Feature selection using normalized fuzzy joint mutual information maximum[J].Computer Engineering and Applications, 2017, 53 (22) :105-110.) 
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES789EBD0C7F31EF04A5CF133023653E85&amp;v=MDIwNzdTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dDlnekx1K3dhaz1OaWZPZmJTd0Y2UysyNDgyWTUwTURRbFB6eEppNzB3TFNYemhyQkEyZjdlWE1MS2FDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>BENNASAR M, HICKS Y, SETCHI R.Feature selection using joint mutual information maximisation[J].Expert Systems with Applications, 2015, 42 (22) :8520-8532.
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Grouped gene selection of cancer via adaptive sparse group lasso based on conditional mutual information">

                                <b>[14]</b>LI J, DONG W, MENG D.Grouped gene selection of cancer via adaptive sparse group lasso based on conditional mutual information[J].IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2018, 15 (6) :2028-2038.
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new feature selection method based on a validity index of feature subset">

                                <b>[15]</b>LIU C, WANG W, ZHAO Q, et al.A new feature selection method based on a validity index of feature subset[J].Pattern Recognition Letters, 2017, 92:1-8.
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-dimensional data">

                                <b>[16]</b>AMARATUNGA D, CABRERA J.High-dimensional data[J].Journal of the National Science Foundation of Sri Lanka, 2016, 44 (1) :3.
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=UCI Machine Learning Repository">

                                <b>[17]</b>DUA, D.AND KARRA TANISKIDOU, E.UCI Machine Learning Repository[DB/OL].[2018-07-13].http://archive.ics.uci.edu/ml.
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mutual information between discrete and continuous data sets">

                                <b>[18]</b>ROSS B C.Mutual information between discrete and continuous data sets[J].PLo S One, 2014, 9 (2) :e87357.
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A study on selection stability measures for various feature selection algorithms">

                                <b>[19]</b>CHELVAN P M, PERUMAL K.A study on selection stability measures for various feature selection algorithms[C]//Proceedings of the 2016 IEEE International Conference on Computational Intelligence and Computing Research.Piscataway, NJ:IEEE, 2017:1-4.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201903020" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903020&amp;v=MDAzMjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURtVXI3Skx6N0JkN0c0SDlqTXJJOUhaSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
