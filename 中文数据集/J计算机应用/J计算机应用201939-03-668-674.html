<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637138991722760000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201903010%26RESULT%3d1%26SIGN%3d6UvXkKhAPE%252fDwfVNSaLzxXo4nKs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903010&amp;v=MjI3MzFwRmlEbFc3M0xMejdCZDdHNEg5ak1ySTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="1 相关理论 ">1 相关理论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="1.1 &lt;b&gt;注意力机制&lt;/b&gt;">1.1 <b>注意力机制</b></a></li>
                                                <li><a href="#57" data-title="1.2 &lt;b&gt;生成对抗网络&lt;/b&gt;">1.2 <b>生成对抗网络</b></a></li>
                                                <li><a href="#59" data-title="1.3 &lt;b&gt;长短期记忆网络模块&lt;/b&gt;">1.3 <b>长短期记忆网络模块</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#65" data-title="2 行人轨迹生成模型 ">2 行人轨迹生成模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="2.1 &lt;b&gt;问题定义&lt;/b&gt;">2.1 <b>问题定义</b></a></li>
                                                <li><a href="#73" data-title="2.2 &lt;b&gt;整体网络结构&lt;/b&gt;">2.2 <b>整体网络结构</b></a></li>
                                                <li><a href="#81" data-title="2.3 &lt;b&gt;注意力模块&lt;/b&gt;">2.3 <b>注意力模块</b></a></li>
                                                <li><a href="#112" data-title="2.4 &lt;b&gt;编码器模块&lt;/b&gt;">2.4 <b>编码器模块</b></a></li>
                                                <li><a href="#129" data-title="2.5 &lt;b&gt;解码器模块&lt;/b&gt;">2.5 <b>解码器模块</b></a></li>
                                                <li><a href="#155" data-title="2.6 &lt;b&gt;判别器&lt;/b&gt;">2.6 <b>判别器</b></a></li>
                                                <li><a href="#171" data-title="2.7 &lt;b&gt;损失函数&lt;/b&gt;">2.7 <b>损失函数</b></a></li>
                                                <li><a href="#178" data-title="2.8 &lt;b&gt;模型训练过程&lt;/b&gt;">2.8 <b>模型训练过程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#195" data-title="3 实验仿真与结果分析 ">3 实验仿真与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#198" data-title="3.1 &lt;b&gt;模型参数与训练过程&lt;/b&gt;">3.1 <b>模型参数与训练过程</b></a></li>
                                                <li><a href="#203" data-title="3.2 &lt;b&gt;预测精度性能比较&lt;/b&gt;">3.2 <b>预测精度性能比较</b></a></li>
                                                <li><a href="#216" data-title="3.3 &lt;b&gt;模型效率比较&lt;/b&gt;">3.3 <b>模型效率比较</b></a></li>
                                                <li><a href="#221" data-title="3.4 &lt;b&gt;定性分析&lt;/b&gt;">3.4 <b>定性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#224" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="图1 AttenGAN总体网络结构">图1 AttenGAN总体网络结构</a></li>
                                                <li><a href="#201" data-title="图2 AttenGAN的训练损失函数">图2 AttenGAN的训练损失函数</a></li>
                                                <li><a href="#212" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;各种轨迹预测模型在&lt;/b&gt;ADE&lt;b&gt;和&lt;/b&gt;FDE&lt;b&gt;上的结果对比&lt;/b&gt;"><b>表</b>1 <b>各种轨迹预测模型在</b>ADE<b>和</b>FDE<b>上的结果对比</b></a></li>
                                                <li><a href="#219" data-title="图3 几种模型预测轨迹的对比">图3 几种模型预测轨迹的对比</a></li>
                                                <li><a href="#220" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;各种轨迹预测模型的时间性能对比&lt;/b&gt;"><b>表</b>2 <b>各种轨迹预测模型的时间性能对比</b></a></li>
                                                <li><a href="#227" data-title="图4 同一场景下的多种合理性预测">图4 同一场景下的多种合理性预测</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" LARGE F, VASQUEZ D, FRAICHARD T, et al. Avoiding cars and pedestrians using velocity obstacles and motion prediction [EB/OL]. [2018- 07- 01]. https://www.researchgate.net/publication/29642615_Avoiding_Cars_and_Pedestrians_using_V-Obstacles_and_Motion_Prediction." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Avoiding cars and pedestrians using velocity obstacles and motion prediction">
                                        <b>[1]</b>
                                         LARGE F, VASQUEZ D, FRAICHARD T, et al. Avoiding cars and pedestrians using velocity obstacles and motion prediction [EB/OL]. [2018- 07- 01]. https://www.researchgate.net/publication/29642615_Avoiding_Cars_and_Pedestrians_using_V-Obstacles_and_Motion_Prediction.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" THOMPSON S, HORIUCHI T, KAGAMI S. A probabilistic model of human motion and navigation intent for mobile robot path planning [C]// Proceedings of the 2009 4th International Conference on Autonomous Robots and Agents. Piscataway, NJ: IEEE, 2009: 663-668." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Probabilistic Model of HumanMotion and Navigation Intent for Mobile Robot Path Planning">
                                        <b>[2]</b>
                                         THOMPSON S, HORIUCHI T, KAGAMI S. A probabilistic model of human motion and navigation intent for mobile robot path planning [C]// Proceedings of the 2009 4th International Conference on Autonomous Robots and Agents. Piscataway, NJ: IEEE, 2009: 663-668.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" BENNEWITZ M. Learning motion patterns of people for compliant robot motion [J]. The International Journal of Robotics Research, 2005, 24 (1) : 31-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Motion Patterns of People for Compliant Robot Motion">
                                        <b>[3]</b>
                                         BENNEWITZ M. Learning motion patterns of people for compliant robot motion [J]. The International Journal of Robotics Research, 2005, 24 (1) : 31-48.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" HELBING D, MOLN&#193;R P. Social force model for pedestrian dynamics [J]. Physical Review E: Statistical Physics Plasmas Fluids and Related Interdisciplinary Topics, 1995, 51 (5) : 4282-4286." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Social force model for pedestrian dynamics">
                                        <b>[4]</b>
                                         HELBING D, MOLN&#193;R P. Social force model for pedestrian dynamics [J]. Physical Review E: Statistical Physics Plasmas Fluids and Related Interdisciplinary Topics, 1995, 51 (5) : 4282-4286.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" TRAUTMAN P, KRAUSE A. Unfreezing the robot: navigation in dense, interacting crowds [C]// Proceedings of the 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. Piscataway, NJ: IEEE, 2010: 797-803." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unfreezing the robot:navigation in dense,interacting crowds">
                                        <b>[5]</b>
                                         TRAUTMAN P, KRAUSE A. Unfreezing the robot: navigation in dense, interacting crowds [C]// Proceedings of the 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. Piscataway, NJ: IEEE, 2010: 797-803.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" MORRIS B T, TRIVEDI M M. Trajectory learning for activity understanding: unsupervised, multilevel, and long-term adaptive approach [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (11) : 2287-2301." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Trajectory Learning for Activity Understanding: Unsupervised, Multilevel, and Long-Term Adaptive Approach">
                                        <b>[6]</b>
                                         MORRIS B T, TRIVEDI M M. Trajectory learning for activity understanding: unsupervised, multilevel, and long-term adaptive approach [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (11) : 2287-2301.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" KITANI K M, ZIEBART B D, BAGNELL J A, et al. Activity forecasting [C]// Proceedings of the 2012 European Conference on Computer Vision, LNCS 7575. Berlin: Springer, 2012: 201-214." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Activity forecasting">
                                        <b>[7]</b>
                                         KITANI K M, ZIEBART B D, BAGNELL J A, et al. Activity forecasting [C]// Proceedings of the 2012 European Conference on Computer Vision, LNCS 7575. Berlin: Springer, 2012: 201-214.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" ALAHI A, GOEL K, RAMANATHAN V, et al. Social LSTM: human trajectory prediction in crowded spaces [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2016: 961-971." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Social LSTM:Human Trajectory Prediction in Crowded Spaces">
                                        <b>[8]</b>
                                         ALAHI A, GOEL K, RAMANATHAN V, et al. Social LSTM: human trajectory prediction in crowded spaces [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2016: 961-971.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" VEMULA A, MUELLING K, OH J. Social attention: modeling attention in human crowds [EB/OL]. [2018- 03- 25]. https://arxiv.org/pdf/1710.04689.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Social attention:modeling attention in human crowds">
                                        <b>[9]</b>
                                         VEMULA A, MUELLING K, OH J. Social attention: modeling attention in human crowds [EB/OL]. [2018- 03- 25]. https://arxiv.org/pdf/1710.04689.pdf.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" GUPTA A, JOHNSON J, LI F-F, et al. Social GAN: socially acceptable trajectories with generative adversarial networks [EB/OL]. [2018- 05- 04]. https://arxiv.org/abs/1803.10892.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Social GAN:socially acceptable trajectories with generative adversarial networks">
                                        <b>[10]</b>
                                         GUPTA A, JOHNSON J, LI F-F, et al. Social GAN: socially acceptable trajectories with generative adversarial networks [EB/OL]. [2018- 05- 04]. https://arxiv.org/abs/1803.10892.pdf.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" MNIH V, HEESS N, GRAVES A, et al. Recurrent models of visual attention [C]// Proceedings of the 27th International Conference on Neural Information Processing Systems. Cambridge, MA: MIT Press, 2014, 2: 2204-2212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent models of visual attention">
                                        <b>[11]</b>
                                         MNIH V, HEESS N, GRAVES A, et al. Recurrent models of visual attention [C]// Proceedings of the 27th International Conference on Neural Information Processing Systems. Cambridge, MA: MIT Press, 2014, 2: 2204-2212.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" CHEN H, SUN M, TU C, et al. Neural sentiment classification with user and product attention [C]// Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas: , 2016: 1650-1659." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural Sentiment Classification with User and Product Attention">
                                        <b>[12]</b>
                                         CHEN H, SUN M, TU C, et al. Neural sentiment classification with user and product attention [C]// Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas: , 2016: 1650-1659.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 卢玲, 杨武, 王远伦, 等.结合注意力机制的长文本分类方法[J].计算机应用, 2018, 38 (5) :1272-1277. (LU L, YANG W, WANG Y L, et al. Long text classification combined with attention mechanism [J]. Journal of Computer Applications, 2018, 38 (5) :1272-1277.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201805010&amp;v=MDM1NDF6N0JkN0c0SDluTXFvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURsVzczTEw=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         卢玲, 杨武, 王远伦, 等.结合注意力机制的长文本分类方法[J].计算机应用, 2018, 38 (5) :1272-1277. (LU L, YANG W, WANG Y L, et al. Long text classification combined with attention mechanism [J]. Journal of Computer Applications, 2018, 38 (5) :1272-1277.) 
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al. Generative adversarial networks[J]. Advances in Neural Information Processing Systems, 2014, 3: 2672-2680.</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" HOCHREITER S, SCHMIDHUBER J. Long short-term memory [M]// GRAVES A. Supervised Sequence Labelling with Recurrent Neural Networks. Berlin: Springer, 2012: 37-45." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long short-term memory">
                                        <b>[15]</b>
                                         HOCHREITER S, SCHMIDHUBER J. Long short-term memory [M]// GRAVES A. Supervised Sequence Labelling with Recurrent Neural Networks. Berlin: Springer, 2012: 37-45.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" XU K, BA J, KIROS R, et al. Show, attend and tell: Neural image caption generation with visual attention [EB/OL]. [2018- 07- 01]. https://arxiv.org/pdf/1502.03044v2.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention">
                                        <b>[16]</b>
                                         XU K, BA J, KIROS R, et al. Show, attend and tell: Neural image caption generation with visual attention [EB/OL]. [2018- 07- 01]. https://arxiv.org/pdf/1502.03044v2.pdf.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" FAN H, SU H, GUIBAS L. A point set generation network for 3D object reconstruction from a single image [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2017: 2463-2471." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A point set generation network for 3Dobject reconstruction from a single image">
                                        <b>[17]</b>
                                         FAN H, SU H, GUIBAS L. A point set generation network for 3D object reconstruction from a single image [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2017: 2463-2471.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" PELLEGRINI S, ESS A, van GOOL L. Improving data association by joint modeling of pedestrian trajectories and groupings [C]// Proceedings of the 2010 European Conference on Computer Vision, LNCS 6311. Berlin: Springer, 2010: 452-465." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving data association by joint modeling of pedestrian trajectories and groupings">
                                        <b>[18]</b>
                                         PELLEGRINI S, ESS A, van GOOL L. Improving data association by joint modeling of pedestrian trajectories and groupings [C]// Proceedings of the 2010 European Conference on Computer Vision, LNCS 6311. Berlin: Springer, 2010: 452-465.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" LERNER A, CHRYSANTHOU Y, LISCHINSKI D. Crowds by example [J]. Computer Graphics Forum, 2007, 26 (3) : 655-664." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000018016&amp;v=MTg1NjNjYXJPNEh0SE1yNDVOWk9vSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rmkva1c3L0FJbDA9Tmlm&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         LERNER A, CHRYSANTHOU Y, LISCHINSKI D. Crowds by example [J]. Computer Graphics Forum, 2007, 26 (3) : 655-664.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" LEE N, CHOI W, VERNAZA P, et al. DESIRE: Distant future prediction in dynamic scenes with interacting Agents [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2017: 2165-2174.This work is partially supported by the National Natural Science Foundation of China (51775332, 51675329, 51675342) , the Program of State Key Laboratory of Mechanical System and Vibration (GZ2016KF001, GKZD020018) , the Open Program of State Key Laboratory of Smart Manufacturing for Special Vehicles and Transmission System (GZ2016KF001) .SUN Yasheng, born in 1995, M. S. candidate. His research interests include deep learning, computer vision.JIANG Qi, born in 1995, M. S. candidate. His research interests include machine learning, natural language processing. HU Jie, born in 1973, Ph. D., professor. His research interests include intelligent design, knowledge fusion. QI Jin, born in 1984, Ph. D., associate research fellow. His research interests include artificial intelligence, knowledge based engineering. PENG Yinghong, born in 1963, Ph. D., professor. His research interests include innovative design, knowledge based engineering." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DESIRE:distant future prediction in dynamic scenes with interacting agents">
                                        <b>[20]</b>
                                         LEE N, CHOI W, VERNAZA P, et al. DESIRE: Distant future prediction in dynamic scenes with interacting Agents [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2017: 2165-2174.This work is partially supported by the National Natural Science Foundation of China (51775332, 51675329, 51675342) , the Program of State Key Laboratory of Mechanical System and Vibration (GZ2016KF001, GKZD020018) , the Open Program of State Key Laboratory of Smart Manufacturing for Special Vehicles and Transmission System (GZ2016KF001) .SUN Yasheng, born in 1995, M. S. candidate. His research interests include deep learning, computer vision.JIANG Qi, born in 1995, M. S. candidate. His research interests include machine learning, natural language processing. HU Jie, born in 1973, Ph. D., professor. His research interests include intelligent design, knowledge fusion. QI Jin, born in 1984, Ph. D., associate research fellow. His research interests include artificial intelligence, knowledge based engineering. PENG Yinghong, born in 1963, Ph. D., professor. His research interests include innovative design, knowledge based engineering.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-09-18 14:12</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(03),668-674 DOI:10.11772/j.issn.1001-9081.2018081645            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于注意力机制的行人轨迹预测生成模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E4%BA%9A%E5%9C%A3&amp;code=41275241&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙亚圣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E5%A5%87&amp;code=41275242&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜奇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E6%B4%81&amp;code=08567430&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡洁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%88%9A%E8%BF%9B&amp;code=08536109&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">戚进</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E9%A2%96%E7%BA%A2&amp;code=05964968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭颖红</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E4%B8%8E%E5%8A%A8%E5%8A%9B%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0054402&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海交通大学机械与动力工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E4%B8%8E%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海交通大学电子信息与电气工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对长短期记忆网络 (LSTM) 在行人轨迹预测问题中孤立考虑单个行人, 且无法进行多种可能性预测的问题, 提出基于注意力机制的行人轨迹预测生成模型 (AttenGAN) , 来对行人交互模式进行建模和概率性地对多种合理可能性进行预测。AttenGAN包括一个生成器和一个判别器, 生成器根据行人过去的轨迹概率性地对未来进行多种可能性预测, 判别器用来判断一个轨迹是真实的还是由生成器伪造生成的, 进而促进生成器生成符合社会规范的预测轨迹。生成器由一个编码器和一个解码器组成, 在每一个时刻, 编码器的LSTM综合注意力机制给出的其他行人的状态, 将当前行人个体的信息编码为隐含状态。预测时, 首先用编码器LSTM的隐含状态和一个高斯噪声连接来对解码器LSTM的隐含状态初始化, 解码器LSTM将其解码为对未来的轨迹预测。在ETH和UCY数据集上的实验结果表明, AttenGAN模型不仅能够给出符合社会规范的多种合理的轨迹预测, 并且在预测精度上相比传统的线性模型 (Linear) 、LSTM模型、社会长短期记忆网络模型 (S-LSTM) 和社会对抗网络 (S-GAN) 模型有所提高, 尤其在行人交互密集的场景下具有较高的精度性能。对生成器多次采样得到的预测轨迹的可视化结果表明, 所提模型具有综合行人交互模式, 对未来进行联合性、多种可能性预测的能力。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BD%A8%E8%BF%B9%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">轨迹预测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短期记忆网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E4%BA%A4%E4%BA%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人交互;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    孙亚圣 (1995—) , 男, 河北张家口人, 硕士研究生, CCF会员, 主要研究方向:深度学习、机器视觉;;
                                </span>
                                <span>
                                    姜奇 (1995—) , 男, 山东东营人, 硕士研究生, 主要研究方向:机器学习、自然语言理解;;
                                </span>
                                <span>
                                    *胡洁 (1973—) , 男, 安徽安庆人, 教授, 博士, 主要研究方向:智能设计、知识融合;电子邮箱jiehu.me.sjtu@outlook.com;
                                </span>
                                <span>
                                    戚进 (1984—) , 男, 浙江金华人, 副研究员, 博士, 主要研究方向:人工智能、知识工程;;
                                </span>
                                <span>
                                    彭颖红 (1963—) , 男, 四川成都人, 教授, 博士, 主要研究方向:创新设计、知识工程。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-09</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (51775332, 51675329, 51675342);</span>
                                <span>机械系统与振动国家重点实验室课题 (GZ2016KF001, GKZD020018);</span>
                                <span>特种车辆及其传动系统智能制造国家重点实验室开放课题 (GZ2016KF001);</span>
                    </p>
            </div>
                    <h1><b>Attention mechanism based pedestrian trajectory prediction generation model</b></h1>
                    <h2>
                    <span>SUN Yasheng</span>
                    <span>JIANG Qi</span>
                    <span>HU Jie</span>
                    <span>QI Jin</span>
                    <span>PENG Yinghong</span>
            </h2>
                    <h2>
                    <span>School of Mechanical Engineering, Shanghai Jiao Tong University</span>
                    <span>School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at that Long Short Term Memory (LSTM) has only one pedestrian considered in isolation and cannot realize prediction with various possibilities, an attention mechanism based generative model for pedestrian trajectory prediction called AttenGAN was proposed to construct pedestrian interaction model and predict multiple reasonable possibilities. The proposed model was composed of a generator and a discriminator. The generator predicted multiple possible future trajectories according to pedestrian's past trajectory probability while the discriminator determined whether the trajectories were really existed or generated by the discriminator and gave feedback to the generator, making predicted trajectories obtained conform social norm more. The generator consisted of an encoder and a decoder. With other pedestrians information obtained by the attention mechanism as input, the encoder encoded the trajectories of the pedestrian as an implicit state. Combined with Gaussian noise, the implicit state of LSTM in the encoder was used to initialize the implicit state of LSTM in the decoder and the decoder decoded it into future trajectory prediction. The experiments on ETH and UCY datasets show that AttenGAN can provide multiple reasonable trajectory predictions and can predict the trajectory with higher accuracy compared with Linear, LSTM, S-LSTM (Social LSTM) and S-GAN (Social Generative Adversarial Network) models, especially in scenes of dense pedestrian interaction. Visualization of predicted trajectories obtained by the generator indicated the ability of this model to capture the interaction pattern of pedestrians and jointly predict multiple reasonable possibilities.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=trajectory%20prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">trajectory prediction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Long%20Short%20Term%20Memory%20(LSTM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Long Short Term Memory (LSTM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Network%20(GAN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Network (GAN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pedestrian%20interaction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pedestrian interaction;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SUN Yasheng, born in 1995, M. S. candidate. His research interests include deep learning, computer vision.;
                                </span>
                                <span>
                                    JIANG Qi, born in 1995, M. S. candidate. His research interests include machine learning, natural language processing.;
                                </span>
                                <span>
                                    HU Jie, born in 1973, Ph. D. , professor. His research interests include intelligent design, knowledge fusion.;
                                </span>
                                <span>
                                    QI Jin, born in 1984, Ph. D. , associate research fellow. His research interests include artificial intelligence, knowledge based engineering.;
                                </span>
                                <span>
                                    PENG Yinghong, born in 1963, Ph. D. , professor. His research interests include innovative design, knowledge based engineering.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-09</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (51775332, 51675329, 51675342);</span>
                                <span>the Program of State Key Laboratory of Mechanical System and Vibration (GZ2016KF001, GKZD020018);</span>
                                <span>the Open Program of State Key Laboratory of Smart Manufacturing for Special Vehicles and Transmission System (GZ2016KF001);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="44">行人轨迹预测是指根据行人过去一段时间的轨迹, 预测其未来的轨迹, 该技术在自动驾驶<citation id="229" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和服务机器人导航<citation id="230" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>中都有着广泛的应用。行人在决策的过程中比较灵活主观, 甚至完全相同的场景, 不同的人都会采取不同的决策, 其机动性和灵活性大大增加了该问题的难度, 其具体的难点可以概括为以下几个方面:</p>
                </div>
                <div class="p1">
                    <p id="45">1) 如何预测出既符合物理约束, 又符合社会规范的轨迹。符合物理约束指预测出的轨迹应该是物理可行的, 例如一个人不能穿过另一个人等。符合社会规范指行人的一些社会学行为, 例如结伴而行、相互礼让等。</p>
                </div>
                <div class="p1">
                    <p id="46">2) 如何对多个行人之间的相互影响进行建模。行人在作决策时不是独立的, 而是存在例如躲避、追赶、跟随、超过等交互性的行为。</p>
                </div>
                <div class="p1">
                    <p id="47">3) 如何预测出多个合理的轨迹。在实际场景中, 往往不只有一种轨迹符合条件, 通常有多个轨迹都是合理的。</p>
                </div>
                <div class="p1">
                    <p id="48">目前行人轨迹预测方法主要有4类:基于社会力模型、基于马尔可夫模型、基于循环神经网络 (Recurrent Neural Network, RNN) 和基于生成对抗网络 (Generative Adversarial Network, GAN) 的方法。</p>
                </div>
                <div class="p1">
                    <p id="49">基于社会力模型<citation id="231" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的方式根据引力与斥力的方式对行人进行建模, 认为行人的目标会对行人产生引力进而吸引他们向目标走, 行人之间产生斥力进而防止行人之间发生碰撞。该类模型具有模型简单直观、复杂性低的优点;但存在模型对模型参数过于敏感, 模型能力不够强, 泛化能力差, 无法对于行人一些社会性的行为如结队而行等进行描述的缺陷。在基于社会力模型思想的基础上, 为了可以概率性地预测轨迹而不是给出单一的轨迹, Trautman等<citation id="232" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了交互式高斯过程IGP, 利用高斯过程对每一个行人的轨迹进行预测, 并根据社会力模型的势函数计算该预测的概率, 进而在预测的过程中考虑了行人之间的相互影响, 同时能够概率性地预测未来的轨迹。</p>
                </div>
                <div class="p1">
                    <p id="50">Morris等<citation id="233" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>使用基于隐含马尔可夫模型的方法对不同类别行人的轨迹进行时空的概率性建模。Kitani等<citation id="234" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>使用基于隐含马尔可夫过程的方式对行人进行建模, 并使用反向强化学习的方式训练模型参数, 进而学习外界静态的物理环境对行人的影响。</p>
                </div>
                <div class="p1">
                    <p id="51">近年来随深度学习的发展, 基于数据驱动的建模方式成为研究热点, 由于行人轨迹预测本质上是一个时序序列, 故其主要以循环神经网络 (RNN) 为代表性的建模方式。但RNN无法进行高层次的空间关系建模, 为刻画行人在空间的交互, Alahi等<citation id="235" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了社会长短期记忆网络模型 (Social Long Short Term Memory, S-LSTM) , 首先对空间进行网格化, 并根据网格直接对每个个体附近网格中的其他个体的隐含特征池化, 利用该池化结果对个体进行符合物理原理和社会规范的轨迹预测。但其假设对该个体的影响是由与该个体的位置远近决定的, 而在实际中, 一个个体的行为决策不仅与空间位置有关, 还与其他个体的运动方向、运动速度有关, 例如一个个体可能会提前行动躲避前面一位比较远的与他相对而行的个体, 而对于他后面距离比较近的个体并不会采取什么行动。因此, Vemula等<citation id="236" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>中使用了结构化RNN对各个个体建模, 并利用时空关系图来描述各个个体的随时间和空间的轨迹变化规律, 时空关系图的每个节点代表每一个行人个体, 每个节点与其他节点用空间边相连, 与自己用时间边相连, 空间边和时间边都用RNN来描述其随时间的变化, 最后在每个节点更新时使用注意力机制来融合与其相邻的边的信息, 该方法较好地对时空进行建模, 但其计算复杂度相对较高。</p>
                </div>
                <div class="p1">
                    <p id="52">Gupta等<citation id="237" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>将生成对抗网络引入行人轨迹预测中, 提出了社会对抗网络 (Social Generative Adversarial Network, S-GAN) 模型, 该模型提出了一种新的池化策略来描述该行人之间的影响, 并利用了生成对抗网络的思想进一步强迫轨迹预测模块预测出符合社会规范的轨迹。</p>
                </div>
                <div class="p1">
                    <p id="53">鉴于基于生成对抗网络方法预测的轨迹更加符合物理约束和社会规范, 且通过生成器的采样可以产生多个合理的轨迹。 故本文借助生成对抗网络的思想, 并使用注意力机制来融合行人之间的相对位置信息进而对人群交互进行建模。实验表明该方法可以有效地提高预测的精度, 并且可以对多种合理的可能性进行预测。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag">1 相关理论</h3>
                <h4 class="anchor-tag" id="55" name="55">1.1 <b>注意力机制</b></h4>
                <div class="p1">
                    <p id="56">注意力机制最早被用于图像处理的领域<citation id="238" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 而后也被广泛应用在自然语言处理方向<citation id="239" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。注意力机制的本质是对于一组感兴趣的特征中的每一个特征进行softmax打分, 例如这一组感兴趣的特征可能是一幅图片, 那么就是对图片中的每一个像素进行打分, 例如这一组感兴趣的特征是一个句子, 那么就是对该句子中的每一个单词进行打分。通常注意力机制的输入为当前状态<i>h</i><sub><i>t</i></sub>和感兴趣的一组特征<i>f</i>= (<i>f</i><sup> 1</sup>, <i>f</i><sup> 2</sup>, …, <i>f</i><sup><i>n</i></sup>) , 输出为对这<i>n</i>个特征的softmax打分<i>s</i>= (<i>s</i><sup>1</sup>, <i>s</i><sup>2</sup>, …, <i>s</i><sup><i>n</i></sup>) , 在后续处理中可以利用该打分对特征进行过滤<citation id="240" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>或重新整合输入到系统中。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57">1.2 <b>生成对抗网络</b></h4>
                <div class="p1">
                    <p id="58">生成对抗网络 (GAN) <citation id="241" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>由一个生成器 (Generator, G) 和一个判别器 (Discriminator, D) 组成。生成器的输入为符合某种先验分布的噪声, 如高斯分布、均匀分布等, 生成器学着把该分布变化为与样本分布一致的分布;判别器的输入为来自生成器中的样本和来自训练集的样本, 判别器学着分辨一个样本是由生成器生成的还是训练集中的。通过同时对生成器和判别器的博弈训练, 生成器最终可以生成类似训练集的样本。由于生成器学到的是一个和训练集类似的概率分布, 每次采样都会给出一个不同的合理样本, 故可以被用来对多种可能性进行预测。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.3 <b>长短期记忆网络模块</b></h4>
                <div class="p1">
                    <p id="60">循环神经网络 (RNN) 被广泛用来对时序序列建模, 每一个时刻, RNN根据上一个时刻的状态<b><i>h</i></b><sup><i>t</i>-1</sup>和当前的输入<b><i>x</i></b><sup><i>t</i></sup>计算出当前时刻的状态<b><i>h</i></b><sup><i>t</i></sup>, 故每一个时刻状态<b><i>h</i></b><sup><i>t</i></sup>包含了它之前时刻的所有有用信息。</p>
                </div>
                <div class="p1">
                    <p id="61"><b><i>h</i></b><sup><i>t</i></sup> = <i>RNN</i> (<b><i>h</i></b><sup><i>t</i>-1</sup>, <b><i>x</i></b><sup><i>t</i></sup>;<b><i>W</i></b><sub>RNN </sub>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="62">为了解决传统RNN训练时后向传播存在的梯度弥散的问题, 长短期记忆网络 (Long Short Term Memory, LSTM) <citation id="242" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>被提出。通过加入输入门、遗忘门和输出门来在每个时刻有选择地遗忘, 有选择地加入新的信息给当前状态<b><i>h</i></b><sup><i>t</i></sup>, 该策略使得后向误差可以向前传播很多步不消失。类似于RNN, LSTM网络也可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="63"><b><i>h</i></b><sup><i>t</i></sup>=<i>LSTM</i> (<b><i>h</i></b><sup> (<i>t</i>-1) </sup>, <b><i>x</i></b><sup><i>t</i></sup>;<b><i>W</i></b><sub>LSTM</sub>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="64">其中:<b><i>h</i></b><sup><i>t</i></sup>为时刻<i>t</i>的隐含状态, <b><i>x</i></b><sup><i>t</i></sup>为时刻<i>t</i>时模型的输入, <b><i>W</i></b><sub>RNN </sub>为RNN的模型参数, <b><i>W</i></b><sub>LSTM</sub>为LSTM的模型参数。</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag">2 行人轨迹生成模型</h3>
                <h4 class="anchor-tag" id="66" name="66">2.1 <b>问题定义</b></h4>
                <div class="p1">
                    <p id="67">给定一系列行人的轨迹<b><i>X</i></b>= (<b><i>X</i></b><sub>1</sub>, <b><i>X</i></b><sub>2</sub>, …, <b><i>X</i></b><sub><i>n</i></sub>) , 其中<b><i>X</i></b><sub><i>i</i></sub>={ (<i>x</i><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>, <i>y</i><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>) ∈<b>R</b><sup>2</sup>|<i>t</i>=1, 2, …, <i>t</i><sub>obs</sub>}。<i>n</i>为该场景下的行人个数, <i>t</i><sub>obs</sub>为观测序列的长度。本文的任务是预测出这些行人未来的轨迹<b><i>Y</i></b>= (<b><i>Y</i></b><sub>1</sub>, <b><i>Y</i></b><sub>2</sub>, …, <b><i>Y</i></b><sub><i>n</i></sub>) , 其中<b><i>Y</i></b><sub><i>i</i></sub>={ (<i>x</i><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>, <i>y</i><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>) ∈<b>R</b><sup>2</sup>|<i>t</i>=<i>t</i><sub>obs</sub>+1, <i>t</i><sub>obs</sub>+2, …, <i>t</i><sub>obs</sub>+<i>t</i><sub>pred</sub>}, <i>t</i><sub>pred</sub>为预测序列的长度。这里, 用<b><i>Y</i></b>表示行人在未来的真实轨迹, 用<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover></math></mathml>表示生成器实际预测出的轨迹。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">2.2 <b>整体网络结构</b></h4>
                <div class="p1">
                    <p id="74">如图1, 整体网络结构包括一个生成器和一个判别器。生成器根据过去的轨迹<b><i>X</i></b><sub><i>i</i></sub>学习生成未来的轨迹<b><i>Y</i></b><sub><i>i</i></sub>。判别器学习判别一个轨迹是生成器生成的轨迹[<b><i>X</i></b><sub><i>i</i></sub>, <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>], 还是来自训练集的真实的轨迹[<b><i>X</i></b><sub><i>i</i></sub>, <b><i>Y</i></b><sub><i>i</i></sub>]。图1为场景中有<i>n</i>=3个行人时AttenGAN的网络结构。生成器由注意力模块, 编码器和解码器组成。注意力模块接收其他行人个体相对于当前个体的相对位置<b><i>R</i></b><sub><i>i</i></sub>和当前个体编码器LSTM上一个时刻<i>t</i>-1的隐含状态<b><i>h</i></b><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>, 通过注意力机制选择最有用的信息作为编码器LSTM的输入<b><i>H</i></b><sub><i>i</i></sub>。编码器接收当前个体的位置<b><i>X</i></b><sub><i>i</i></sub>与注意力机制提取的周围的其他行人的重要信息<b><i>H</i></b><sub><i>i</i></sub>, 更新得到当前个体的状态<b><i>h</i></b><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>。解码器根据当前的个体的状态<b><i>h</i></b><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>并连接一个高斯噪声<b><i>z</i></b>来对解码器的LSTM进行初始化, 并解码产生相应的预测路径<mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>。判别器的结构与生成器中编码器的结构相似。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903010_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 AttenGAN总体网络结构" src="Detail/GetImg?filename=images/JSJY201903010_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 AttenGAN总体网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903010_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Overview of AttenGAN architecture</p>

                </div>
                <h4 class="anchor-tag" id="81" name="81">2.3 <b>注意力模块</b></h4>
                <div class="p1">
                    <p id="82">行人个体的行为决策不仅与他自己当前的位置信息<b><i>X</i></b><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>和过去的状态<b><i>h</i></b><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>有关, 也与他周围的行人个体有关。为了刻画他周围的行人个体对他的影响, 使用注意力机制来选择对当前行人有用的其他行为个体的位置信息<b><i>H</i></b><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>输入到编码器LSTM模块。</p>
                </div>
                <div class="p1">
                    <p id="86"><b><i>R</i></b><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>={ (<i>x</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup></mrow></math></mathml>-<i>x</i><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>, <i>y</i><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>t</mi></msubsup></mrow></math></mathml>-<i>y</i><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>) ∈<b>R</b><sup>2</sup>|<i>t</i>=1, …, <i>t</i><sub>obs</sub>}      (3) </p>
                </div>
                <div class="p1">
                    <p id="92"><b><i>C</i></b><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>=<i>FC</i> (<b><i>R</i></b><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>;<b><i>W</i></b><sub>fc1</sub>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="95"><b><i>C</i></b><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>=[<b><i>C</i></b><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow><mi>t</mi></msubsup></mrow></math></mathml>, <b><i>C</i></b><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow><mi>t</mi></msubsup></mrow></math></mathml>, …, <b><i>C</i></b><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>n</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>]      (5) </p>
                </div>
                <div class="p1">
                    <p id="100"><b><i>H</i></b><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>=<i>f</i><sup>att</sup> (<b><i>C</i></b><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>, <b><i>h</i></b><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mrow><mi mathvariant="bold-italic">t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="104">首先, 将某个行人相对当前行人的位置<b><i>R</i></b><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>经过一个全连接网络<i>FC</i> (·) 将其从坐标空间映射到特征空间<b><i>C</i></b><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>, 其中<b><i>W</i></b><sub>fc1</sub>是全连接网络的参数, 连接所有位置信息<b><i>C</i></b><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>得到<b><i>C</i></b><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>。然后根据行人之前的状态<b><i>h</i></b><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>和其他行人的位置信息<b><i>C</i></b><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>, 注意力机制<i>f</i><sup>att</sup>选择出那些对该行人有用的信息<b><i>H</i></b><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>, 该注意力机制的采用文献<citation id="243" type="reference">[<a class="sup">16</a>]</citation>的“软注意力”机制, 因其完全可导, 网络可以直接进行端对端的训练。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">2.4 <b>编码器模块</b></h4>
                <div class="p1">
                    <p id="113">编码器模块LSTM的输入为注意力模块提取的对当前行人有用的周围行人的位置信息<b><i>H</i></b><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>和当前行人的位置信息<b><i>X</i></b><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="116"><b><i>h</i></b><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">e</mi><mi mathvariant="bold-italic">i</mi></mrow><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>=<i>LSTM</i> (<b><i>h</i></b><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">e</mi><mi mathvariant="bold-italic">i</mi></mrow><mrow><mi mathvariant="bold-italic">t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>, [<b><i>E</i></b><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>, <b><i>H</i></b><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>];<b><i>W</i></b><sub>en</sub>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="121">其中:</p>
                </div>
                <div class="p1">
                    <p id="122"><b><i>E</i></b><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>=<i>FC</i> (<b><i>X</i></b><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>;<b><i>W</i></b><sub>fc2</sub>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="125">每个时刻<i>t</i>编码器首先将当前行人的位置<b><i>X</i></b><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>通过一个全连接网络<i>FC</i> (·) 将其从坐标空间转化到特征空间得到特征<b><i>E</i></b><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>, 然后与周围行人的位置信息<b><i>H</i></b><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>连接输入到LSTM中进行编码, 重复该过程直到将观测序列<i>t</i>=1, 2, …, <i>t</i><sub>obs</sub>时刻的信息全部编码完毕。<b><i>W</i></b><sub>en</sub>为编码器LSTM的参数, <b><i>W</i></b><sub>fc2</sub>为全连接网络的参数。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">2.5 <b>解码器模块</b></h4>
                <div class="p1">
                    <p id="130">解码器模块LSTM的初始隐含状态<b><i>h</i></b><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>由编码器最后时刻<i>t</i>=<i>t</i><sub>obs</sub>的状态<b><i>h</i></b><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>e</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>和高斯噪声<b><i>z</i></b>决定。将<b><i>h</i></b><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>e</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>经过一个多层感知机模块<i>MLP</i> (·) 并与噪声<b><i>z</i></b>连接得到<b><i>h</i></b><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="135"><b><i>h</i></b><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>=[<i>MLP</i> (<b><i>h</i></b><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>e</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>) , <b><i>z</i></b>;<b><i>W</i></b><sub>MLP1</sub>]      (9) </p>
                </div>
                <div class="p1">
                    <p id="138">在对每个时刻<i>t</i>-1行人位置作预测时, 首先将预测的上一个时刻<i>t</i>-1的位置<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover></math></mathml><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>通过一个全连接网络<i>FC</i> (·) 从坐标空间转化到特征空间得到特征<b><i>L</i></b><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>, 根据LSTM计算出当前的状态<b><i>h</i></b><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>, 再将当前的状态<b><i>h</i></b><mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>通过一个多层感知机<i>MLP</i> (·) 转化到坐标空间得到<mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover></math></mathml><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>。注意到在预测第一个时刻<i>t</i>=<i>t</i><sub>obs</sub>+1的时候, 直接使用观测序列最后一个观测到的位置信息<b><i>X</i></b><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>s</mtext></mrow></msub></mrow></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="147"><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">L</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mi>F</mi><mi>C</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>;<b><i>W</i></b><sub>fc3</sub>)      (10) </p>
                </div>
                <div class="p1">
                    <p id="149"><b><i>h</i></b><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>=<i>LSTM</i> (<b><i>h</i></b><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>, <b><i>L</i></b><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>;<b><i>W</i></b><sub>de</sub>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="153"><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>, </mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">) </mo><mo>=</mo><mi>Μ</mi><mi>L</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>;<b><i>W</i></b><sub>MLP2</sub>)      (12) </p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">2.6 <b>判别器</b></h4>
                <div class="p1">
                    <p id="156">判别器的输入为来自生成器生成的轨迹[<b><i>X</i></b><sub><i>i</i></sub>, <mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>]或来自训练集的真实的轨迹[<b><i>X</i></b><sub><i>i</i></sub>, <b><i>Y</i></b><sub><i>i</i></sub>], 这里将这两种轨迹都用<b><i>T</i></b><sub><i>i</i></sub>表示。判别器的结构类似于编码器, 首先将当前轨迹<b><i>T</i></b><mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>从坐标空间转化到特征空间得到<b><i>P</i></b><mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>, 然后使用LSTM不断地更新当前状态<b><i>h</i></b><mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>, 最后将最终时刻的状态<b><i>h</i></b><mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>通过一个多层感知机<i>MLP</i> (·) 以及一个softmax的激活函数得到该轨迹真实程度的打分。</p>
                </div>
                <div class="p1">
                    <p id="162"><b><i>P</i></b><mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>=<i>FC</i> (<b><i>T</i></b><mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>;<b><i>W</i></b><sub>fc4</sub>)      (13) </p>
                </div>
                <div class="p1">
                    <p id="165"><b><i>h</i></b><mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>i</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>=<i>LSTM</i> (<b><i>h</i></b><mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>i</mi></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>, <b><i>P</i></b><mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>;<b><i>W</i></b><sub><i>dis</i></sub>)      (14) </p>
                </div>
                <div class="p1">
                    <p id="169"><b><i>Y</i></b><sub><i>disi</i></sub>=<i>MLP</i> (<b><i>h</i></b><mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">s</mi><mi mathvariant="bold-italic">i</mi></mrow><mi mathvariant="bold-italic">t</mi></msubsup></mrow></math></mathml>;<b><i>W</i></b><sub>MLP3</sub>)      (15) </p>
                </div>
                <h4 class="anchor-tag" id="171" name="171">2.7 <b>损失函数</b></h4>
                <div class="p1">
                    <p id="172">总体的损失函数包含两部分, 一部分是预测的轨迹与真实的轨迹之间的偏移误差, 另一部分是对抗损失, 其中, <i>λ</i>为超参数, 用来平衡位置偏移误差损失和对抗损失。整个模型的优化目标可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="173" class="code-formula">
                        <mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mtext>G</mtext><mtext>A</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><mo>, </mo><mi>D</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><mi>L</mi><msub><mrow></mrow><mrow><mi>L</mi><mn>2</mn></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="174">其中, 对抗损失为:</p>
                </div>
                <div class="p1">
                    <p id="175"><i>L</i><sub>GAN</sub>=<i>E</i><sub><i>Y</i><sub><i>i</i></sub></sub>[log <i>D</i> (<b><i>Y</i></b><sub><i>i</i></sub>) ]+<i>E</i><sub><i>z</i></sub>[log (1-<i>D</i> (<i>G</i> (<b><i>X</i></b><sub><i>i</i></sub>, <b><i>z</i></b>) ) ) ]      (17) </p>
                </div>
                <div class="p1">
                    <p id="176">位置偏移损失为:</p>
                </div>
                <div class="p1">
                    <p id="177"><i>L</i><sub><i>L</i>2</sub> (<i>G</i>) =<i>E</i><sub><i>z</i></sub>[‖<b><i>Y</i></b><sub><i>i</i></sub>-<i>G</i> (<b><i>X</i></b><sub><i>i</i></sub>, <b><i>z</i></b>) ‖<sub>2</sub>]      (18) </p>
                </div>
                <h4 class="anchor-tag" id="178" name="178">2.8 <b>模型训练过程</b></h4>
                <div class="p1">
                    <p id="179">对抗训练过程中, 生成器<i>G</i>根据过去的轨迹<b><i>X</i></b><sub><i>i</i></sub>以及从先验噪声分布中采样的<b><i>z</i></b>伪造出未来可能的轨迹<i>G</i> (<b><i>X</i></b><sub><i>i</i></sub>, <b><i>z</i></b>) , 该轨迹输入到判别器后得到<i>D</i> (<i>G</i> (<b><i>X</i></b><sub><i>i</i></sub>, <b><i>z</i></b>) ) , 生成器尽力使其接近1, 但判别器尽量使其接近0, 在这样的博弈训练过程中, 最终两者达到平衡, 由此达到让生成器预测出符合物理约束和社会规范的轨迹的目的。具体的训练过程见以下伪代码。</p>
                </div>
                <div class="p1">
                    <p id="180">为保证生成器生成轨迹的多样性, 在计算位置偏移损失时, 本文进行<i>k</i>次采样, 并选用损失最小的那组数据的对网络进行反向传播优化。由于训练时误差回传使用的是效果最好的一种猜测, 该种方式可以促进模型大胆地对各种可能性进行猜测, 而不是给出一个相对保险的轨迹预测, 该策略可以有效地避免生成器预测的轨迹大都相似的问题<citation id="244" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。本文中取<i>k</i>=10, 在对模型评价的时候, 同理也是随机进行<i>k</i>次采样选用最小的偏移误差作为模型在偏移误差指标上的最终表现。模型训练过程如GAN_LOOP所示。</p>
                </div>
                <div class="p1">
                    <p id="181">网络训练过程GAN_LOOP:</p>
                </div>
                <div class="area_img" id="249">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201903010_24900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="195" name="195" class="anchor-tag">3 实验仿真与结果分析</h3>
                <div class="p1">
                    <p id="196">本文在数据集ETH<citation id="245" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>和UCY<citation id="246" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>上验证所提出的AttenGAN模型, 这两个数据集包含五个不同的真实的行人交互的场景, ETH数据集包含ETH和Hotel两个场景, UCY数据集包含Zara1、Zara2和Univ三个场景。实验运行在Ubuntu 16.04 LTS 的操作系统上, GPU为NVIDIA GTX 1080TI, CPU为i7700k, 采用Pytorch 0.4的深度学习框架, CUDA toolkit 8.0的运行环境。</p>
                </div>
                <div class="p1">
                    <p id="197">为验证本文提出的模型的有效性, 选取了经典的线性回归模型Linear、朴素的长短期记忆网络模型LSTM以及最近提出的社会长短期记忆网络模型 (S-LSTM) 和社会对抗网络模型 (S-GAN) 作对比。</p>
                </div>
                <h4 class="anchor-tag" id="198" name="198">3.1 <b>模型参数与训练过程</b></h4>
                <div class="p1">
                    <p id="199">网络训练采用批量训练的方式, 每一批包含64组数据, 每一组数据包含若干个行人, 观测序列长度<i>t</i><sub>obs</sub>=8, 预测序列长度<i>t</i><sub>pred</sub>=8, 训练和预测阶段生成器采样次数<i>k</i>=8。使用Adam优化算法对生成器和判别器进行同时训练降低其损失函数。训练迭代次数设为8 000, 生成器和判别器的学习速率都设为1×10<sup>-3</sup>, 每隔4 000次将学习速率减小为原来的0.1倍, Adam优化器中的权重衰减系数设为1×10<sup>-5</sup>。</p>
                </div>
                <div class="p1">
                    <p id="200">测试集与训练集的划分方式与文献<citation id="247" type="reference">[<a class="sup">9</a>,<a class="sup">10</a>]</citation>类似, 每个场景中划分70%的数据为训练集, 30%的数据为验证集。采用五折交叉验证的方式, 用其他4个场景中的训练数据训练网络, 选用在验证集上表现最好的网络在目标场景进行测试和精度计算, 在5个场景中重复这样的操作。模型训练过程中生成损失, 判别损失和位置偏移损失如图2所示。</p>
                </div>
                <div class="area_img" id="201">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903010_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 AttenGAN的训练损失函数" src="Detail/GetImg?filename=images/JSJY201903010_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 AttenGAN的训练损失函数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903010_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Loss of AttenGAN model training</p>

                </div>
                <div class="p1">
                    <p id="202">由图2可知, 随着训练过程迭代次数的增加, 生成损失和位置偏移损失呈现缓慢下降趋势, 判别损失呈现缓慢上升趋势, 但可以看到最终都波动减小, 趋于平缓。综合损失函数的变化趋势, 实验中将训练的迭代次数设为8 000。</p>
                </div>
                <h4 class="anchor-tag" id="203" name="203">3.2 <b>预测精度性能比较</b></h4>
                <div class="p1">
                    <p id="204">类似于文献<citation id="248" type="reference">[<a class="sup">20</a>]</citation>, 选用平均偏移误差 (Average Differential Error, ADE) 和最终偏移误差 (Final Differential Error, FDE) 作为评价指标来刻画预测轨迹的准确性。</p>
                </div>
                <div class="p1">
                    <p id="205" class="code-formula">
                        <mathml id="205"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>A</mi><mi>D</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>t</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow></mfrac></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mi>t</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>s</mtext></mrow></msub><mo>+</mo><mn>1</mn></mrow><mrow><mi>t</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>s</mtext></mrow></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow></munderover><mrow><msqrt><mrow><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mstyle><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><mi>D</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msqrt><mrow><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow></msubsup><mo>-</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow></msubsup><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mrow><mi>t</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mstyle><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="206">其中:平均偏移误差ADE用来刻画预测序列的在所有预测时刻<i>t</i>平均的准确性, 最终偏移误差FDE用来刻画预测序列累计在最后时刻<i>t</i><sub>pred</sub>的准确性。</p>
                </div>
                <div class="p1">
                    <p id="207">各种轨迹模型的结果在平均偏移误差ADE上的对比结果如表1的ADE部分, 在最终偏移误差FDE上的对比结果如表1的FDE部分, 表中单位为米 (m) , 表中每行描述一个方法在不同场景下的计算偏移误差。ADE和FDE数值越小表示模型预测与真实偏移越小, 模型预测越准确, 表中用下划线标明了在每个场景上表现性能最好的模型。</p>
                </div>
                <div class="p1">
                    <p id="208">从表1中可以看出, 考虑平均偏移误差ADE时, 所提的AttenGAN模型在Univ、Zara1和Zara2的场景上的预测精度性能均是最好的。相对于基于网格划分与“社会池化”的S-LSTM模型, S-GAN和AttenGAN使用了对抗机制, 因此预测</p>
                </div>
                <div class="p1">
                    <p id="209">精度更高。而S-GAN所提的基于“池化模块”融合的方式需要在观测序列结束的时刻<i>t</i><sub>obs</sub>来融合周围行人的信息, AttenGAN则可以在每一个时刻<i>t</i>均进行信息融合, 并且, 不同于S-GAN在“池化模块”对于所有行人作全局性的最大池化操作, AttenGAN“有选择”地融合对当前行人有用的那些信息, 因此模型具有更强的表现能力, 进而可以精确地对行人交互模式进行刻画。故相比于S-GAN, AttenGAN的预测精度得以进一步的提高。</p>
                </div>
                <div class="p1">
                    <p id="210">从表1的FDE部分可以看出, 考虑最终偏移误差FDE时, 所提的AttenGAN模型在ETH、Univ和Zara2的场景中预测精度性能均是最好的。</p>
                </div>
                <div class="p1">
                    <p id="211">但是注意到在Hotel场景中, 线性回归Linear模型在ADE和FDE指标上都表现最好, 猜测这可能与Hotel场景中行人交互较少, 大多为线性化的轨迹有一定关系。</p>
                </div>
                <div class="area_img" id="212">
                    <p class="img_tit"><b>表</b>1 <b>各种轨迹预测模型在</b>ADE<b>和</b>FDE<b>上的结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Performance comparison of various trajectory prediction methods in terms of ADE and FDE</p>
                    <p class="img_note"></p>
                    <table id="212" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="5"><br />ADE/m</td><td rowspan="2"></td><td colspan="5"><br />FDE/m</td></tr><tr><td><br />Linear</td><td>LSTM</td><td>S-LSTM</td><td>S-GAN</td><td>AttenGAN</td><td><br />Linear</td><td>LSTM</td><td>S-LSTM</td><td>S-GAN</td><td>AttenGAN</td></tr><tr><td>ETH</td><td>0.84</td><td>0.70</td><td>0.73</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.72</td><td></td><td>1.60</td><td>1.45</td><td>1.48</td><td>1.47</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mo>.</mo><mn>4</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />Hotel</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.55</td><td>0.49</td><td>0.52</td><td>0.38</td><td></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>1.17</td><td>1.01</td><td>1.05</td><td>0.71</td></tr><tr><td><br />Univ</td><td>0.56</td><td>0.36</td><td>0.41</td><td>0.36</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td></td><td>1.01</td><td>0.77</td><td>0.84</td><td>0.75</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>9</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />Zara1</td><td>0.41</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.27</td><td>0.26</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td></td><td>0.74</td><td>0.53</td><td>0.56</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>4</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.49</td></tr><tr><td><br />Zara2</td><td>0.53</td><td>0.31</td><td>0.33</td><td>0.29</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>3</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td></td><td>0.95</td><td>0.65</td><td>0.79</td><td>0.58</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>4</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />平均</td><td>0.53</td><td>0.43</td><td>0.45</td><td>0.42</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td></td><td>0.98</td><td>0.91</td><td>0.91</td><td>0.86</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>7</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="213">综合表1可以看出, 基于神经网络的模型比传统的线性Linear模型预测精度更高, 这是由于神经网络模型更加复杂, 表现能力更强;基于生成对抗网络GAN的模型如S-GAN和AttenGAN比直接对偏移误差优化的LSTM和S-LSTM预测精度高, 这是由于生成对抗的训练方式提高了模型对概率分布的刻画性能, 进一步优化了性能。由于注意力机制的引入, 相比S-GAN本文提出的AttenGAN模型在预测精度性能上有进一步的提升, 平均ADE和平均FDE更低。</p>
                </div>
                <div class="p1">
                    <p id="214">图3中给出了Linear、S-LSTM和AttenGAN在ETH数据集的Hotel场景上的预测轨迹对比, 虚线为模型可观测的过去的轨迹, 最后2张图为AttenGAN表现不好的情况。模型根据观测的8个时刻 (3.2 s) 的轨迹, 向前预测8个时刻 (3.2 s) 。由于AttenGAN每次会预测10条轨迹, 这里展示位置偏移误差最小的一条轨迹。从图3中可以看出, AttenGAN能够预测出与真实轨迹相近的轨迹。在图3 (a) 中AttenGAN成功地预测出该行人将前面的人超过并且又回到原路线的行为, Linear则无法对这样的场景进行较好的预测, 而S-LSTM却给出了超过前面行人然后“向右”避让另一个行人的预测轨迹。在图3 (c) 中, AttenGAN预测出了两个行人并排行走面对冲突时, 选择保持原方向等待对方让路的行为, 而S-LSTM则预测该行人会采取“避让”的方式。这在一定程度上体现了AttenGAN可以进行多种可能性的预测的优势, 如在上述情况S-LSTM预测的路线也是有可能的, 但由于其预测的单一性, 无法对真实情况正确建模。图3 (e) (f) 为AttenGAN预测失败的情形, 在图3 (e) 中该行人向右转走向公交车, AttenGAN则预测其会继续向前走, 在图3 (f) 中行人斜向穿过了人行道, AttenGAN则预测其会减速跟随前面的行人继续在人行道上行走, 失败的情形说明AttenGAN缺乏对行人某些有目的的特定行为进行建模的能力, 但AttenGAN仍然给出了较为合理的预测轨迹。</p>
                </div>
                <h4 class="anchor-tag" id="216" name="216">3.3 <b>模型效率比较</b></h4>
                <div class="p1">
                    <p id="217">本节对比基于神经网络实现方式的各种轨迹预测模型的时间性能, 训练所用参数均和3.1节中的模型参数一致, 每个模型训练的迭代次数都设为8 000。为反映模型的实时性能, 各个模型的预测时间平均为预测一组数据所需要的时间。</p>
                </div>
                <div class="p1">
                    <p id="218">从表2中可以看出, 从上到下, 所提模型的复杂程度逐渐增加, 模型训练时间和预测时间均增加。S-LSTM“社会池化”层需要融合网格中的多个隐含状态, 故时间开销要大于LSTM。相比基于LSTM的方法, 基于GAN方法的S-GAN和AttenGAN需要训练生成模型和判别模型两个模型, 且要进行多次采样, 而每一次采样生成器都会进行一次前向传播, 故训练所用时间更长。同理, S-GAN与AttenGAN模型在预测时间方面相比其他方法更久也是由于其每次会预测多个合理序列而不是一个平均性的序列。相比S-GAN, AttenGAN训练时间更长一些, 这是由于AttenGAN引入的注意力机制在每一个时刻都进行信息融合, 而S-GAN只在观测了完整观测序列后进行一次全局池化, 故AttenGAN时间花费更大。综上, AttenGAN牺牲了部分时间性能来保证其预测精度以及所生成轨迹的多样性, 但模型的实时性能依旧可以保证。</p>
                </div>
                <div class="area_img" id="219">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903010_219.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 几种模型预测轨迹的对比" src="Detail/GetImg?filename=images/JSJY201903010_219.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 几种模型预测轨迹的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903010_219.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Comparison of predicted trajectories by several models</p>

                </div>
                <div class="area_img" id="220">
                    <p class="img_tit"><b>表</b>2 <b>各种轨迹预测模型的时间性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Performance comparison of various trajectory prediction methods in terms of time efficiency</p>
                    <p class="img_note"></p>
                    <table id="220" border="1"><tr><td>方法</td><td>训练<br />时间/min</td><td>预测<br />时间/ms</td><td></td><td>方法</td><td>训练<br />时间/min</td><td>预测<br />时间/ms</td></tr><tr><td>LSTM</td><td>13.4</td><td>2.7</td><td></td><td>S-GAN</td><td>58.5</td><td>29.4</td></tr><tr><td><br />S-LSTM</td><td>21.6</td><td>4.2</td><td></td><td>AttenGAN</td><td>69.2</td><td>33.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="221" name="221">3.4 <b>定性分析</b></h4>
                <div class="p1">
                    <p id="222">给定一个场景, 根据过去的轨迹未来可能会有多种合理性的预测, 不同于传统模型给出一个平均化的轨迹预测, 得益于生成对抗的思想, 每一次生成器从噪声取样AttenGAN都会给出一种合理性的预测。图4中为UCY数据集中Zara2场景在4分40秒时, AttenGAN给出的3种预测可能性, 模型向前预测8个时刻 (3.2 s) , 其中实线是真实的行人轨迹, 虚线是模型预测的行人轨迹, 箭头方向指示行人的运动方向, 箭头位置代表行人最终所在位置。图4 (a) 为模型预测行人之间保持原行走速度和方向不发生碰撞的情形;图4 (b) 为模型预测的右边行人通过减慢速度并改变方向来与避免与左边行人发生碰撞的情形;图4 (c) 为模型预测的行人之间采用“靠右”的社会规范进行避让的情形, 该预测与真实的行人轨迹比较贴合。</p>
                </div>
                <div class="p1">
                    <p id="223">可以看出, 模型可以预测出的多种行人之间相互避让情形的合理轨迹, 能够基本满足物理约束和社会规范。</p>
                </div>
                <h3 id="224" name="224" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="225">为对行人的交互性运动模式进行建模, 本文提出了基于注意力机制的轨迹预测生成模型, 并引入生成对抗网络对模型进行训练。实验结果表明注意力机制可以很好地描述行人之间的交互性运动模式, 并可以对行人轨迹进行联合性的预测, 在预测精度性能方面上优于Linear、LSTM、S-LSTM和S-GAN模型。同时, 基于生成对抗网络的训练方式该模型可以对未来多种合理的可能性轨迹进行预测, 而非仅仅预测一个平均的轨迹。</p>
                </div>
                <div class="p1">
                    <p id="226">由于模型复杂程度的提高, 可以看出, 该模型在时间性能上与其他模型相比较差, 且在行人交互较少的场景中表现欠佳, 推测可能在用注意力机制进行信息融合时引入了部分噪声。</p>
                </div>
                <div class="area_img" id="227">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903010_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 同一场景下的多种合理性预测" src="Detail/GetImg?filename=images/JSJY201903010_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 同一场景下的多种合理性预测  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903010_227.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Many reasonable trajectory predictions in one scene</p>

                </div>
                <div class="p1">
                    <p id="228">通过注意力机制, 可以将其他模态信息例如图片等信息加入到该模型中, 进而进一步提高预测的准确性。后续研究会考虑将其他模态的信息例如图片, 视频信息融合到模型中来刻画外界物理环境对行人的影响, 以此进一步提高模型的预测精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Avoiding cars and pedestrians using velocity obstacles and motion prediction">

                                <b>[1]</b> LARGE F, VASQUEZ D, FRAICHARD T, et al. Avoiding cars and pedestrians using velocity obstacles and motion prediction [EB/OL]. [2018- 07- 01]. https://www.researchgate.net/publication/29642615_Avoiding_Cars_and_Pedestrians_using_V-Obstacles_and_Motion_Prediction.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Probabilistic Model of HumanMotion and Navigation Intent for Mobile Robot Path Planning">

                                <b>[2]</b> THOMPSON S, HORIUCHI T, KAGAMI S. A probabilistic model of human motion and navigation intent for mobile robot path planning [C]// Proceedings of the 2009 4th International Conference on Autonomous Robots and Agents. Piscataway, NJ: IEEE, 2009: 663-668.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Motion Patterns of People for Compliant Robot Motion">

                                <b>[3]</b> BENNEWITZ M. Learning motion patterns of people for compliant robot motion [J]. The International Journal of Robotics Research, 2005, 24 (1) : 31-48.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Social force model for pedestrian dynamics">

                                <b>[4]</b> HELBING D, MOLNÁR P. Social force model for pedestrian dynamics [J]. Physical Review E: Statistical Physics Plasmas Fluids and Related Interdisciplinary Topics, 1995, 51 (5) : 4282-4286.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unfreezing the robot:navigation in dense,interacting crowds">

                                <b>[5]</b> TRAUTMAN P, KRAUSE A. Unfreezing the robot: navigation in dense, interacting crowds [C]// Proceedings of the 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. Piscataway, NJ: IEEE, 2010: 797-803.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Trajectory Learning for Activity Understanding: Unsupervised, Multilevel, and Long-Term Adaptive Approach">

                                <b>[6]</b> MORRIS B T, TRIVEDI M M. Trajectory learning for activity understanding: unsupervised, multilevel, and long-term adaptive approach [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (11) : 2287-2301.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Activity forecasting">

                                <b>[7]</b> KITANI K M, ZIEBART B D, BAGNELL J A, et al. Activity forecasting [C]// Proceedings of the 2012 European Conference on Computer Vision, LNCS 7575. Berlin: Springer, 2012: 201-214.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Social LSTM:Human Trajectory Prediction in Crowded Spaces">

                                <b>[8]</b> ALAHI A, GOEL K, RAMANATHAN V, et al. Social LSTM: human trajectory prediction in crowded spaces [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2016: 961-971.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Social attention:modeling attention in human crowds">

                                <b>[9]</b> VEMULA A, MUELLING K, OH J. Social attention: modeling attention in human crowds [EB/OL]. [2018- 03- 25]. https://arxiv.org/pdf/1710.04689.pdf.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Social GAN:socially acceptable trajectories with generative adversarial networks">

                                <b>[10]</b> GUPTA A, JOHNSON J, LI F-F, et al. Social GAN: socially acceptable trajectories with generative adversarial networks [EB/OL]. [2018- 05- 04]. https://arxiv.org/abs/1803.10892.pdf.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent models of visual attention">

                                <b>[11]</b> MNIH V, HEESS N, GRAVES A, et al. Recurrent models of visual attention [C]// Proceedings of the 27th International Conference on Neural Information Processing Systems. Cambridge, MA: MIT Press, 2014, 2: 2204-2212.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural Sentiment Classification with User and Product Attention">

                                <b>[12]</b> CHEN H, SUN M, TU C, et al. Neural sentiment classification with user and product attention [C]// Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas: , 2016: 1650-1659.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201805010&amp;v=MDE3MzZXNzNMTHo3QmQ3RzRIOW5NcW85RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGw=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 卢玲, 杨武, 王远伦, 等.结合注意力机制的长文本分类方法[J].计算机应用, 2018, 38 (5) :1272-1277. (LU L, YANG W, WANG Y L, et al. Long text classification combined with attention mechanism [J]. Journal of Computer Applications, 2018, 38 (5) :1272-1277.) 
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al. Generative adversarial networks[J]. Advances in Neural Information Processing Systems, 2014, 3: 2672-2680.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long short-term memory">

                                <b>[15]</b> HOCHREITER S, SCHMIDHUBER J. Long short-term memory [M]// GRAVES A. Supervised Sequence Labelling with Recurrent Neural Networks. Berlin: Springer, 2012: 37-45.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention">

                                <b>[16]</b> XU K, BA J, KIROS R, et al. Show, attend and tell: Neural image caption generation with visual attention [EB/OL]. [2018- 07- 01]. https://arxiv.org/pdf/1502.03044v2.pdf.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A point set generation network for 3Dobject reconstruction from a single image">

                                <b>[17]</b> FAN H, SU H, GUIBAS L. A point set generation network for 3D object reconstruction from a single image [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2017: 2463-2471.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving data association by joint modeling of pedestrian trajectories and groupings">

                                <b>[18]</b> PELLEGRINI S, ESS A, van GOOL L. Improving data association by joint modeling of pedestrian trajectories and groupings [C]// Proceedings of the 2010 European Conference on Computer Vision, LNCS 6311. Berlin: Springer, 2010: 452-465.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000018016&amp;v=MTk5NjYwPU5pZmNhck80SHRITXI0NU5aT29KWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaS9rVzcvQUls&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> LERNER A, CHRYSANTHOU Y, LISCHINSKI D. Crowds by example [J]. Computer Graphics Forum, 2007, 26 (3) : 655-664.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DESIRE:distant future prediction in dynamic scenes with interacting agents">

                                <b>[20]</b> LEE N, CHOI W, VERNAZA P, et al. DESIRE: Distant future prediction in dynamic scenes with interacting Agents [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition. Washington, DC: IEEE Computer Society, 2017: 2165-2174.This work is partially supported by the National Natural Science Foundation of China (51775332, 51675329, 51675342) , the Program of State Key Laboratory of Mechanical System and Vibration (GZ2016KF001, GKZD020018) , the Open Program of State Key Laboratory of Smart Manufacturing for Special Vehicles and Transmission System (GZ2016KF001) .SUN Yasheng, born in 1995, M. S. candidate. His research interests include deep learning, computer vision.JIANG Qi, born in 1995, M. S. candidate. His research interests include machine learning, natural language processing. HU Jie, born in 1973, Ph. D., professor. His research interests include intelligent design, knowledge fusion. QI Jin, born in 1984, Ph. D., associate research fellow. His research interests include artificial intelligence, knowledge based engineering. PENG Yinghong, born in 1963, Ph. D., professor. His research interests include innovative design, knowledge based engineering.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201903010" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903010&amp;v=MjI3MzFwRmlEbFc3M0xMejdCZDdHNEg5ak1ySTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
