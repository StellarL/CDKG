<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136768483252500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905048%26RESULT%3d1%26SIGN%3d5QHhmMbXeQqmFUDCC8g2sQDb76o%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905048&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905048&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905048&amp;v=MTc3MTdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5WcjdLTHo3QmQ3RzRIOWpNcW85QmI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="1 本文方法 ">1 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="1.1 &lt;b&gt;数据处理&lt;/b&gt;">1.1 <b>数据处理</b></a></li>
                                                <li><a href="#60" data-title="1.2 CDGAN&lt;b&gt;模型&lt;/b&gt;">1.2 CDGAN<b>模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#87" data-title="2.1 &lt;b&gt;分析结果评价指标&lt;/b&gt;">2.1 <b>分析结果评价指标</b></a></li>
                                                <li><a href="#96" data-title="2.2 &lt;b&gt;实验结果&lt;/b&gt;">2.2 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="图1 深度学习模型训练和测试流程">图1 深度学习模型训练和测试流程</a></li>
                                                <li><a href="#59" data-title="图2 数据增强示例">图2 数据增强示例</a></li>
                                                <li><a href="#65" data-title="图3 CDGAN模型结构">图3 CDGAN模型结构</a></li>
                                                <li><a href="#73" data-title="图4 改进U-net模型框架">图4 改进U-net模型框架</a></li>
                                                <li><a href="#94" data-title="图5 模型评估覆盖集">图5 模型评估覆盖集</a></li>
                                                <li><a href="#98" data-title="图6 三种评估值随训练次数的变化曲线">图6 三种评估值随训练次数的变化曲线</a></li>
                                                <li><a href="#101" data-title="图7 三种模型的预测结果">图7 三种模型的预测结果</a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;各模型在测试集中的评估结果&lt;/b&gt;"><b>表</b>1 <b>各模型在测试集中的评估结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="137">


                                    <a id="bibliography_1" title=" WEN D, HUANG X, ZHANG L, et al.A novel automatic change detection method for urban high-resolution remotely sensed imagery based on multiindex scene representation[J].IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (1) :609-625." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Novel Automatic Change Detection Method for Urban Highresolution Remotely Sensed Imagery Based on Multi-index Scene Representation">
                                        <b>[1]</b>
                                         WEN D, HUANG X, ZHANG L, et al.A novel automatic change detection method for urban high-resolution remotely sensed imagery based on multiindex scene representation[J].IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (1) :609-625.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_2" title=" WALTER V.Object-based classification of remote sensing data for change detection[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2004, 58 (3/4) :225-238." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501968868&amp;v=MTE0ODFyM0lLRndSYXhjPU5pZk9mYks3SHRETnFvOUViZTBIQkhveG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         WALTER V.Object-based classification of remote sensing data for change detection[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2004, 58 (3/4) :225-238.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_3" title=" CELIK T.Multiscale change detection in multitemporal satellite images[J].IEEE Geoscience and Remote Sensing Letters, 2009, 6 (4) :820-824." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiscale change detection in multitemporal satellite images">
                                        <b>[3]</b>
                                         CELIK T.Multiscale change detection in multitemporal satellite images[J].IEEE Geoscience and Remote Sensing Letters, 2009, 6 (4) :820-824.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_4" title=" CELIK T, MA K.Multitemporal image change detection using undecimated discrete wavelet transform and active contours[J].IEEE Transactions on Geoscience and Remote Sensing, 2011, 49 (2) :706-716." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multitemporal Image Change Detection Using Undecimated Discrete Wavelet Transform and Active Contours">
                                        <b>[4]</b>
                                         CELIK T, MA K.Multitemporal image change detection using undecimated discrete wavelet transform and active contours[J].IEEE Transactions on Geoscience and Remote Sensing, 2011, 49 (2) :706-716.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_5" title=" MURA M D, BENEDIKTSSON J A, BOVOLO F, et al.An unsupervised technique based on morphological filters for change detection in very high resolution images[J].IEEE Geoscience and Remote Sensing Letters, 2008, 5 (3) :433-437." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Unsupervised Technique Based on Morphological Filters for Change Detection in Very High Resolution Images">
                                        <b>[5]</b>
                                         MURA M D, BENEDIKTSSON J A, BOVOLO F, et al.An unsupervised technique based on morphological filters for change detection in very high resolution images[J].IEEE Geoscience and Remote Sensing Letters, 2008, 5 (3) :433-437.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_6" title=" CAMPS-VALLS G, GOMEZCHOVA L, MUNOZMARI J, et al.Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection[J].IEEE Transactions on Geoscience and Remote Sensing, 2008, 46 (6) :1822-1835." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection">
                                        <b>[6]</b>
                                         CAMPS-VALLS G, GOMEZCHOVA L, MUNOZMARI J, et al.Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection[J].IEEE Transactions on Geoscience and Remote Sensing, 2008, 46 (6) :1822-1835.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_7" title=" 张志强, 张新长, 辛秦川, 等.结合像元级和目标级的高分辨率遥感影像建筑物变化检测[J].测绘学报, 2018, 47 (1) :102-112. (ZHANG Z Q, ZHANG X C, XIN Q C, et al.Combining the pixel-based methods for building change detection using high-resolution remote sensing images[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (1) :102-112.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201801014&amp;v=MzAxMTdyN05KaVhUYkxHNEg5bk1ybzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         张志强, 张新长, 辛秦川, 等.结合像元级和目标级的高分辨率遥感影像建筑物变化检测[J].测绘学报, 2018, 47 (1) :102-112. (ZHANG Z Q, ZHANG X C, XIN Q C, et al.Combining the pixel-based methods for building change detection using high-resolution remote sensing images[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (1) :102-112.) 
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_8" title=" 李亮, 王蕾, 孙晓鹏, 等.面向对象变化向量分析的遥感影像变化检测[J].遥感信息, 2017, 32 (6) :71-77. (LI L, WANG L, SUN X P, et al.Remote sensing change detection method based on object-oriented change vector analysis[J].Remote Sensing Information, 2017, 32 (6) :71-77.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXX201706012&amp;v=MzE3NTVFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblZyN05QQ3JUZHJHNEg5Yk1xWTk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         李亮, 王蕾, 孙晓鹏, 等.面向对象变化向量分析的遥感影像变化检测[J].遥感信息, 2017, 32 (6) :71-77. (LI L, WANG L, SUN X P, et al.Remote sensing change detection method based on object-oriented change vector analysis[J].Remote Sensing Information, 2017, 32 (6) :71-77.) 
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_9" title=" VOLPI M, TUIA D, BOVOLO F, et al.Supervised change detection in VHR images using contextual information and support vector machines[J].International Journal of Applied Earth Observation and Geoinformation, 2013, 20 (2) :77-85." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600261241&amp;v=MjYyODNpclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3UmF4Yz1OaWZPZmJLOEh0RE1xWTlGWnUwT0RuZzRvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         VOLPI M, TUIA D, BOVOLO F, et al.Supervised change detection in VHR images using contextual information and support vector machines[J].International Journal of Applied Earth Observation and Geoinformation, 2013, 20 (2) :77-85.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_10" title=" CHEN K, HUO C, ZHOU Z, et al.Semi-supervised change detection via Gaussian processes[C]// Proceedings of the 2009 IEEE International Geoscience and Remote Sensing Symposium.Piscataway, NJ:IEEE, 2009:996-999." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Change Detection via Gaussian Processes">
                                        <b>[10]</b>
                                         CHEN K, HUO C, ZHOU Z, et al.Semi-supervised change detection via Gaussian processes[C]// Proceedings of the 2009 IEEE International Geoscience and Remote Sensing Symposium.Piscataway, NJ:IEEE, 2009:996-999.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_11" title=" GHOSH S, ROY M, GHOSH A.Semi-supervised change detection using modified self-organizing feature map neural network[J].Applied Soft Computing, 2014, 15:1-20." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised change detection using modified self-organizing feature map neural network">
                                        <b>[11]</b>
                                         GHOSH S, ROY M, GHOSH A.Semi-supervised change detection using modified self-organizing feature map neural network[J].Applied Soft Computing, 2014, 15:1-20.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_12" title=" LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradient-based learning applied to document recognition">
                                        <b>[12]</b>
                                         LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 39 (4) :640-651.</a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_14" title=" RONNEBERGER O, FISCHER P, BROX T, et al.U-Net:Convolutional networks for biomedical image segmentation[C]// Proceedings of the 2015 International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer, 2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[14]</b>
                                         RONNEBERGER O, FISCHER P, BROX T, et al.U-Net:Convolutional networks for biomedical image segmentation[C]// Proceedings of the 2015 International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer, 2015:234-241.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_15" title=" GOODFELLOW I J, POUGETABADIE J, MIRZA M, et al.Generative adversarial nets[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Nets">
                                        <b>[15]</b>
                                         GOODFELLOW I J, POUGETABADIE J, MIRZA M, et al.Generative adversarial nets[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2014:2672-2680.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_16" title=" LUC P, COUPRIE C, CHINTALA S, et al.Semantic segmentation using adversarial networks[J/OL].arXiv Preprint, 2016, 2016:arXiv:1611.08408[2016- 11- 25].https://arxiv.org/abs/1611.08408." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation using adversarial networks">
                                        <b>[16]</b>
                                         LUC P, COUPRIE C, CHINTALA S, et al.Semantic segmentation using adversarial networks[J/OL].arXiv Preprint, 2016, 2016:arXiv:1611.08408[2016- 11- 25].https://arxiv.org/abs/1611.08408.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_17" title=" SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J/OL].arXiv Preprint, 2014, 2014:arXiv:1409.1556 (2014- 09- 04) [2015- 04- 10].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[17]</b>
                                         SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J/OL].arXiv Preprint, 2014, 2014:arXiv:1409.1556 (2014- 09- 04) [2015- 04- 10].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_18" title=" HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[18]</b>
                                         HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_19" title=" LIN T, GOYAL P, GIRSHICK R B, et al.Focal loss for dense object detection[J/OL].arXiv Preprint, 2017, 2017:arXiv:1708.02002 (2017- 08- 07) [2018- 02- 07].https://arxiv.org/abs/1708.02002." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focal loss for dense object detection">
                                        <b>[19]</b>
                                         LIN T, GOYAL P, GIRSHICK R B, et al.Focal loss for dense object detection[J/OL].arXiv Preprint, 2017, 2017:arXiv:1708.02002 (2017- 08- 07) [2018- 02- 07].https://arxiv.org/abs/1708.02002.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_20" title=" IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]// Proceedings of the 32nd International Conference on International Conference on Machine Learning.New York:ACM, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[20]</b>
                                         IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]// Proceedings of the 32nd International Conference on International Conference on Machine Learning.New York:ACM, 2015:448-456.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_21" title=" 杨朔, 陈丽芳, 石瑀, 等.基于深度生成式对抗网络的蓝藻语义分割[J].计算机应用, 2018, 38 (6) :1554-1561. (YANG S, CHEN L F, SHI Y, et al.Semantic segmentation of blue-green algae based on deep generative adversarial net[J].Journal of Computer Applications, 2018, 38 (6) :1554-1561.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806006&amp;v=MDMyNzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVnI3Tkx6N0JkN0c0SDluTXFZOUZZb1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         杨朔, 陈丽芳, 石瑀, 等.基于深度生成式对抗网络的蓝藻语义分割[J].计算机应用, 2018, 38 (6) :1554-1561. (YANG S, CHEN L F, SHI Y, et al.Semantic segmentation of blue-green algae based on deep generative adversarial net[J].Journal of Computer Applications, 2018, 38 (6) :1554-1561.) 
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_22" title=" 高凯珺, 孙韶媛, 姚广顺, 等.基于深度学习的无人车夜视图像语义分割[J].应用光学, 2017, 38 (3) :421-428. (GAO K J, SUN S Y, YAO G S, et al.Semantic segmentation of night vision images for unmanned vehicles based on deep learning[J].Journal of Applied Optics, 2017, 38 (3) :421-428.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYGX201703013&amp;v=MDI2MDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblZyN05QRFRNZHJHNEg5Yk1ySTlFWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         高凯珺, 孙韶媛, 姚广顺, 等.基于深度学习的无人车夜视图像语义分割[J].应用光学, 2017, 38 (3) :421-428. (GAO K J, SUN S Y, YAO G S, et al.Semantic segmentation of night vision images for unmanned vehicles based on deep learning[J].Journal of Applied Optics, 2017, 38 (3) :421-428.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 09:33</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1518-1522 DOI:10.11772/j.issn.1001-9081.2018102083            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于生成对抗网络的地面新增建筑检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%8E%89%E9%BE%99&amp;code=41746667&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王玉龙</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%B2%E5%86%9B&amp;code=38256107&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒲军</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E6%B1%9F%E5%8D%8E&amp;code=32655752&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵江华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%8E%E5%BB%BA%E8%BE%89&amp;code=10348230&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黎建辉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%BF%A1%E6%81%AF%E4%B8%AD%E5%BF%83&amp;code=0021859&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院计算机网络信息中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统的基于地物纹理和空间特征的方法很难精确识别地面新增建筑的问题, 提出了一种基于生成对抗网络的新增建筑变化检测模型 (CDGAN) 。首先, 使用Focal损失函数改进传统图像分割网络 (U-net) , 并以此作为模型的生成器 (G) , 用于生成遥感影像的分割结果;然后, 设计了一个16层的卷积神经网络 (VGG-net) 作为鉴别器 (D) , 用于区分生成的结果和人工标注 (GT) 的真实结果;最后, 对生成器和判别器进行对抗训练, 从而得到具有分割能力的生成器。实验结果表明, CDGAN模型的检测准确率达到92%, 比传统U-net模型的平均区域重合度 (IU) 提升了3.7个百分点, 有效地提升了遥感影像中地面新增建筑物的检测精度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感影像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%98%E5%8C%96%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">变化检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Focal%E6%8D%9F%E5%A4%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Focal损失;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王玉龙 (1991—) , 男, 河北邢台人, 硕士研究生, 主要研究方向:遥感图像处理、数据挖掘;;
                                </span>
                                <span>
                                    蒲军 (1994—) , 男, 四川绵阳人, 硕士研究生, 主要研究方向:城市计算、交通数据挖掘;;
                                </span>
                                <span>
                                    赵江华 (1989—) , 女, 河北保定人, 助理研究员, 博士研究生, 主要研究方向:遥感数据处理与分析;;
                                </span>
                                <span>
                                    *黎建辉 (1973—) , 男, 湖北咸宁人, 研究员, 博士生导师, 博士, 主要研究方向:大数据资源开放共享、科学大数据管理、数据出版。电子邮箱lijh@cnic.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (91546125);</span>
                                <span>中国科学院战略性先导科技专项 (XDA19020104);</span>
                    </p>
            </div>
                    <h1><b>Detection of new ground buildings based on generative adversarial network</b></h1>
                    <h2>
                    <span>WANG Yulong</span>
                    <span>PU Jun</span>
                    <span>ZHAO Jianghua</span>
                    <span>LI Jianhui</span>
            </h2>
                    <h2>
                    <span>Computer Network Information Center, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the inaccuracy of the methods based on ground textures and space features in detecting new ground buildings, a novel Change Detection model based on Generative Adversarial Networks (CDGAN) was proposed. Firstly, a traditional image segmentation network (U-net) was improved by Focal loss function, and it was used as the Generator (G) of the model to generate the segmentation results of remote sensing images. Then, a convolutional neutral network with 16 layers (VGG-net) was designed as the Discriminator (D) , which was used for discriminating the generated results and the Ground Truth (GT) results. Finally, the Generator and Discriminator were trained in an adversarial way to get a Generator with segmentation capability. The experimental results show that, the detection accuracy of CDGAN reaches 92%, and the IU (Intersection over Union) value of the model is 3.7 percentage points higher than that of the traditional U-net model, which proves that the proposed model effectively improves the detection accuracy of new ground buildings in remote sensing images.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Network%20(GAN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Network (GAN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=change%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">change detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image semantic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Focal%20loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Focal loss;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Yulong, born in 1991, M. S. candidate. His research interests include remote sensing image processing, data mining. ;
                                </span>
                                <span>
                                    PU Jun, born in 1994, M. S. candidate. His research interests include urban computing, traffic data mining. ;
                                </span>
                                <span>
                                    ZHAO Jianghua, born in 1989, Ph. D. candidate, assistant research fellow. Her research interests include remote sensing data processing and analysis. ;
                                </span>
                                <span>
                                    LI Jianhui, born in 1973, Ph. D. , research fellow. His research interests include open sharing of resources, scientific management of big data, data publishing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (91546125);</span>
                                <span>the Strategic Priority Research Program of the Chinese Academy of Sciences (XDA19020104);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">违规建筑是未经国土资源部审批的建筑, 在国土监察工作中, 监管地面建筑物的改、扩、拆、建是一项十分重要的工作。传统方法一般是基于高分辨率遥感影像提取建筑物变化信息, 但由于地物的纹理和结构特征往往存在类内高方差和类间低方差的问题, 因此识别效果较差<citation id="181" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。为提高变化检测结果的准确性, 很多研究人员利用影像的空间特征进行地物变化检测, 例如: Walter等<citation id="182" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出一种灰度共生矩阵, 基于对象划分的思想将变化检测问题转换为分类问题;Celik<citation id="183" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>首先使用马尔可夫随机场的方法对多时序的遥感影像进行了多尺度的变换检测, 在之后的工作中, Celik基于小波变换提出一种主动轮廓模型<citation id="184" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。此外, 形态学剖面方法也是一种常用的基于空间特征的地物变换检测方法<citation id="185" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。这些方法主要基于原始图像的空间特征, 但是空间特征往往涉及参数较多从而产生高维的特征空间, 在处理大尺度遥感图像时会产生泛化性低和鲁棒性差等问题<citation id="186" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="49">除了传统的基于地物纹理和空间特征的方法, 研究者们也提出了一些基于机器学习的方法来进行变化检测: Camps-Valls等<citation id="187" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>引入了多时相图像分类和改变基于复合内核的检测框架;张志强等<citation id="188" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出基于像元级的图像分割方法, 即利用光谱特征、纹理特征等特征作为机器学习方法的特征构造分类器, 逐像元进行变化检测;李亮等<citation id="189" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用直方图相交距离法进行遥感影像分割, 通过比较两期遥感影像的像斑变化向量进行变化检测;Volpi等<citation id="190" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>利用支持向量机的监督学习方法进行变换检测。同时, 一些利用半监督学习的方法, 例如高斯过程<citation id="191" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和自组织特征映射神经网络<citation id="192" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>也在地物变换检测领域取得较大的突破。</p>
                </div>
                <div class="p1">
                    <p id="50">在使用多时相的遥感影像进行变化检测时, 由于任意两期遥感影像会受到拍摄角度、拍摄季节、拍摄天气、有无云雾遮挡等因素的影响, 因此精确识别地面新增建筑很困难。而且, 通常地面新增建筑像素点数量占整个遥感影像的像素点数比例较小, 即正负样本比例失衡, 这导致了大多数模型的拟合效果较差。随着计算机视觉技术的快速发展, 基于深度学习的方法逐渐成为了图像处理领域研究的热点, 其中卷积神经网络 (Convolutional Neural Network, CNN) <citation id="193" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>经过多领域的实验论证, 在图像分类和分割方面取得了广泛的应用。之后, 对CNN网络结构进行改进的工作不断增多。2014年, 加州大学伯克利分校的Long等<citation id="194" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了全卷积网络 (Fully Convolution Neutral Network, FCN) , 与经典的图像识别网络 (例如ResNet152) 相比, FCN不需要添加全连接层就可以进行密集型像素预测。由于卷积神经网络的池化层在扩大感受域和聚合语义的同时, 会造成原始影像的位置信息丢失, 为解决这个问题, Ronneberger等<citation id="195" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在FCN的基础上提出了U-net模型, 并使用该网络检测医学图像中的细胞边缘;2014年, 蒙特利尔大学的Goodfellow等<citation id="196" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了生成对抗网络 (Generative Adversarial Net, GAN) , 该模型的核心思想是博弈论中的纳什均衡。博弈双方分别是生成器 (Generator, G) 和鉴别器 (Discriminator, D) , 生成器的主要目的是学习真实数据的分布, 鉴别器的主要目的是判断输入数据是来源于真实数据还是生成数据。GAN的强大之处在于GAN可以充分学习原始样本集的数据分布<citation id="197" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。2016年, Luc等<citation id="198" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>第一次将对抗网络应用到了图像分割领域中。</p>
                </div>
                <div class="p1">
                    <p id="51">虽然U-net模型得到较为广泛的应用, 然而其依然存在很多弊端, 例如U-net严重依赖训练样本数量和分布, 模型很容易过拟合, 模型的域适应性较弱。为了使模型鲁棒性更强、防止过拟合、消除模型对训练样本空间的过度依赖, 针对遥感影像变化检测任务, 本文提出了一种用于变化检测的生成对抗网络模型 (Generative Adversarial Nets for Change Detection, CDGAN) , 使用该模型检测两期遥感影像中的新增建筑变化区域。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">1 本文方法</h3>
                <div class="p1">
                    <p id="53">本文提出了用于变化检测的生成对抗网络模型, 即CDGAN, 生成器使用改进U-net模型, 鉴别器为一个带有全连接层的卷积神经网络结构 (VGG-net) <citation id="199" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。鉴别器的结构与常规的图像分类模型类似 (如VGG-net和ResNet<citation id="200" type="reference"><link href="169" rel="bibliography" /><link href="171" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>) , 鉴别器用来解决二分类问题。</p>
                </div>
                <div class="p1">
                    <p id="54">在实验之前需要将数据集划分为训练集、验证集和测试集三部分:训练集用于训练模型, 数据量占整个标注数据集的80%;验证集用于在训练过程中验证模型的拟合状态, 其数据量占整个标注数据集的10%;测试集数据在经过训练后的模型上进行预测, 对模型进行整体评估, 其数据量占整个标注集的10%。模型训练和测试流程如图1所示。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">1.1 <b>数据处理</b></h4>
                <div class="p1">
                    <p id="56">实验数据采用杭州市2006年、2013年的两期高分遥感卫星影像。影像长宽分别为27 392像元和19 968像元, 包括红、绿、蓝三个波段, 空间分辨率为1 m。实验将该两期图像依次切分成长宽都为512像元的子图, 该子图分别是2006年与2013年相同地理区域的遥感影像。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905048_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 深度学习模型训练和测试流程" src="Detail/GetImg?filename=images/JSJY201905048_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 深度学习模型训练和测试流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905048_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Training and testing flowchart of deep learning model</p>

                </div>
                <div class="p1">
                    <p id="58">在训练模型之前, 需要对原始遥感影像进行预处理, 包括合并和数据增强两个步骤。首先, 合并两期遥感图像子图, 得到六通道的三维矩阵数据, 并对三维数据进行翻转、旋转等处理, 与此同时以相同变换形式对人工标注 (Ground Truth, GT) 作规范化处理, 处理后的GT即为训练模型时需要的label值。GT为两通道的图像, 第一通道表示该像素点是非新增建筑的概率, 第二通道表示该像素点是地面新增建筑的概率。数据增强对模型的鲁棒性和不变性具有至关重要的作用, 同时又能防止样本较少造成的模型过拟合问题, 因此, 本文对遥感影像进行了数据增强处理 (如图2所示) , 数据增强的方法包括水平翻转、上下翻转、平移、旋转和仿射。图2中第1、4列为2006年的某区域遥感影像, 第2、5列为2013年对应地区的遥感影像, 第3、6列为对应2013年比2006年地面新增建筑区域标注。</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905048_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 数据增强示例" src="Detail/GetImg?filename=images/JSJY201905048_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 数据增强示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905048_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Examples of data enhancement</p>

                </div>
                <h4 class="anchor-tag" id="60" name="60">1.2 CDGAN<b>模型</b></h4>
                <div class="p1">
                    <p id="61">CDGAN是以GAN为出发点根据特定场景设计出的模型<citation id="201" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 主要用于图像语义分割问题, 其结构如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="62">在CDGAN的训练阶段, 鉴别器被不断的优化, 直至鉴别器能够精确地判断数据来源。生成器与鉴别器交替训练, 最终达到纳什均衡。</p>
                </div>
                <div class="p1">
                    <p id="63">CDGAN的训练过程如下:</p>
                </div>
                <div class="p1">
                    <p id="64">1) 训练生成器。首先固定鉴别器模型参数。针对训练样本<i>x</i>, 生成器输出结果记为<i>G</i> (<i>x</i>) , <i>G</i> (<i>x</i>) 值与GT值产生交叉熵损失。使用梯度下降算法最小化该损失。将原始样本<i>x</i>与<i>G</i> (<i>x</i>) 合并为512×512×8像元的中间数据 (<i>G</i> (<i>x</i>) , <i>x</i>) , 作为鉴别器的输入数据。鉴别器输出结果记为<i>D</i> (<i>G</i> (<i>x</i>) , <i>x</i>) , <i>D</i> (<i>G</i> (<i>x</i>) , <i>x</i>) 以正样本计算交叉熵损失, 最小化该损失优化生成器。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905048_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 CDGAN模型结构" src="Detail/GetImg?filename=images/JSJY201905048_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 CDGAN模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905048_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Framework of CDGAN model</p>

                </div>
                <div class="p1">
                    <p id="66">2) 训练鉴别器。训练鉴别器所使用的负样本由生成器输出图像和原始待分割图像合并得到, 正样本由原始待分割图像和该原始图像对应的GT合并得到。在训练阶段, 首先固定生成器模型参数, 规定 (<i>G</i> (<i>x</i><sub><i>i</i></sub>) , <i>x</i><sub><i>i</i></sub>) 作为鉴别器负样本, (<i>GT</i>, <i>x</i><sub><i>i</i></sub>) 作为鉴别器正样本, 最小化二分类交叉熵损失来优化鉴别器。</p>
                </div>
                <div class="p1">
                    <p id="67">生成器和鉴别器损失函数的定义如下:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>o</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>l</mi></mstyle><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>g</mi><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>l</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>c</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>o</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>l</mi></mstyle><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>c</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>g</mi><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mo>+</mo><mi>l</mi><msub><mrow></mrow><mrow><mtext>b</mtext><mtext>c</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中:<i>N</i>表示样本个数, <i>gt</i><sub><i>i</i></sub>表示第<i>i</i>个样本的GT, <i>x</i><sub><i>i</i></sub>表示第<i>i</i>个样本, <i>l</i><sub>bce</sub>表示交叉熵损失, <i>l</i><sub>fc</sub>表示Focal损失<citation id="202" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">1.2.1 改进的U-net模型</h4>
                <div class="p1">
                    <p id="72">CDGAN模型中的生成器是改进的U-net模型, 该模型是一种端到端的全卷积网络模型, 即模型的网络结构中没有出现全连接层。U-net模型网络结构左半部分为收缩路径, 右半部分为膨胀路径。改进的U-net模型的具体结构如图4所示。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905048_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 改进U-net模型框架" src="Detail/GetImg?filename=images/JSJY201905048_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 改进U-net模型框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905048_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Framework of improved U-net model</p>

                </div>
                <div class="p1">
                    <p id="74">图4的左半部分是U-net的收缩路径, 由4个DownBlock模块依次连接, 每个DownBlock对特征图进行下采样处理。图4的右半部分为膨胀路径, 该路径由4个UpBlock上采样模块依次连接。DownBlock和UpBlock模块是由卷积层、归一化层、激活层和池化层等组成。</p>
                </div>
                <div class="p1">
                    <p id="75">1) 在收缩路径中, 输入值为规范化后的三维矩阵数据, 记为<b><i>X</i></b>, 大小为512×512×6像元, 值域为[0, 1], 其中6为通道数, <b><i>X</i></b>是由前期图像和后期图像合并得到。在U-net训练阶段, 数据分为4步进行传递。每一步都是相同的下采样过程, 即图4中的DownBlock。DownBlock遵循卷积神经网络的经典结构, 包括两个3×3大小的卷积层、两个归一化层 (Batch Normalization, BN) <citation id="203" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、两个Relu激活函数层和一个2× 2大小的池化层 (Max Pooling) 。每层数据经过卷积处理输出后加入局部归一化层使得分类曲面可以更好地落到数据中心从而防止陷入局部最优<citation id="204" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 还可以防止网络在反向传播过程中的梯度消失或爆炸。每一次Max Pooling处理后得到新的特征图, 该特征图的长和宽减半、通道数扩大一倍。同时, 每次Max Pooling处理前需要将该层数据复制并传递到膨胀路径等待合并。</p>
                </div>
                <div class="p1">
                    <p id="76">2) 在膨胀路径中, 膨胀路径的输入数据为收缩路径多个阶段的输出热图, 该膨胀路径将特征图经过4次反卷积上采样操作得到512×512×2像元大小的输出数据。膨胀路径每一步的输入特征图一部分由上一阶段特征图经过两倍上采样得到, 另一部分由收缩路径对应阶段的特征图合并后得到。合并两个路径中的特征图会使分割后的目标区域位置预测更加准确。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">1.2.2 损失函数</h4>
                <div class="p1">
                    <p id="78">在本文的实验中, 生成器<i>U</i>-<i>net</i>结构使用两种损失函数作对比, 分别为<i>Focal</i>损失函数和交叉熵损失函数。<i>Focal</i>损失函数可以提升难例在训练时的权重, 降低非难例的权重, 从而使模型更专注于难例的训练<citation id="205" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。<i>Focal</i>损失函数的形式如下:</p>
                </div>
                <div class="p1">
                    <p id="79"><i>FC</i> (<i>P</i><sub><i>t</i></sub>) =-<i>α</i><sub><i>t</i></sub> (1-<i>P</i><sub><i>t</i></sub>) <sup><i>γ</i></sup> log (<i>P</i><sub><i>t</i></sub>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>p</mi><mo>, </mo></mtd><mtd columnalign="left"><mi>y</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>1</mn><mo>-</mo><mi>p</mi></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中:<i>p</i>为模型预测像素点类别为1的概率值, <i>γ</i>为权重指数, <i>α</i>为样本损失系数。当一个正样本被模型预测的概率<i>p</i>越接近1时, <i>FC</i> (<i>P</i><sub><i>t</i></sub>) 值越小;否则, <i>FC</i> (<i>P</i><sub><i>t</i></sub>) 值越大。同理, 当一个负样本被模型预测的概率<i>p</i>越接近于0时, <i>P</i><sub><i>t</i></sub>更接近1, <i>FC</i> (<i>P</i><sub><i>t</i></sub>) 值越小;否则, 该值越大。当 (<i>P</i><sub><i>t</i></sub>-0.5) <sup>2</sup>≈0时, 样本即被视为难例。</p>
                </div>
                <div class="p1">
                    <p id="82">实验还使用了交叉熵损失函数作为对比, 其形式如下:</p>
                </div>
                <div class="p1">
                    <p id="83"><i>CE</i> (<i>P</i><sub><i>t</i></sub>) =-<i>α</i><sub><i>t</i></sub> log (<i>P</i><sub><i>t</i></sub>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="84">Focal损失与交叉熵损失相比较, Focal损失添加了 (1-<i>P</i><sub><i>t</i></sub>) <sup><i>γ</i></sup>系数, 所以增加了难例的损失贡献, 使得模型在训练的过程中注意力集中在辨识难例上。</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag">2 实验结果与分析</h3>
                <div class="p1">
                    <p id="86">实验采用两种结构作对比实验, 分别为<i>U</i>-<i>net</i>和<i>CDGAN</i>。<i>U</i>-<i>net</i>模型是全卷积网络, 可用于识别遥感影像中的地面新增建筑区域, <i>CDGAN</i>是对抗模型, 其中包含<i>U</i>-<i>net</i>结构作为<i>CDGAN</i>的子结构。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">2.1 <b>分析结果评价指标</b></h4>
                <div class="p1">
                    <p id="88">在遥感影像变化检测的研究中通常使用多种指标来评估模型的预测能力。常用的评估方法有像素准确率 (<i>PixelACC</i>) 、像素平均准确率 (<i>MeanACC</i>) 和平均区域重合度 (<i>Intersection over Union</i>, <i>IU</i>) <citation id="206" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="89">定义三种评估方法如下:</p>
                </div>
                <div class="p1">
                    <p id="90"><i>PixelACC</i>= (<i>U</i>-<i>A</i>∪<i>B</i>+<i>A</i>∩<i>B</i>) /<i>U</i>      (6) </p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>A</mi><mi>C</mi><mi>C</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⋅</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mi>A</mi><mstyle displaystyle="true"><mo>∩</mo><mi>B</mi></mstyle></mrow><mi>B</mi></mfrac><mo>+</mo><mfrac><mrow><mi>U</mi><mo>-</mo><mi>A</mi><mstyle displaystyle="true"><mo>∪</mo><mi>B</mi></mstyle></mrow><mrow><mi>U</mi><mo>-</mo><mi>B</mi></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92"><i>IU</i>= (<i>A</i>∩<i>B</i>) / (<i>A</i>∪<i>B</i>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="93">其中:<i>U</i>为整张遥感影像的像素集合;<i>A</i>为人工标注集合, 即GT集合;<i>B</i>为模型的预测结果集。一张待预测的遥感影像可以用图5的结构描述。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905048_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 模型评估覆盖集" src="Detail/GetImg?filename=images/JSJY201905048_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 模型评估覆盖集  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905048_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Model evaluation coverage set</p>

                </div>
                <div class="p1">
                    <p id="95">PixelACC反映了模型对所有类别的检测能力、MeanACC反映了模型对各类别的平均检测能力, IU值反映了模型对指定类别的检测能力。IU值可以整体反映出分割区域的完整性和分割位置的准确性, 通常被用来作为最终的评价指标<citation id="207" type="reference"><link href="177" rel="bibliography" /><link href="179" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">2.2 <b>实验结果</b></h4>
                <div class="p1">
                    <p id="97">本文使用三个模型进行实验, 分别为使用<i>Focal</i>损失的<i>CDGAN</i>模型 (记为<i>CDGAN</i> (<i>FC</i>) ) 、使用交叉熵损失的<i>U</i>-<i>net</i>模型 (记为<i>U</i>-<i>net</i> (<i>CE</i>) ) 和使用<i>Focal</i>损失的<i>U</i>-<i>net</i>模型 (记为<i>U</i>-<i>net</i> (<i>FC</i>) ) 。图6展示了这三个模型在验证集上的各项评估系数。从各模型的评估值中可以得出, <i>CDGAN</i> (<i>FC</i>) 模型优于<i>U</i>-<i>net</i> (<i>CE</i>) 模型和<i>U</i>-<i>net</i> (<i>FC</i>) 模型。<i>U</i>-<i>net</i> (<i>FC</i>) 模型与<i>U</i>-<i>net</i> (<i>CE</i>) 模型相比, <i>PixcelACC</i>评估值和<i>IU</i>评估值都有所提升。而<i>CDGAN</i> (<i>FC</i>) 模型在这三种模型中分类效果最佳, <i>PixelACC</i>值、<i>MeanACC</i>值和<i>IU</i>值均高于其他两种模型。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905048_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 三种评估值随训练次数的变化曲线" src="Detail/GetImg?filename=images/JSJY201905048_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 三种评估值随训练次数的变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905048_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Curves of three evaluation metrics change with training times</i></p>

                </div>
                <div class="p1">
                    <p id="99">随机抽样3个测试样本, <i>U</i>-<i>net</i>模型和<i>CDGAN</i>模型的分割结果如图7所示, 图7 (<i>c</i>) 中白色像素区域为地面新增建筑区域, 黑色像素区域为非新增建筑区域。</p>
                </div>
                <div class="p1">
                    <p id="100">从第二个样本中可以明显看出, <i>CDGAN</i>网络检测到了<i>U</i>-<i>net</i>未能识别的区域, <i>U</i>-<i>net</i>模型未能识别出左下角的新增建筑区域, 而<i>CDGAN</i>模型识别到了大部分的新增建筑区域。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905048_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 三种模型的预测结果" src="Detail/GetImg?filename=images/JSJY201905048_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 三种模型的预测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905048_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Prediction results of three models</i></p>

                </div>
                <div class="p1">
                    <p id="102">表1是训练好的模型在测试集中的评估系数, 三个模型固定学习率为0.001, <i>CDGAN</i>的各项评估系数均高于<i>U</i>-<i>net</i>。从表1可以看出使用<i>Focal</i>损失函数可以提升<i>U</i>-<i>net</i>模型预测效果。</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表</b>1 <b>各模型在测试集中的评估结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Evaluation results of each model in the test set</i></p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><br />模型</td><td><i>PixelACC</i></td><td><i>MeanACC</i></td><td><i>IU</i></td></tr><tr><td><br /><i>U</i>-<i>net</i> (<i>CE</i>) </td><td>0.907 906</td><td>0.678 749</td><td>0.348 478</td></tr><tr><td><br /><i>U</i>-<i>net</i> (<i>FC</i>) </td><td>0.919 811</td><td>0.689 059</td><td>0.360 908</td></tr><tr><td><br /><i>CDGAN</i> (<i>FC</i>) </td><td>0.920 911</td><td>0.817 496</td><td>0.397 589</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="104">文献<citation id="208" type="reference">[<a class="sup">22</a>]</citation>中对无人车夜视图进行图像语义分割。该文献使用了红外摄像仪捕捉到的彩色图像与其人工标注作为数据集, 并使用了<i>PixelACC</i>值和<i>MeanACC</i>值作为模型评估方法, 分别达到了0.865与0.763的结果。文献<citation id="209" type="reference">[<a class="sup">16</a>]</citation>中使用对抗网络算法进行语义分割研究, 该文献中使用<i>Stanford Background</i> 数据集对图像中的每个像素点进行多分类预测, 如<i>sky</i>、<i>tree</i>、<i>road</i>等类别。最终<i>PixelACC</i>和<i>MeanACC</i>值分别达到了0.687和0.752。</p>
                </div>
                <div class="p1">
                    <p id="105">从上述实验结果可以得出以下结论:在遥感影像地面新增建筑识别任务中, <i>Focal</i>损失函数在处理分割问题时优于交叉熵损失, <i>GAN</i>模型优于<i>U</i>-<i>net</i>分割模型。本文提出的<i>CDGAN</i>模型能有效提升地面新增建筑区域识别的准确率。</p>
                </div>
                <h3 id="106" name="106" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="107">本文首先提出了面临的问题以及问题背景, 即从两期相同地区的遥感影像中获取地面新增建筑的位置, 随后提出了解决该问题的思路。解决方法的第一阶段是通过将两期遥感影像合并成一张六通道的图像作为样本数据, 并采用多种数据增强方法使原始数据集覆盖面更广, 既可以扩大训练样本数量, 又能提升分割模型的鲁棒性, 为后期训练分割模型作准备; 第二阶段提出了针对多通道图像数据的两种分割网络<i>U</i>-<i>net</i> 和<i>CDGAN</i>, 并详细介绍了这两种网络的设计与训练方法, 两种模型使用交叉熵损失和<i>Focal</i>损失进行对比。结果表明, 使用<i>Focal</i>损失函数的<i>CDGAN</i>模型分割效果最佳。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="137">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Novel Automatic Change Detection Method for Urban Highresolution Remotely Sensed Imagery Based on Multi-index Scene Representation">

                                <b>[1]</b> WEN D, HUANG X, ZHANG L, et al.A novel automatic change detection method for urban high-resolution remotely sensed imagery based on multiindex scene representation[J].IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (1) :609-625.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501968868&amp;v=MDc2MTU9TmlmT2ZiSzdIdEROcW85RWJlMEhCSG94b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3UmF4Yw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> WALTER V.Object-based classification of remote sensing data for change detection[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2004, 58 (3/4) :225-238.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiscale change detection in multitemporal satellite images">

                                <b>[3]</b> CELIK T.Multiscale change detection in multitemporal satellite images[J].IEEE Geoscience and Remote Sensing Letters, 2009, 6 (4) :820-824.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multitemporal Image Change Detection Using Undecimated Discrete Wavelet Transform and Active Contours">

                                <b>[4]</b> CELIK T, MA K.Multitemporal image change detection using undecimated discrete wavelet transform and active contours[J].IEEE Transactions on Geoscience and Remote Sensing, 2011, 49 (2) :706-716.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Unsupervised Technique Based on Morphological Filters for Change Detection in Very High Resolution Images">

                                <b>[5]</b> MURA M D, BENEDIKTSSON J A, BOVOLO F, et al.An unsupervised technique based on morphological filters for change detection in very high resolution images[J].IEEE Geoscience and Remote Sensing Letters, 2008, 5 (3) :433-437.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection">

                                <b>[6]</b> CAMPS-VALLS G, GOMEZCHOVA L, MUNOZMARI J, et al.Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection[J].IEEE Transactions on Geoscience and Remote Sensing, 2008, 46 (6) :1822-1835.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201801014&amp;v=Mjg0MDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5WcjdOSmlYVGJMRzRIOW5Ncm85RVlJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 张志强, 张新长, 辛秦川, 等.结合像元级和目标级的高分辨率遥感影像建筑物变化检测[J].测绘学报, 2018, 47 (1) :102-112. (ZHANG Z Q, ZHANG X C, XIN Q C, et al.Combining the pixel-based methods for building change detection using high-resolution remote sensing images[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (1) :102-112.) 
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXX201706012&amp;v=MDI1NzVuVnI3TlBDclRkckc0SDliTXFZOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeUQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 李亮, 王蕾, 孙晓鹏, 等.面向对象变化向量分析的遥感影像变化检测[J].遥感信息, 2017, 32 (6) :71-77. (LI L, WANG L, SUN X P, et al.Remote sensing change detection method based on object-oriented change vector analysis[J].Remote Sensing Information, 2017, 32 (6) :71-77.) 
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600261241&amp;v=MTM2NDRUTW53WmVadEZpbmxVcjNJS0Z3UmF4Yz1OaWZPZmJLOEh0RE1xWTlGWnUwT0RuZzRvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> VOLPI M, TUIA D, BOVOLO F, et al.Supervised change detection in VHR images using contextual information and support vector machines[J].International Journal of Applied Earth Observation and Geoinformation, 2013, 20 (2) :77-85.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Change Detection via Gaussian Processes">

                                <b>[10]</b> CHEN K, HUO C, ZHOU Z, et al.Semi-supervised change detection via Gaussian processes[C]// Proceedings of the 2009 IEEE International Geoscience and Remote Sensing Symposium.Piscataway, NJ:IEEE, 2009:996-999.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised change detection using modified self-organizing feature map neural network">

                                <b>[11]</b> GHOSH S, ROY M, GHOSH A.Semi-supervised change detection using modified self-organizing feature map neural network[J].Applied Soft Computing, 2014, 15:1-20.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradient-based learning applied to document recognition">

                                <b>[12]</b> LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014, 39 (4) :640-651.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[14]</b> RONNEBERGER O, FISCHER P, BROX T, et al.U-Net:Convolutional networks for biomedical image segmentation[C]// Proceedings of the 2015 International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer, 2015:234-241.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Nets">

                                <b>[15]</b> GOODFELLOW I J, POUGETABADIE J, MIRZA M, et al.Generative adversarial nets[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2014:2672-2680.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation using adversarial networks">

                                <b>[16]</b> LUC P, COUPRIE C, CHINTALA S, et al.Semantic segmentation using adversarial networks[J/OL].arXiv Preprint, 2016, 2016:arXiv:1611.08408[2016- 11- 25].https://arxiv.org/abs/1611.08408.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[17]</b> SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J/OL].arXiv Preprint, 2014, 2014:arXiv:1409.1556 (2014- 09- 04) [2015- 04- 10].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[18]</b> HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focal loss for dense object detection">

                                <b>[19]</b> LIN T, GOYAL P, GIRSHICK R B, et al.Focal loss for dense object detection[J/OL].arXiv Preprint, 2017, 2017:arXiv:1708.02002 (2017- 08- 07) [2018- 02- 07].https://arxiv.org/abs/1708.02002.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[20]</b> IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]// Proceedings of the 32nd International Conference on International Conference on Machine Learning.New York:ACM, 2015:448-456.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806006&amp;v=MDA5NTREblZyN05MejdCZDdHNEg5bk1xWTlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 杨朔, 陈丽芳, 石瑀, 等.基于深度生成式对抗网络的蓝藻语义分割[J].计算机应用, 2018, 38 (6) :1554-1561. (YANG S, CHEN L F, SHI Y, et al.Semantic segmentation of blue-green algae based on deep generative adversarial net[J].Journal of Computer Applications, 2018, 38 (6) :1554-1561.) 
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYGX201703013&amp;v=MDU2NzlHRnJDVVI3cWZadVpzRnlEblZyN05QRFRNZHJHNEg5Yk1ySTlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> 高凯珺, 孙韶媛, 姚广顺, 等.基于深度学习的无人车夜视图像语义分割[J].应用光学, 2017, 38 (3) :421-428. (GAO K J, SUN S Y, YAO G S, et al.Semantic segmentation of night vision images for unmanned vehicles based on deep learning[J].Journal of Applied Optics, 2017, 38 (3) :421-428.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905048" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905048&amp;v=MTc3MTdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5WcjdLTHo3QmQ3RzRIOWpNcW85QmI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
