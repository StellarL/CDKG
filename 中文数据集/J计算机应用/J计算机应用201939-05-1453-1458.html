<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136766154502500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905037%26RESULT%3d1%26SIGN%3dRWi4M3bZEk053oUVGBSXKvL5Lbw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905037&amp;v=Mjg4MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyL0lMejdCZDdHNEg5ak1xbzlHWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 最大间隔准则 ">1 最大间隔准则</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="2 本文算法 ">2 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="2.1 &lt;b&gt;降噪&lt;/b&gt;">2.1 <b>降噪</b></a></li>
                                                <li><a href="#85" data-title="2.2 &lt;b&gt;图像分块及构建多流形模型&lt;/b&gt;">2.2 <b>图像分块及构建多流形模型</b></a></li>
                                                <li><a href="#103" data-title="2.3 RMMDLGE/MMC&lt;b&gt;的目标函数模型&lt;/b&gt;">2.3 RMMDLGE/MMC<b>的目标函数模型</b></a></li>
                                                <li><a href="#131" data-title="2.4 &lt;b&gt;目标函数优化&lt;/b&gt;">2.4 <b>目标函数优化</b></a></li>
                                                <li><a href="#156" data-title="2.5 &lt;b&gt;分类识别&lt;/b&gt;">2.5 <b>分类识别</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#166" data-title="3.1 &lt;b&gt;实验数据库&lt;/b&gt;">3.1 <b>实验数据库</b></a></li>
                                                <li><a href="#171" data-title="3.2 &lt;b&gt;参数选择&lt;/b&gt;">3.2 <b>参数选择</b></a></li>
                                                <li><a href="#176" data-title="3.3 &lt;b&gt;分块策略研究&lt;/b&gt;">3.3 <b>分块策略研究</b></a></li>
                                                <li><a href="#179" data-title="3.4 &lt;b&gt;实验一&lt;/b&gt;">3.4 <b>实验一</b></a></li>
                                                <li><a href="#185" data-title="3.5 &lt;b&gt;实验二&lt;/b&gt;">3.5 <b>实验二</b></a></li>
                                                <li><a href="#194" data-title="3.6 &lt;b&gt;实验结果分析&lt;/b&gt;">3.6 <b>实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#198" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="图1 本文算法应用在两个流形数据集的结果">图1 本文算法应用在两个流形数据集的结果</a></li>
                                                <li><a href="#170" data-title="图2 人脸数据库">图2 人脸数据库</a></li>
                                                <li><a href="#173" data-title="图3 Yale库上平均识别率随&lt;i&gt;k&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;、&lt;i&gt;k&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;、&lt;i&gt;k&lt;/i&gt;&lt;sub&gt;3&lt;/sub&gt;、&lt;i&gt;λ&lt;/i&gt;的变化">图3 Yale库上平均识别率随<i>k</i><sub>1</sub>、<i>k</i><sub>2</sub>、<i>k</i><sub>3</sub>、<i>λ</i>的变化</a></li>
                                                <li><a href="#175" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同人脸库可调参数的取值&lt;/b&gt;"><b>表</b>1 <b>不同人脸库可调参数的取值</b></a></li>
                                                <li><a href="#178" data-title="图4 分块尺寸对算法识别性能影响">图4 分块尺寸对算法识别性能影响</a></li>
                                                <li><a href="#181" data-title="图5 Yale人脸数据库实验结果">图5 Yale人脸数据库实验结果</a></li>
                                                <li><a href="#182" data-title="图6 FERET人脸数据库实验结果">图6 FERET人脸数据库实验结果</a></li>
                                                <li><a href="#183" data-title="图7 ORL人脸数据库实验结果">图7 ORL人脸数据库实验结果</a></li>
                                                <li><a href="#184" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;各算法在三个人脸库上的最高识别率&lt;/b&gt;"><b>表</b>2 <b>各算法在三个人脸库上的最高识别率</b></a></li>
                                                <li><a href="#189" data-title="图8 人脸加噪">图8 人脸加噪</a></li>
                                                <li><a href="#190" data-title="图9 添加噪声的Yale人脸数据库实验结果">图9 添加噪声的Yale人脸数据库实验结果</a></li>
                                                <li><a href="#191" data-title="图10 添加噪声的FERET人脸数据库实验结果">图10 添加噪声的FERET人脸数据库实验结果</a></li>
                                                <li><a href="#192" data-title="图11 添加噪声的ORL人脸数据库实验结果">图11 添加噪声的ORL人脸数据库实验结果</a></li>
                                                <li><a href="#193" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各算法在三个人脸库 (添加噪声) 上的最高识别率&lt;/b&gt;"><b>表</b>3 <b>各算法在三个人脸库 (添加噪声) 上的最高识别率</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="221">


                                    <a id="bibliography_1" title=" SEUNG H S, LEE D D.The manifold ways of perception [J].Science, 2000, 290 (5500) :2268-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The manifold ways of perception">
                                        <b>[1]</b>
                                         SEUNG H S, LEE D D.The manifold ways of perception [J].Science, 2000, 290 (5500) :2268-2269.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_2" title=" MIN W L, LU K, HE X F.Locality pursuit embedding [J].Pattern Recognition, 2004, 37 (4) :781-788." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740419&amp;v=MDEzMTV3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3VmFoST1OaWZPZmJLN0h0RE5xWTlGWSs4UENIMA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         MIN W L, LU K, HE X F.Locality pursuit embedding [J].Pattern Recognition, 2004, 37 (4) :781-788.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_3" title=" GUI J, SUN Z N, JIA W, et al.Discriminant sparse neighborhood preserving embedding for face recognition [J].Pattern Recognition, 2012, 45 (8) :2884-2893." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600737824&amp;v=MTkzMDFyM0lLRndWYWhJPU5pZk9mYks3SHRETnFZOUZZK2dJQkg0OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         GUI J, SUN Z N, JIA W, et al.Discriminant sparse neighborhood preserving embedding for face recognition [J].Pattern Recognition, 2012, 45 (8) :2884-2893.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_4" title=" WAN M H, LI M, YANG G W, et al.Feature extraction using two-dimensional maximum embedding difference [J].Information Sciences, 2014, 274:55-69." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300029510&amp;v=MDU1NTBxUVRNbndaZVp0RmlubFVyM0lLRndWYWhJPU5pZk9mYks4SHRMT3JJOUZaT2tHQ1gwNW9CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         WAN M H, LI M, YANG G W, et al.Feature extraction using two-dimensional maximum embedding difference [J].Information Sciences, 2014, 274:55-69.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_5" title=" AI Z H, WONG W K, XU Y, et al.Approximate orthogonal sparse embedding for dimensionality reduction[J].IEEE Transactions on Neural Networks and Learning Systems, 2016, 27 (4) :723-735." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Approximate orthogonal sparse embedding for dimensionality eduction">
                                        <b>[5]</b>
                                         AI Z H, WONG W K, XU Y, et al.Approximate orthogonal sparse embedding for dimensionality reduction[J].IEEE Transactions on Neural Networks and Learning Systems, 2016, 27 (4) :723-735.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_6" title=" ROWEIS S T, SAUL L K.Nonlinear dimensionality reduction by locally linear embedding[J].Science, 2000, 290 (5500) :2323-2326." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nonlinear dimensionality reduction by locally linear embedding">
                                        <b>[6]</b>
                                         ROWEIS S T, SAUL L K.Nonlinear dimensionality reduction by locally linear embedding[J].Science, 2000, 290 (5500) :2323-2326.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_7" title=" HE X F, YAN S C, HU Y X, et al.Face recognition using Laplacianfaces [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (3) :328-340." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition using laplacianfaces">
                                        <b>[7]</b>
                                         HE X F, YAN S C, HU Y X, et al.Face recognition using Laplacianfaces [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (3) :328-340.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_8" title=" TENENBAUM J B, SILVA V D, LANGFORD J C.A global geometric framework for nonlinear dimensionality reduction [J].Science, 2000, 290 (5500) :2319-2323." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A global geometric framework for nonlinear dimensionality reduction">
                                        <b>[8]</b>
                                         TENENBAUM J B, SILVA V D, LANGFORD J C.A global geometric framework for nonlinear dimensionality reduction [J].Science, 2000, 290 (5500) :2319-2323.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_9" title=" BELKIN M, NIYOGI P.Laplacian eigenmaps for dimensionality reduction and data representation [J].Neural Computation, 2003, 15 (6) :1373-1396." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012194&amp;v=MDk0MjdRVE1ud1plWnRGaW5sVXIzSUtGd1ZhaEk9TmlmSlpiSzlIdGpNcW85RlpPb05EWFU5b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         BELKIN M, NIYOGI P.Laplacian eigenmaps for dimensionality reduction and data representation [J].Neural Computation, 2003, 15 (6) :1373-1396.
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_10" title=" LI B, WANG C, HUANG D S.Supervised feature extraction based on orthogonal discriminant projection [J].Neurocomputing, 2009, 73 (1) :191-196." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501911852&amp;v=MDc2MzNlb09CSGs3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3VmFoST1OaWZPZmJLN0h0RE5xbzlFYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         LI B, WANG C, HUANG D S.Supervised feature extraction based on orthogonal discriminant projection [J].Neurocomputing, 2009, 73 (1) :191-196.
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_11" title=" YAN S, XU D, ZHANG B, et al.Graph embedding and extensions:a general framework for dimensionality reduction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (1) :40-51." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph embedding and extensions:a general framework for dimensionality reduction">
                                        <b>[11]</b>
                                         YAN S, XU D, ZHANG B, et al.Graph embedding and extensions:a general framework for dimensionality reduction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (1) :40-51.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_12" title=" LIN K Z, RONG Y H, WU D, et al.Discriminant locality preserving projections based on neighborhood maximum margin [J].International Journal of Hybrid Information Technology, 2014, 7 (6) :165-174." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminant Locality Preserving Projections Based on Neighborhood Maximum Margin">
                                        <b>[12]</b>
                                         LIN K Z, RONG Y H, WU D, et al.Discriminant locality preserving projections based on neighborhood maximum margin [J].International Journal of Hybrid Information Technology, 2014, 7 (6) :165-174.
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_13" title=" LI B, HUANG D S, WANG C, et al.Feature extraction using constrained maximum variance mapping [J].Pattern Recognition, 2008, 41 (11) :3287-3294." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739090&amp;v=MTE4NDVZOUZZK2dHREhVNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGd1ZhaEk9TmlmT2ZiSzdIdEROcQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         LI B, HUANG D S, WANG C, et al.Feature extraction using constrained maximum variance mapping [J].Pattern Recognition, 2008, 41 (11) :3287-3294.
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_14" title=" YANG W K, SUN C Y, ZHANG L.A multi-manifold discriminant analysis method for image feature extraction [J].Pattern Recognition, 2011, 44 (8) :1649-1657." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738184&amp;v=MzI0MDIvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGd1ZhaEk9TmlmT2ZiSzdIdEROcVk5RlkrZ0hEWFE5b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         YANG W K, SUN C Y, ZHANG L.A multi-manifold discriminant analysis method for image feature extraction [J].Pattern Recognition, 2011, 44 (8) :1649-1657.
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_15" title=" WAN M H, LAI Z H.Multi-manifold Locality Graph Embedding based on the Maximum Margin Criterion (MLGE/MMC) for face recognition [J].IEEE Access, 2017, 5:9823-9830." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-manifold locality graphembedding based on the maximum margin criterion (MLGE/MMC)for face recognition">
                                        <b>[15]</b>
                                         WAN M H, LAI Z H.Multi-manifold Locality Graph Embedding based on the Maximum Margin Criterion (MLGE/MMC) for face recognition [J].IEEE Access, 2017, 5:9823-9830.
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_16" title=" LI H, JIANG T, ZHANG K.Efficient and robust feature extraction by maximum margin criterion [J].IEEE Transactions on Neural Networks, 2006, 17 (1) :157-165." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient and robust feature extraction by maximum margin criterion">
                                        <b>[16]</b>
                                         LI H, JIANG T, ZHANG K.Efficient and robust feature extraction by maximum margin criterion [J].IEEE Transactions on Neural Networks, 2006, 17 (1) :157-165.
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_17" title=" HOU C, NIE F, LI X, et al.Joint embedding learning and sparse regression:a framework for unsupervised feature selection [J].IEEE Transactions on Cybernetics, 2014, 44 (6) :793-804." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint embedding learning and sparse regression:A framework for unsupervised feature selection">
                                        <b>[17]</b>
                                         HOU C, NIE F, LI X, et al.Joint embedding learning and sparse regression:a framework for unsupervised feature selection [J].IEEE Transactions on Cybernetics, 2014, 44 (6) :793-804.
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_18" title=" SHI J, MALIK J.Normalized cuts and image segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 22 (8) :888-905." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Normalized cuts and image segmentation">
                                        <b>[18]</b>
                                         SHI J, MALIK J.Normalized cuts and image segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 22 (8) :888-905.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_19" title=" LU J, TAN Y, WANG G.Discriminative multimanifold analysis for face recognition from a single training sample per person [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (1) :39-51." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative Multimanifold Analysis for Face Recognition from a Single Training Sample per Person">
                                        <b>[19]</b>
                                         LU J, TAN Y, WANG G.Discriminative multimanifold analysis for face recognition from a single training sample per person [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (1) :39-51.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_20" title=" TURK M, PENTLAND A.Eigenfaces for recognition [J].Journal of Cognitive Neuroscience, 1991, 3 (1) :72-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500007384&amp;v=Mjc4NjdsVXIzSUtGd1ZhaEk9TmlmSlpiSzlIdGpNcW85RlpPc0lEM1E5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         TURK M, PENTLAND A.Eigenfaces for recognition [J].Journal of Cognitive Neuroscience, 1991, 3 (1) :72-86.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_21" title=" 董西伟, 尧时茂, 王玉伟, 等.基于虚拟样本图像集的多流形鉴别学习算法[J].计算机应用研究, 2018, 35 (6) :1871-1878. (DONG X W, YAO S M, WANG Y W, et al.Virtual sample image set based multi manifold discriminant learning algorithm [J].Application Research of Computers, 2018, 35 (6) :1871-1878.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201806062&amp;v=MDYxMTdyL0lMejdTWkxHNEg5bk1xWTlEWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         董西伟, 尧时茂, 王玉伟, 等.基于虚拟样本图像集的多流形鉴别学习算法[J].计算机应用研究, 2018, 35 (6) :1871-1878. (DONG X W, YAO S M, WANG Y W, et al.Virtual sample image set based multi manifold discriminant learning algorithm [J].Application Research of Computers, 2018, 35 (6) :1871-1878.) 
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_22" title=" WITTEN I H, HALL M A.Data Mining Practical Machine Learning Tools and Techniques [M].3nd ed.San Francisco, CA:Morgan Kaufmann Publishers, 2011:340-345." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data Mining Practical Machine Learning Tools and Techniques">
                                        <b>[22]</b>
                                         WITTEN I H, HALL M A.Data Mining Practical Machine Learning Tools and Techniques [M].3nd ed.San Francisco, CA:Morgan Kaufmann Publishers, 2011:340-345.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 09:47</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1453-1458 DOI:10.11772/j.issn.1001-9081.2018102113            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于最大间隔准则的鲁棒多流形判别局部图嵌入算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%B4%8B&amp;code=22557429&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨洋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%AD%A3%E7%BE%A4&amp;code=09319574&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王正群</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E6%98%A5%E6%9E%97&amp;code=29350228&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐春林</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%A5%E9%99%88&amp;code=41746657&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">严陈</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9E%A0%E7%8E%B2&amp;code=41746658&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鞠玲</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%89%AC%E5%B7%9E%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0155747&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">扬州大学信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E6%96%B9%E6%BF%80%E5%85%89%E7%A7%91%E6%8A%80%E9%9B%86%E5%9B%A2%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=1697017&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北方激光科技集团有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有的多流形人脸识别算法大多直接使用带有噪声的原始数据进行处理, 而带有噪声的数据往往会对算法的准确率产生负面影响的问题, 提出了一种基于最大间距准则的鲁棒多流形判别局部图嵌入算法 (RMMDLGE/MMC) 。首先, 通过引入一个降噪投影对原始数据进行迭代降噪处理, 提取出更加纯净的数据;其次, 对数据图像进行分块, 建立多流形模型;再次, 结合最大间隔准则的思想, 寻求最优的投影矩阵使得不同流形上的样本距离尽可能大, 同时相同流形上的样本距离尽可能小;最后, 计算待识样本流形到训练样本流形的距离进行分类识别。实验结果表明, 与表现较好的最大间距准则框架下的多流形局部图嵌入算法 (MLGE/MMC) 相比, 所提算法在添加噪声的ORL、Yale和FERET库上的分类识别率分别提高了1.04、1.28和2.13个百分点, 分类效果明显提高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%B5%81%E5%BD%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多流形;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%99%8D%E5%99%AA%E6%8A%95%E5%BD%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">降噪投影;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%B5%8C%E5%85%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图嵌入;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%A4%A7%E9%97%B4%E9%9A%94%E5%87%86%E5%88%99&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最大间隔准则;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类识别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    杨洋 (1990—) , 男, 江苏扬州人, 硕士研究生, 主要研究方向:模式识别;;
                                </span>
                                <span>
                                    *王正群 (1965—) , 男, 江苏如东人, 教授, 博士, 主要研究方向:模式识别、机器学习;电子邮箱wzq@yzu.edu.com;
                                </span>
                                <span>
                                    徐春林 (1969—) , 男, 江苏兴化人, 研究员级高级工程师, 硕士, 主要研究方向:信号处理;;
                                </span>
                                <span>
                                    严陈 (1993—) , 男, 江苏如东人, 硕士研究生, 主要研究方向:模式识别;;
                                </span>
                                <span>
                                    鞠玲 (1994—) , 女, 江苏扬州人, 硕士研究生, 主要研究方向:模式识别。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61402395);</span>
                                <span>江苏省科技攻关项目 (BY201506-01);</span>
                    </p>
            </div>
                    <h1><b>Robust multi-manifold discriminant local graph embedding based on maximum margin criterion</b></h1>
                    <h2>
                    <span>YANG Yang</span>
                    <span>WANG Zhengqun</span>
                    <span>XU Chunlin</span>
                    <span>YAN Chen</span>
                    <span>JU Ling</span>
            </h2>
                    <h2>
                    <span>School of Information Engineering, Yangzhou University</span>
                    <span>North Laser Technology Group Corporation Limited</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In most existing multi-manifold face recognition algorithms, the original data with noise are directly processed, but the noisy data often have a negative impact on the accuracy of the algorithm. In order to solve the problem, a Robust Multi-Manifold Discriminant Local Graph Embedding algorithm based on the Maximum Margin Criterion (RMMDLGE/MMC) was proposed. Firstly, a denoising projection was introduced to process the original data for iterative noise reduction, and the purer data were extracted. Secondly, the data image was divided into blocks and a multi-manifold model was established. Thirdly, combined with the idea of maximum margin criterion, an optimal projection matrix was sought to maximize the sample distances on different manifolds while to minimize the sample distances on the same manifold. Finally, the distance from the test sample manifold to the training sample manifold was calculated for classification and identification. The experimental results show that, compared with Multi-Manifold Local Graph Embedding algorithm based on the Maximum Margin Criterion (MLGE/MMC) which performs well, the classification recognition rate of the proposed algorithm is improved by 1.04, 1.28 and 2.13 percentage points respectively on ORL, Yale and FERET database with noise and the classification effect is obviously improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-manifold&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-manifold;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=denoising%20projection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">denoising projection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=graph%20embedding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">graph embedding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=maximum%20margin%20criterion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">maximum margin criterion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification%20and%20identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification and identification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YANG Yang, born in 1990, M. S. candidate. His research interest includes pattern recognition.;
                                </span>
                                <span>
                                    WANG Zhengqun, born in 1965, Ph. D. , professor. His research interests include pattern recognition, machine learning. ;
                                </span>
                                <span>
                                    XU Chunlin, born in 1969, M. S. , professor of engineering. His research interest includes signal processing. ;
                                </span>
                                <span>
                                    YAN Chen, born in 1993, M. S. candidate. His research interest includes pattern recognition. ;
                                </span>
                                <span>
                                    JU Ling, born in 1994, M. S. candidate. Her research interest includes pattern recognition.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-19</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61402395);</span>
                                <span>the Prospective Joint Research Project of Jiangsu Province (BY201506-01);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">在过去十几年中, 流形学习已经成为机器学习与数据挖掘领域的一个重要的研究课题<citation id="275" type="reference"><link href="221" rel="bibliography" /><link href="223" rel="bibliography" /><link href="225" rel="bibliography" /><link href="227" rel="bibliography" /><link href="229" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。目前, 局部线性嵌入 (Locally Linear Embedding, LLE) <citation id="265" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、局部保持投影 (Locality Preserving Projection, LPP) <citation id="266" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、等距特征映射 (ISOmetric MAPping, ISOMAP) <sup></sup><citation id="267" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和拉普拉斯特征映射 (Laplacian Eigenmap, LE) <citation id="268" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等经典流形学习算法已在人脸识别和基因分类等领域得到广泛应用。之后, 又有学者通过加入样本类别信息, 提出了正交鉴别投影 (Orthogonal Discriminant Projection, ODP) <citation id="269" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、边界费舍尔分析 (Margin Fisher Analysis, MFA) <citation id="270" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和鉴别的局部保持投影 (Discriminant Locality Preserving Projections, DLPP) <citation id="271" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等算法。实际上, 不同类别的样本数据差异明显, 而在传统的流形学习算法中, 都默认假设它们位于同一流形内, 这显然是不合适的。为此, 文献<citation id="272" type="reference">[<a class="sup">13</a>]</citation>用局部散布和类间散布来描述子流形和多流形信息, 并在Fisher框架下计算投影, 提出了约束最大方差映射 (Constrained Maximum Variance Mapping, CMVM) 算法; 文献<citation id="273" type="reference">[<a class="sup">14</a>]</citation>则将子流形和多流形的信息分别用类内图和类间图来表示, 提出了多流形判别分析 (Multi-Manifold Discriminant Analysis, MMDA) 算法; 文献<citation id="274" type="reference">[<a class="sup">15</a>]</citation>提出了最大间距准则框架下的多流形局部图嵌入 (Multi-Manifold Locally Graph Embedding based on Maximum Margin Criterion, MLGE/MMC) 算法, 利用图像分块的思想, 对分块的小图像构建多流形, 来处理小样本问题。</p>
                </div>
                <div class="p1">
                    <p id="49">但是, 上述多流形算法的识别性能均受到原始数据中噪声带来的影响。本文针对多流形人脸识别算法处理带有噪声的真实数据的鲁棒性问题, 提出了一种基于最大间距准则的鲁棒多流形判别局部图嵌入 (Robust Multi-Manifold Discriminant Local Graph Embedding based on Maximum Margin Criterion, RMMDLGE/MMC) 算法。首先对原始数据进行迭代降噪处理, 提取出更加纯净的数据;再结合多流形的思想, 对数据图像进行分块, 建立多流形模型;接着在最大间隔准则 (Maximum Margin Criterion, MMC) 的框架下, 寻求最优的投影矩阵使位于不同流形上的数据样本之间的距离尽可能大, 同时位于同一流形上的数据样本之间的距离尽可能小;最后通过计算待识样本流形到训练样本流形的距离进行分类识别。在ORL、Yale和FERET库上的实验, 验证了所提算法的有效性。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">1 最大间隔准则</h3>
                <div class="p1">
                    <p id="51">在MMC算法中, 假设在高维欧氏空间中有样本集<b><i>X</i></b>={<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>N</i></sub>}, <b><i>x</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>D</i></sup>是<i>D</i>维实向量。根据文献<citation id="276" type="reference">[<a class="sup">16</a>]</citation>介绍, 可以定义MMC目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="52">max tr (<b><i>W</i></b><sup>T</sup> (<b><i>S</i></b><sub><i>b</i></sub>-<b><i>S</i></b><sub><i>w</i></sub>) <b><i>W</i></b>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="53">s.t. <b><i>W</i></b><sup>T</sup><b><i>W</i></b>=<b><i>I</i></b></p>
                </div>
                <div class="p1">
                    <p id="54">其中:<b><i>W</i></b>为投影矩阵, <b><i>S</i></b><sub><i>w</i></sub>为类内散布矩阵, <b><i>S</i></b><sub><i>b</i></sub>为类间散布矩阵。</p>
                </div>
                <div class="p1">
                    <p id="55"><b><i>S</i></b><sub><i>b</i></sub>和<b><i>S</i></b><sub><i>w</i></sub>的具体定义如下:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>b</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>Ρ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>w</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中:<i>N</i>为所有样本的个数, <i>N</i><sub><i>i</i></sub>为第<i>i</i>类训练样本的个数, <i>P</i><sub><i>i</i></sub>=<i>N</i><sub><i>i</i></sub>/<i>N</i>为第<i>i</i>类的先验概率, <i>μ</i><sub><i>i</i></sub>表示第<i>i</i>类训练样本的均值, <i>μ</i>表示所有样本的均值, <i>C</i>为类别总数。</p>
                </div>
                <div class="p1">
                    <p id="59">利用拉格朗日乘数法求式 (1) 的最优解, 可得如下特征方程:</p>
                </div>
                <div class="p1">
                    <p id="60"> (<b><i>S</i></b><sub><i>b</i></sub>-<b><i>S</i></b><sub><i>w</i></sub>) <b><i>w</i></b><sub><i>i</i></sub>=<i>λ</i><sub><i>i</i></sub><b><i>w</i></b><sub><i>i</i></sub>      (4) </p>
                </div>
                <div class="p1">
                    <p id="61">则MMC的最优投影矩阵<b><i>W</i></b>可由<b><i>S</i></b><sub><i>b</i></sub>-<b><i>S</i></b><sub><i>w</i></sub>的前<i>d</i>个最大的非零特征值对应的特征向量组成, 即:</p>
                </div>
                <div class="p1">
                    <p id="62"><b><i>W</i></b>=[<b><i>w</i></b><sub>1</sub>, <b><i>w</i></b><sub>2</sub>, …, <b><i>w</i></b><sub><i>d</i></sub>]      (5) </p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">2 本文算法</h3>
                <div class="p1">
                    <p id="64">真实世界中的数据往往带有噪声, 带有噪声的数据会对算法的性能产生负面的影响。针对此问题, 本文算法<i>RMMDLGE</i>/<i>MMC</i>设计降噪处理方法。此外, 算法对数据图像分块, 建立多流形模型, 构造目标函数, 优化目标函数获得投影变换矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">2.1 <b>降噪</b></h4>
                <div class="p1">
                    <p id="66">通过求解下面的优化问题得到降噪投影矩阵, 来对原始数据进行处理:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ρ</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">Ρ</mi></munder><mspace width="0.25em" /><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mo>, </mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mo>, </mo><mn>1</mn></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">X</mi><mo>=</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo>+</mo><mi mathvariant="bold-italic">Ζ</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中:<b><i>P</i></b>∈<b>R</b><sup><i>N</i>×<i>N</i></sup>是降噪投影矩阵, <b><i>P</i></b><sup>T</sup><b><i>X</i></b>表示降噪后纯净的数据, <b><i>Z</i></b>=<b><i>X</i></b>-<b><i>P</i></b><sup>T</sup><b><i>X</i></b>表示噪声, <i>λ</i>为平衡因子, ‖<b><i>Z</i></b><sup>T</sup>‖<sub>2, 1</sub>为<i>l</i><sub>2, 1</sub>范数正则化噪声项, ‖<b><i>P</i></b><sup>T</sup>‖<sub>2, 1</sub>为<i>l</i><sub>2, 1</sub>范数正则化降噪投影项。通过最小化<i>λ</i>‖<b><i>Z</i></b><sup>T</sup>‖<sub>2, 1</sub>+‖<b><i>P</i></b><sup>T</sup>‖<sub>2, 1</sub>可以求得降噪投影矩阵<b><i>P</i></b>, 因为基于<i>l</i><sub>2, 1</sub>范数的正则化项要求满足行稀疏性, 使其对带有噪声的数据样本具有一定的鲁棒性<citation id="277" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="69">求解式 (6) , 将噪声<b><i>Z</i></b>=<b><i>X</i></b>-<b><i>P</i></b><sup>T</sup><b><i>X</i></b>代入, 问题转化为:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>p</mi></munder><mspace width="0.25em" /><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Ρ</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mo>, </mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mo>, </mo><mn>1</mn></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">参照文献<citation id="278" type="reference">[<a class="sup">18</a>]</citation>处理<i>l</i><sub>2, 1</sub>范数优化问题的方法, 有‖<b><i>B</i></b><sup>T</sup>‖<sub>2, 1</sub>=tr (<b><i>BFB</i></b><sup>T</sup>) , 其中<b><i>F</i></b>是对角矩阵, 其对角线上的元素满足<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mo>, </mo><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>是<b><i>B</i></b>的第<i>i</i>列。因此, 式 (7) 可以进一步化简为:</p>
                </div>
                <div class="p1">
                    <p id="73"><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>p</mi></munder><mspace width="0.25em" /><mi>λ</mi><mspace width="0.25em" /><mtext>t</mtext><mtext>r</mtext><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">V</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>+</mo><mtext>t</mtext><mtext>r</mtext></mrow></math></mathml> (<b><i>PΛP</i></b><sup>T</sup>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="75">其中:<b><i>V</i></b>和<i>Λ</i>是对角矩阵。<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>m</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo stretchy="false">∥</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mo>, </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mi>m</mi></msub></mrow></math></mathml>是对应于<b><i>X</i></b>-<b><i>P</i></b><sup>T</sup><b><i>X</i></b>的第<i>m</i>列。<mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Λ</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>n</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mo>, </mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></math></mathml>是对应于<b><i>P</i></b>的第<i>n</i>列。初始化降噪投影矩阵<b><i>P</i></b><sup> (0) </sup>为随机的酉矩阵。</p>
                </div>
                <div class="p1">
                    <p id="78">对式 (8) 进行求导, 令<mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ρ</mi><mo stretchy="false">]</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>, 得降噪投影矩阵的更新公式:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>+</mo><mover accent="true"><mi>λ</mi><mo>˜</mo></mover><mi mathvariant="bold-italic">Λ</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>λ</mi><mo>˜</mo></mover><mo>=</mo><mi>λ</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math></mathml>, 为了方便表示, 后面继续用<i>λ</i>代替。</p>
                </div>
                <div class="p1">
                    <p id="83">当得到降噪投影矩阵<b><i>P</i></b>后, 迭代更新对角矩阵<b><i>V</i></b>和<i>Λ</i>。在两个经典的流形数据集上, ‖<b><i>P</i></b><sup> (<i>t</i>+1) </sup>-<b><i>P</i></b><sup> (<i>t</i>) </sup>‖<sub>F</sub>的大小随着迭代次数<i>t</i>的增加, 其值的趋势变化如图1。容易看出, ‖<b><i>P</i></b><sup> (<i>t</i>+1) </sup>-<b><i>P</i></b><sup> (<i>t</i>) </sup>‖<sub>F</sub>在迭代10次以后基本趋近于零。</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法应用在两个流形数据集的结果" src="Detail/GetImg?filename=images/JSJY201905037_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法应用在两个流形数据集的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Results of the proposed algorithm applied to two manifold datasets</p>

                </div>
                <h4 class="anchor-tag" id="85" name="85">2.2 <b>图像分块及构建多流形模型</b></h4>
                <div class="p1">
                    <p id="86">首先假设<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="bold-italic">X</mi><mo>∼</mo></mrow><mo>=</mo><mo stretchy="false">{</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup><mo>, </mo><mspace width="0.25em" /><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><mo stretchy="false">}</mo></mrow></math></mathml>为训练样本图像, 其中<mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover></math></mathml><sub><i>i</i></sub>为第<i>i</i>个人脸图片, <i>m</i>和<i>n</i>分别表示训练样本图像的高和宽, 若训练样本有<i>C</i>类, 第<i>i</i>类训练样本的数量为<i>N</i><sub><i>i</i></sub>, 将训练样本<b><i>X</i></b>～可以重新表示为[<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub>1</sub>, <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub>2</sub>, …, <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>c</i></sub>], 则<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi></mrow></msub><mo stretchy="false"> (</mo><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>C</mi><mo>, </mo><mn>1</mn><mo>≤</mo><mi>r</mi><mo>≤</mo><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>代表第<i>i</i>类中的第<i>r</i>个样本。图像分块的具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="93">1) 将第<i>i</i>类训练样本<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>中的第<i>r</i>个样本图像<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>ir</i></sub>∈<b>R</b><sup><i>m</i>×<i>n</i></sup>分割成<i>S</i>块大小为<i>a</i>×<i>b</i>的互不重叠的子块, 其中<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false"> (</mo><mi>a</mi><mo>×</mo><mi>b</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="97">2) 类似的, 将第<i>i</i>类中的所有样本图像都分割成<i>S</i>块大小为<i>a</i>×<i>b</i>的互不重叠的子块。</p>
                </div>
                <div class="p1">
                    <p id="98">3) 把第<i>i</i>类中训练图像分割出来的每个子块转化为列向量<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mi>d</mi></msup><mo stretchy="false"> (</mo><mi>d</mi><mo>=</mo><mi>a</mi><mo>×</mo><mi>b</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 同一类中的所有子块位于一个流形内, 每个子块转化的列向量对应流形内的一个点, 利用这些列向量构成一个流形<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false">{</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>, </mo><mi>r</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>s</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>S</mi><mo stretchy="false">}</mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="101">4) 将<i>C</i>个类别的样本都进行分块并构建流形, 可得流形集<b><i>M</i></b>=[<b><i>M</i></b><sub>1</sub>, <b><i>M</i></b><sub>2</sub>, …, <b><i>M</i></b><sub><i>i</i></sub>, …, <b><i>M</i></b><sub><i>C</i></sub>], 其中<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false">{</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>, </mo><mi>r</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>s</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>S</mi><mo stretchy="false">}</mo></mrow></math></mathml>。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">2.3 RMMDLGE/MMC<b>的目标函数模型</b></h4>
                <div class="p1">
                    <p id="104">结合<i>MMC</i>, 将算法的目标函数描述为:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub></mrow></munder><mi>J</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>J</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>J</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>p</mi></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mi>A</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>p</mi></mrow></msub></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></mstyle></mrow></mstyle><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><mn>2</mn></mrow></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>q</mi></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mi>B</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>q</mi></mrow></msub></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></mstyle></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">其中: <b><i>W</i></b><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>C</i>) 是第<i>i</i>个流形的投影矩阵, <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irsp</i></sub>表示<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>的第<i>p</i>个流形外部<i>k</i><sub>1</sub>近邻。<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irsq</i></sub>表示<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>的第<i>q</i>个流形内部<i>k</i><sub>2</sub>近邻。<i>A</i><sub><i>irsp</i></sub>是<mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irsp</i></sub>和<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>的相似性权值, <i>B</i><sub><i>irsq</i></sub>是<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irsq</i></sub>和<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>的相似性权值。下面给出<i>A</i><sub><i>irsp</i></sub>和<i>B</i><sub><i>irsq</i></sub>的具体定义。</p>
                </div>
                <div class="p1">
                    <p id="115">假设<i>A</i><sub><i>irsp</i></sub>为第<i>i</i>个流形<b><i>M</i></b><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>c</i>) 中, 点<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>与流形外部点<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irsp</i></sub>之间的相似性权值, <i>B</i><sub><i>irsq</i></sub>为第<i>i</i>个流形<b><i>M</i></b><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>c</i>) 中, 点<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>与流形内部点<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irsq</i></sub>之间的相似度权值, 则有:</p>
                </div>
                <div class="area_img" id="120">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201905037_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="122">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201905037_12200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="124">其中:<i>N</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext></mrow><mrow><mi>k</mi><mn>1</mn></mrow></msubsup></mrow></math></mathml> (<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>) 表示<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>的流形外部<i>k</i><sub>1</sub>近邻, <i>N</i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msubsup></mrow></math></mathml> (<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>) 表示<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub><i>irs</i></sub>的流形内部<i>k</i><sub>2</sub>近邻, <i>σ</i>为热核参数。</p>
                </div>
                <h4 class="anchor-tag" id="131" name="131">2.4 <b>目标函数优化</b></h4>
                <div class="p1">
                    <p id="132">目标函数中待求解的投影矩阵的个数为<i>C</i>个, 目前还没有算法可以同时求出<i>C</i>个投影矩阵, 本文采用依次固定其中<i>C</i>-1个投影矩阵<b><i>W</i></b><sub>1</sub>, <b><i>W</i></b><sub>2</sub>, …, <b><i>W</i></b><sub><i>i</i>-1</sub>, <b><i>W</i></b><sub><i>i</i>+1</sub>, …, <b><i>W</i></b><sub><i>C</i></sub>, 对第<i>i</i>个流形的投影矩阵<b><i>W</i></b><sub><i>i</i></sub>进行求解的方法。对<b><i>W</i></b><sub><i>i</i></sub>进行求解时, 目标函数 (10) 可以重写为:</p>
                </div>
                <div class="p1">
                    <p id="133" class="code-formula">
                        <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub></mrow></munder><mi>J</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>J</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>J</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo stretchy="false">[</mo><mi>J</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>V</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">]</mo><mo>-</mo><mo stretchy="false">[</mo><mi>J</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>V</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">]</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo stretchy="false">[</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>V</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">]</mo><mo>-</mo><mo stretchy="false">[</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">E</mi><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>V</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="134">其中:<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>≠</mo><mi>i</mi></mrow><mi>C</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac></mrow></mstyle></mrow></mstyle></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>j</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>j</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>j</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>j</mi><mi>r</mi><mi>s</mi><mi>p</mi></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mi>A</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>r</mi><mi>s</mi><mi>p</mi></mrow></msub></mrow></math></mathml>和<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>V</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>≠</mo><mi>i</mi></mrow><mi>C</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac></mrow></mstyle></mrow></mstyle></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>j</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>j</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>j</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>j</mi><mi>r</mi><mi>s</mi><mi>q</mi></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mi>B</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>r</mi><mi>s</mi><mi>q</mi></mrow></msub></mrow></math></mathml>是两个常数, 对<b><i>W</i></b><sub><i>i</i></sub>的求解没有影响, 故可以将其忽略, 式 (13) 可以简化为:</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub></mrow></munder><mi>J</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>J</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>J</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">进一步化简, 式 (14) 中<i>J</i><sub>1</sub> (<b><i>W</i></b><sub><i>i</i></sub>) 等价于:</p>
                </div>
                <div class="area_img" id="140">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201905037_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="142">其中, </p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac></mrow></mstyle></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>p</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>p</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>A</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>p</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">类似的, </p>
                </div>
                <div class="area_img" id="145">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201905037_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="147">其中:</p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">E</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac></mrow></mstyle></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>q</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>q</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>B</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi><mi>q</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149">将式 (15) 和 (17) 代入, 式 (14) 简化为:</p>
                </div>
                <div class="area_img" id="150">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201905037_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="152"><b><i>W</i></b><sub><i>i</i></sub>的最优解可通过求解下面的特征方程获得:</p>
                </div>
                <div class="p1">
                    <p id="153"> (<b><i>D</i></b>-<b><i>E</i></b>) <b><i>w</i></b>=<i>λ</i><b><i>w</i></b>      (20) </p>
                </div>
                <div class="p1">
                    <p id="154">投影矩阵<b><i>W</i></b><sub><i>i</i></sub>由<b><i>D</i></b>-<b><i>E</i></b>的前<i>d</i><sub><i>i</i></sub>个最大的特征值所对应的特征向量构成。具体的, 若{<i>λ</i><sub>1</sub>, <i>λ</i><sub>2</sub>, …, <i>λ</i><sub><i>di</i></sub>}为<b><i>D</i></b>-<b><i>E</i></b>的前<i>d</i><sub><i>i</i></sub>个最大的特征值, 其对应的特征向量为{<b><i>w</i></b><sub>1</sub>, <b><i>w</i></b><sub>2</sub>, …, <b><i>w</i></b><sub><i>di</i></sub>}, 则投影矩阵<b><i>W</i></b><sub><i>i</i></sub>=[<b><i>w</i></b><sub>1</sub>, <b><i>w</i></b><sub>2</sub>, …, <b><i>w</i></b><sub><i>di</i></sub>]。然而对于<i>C</i>个流形就有<i>C</i>个投影矩阵的维度需要确定, 它们的排列组合有<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>种。要从其中选出最优的维度组合将带来巨大的时间复杂度, 本文参考文献<citation id="279" type="reference">[<a class="sup">19</a>]</citation>, 取大于零的特征值的数量来自动确定每个流形投影矩阵的维数。</p>
                </div>
                <h4 class="anchor-tag" id="156" name="156">2.5 <b>分类识别</b></h4>
                <div class="p1">
                    <p id="157">由2.4节, 本文已经得到了<i>C</i>个流形的投影矩阵<b><i>W</i></b><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>C</i>) 。假设<b><i>u</i></b>∈<b>R</b><sup><i>m</i>×<i>n</i></sup>是一个无标记的测试样本图像, 首先同训练样本一样切割成<mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false"> (</mo><mi>a</mi><mo>×</mo><mi>b</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>个大小为<i>a</i>×<i>b</i>的互不重叠的子块, 将每个子块用一个向量表示, 即<b><i>u</i></b><sub><i>s</i></sub>∈<b>R</b><sup><i>d</i></sup> (<i>d</i>=<i>a</i>×<i>b</i>) 。测试样本的所有子块对应的向量构成一个流形<b><i>M</i></b><sub><i>u</i></sub>=[<b><i>u</i></b><sub>1</sub>, <b><i>u</i></b><sub>2</sub>, …, <b><i>u</i></b><sub><i>S</i></sub>], 然后将其投影到<i>C</i>个低维空间来计算测试样本流形<b><i>M</i></b><sub><i>u</i></sub>到每个训练样本流形<b><i>M</i></b><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>C</i>) 之间的距离<i>d</i> (<b><i>M</i></b><sub><i>u</i></sub>, <b><i>M</i></b><sub><i>i</i></sub>) 。<i>d</i> (<b><i>M</i></b><sub><i>u</i></sub>, <b><i>M</i></b><sub><i>i</i></sub>) 的具体定义如下:</p>
                </div>
                <div class="p1">
                    <p id="159" class="code-formula">
                        <mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mn>3</mn></msub></mrow></mfrac></mrow></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi><mi>s</mi></mrow></msub><mo>∈</mo><mi>Ν</mi><msubsup><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>r</mtext></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>3</mn></msub></mrow></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>s</mi></msub><mo stretchy="false">) </mo></mrow></munder><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>s</mi></msub><mo>-</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>r</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="160">其中, <i>N</i><mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>r</mtext></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>3</mn></msub></mrow></msubsup></mrow></math></mathml> (<b><i>u</i></b><sub><i>s</i></sub>) 表示流形<b><i>M</i></b><sub><i>u</i></sub>中子块<b><i>u</i></b><sub><i>s</i></sub> (<i>s</i>=1, 2, …, <i>S</i>) 在流形<b><i>M</i></b><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>C</i>) 中的<i>k</i><sub>3</sub>近邻。通过式 (21) , 本文把计算流形<b><i>M</i></b><sub><i>u</i></sub>和<b><i>M</i></b><sub><i>i</i></sub>之间的距离转化为求流形<b><i>M</i></b><sub><i>u</i></sub>内各个子块<b><i>u</i></b><sub><i>s</i></sub>到流形<b><i>M</i></b><sub><i>i</i></sub>内最近的<i>k</i><sub>3</sub>个子块的平均距离。假设测试样本所对应的类别标签为<i>L</i>, 则有:</p>
                </div>
                <div class="p1">
                    <p id="162"><mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>i</mi></munder><mspace width="0.25em" /><mi>d</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>; <i>i</i>=1, 2, …, <i>C</i>      (22) </p>
                </div>
                <h3 id="164" name="164" class="anchor-tag">3 实验与分析</h3>
                <div class="p1">
                    <p id="165">在本章中, 为了验证本文算法的有效性, 本文在<i>ORL</i>、<i>Yale</i>和<i>FERET</i>三个公开的人脸数据库上将本文算法与<i>PCA</i><citation id="280" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、<i>LPP</i>、<i>DLPP</i>、<i>MFA</i>和<i>MLGE</i>/<i>MMC</i>等五种算法进行比较。同时为了保证实验结果的准确性, 在实验中将人脸图像调整为统一像素, 对于分块策略, 本文参考文献<citation id="281" type="reference">[<a class="sup">21</a>]</citation>将每个子块的大小设置为20×20。实验环境:华硕笔记本电脑, <i>CPU</i>: <i>Intel CORE i</i>7, 内存:8 <i>GB</i>, <i>Matlab R</i>2014<i>a</i>。</p>
                </div>
                <h4 class="anchor-tag" id="166" name="166">3.1 <b>实验数据库</b></h4>
                <div class="p1">
                    <p id="167">1) <i>ORL</i>人脸库共有400张人脸图像, 这些图像一共来自于40名受试者, 每名受试者有10张图像, 每张图像的尺寸均为112×92像素。这些采集的人脸图像在表情、光照、面部装饰物等方面均有一些差异。图2 (<i>a</i>) 中展示了<i>ORL</i>人脸库上某个人的人脸图像, 其尺寸已归一化成60×60像素。</p>
                </div>
                <div class="p1">
                    <p id="168">2) <i>Yale</i>人脸库包含了165张人脸图像, 这些图像一共来自于15名受试者, 每名受试者有11张图像, 每张图像的尺寸均为100×80像素。这些人脸图像均采集于不同光照表情、光照和面部姿态的条件下。图2 (<i>b</i>) 中展示了<i>Yale</i>人脸库上某个人的人脸图像, 其尺寸已归一化成60×60像素。</p>
                </div>
                <div class="p1">
                    <p id="169">3) <i>FERET</i>人脸库的规模很大, 包含1 199个人将近 14 000 张人脸图像, 每张图像的尺寸均为80×80像素, 每张人脸图像在姿态、面部表情和拍摄角度等方面均有一些差异。本次实验选用<i>FERET</i>人脸库的一个子集, 共包含200个人, 每人7张图像, 总共1 400张不同的图像。图2 (<i>c</i>) 中展示了<i>FERET</i>人脸库上某个人的人脸图像, 其尺寸已归一化成60×60像素。</p>
                </div>
                <div class="area_img" id="170">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 人脸数据库" src="Detail/GetImg?filename=images/JSJY201905037_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 人脸数据库  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Face database</i></p>

                </div>
                <h4 class="anchor-tag" id="171" name="171">3.2 <b>参数选择</b></h4>
                <div class="p1">
                    <p id="172">本文算法所涉及的参数有流形外部近邻数<i>k</i><sub>1</sub>、流形内部近邻数<i>k</i><sub>2</sub>、分类近邻数<i>k</i><sub>3</sub>和平衡因子<i>λ</i>。本文在三个人脸库上分别使用四重交叉验证<citation id="282" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>的方法评估RMMDLGE/MMC可调参数的有效性。以Yale人脸库为例, 固定的选取每人前三张图像作为训练样本, 剩下的作为测试样本。图3显示了<i>k</i><sub>1</sub>、<i>k</i><sub>2</sub>、<i>k</i><sub>3</sub>、<i>λ</i>的变化对识别率的影响: 对于<i>k</i><sub>1</sub>, 主要考察步长为5, 在[5, 50]范围内, 其他参数 (<i>k</i><sub>2</sub>=8, <i>k</i><sub>3</sub>=1, <i>λ</i>=200) 固定时, 识别率的变化情况; 对于<i>k</i><sub>2</sub>, 主要考察步长为2, 在[1, 20]范围内, 其他的参数 (<i>k</i><sub>1</sub>=10, <i>k</i><sub>3</sub>=1, <i>λ</i>=200) 固定时, 识别率的变化情况; 对于<i>k</i><sub>3</sub>, 主要考察步长为1, 在[1, 10]范围内, 其他的参数 (<i>k</i><sub>1</sub>=10, <i>k</i><sub>2</sub>=7, <i>λ</i>=200) 固定时, 识别率的变化情况; 对于<i>λ</i>, 主要考察步长为100, 在[100, 1 000]范围内, 其他的参数 (<i>k</i><sub>1</sub>=10, <i>k</i><sub>2</sub>=7, <i>k</i><sub>3</sub>=4) 固定时, 识别率的变化情况。</p>
                </div>
                <div class="area_img" id="173">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Yale库上平均识别率随k1、k2、k3、λ的变化" src="Detail/GetImg?filename=images/JSJY201905037_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Yale库上平均识别率随<i>k</i><sub>1</sub>、<i>k</i><sub>2</sub>、<i>k</i><sub>3</sub>、<i>λ</i>的变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Average recognition rate on Yale database varies with<i>k</i><sub>1</sub>, <i>k</i><sub>2</sub>, <i>k</i><sub>3</sub>and<i>λ</i></p>

                </div>
                <div class="p1">
                    <p id="174">表1列出了分别在三个人脸数据库上, 按照上述同样的方法, 可调参数的取值。</p>
                </div>
                <div class="area_img" id="175">
                    <p class="img_tit"><b>表</b>1 <b>不同人脸库可调参数的取值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Values of adjustable parameters in different face databases</p>
                    <p class="img_note"></p>
                    <table id="175" border="1"><tr><td>人脸库</td><td><i>k</i><sub>1</sub></td><td><i>k</i><sub>2</sub></td><td><i>k</i><sub>3</sub></td><td><i>λ</i></td></tr><tr><td><br />ORL</td><td>15</td><td>5</td><td>2</td><td>100</td></tr><tr><td><br />Yale</td><td>10</td><td>7</td><td>4</td><td>200</td></tr><tr><td><br />FERET</td><td>20</td><td>3</td><td>1</td><td>200</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="176" name="176">3.3 <b>分块策略研究</b></h4>
                <div class="p1">
                    <p id="177">本节进一步在<i>ORL</i>、<i>Yale</i>和<i>FERET</i>三个人脸库上评估分块尺寸<i>a</i>×<i>b</i>对于RMMDLGE/MMC识别率的影响。选取最优的可调参数<i>k</i><sub>1</sub>、<i>k</i><sub>2</sub>、<i>k</i><sub>3</sub>、<i>λ</i>, 其余设置与3.2节相同。图4显示不同分块大小对RMMDLGE/MMC性能的影响。容易发现, 随着分块尺寸<i>a</i>×<i>b</i>变化, 识别率波动很小。因此, 本文算法对于分块尺寸<i>a</i>×<i>b</i>的变化不敏感。</p>
                </div>
                <div class="area_img" id="178">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 分块尺寸对算法识别性能影响" src="Detail/GetImg?filename=images/JSJY201905037_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 分块尺寸对算法识别性能影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Effect of block size on algorithm recognition performance</p>

                </div>
                <h4 class="anchor-tag" id="179" name="179">3.4 <b>实验一</b></h4>
                <div class="p1">
                    <p id="180">将三个数据集都划分成两个主要的子集:训练集和测试集。具体的, 从各数据集的每类人中随机地选取<i>n</i>张图片作为训练样本, 将剩余的人脸图片作为测试样本, 以避免原始人脸数据库的分布对实验结果的影响, 在每个人脸数据库上独立重复进行10次实验, 取10次的平均值作为最终的识别率。为了让在ORL、Yale和FERET人脸数据库上的实验结果更加直观, 用曲线图5～7来分别表示本文算法和其他算法在不同训练样本数下的识别率比较情况。表2列出了各算法在三个人脸库上的最高识别率和标准差, 即ORL库<i>n</i>=6, Yale库<i>n</i>=5, FERET库<i>n</i>=5, 进行比较。</p>
                </div>
                <div class="area_img" id="181">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Yale人脸数据库实验结果" src="Detail/GetImg?filename=images/JSJY201905037_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 Yale人脸数据库实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Experimental results on Yale face database</p>

                </div>
                <div class="area_img" id="182">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 FERET人脸数据库实验结果" src="Detail/GetImg?filename=images/JSJY201905037_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 FERET人脸数据库实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Experimental results on FERET face database</p>

                </div>
                <div class="area_img" id="183">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_183.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 ORL人脸数据库实验结果" src="Detail/GetImg?filename=images/JSJY201905037_183.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 ORL人脸数据库实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_183.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Experimental results on ORL face database</p>

                </div>
                <div class="area_img" id="184">
                    <p class="img_tit"><b>表</b>2 <b>各算法在三个人脸库上的最高识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 The highest recognition rate of each algorithm on three face databases</p>
                    <p class="img_note"></p>
                    <table id="184" border="1"><tr><td><br />算法</td><td>ORL/%</td><td>Yale/%</td><td>FERET/%</td></tr><tr><td><br />PCA</td><td>91.27 (±0.7) </td><td>93.33 (±1.2) </td><td>76.89 (±0.6) </td></tr><tr><td><br />LPP</td><td>90.13 (±0.9) </td><td>94.41 (±0.4) </td><td>84.00 (±1.1) </td></tr><tr><td><br />DLPP</td><td>95.50 (±1.0) </td><td>95.90 (±0.3) </td><td>85.12 (±0.5) </td></tr><tr><td><br />MFA</td><td>96.98 (±1.3) </td><td>97.14 (±0.6) </td><td>91.05 (±1.0) </td></tr><tr><td><br />MLGE/MMC</td><td>97.14 (±0.4) </td><td>96.67 (±0.9) </td><td>93.31 (±0.8) </td></tr><tr><td><br />RMMDLGE/MMC</td><td>97.14 (±1.1) </td><td>97.78 (±0.4) </td><td>95.10 (±0.7) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="185" name="185">3.5 <b>实验二</b></h4>
                <div class="p1">
                    <p id="186">进一步验证本文算法对噪声的鲁棒性, 在三个人脸数据集上按式 (23) 分别加入随机高斯白噪声进行实验, 加入高斯白噪声的人脸如图8。</p>
                </div>
                <div class="p1">
                    <p id="187" class="code-formula">
                        <mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi></mrow></msub><mo>´</mo><mo>=</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi></mrow></msub><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo>×</mo><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>n</mi><mo stretchy="false"> (</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>r</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="188">同样的, 用随机采样的方式在各数据集的每类人中选取<i>n</i>个训练样本, 剩余的样本作为测试样本, 实验10次, 取平均识别率。为了让在添加噪声后的ORL、Yale和FERET人脸数据库上的实验结果更加直观, 用曲线图9～11来分别表示本文算法和其他算法在不同训练样本数下的识别率比较情况。表3列出了各算法在添加噪声后的三个人脸库上的最高识别率和标准差, 即ORL库<i>n</i>=6, Yale库<i>n</i>=5, FERET库<i>n</i>=5, 进行比较。</p>
                </div>
                <div class="area_img" id="189">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_189.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 人脸加噪" src="Detail/GetImg?filename=images/JSJY201905037_189.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 人脸加噪  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_189.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Face adding noise</p>

                </div>
                <div class="area_img" id="190">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 添加噪声的Yale人脸数据库实验结果" src="Detail/GetImg?filename=images/JSJY201905037_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 添加噪声的Yale人脸数据库实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Experimental results on Yale face database with noise test results</p>

                </div>
                <div class="area_img" id="191">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 添加噪声的FERET人脸数据库实验结果" src="Detail/GetImg?filename=images/JSJY201905037_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 添加噪声的FERET人脸数据库实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Experimental results on FERET face database with noise test results</p>

                </div>
                <div class="area_img" id="192">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905037_192.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 添加噪声的ORL人脸数据库实验结果" src="Detail/GetImg?filename=images/JSJY201905037_192.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 添加噪声的ORL人脸数据库实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905037_192.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Experimental results on ORL face database with noise test results</p>

                </div>
                <div class="area_img" id="193">
                    <p class="img_tit"><b>表</b>3 <b>各算法在三个人脸库 (添加噪声) 上的最高识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 The highest recognition rate of each algorithm on three face databases with noise</p>
                    <p class="img_note"></p>
                    <table id="193" border="1"><tr><td><br />算法</td><td>ORL/%</td><td>Yale/%</td><td>FERET/%</td></tr><tr><td><br />PCA</td><td>91.00 (±0.5) </td><td>92.16 (±0.6) </td><td>76.41 (±1.0) </td></tr><tr><td><br />LPP</td><td>89.98 (±0.8) </td><td>93.33 (±0.2) </td><td>83.43 (±0.7) </td></tr><tr><td><br />DLPP</td><td>94.22 (±1.2) </td><td>93.92 (±0.4) </td><td>84.70 (±0.9) </td></tr><tr><td><br />MFA</td><td>96.18 (±0.2) </td><td>96.50 (±0.8) </td><td>90.46 (±0.5) </td></tr><tr><td><br />MLGE/MMC</td><td>96.10 (±1.3) </td><td>96.50 (±1.0) </td><td>92.75 (±0.7) </td></tr><tr><td><br />RMMDLGE/MMC</td><td>97.14 (±0.6) </td><td>97.78 (±0.9) </td><td>94.88 (±0.3) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="194" name="194">3.6 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="195">1) 在三个人脸数据集<i>ORL</i>、<i>Yale</i>和<i>FERET</i>的实验结果表明, 随着训练样本数的增加, 本文算法<i>RMMDLGE</i>/<i>MMC</i>的平均识别率均优于其他几种算法。由图5～7和图9～11可以看出, 当添加噪声后, <i>RMMDLGE</i>/<i>MMC</i>的识别效果优势更加明显。</p>
                </div>
                <div class="p1">
                    <p id="196">2) 在未加入噪声的<i>Yale</i>人脸库上, 当训练样本数为2时, <i>RMMDLGE</i>/<i>MMC</i>算法的识别率比排在第二位的<i>MLGE</i>/<i>MMC</i>算法高出2.66%;在未加入噪声的<i>FERET</i>人脸库上, 当训练样本数为3时, <i>RMMDLGE</i>/<i>MMC</i>算法的识别率比排在第二位的<i>MLGE</i>/<i>MMC</i>算法高出3.55%。因此, 在训练样本数量较少时, 本文所提算法<i>RMMDLGE</i>/<i>MMC</i>可以取得很好的识别效果。</p>
                </div>
                <div class="p1">
                    <p id="197">3) 由表2和表3可以看出, 本文算法<i>RMMDLGE</i>/<i>MMC</i>相对于其他算法在添加噪声的<i>ORL</i>、<i>Yale</i>和<i>FERET</i>库上的分类识别率分别提高了1.04、1.28和2.13个百分点, 算法<i>RMMDLGE</i>/<i>MMC</i>的最高识别率受噪声的影响最小, 充分表明它对噪声具有较强的鲁棒性。</p>
                </div>
                <h3 id="198" name="198" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="199">为了解决多流形算法对噪声的鲁棒性问题, 本文提出了一种基于最大间距准则的鲁棒多流形判别局部图嵌入算法<i>RMMDLGE</i>/<i>MMC</i>。首先, 通过引入一个降噪投影对原始数据进行迭代降噪处理, 提取出更加纯净的数据; 再结合多流形的思想, 对数据图像进行分块, 建立多流形模型; 接着在最大间隔准则的框架下, 寻求最优的投影矩阵使得不同流形上的样本距离尽可能大, 同时相同流形上的样本距离尽可能小; 最后通过计算待识样本流形到训练样本流形的距离进行分类识别。在标准人脸图像库上的实验结果验证了本文算法的有效性。然而, 在运用到实际的过程中, 算法中过多的可调参数将造成参数选择问题, 进一步减少算法中的可调参数将是下一步研究和改进的方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="221">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The manifold ways of perception">

                                <b>[1]</b> SEUNG H S, LEE D D.The manifold ways of perception [J].Science, 2000, 290 (5500) :2268-2269.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740419&amp;v=MTUwNDUrOFBDSDB3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3VmFoST1OaWZPZmJLN0h0RE5xWTlGWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> MIN W L, LU K, HE X F.Locality pursuit embedding [J].Pattern Recognition, 2004, 37 (4) :781-788.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600737824&amp;v=MDU0MDI0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRndWYWhJPU5pZk9mYks3SHRETnFZOUZZK2dJQkg0OW9CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> GUI J, SUN Z N, JIA W, et al.Discriminant sparse neighborhood preserving embedding for face recognition [J].Pattern Recognition, 2012, 45 (8) :2884-2893.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300029510&amp;v=Mjg3ODRDWDA1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3VmFoST1OaWZPZmJLOEh0TE9ySTlGWk9rRw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> WAN M H, LI M, YANG G W, et al.Feature extraction using two-dimensional maximum embedding difference [J].Information Sciences, 2014, 274:55-69.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Approximate orthogonal sparse embedding for dimensionality eduction">

                                <b>[5]</b> AI Z H, WONG W K, XU Y, et al.Approximate orthogonal sparse embedding for dimensionality reduction[J].IEEE Transactions on Neural Networks and Learning Systems, 2016, 27 (4) :723-735.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nonlinear dimensionality reduction by locally linear embedding">

                                <b>[6]</b> ROWEIS S T, SAUL L K.Nonlinear dimensionality reduction by locally linear embedding[J].Science, 2000, 290 (5500) :2323-2326.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition using laplacianfaces">

                                <b>[7]</b> HE X F, YAN S C, HU Y X, et al.Face recognition using Laplacianfaces [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (3) :328-340.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A global geometric framework for nonlinear dimensionality reduction">

                                <b>[8]</b> TENENBAUM J B, SILVA V D, LANGFORD J C.A global geometric framework for nonlinear dimensionality reduction [J].Science, 2000, 290 (5500) :2319-2323.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012194&amp;v=MTc4ODFoST1OaWZKWmJLOUh0ak1xbzlGWk9vTkRYVTlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRndWYQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> BELKIN M, NIYOGI P.Laplacian eigenmaps for dimensionality reduction and data representation [J].Neural Computation, 2003, 15 (6) :1373-1396.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501911852&amp;v=MjM1NjRyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRndWYWhJPU5pZk9mYks3SHRETnFvOUViZW9PQkhrN29CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> LI B, WANG C, HUANG D S.Supervised feature extraction based on orthogonal discriminant projection [J].Neurocomputing, 2009, 73 (1) :191-196.
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph embedding and extensions:a general framework for dimensionality reduction">

                                <b>[11]</b> YAN S, XU D, ZHANG B, et al.Graph embedding and extensions:a general framework for dimensionality reduction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (1) :40-51.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminant Locality Preserving Projections Based on Neighborhood Maximum Margin">

                                <b>[12]</b> LIN K Z, RONG Y H, WU D, et al.Discriminant locality preserving projections based on neighborhood maximum margin [J].International Journal of Hybrid Information Technology, 2014, 7 (6) :165-174.
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739090&amp;v=MTIzMTRLN0h0RE5xWTlGWStnR0RIVTVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRndWYWhJPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> LI B, HUANG D S, WANG C, et al.Feature extraction using constrained maximum variance mapping [J].Pattern Recognition, 2008, 41 (11) :3287-3294.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738184&amp;v=MDg3NDhUTW53WmVadEZpbmxVcjNJS0Z3VmFoST1OaWZPZmJLN0h0RE5xWTlGWStnSERYUTlvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> YANG W K, SUN C Y, ZHANG L.A multi-manifold discriminant analysis method for image feature extraction [J].Pattern Recognition, 2011, 44 (8) :1649-1657.
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-manifold locality graphembedding based on the maximum margin criterion (MLGE/MMC)for face recognition">

                                <b>[15]</b> WAN M H, LAI Z H.Multi-manifold Locality Graph Embedding based on the Maximum Margin Criterion (MLGE/MMC) for face recognition [J].IEEE Access, 2017, 5:9823-9830.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient and robust feature extraction by maximum margin criterion">

                                <b>[16]</b> LI H, JIANG T, ZHANG K.Efficient and robust feature extraction by maximum margin criterion [J].IEEE Transactions on Neural Networks, 2006, 17 (1) :157-165.
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint embedding learning and sparse regression:A framework for unsupervised feature selection">

                                <b>[17]</b> HOU C, NIE F, LI X, et al.Joint embedding learning and sparse regression:a framework for unsupervised feature selection [J].IEEE Transactions on Cybernetics, 2014, 44 (6) :793-804.
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Normalized cuts and image segmentation">

                                <b>[18]</b> SHI J, MALIK J.Normalized cuts and image segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002, 22 (8) :888-905.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative Multimanifold Analysis for Face Recognition from a Single Training Sample per Person">

                                <b>[19]</b> LU J, TAN Y, WANG G.Discriminative multimanifold analysis for face recognition from a single training sample per person [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (1) :39-51.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500007384&amp;v=MTA5MzF0RmlubFVyM0lLRndWYWhJPU5pZkpaYks5SHRqTXFvOUZaT3NJRDNROW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> TURK M, PENTLAND A.Eigenfaces for recognition [J].Journal of Cognitive Neuroscience, 1991, 3 (1) :72-86.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201806062&amp;v=MTQ0ODlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5Vci9JTHo3U1pMRzRIOW5NcVk5RFpvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 董西伟, 尧时茂, 王玉伟, 等.基于虚拟样本图像集的多流形鉴别学习算法[J].计算机应用研究, 2018, 35 (6) :1871-1878. (DONG X W, YAO S M, WANG Y W, et al.Virtual sample image set based multi manifold discriminant learning algorithm [J].Application Research of Computers, 2018, 35 (6) :1871-1878.) 
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data Mining Practical Machine Learning Tools and Techniques">

                                <b>[22]</b> WITTEN I H, HALL M A.Data Mining Practical Machine Learning Tools and Techniques [M].3nd ed.San Francisco, CA:Morgan Kaufmann Publishers, 2011:340-345.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905037" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905037&amp;v=Mjg4MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyL0lMejdCZDdHNEg5ak1xbzlHWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
