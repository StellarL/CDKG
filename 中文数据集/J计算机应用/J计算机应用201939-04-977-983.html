<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775441690000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904008%26RESULT%3d1%26SIGN%3du7PTYLCQ%252fMM1VUrqRgeKOHYdxr0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904008&amp;v=MjAxMjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ckpMejdCZDdHNEg5ak1xNDlGYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="1 网络结构 ">1 网络结构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="1.1 &lt;b&gt;整体网络结构&lt;/b&gt;">1.1 <b>整体网络结构</b></a></li>
                                                <li><a href="#66" data-title="1.2 &lt;b&gt;基准网络结构&lt;/b&gt;">1.2 <b>基准网络结构</b></a></li>
                                                <li><a href="#70" data-title="1.3 &lt;b&gt;孪生网络&lt;/b&gt;">1.3 <b>孪生网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="2 目标函数 ">2 目标函数</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="2.1 &lt;b&gt;双向最大边界排序损失&lt;/b&gt;">2.1 <b>双向最大边界排序损失</b></a></li>
                                                <li><a href="#91" data-title="2.2 &lt;b&gt;目标函数&lt;/b&gt;">2.2 <b>目标函数</b></a></li>
                                                <li><a href="#103" data-title="2.3 &lt;b&gt;参数分析&lt;/b&gt;">2.3 <b>参数分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#107" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="3.1 &lt;b&gt;数据集&lt;/b&gt;">3.1 <b>数据集</b></a></li>
                                                <li><a href="#112" data-title="3.2 &lt;b&gt;评价准则和实验设置&lt;/b&gt;">3.2 <b>评价准则和实验设置</b></a></li>
                                                <li><a href="#125" data-title="3.3 &lt;b&gt;有效性验证&lt;/b&gt;">3.3 <b>有效性验证</b></a></li>
                                                <li><a href="#128" data-title="3.4 &lt;i&gt;Market&lt;/i&gt;-1501&lt;b&gt;数据集实验&lt;/b&gt;">3.4 <i>Market</i>-1501<b>数据集实验</b></a></li>
                                                <li><a href="#132" data-title="3.5 CUHK03&lt;b&gt;数据集实验&lt;/b&gt;">3.5 CUHK03<b>数据集实验</b></a></li>
                                                <li><a href="#135" data-title="3.6 DukeMTMC-reID&lt;b&gt;数据集实验&lt;/b&gt;">3.6 DukeMTMC-reID<b>数据集实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#139" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="图1 整体网络结构">图1 整体网络结构</a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;基准网络结构总结&lt;/b&gt;"><b>表</b>1 <b>基准网络结构总结</b></a></li>
                                                <li><a href="#72" data-title="图2 孪生网络与伪孪生网络结构示意图">图2 孪生网络与伪孪生网络结构示意图</a></li>
                                                <li><a href="#99" data-title="图3 不同方法的特征图对比">图3 不同方法的特征图对比</a></li>
                                                <li><a href="#102" data-title="图4 本文方法在Market-1501数据集的可视化效果">图4 本文方法在Market-1501数据集的可视化效果</a></li>
                                                <li><a href="#105" data-title="图5 各参数对实验的影响">图5 各参数对实验的影响</a></li>
                                                <li><a href="#117" data-title="图6 本文方法与三种基准网络的性能对比">图6 本文方法与三种基准网络的性能对比</a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;i&gt;Market&lt;/i&gt;-1501&lt;b&gt;数据集上的性能对比&lt;/b&gt; %"><b>表</b>2 <i>Market</i>-1501<b>数据集上的性能对比</b> %</a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表&lt;/b&gt;3 CUHK03&lt;b&gt;数据集上的性能对比&lt;/b&gt; %"><b>表</b>3 CUHK03<b>数据集上的性能对比</b> %</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表&lt;/b&gt;4 DukeMTMC-reID&lt;b&gt;数据集上的性能对比&lt;/b&gt; %"><b>表</b>4 DukeMTMC-reID<b>数据集上的性能对比</b> %</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="182">


                                    <a id="bibliography_1" title="ZHENG L, SHEN L, TIAN L, et al.Scalable person re-identification:a benchmark[C]//ICCV 2015:Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1116-1124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable person re-identification A benchmark">
                                        <b>[1]</b>
                                        ZHENG L, SHEN L, TIAN L, et al.Scalable person re-identification:a benchmark[C]//ICCV 2015:Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1116-1124.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_2" title="MATSUKAWA T, OKABE T, SUZUKI E, et al.Hierarchical Gaussian descriptor for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:1363-1372." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Gaussian descriptor for person re-identification">
                                        <b>[2]</b>
                                        MATSUKAWA T, OKABE T, SUZUKI E, et al.Hierarchical Gaussian descriptor for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:1363-1372.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_3" title="LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//CVPR2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:2197-2206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by Local Maximal Occurrence representation and metric learning">
                                        <b>[3]</b>
                                        LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//CVPR2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:2197-2206.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_4" title="KOESTINGER M, HIRZER M, WOHLHART P, et al.Large scale metric learning from equivalence constraints[C]//CVPR 2012:Proceedings of the 2012 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2012:2288-2295." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large scale metric learning from equivalence constraints">
                                        <b>[4]</b>
                                        KOESTINGER M, HIRZER M, WOHLHART P, et al.Large scale metric learning from equivalence constraints[C]//CVPR 2012:Proceedings of the 2012 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2012:2288-2295.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_5" title="WEINBERGER K Q, SAUL L K.Distance metric learning for large margin nearest neighbor classification[J].Journal of Machine Learning Research, 2009, 10 (2) :207-244." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distance metric learning for large margin nearest neighbor classification">
                                        <b>[5]</b>
                                        WEINBERGER K Q, SAUL L K.Distance metric learning for large margin nearest neighbor classification[J].Journal of Machine Learning Research, 2009, 10 (2) :207-244.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_6" title="ZHENG L, YANG Y, HAUPTMANN A G.Person re-identification:past, present and future[EB/OL].[2018-05-10].https://arxiv.org/pdf/1610.02984." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification:past,present and future">
                                        <b>[6]</b>
                                        ZHENG L, YANG Y, HAUPTMANN A G.Person re-identification:past, present and future[EB/OL].[2018-05-10].https://arxiv.org/pdf/1610.02984.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_7" title="VARIOR R R, HALOI M, WANG G.Gated siamese convolutional neural network architecture for human re-identification[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:791-808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gated siamese convolutional neural network architecture for human re-identification">
                                        <b>[7]</b>
                                        VARIOR R R, HALOI M, WANG G.Gated siamese convolutional neural network architecture for human re-identification[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:791-808.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_8" title="陈首兵, 王洪元, 金翠, 等.基于孪生网络和重排序的行人重识别[J].计算机应用, 2018, 38 (11) :3161-3166. (CHEN SB, WANG H Y, JIN C, et al.Person re-identification based on siamese network and reranking[J].Journal of Computer Applications, 2018, 38 (11) :3161-3166.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811021&amp;v=MjM1NDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ckpMejdCZDdHNEg5bk5ybzlIWllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        陈首兵, 王洪元, 金翠, 等.基于孪生网络和重排序的行人重识别[J].计算机应用, 2018, 38 (11) :3161-3166. (CHEN SB, WANG H Y, JIN C, et al.Person re-identification based on siamese network and reranking[J].Journal of Computer Applications, 2018, 38 (11) :3161-3166.) 
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_9" title="ZHENG Z, ZHENG L, YANG Y.A discriminatively learned CNNembedding for person reidentification[J].ACM Transactions on Multimedia Computing, Communications, and Applications, 2017, 14 (1) :13." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM68E09C1E98C844456DEFD79E7048C559&amp;v=MTg1MjJvTFBIanIyUlUxZmJyblFMK1dDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6TDI3eGFrPU5pZklZN1d3YTlIRjNJNHdiZU44QkhnOXl4TVZuaw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        ZHENG Z, ZHENG L, YANG Y.A discriminatively learned CNNembedding for person reidentification[J].ACM Transactions on Multimedia Computing, Communications, and Applications, 2017, 14 (1) :13.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_10" title="ZHENG Z, ZHENG L, YANG Y.Unlabeled samples generated by GAN improve the person re-identification baseline in vitro[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:3774-3782." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro">
                                        <b>[10]</b>
                                        ZHENG Z, ZHENG L, YANG Y.Unlabeled samples generated by GAN improve the person re-identification baseline in vitro[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:3774-3782.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_11" title="WANG F, ZUO W, LIN L, et al.Joint learning of single-image and cross-image representations for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:1288-1296." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Learning of Single-Image and Cross-Image Representations for Person Re-identification">
                                        <b>[11]</b>
                                        WANG F, ZUO W, LIN L, et al.Joint learning of single-image and cross-image representations for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:1288-1296.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_12" title="SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-05-10].https://arxiv.org/pdf/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[12]</b>
                                        SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-05-10].https://arxiv.org/pdf/1409.1556.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_13" title="SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[13]</b>
                                        SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:1-9.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_14" title="HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[14]</b>
                                        HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_15" title="ZHENG Z, ZHENG L, GARRETT M, et al.Dual-path convolutional image-text embedding with instance loss[EB/OL].[2018-05-10].https://arxiv.org/pdf/1711.05535." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dual-path convolutional image-text embedding with instance loss">
                                        <b>[15]</b>
                                        ZHENG Z, ZHENG L, GARRETT M, et al.Dual-path convolutional image-text embedding with instance loss[EB/OL].[2018-05-10].https://arxiv.org/pdf/1711.05535.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_16" title="van der MAATEN L.Accelerating t-SNE using tree-based algorithms[J].The Journal of Machine Learning Research, 2014, 15 (1) :3221-3245." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating t-SNE using tree-based algorithms">
                                        <b>[16]</b>
                                        van der MAATEN L.Accelerating t-SNE using tree-based algorithms[J].The Journal of Machine Learning Research, 2014, 15 (1) :3221-3245.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_17" title="LI W, ZHAO R, XIAO T, et al.DeepReid:deep filter pairing neural network for person re-identification[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:152-159." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepReID:deep filter pairing neural network for person re-identification">
                                        <b>[17]</b>
                                        LI W, ZHAO R, XIAO T, et al.DeepReid:deep filter pairing neural network for person re-identification[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:152-159.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_18" title="RISTANI E, SOLERA F, ZOU R, et al.Performance measures and a data set for multi-target, multi-camera tracking[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:17-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance measures and a data set for multi-target,multi-camera tracking">
                                        <b>[18]</b>
                                        RISTANI E, SOLERA F, ZOU R, et al.Performance measures and a data set for multi-target, multi-camera tracking[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:17-35.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_19" title="ZHANG L, XIANG T, GONG S.Learning a discriminative null space for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:1239-1248." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Null Space for Person Re-identification">
                                        <b>[19]</b>
                                        ZHANG L, XIANG T, GONG S.Learning a discriminative null space for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:1239-1248.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_20" title="USTINOVA E, GANIN Y, LEMPITSKY V.Multi-region bilinear convolutional neural networks for person re-identification[C]//AVSS 2017:Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance.Washington, DC:IEEE Computer Society, 2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-Region bilinear convolutional neural networks for person re-identification">
                                        <b>[20]</b>
                                        USTINOVA E, GANIN Y, LEMPITSKY V.Multi-region bilinear convolutional neural networks for person re-identification[C]//AVSS 2017:Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance.Washington, DC:IEEE Computer Society, 2017:1-6.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_21" title="VARIOR R R, SHUAI B, LU J, et al.A siamese long short-term memory architecture for human re-identification[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:135-153." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A siamese long short-term memory architecture for human re-identification&amp;quot;">
                                        <b>[21]</b>
                                        VARIOR R R, SHUAI B, LU J, et al.A siamese long short-term memory architecture for human re-identification[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:135-153.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_22" title="BARBOSA I B, CRISTANI M, CAPUTO B, et al.Looking beyond appearances:Synthetic training data for deep cnns in re-identification[J].Computer Vision and Image Understanding, 2018, 167:50-62." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE3E2EA8B65341A4DDFB5F68EA6F7D4D9&amp;v=MDc3MjFMMjd4YWs9TmlmT2ZjYTdhOU81M29jM1l1NE1DSDFJeTJKbm5FMTRQbm5xMldNekQ3WGdRYzZXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        BARBOSA I B, CRISTANI M, CAPUTO B, et al.Looking beyond appearances:Synthetic training data for deep cnns in re-identification[J].Computer Vision and Image Understanding, 2018, 167:50-62.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_23" title="WANG Y, CHEN Z, WU F, et al.Person re-identification with cascaded pairwise convolutions[C]//CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2018:1470-1478." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person Re-identification with Cascaded Pairwise Convolutions">
                                        <b>[23]</b>
                                        WANG Y, CHEN Z, WU F, et al.Person re-identification with cascaded pairwise convolutions[C]//CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2018:1470-1478.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_24" title="LV J, CHEN W, LI Q, et al.Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns[C]//CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2018:7948-7956." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns">
                                        <b>[24]</b>
                                        LV J, CHEN W, LI Q, et al.Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns[C]//CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2018:7948-7956.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-11-12 13:15</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),977-983 DOI:10.11772/j.issn.1001-9081.2018091889            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于孪生网络和双向最大边界排序损失的行人再识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A5%81%E5%AD%90%E6%A2%81&amp;code=41475748&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">祁子梁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B2%E5%AF%92%E5%86%B0&amp;code=32773597&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曲寒冰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E4%BC%A0%E8%99%8E&amp;code=41475749&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵传虎</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%91%A3%E8%89%AF&amp;code=39044133&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">董良</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%8D%9A%E6%98%AD&amp;code=41475750&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李博昭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%95%BF%E7%94%9F&amp;code=41475751&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王长生</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E5%8C%97%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0149979&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河北工业大学人工智能与数据科学学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E5%B8%82%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E9%99%A2%E5%8C%97%E4%BA%AC%E5%B8%82%E6%96%B0%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0202387&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京市科学技术研究院北京市新技术应用研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对在实际场景中存在的不同行人图像之间比相同行人图像之间更相似所造成的行人再识别准确率较低的问题, 提出一种基于孪生网络并结合识别损失和双向最大边界排序损失的行人再识别方法。首先, 对在超大数据集上预训练过的神经网络模型进行结构改造, 主要是对最后的全连接层进行改造, 使模型可以在行人再识别数据集上进行识别判断;其次, 联合识别损失和排序损失监督网络在训练集上的训练, 并通过正样本对的相似度值减去负样本对的相似度值大于预定阈值这一判定条件, 来使得负例图像对之间的距离大于正例图像对之间的距离;最后, 使用训练好的神经网络模型在测试集上测试, 提取特征并比对特征之间的余弦相似度。在公开数据集Market-1501、CUHK03和DukeMTMC-reID上进行的实验结果表明, 所提方法分别取得了89.4%、86.7%、77.2%的rank-1识别率, 高于其他典型的行人再识别方法, 并且该方法在基准网络结构下最高达到了10.04%的rank-1识别率提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人再识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孪生网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E5%90%91%E6%9C%80%E5%A4%A7%E8%BE%B9%E7%95%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双向最大边界;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8E%92%E5%BA%8F%E6%8D%9F%E5%A4%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">排序损失;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    祁子梁 (1993—) , 男, 河北邯郸人, 硕士研究生, 主要研究方向:计算机视觉、行人再识别;;
                                </span>
                                <span>
                                    *曲寒冰 (1977—) , 男, 黑龙江哈尔滨人, 副研究员, 博士, CCF会员, 主要研究方向:机器学习、计算机视觉、生物识别、图像处理;电子邮箱quhanbing@gmail.com;
                                </span>
                                <span>
                                    赵传虎 (1993—) , 男, 河南平顶山人, 硕士, 主要研究方向:机器学习、数据挖掘;;
                                </span>
                                <span>
                                    董良 (1990—) , 男, 河北邢台人, 硕士, 主要研究方向:数据挖掘、知识发现、机器学习、时空模式、社会网络;;
                                </span>
                                <span>
                                    李博昭 (1993—) , 女, 河北邢台人, 硕士, 主要研究方向:机器学习、图像处理、模式识别;;
                                </span>
                                <span>
                                    王长生 (1989—) , 男, 山东潍坊人, 硕士研究生, 主要研究方向:数据挖掘、机器学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2018YFC08097000, 2018YFC0704800, 2018YFF0301000);</span>
                                <span>国家自然科学基金资助项目 (91746207);</span>
                                <span>北京市科学技术研究院萌芽计划项目 (GS201817);</span>
                    </p>
            </div>
                    <h1><b>Person re-identification based on Siamese network and bidirectional max margin ranking loss</b></h1>
                    <h2>
                    <span>QI Ziliang</span>
                    <span>QU Hanbing</span>
                    <span>ZHAO Chuanhu</span>
                    <span>DONG Liang</span>
                    <span>LI Bozhao</span>
                    <span>WANG changsheng</span>
            </h2>
                    <h2>
                    <span>School of Artificial Intelligence, Hebei University of Technology</span>
                    <span>Beijing Institute of New Technology Applications, Beijing Academy of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Focusing on the low accuracy of person re-identification caused by that the similarity between different pedestrians' images is more than that between the same pedestrians' images in reality, a person re-identification method based on Siamese network combined with identification loss and bidirectional max margin ranking loss was proposed. Firstly, a neural network model which was pre-trained on a huge dataset, especially its final full-connected layer was structurally modified so that it can output correct results on the person re-identification dataset. Secondly, training of the network on the training set was supervised by the combination of identification loss and ranking loss. And according to that the difference between the similarity of the positive and negative sample pairs is greater than the predetermined value, the distance between negative sample pair was made to be larger than that of positive sample pair. Finally, a trained neural network model was used to test on the test set, extracting features and comparing the cosine similarity between the features. Experimental result on the open datasets Market-1501, CUHK03 and DukeMTMC-reID show that rank-1 recognition rates of the proposed method reach 89.4%, 86.7%, and 77.2% respectively, which are higher than those of other classical methods. Moreover, the proposed method can achieve a rank-1 rate improvement of up to 10.04% under baseline network structure.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=person%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">person re-identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Siamese%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Siamese network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=bidirectional%20max%20margin&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">bidirectional max margin;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ranking%20loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ranking loss;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    QI Ziliang, born in 1993, M. S. candidate. His research interests include computer vision, person re-identification.;
                                </span>
                                <span>
                                    QU Hanbing, born in 1977, Ph. D. , associate research fellow. His research interests include machine learning, computer vision, biological recognition, image processing.;
                                </span>
                                <span>
                                    ZHAO Chuanhu, born in 1993, M. S. His research interests include machine learning, data mining.;
                                </span>
                                <span>
                                    DONG Liang, born in 1990, M. S. His research interests include data mining, knowledge discovery, machine learning, space-time patterns, social network.;
                                </span>
                                <span>
                                    LI Bozhao, born in 1993, M. S. Her research interests include machine learning, image processing, pattern recognition.;
                                </span>
                                <span>
                                    WANG Changsheng, born in 1989, M. S. candidate. His research interests include data mining, machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-10</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Key R&amp;D Program of China (2018YFC08097000, 2018YFC0704800, 2018YFF0301000);</span>
                                <span>the National Natural Science Foundation of China (91746207);</span>
                                <span>the Beijing Academy of Science and Technology Budding Plan (GS201817);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="52">行人再识别的目的是识别出跨摄像头、跨场景下的行人是否为同一个人, 可以帮助进行进一步的查询跟踪, 其应用领域广泛, 如视频监控、城市监管、刑事安防等<citation id="230" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。近年来, 行人再识别技术引起了人们的广泛关注, 虽然取得了大量的研究成果, 但是行人再识别的研究依然存在着诸多挑战。例如:1) 由于摄像机一般架设在较高位置, 距离行人目标较远, 所采集的图像内行人细节特征不明显, 导致再识别精度不高;2) 行人处于非合作状态, 造成拍摄视角更加多样;3) 即使视角相同, 但由于服装或姿势的变化, 导致不同行人的图像可能比相同行人的图像更加相似。</p>
                </div>
                <div class="p1">
                    <p id="53">行人再识别的研究方法大体可以分为无监督学习方法和有监督学习方法两种, 当前大部分行人再识别技术采用的是有监督学习方法。基于有监督学习的行人再识别方法可以概括为三大类:基于特征提取的方法、基于度量学习的方法和基于深度学习的方法。其中, 早期的行人再识别方法主要为特征提取和度量学习两种, 并且这些早期方法只关注其中的某一种, 没有把这两个过程进行很好的结合, 而深度学习则提供了较好的端到端解决方法。</p>
                </div>
                <div class="p1">
                    <p id="54">特征提取通过提取具有分辨力和鲁棒性的行人特征来解决行人再识别问题, 这里的特征是指通过研究者对研究对象观察研究后, 人工选择并提取的特征, 常用的特征如文献<citation id="231" type="reference">[<a class="sup">2</a>,<a class="sup">3</a>]</citation>中采用的颜色特征、纹理特征, 以及多种特征的组合。但是通过人工提取的特征大部分只能适应特定情况下拍摄的图像, 对于未考虑到的情况无法很好地适用, 并且设计特征需要较高的经验水平。当前, 随着行人再识别研究的进行, 人工特征研究对识别率的提升变得越来越小。</p>
                </div>
                <div class="p1">
                    <p id="55">鉴于在特征研究中存在的问题和困难, 度量学习的方法被应用于行人再识别问题, 例如, XQDA (Cross-view Quadratic Discriminant Analysis) 算法<citation id="232" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、KISSME (Keep It Simple and Straightforward MEtric) 算法<citation id="233" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、最大边界近邻 (Large Margin Nearest Neighbor, LMNN) 算法<citation id="234" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。该类方法的主要思想是通过学习一个映射矩阵, 将特征从原始特征空间映射到另一个具有高区分度的特征空间, 使得在原始特征空间难区分甚至不可分的特征变得容易区分。这种方法在一定程度上降低了对特征的要求, 提高了识别算法的鲁棒性, 但是这些特征提取和度量学习相互独立的处理方法还是不能达到令人满意的效果, 在拍摄条件和环境差异较大时无法取得良好的效果。</p>
                </div>
                <div class="p1">
                    <p id="56">随着计算机硬件的发展、计算能力的不断提升, 以及大规模数据集的出现, 深度学习开始应用于包括计算机视觉领域在内的各个领域, 并取得了优异表现<citation id="235" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。自从在2012年的ImageNet竞赛中获胜, 深度学习吸引了许多的研究者。随着其发展, LeNet5、AlexNet、VGGNet、GoogLeNet、ResNet等优秀的卷积神经网络 (Convolutional Neural Network, CNN) 模型不断被提出, 网络的结构越来越深, 网络性能也不断提升。</p>
                </div>
                <div class="p1">
                    <p id="57">由于深度学习在计算机视觉领域展现出了优越的性能, 以及非深度学习方法的局限性, 深度学习方法开始被用于行人再识别课题的研究。其中一些深度学习的研究方法可以看作从分类的角度来解决行人再识别的问题, 如文献<citation id="236" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>]</citation>方法, 这些方法通过softmax函数连接交叉熵损失来判断某个行人的ID, 依此来确定某一幅图像是否属于某一个人;还有一些方法从排序的角度来考虑这个问题, 如文献<citation id="237" type="reference">[<a class="sup">9</a>,<a class="sup">10</a>]</citation>, 这类方法的出发点就是类内距离应该比类间距离更近, 与被检索图像属于同一类别的行人图像排在前面。</p>
                </div>
                <div class="p1">
                    <p id="58">针对不同行人图像之间比相同行人图像之间更相似所造成的行人再识别准确率较低的问题, 本文从排序角度进行分析, 利用深度学习的方法来解决行人再识别问题。本文方法是一种有监督学习方法, 实质上就是通过更有效的损失函数来对具有较深结构的网络进行监督训练, 从而得到更好的网络权重, 之后通过提取更具分辨力和鲁棒性的深度特征来解决行人再识别问题。</p>
                </div>
                <div class="p1">
                    <p id="59">本文网络结构与文献<citation id="238" type="reference">[<a class="sup">11</a>]</citation>中的网络结构相似, 但其网络结构为一个较浅的8层的CNN结构, 且未经过预训练, 而本文采用了网络结构更复杂、效果更好的预训练过的CNN结构来处理图像对, 这使得所提取的特征可以对图像进行更好的表达。虽然本文方法在操作上与文献<citation id="239" type="reference">[<a class="sup">9</a>]</citation>比较相似, 但其仅仅使用了平方损失, 而本文使用了更好的损失函数 (排序损失) 来监督网络的学习, 其优势在于可以使得正样本之间的距离小于负样本之间的距离。同时, 与文献<citation id="240" type="reference">[<a class="sup">9</a>]</citation>不同之处还在于本文方法中正负样本的比例是固定的, 这样可以降低正负样本的数量不平衡所造成的影响, 从而得到较高识别率。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">1 网络结构</h3>
                <h4 class="anchor-tag" id="61" name="61">1.1 <b>整体网络结构</b></h4>
                <div class="p1">
                    <p id="62">本文的整体网络结构是在孪生网络 (Siamese网络) 结构的基础上进行的改进, 所提出的网络主要包括两个判别力很强的CNN模型, 并且融合了softmax损失和双向最大边界排序损失 (本文简称为排序损失) , 网络结构如图1所示。图1中输入图像序列是一个四元图像组, 并且这些输入图像需要调整大小来适应网络输入尺寸, CNN模型可以是任何一个基准网络结构, 或者是一个重新设计定义的网络结构。为了得到更好的结果和提高训练效率, 并对所提出的联合损失的有效性进行验证, 本文采用已经在ImageNet数据集上预训练过的VGGNet-16、GoogLeNet、ResNet-50基准网络作为CNN模型进行实验。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904008_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 整体网络结构" src="Detail/GetImg?filename=images/JSJY201904008_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 整体网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904008_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Overall network structure</p>

                </div>
                <div class="p1">
                    <p id="64">以ResNet-50为例, 首先, 为了使本文模型能够在行人数据集上进行训练和预测, 需要去掉原始ResNet-50中与最后的池化层相连的全连接 (Fully Connected, FC) 层和结果预测层;然后, 在ResNet-50后添加用于防止过拟合的Dropout层和用于预测的1×1×2 048×<i>N</i>维的全连接层, 其中2 048是特征维数, <i>N</i>是数据集的实例个数;最后, 通过连接softmax损失层, 得到识别损失, 并且使用该网络最后池化层的输出作为图像特征来对输入图像进行表示, 图像特征会用于测试阶段。</p>
                </div>
                <div class="p1">
                    <p id="65">本文所提出的网络结构不仅利用到了结构和参数非常优秀的预训练模型, 同时也可以通过孪生网络结构很方便地将softmax损失和排序损失结合在一起。下面分别对构建整体网络结构所用到的技术进行介绍。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.2 <b>基准网络结构</b></h4>
                <div class="p1">
                    <p id="67">本文中孪生网络的两个分支结构采用在ImageNet竞赛的大规模数据集上预训练过的网络模型作为基准网络结构 (baseline network) , 该数据集有10万左右的图片, 包括各种类别的对象1 000类。使用预训练过的模型有以下优点:1) 由于预训练数据集包含对象类别较多, 图片数量很大, 因此使用预训练模型的初始权重相对人为设置更加合理, 在训练时有利于快速找到最优解;2) 由于参数量巨大, 重新对复杂网络模型进行训练需要很多的计算资源和时间, 不必要的重复训练会造成很大的浪费;3) 这些预训练模型的性能已经通过大量研究者的实验验证, 性能有所保障。表1对当前常用的经典基准网络结构的特点进行简单总结。</p>
                </div>
                <div class="area_img" id="68">
                    <p class="img_tit"><b>表</b>1 <b>基准网络结构总结</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Summary of baseline network structure</p>
                    <p class="img_note"></p>
                    <table id="68" border="1"><tr><td>网络名称</td><td>出现年份</td><td>层数</td><td>数据增强</td><td>Inception</td><td>卷积核</td><td>Dropout</td><td>批归一化</td><td>特征维数</td></tr><tr><td><br />AlexNet</td><td>2 012</td><td>8</td><td>是</td><td>否</td><td>11, 5, 3</td><td>是</td><td>否</td><td>4 096</td></tr><tr><td><br />VGGNet-16</td><td>2 014</td><td>16</td><td>是</td><td>否</td><td>3</td><td>是</td><td>否</td><td>4 096</td></tr><tr><td><br />GoogLeNet</td><td>2 014</td><td>22</td><td>是</td><td>是</td><td>7, 1, 3, 5</td><td>是</td><td>否</td><td>1 024</td></tr><tr><td><br />ResNet-50</td><td>2 015</td><td>50</td><td>是</td><td>否</td><td>7, 1, 3, 5</td><td>是</td><td>是</td><td>2 048</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="69">表1中的层数计算的是网络中包含可变参数的层的数量, 比如卷积层和全连接层, 那些参数固定或不包含可变参数的层未计算进去, 其中参数可变与否是针对训练阶段而言的。另外, 尽管表1中的网络结构都有相当多不同的版本和各种结构上的改进, 但考虑到训练的效率和实验验证效果, 本文只采用了表格中所展示的基准结构进行实验, 其中包括VGGNet-16<citation id="241" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、GoogLeNet<citation id="242" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和ResNet-50<citation id="243" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">1.3 <b>孪生网络</b></h4>
                <div class="p1">
                    <p id="71">孪生网络通常用来度量两个输入样本之间的相似性 (两个输入样本为同类, 即输入都是图像或者都是文本) , 判断输入样本是否为相同标签。同时, 对应孪生网络的还有伪孪生网络 (pseudo-Siamese network, 两种输入样本可以是不同类型的, 比如一个是图像另一个是文本) , 用来判别输入样本是否匹配。两类网络的基本结构如图2所示。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904008_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 孪生网络与伪孪生网络结构示意图" src="Detail/GetImg?filename=images/JSJY201904008_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 孪生网络与伪孪生网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904008_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Schematic diagram of Siamese network and pseudo-Siamese network</p>

                </div>
                <div class="p1">
                    <p id="73">图2中孪生网络的两个分支使用相同的结构并且权重<i>W</i>共享, 输入无论从类型还是数量上也都是相同的, 而伪孪生网络则可以使用不同的结构 (比如一个为VGGNet, 另一个为GoogLeNet) , 且权重不共享, 输入也不相同。对于本文来讲更加适合使用Siamese网络结构的依据如下:1) 行人再识别问题中输入的都是行人图像, 即输入的是相同类型的数据;2) 在训练过程中利用到两幅图像是否为相同行人这一已知信息;3) 充分利用每幅图像自身所包含的各种信息 (颜色和形状等) 。Siamese网络通过把输入图像映射到特征空间中, 从而在特征空间中用度量函数对比图像特征的相似度。通常度量函数可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="74"><i>D</i><sub><i>W</i></sub> (<b><i>X</i></b><sub>1</sub>, <b><i>X</i></b><sub>2</sub>) =‖<i>F</i><sub><i>W</i></sub> (<b><i>X</i></b><sub>1</sub>) -<i>F</i><sub><i>W</i></sub> (<b><i>X</i></b><sub>2</sub>) ‖      (1) </p>
                </div>
                <div class="p1">
                    <p id="75">其中:<b><i>X</i></b><sub>1</sub>、<b><i>X</i></b><sub>2</sub>代表输入向量;<i>F</i><sub><i>W</i></sub> (·) 为映射函数;<i>W</i>表示映射函数中的权重。通过训练找到合适的<i>W</i>, 使得当<b><i>X</i></b><sub>1</sub>和<b><i>X</i></b><sub>2</sub>属于相同类别时<i>D</i><sub><i>W</i></sub>较小, 当类别不同时<i>D</i><sub><i>W</i></sub>较大。可以看到, 当输入为相同类别图像对时, 即输入图像属于相同行人, 则只需要在训练时最小化<i>D</i><sub><i>W</i></sub>即可。<i>D</i><sub><i>W</i></sub>可以是欧氏距离、余弦距离或其他度量方法。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">2 目标函数</h3>
                <h4 class="anchor-tag" id="77" name="77">2.1 <b>双向最大边界排序损失</b></h4>
                <div class="p1">
                    <p id="78">通常用于两个特征向量之间距离度量的是欧氏距离, 但对于行人再识别问题来说, 仅仅使用欧氏距离无法区分那些外观非常相似的不同行人的图像。因此, 本文使用余弦相似度来度量特征向量的相似度, 即通过两个向量之间的夹角来判断图像是否属于同一行人。余弦相似度可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>cos</mi></mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mspace width="0.25em" /><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>x</mi></msub></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mo>×</mo><mfrac><mrow><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>y</mi></msub></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中:‖*‖<sub>2</sub>代表<i>L</i><sub>2</sub>规范化; <b><i>f</i></b>代表整幅图像的特征向量。</p>
                </div>
                <div class="p1">
                    <p id="81">排序损失函数经常在图像检索和图像文本匹配的研究中作为目标函数, 为了更方便和有效率地把它应用到行人再识别的研究当中, 本文参考了文献<citation id="244" type="reference">[<a class="sup">9</a>,<a class="sup">15</a>]</citation>中的一些实现方法。假设有一个四元组输入: (<b><i>X</i></b><sub>+</sub>, <b><i>Y</i></b><sub>+</sub>, <b><i>X</i></b><sub>-</sub>, <b><i>Y</i></b><sub>-</sub>) , 其中<b><i>X</i></b><sub>+</sub>和<b><i>Y</i></b><sub>+</sub>代表相同图像对, 它们具有相同的类别标签;<b><i>X</i></b><sub>-</sub>、<b><i>Y</i></b><sub>-</sub>分别和<b><i>X</i></b><sub>+</sub>、<b><i>Y</i></b><sub>+</sub>有不相同的类别标签。由此可以很明显地得到:</p>
                </div>
                <div class="p1">
                    <p id="82"><i>Dis</i> (<b><i>X</i></b><sub>+</sub>, <b><i>Y</i></b><sub>+</sub>) &lt; <i>Dis</i> (<b><i>X</i></b><sub>+</sub>, <b><i>Y</i></b><sub>-</sub>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="83"><i>Dis</i> (<b><i>Y</i></b><sub>+</sub>, <b><i>X</i></b><sub>+</sub>) &lt; <i>Dis</i> (<b><i>Y</i></b><sub>+</sub>, <b><i>X</i></b><sub>-</sub>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="84">其中<i>Dis</i> (*, *) 代表的是两个输入之间的距离。式 (3) ～ (4) 为相同行人图像之间的距离小于不同行人图像, 即相同行人图像之间的相似度大于不同行人图像之间的相似度。</p>
                </div>
                <div class="p1">
                    <p id="85">本文所使用的双向最大边界排序损失函数的公式为:</p>
                </div>
                <div class="area_img" id="181">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904008_18100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="88">其中:<i>m</i>是最大损失边界, 它的大小影响训练中收敛的速度和效率;<i>D</i> (*, *) 为式 (2) 所示的余弦相似度分数, 图像的相似度越大则这个分数会越高。由式 (3) ～ (4) 可知正样本对之间的相似度大于负样本对之间的相似度, 排序损失函数的作用就是在训练中约束这种关系。因此在式 (5) 中, 当正样本对的相似性分数减去负样本对的相似性分数大于<i>m</i>时, 即符合判定条件, 损失为0。</p>
                </div>
                <div class="p1">
                    <p id="89">本文截取了ResNet-50网络中某一非线性层 (res4fx) 的结果来可视化输出其特征图, 分别对使用了双向最大边界损失的本文方法, 文献<citation id="245" type="reference">[<a class="sup">9</a>]</citation>方法 (使用了欧氏距离) 和基准网络ResNet-50 (仅使用了识别损失) 的效果进行展示, 其中原始图像来自Market-1501数据集, 结果如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="90">由图3可看出:1) 相对于文献<citation id="246" type="reference">[<a class="sup">9</a>]</citation>方法, 本文方法训练出的网络对于图像中行人信息的表达更加准确, 对于一些背景的处理效果更好;2) 相对于基准网络ResNet-50, 本文方法对于行人和背景之间的差异性表现更好, 即图像中行人的信息得到了更高的响应值 (对应特征图中采用深色进行表示) 。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">2.2 <b>目标函数</b></h4>
                <div class="p1">
                    <p id="92">受文献<citation id="247" type="reference">[<a class="sup">9</a>]</citation>的启发, 本文最后的目标函数同时利用识别损失和排序损失。通过两个较深的基准网络结构提取两幅图像的特征, 然后衔接<i>FC</i>层和<i>softmax</i>损失层产生每一类的概率和识别损失, 其中<i>softmax</i>损失定义为:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>o</mtext><mtext>f</mtext><mtext>t</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">其中: <i>N</i>代表样本的个数; <i>C</i>表示样本类别的种类数; <b><i>p</i></b><sub><i>i</i></sub>表示softmax函数输出的<i>C</i>维的表示概率的列向量; <i>y</i><sub><i>i</i></sub>表示某个样本; <b><i>p</i></b><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>代表该样本在真实标签位置<i>i</i>上的值;分母是对该样本在所有标签位置上的值进行求和。</p>
                </div>
                <div class="p1">
                    <p id="96">softmax损失可以充分利用图像类别信息, 但是只使用该损失函数无法有效地对那些相似的不同类别图像进行训练。因此, 为了学习到一个判别力好的特征, 并且在特征空间中具有相同类标签的特征向量之间比具有不同类标签的特征向量距离更近, 本文把两种损失函数结合, 得到最终的目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="97"><i>L</i><sub>object</sub>=<i>L</i><sub>softmax1</sub>+<i>L</i><sub>softmax2</sub>+<i>μL</i><sub>rank</sub>      (7) </p>
                </div>
                <div class="p1">
                    <p id="98">其中:<i>L</i><sub>object</sub>代表整个网络计算得到的损失;<i>L</i><sub>softmax1</sub>和<i>L</i><sub>softmax2</sub>为网络中两个分支的softmax损失;<i>L</i><sub>rank</sub>为双向最大边界排序损失; <i>μ</i>是用来权衡两种损失之间的影响, 当<i>μ</i>=0时, 只有softmax损失函数产生作用, 并且本文在实验验证中得到 <i>μ</i>=0.5可以取得较好的实验结果。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904008_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同方法的特征图对比" src="Detail/GetImg?filename=images/JSJY201904008_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同方法的特征图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904008_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Feature map comparison among different methods</p>

                </div>
                <div class="p1">
                    <p id="100">通过式 (7) 对两种损失的结合, 不仅充分利用了图像的类别信息, 也使得相同类的特征向量之间比不同类的特征向量之间距离更近, 从而降低了由于不同行人的图像非常相似导致误判的可能性。并且相对于只使用一种损失的方法, 本文方法充分利用了所有的已知信息去训练, 因此可以达到较好的效果。</p>
                </div>
                <div class="p1">
                    <p id="101">图4通过可视化展示了上述目标函数 (式 (7) ) 的处理效果。为了得到该可视化结果, 首先, 通过本文方法训练得到网络权重, 基准网络为ResNet-50;然后, 提取行人图像特征并通过主成分分析法 (Principal Component Analysis, PCA) 降维使得特征降为50维, 行人图像为来自Market-1501数据集的421人的7 000张图像;最后, 使用Barnes-Hut t-SNE (tree-based approaches for Stochastic Neighbor Embedding) 算法<citation id="248" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>使特征降到2维 (坐标位置) 并聚类。由图4容易看出, 相似的图像距离较近, 不同图像距离较远, 即本文方法在直观上是有效的。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904008_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 本文方法在Market-1501数据集的可视化效果" src="Detail/GetImg?filename=images/JSJY201904008_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 本文方法在Market-1501数据集的可视化效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904008_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Visualization of the proposed method on Market-1501 dataset</p>

                </div>
                <h4 class="anchor-tag" id="103" name="103">2.3 <b>参数分析</b></h4>
                <div class="p1">
                    <p id="104">由式 (7) 可知本文方法中较重要的参数有损失边界和损失权重, 为了得到更好的改进提升, 这两个参数需要通过实验得到。本节实验在<i>Market</i>-1501数据集上采用<i>ResNet</i>-50为基准进行, 为了实验效果和速度, 采用单检索 (<i>Single Query</i>) 的测试方式, 实验结果如图5所示。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904008_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 各参数对实验的影响" src="Detail/GetImg?filename=images/JSJY201904008_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 各参数对实验的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904008_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Effect of different parameters on experiment</i></p>

                </div>
                <div class="p1">
                    <p id="106">图5中横坐标分别为<i>m</i>和<i>μ</i>的值, 纵坐标为在不同参数下两种评价指标第一准确度 (rank-1) 和平均精度均值 (mean Average Precision, mAP) 的数值。rank-1是最重要的指标, 即检索序列结果排序在第一位 (相关程度第一) 的结果为正确结果的概率。通过实验结果可知, <i>m</i>和<i>μ</i>值均为0.5左右时可以取得最好的效果。通过对图5分析可得:1) 当<i>m</i>很小时排序损失作用较小, 无法取得较大的效果提升;2) 当<i>m</i>较大时容易导致过拟合现象的产生, 因此准确率也会下降;3) 当<i>μ</i>为0时, 结果几乎和只用softmax损失是一样的, 即<i>L</i><sub>object</sub>=<i>L</i><sub>softmax</sub>, 这和本文的预期相符;4) 当<i>μ</i>较大时, 由于识别损失无法充分发挥作用, 即对图像信息的利用不够充分, 因此也会造成准确率下降。</p>
                </div>
                <h3 id="107" name="107" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="108" name="108">3.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="109">清华大学的<i>Market</i>-1501数据集<citation id="249" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>拍摄场景为清华校园, 包含了1 501位行人的标记好的32 668幅图像, 这些行人图像是用6个不同的摄像机拍摄的。本文使用751位行人的12 936张图像进行训练, 750位行人的19 732张图像进行测试。该数据集中的边界框 (<i>bounding box</i>) 是用可变形部分模型 (<i>Deformable Part Model</i>, <i>DPM</i>) 获得的。</p>
                </div>
                <div class="p1">
                    <p id="110">香港中文大学的<i>CUHK</i>03数据集<citation id="250" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>包含1 467位行人的14 097幅图像, 这些图像拍摄于两个摄像头, 每个摄像头下每个行人平均有4.8幅图像。该数据集同时具有人工标记和<i>DPM</i>算法检测的边界框, 为了更接近实际应用场景, 本文使用<i>DPM</i>检测到的边界数据进行实验。</p>
                </div>
                <div class="p1">
                    <p id="111"><i>DukeMTMC</i>-<i>reID</i>数据集<citation id="251" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>是行人跟踪数据集<i>DukeMTMC</i>的一个子集。<i>DukeMTMC</i>数据集由杜克大学学者在文献<citation id="252" type="reference">[<a class="sup">18</a>]</citation>中提出, 包含8个不同视角的85 <i>min</i>高分辨率视频。<i>DukeMTMC</i>-<i>reID</i>数据集的图片截取自<i>DukeMTMC</i>的视频, 它的结构类似于<i>Market</i>-1501数据集, 包含702位行人, 其中16 522张为训练图片, 17 661张图像用于测试, 2 228张作为查询图像。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">3.2 <b>评价准则和实验设置</b></h4>
                <div class="p1">
                    <p id="113">为了验证本文方法的有效性, 本文采用3种基准网络 (<i>VGGNet</i>、<i>GoogLeNet</i>和<i>ResNet</i>-50) 和3个公开的行人再识别数据集 (<i>Market</i>-1501、<i>CUHK</i>03和<i>DukeMTMC</i>-<i>reID</i>) 进行实验。使用<i>Matconvnet</i>深度学习框架进行算法实现, 实验环境为<i>Ubuntu</i> 16.04, <i>Matlab</i> 2016<i>a</i>, <i>NVIDIA Tesla P</i>100显卡。</p>
                </div>
                <div class="p1">
                    <p id="114">本文使用随机梯度下降 (<i>Stochastic Gradient Descent</i>, <i>SGD</i>) 算法对网络进行训练, 训练中采用批训练的方式, 批尺寸 (<i>mini</i>-<i>batch size</i>) 设置为10, 训练的动量因子固定为0.9, 共训练40次, 前20次学习率为0.1, 其后10次为0.02, 最后10次为0.01。</p>
                </div>
                <div class="p1">
                    <p id="115">在训练阶段:首先, 数据集中的原始行人图像被重置为256×256像素的大小, 并且从该图像中裁剪出224×224大小的图像作为网络输入;然后, 图像数据输入网络之中, 经前向传播计算得到损失;最后, 进行损失反向传播, 并调整网络参数, 其中排序损失边界和损失权重为0.5。</p>
                </div>
                <div class="p1">
                    <p id="116">在测试阶段:由于本文网络中的两个分支权重共享, 因此只使用其中一个进行特征提取即可。首先, 对图像进行预处理;然后, 通过训练过的网络进行特征提取, 对应<i>VGGNet</i>-16、<i>GoogLeNet</i>和<i>ResNet</i>-50分别为4 096维、1 024维和2 048维的深度特征;最后, 计算对比所提特征的余弦距离得到余弦相似度获得最终结果。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904008_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 本文方法与三种基准网络的性能对比" src="Detail/GetImg?filename=images/JSJY201904008_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 本文方法与三种基准网络的性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904008_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Performance comparison between the proposed method and three baseline networks</i></p>

                </div>
                <div class="p1">
                    <p id="118">本文在实验中主要采用两种评价指标, 分别为累计匹配特征曲线 (<i>Cumulated Matching Characteristics curve</i>, <i>CMC</i>) 和平均精度均值 (<i>mAP</i>) 。累计匹配特征曲线从排序的角度对再识别问题进行评价, 即把检索结果按照相关程度进行排序。当然, 排序靠后的检索结果正确率也很重要, 尽管排序靠后的检索结果相关程度不如在第一位的结果, 但对于辅助人工识别具有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="119">平均精度均值则是把行人再识别当作图像检索问题来进行评价, 该指标可以从整体上对算法进行评价。其中, 精度均值 (<i>Average Precision</i>, <i>AP</i>) 计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo><mo>×</mo><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="121">其中:<i>r</i>是某幅图像在检索序列结果中的位置序号;<i>P</i> (<i>r</i>) 是检索序列结果中到当前位置<i>r</i>时正确结果所占的比例;<i>relevant</i> (<i>r</i>) 则代表当前位置的结果是否正确, 正确为1, 错误为0。通过查询多个图像后, 计算这些图像AP的均值得到mAP, 由此mAP计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mn>1</mn><mi>Q</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mi>A</mi></mstyle><mi>Ρ</mi><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">其中:<i>Q</i>是查询次数, 即查询的图像个数。</p>
                </div>
                <div class="p1">
                    <p id="124">为验证本文方法的有效性和准确性, 本文将和以下方法进行比较:KISSME+BoW (Bag of Words) 算法<citation id="253" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、LOMO (LOcal Maximal Occurrence) +XQDA算法<citation id="254" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Gated SCNN (Gated Siamese CNN) 算法<citation id="255" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、Siamese-reranking (Siamese network and reranking) 算法<citation id="256" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Verif.-Classif. (Verification and Classification models) 算法<citation id="257" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、GAN ResNet (Generative Adversarial Residual Networks) 算法<citation id="258" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、SI-CI (Single-Image and Cross-Image representations) 算法<citation id="259" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、DNS (Discriminative Null Space) 算法<citation id="260" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、Multiregion CNN (Multi-region bilinear CNN) 算法<citation id="261" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、S-LSTM (Siamese Long Short-Term Memory architecture) 算法<citation id="262" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、SOMAnet (SOMAset network) 算法<citation id="263" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、BraidNet-CS+SRL (cascaded pairwise convolutions+Sample Rate Learning) 算法<citation id="264" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、TFusion-sup (transfer learning of spatial-temporal patterns) 算法<citation id="265" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125">3.3 <b>有效性验证</b></h4>
                <div class="p1">
                    <p id="126">本文通过对行人再识别问题进行分析, 提出并采用了性能更好的损失函数, 因此在有效性实验部分本文分别采用<i>VGGNet</i>-16、<i>GoogLeNet</i>和<i>ResNet</i>-50作为基准网络, 通过在<i>Market</i>-1501数据集上比较本文方法和基准网络的<i>CMC</i>曲线来进行有效性验证, 实验结果如图6所示。</p>
                </div>
                <div class="p1">
                    <p id="127">由图6可知, 与基准网络的结果相比本文方法取得了较大的提升, 其中对<i>VGGNet</i>-16、<i>GoogLeNet</i>和<i>ResNet</i>-50三种基准网络在<i>Market</i>-1501数据集上<i>rank</i>-1的提升幅度分别为5.85%、8.94%和10.04%, 并且通过对比在三种基准网络上的提升幅度也可以得出, 当网络结构较深时本文方法可以达到更好的提升效果。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128">3.4 <i>Market</i>-1501<b>数据集实验</b></h4>
                <div class="p1">
                    <p id="129">本文在<i>Market</i>-1501数据集上采用了两种测试方法, 即单检索 (<i>Single</i>-<i>Query</i>) 和多检索 (<i>Multi</i>-<i>Query</i>) 。单检索时被检索的图像为单帧行人图像, 多检索时被检索的图像为多帧同一行人的图像的均值图像, 其中多检索可以利用到更多的行人图像信息, 但单检索更接近实际情况, 实验结果如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="130">从表2可知, 在<i>Market</i>-1501数据集上, 本文方法在使用<i>ResNet</i>-50作为基准网络时分别在单检索和多检索条件下取得了84.5%和89.4%的<i>rank</i>-1准确率, 67.4%和79.2%的<i>mAP</i>。从实验结果数据可得, 本文方法取得了最好的综合效果, 并且, 在多检索条件下采用多幅行人图像用于检索, 有利于提高再识别准确率。</p>
                </div>
                <div class="area_img" id="131">
                    <p class="img_tit"><b>表</b>2 <i>Market</i>-1501<b>数据集上的性能对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Performance comparison on Market</i>-1501 <i>dataset</i> %</p>
                    <p class="img_note"></p>
                    <table id="131" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="2"><br />单检索</td><td rowspan="2"></td><td colspan="2"><br />多检索</td></tr><tr><td><br /><i>rank</i>-1</td><td><i>mAP</i></td><td><br /><i>rank</i>-1</td><td><i>mAP</i></td></tr><tr><td><i>KISSME</i>+<i>BoW</i><sup>[1]</sup></td><td>44.4</td><td>20.8</td><td></td><td>—</td><td>—</td></tr><tr><td><br /><i>DNS</i><sup>[19]</sup></td><td>55.4</td><td>29.9</td><td></td><td>71.6</td><td>46.0</td></tr><tr><td><br /><i>Gated SCNN</i><sup>[7]</sup></td><td>65.9</td><td>39.6</td><td></td><td>76.0</td><td>48.5</td></tr><tr><td><br /><i>S</i>-<i>LSTM</i><sup>[21]</sup></td><td>—</td><td>—</td><td></td><td>61.6</td><td>35.3</td></tr><tr><td><br /><i>Verif</i>.-<i>Classif</i>.<sup>[9]</sup></td><td>79.5</td><td>59.9</td><td></td><td>85.5</td><td>70.3</td></tr><tr><td><br /><i>Multiregion CNN</i><sup>[20]</sup></td><td>45.6</td><td>26.1</td><td></td><td>56.6</td><td>32.3</td></tr><tr><td><br /><i>GAN ResNet</i><sup>[10]</sup></td><td>78.1</td><td>56.2</td><td></td><td>85.1</td><td>48.5</td></tr><tr><td><br /><i>SOMAnet</i><sup>[22]</sup></td><td>73.9</td><td>47.9</td><td></td><td>81.3</td><td>57.0</td></tr><tr><td><br /><i>TFusion</i>-<i>sup</i><sup>[24]</sup></td><td>73.1</td><td>—</td><td></td><td>—</td><td>—</td></tr><tr><td><br /><i>Siamese</i>-<i>reranking</i><sup>[8]</sup></td><td>83.4</td><td><b>68.8</b></td><td></td><td>88.3</td><td>78.3</td></tr><tr><td><br />BraidNet-CS+SRL<sup>[23]</sup></td><td>83.7</td><td>—</td><td></td><td>—</td><td>69.5</td></tr><tr><td><br />ResNet-50<sup>[14]</sup></td><td>73.7</td><td>51.5</td><td></td><td>81.5</td><td>64.0</td></tr><tr><td><br />本文方法 (GoogLeNet) </td><td>65.0</td><td>42.9</td><td></td><td>71.3</td><td>53.1</td></tr><tr><td><br />本文方法 (VGGNet-16) </td><td>71.1</td><td>45.9</td><td></td><td>77.8</td><td>57.0</td></tr><tr><td><br />本文方法 (ResNet-50) </td><td><b>84.5</b></td><td>67.4</td><td></td><td><b>89.4</b></td><td><b>79.2</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="132" name="132">3.5 CUHK03<b>数据集实验</b></h4>
                <div class="p1">
                    <p id="133">在CUHK03数据集中分别在单摄像采集 (single-shot) 和多摄像采集 (multi-shot) 的条件下进行实验。单摄像采集条件下每个被检索图像在搜索集中只有一幅对应的正确图像, 多摄像采集条件下则是使用来自其他摄像机的所有图像用于待检索, 对应正确的行人图像不只一幅。其中多摄像采集的情况非常接近图像检索, 并且可以在一定程度上降低随机采样对结果带来的影响, 实验结果如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="134">从表3可知, 本文方法在CUHK03数据集上使用ResNet-50作为基准网络, 在单摄像采集 (single-shot) 条件下取得了82.9%的rank-1准确率和89.2%的mAP;在多摄像采集 (multi-shot) 条件下取得了86.7%的rank-1准确率和77.8%的mAP。表3中, 在单摄像采集条件下, Siamese-reranking方法<citation id="266" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>取得了最好的rank-1结果, 原因是其在训练网络并提取特征后采用了重排序技术, 使得特征更相似的图像有更大的概率被排在前面, 处理过程较为复杂;然而, 本文仅仅通过改进网络和损失函数来提高深度特征对行人图像的表达能力, 直接提升网络本身的性能, 没有进行进一步的处理。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">3.6 DukeMTMC-reID<b>数据集实验</b></h4>
                <div class="p1">
                    <p id="136">DukeMTMC-reID数据集的数据在结构上和Market-1501相似, 由于该数据集提出较晚, 因此本文只列出了在单检索 (Single Query) 时本文方法与其他一些方法的对比。从表4可知, 在DukeMTMC-reID数据集上本文方法使用ResNet-50作为基准网络取得了77.2%的rank-1准确率和53.9%的mAP。表4中得到最好mAP的方法是BraidNet-CS+SRL算法<citation id="267" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 该方法比BraidNet-CS多了对负样本率进行自适应学习的步骤, 使得参数的选择更加合理, 但同时也需要更多的学习时间。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表</b>3 CUHK03<b>数据集上的性能对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Performance comparison on CUHK03 dataset %</p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td>测试<br />条件</td><td>方法</td><td>rank-1</td><td>rank-5</td><td>rank-10</td><td>mAP</td></tr><tr><td rowspan="13"><br />单摄像采集</td><td>KISSME+BoW<sup>[1]</sup></td><td>11.7</td><td>33.3</td><td>48.0</td><td>—</td></tr><tr><td><br />DNS<sup>[19]</sup></td><td>54.7</td><td>84.8</td><td>88.3</td><td>—</td></tr><tr><td><br />Verif.-Classif.<sup>[9]</sup></td><td>66.1</td><td>90.1</td><td>95.3</td><td>71.2</td></tr><tr><td><br />Multiregion CNN<sup>[20]</sup></td><td>45.6</td><td>26.1</td><td>56.6</td><td>32.3</td></tr><tr><td><br />GAN ResNet<sup>[10]</sup></td><td>73.1</td><td><b>92.7</b></td><td>96.7</td><td>77.4</td></tr><tr><td><br />SOMAnet<sup>[22]</sup></td><td>72.4</td><td>—</td><td>—</td><td>85.9</td></tr><tr><td><br />ResNet-50<sup>[14]</sup></td><td>71.5</td><td>91.5</td><td>95.9</td><td>75.8</td></tr><tr><td><br />Siamese-reranking<sup>[8]</sup></td><td><b>85.6</b></td><td>—</td><td>—</td><td>88.3</td></tr><tr><td><br />SI-CI <sup>[11]</sup></td><td>52.2</td><td>84.3</td><td>92.3</td><td>—</td></tr><tr><td><br />BraidNet-CS<sup>[23]</sup></td><td>83.3</td><td>—</td><td><b>98.0</b></td><td>—</td></tr><tr><td><br />本文方法 (GoogLeNet) </td><td>63.6</td><td>80.7</td><td>89.5</td><td>47.9</td></tr><tr><td><br />本文方法 (VGGNet-16) </td><td>66.8</td><td>81.2</td><td>92.4</td><td>51.3</td></tr><tr><td><br />本文方法 (ResNet-50) </td><td>82.9</td><td>91.8</td><td>97.4</td><td><b>89.2</b></td></tr><tr><td rowspan="10"><br />多摄像采集</td><td><br />Gated SCNN<sup>[7]</sup></td><td>68.1</td><td>88.1</td><td>94.6</td><td>58.8</td></tr><tr><td><br />GAN ResNet<sup>[10]</sup></td><td>73.1</td><td>92.7</td><td><b>96.7</b></td><td>77.4</td></tr><tr><td><br />S-LSTM<sup>[21]</sup></td><td>57.3</td><td>80.1</td><td>88.3</td><td>46.3</td></tr><tr><td><br />Verif.-Classif.<sup>[9]</sup></td><td>73.1</td><td>—</td><td>—</td><td>68.2</td></tr><tr><td><br />SOMAnet<sup>[22]</sup></td><td>85.9</td><td>—</td><td>—</td><td>—</td></tr><tr><td><br />VGGNet-16<sup>[12]</sup></td><td>58.8</td><td>80.2</td><td>87.3</td><td>51.0</td></tr><tr><td><br />ResNet-50<sup>[14]</sup></td><td>77.1</td><td>89.6</td><td>93.9</td><td>73.1</td></tr><tr><td><br />本文方法 (GoogLeNet) </td><td>67.2</td><td>87.0</td><td>91.1</td><td>59.7</td></tr><tr><td><br />本文方法 (VGGNet-16) </td><td>70.8</td><td>86.9</td><td>95.5</td><td>61.2</td></tr><tr><td><br />本文方法 (ResNet-50) </td><td><b>86.7</b></td><td><b>93.7</b></td><td>96.1</td><td><b>77.8</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="138">
                                            <p class="img_tit">
                                                <b>表</b>4 DukeMTMC-reID<b>数据集上的性能对比</b> %
                                                    <br />
                                                Tab. 4 Performance comparison on DukeMTMC-reID dataset %
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904008_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201904008_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904008_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 DukeMTMC-reID数据集上的性能对比 %" src="Detail/GetImg?filename=images/JSJY201904008_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="139" name="139" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="140">本文通过提出一种基于孪生网络结合识别损失和双向最大边界排序损失的方法, 解决了在实际中不同行人图像之间比相同行人图像之间更相似所造成的行人再识别准确率较低的问题, 并且通过数据可视化手段展示了识别处理效果。尽管本文只使用了3种基准网络验证本文方法的有效性, 但本文所提的排序损失和对应的网络结构可以适用于所有的基准CNN结构。同时, 通过在三个公开数据集上与其他方法进行对比实验, 本文方法取得了较好的综合性能。</p>
                </div>
                <div class="p1">
                    <p id="141">本文方法存在的主要问题是:1) 尽管没有通过实验数据对运行时间进行分析, 但是由于使用了拥有大量参数的网络模型, 并且所提出的损失函数在训练时计算较慢, 因此训练时间相对较长;2) 文中实验的参数并不是通过自适应学习得到的, 而是通过在某一数据集上实验获得的, 因此参数选择的合理性需要进行进一步的验证。</p>
                </div>
                <div class="p1">
                    <p id="142">基于现有结论, 本文认为未来可以沿以下几个方向开展进一步的研究工作:首先, 对如何把本文方法应用于更大规模的再识别数据集进行研究, 或者验证其在实际场景中的效果;其次, 对损失函数计算的优化和对算法参数的选择上需要采用更合理的方法;最后, 尽管本文方法是针对行人再识别提出的, 但也可以对其应用到其他课题进行探索, 如图像检索等。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="182">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable person re-identification A benchmark">

                                <b>[1]</b>ZHENG L, SHEN L, TIAN L, et al.Scalable person re-identification:a benchmark[C]//ICCV 2015:Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1116-1124.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Gaussian descriptor for person re-identification">

                                <b>[2]</b>MATSUKAWA T, OKABE T, SUZUKI E, et al.Hierarchical Gaussian descriptor for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:1363-1372.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by Local Maximal Occurrence representation and metric learning">

                                <b>[3]</b>LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//CVPR2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:2197-2206.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large scale metric learning from equivalence constraints">

                                <b>[4]</b>KOESTINGER M, HIRZER M, WOHLHART P, et al.Large scale metric learning from equivalence constraints[C]//CVPR 2012:Proceedings of the 2012 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2012:2288-2295.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distance metric learning for large margin nearest neighbor classification">

                                <b>[5]</b>WEINBERGER K Q, SAUL L K.Distance metric learning for large margin nearest neighbor classification[J].Journal of Machine Learning Research, 2009, 10 (2) :207-244.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification:past,present and future">

                                <b>[6]</b>ZHENG L, YANG Y, HAUPTMANN A G.Person re-identification:past, present and future[EB/OL].[2018-05-10].https://arxiv.org/pdf/1610.02984.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gated siamese convolutional neural network architecture for human re-identification">

                                <b>[7]</b>VARIOR R R, HALOI M, WANG G.Gated siamese convolutional neural network architecture for human re-identification[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:791-808.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811021&amp;v=MDA2MjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWN3JKTHo3QmQ3RzRIOW5Ocm85SFpZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>陈首兵, 王洪元, 金翠, 等.基于孪生网络和重排序的行人重识别[J].计算机应用, 2018, 38 (11) :3161-3166. (CHEN SB, WANG H Y, JIN C, et al.Person re-identification based on siamese network and reranking[J].Journal of Computer Applications, 2018, 38 (11) :3161-3166.) 
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM68E09C1E98C844456DEFD79E7048C559&amp;v=MTY0ODVlTjhCSGc5eXhNVm5rb0xQSGpyMlJVMWZicm5RTCtXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekwyN3hhaz1OaWZJWTdXd2E5SEYzSTR3Yg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>ZHENG Z, ZHENG L, YANG Y.A discriminatively learned CNNembedding for person reidentification[J].ACM Transactions on Multimedia Computing, Communications, and Applications, 2017, 14 (1) :13.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro">

                                <b>[10]</b>ZHENG Z, ZHENG L, YANG Y.Unlabeled samples generated by GAN improve the person re-identification baseline in vitro[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:3774-3782.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Learning of Single-Image and Cross-Image Representations for Person Re-identification">

                                <b>[11]</b>WANG F, ZUO W, LIN L, et al.Joint learning of single-image and cross-image representations for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2016:1288-1296.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[12]</b>SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-05-10].https://arxiv.org/pdf/1409.1556.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[13]</b>SZEGEDY C, LIU W, JIA Y, et al.Going deeper with convolutions[C]//CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:1-9.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[14]</b>HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dual-path convolutional image-text embedding with instance loss">

                                <b>[15]</b>ZHENG Z, ZHENG L, GARRETT M, et al.Dual-path convolutional image-text embedding with instance loss[EB/OL].[2018-05-10].https://arxiv.org/pdf/1711.05535.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating t-SNE using tree-based algorithms">

                                <b>[16]</b>van der MAATEN L.Accelerating t-SNE using tree-based algorithms[J].The Journal of Machine Learning Research, 2014, 15 (1) :3221-3245.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepReID:deep filter pairing neural network for person re-identification">

                                <b>[17]</b>LI W, ZHAO R, XIAO T, et al.DeepReid:deep filter pairing neural network for person re-identification[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:152-159.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance measures and a data set for multi-target,multi-camera tracking">

                                <b>[18]</b>RISTANI E, SOLERA F, ZOU R, et al.Performance measures and a data set for multi-target, multi-camera tracking[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:17-35.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Null Space for Person Re-identification">

                                <b>[19]</b>ZHANG L, XIANG T, GONG S.Learning a discriminative null space for person re-identification[C]//CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:1239-1248.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-Region bilinear convolutional neural networks for person re-identification">

                                <b>[20]</b>USTINOVA E, GANIN Y, LEMPITSKY V.Multi-region bilinear convolutional neural networks for person re-identification[C]//AVSS 2017:Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance.Washington, DC:IEEE Computer Society, 2017:1-6.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A siamese long short-term memory architecture for human re-identification&amp;quot;">

                                <b>[21]</b>VARIOR R R, SHUAI B, LU J, et al.A siamese long short-term memory architecture for human re-identification[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:135-153.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE3E2EA8B65341A4DDFB5F68EA6F7D4D9&amp;v=MTI4ODVjYTdhOU81M29jM1l1NE1DSDFJeTJKbm5FMTRQbm5xMldNekQ3WGdRYzZXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekwyN3hhaz1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>BARBOSA I B, CRISTANI M, CAPUTO B, et al.Looking beyond appearances:Synthetic training data for deep cnns in re-identification[J].Computer Vision and Image Understanding, 2018, 167:50-62.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person Re-identification with Cascaded Pairwise Convolutions">

                                <b>[23]</b>WANG Y, CHEN Z, WU F, et al.Person re-identification with cascaded pairwise convolutions[C]//CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2018:1470-1478.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns">

                                <b>[24]</b>LV J, CHEN W, LI Q, et al.Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns[C]//CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2018:7948-7956.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904008" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904008&amp;v=MjAxMjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3ckpMejdCZDdHNEg5ak1xNDlGYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
