<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136666945783750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907041%26RESULT%3d1%26SIGN%3dUWgfZbggQqBjneaa518YWW1akVI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907041&amp;v=MDg3MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1ZydkFMejdCZDdHNEg5ak1xSTlCWlk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="1 改进的基于UNet++的候选结节检测 ">1 改进的基于UNet++的候选结节检测</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="1.1 &lt;b&gt;改进&lt;/b&gt;UNet++&lt;b&gt;网络结构&lt;/b&gt;">1.1 <b>改进</b>UNet++<b>网络结构</b></a></li>
                                                <li><a href="#76" data-title="1.2 &lt;b&gt;基于深监督的整合算法&lt;/b&gt;">1.2 <b>基于深监督的整合算法</b></a></li>
                                                <li><a href="#81" data-title="1.3 &lt;b&gt;改进基于&lt;/b&gt;Dice&lt;b&gt;系数的损失函数&lt;/b&gt;">1.3 <b>改进基于</b>Dice<b>系数的损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="2 多尺度三维卷积融合分类模型 ">2 多尺度三维卷积融合分类模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="2.1 &lt;b&gt;多尺度的三维卷积神经网络设计&lt;/b&gt;">2.1 <b>多尺度的三维卷积神经网络设计</b></a></li>
                                                <li><a href="#98" data-title="2.2 &lt;b&gt;融合分类结果&lt;/b&gt;">2.2 <b>融合分类结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#106" data-title="3.1 &lt;b&gt;实验设计细节&lt;/b&gt;">3.1 <b>实验设计细节</b></a></li>
                                                <li><a href="#124" data-title="3.2 &lt;b&gt;候选结节检测结果&lt;/b&gt;">3.2 <b>候选结节检测结果</b></a></li>
                                                <li><a href="#134" data-title="3.3 &lt;b&gt;结节分类结果&lt;/b&gt;">3.3 <b>结节分类结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#143" data-title="4 讨论 ">4 讨论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#145" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="图1 改进的肺结节检测系统框架">图1 改进的肺结节检测系统框架</a></li>
                                                <li><a href="#74" data-title="图2 图1中卷积块的结构">图2 图1中卷积块的结构</a></li>
                                                <li><a href="#95" data-title="图3 三维卷积分类网络结构">图3 三维卷积分类网络结构</a></li>
                                                <li><a href="#127" data-title="图4 同一网络结构不同结果处理的对比">图4 同一网络结构不同结果处理的对比</a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;候选结节检测模型结果比较&lt;/b&gt;"><b>表</b>1 <b>候选结节检测模型结果比较</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;候选结节检测阶段各个算法的实验结果对比&lt;/b&gt;"><b>表</b>2 <b>候选结节检测阶段各个算法的实验结果对比</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各模型在不同&lt;/b&gt;FPs/scan&lt;b&gt;下的敏感度&lt;/b&gt;"><b>表</b>3 <b>各模型在不同</b>FPs/scan<b>下的敏感度</b></a></li>
                                                <li><a href="#140" data-title="图5 本文算法结节分类结果样例">图5 本文算法结节分类结果样例</a></li>
                                                <li><a href="#142" data-title="图6 不同算法的FROC曲线对比">图6 不同算法的FROC曲线对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="184">


                                    <a id="bibliography_1" title=" MURPHY K, van GINNEKEN B, SCHILHAM A M, et al.A large-scale evaluation of automatic pulmonary nodule detection in chest CT using local image features and k-nearest-neighbour classification[J].Medical Image Analysis, 2009, 13 (5) :757-770." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300711418&amp;v=MDU1NTZaZVp0RmlubFVyM0lKMXNSYmhvPU5pZk9mYks3SHRETnJJOUZZK29PQ0gweG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         MURPHY K, van GINNEKEN B, SCHILHAM A M, et al.A large-scale evaluation of automatic pulmonary nodule detection in chest CT using local image features and k-nearest-neighbour classification[J].Medical Image Analysis, 2009, 13 (5) :757-770.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_2" title=" LOPEZ T, FIORINA E, PENNAZIO F, et al.Large scale validation of the M5L lung CAD on heterogeneous CT datasets[J].Medical Physics, 2015, 42 (4) :1477-1489." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large scale validation of the M5Llung CAD on heterogeneous CTdatasets">
                                        <b>[2]</b>
                                         LOPEZ T, FIORINA E, PENNAZIO F, et al.Large scale validation of the M5L lung CAD on heterogeneous CT datasets[J].Medical Physics, 2015, 42 (4) :1477-1489.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_3" title=" LI C F, ZHU G C, WU X J, et al.False-positive reduction on lung nodules detection in chest radiographs by ensemble of convolutional neural networks[J].IEEE Access, 2018, 6 (99) :16060-16067." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=False-positive reduction on lung nodules detection in chest radiographs by ensemble of convolutional neural networks">
                                        <b>[3]</b>
                                         LI C F, ZHU G C, WU X J, et al.False-positive reduction on lung nodules detection in chest radiographs by ensemble of convolutional neural networks[J].IEEE Access, 2018, 6 (99) :16060-16067.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_4" title=" 吕晓琪, 吴凉, 谷宇, 等.基于三维卷积神经网络的低剂量CT肺结节检测[J].光学精密工程, 2018, 26 (5) :1211-1218. (LYU X Q, WU L, GU Y, et al.Detection of low dose CT pulmonary nodules based on 3D convolution neural network[J].Optics and Precision Engineering, 2018, 26 (5) :1211-1218.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201805023&amp;v=MjQzODg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVnJ2QUlqWEJZN0c0SDluTXFvOUhaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         吕晓琪, 吴凉, 谷宇, 等.基于三维卷积神经网络的低剂量CT肺结节检测[J].光学精密工程, 2018, 26 (5) :1211-1218. (LYU X Q, WU L, GU Y, et al.Detection of low dose CT pulmonary nodules based on 3D convolution neural network[J].Optics and Precision Engineering, 2018, 26 (5) :1211-1218.) 
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     DOU Q, CHEN H, YU L Q, et al.Multi-level contextual 3D CNNs for false positive reduction in pulmonary nodule detection[J].IEEE Transactions on Biomedical Engineering, 2017, 64 (7) :1558-1567.</a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_6" title=" XIE H T, YANG D B, SUN N N, et al.Automated pulmonary nodule detection in CT images using deep convolutional neural networks[J].Pattern Recognition, 2019, 85:109-119." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESEB10EA11F8A97E877C4C399C5DC0A9A5&amp;v=Mjc1MTVLSDlHNTNvNUVFdU4rQlh0TXh4RVVtVHNPUzNicjN4ZEJDckxsVE11YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3MjZ4S0E9TmlmT2ZjYg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         XIE H T, YANG D B, SUN N N, et al.Automated pulmonary nodule detection in CT images using deep convolutional neural networks[J].Pattern Recognition, 2019, 85:109-119.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_7" title=" LI W, CAO P, ZHAO D Z, et al.Pulmonary nodule classification with deep convolutional neural networks on computed tomography images[J].Computational and Mathematical Methods in Medicine, 2016, 2016:6215085." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pulmonary nodule classification with deep convolutional neural networks on computed tomography images">
                                        <b>[7]</b>
                                         LI W, CAO P, ZHAO D Z, et al.Pulmonary nodule classification with deep convolutional neural networks on computed tomography images[J].Computational and Mathematical Methods in Medicine, 2016, 2016:6215085.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_8" title=" DING J, LI A X, HU Z Q, et al.Accurate pulmonary nodule detection in computed tomography images using deep convolutional neural networks[C]// Proceedings of the 2017 International Conference on Medical Computing and Computer-Assisted Intervention.Berlin:Springer, 2017:559-567." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate pulmonary nodule detection in computed tomography images using deep convolutional neural networks">
                                        <b>[8]</b>
                                         DING J, LI A X, HU Z Q, et al.Accurate pulmonary nodule detection in computed tomography images using deep convolutional neural networks[C]// Proceedings of the 2017 International Conference on Medical Computing and Computer-Assisted Intervention.Berlin:Springer, 2017:559-567.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_9" title=" KAMNITSAS, K, LEDIG C, NEWCOMBE V F, et al.Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation[J].Medical Image Analysis, 2016, 36:61-78." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES75987FC868EC3E9704EAA1312A88E06D&amp;v=MTg2NDFyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3NzI2eEtBPU5pZk9mYlM5RjluTDJmeE5ZdU42ZjM5TXhoRVQ3a29NT1g3aHJSQkVjYnJoUmJ6ckNPTnZGU2lXVw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         KAMNITSAS, K, LEDIG C, NEWCOMBE V F, et al.Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation[J].Medical Image Analysis, 2016, 36:61-78.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_10" title=" DOU Q, CHEN H, YU L Q, et al.Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks[J].IEEE Transactions on Medical Imaging, 2016, 35 (5) :1182-1195." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks">
                                        <b>[10]</b>
                                         DOU Q, CHEN H, YU L Q, et al.Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks[J].IEEE Transactions on Medical Imaging, 2016, 35 (5) :1182-1195.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_11" title=" CHEN H, DOU Q, YU L Q, et al.VoxResNet:deep voxelwise residual networks for brain segmentation from 3D MR images [J].NeuroImage, 2018, 170:446-455." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES2080426029567673FF8336544C35F8FC&amp;v=MTM1MjB4S0E9TmlmT2ZiRzRGdEhJcllsRlp1SUtDbnMveUJWbG5EZCtTM25ucUJaR2VyZmlUY3pzQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzcyNg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         CHEN H, DOU Q, YU L Q, et al.VoxResNet:deep voxelwise residual networks for brain segmentation from 3D MR images [J].NeuroImage, 2018, 170:446-455.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_12" title=" RONNEBERGER O, FISCHER P, BROX T.U-Net:convolutional networks for biomedical image segmentation[C]// Proceedings of the 2015 International Conference on Medical Computing and Computer-Assisted Intervention.Berlin:Springer, 2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">
                                        <b>[12]</b>
                                         RONNEBERGER O, FISCHER P, BROX T.U-Net:convolutional networks for biomedical image segmentation[C]// Proceedings of the 2015 International Conference on Medical Computing and Computer-Assisted Intervention.Berlin:Springer, 2015:234-241.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_13" title=" ZHOU Z W, MAHFUZUR RAHMAN SIDDIQUEE M, LIANG J M, et al.UNet++:a nested U-Net architecture for medical image segmentation[C]// Proceedings of the 2018 International Conference on Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support.Berlin:Springer, 2018:3-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=UNet++:a nested U-Net architecture for medical image segmentation">
                                        <b>[13]</b>
                                         ZHOU Z W, MAHFUZUR RAHMAN SIDDIQUEE M, LIANG J M, et al.UNet++:a nested U-Net architecture for medical image segmentation[C]// Proceedings of the 2018 International Conference on Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support.Berlin:Springer, 2018:3-11.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_14" title=" LEE C Y, XIE S, GALLAGHER P, et al.Deeply-supervised nets [C]// Proceedings of the 2015 18th International Conference on Artificial Intelligence and Statistics.San Diego, CA:[s.n.], 2015:562-570." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deeply-supervised nets">
                                        <b>[14]</b>
                                         LEE C Y, XIE S, GALLAGHER P, et al.Deeply-supervised nets [C]// Proceedings of the 2015 18th International Conference on Artificial Intelligence and Statistics.San Diego, CA:[s.n.], 2015:562-570.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_15" title=" SUDRE C H, LI W Q, VERCAUTEREN T, et al.Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations [C]// Proceedings of the 2017 International Conference on Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support.Berlin:Springer, 2017:240-248." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalised Dice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations">
                                        <b>[15]</b>
                                         SUDRE C H, LI W Q, VERCAUTEREN T, et al.Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations [C]// Proceedings of the 2017 International Conference on Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support.Berlin:Springer, 2017:240-248.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_16" title=" SETIO A A A, TRAVERSO A, DE BEL T, et al.Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images:the LUNA16 challenge [J].Medical Image Analysis, 2017, 42:1-13." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESCA987C3548DE267FA3D128C5502E1EA0&amp;v=MDE3ODcxZThlVk1NdWZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3NzI2eEtBPU5pZk9mY0RKRjluTDNJeEFZT043ZVg0L3lHQmk2VXQ4U25lUnFSYw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         SETIO A A A, TRAVERSO A, DE BEL T, et al.Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images:the LUNA16 challenge [J].Medical Image Analysis, 2017, 42:1-13.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_17" title=" LOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// Proceedings of the 2015 International Conference on Machine Learning.New York:ACM, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[17]</b>
                                         LOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// Proceedings of the 2015 International Conference on Machine Learning.New York:ACM, 2015:448-456.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_18" title=" ARMATO S G, MCLENNAN G, BIDAUT L, et al.The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) :a completed reference database of lung nodules on CT scans[J].Medical Physics, 2011, 38 (2) :915-931." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans">
                                        <b>[18]</b>
                                         ARMATO S G, MCLENNAN G, BIDAUT L, et al.The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) :a completed reference database of lung nodules on CT scans[J].Medical Physics, 2011, 38 (2) :915-931.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_19" title=" HE K M, ZHANG X Y, REN S Q, et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification[C]// ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1026-1034." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Delving deep into rectifiers:Surpassing Human-level performance on ImageNet classification">
                                        <b>[19]</b>
                                         HE K M, ZHANG X Y, REN S Q, et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification[C]// ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1026-1034.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_20" title=" SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">
                                        <b>[20]</b>
                                         SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_21" title=" REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transaction on Pattern Analysis and Machine Intelligence, 2015, 39:1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">
                                        <b>[21]</b>
                                         REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transaction on Pattern Analysis and Machine Intelligence, 2015, 39:1137-1149.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_22" title=" NIEMEIJER M, LOOG M, ABRAMOFF M D, et al.On combining computer-aided detection systems[J].IEEE Transactions on Medical Imaging, 2011, 30:215-223." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On combining computer-aided detection systems">
                                        <b>[22]</b>
                                         NIEMEIJER M, LOOG M, ABRAMOFF M D, et al.On combining computer-aided detection systems[J].IEEE Transactions on Medical Imaging, 2011, 30:215-223.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_23" title=" HE K M, ZHANG X Y, SUN J, et al.Deep residual learning for image recognition[C]// CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[23]</b>
                                         HE K M, ZHANG X Y, SUN J, et al.Deep residual learning for image recognition[C]// CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_24" title=" HUANG G, LIU Z, LAURENS V D M, et al.Densely connected convolutional networks[C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:2261-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">
                                        <b>[24]</b>
                                         HUANG G, LIU Z, LAURENS V D M, et al.Densely connected convolutional networks[C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:2261-2269.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_25" title=" 秦品乐, 李启, 曾建潮, 等.基于多尺度密集网络的肺结节图像检索算法研究[J].计算机应用, 2019, 39 (2) :392-397. (QIN P L, LI Q, ZENG J C, et al.Research on image retrieval algorithm of pulmonary nodules based on multi-scale dense network[J].Journal of Computer Applications, 2019, 39 (2) :392-397.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201902016&amp;v=MTAwOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVnJ2QUx6N0JkN0c0SDlqTXJZOUVZb1E=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         秦品乐, 李启, 曾建潮, 等.基于多尺度密集网络的肺结节图像检索算法研究[J].计算机应用, 2019, 39 (2) :392-397. (QIN P L, LI Q, ZENG J C, et al.Research on image retrieval algorithm of pulmonary nodules based on multi-scale dense network[J].Journal of Computer Applications, 2019, 39 (2) :392-397.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-19 15:15</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),2109-2115 DOI:10.11772/j.issn.1001-9081.2019010056            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度卷积神经网络的肺结节检测算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%93%E5%BF%A0%E8%B1%AA&amp;code=42202205&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邓忠豪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%99%93%E4%B8%9C&amp;code=27767566&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈晓东</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E4%B8%8A%E6%B5%B7%E9%AB%98%E7%AD%89%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=1694425&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院上海高等研究院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0151376&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海科技大学信息科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在传统的肺结节检测算法中, 存在检测敏感度低, 假阳性数量大的问题。针对这一问题, 提出了基于深度卷积神经网络 (CNN) 的肺结节检测算法。首先, 有目的性地简化传统的全卷积分割网络;然后, 创新地加入对部分CNN层的深监督并使用改进的加权损失函数, 获得高质量的候选肺结节, 保证高敏感度;其次, 设计了基于多尺度上下文信息的三维深度CNN来增强对图像的特征提取;最后, 将训练得到的融合分类模型用于候选结节分类, 以达到降低假阳率的目的。所提算法使用了LUNA16数据集, 并通过对比实验验证算法的性能。在检测阶段, 当每个CT检测出的候选结节数为50.2时, 获得的敏感度为94.3%, 与传统的全卷积分割网络相比提升了4.2个百分点;在分类阶段, 竞争性能指标达到0.874。实验结果表明, 所提算法能够有效提高检测敏感度和降低假阳率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%82%BA%E7%BB%93%E8%8A%82%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">肺结节检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E7%9B%91%E7%9D%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深监督;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E6%9D%83%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加权损失函数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    邓忠豪 (1993—) , 男, 湖南湘西人, 硕士研究生, 主要研究方向:深度学习、医学图像处理;;
                                </span>
                                <span>
                                    *陈晓东 (1969—) , 男, 广东兴宁人, 研究员, 博士, 主要研究方向:物联网技术、机器学习。电子邮箱chenxd@sari.ac.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-09</p>

            </div>
                    <h1><b>Pulmonary nodule detection algorithm based on deep convolutional neural network</b></h1>
                    <h2>
                    <span>DENG Zhonghao</span>
                    <span>CHEN Xiaodong</span>
            </h2>
                    <h2>
                    <span>Shanghai Advanced Research Institute, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
                    <span>School of Information Science and Technology, ShanghaiTech University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In traditional pulmonary nodule detection algorithms, there are problems of low detection sensitivity and large number of false positives. To solve these problems, a pulmonary nodule detection algorithm based on deep Convolutional Neural Network (CNN) was proposed. Firstly, the traditional full convolution segmentation network was simplified on purpose. Then, in order to obtain high-quality candidate pulmonary nodules and ensure high sensitivity, the deep supervision of partial CNN layers was innovatively added and the improved weighted loss function was used. Thirdly, three-dimensional deep CNNs based on multi-scale contextual information were designed to enhance the feature extraction of images. Finally, the trained fusion classification model was used for candidate nodule classification to achieve the purpose of reducing false positive rate. The performance of algorithm was verified through comparison experiments on LUNA16 dataset. In the detection stage, when the number of candidate nodules detected by each CT (Computed Tomography) is 50.2, the sensitivity of this algorithm is 94.3%, which is 4.2 percentage points higher than that of traditional full convolution segmentation network. In the classification stage, the competition performance metric of this algorithm reaches 0.874. The experimental results show that the proposed algorithm can effectively improve the detection sensitivity and reduce the false positive rate.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pulmonary%20nodule%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pulmonary nodule detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20supervision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep supervision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weighted%20loss%20function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weighted loss function;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    DENG Zhonghao, born in 1993, M. S. candidate. His research interests include deep learning, medical image processing.;
                                </span>
                                <span>
                                    CHEN Xiaodong, born in 1969, Ph. D. , research fellow. His research interests include Internet of things technology, machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-09</p>
                            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="54">肺癌是世界上发病率和死亡率最高的癌症之一。肺癌的早期无症状表现将导致患者可能错过最佳治疗时间, 为了有效地防止这种情况的出现, 需要提前进行肺结节检测。NLST (National Lung Screening Trial) 实验表明, 使用低剂量胸部CT (Computed Tomography) 对高危人群进行每年3次的检查, 7年后肺癌的死亡率降低了20%, CT扫描也是目前在肺癌筛查中使用最多的方法。由于CT扫描技术的发展和快速增长的需求, 放射科医生所需分析的数据量大幅度增加, 导致工作量增大和诊断难度的提高, 因此, 计算机辅助诊断系统逐渐发展起来, 用它来辅助放射科医生分析CT图像, 帮助提高肺癌筛查的速度和准确率。</p>
                </div>
                <div class="p1">
                    <p id="55">随着人工智能在医学图像领域的发展, 计算机辅助诊断系统发展迅速。用于肺结节检测的计算机辅助诊断系统可以分为两部分:1) 检测候选肺结节;2) 去除假阳性 (候选结节分类) 。检测候选肺结节部分, 从CT图像中尽可能多地检测出真实结节, 保证检测敏感度;去除假阳性部分, 从所有候选结节中对真实结节和假阳性结节进行良恶性二分类。</p>
                </div>
                <div class="p1">
                    <p id="56">为了提高检测敏感度和降低假阳率, Murphy等<citation id="234" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>利用传统计算机视觉手工提取特征的方法, 引入曲率特征和形状指数用来计算所有体素, 得到满足阈值的可疑种子体素, 这些种子体素覆盖在候选结节周围, 再使用滞后阈值法得到候选结节, 最后使用2个<i>K</i>-最近邻分类器来降低假阳率。Lopez等<citation id="235" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>通过专门设计的滤波函数检测候选结节, 然后利用基于小型手工特征集的前馈神经网络来实现结节分类。</p>
                </div>
                <div class="p1">
                    <p id="57">使用人工设计特征的结节检测方法存在一定缺陷, 因为它总是假设肺结节存在某一特点 (比如:形状、大小、纹理等) , 但这些表层特征缺乏鉴别力, 因为肺结节的形状、大小、纹理等特征具有高可变性, 很难识别真实结节和假阳性结节。随着深度卷积神经网络 (Convolutional Neural Network, CNN) 在图像领域的巨大成功, 从大量的训练数据中学习到高级特征, 这些高级特征的表达能力得到了广泛的认可。在最近的研究中, Li等<citation id="236" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>针对单个二维卷积神经网络学习能力的局限性提出了将输入数据以滑动窗口为229×229的大小进行采样, 然后缩小为12×12、32×32、60×60的尺寸输入到三种对应的二维卷积神经网络中, 肺结节的检测结果由这3个不同网络的输出经过逻辑与操作之后得到。吕晓琪等<citation id="237" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>为了提高分类准确率设计了一个三维卷积神经网络, 网络的输入尺寸是以候选结节为中心的32×32×8的结节块, 该方法验证了三维卷积神经网络能够更加充分地利用肺结节的三维空间信息, 有效地降低假阳率。Dou等<citation id="238" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>为了解决由于结节的高可变性所造成的识别敏感度低的问题, 引入3个不同尺度的三维卷积神经网络, 并融合候选结节在这3个尺度下的分类概率, 作为最终结果。基于深度学习的肺结节检测方法<citation id="239" type="reference"><link href="188" rel="bibliography" /><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><link href="194" rel="bibliography" /><link href="196" rel="bibliography" /><link href="198" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>取得更好的性能, 神经网络自主学习的高效特征能够更有效区分真实结节和非结节。值得一提的是, 虽然三维的医学图像数据在临床实践中已经广泛流行, 但是三维的卷积神经网络在医学图像的应用中还处在初级阶段, 只有少量的用于三维数据的卷积神经网络被提出<citation id="240" type="reference"><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><link href="198" rel="bibliography" /><link href="200" rel="bibliography" /><link href="202" rel="bibliography" /><link href="204" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>, 而利用图像的空间维度的上下文信息可以有效地提高分类准确率, 因此, 设计出一个理想的三维卷积网络对肺结节检测系统的分类性能来说至关重要。</p>
                </div>
                <div class="p1">
                    <p id="58">UNet<citation id="241" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>是候选结节检测常用的神经网络, 整个网络结构包括23个卷积层, 这些卷积层包含卷积、最大值池化、反卷积、全卷积的操作, 是一种编码—解码的拓扑结构。UNet分割网络在结构上有3个重要的特点:下采样、上采样和跨层连接。浅层抓取简单特征, 通过下采样, 由于感受野的增大, 抓取到更深层、抽象的特征;上采样的作用是还原位置信息;跨层连接则整合了神经网络层抓取的不同分辨率、不同强度语义的特征及位置信息。同时, 跨层连接不增加额外的时间和计算量, 但是, 在特定的数据集下, UNet网络结构中的4次下采样并非效果最好。Zhou等<citation id="242" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出UNet++网络, 在传统UNet的基础上增加了更多的上采样, 将不同层次的特征通过特征叠加的方式整合, 并在不同层次的语义分割结果上均加入深监督<citation id="243" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="59">本文提出了改进的UNet++网络, 应用于肺结节检测的候选结节提取部分。首先精简了UNet++的网络结构, 由4次下采样减小到3次, 然后, 对于深监督的L1、L2、L3<citation id="244" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的分割结果, 仅取L2和L3分割结果的平均值, 舍弃L1的结果。L2的分割结果更侧重于找到更多候选结节, 提高敏感度;L3的分割结果则更加准确, 去除了L2的部分假阳结节, 降低了假阳率。由于L2的下采样次数较少, L3的较高感受野能获取在L2下无法正确分割的真实结节。结合L2和L3的分割结果, 提高了检测敏感度, 并为第二阶段的结节分类任务提供更多高质量的候选结节。最后, 为了进一步提高检测敏感度, 改进基于Dice系数<citation id="245" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的损失函数。使用以上改进的算法, 候选结节的检测敏感度达到94.3%。</p>
                </div>
                <div class="p1">
                    <p id="60">在候选结节二分类阶段, 本文设计了两个独立的三维卷积神经网络, 这两个网络的输入是不同尺寸的候选结节三维块。这一阶段, 充分利用结节在空间维度上的信息, 更有利于鉴别真实结节和难以识别的非结节。考虑到不同肺结节的直径大小、形状和位置信息存在的差异, 使用两个三维卷积网络, 然后融合这两个网络模型的分类结果, 使得分类模型更有鲁棒性, 获得了0.874的平均敏感度分数。</p>
                </div>
                <div class="p1">
                    <p id="61">综上所述, 本文提出了改进的肺结节检测系统框架, 具体可分为两个阶段:候选结节检测和假阳性去除。改进的算法框架结构如图1所示。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907041_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 改进的肺结节检测系统框架" src="Detail/GetImg?filename=images/JSJY201907041_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 改进的肺结节检测系统框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907041_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of the improved pulmonary nodule detection system</p>

                </div>
                <div class="p1">
                    <p id="63">本文算法使用的数据集是LUNA16 (LUng Nodule Analysis 2016) <citation id="246" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 并取得了良好的表现性能。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">1 改进的基于UNet++的候选结节检测</h3>
                <div class="p1">
                    <p id="65">候选结节检测是肺结节检测系统的重要步骤之一, 目的是在CT图像的检测任务中限制候选结节数目, 并保证高敏感度。在这一阶段, 使用改进的UNet++网络提取候选结节, 输入是经过肺实质分割之后的二维图像, 加入深监督和改进的损失函数, 综合两层输出的结果, 得到最终的分割结果。</p>
                </div>
                <div class="p1">
                    <p id="66">敏感度 (Sensitivity) 是检测系统中最重要的指标之一, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中:<i>TP</i> (True Positive) 表示真正例即真实结节数, <i>FN</i> (False Negative) 表示假反例。</p>
                </div>
                <div class="p1">
                    <p id="69">本文针对LUNA16数据集的特点, 对传统UNet++网络进行了以下三个改进:1) 精简了网络结构 (图1 Stage1) , 并使用3个堆叠的卷积操作组成了图1 Stage1的卷积块结构;2) 加入深监督时, 舍弃对L1的网络输出的监督, 仅整合L2和L3的输出结果;3) 改进基于Dice系数的损失函数。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">1.1 <b>改进</b>UNet++<b>网络结构</b></h4>
                <div class="p1">
                    <p id="71">传统的UNet++网络<citation id="247" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>分无深监督和深监督两个模式:无深监督就是仅输出最后一个卷积层的结果;深监督模式则是加入了对4个子网络分割结果的监督。</p>
                </div>
                <div class="p1">
                    <p id="72">而本文使用的语义分割神经网络如图1中的Stage1所示。网络结构使用了简化的UNet++, 这个网络模型也是解码—编码的U型结构, 核心思想是特征的再利用。首先通过下采样提取图像特征, 每次下采样之后都伴随上采样, 并将它与前面所有同分辨率的特征图作叠加。L2、L3表示了分别在经过不同的编码—解码的U型拓扑网络结构后再加入1×1的卷积, 得到的两个分割结果。L2、L3中的数字, 代表了经过的下采样的次数。图1 Stage1中的卷积块的结构如图2所示。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907041_074.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 图1中卷积块的结构" src="Detail/GetImg?filename=images/JSJY201907041_074.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 图1中卷积块的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907041_074.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Structure of convolutional blocks in Fig.1</p>

                </div>
                <div class="p1">
                    <p id="75">设计的一个卷积块由3个堆叠的3×3卷积层组成, <i>n</i>表示当前使用的卷积核的数量, 顺着下采样 (图1 Stage1) 的箭头向下, 使用的卷积核数量<i>n</i>分别为32, 64, 128和256。同时, 用到批量标准化<citation id="248" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>操作, 用于加速训练和防止模型过拟合, 用ReLU (Rectified Linear Unit) 作为激活函数。与传统的UNet的2个堆叠卷积层相比, 使用3个堆叠层可以更有效地整合特征信息;剪去UNet++的第4个下采样过程<citation id="249" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 是为了让模型能够抓取更多候选结节, 尽可能多地保留真实结节, 为假阳性去除阶段提供具有高敏感度的数据, 以提高整个算法框架的性能。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">1.2 <b>基于深监督的整合算法</b></h4>
                <div class="p1">
                    <p id="77">与传统UNet++的深监督不同, 图1 Stage1中, 网络最后的分割结果由L2和L3两部分的均值确定。输入的图像经过3个下采样、6个上采样及图中对应的跨层连接操作, 得到特征图, 再通过1×1的卷积核得到一个分割结果L3。同样, L2是经过2个下采样、3个上采样的U型结构后得到。在训练的过程中, 监督L2和L3, 并选择在验证集上表现最好的模型。L2和L3的分割结果侧重点不同, L2获得的分割结果是较为粗略的, 对真实结节的鉴别能力较弱, 但它对一些较为复杂的结节和难鉴别的非结节的同时提取, 可以有效保证敏感度。L3的结果侧重于分割准确度, 能够精确定位和识别真实结节。对L2和L3的输出取平均, 能够保证敏感度和准确度, 可以减小第二阶段假阳性去除的难度。分割网络最终的输出结果如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false"> (</mo><mi>Ο</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mn>2</mn></mrow></msub><mo>+</mo><mi>Ο</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mn>3</mn></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">对L2使用深监督, 而不是重新训练一个更小的只有2个下采样的U型网络模型, 是因为剪去复杂的层结构之后, 网络学习的特征太过简单, 对肺结节的识别能力较弱;相反, L2的输出结果在训练过程中, 受到更深层的神经元的损失函数在反向传播中的影响, 能使L2分割结果综合更多高效的特征, 与L3的互相影响之下, 使得加入了深监督的模型对肺结节有更强的鉴别能力。</p>
                </div>
                <div class="p1">
                    <p id="80">舍弃L1<citation id="250" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 是因为L1学到的特征较浅, 对真实结节的鉴别能力不够, 不利于提高敏感度, 因此, 仅选择L2和L3的分割进行聚合。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">1.3 <b>改进基于</b>Dice<b>系数的损失函数</b></h4>
                <div class="p1">
                    <p id="82">检测阶段的主要任务是保证检测敏感度, 尽可能多地保留真实结节且能排除非结节。训练阶段, 输入是二维512×512的肺实质图像, 这些图像是从原数据集中的CT扫描件通过横向切割并加以预处理得到的。输出结果是相同大小的像素级别的分类结果, 即输入数据的每个像素值属于结节的概率分布情况, 然后设定阈值即可得到分割结果。利用本文前面提到的改进后的UNet++网络, 在特征提取和特征整合方面更加全面, 能够更加准确地区分血管等其他非结节区域。结节的平均直径在约10 mm, 在像素级分类任务中, 真实结节区域像素属于正样本, 其他区域像素属于负样本, 因此存在严重的正负样本不平衡的问题, 约为1∶2 600。传统方法是使用Dice系数的损失函数, 如式 (3) 所示:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>D</mtext><mtext>i</mtext><mtext>c</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mrow><mi>Y</mi><mo>^</mo></mrow><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mn>2</mn><mo>⋅</mo><mi>Y</mi><msub><mrow></mrow><mi>k</mi></msub><mo>⋅</mo><mrow><mi>Y</mi><mo>^</mo></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>Y</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mrow><mi>Y</mi><mo>^</mo></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中:<i>Y</i>和<i>Y</i>^表示真实标签和像素级预测值, <i>N</i>表示<i>batch</i>的大小, <i>k</i>表示第<i>k</i>个图像。基于Dice系数的损失函数在训练准确度上表现很好, 但是使用Dice损失函数, 会出现模型为追求高Dice (损失函数里取负Dice) , 而使部分真实结节被“舍弃”。为了更充分地检测并保留真实结节, 排除由于模型和目标 (损失) 函数等原因而导致的无法检测出一些难以鉴别的真实结节这一情况, 本文对基于Dice的损失函数进行了改进。</p>
                </div>
                <div class="p1">
                    <p id="85">从检测任务的目的入手, 为了让模型可以得到更多候选结节, 并提高敏感度, 本文设计的损失函数的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>o</mtext><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mrow><mi>Y</mi><mo>^</mo></mrow><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mfrac><mn>3</mn><mn>2</mn></mfrac></mrow></mstyle><mrow><mo> (</mo><mrow><mfrac><mrow><mi>Y</mi><msub><mrow></mrow><mi>k</mi></msub><mo>⋅</mo><mrow><mi>Y</mi><mo>^</mo></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>Y</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo>⋅</mo><mrow><mi>Y</mi><mo>^</mo></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式 (4) 是本文在候选结节检测阶段使用的损失函数, 与式 (3) 的损失函数相比, 减少了预测值所占的权重, 目的是使模型在保证分割准确度的同时, 适当地增加可供选择送入下一步分类的候选结节数量。</p>
                </div>
                <div class="p1">
                    <p id="88">经过以上的候选结节检测阶段, 为下一阶段的分类任务提供了精选的候选结节的信息, 其中包括结节位置信息、直径大小和概率等。</p>
                </div>
                <h3 id="89" name="89" class="anchor-tag">2 多尺度三维卷积融合分类模型</h3>
                <div class="p1">
                    <p id="90">在肺结节检测系统的假阳性去除阶段, 设计使用了多尺度的候选结节三维块作为输入, 训练两个对应尺度下的分类网络模型, 并采用融合后的分类分数作为最终分类结果。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">2.1 <b>多尺度的三维卷积神经网络设计</b></h4>
                <div class="p1">
                    <p id="92">上一阶段完成了对肺结节的候选结节检测与提取, 使用的是二维肺实质分割之后的图像, 是因为考虑到训练过程中时间和内存的代价;但是, 对于已经提取到的候选结节, 目标可以缩小到只有肺结节大小的范围, 这时, 如果再用结节的二维像素信息作为训练数据训练二维的分类器, 就无法充分利用CT扫描件的空间信息, 使得整个肺结节检测系统的良恶性二分类能力较弱。</p>
                </div>
                <div class="p1">
                    <p id="93">本文设计了多尺度的三维卷积分类网络。以二维的候选结节的像素中心为质心, 提取两个固定尺寸的结节三维块作为分类网络训练集数据。如图1 Stage2所示, 提取块的大小分别为25×25×8和50×50×20, 再分别用于两个不同的三维卷积分类网络的训练。相比单一尺度, 使用多尺度有如下3个优点:1) 对于直径较大的结节, 多尺度能够完整地包含结节的所有信息, 不会出现单一尺度下由于设置尺寸偏小导致的部分结节信息不全的情况;2) 对于直径较小的结节, 多尺度能够让小的结节在小尺寸的三维块中保持较大的占比, 保证网络学习到有用的特征;3) 使用多尺度, 在保证结节信息完整性的同时, 增加结节周围环境的上下文体素信息, 这些环境的上下文信息能够帮助分类模型提高识别准确率。</p>
                </div>
                <div class="p1">
                    <p id="94">图3是图1 Stage2三维卷积分类网络的详细结构图。输入分别为25×25×8和50×50×20的候选结节体块, 后者包括了图3所示的所有结构。前者体块由于分辨率相对较小, 不包含灰色部分的操作, 除此之外, 其他的网络配置均相同。三维卷积网络中用到了批量标准化层<citation id="251" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 使用了ReLU作为激活函数。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907041_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 三维卷积分类网络结构" src="Detail/GetImg?filename=images/JSJY201907041_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 三维卷积分类网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907041_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Architecture of 3D deep convolutional classification network</p>

                </div>
                <div class="p1">
                    <p id="96">图3对这两种不同尺度的结节体块使用不同的网络。是考虑了感受野对网络分类表现性能的重要影响。理论上, 卷积神经网络的感受野会随着网络层的加深而增大。如果感受野太小, 网络受限于局部信息, 会对变化较大目标缺乏鉴别能力;如果感受野过大, 那么过多的冗余信息和噪声都将干扰训练过程, 降低模型的表现性能, 因此, 对不同尺度的输入采用不同的卷积神经网络结构是十分必要的。</p>
                </div>
                <div class="p1">
                    <p id="97">同时, 在本文设计的网络中, 前面的4个卷积操作, 用到的卷积核尺寸为5×5×3和2×2×1, 而并非立方体的卷积核。原因是CT扫描件的三个坐标轴 (<i>x</i>, <i>y</i>, <i>z</i>) 中, <i>z</i>轴的长度与<i>x</i>、<i>y</i>轴相比较窄, 所以, 输入数据的尺寸都在<i>z</i>轴方向偏少。使用非立方体的卷积核, 能够更有效地促进结节块在三个坐标轴方向上的特征融合和信息整合。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">2.2 <b>融合分类结果</b></h4>
                <div class="p1">
                    <p id="99">多尺度分类网络得到相同候选结节在两种尺度下的两个分类结果。对于用作测试的一个候选结节<i>T</i><sub><i>i</i></sub>而言, 两个网络模型得到的分类结果可以分别记为<i>P</i><sub>1</sub> (<i>Y</i>^=<i>k</i>|<i>T</i><sub><i>i</i></sub>;<i>θ</i><sub>1</sub>) 和<i>P</i><sub>2</sub> (<i>Y</i>^=<i>k</i>|<i>T</i><sub><i>i</i></sub>;<i>θ</i><sub>2</sub>) , 表示结节<i>T</i><sub><i>i</i></sub>在两个模型下的预测值<i>Y</i>^等于<i>k</i>的概率, <i>k</i>取值为0 (非结节) 和1 (结节) 。</p>
                </div>
                <div class="p1">
                    <p id="100">为了将不同模型所侧重的多层上下文信息整合在一起进行最终分类, 本文采取的方法是将这两个网络模型的softmax层输出进行融合。融合操作的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>i</mtext><mtext>n</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mrow><mi>Y</mi><mo>^</mo></mrow><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><mi>Τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo stretchy="false">}</mo></mrow></munder><mi>w</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>Ρ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mrow><mi>Y</mi><mo>^</mo></mrow><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><mi>Τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">其中, 等号左边表示融合后的最终分类为真实结节的概率。各分类结果对应的权重<i>w</i><sub><i>j</i></sub>通过网格搜索法得到 (<i>w</i><sub>1</sub>=0.6, <i>w</i><sub>2</sub>=0.4) 。</p>
                </div>
                <h3 id="103" name="103" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="104">本章将对本文提出的肺结节检测算法在LUNA16数据集上的表现性能进行结果评价。LUNA16数据集来源于公开的更大的数据集LIDC-IDRI (Lung Image Database Consortium and Image Database Resource Initiative) <citation id="252" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。整个数据集去除了切片厚度大于3 mm的CT图像, 同时去除了space不一致和缺失部分切片的图像, 最后产生了888张CT扫描件。</p>
                </div>
                <div class="p1">
                    <p id="105">这些CT扫描件的横切图像大小为512×512, 总共包含1 186个专家标注的真实肺结节。候选结节检测任务中, 检测出的结节在真实结节标签的半径范围以内时, 即认为是一个正确的检测结果;在假阳性去除阶段, 正负样本由上一阶段的检测结果得到。实验时, 对LUNA16数据集进行10-折交叉验证。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">3.1 <b>实验设计细节</b></h4>
                <h4 class="anchor-tag" id="107" name="107">3.1.1 候选结节检测</h4>
                <div class="p1">
                    <p id="108">在候选结节检测阶段, 使用的是CT扫描件的横切面图像。首先对这些图像进行预处理, 以得到肺实质部分。具体操作步骤的描述如下:</p>
                </div>
                <div class="p1">
                    <p id="109">1) 二值化;</p>
                </div>
                <div class="p1">
                    <p id="110">2) 清除边界和形态学闭运算;</p>
                </div>
                <div class="p1">
                    <p id="111">3) 连通域分析;</p>
                </div>
                <div class="p1">
                    <p id="112">4) 肺部区域填充;</p>
                </div>
                <div class="p1">
                    <p id="113">5) 获得肺部区域掩码。</p>
                </div>
                <div class="p1">
                    <p id="114">经过上述预处理, 得到512×512的肺部区域图像, 过滤掉空气、骨骼等冗余的像素信息, 在数据输入神经网络之前, 还作了标准化处理。</p>
                </div>
                <div class="p1">
                    <p id="115">为了增加更多的样本, 防止模型过拟合, 本阶段对样本进行了数据增强:随机旋转角度[-10°, 10°];在<i>x</i>、<i>y</i>坐标轴方向上平移[-10, 10];尺寸缩放比例为[0.9, 1.1];加入水平翻转。</p>
                </div>
                <div class="p1">
                    <p id="116">训练本文第1章提出的改进后的全卷积网络, 权重初始化使用基于高斯分布的方法<citation id="253" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 设置初始学习率为0.001, 在池化层之后加入Dropout<citation id="254" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation> (系数为0.2) , 采用Adam优化算法, batchsize大小设置为8。训练时, 深监督中两个分割结果的损失对全局损失的权值占比为[0.5, 0.5]。处理分割结果时, 以得到的候选区域为中心, 提取区域面积两倍比例的矩形框来确定检测候选结节。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">3.1.2 结节分类</h4>
                <div class="p1">
                    <p id="118">对候选结节检测得到的结果, 提取对应的25×25×8和50×50×20的三维块。使用3.1.1节中的数据增强方法来增加正样本, 并在<i>z</i>轴上进行相同的变换处理。初始学习率为0.01, </p>
                </div>
                <div class="p1">
                    <p id="119">在池化层后加入参数为0.2的Dropout层。优化算法使用随机梯度下降法 (momentum=0.9) 。三维卷积分类网络使用的损失函数<citation id="255" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>如下:</p>
                </div>
                <div class="p1">
                    <p id="120"><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>C</mi><mo>-</mo><mn>1</mn></mrow></munderover><mn>1</mn></mstyle></mrow></mstyle><mo stretchy="false">{</mo><mi>Y</mi><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mi>c</mi><mo stretchy="false">}</mo><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mrow><mi>Y</mi><mo>^</mo></mrow><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mi>c</mi><mo stretchy="false">|</mo><mi>Τ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>θ</i>)                     (6) </p>
                </div>
                <div class="p1">
                    <p id="123">其中1表示指示函数, <i>P</i> (<i>Y</i>^<sup><i>i</i></sup>=<i>c</i>|<i>T</i><sub><i>i</i></sub>;<i>θ</i>) 表示输入的结节块属于类别<i>c</i>的概率。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">3.2 <b>候选结节检测结果</b></h4>
                <div class="p1">
                    <p id="125">在候选结节检测阶段, 评价结果的好坏主要分为以下三个指标:1) 平均Dice系数, 用来描述分割结果与真实值掩码的相似度;2) 敏感度, 理想情况下, 检测模型应当检测出所有的真实结节;3) 每个CT扫描件中检测出的候选结节个数 (用“候选结节数/scan”表示) 。性能好的检测模型应当在较低“候选结节数/scan”的情况下表现较高的敏感度。</p>
                </div>
                <div class="p1">
                    <p id="126">本文设计了多组对比实验来验证提出的改进网络带来的性能优化。首先对本文改进的模型进行自身对比实验, 是为了验证1.2节中提出的基于深监督的整合算法的有效性, 自身对比结果如图4所示。图4中, 使用了相同的经过精简的UNet++网络结构, 只是最后处理网络输出的过程不同, 图中第1、2、4、5个直方图对应同一个网络结构, 第3个直方图是在无深监督的条件下训练的模型。可以看出, 在深监督的情况下, 只取L2或者L3的分割结果得到的敏感度相对较低;通过深监督所有的 (L1, L2, L3) 分割结果后, 第4个直方图的检测敏感度比无深监督UNet++模型的检测敏感度高;第5个直方图是使用了本文的仅对L2、L3分割结果加入深监督的方法, 得到了较高敏感度的提升, 从第4种方法的93.2%提升至94.3%。</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907041_127.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 同一网络结构不同结果处理的对比" src="Detail/GetImg?filename=images/JSJY201907041_127.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 同一网络结构不同结果处理的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907041_127.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of different processing results with same network structure</p>

                </div>
                <div class="p1">
                    <p id="128">然后, 使用传统UNet、UNet++和经过改进的UNet++分别训练了基于Dice损失函数和改进的损失函数的一共6个模型, 并比较各模型在上述3个评价指标下的分割性能。实验结果如表1 (表中的UNet++均使用了深监督方法) 。前三个对比实验, 3种网络使用了基于Dice系数的损失函数, 虽然改进的模型在平均Dice系数指标略低于UNet++, 但检测敏感度上比传统UNet和UNet++高;使用了改进的加权损失函数之后, 本文提出的模型达到0.705的平均Dice系数和94.3%的敏感度, 在图像分割性能上优于前两个模型。表1的结果这也验证了本文提出的基于深监督的分割结果整合算法和改进的加权损失函数能够提高候选结节检测的敏感度, 更加能够检测出所有可疑的结节, 与传统UNet相比提升了4.2个百分点。</p>
                </div>
                <div class="p1">
                    <p id="129">最后, 本文设计实验对比了所提算法与肺结节检测阶段相同研究工作<citation id="256" type="reference"><link href="184" rel="bibliography" /><link href="194" rel="bibliography" /><link href="208" rel="bibliography" /><link href="214" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">6</a>,<a class="sup">13</a>,<a class="sup">16</a>]</sup></citation>的其他算法在LUNA16数据集下的各项数据结果。对比实验结果和算法比较如表2所示。</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit"><b>表</b>1 <b>候选结节检测模型结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Result comparison of candidate nodule detection models</p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td><br />模型</td><td>损失函数</td><td>平均Dice<br />系数</td><td>敏感<br />度/%</td><td>候选结<br />节数/scan</td></tr><tr><td>UNet</td><td></td><td>0.670</td><td>90.1</td><td>51.3</td></tr><tr><td><br />UNet++</td><td>基于Dice</td><td>0.719</td><td>90.8</td><td>30.2</td></tr><tr><td><br />本文改进的模型</td><td></td><td>0.703</td><td>91.5</td><td>36.7</td></tr><tr><td><br />UNet</td><td></td><td>0.653</td><td>90.6</td><td>63.4</td></tr><tr><td><br />UNet++</td><td>加权损失函数</td><td>0.698</td><td>93.2</td><td>39.0</td></tr><tr><td><br />本文改进的模型</td><td></td><td>0.705</td><td>94.3</td><td>50.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="131">
                    <p class="img_tit"><b>表</b>2 <b>候选结节检测阶段各个算法的实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Comparison experiment results of each algorithm in candidate nodule detection stage</p>
                    <p class="img_note"></p>
                    <table id="131" border="1"><tr><td><br />检测框架</td><td>基础算法</td><td>敏感度/%</td><td>候选结节数/scan</td></tr><tr><td><br />ISICAD</td><td>手工特征</td><td>85.6</td><td>335.9</td></tr><tr><td><br />Xie等<sup>[6]</sup></td><td>Faster R-CNN</td><td>86.4</td><td>4.7</td></tr><tr><td><br />ZNET</td><td>UNet</td><td>93.3</td><td>8.0</td></tr><tr><td><br />Baseline</td><td>UNet++</td><td>90.8</td><td>30.2</td></tr><tr><td><br />本文算法</td><td>改进的UNet++</td><td><b>94.3</b></td><td><b>50.2</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="132">从表2可以看出, 在使用了相同数据集的对比实验下, 本文所提算法在敏感度 (94.3%) 的指标上优于其他算法。同时, 表2中也加入了在上一个对比实验中 (表1第二项实验结果) 的一个对比算法“Baseline”, 它表示使用UNet++网络作为检测网络的实验结果。ISICAD<citation id="257" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>使用传统手工提取方法, 引入形状指数 (Shape Index, SI) 和曲率 (CurVedness, CV) 特征, 实验结果获得了85.6%的敏感度, 每个CT扫描件的候选结节数为335.9;Xie等<citation id="258" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>使用了基于Faster R-CNN (Faster Region-based Convolutional Neural Network) <citation id="259" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>的改进网络, 实验获得86.4%的检测敏感度和较低的候选结节数4.7/scan, 虽然本文算法在降低候选结节数 (50.2/scan) 上不及Xie等<citation id="260" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>的算法, 但获得的敏感度高于该算法;ZNET<citation id="261" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>用了UNet网络, 实验结果的敏感度指标为93.3%, 本文所提算法与它相比, 敏感度提高了1.0个百分点。</p>
                </div>
                <div class="p1">
                    <p id="133">经过多组对比实验, 验证了本文基于UNet++的改进网络, 在候选结节检测阶段可以获得更高的检测敏感度。虽然本文的算法在每个CT扫描件检测的候选结节的数量相比较而言不是最低, 但是达到的较高的敏感度对下一阶段结节分类和整个肺结节检测系统的性能提升都至关重要。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">3.3 <b>结节分类结果</b></h4>
                <div class="p1">
                    <p id="135">这一阶段实验结果的评价指标是LUNA16使用的竞争性能指标CPM (Competition Performance Metric) <citation id="262" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>分数。这个分数是在受试者特征曲线 (Free Receiver Operation Characteristic, FROC) 上, 计算不同的“每个CT扫描件的平均假阳性数目” (即average number of False Positives per scan, FPs/scan) 在 (1/8, 1/4, 1/2, 1, 2, 4, 8) 下的敏感度均值。目前, FPs/scan在1到4的范围广泛使用于临床实践当中, 因此, CPM指标具有广泛的认可度。</p>
                </div>
                <div class="p1">
                    <p id="136">首先设计实验验证多尺度分类融合方法的有效性。分别使用固定尺寸25×25×8、50×50×20和两种尺度融合, 它们使用对应的网络结构 (见图1 Stage2和2.1节) , 一共产生三种分类结果。计算它们在不同FPs/scan下的敏感度。对比实验结果如表3。为了描述方便, 单一尺度25×25×8、50×50×20训练的分类模型分别取名为M1和M2, 同时, 在表3中敏感度用小数表示。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表</b>3 <b>各模型在不同</b>FPs/scan<b>下的敏感度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Sensitivity of models under different FPs/ scan</p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />FPs/scan</td><td>M1</td><td>M2</td><td>多尺度融合模型</td></tr><tr><td><br />8</td><td>0.931</td><td>0.930</td><td><b>0.936</b></td></tr><tr><td><br />4</td><td>0.914</td><td>0.908</td><td>0.923</td></tr><tr><td><br />2</td><td>0.887</td><td>0.899</td><td>0.913</td></tr><tr><td><br />1</td><td>0.851</td><td>0.843</td><td>0.901</td></tr><tr><td><br />0.5</td><td>0.750</td><td>0.765</td><td>0.864</td></tr><tr><td><br />0.25</td><td>0.692</td><td>0.732</td><td>0.842</td></tr><tr><td><br />0.125</td><td>0.633</td><td>0.685</td><td>0.741</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="138">从表3可以看出, 所有模型在FPs/scan为4的时候, 均达到了90%以上的敏感度。在FPs/scan为0.125, 0.25和0.5的情况下取得高敏感度相对困难, M1和M2在上述情况下敏感度都低于77%。本文提出的多尺度融合分类模型在实验结果中表现突出, 在有效降低假阳性数量的同时保持了较高的敏感度。当每次扫描限制在0.125的平均假阳性数目的时候, M1和M2获得敏感度分别为63.3%和68.5%, 提出的融合模型获得74.1%的敏感度, 比前两者提升了10.8和5.6个百分点;同时还可以看出, 多尺度融合模型得到的敏感度, 在每个限制条件下都高于单一尺度模型。这一对比实验验证了融合模型能有效地提高敏感度和降低假阳率, 说明了包含不同层次上下文信息的网络可以相互补充, 它们的结合可以提高结节检测系统的性能。</p>
                </div>
                <div class="p1">
                    <p id="139">图5 (<i>P</i>值表示分类概率) 显示了利用多尺度融合模型识别真实肺结节的分类概率样例, 模型对形状多样的结节体现了良好的鉴别能力。</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907041_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 本文算法结节分类结果样例" src="Detail/GetImg?filename=images/JSJY201907041_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 本文算法结节分类结果样例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907041_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Examples of pulmonary nodule classification results of the proposed method</p>

                </div>
                <div class="p1">
                    <p id="141">进一步地, 本部分实验还将本文算法框架的结果与目前流行的算法框架<citation id="263" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>进行比较, 结果如图6所示。本文算法取得了0.874的CPM分数, 这一指标优于其他进行比较的流行算法。Xie等<citation id="264" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在结节检测和分类阶段都使用了二维的卷积神经网络, 在时间和空间的节省上优于其他流行算法, 从图6可以看出, 该方法在每个CT扫描件的平均假阳性数 (FPs/scan) 为0.125时, 取得了73.4%的较高敏感度, 而本文算法在0.125 FPs/scan时, 敏感度为74.1%, 略高于Xie等<citation id="265" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>的方法;相比其他流行算法, 虽然在FPs/scan大于2之后, DIAG CONVNET<citation id="266" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和JackFPR<citation id="267" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的敏感度更高, 但是本文算法在平均假阳性数量为0.125, 0.25, 0.5和1时, 均取得了更高敏感度, 例如, 在FPs/scan为0.25时, 敏感度大于84%。</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907041_142.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同算法的FROC曲线对比" src="Detail/GetImg?filename=images/JSJY201907041_142.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同算法的FROC曲线对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907041_142.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 FROC curves comparison of different algorithms</p>

                </div>
                <h3 id="143" name="143" class="anchor-tag">4 讨论</h3>
                <div class="p1">
                    <p id="144">使用本文提出的肺结节检测系统框架, 其表现性能在LUNA16数据集上得到验证。本文算法框架对候选结节的检测和假阳性去除两个阶段都作了改进和优化。候选结节检测部分, 结合传统的UNet和UNet++的优点, 获得了高敏感度的检测结果, 原因在于UNet++网络结构本身的特征再利用的性质, 使得它比传统UNet学习到更高效的特征;同时, 对UNet++网络结构的精简和深监督对象的改进以及加权的损失函数的引入, 也都有效提高了检测部分的算法性能;但是这一阶段, 只用到结节的二维信息, 在未来的研究工作中, 如果在检测部分也加入三维的图像信息, 可能对敏感度的提高有积极的作用。假阳性去除阶段, 使用了多尺度三维卷积神经网络的融合模型用于分类。三维的上下文信息对于结节的准确分类十分重要, 它能为网络自主学习提供空间维度的信息。同时, 网络本身的结构也很重要。在分类阶段, 本文使用到两种不同的网络结构来适应不同的输入。三维卷积神经网络的设计中, 为了得到具有更强结节鉴别能力的模型, 可以引入ResNet (Residual Neural Network) <citation id="268" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、DenseNet (Densely connected convolutional Network) <citation id="269" type="reference"><link href="230" rel="bibliography" /><link href="232" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>等经典卷积神经网络的层结构。分类过程中, 会出现结节直径很小或结节周围环境极为复杂的情况。针对这些结节, 使用一些特殊的数据增强方法或是设计一个适应更小尺度输入的网络结构, 可能会在一定程度上帮助提高识别率。</p>
                </div>
                <h3 id="145" name="145" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="146">肺结节自动检测系统对医生的辅助诊断意义重大。本文针对传统肺结节检测算法中检测敏感度低、假阳性数量大的问题提出了基于深度卷积神经网络的肺结节检测算法。创新地提出:1) 精简UNet++模型, 降低了模型复杂度, 同时保证高敏感度;2) 引入对最佳的分割输出层的深监督;3) 改进了基于Dice系数的损失函数;4) 设计了多尺度的三维深度卷积神经网络训练分类模型, 并融合模型结果来降低假阳率。经过多组对比实验, 所提改进算法的表现性能超过了传统的算法;但本文算法也存在一定的局限性, 在本文第4章讨论了所提算法的优点、不足和针对性的改进方案。</p>
                </div>
                <div class="p1">
                    <p id="147">本文提出的算法框架有较强的可迁移能力。理论上, 本文的算法也可以应用在其他图像数据集上和更多的医学图像处理任务当中。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="184">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300711418&amp;v=MTM1MjFyM0lKMXNSYmhvPU5pZk9mYks3SHRETnJJOUZZK29PQ0gweG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> MURPHY K, van GINNEKEN B, SCHILHAM A M, et al.A large-scale evaluation of automatic pulmonary nodule detection in chest CT using local image features and k-nearest-neighbour classification[J].Medical Image Analysis, 2009, 13 (5) :757-770.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large scale validation of the M5Llung CAD on heterogeneous CTdatasets">

                                <b>[2]</b> LOPEZ T, FIORINA E, PENNAZIO F, et al.Large scale validation of the M5L lung CAD on heterogeneous CT datasets[J].Medical Physics, 2015, 42 (4) :1477-1489.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=False-positive reduction on lung nodules detection in chest radiographs by ensemble of convolutional neural networks">

                                <b>[3]</b> LI C F, ZHU G C, WU X J, et al.False-positive reduction on lung nodules detection in chest radiographs by ensemble of convolutional neural networks[J].IEEE Access, 2018, 6 (99) :16060-16067.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201805023&amp;v=MjU5NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dWcnZBSWpYQlk3RzRIOW5NcW85SFo0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 吕晓琪, 吴凉, 谷宇, 等.基于三维卷积神经网络的低剂量CT肺结节检测[J].光学精密工程, 2018, 26 (5) :1211-1218. (LYU X Q, WU L, GU Y, et al.Detection of low dose CT pulmonary nodules based on 3D convolution neural network[J].Optics and Precision Engineering, 2018, 26 (5) :1211-1218.) 
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 DOU Q, CHEN H, YU L Q, et al.Multi-level contextual 3D CNNs for false positive reduction in pulmonary nodule detection[J].IEEE Transactions on Biomedical Engineering, 2017, 64 (7) :1558-1567.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESEB10EA11F8A97E877C4C399C5DC0A9A5&amp;v=MDY5MTkrQlh0TXh4RVVtVHNPUzNicjN4ZEJDckxsVE11YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3MjZ4S0E9TmlmT2ZjYktIOUc1M281RUV1Tg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> XIE H T, YANG D B, SUN N N, et al.Automated pulmonary nodule detection in CT images using deep convolutional neural networks[J].Pattern Recognition, 2019, 85:109-119.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pulmonary nodule classification with deep convolutional neural networks on computed tomography images">

                                <b>[7]</b> LI W, CAO P, ZHAO D Z, et al.Pulmonary nodule classification with deep convolutional neural networks on computed tomography images[J].Computational and Mathematical Methods in Medicine, 2016, 2016:6215085.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate pulmonary nodule detection in computed tomography images using deep convolutional neural networks">

                                <b>[8]</b> DING J, LI A X, HU Z Q, et al.Accurate pulmonary nodule detection in computed tomography images using deep convolutional neural networks[C]// Proceedings of the 2017 International Conference on Medical Computing and Computer-Assisted Intervention.Berlin:Springer, 2017:559-567.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES75987FC868EC3E9704EAA1312A88E06D&amp;v=Mjg4MjNUN2tvTU9YN2hyUkJFY2JyaFJienJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3NzI2eEtBPU5pZk9mYlM5RjluTDJmeE5ZdU42ZjM5TXhoRQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> KAMNITSAS, K, LEDIG C, NEWCOMBE V F, et al.Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation[J].Medical Image Analysis, 2016, 36:61-78.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks">

                                <b>[10]</b> DOU Q, CHEN H, YU L Q, et al.Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks[J].IEEE Transactions on Medical Imaging, 2016, 35 (5) :1182-1195.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES2080426029567673FF8336544C35F8FC&amp;v=MjYzMjJISXJZbEZadUlLQ25zL3lCVmxuRGQrUzNubnFCWkdlcmZpVGN6c0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3MjZ4S0E9TmlmT2ZiRzRGdA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> CHEN H, DOU Q, YU L Q, et al.VoxResNet:deep voxelwise residual networks for brain segmentation from 3D MR images [J].NeuroImage, 2018, 170:446-455.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">

                                <b>[12]</b> RONNEBERGER O, FISCHER P, BROX T.U-Net:convolutional networks for biomedical image segmentation[C]// Proceedings of the 2015 International Conference on Medical Computing and Computer-Assisted Intervention.Berlin:Springer, 2015:234-241.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=UNet++:a nested U-Net architecture for medical image segmentation">

                                <b>[13]</b> ZHOU Z W, MAHFUZUR RAHMAN SIDDIQUEE M, LIANG J M, et al.UNet++:a nested U-Net architecture for medical image segmentation[C]// Proceedings of the 2018 International Conference on Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support.Berlin:Springer, 2018:3-11.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deeply-supervised nets">

                                <b>[14]</b> LEE C Y, XIE S, GALLAGHER P, et al.Deeply-supervised nets [C]// Proceedings of the 2015 18th International Conference on Artificial Intelligence and Statistics.San Diego, CA:[s.n.], 2015:562-570.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalised Dice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations">

                                <b>[15]</b> SUDRE C H, LI W Q, VERCAUTEREN T, et al.Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations [C]// Proceedings of the 2017 International Conference on Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support.Berlin:Springer, 2017:240-248.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESCA987C3548DE267FA3D128C5502E1EA0&amp;v=MjAzMDJwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzcyNnhLQT1OaWZPZmNESkY5bkwzSXhBWU9ON2VYNC95R0JpNlV0OFNuZVJxUmMxZThlVk1NdWZDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> SETIO A A A, TRAVERSO A, DE BEL T, et al.Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images:the LUNA16 challenge [J].Medical Image Analysis, 2017, 42:1-13.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[17]</b> LOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// Proceedings of the 2015 International Conference on Machine Learning.New York:ACM, 2015:448-456.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans">

                                <b>[18]</b> ARMATO S G, MCLENNAN G, BIDAUT L, et al.The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) :a completed reference database of lung nodules on CT scans[J].Medical Physics, 2011, 38 (2) :915-931.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Delving deep into rectifiers:Surpassing Human-level performance on ImageNet classification">

                                <b>[19]</b> HE K M, ZHANG X Y, REN S Q, et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification[C]// ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1026-1034.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">

                                <b>[20]</b> SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">

                                <b>[21]</b> REN S Q, HE K M, GIRSHICK R, et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transaction on Pattern Analysis and Machine Intelligence, 2015, 39:1137-1149.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On combining computer-aided detection systems">

                                <b>[22]</b> NIEMEIJER M, LOOG M, ABRAMOFF M D, et al.On combining computer-aided detection systems[J].IEEE Transactions on Medical Imaging, 2011, 30:215-223.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[23]</b> HE K M, ZHANG X Y, SUN J, et al.Deep residual learning for image recognition[C]// CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:770-778.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">

                                <b>[24]</b> HUANG G, LIU Z, LAURENS V D M, et al.Densely connected convolutional networks[C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2017:2261-2269.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201902016&amp;v=MjU3MjhadVpzRnkvZ1ZydkFMejdCZDdHNEg5ak1yWTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 秦品乐, 李启, 曾建潮, 等.基于多尺度密集网络的肺结节图像检索算法研究[J].计算机应用, 2019, 39 (2) :392-397. (QIN P L, LI Q, ZENG J C, et al.Research on image retrieval algorithm of pulmonary nodules based on multi-scale dense network[J].Journal of Computer Applications, 2019, 39 (2) :392-397.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907041" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907041&amp;v=MDg3MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1ZydkFMejdCZDdHNEg5ak1xSTlCWlk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
