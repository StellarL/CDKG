<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136766133252500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905036%26RESULT%3d1%26SIGN%3dVNizV3sJFeohv%252fanJvudbQcUUsA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905036&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905036&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905036&amp;v=MDYwMDc0SDlqTXFvOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXI3Qkx6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="1 深蹲姿势检测方法 ">1 深蹲姿势检测方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="1.1 &lt;b&gt;特征定义&lt;/b&gt;">1.1 <b>特征定义</b></a></li>
                                                <li><a href="#60" data-title="1.2 &lt;b&gt;特征提取与计算&lt;/b&gt;">1.2 <b>特征提取与计算</b></a></li>
                                                <li><a href="#83" data-title="1.3 &lt;b&gt;算法流程&lt;/b&gt;">1.3 <b>算法流程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;健康个体深蹲时的标准角度范围 (平均值&lt;/b&gt;&#177;SD) "><b>表</b>1 <b>健康个体深蹲时的标准角度范围 (平均值</b>±SD) </a></li>
                                                <li><a href="#53" data-title="图1 标准与非标准深蹲姿势的比较">图1 标准与非标准深蹲姿势的比较</a></li>
                                                <li><a href="#62" data-title="图2 由Kinect获取的25个人体骨架关节点和创建的3D图像空间">图2 由Kinect获取的25个人体骨架关节点和创建的3D图像空间</a></li>
                                                <li><a href="#67" data-title="图3 计算躯干角度的关键步骤">图3 计算躯干角度的关键步骤</a></li>
                                                <li><a href="#85" data-title="图4 本文算法流程">图4 本文算法流程</a></li>
                                                <li><a href="#92" data-title="图5 深蹲事件中的阈值示例">图5 深蹲事件中的阈值示例</a></li>
                                                <li><a href="#96" data-title="图6 在50个不标准深蹲视频中, 非标准帧与总帧数的比率">图6 在50个不标准深蹲视频中, 非标准帧与总帧数的比率</a></li>
                                                <li><a href="#98" data-title="图7 深蹲时人体骨架中的五个关节点">图7 深蹲时人体骨架中的五个关节点</a></li>
                                                <li><a href="#100" data-title="图8 一个典型的非标准深蹲视频中的角度变化曲线图">图8 一个典型的非标准深蹲视频中的角度变化曲线图</a></li>
                                                <li><a href="#104" data-title="图9 四种典型的非标准深蹲视频中的角度变化曲线">图9 四种典型的非标准深蹲视频中的角度变化曲线</a></li>
                                                <li><a href="#105" data-title="图10 深蹲期间每一帧的时间消耗">图10 深蹲期间每一帧的时间消耗</a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;六种不同类型深蹲姿势的实验结果&lt;/b&gt;"><b>表</b>2 <b>六种不同类型深蹲姿势的实验结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="121">


                                    <a id="bibliography_1" title=" CHIU L Z.Sitting back in the squat[J].Strength and Conditioning Journal, 2009, 31 (6) :25-27." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sitting back in the squat">
                                        <b>[1]</b>
                                         CHIU L Z.Sitting back in the squat[J].Strength and Conditioning Journal, 2009, 31 (6) :25-27.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_2" title=" YAO L Y, MING W D, Cui H.A new Kinect approach to judge unhealthy sitting posture based on neck angle and torso angle[C]// Proceedings of the 2017 International Conference on Image and Graphics, LNCS 10666.Berlin:Springer-Verlag, 2017:340-350." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new Kinect approach to judge unhealthy sitting posture based on neck angle and torso angle">
                                        <b>[2]</b>
                                         YAO L Y, MING W D, Cui H.A new Kinect approach to judge unhealthy sitting posture based on neck angle and torso angle[C]// Proceedings of the 2017 International Conference on Image and Graphics, LNCS 10666.Berlin:Springer-Verlag, 2017:340-350.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_3" title=" FANG B, SUN F C, LIU H P, et al.3D human gesture capturing and recognition by the IMMU-based data glove[J].Neurocomputing, 2017, 277:198-207." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB5C3452D9F95DEE48AFB98E9357DF2B0&amp;v=MDMyNjRSOGlmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekxxK3dhQT1OaWZPZmNHOWJkTElxbzB4YlowR0NRaE11aElibTBrUFFYZVhwUkV3ZnNiaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         FANG B, SUN F C, LIU H P, et al.3D human gesture capturing and recognition by the IMMU-based data glove[J].Neurocomputing, 2017, 277:198-207.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_4" title=" FERRONE A, JIANG X, MAIOLO L, et al.A fabric-based wearable band for hand gesture recognition based on filament strain sensors:A preliminary investigation[C]// Proceedings of the 2016 IEEE Healthcare Innovation Point-of-Care Technologies Conference.Piscataway, NJ:IEEE, 2016:113-116." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fabric-based wearable band for hand gesture recognition based on filament strain sensors:A preliminary investigation">
                                        <b>[4]</b>
                                         FERRONE A, JIANG X, MAIOLO L, et al.A fabric-based wearable band for hand gesture recognition based on filament strain sensors:A preliminary investigation[C]// Proceedings of the 2016 IEEE Healthcare Innovation Point-of-Care Technologies Conference.Piscataway, NJ:IEEE, 2016:113-116.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_5" title=" WU D, SHAO L.Deep dynamic neural networks for gesture segmentation and recognition[C]// Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:552-571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep dynamic neural networks for gesture segmentation and recognition">
                                        <b>[5]</b>
                                         WU D, SHAO L.Deep dynamic neural networks for gesture segmentation and recognition[C]// Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:552-571.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_6" title=" LI Y, WANG X G, LIU W Y, et al.Deep attention network for joint hand gesture localization and recognition using static RGB-D images[J].Information Sciences, 2018, 441:66-78." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFD0832E94A28C3E004C8E085CFD682C9&amp;v=MjcyNzFERGJTY1I4bVdDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THErd2FBPU5pZk9mY1hNSHRuUHJmcE1ZSm9OQkE4NnVoWVQ3a3gxUFgvcXFXRg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         LI Y, WANG X G, LIU W Y, et al.Deep attention network for joint hand gesture localization and recognition using static RGB-D images[J].Information Sciences, 2018, 441:66-78.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_7" title=" 曾星, 孙备, 罗武胜, 等.基于深度传感器的坐姿检测系统[J].计算机科学, 2018, 45 (7) :237-242. (ZENG X, SUN B, LUO W S, et al.Sitting posture detection system based on depth sensor[J].Computer Science, 2018, 45 (7) :237-242.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201807041&amp;v=MDU1NjdmWnVac0Z5RG5VcjdBTHo3QmI3RzRIOW5NcUk5QlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         曾星, 孙备, 罗武胜, 等.基于深度传感器的坐姿检测系统[J].计算机科学, 2018, 45 (7) :237-242. (ZENG X, SUN B, LUO W S, et al.Sitting posture detection system based on depth sensor[J].Computer Science, 2018, 45 (7) :237-242.) 
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_8" title=" YAO L Y, MING W D, LU K Q.A new approach to fall detection based on the human torso motion model[J].Applied Sciences, 2017, 7 (10) :993." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new approach to fall detection based on the human torso motion model">
                                        <b>[8]</b>
                                         YAO L Y, MING W D, LU K Q.A new approach to fall detection based on the human torso motion model[J].Applied Sciences, 2017, 7 (10) :993.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_9" title=" BACCOUCHE M, MAMALET F, WOLF C, et al.Sequential deep learning for human action recognition[C]// Proceedings of the 2011 International Workshop on Human Behavior Unterstanding, LNCS 7065.Berlin:Springer-Verlag, 2011:29-39." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sequential deep learning for human action recognition">
                                        <b>[9]</b>
                                         BACCOUCHE M, MAMALET F, WOLF C, et al.Sequential deep learning for human action recognition[C]// Proceedings of the 2011 International Workshop on Human Behavior Unterstanding, LNCS 7065.Berlin:Springer-Verlag, 2011:29-39.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_10" title=" NG J Y, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:4694-4702." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond short snippets:Deep networks for video classification">
                                        <b>[10]</b>
                                         NG J Y, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:4694-4702.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_11" title=" 吴亮, 何毅, 梅雪, 等.基于时空兴趣点和概率潜动态条件随机场模型的在线行为识别方法[J].计算机应用, 2018, 38 (6) :1760-1764. (WU L, HE Y, MEI X, et al.Online behavior recognition using space-time interest points and probabilistic latent-dynamic conditional random field model[J].Journal of Computer Applications, 2018, 38 (6) :1760-1764.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806040&amp;v=MDM2MTQ5QlpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VcjdBTHo3QmQ3RzRIOW5NcVk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         吴亮, 何毅, 梅雪, 等.基于时空兴趣点和概率潜动态条件随机场模型的在线行为识别方法[J].计算机应用, 2018, 38 (6) :1760-1764. (WU L, HE Y, MEI X, et al.Online behavior recognition using space-time interest points and probabilistic latent-dynamic conditional random field model[J].Journal of Computer Applications, 2018, 38 (6) :1760-1764.) 
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_12" title=" 姬晓飞, 左鑫孟.基于关键帧特征库统计特征的双人交互行为识别[J].计算机应用, 2016, 36 (8) :2287-2291. (JI X F, ZUO X M.Human interaction recognition based on statistical features of key frame feature library[J].Journal of Computer Applications, 2016, 36 (8) :2287-2291.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201608042&amp;v=MDgxNjBMejdCZDdHNEg5Zk1wNDlCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyN0E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         姬晓飞, 左鑫孟.基于关键帧特征库统计特征的双人交互行为识别[J].计算机应用, 2016, 36 (8) :2287-2291. (JI X F, ZUO X M.Human interaction recognition based on statistical features of key frame feature library[J].Journal of Computer Applications, 2016, 36 (8) :2287-2291.) 
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_13" title=" KALIATAKIS G, STERGIOY A, VIDAKIS N.Conceiving human interaction by visualising depth data of head pose changes and emotion recognition via facial expressions[J].Computers, 2017, 6 (3) :25-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conceiving human interaction by visualising depth data of head pose changes and emotion recognition via facial expressions">
                                        <b>[13]</b>
                                         KALIATAKIS G, STERGIOY A, VIDAKIS N.Conceiving human interaction by visualising depth data of head pose changes and emotion recognition via facial expressions[J].Computers, 2017, 6 (3) :25-37.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_14" title=" MAITI S, REDDY S, RAHEJA J L.View invariant real-time gesture recognition[J].Optik—International Journal for Light and Electron Optics, 2015, 126 (23) :3737-3742." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122200144633&amp;v=MzI1NTZGaW5sVXIzSUtGd1ZheG89TmlmT2ZiSzlIOVBPclk5RlplOExDbjg2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         MAITI S, REDDY S, RAHEJA J L.View invariant real-time gesture recognition[J].Optik—International Journal for Light and Electron Optics, 2015, 126 (23) :3737-3742.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_15" title=" 张全贵, 蔡丰, 李志强.基于耦合多隐马尔可夫模型和深度图像数据的人体动作识别[J].计算机应用, 2018, 38 (2) :454-457. (ZHANG Q G, CAI F, LI Z Q.Human action recognition based on coupled multi-hidden Markov model and depth image data[J].Journal of Computer Applications, 2018, 38 (2) :454-457.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201802028&amp;v=MjE1MDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXI3QUx6N0JkN0c0SDluTXJZOUhiSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         张全贵, 蔡丰, 李志强.基于耦合多隐马尔可夫模型和深度图像数据的人体动作识别[J].计算机应用, 2018, 38 (2) :454-457. (ZHANG Q G, CAI F, LI Z Q.Human action recognition based on coupled multi-hidden Markov model and depth image data[J].Journal of Computer Applications, 2018, 38 (2) :454-457.) 
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_16" title=" 谈家谱, 徐文胜.基于Kinect的指尖检测与手势识别方法[J].计算机应用, 2015, 35 (6) :1795-1800. (TAN J P, XU W S.Fingertip detection and gesture recognition method based on Kinect[J].Journal of Computer Applications, 2015, 35 (6) :1795-1800.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201506060&amp;v=MTEyNTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VcjdBTHo3QmQ3RzRIOVRNcVk5RFpJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         谈家谱, 徐文胜.基于Kinect的指尖检测与手势识别方法[J].计算机应用, 2015, 35 (6) :1795-1800. (TAN J P, XU W S.Fingertip detection and gesture recognition method based on Kinect[J].Journal of Computer Applications, 2015, 35 (6) :1795-1800.) 
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_17" title=" CHOUBIK Y, MAHMOUDI A.Machine learning for real time poses classification using Kinect skeleton data[C]// Proceedings of the 2016 International Conference on Computer Graphics, Imaging and Visualization.Piscataway, NJ:IEEE, 2016:307-311." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning for real time poses classification using Kinect skeleton data">
                                        <b>[17]</b>
                                         CHOUBIK Y, MAHMOUDI A.Machine learning for real time poses classification using Kinect skeleton data[C]// Proceedings of the 2016 International Conference on Computer Graphics, Imaging and Visualization.Piscataway, NJ:IEEE, 2016:307-311.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_18" title=" WINWOOD P W, CRONIN J B, BROWN S R, et al.A biomechanical analysis of the heavy sprint-style sled pull and comparison with the back squat[J].International Journal of Sports Science and Coaching, 2015, 10 (5) :851-868." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SMUD&amp;filename=SMUD15121700000017&amp;v=MjE2NTlpclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0Z3VmF4bz1OaURlYXJLOUg5UE5xSTlGWk9zUERIMCtvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         WINWOOD P W, CRONIN J B, BROWN S R, et al.A biomechanical analysis of the heavy sprint-style sled pull and comparison with the back squat[J].International Journal of Sports Science and Coaching, 2015, 10 (5) :851-868.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_19" title=" STEVENS W R Jr, KOKOSZKA A Y, ANDEERSON A M, et al.Automated event detection algorithm for two squatting protocols[J].Gait and Posture, 2018, 59:253-257." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3CE538D4F823191FED4D57BEDE6C8201&amp;v=MjgwMTM3cWVDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THErd2FBPU5pZk9mYkRMYTlUUHAvdEJFdU1ORDMwd3ptQm1uanNKVFhpUTJXWkFmOEdjUg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         STEVENS W R Jr, KOKOSZKA A Y, ANDEERSON A M, et al.Automated event detection algorithm for two squatting protocols[J].Gait and Posture, 2018, 59:253-257.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 09:47</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1448-1452 DOI:10.11772/j.issn.1001-9081.2018102137            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于人体骨架的非标准深蹲姿势检测方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%96%BB%E9%9C%B2&amp;code=41746656&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">喻露</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E5%89%91%E9%94%8B&amp;code=17698733&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡剑锋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E7%A3%8A%E5%B2%B3&amp;code=31800365&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚磊岳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E6%98%8C%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0252160&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南昌大学信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%A5%BF%E7%A7%91%E6%8A%80%E5%AD%A6%E9%99%A2%E5%8D%8F%E5%90%8C%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83&amp;code=1016175&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江西科技学院协同创新中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对健身者在健身过程中因缺乏监督指导而导致姿势不正确甚至危及健康的问题, 提出了一种深蹲姿势实时检测的新方法。通过Kinect摄像头提取人体关节三维信息, 对健身中最常见的深蹲行为进行抽象与建模, 解决了计算机视觉技术对于细微动作变化难以检测的问题。首先, 通过Kinect摄像头捕获深度图像, 实时获取人体关节点的三维坐标;然后, 将深蹲姿势抽象为躯干角度、髋部角度、膝部角度和踝部角度, 并进行数字化建模, 逐帧记录下角度变化;最后, 在深蹲完成后, 采用阈值比较的方法, 计算一定时间段内非标准帧比率。如计算比率大于所给定阈值, 则判定此次深蹲为不标准;如低于阈值则为标准深蹲姿势。通过对六种不同类型的深蹲姿势进行实验, 结果表明, 该方法可检测出不同类型的非标准深蹲姿势, 并且在六种不同类型的深蹲姿势中平均识别率在90%以上, 能够对健身者起到提醒指导的作用。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E8%B9%B2%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深蹲检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A7%BF%E5%8A%BF%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姿势检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Kinect&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Kinect;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AA%A8%E6%9E%B6%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">骨架信息;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *喻露 (1994—) , 男, 江西宜春人, 硕士研究生, 主要研究方向:计算机视觉、行为识别、目标跟踪;电子邮箱lu_yu0413@163.com;
                                </span>
                                <span>
                                    胡剑锋 (1976—) , 男, 江西景德镇人, 教授, 博士, 主要研究方向:神经网络、脑电波、机器学习;;
                                </span>
                                <span>
                                    姚磊岳 (1982—) , 男, 浙江舟山人, 教授, 博士, 主要研究方向:计算机视觉、信息处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61762045);</span>
                                <span>江西省科技厅项目 (20171BAB202031) ;江西省科技厅科技攻关项目 (20171BBE50060) ;江西省科技厅科技计划专项重点研发项目 (20181BBE50018);</span>
                                <span>江西省博士后援助项目 (2017KY33);</span>
                                <span>江西省教育厅项目 (GJJ161143, GJJ151146);</span>
                                <span>南昌市科技局科技规划项目 (2016-ZCJHCXY-013);</span>
                    </p>
            </div>
                    <h1><b>Detection method of non-standard deep squat posture based on human skeleton</b></h1>
                    <h2>
                    <span>YU Lu</span>
                    <span>HU Jianfeng</span>
                    <span>YAO Leiyue</span>
            </h2>
                    <h2>
                    <span>Information Engineering School, Nanchang University</span>
                    <span>Center of Collaboration and Innovation, Jiangxi University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Concerning the problem that the posture is not correct and even endangers the health of body builder caused by the lack of supervision and guidance in the process of bodybuilding, a new method of real-time detection of deep squat posture was proposed. The most common deep squat behavior in bodybuilding was abstracted and modeled by three-dimensional information of human joints extracted through Kinect camera, solving the problem that computer vision technology is difficult to detect small movements. Firstly, Kinect camera was used to capture the depth images to obtain three-dimensional coordinates of human body joints in real time. Then, the deep squat posture was abstracted as torso angle, hip angle, knee angle and ankle angle, and the digital modeling was carried out to record the angle changes frame by frame. Finally, after completing the deep squat, a threshold comparison method was used to calculate the non-standard frame ratio in a certain period of time. If the calculated ratio was greater than the given threshold, the deep squat was judged as non-standard, otherwise judged as standard. The experiment results of six different types of deep squat show that the proposed method can detect different types of non-standard deep squat, and the average recognition rate is more than 90% of the six different types of deep squat, which can play a role in reminding and guiding bodybuilders.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20squat%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep squat detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=posture%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">posture detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Kinect&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Kinect;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depth%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depth image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=skeleton%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">skeleton information;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YU Lu, born in 1994, M. S. candidate. His research interests include computer vision, action recognition, object tracking. ;
                                </span>
                                <span>
                                    HU Jianfeng, born in 1976, Ph. D. , professor. His research interests include neural network, brain waves, machine learning. ;
                                </span>
                                <span>
                                    YAO Leiyue, born in 1982, Ph. D. , professor. His research interests include computer vision, information processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61762045);</span>
                                <span>the Project of Science and Technology Department of Jiangxi Province (20171BAB202031) ;the Science and Technology Research Project of Jiangxi Science and Technology Department (20171BBE50060) ;the Science and Technology Plan Special Key Research and Development Project of Jiangxi Science and Technology Department (20181BBE50018);</span>
                                <span>the Postdoctoral Assistance Project of Jiangxi Province (2017KY33);</span>
                                <span>the Project of Department of Education of Jiangxi Province (GJJ151146, GJJ161143);</span>
                                <span>the Science and Technology Planning Project of Nanchang Science and Technology Bureau (2016-ZCJHCXY-013);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">深蹲被称为力量训练之王, 是增加腿部和臀部力量的基本练习动作。保持标准的深蹲姿势可以训练到臀部、大腿, 并有利于下半身的骨骼、韧带和肌腱的锻炼。但是, 长期使用不标准的深蹲姿势不仅浪费健身者的时间, 而且还会增加韧带、半月板和膝盖受伤的风险。标准的深蹲姿势对于很多运动员都是较难掌握的<citation id="159" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。人们通常通过自己的主观意识来判断深蹲姿势是否标准, 此方法带有很强的个人色彩, 难以客观准确地对深蹲姿势进行判断;同时, 使用昂贵的费用聘请私人教练也使得健身成本增加, 且大部分健身者都没有经济条件聘请私人教练, 使得很多人对健身望而却步, 因此对深蹲姿势进行自动检测具有重要的实际意义, 能够使得这项最基本的练习动作被更多人掌握, 同时又可减少锻炼者因长期使用错误姿势而导致的严重后果。</p>
                </div>
                <div class="p1">
                    <p id="43">深蹲属于一种行为动作, 而关于行为动作领域的研究近些年来越来越多。有关领域目前的研究方法通常是基于可穿戴传感器和计算机视觉技术<citation id="160" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。这些研究方法大多用来完成手势识别<citation id="163" type="reference"><link href="125" rel="bibliography" /><link href="127" rel="bibliography" /><link href="129" rel="bibliography" /><link href="131" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation><sup></sup>、坐姿检测<citation id="161" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、摔倒检测<citation id="162" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、行为分类<citation id="164" type="reference"><link href="137" rel="bibliography" /><link href="139" rel="bibliography" /><link href="141" rel="bibliography" /><link href="143" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>等任务, 且都能够取得较好的效果。</p>
                </div>
                <div class="p1">
                    <p id="44">上述两类方法也有着不容忽视的缺点:首先, 可穿戴传感器会给使用者造成不适;此外, 由于受到挤压碰撞等外部因素, 可穿戴设备会逐渐损坏, 导致无法收集信息。而基于计算机视觉的方法大多需要经过训练, 训练过程是极度耗时的, 并且此类方法严重依赖于训练数据集, 而且深蹲是一种顺时动作, 且动作变化快, 一般的计算机视觉技术对于这种细微动作的变化较难检测, 因此很少有研究对运动姿势 (如深蹲) 是否标准, 行为是否准确提出疑问。</p>
                </div>
                <div class="p1">
                    <p id="45">Kinect深度传感器能够自动捕获人体的深度图像, 并实时跟踪人体骨架, 检测到细微的动作变化:一方面, Kinect获取的深度图像不同于彩色图像, 可以提供更多的空间信息, 同时又能保护个人隐私, 因此, 通过分析深度图像来识别和检测姿势的方法一直以来都备受关注;另一方面, 人体的骨骼特征也为行为识别、姿势检测等任务提供了重要的行为特征。Kinect因上述功能和其具有的精确性与实用性等特点, 已经使其成为一种多功能组件, 进而可以集成到日常生活的各种应用中<citation id="165" type="reference"><link href="145" rel="bibliography" /><link href="147" rel="bibliography" /><link href="149" rel="bibliography" /><link href="151" rel="bibliography" /><link href="153" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="46">本文利用Kinect深度传感器提出了一种基于骨架信息来检测非标准深蹲姿势的方法。首先, 针对深蹲姿势, 提出了使用躯干角度、髋部角度、膝部角度和踝部角度作为深蹲期间的4个代表性特征; 然后, 把深蹲过程分为4个阶段, 并使用关键帧检测技术, 对每一阶段的角度特征逐帧的计算和记录; 最后, 采用阈值比较的方法, 对深蹲姿势进行检测判断。该方法无需佩戴任何的可穿戴传感器, 不会给锻炼者带来不便, 且不需要使用训练数据集, 能够做到实时准确的检测。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag">1 深蹲姿势检测方法</h3>
                <h4 class="anchor-tag" id="48" name="48">1.1 <b>特征定义</b></h4>
                <div class="p1">
                    <p id="49">在对所提方法进行建模之前, 首先需要建立可用于区分标准深蹲姿势和非标准深蹲姿势的界限。本文中Winwood等<citation id="166" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>的研究结果被用来建模深蹲姿势。表1的数据显示了当健康个体深蹲时关节点应保持的角度范围 (其中SD (Standard Deviation) 为标准差) 。</p>
                </div>
                <div class="area_img" id="50">
                    <p class="img_tit"><b>表</b>1 <b>健康个体深蹲时的标准角度范围 (平均值</b>±SD)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Standard angle range when healthy individual deep squat (mean±SD) </p>
                    <p class="img_note"></p>
                    <table id="50" border="1"><tr><td><br />角度类型</td><td>角度范围</td><td></td><td>角度类型</td><td>角度范围</td></tr><tr><td><br />躯干角度</td><td>38.8±5.2</td><td></td><td>膝部角度</td><td>62.6±6.3</td></tr><tr><td><br />髋部角度</td><td>57.0±9.7</td><td></td><td>踝部角度</td><td>81.0±7.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="51">因此, 本文将上述4个角度提取为深蹲运动期间的4个代表性特征。</p>
                </div>
                <div class="p1">
                    <p id="52">图1显示了标准和非标准深蹲姿势的比较。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 标准与非标准深蹲姿势的比较" src="Detail/GetImg?filename=images/JSJY201905036_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 标准与非标准深蹲姿势的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Comparison of standard and non-standard deep squat postures</p>

                </div>
                <div class="p1">
                    <p id="54">图1 (a) 显示了典型的标准深蹲姿势, 其中考虑了表1中的4个角度。根据文献<citation id="167" type="reference">[<a class="sup">18</a>]</citation>中提出, 图1 (a) 中描述的4个角度定义如下:</p>
                </div>
                <div class="p1">
                    <p id="55">躯干角度 肩关节到髋关节与垂直轴的夹角;</p>
                </div>
                <div class="p1">
                    <p id="56">髋部角度 肩关节、髋关节和膝关节的内角;</p>
                </div>
                <div class="p1">
                    <p id="57">膝部角度 髋关节、膝关节和踝关节的内角;</p>
                </div>
                <div class="p1">
                    <p id="58">踝部角度 膝关节、踝关节和足关节的内角。</p>
                </div>
                <div class="p1">
                    <p id="59">如上述4个角度值中的任何一个不在标准范围内, 则非标准深蹲姿势可被检测到。图1 (b) 显示了3种典型的非标准深蹲姿势。详细的检测算法见1.3节。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.2 <b>特征提取与计算</b></h4>
                <div class="p1">
                    <p id="61">在RGB图像中提取人体骨架是一项艰巨繁琐的任务, 且仅能提供平面 (二维) 信息。而在Kinect中, 能够在深度图像中, 提取人体骨架的三维信息。如图2 (a) 所示, 通过Kinect所提供的API可以准确提取并跟踪人体骨架的总共25个关节点。如图2 (b) 所示, Kinect创建的是三维 (three-Dimensional, 3D) 的图像空间, 因此这25个关节点为3D坐标信息。对25个关节点进行组合相连即可组成相应的3D骨架坐标。本文方法所提出的深蹲过程中4个代表性角度特征, 可由这25个关节点中的肩关节、髋关节、膝关节、踝关节、足关节, 5个关节点构成, 并通过余弦定理, 计算其空间角度。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 由Kinect获取的25个人体骨架关节点和创建的3D图像空间" src="Detail/GetImg?filename=images/JSJY201905036_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 由Kinect获取的25个人体骨架关节点和创建的3D图像空间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Twenty-five joint points of human skeleton obtained and 3D image space created by Kinect</p>

                </div>
                <div class="p1">
                    <p id="63">对于躯干角度, 可采用肩关节和髋关节来计算得到。从肩关节到髋关节的矢量<b><i>SH</i></b>可使用式 (1) 计算:</p>
                </div>
                <div class="p1">
                    <p id="64"><b><i>SH</i></b>= (<i>X</i><sub><i>s</i></sub>-<i>X</i><sub><i>h</i></sub>, <i>Y</i><sub><i>s</i></sub>-<i>Y</i><sub><i>h</i></sub>, <i>Z</i><sub><i>s</i></sub>-<i>Z</i><sub><i>h</i></sub>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="65">其中:<i>S</i> (<i>X</i><sub><i>s</i></sub>, <i>Y</i><sub><i>s</i></sub>, <i>Z</i><sub><i>s</i></sub>) 和<i>H</i> (<i>X</i><sub><i>h</i></sub>, <i>Y</i><sub><i>h</i></sub>, <i>Z</i><sub><i>h</i></sub>) 分别表示肩关节和髋关节在3D空间中的坐标信息。</p>
                </div>
                <div class="p1">
                    <p id="66">躯干角度由重力矢量<b><i>GH</i></b>和矢量<b><i>SH</i></b>构成。其具体计算方式将在下文详述。图3显示了其关键步骤。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 计算躯干角度的关键步骤" src="Detail/GetImg?filename=images/JSJY201905036_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 计算躯干角度的关键步骤  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Key steps to calculate torso angle</p>

                </div>
                <div class="p1">
                    <p id="68">首先, 当Kinect检测到人体时, 即可获取人体的关节点, 将肩关节和髋关节相连便可提取矢量<b><i>SH</i></b>。然后在深度图像中绘制并计算矢量线, 如图3 (a) 所示。矢量线在3D空间坐标中的表示如图3 (b) 所示。</p>
                </div>
                <div class="p1">
                    <p id="69">其次, 可以通过几何中的矢量平移理论将矢量<b><i>SH</i></b>中的矢量点<b><i>H</i></b>移动到原点坐标, 这样就可以让<b><i>SH</i></b>在二维 (two-dimensional, 2D) 平面坐标中表示, 其最后状态如图3 (c) 所示。</p>
                </div>
                <div class="p1">
                    <p id="70">第三, 由于重力矢量总是垂直于地面, 因此<i>Y</i>轴上的任何点和原点坐标都可形成重力矢量。为了将重力矢量<b><i>GH</i></b>和矢量<b><i>SH</i></b>保持在相同的2D平面坐标中, 3D空间坐标中的重力点位置可以定义为<i>G</i> (<i>X</i><sub><i>h</i></sub>, 0, <i>Z</i><sub><i>h</i></sub>) , 其中<i>X</i>轴和<i>Z</i>轴的值与髋部相对应位置的值相等。使用式 (2) 计算重力矢量<b><i>GH</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="71"><b><i>GH</i></b>= (<i>X</i><sub><i>g</i></sub>-<i>X</i><sub><i>h</i></sub>, <i>Y</i><sub><i>g</i></sub>-<i>Y</i><sub><i>h</i></sub>, <i>Z</i><sub><i>g</i></sub>-<i>Z</i><sub><i>h</i></sub>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="72">由于<i>X</i><sub><i>g</i></sub>等于<i>X</i><sub><i>h</i></sub>, <i>Z</i><sub><i>g</i></sub>等于<i>Z</i><sub><i>h</i></sub>, 式 (2) 可以简化为式 (3) :</p>
                </div>
                <div class="p1">
                    <p id="73"><b><i>GH</i></b>= (0, -<i>Y</i><sub><i>h</i></sub>, 0)      (3) </p>
                </div>
                <div class="p1">
                    <p id="74">最后, 可以通过式 (4) 计算躯干角度:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>cos</mi></mrow><mo stretchy="false"> (</mo><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>cos</mi></mrow><mo stretchy="false"> (</mo><mrow><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>, </mo><mrow><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>⋅</mo><mrow><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Η</mi></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>|</mo></mrow><mo>×</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">对于髋部角度、踝部角度和膝部角度, 可直接使用空间向量法计算。由于这3个角度计算方法相同, 用膝部角度作为实例来说明其具体的计算方式。</p>
                </div>
                <div class="p1">
                    <p id="77">为了计算膝部角度, 可预先定义空间矢量<b><i>KH</i></b>和<b><i>KA</i></b>。<b><i>KH</i></b>是膝关节到髋关节的矢量, <b><i>KA</i></b>是膝关节到踝关节的矢量。这两个矢量可以通过式 (5) ～ (6) 计算得到:</p>
                </div>
                <div class="p1">
                    <p id="78"><b><i>KH</i></b>= (<i>X</i><sub><i>k</i></sub>-<i>X</i><sub><i>h</i></sub>, <i>Y</i><sub><i>k</i></sub>-<i>Y</i><sub><i>h</i></sub>, <i>Z</i><sub><i>k</i></sub>-<i>Z</i><sub><i>h</i></sub>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="79"><b><i>KA</i></b>= (<i>X</i><sub><i>k</i></sub>-<i>X</i><sub><i>a</i></sub>, <i>Y</i><sub><i>k</i></sub>-<i>Y</i><sub><i>a</i></sub>, <i>Z</i><sub><i>k</i></sub>-<i>Z</i><sub><i>a</i></sub>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="80">其中:<i>K</i> (<i>X</i><sub><i>k</i></sub>, <i>Y</i><sub><i>k</i></sub>, <i>Z</i><sub><i>k</i></sub>) 、<i>H</i> (<i>X</i><sub><i>h</i></sub>, <i>Y</i><sub><i>h</i></sub>, <i>Z</i><sub><i>h</i></sub>) 和<i>A</i> (<i>X</i><sub><i>a</i></sub>, <i>Y</i><sub><i>a</i></sub>, <i>Z</i><sub><i>a</i></sub>) 分别表示膝关节、髋关节和踝关节在3D空间中的坐标信息。然后, 即可用式 (7) 计算膝部角度。</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>cos</mi></mrow><mo stretchy="false"> (</mo><mi>b</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>cos</mi></mrow><mo stretchy="false"> (</mo><mrow><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>, </mo><mrow><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">A</mi></mrow><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>⋅</mo><mrow><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">A</mi></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>|</mo></mrow><mo>×</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">A</mi></mrow><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">同样的方法可以用来计算髋部角度和踝部角度。到此, 4个角度计算完毕, 接下来便可进行检测任务。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">1.3 <b>算法流程</b></h4>
                <div class="p1">
                    <p id="84">在本文实验中, <i>Kinect</i>深度传感器的帧频为30 <i>fps</i> (<i>frame</i>/<i>second</i>) 。算法流程如图4所示。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 本文算法流程" src="Detail/GetImg?filename=images/JSJY201905036_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 本文算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Flow chart of the proposed algorithm</i></p>

                </div>
                <div class="p1">
                    <p id="86">总体而言, 算法可分为两个步骤。图4中第一步定义了深蹲期间中的4个阶段, 并且详述了提取并计算算法中的4个代表性角度特征的关键步骤。首先, 当<i>Kinect</i>检测到人体时, 则可获取该实验者的深度图像并跟踪该实验者骨架; 接着, 对深蹲期间4个阶段的关键帧进行定义, 而关键帧检测是所有行为识别、姿势检测等方法的重大难题。<i>Stevens</i>等<citation id="168" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了一种深蹲事件自动检测算法, 该算法使用矢状面膝关节速度 (<i>Sagittal Plane Knee Velocity</i>, <i>SPKV</i>) 的绝对值阈值或峰值<i>SPKV</i>的相对值阈值。根据该算法, 深蹲事件可分为4个步骤:</p>
                </div>
                <div class="p1">
                    <p id="87">1) 深蹲开始 (<i>START</i>) :<i>SPKV</i>大于5 (°) /<i>s</i>时 (绝对阈值) 的帧 (图5 (<i>a</i>) ) 。</p>
                </div>
                <div class="p1">
                    <p id="88">2) 下降结束 (<i>DEnd</i>) :质心垂直速度在-25 <i>mm</i>/<i>s</i>～-10 <i>mm</i>/<i>s</i>时的帧, 并且在接下来的15帧中, 质心垂直速度平稳或大于等于-5 <i>mm</i>/<i>s</i> (图5 (<i>b</i>) ) 。</p>
                </div>
                <div class="p1">
                    <p id="89">3) 上升开始 (<i>Astart</i>) :<i>SPKV</i>从-5 (°) /<i>s</i>下降并继续变得更小时的帧 (图5 (<i>a</i>) ) 。</p>
                </div>
                <div class="p1">
                    <p id="90">4) 深蹲结束 (<i>END</i>) :<i>SPKV</i>增加到-5 (°) /<i>s</i>并接近0 (°) /<i>s</i>时的帧 (图5 (<i>a</i>) ) 。</p>
                </div>
                <div class="p1">
                    <p id="91">上述4个阶段完成后, 一次完整的深蹲运动即结束。由于深蹲运动最关键的阶段为下蹲到短暂停止时, 因此需要计算和记录4个角度的帧在<i>DEnd</i>和<i>Astart</i>之间 (图5) 。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 深蹲事件中的阈值示例" src="Detail/GetImg?filename=images/JSJY201905036_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 深蹲事件中的阈值示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Example of deep squat event thresholds</i></p>

                </div>
                <div class="p1">
                    <p id="93">深蹲姿势检测的关键步骤如图4中第二步所示。单次的深蹲为一种短时间运动, 为更好地观察角度变化, 实验者需要在下降结束 (<i>DEnd</i>) 时停留一段时间, 因此单个或几个不标准帧值不能作为判断标准, 本文采用阈值比较的方法。如图6所示, 当非标准帧与总帧数的比率大于20%时, 深蹲姿势为非标准的。也就是说, 非标准帧比率在深蹲完成时被计算且与给定阈值进行比较。当比率大于阈值 (20%) 时, 深蹲姿势被判断为非标准, 否则它是标准的。</p>
                </div>
                <h3 id="94" name="94" class="anchor-tag">2 实验结果与分析</h3>
                <div class="p1">
                    <p id="95">实验平台: <i>Kinect v</i>2+<i>emgu</i>.<i>cv</i> 3.1+<i>VisualStudio</i> 2013。在本文实验中, 测试数据包括6种不同类型的深蹲姿势, 其中包含8个标准深蹲姿势和5种不同类型总共68个非标准深蹲姿势。由于一次深蹲运动的时间较短, 视频的时间都在 10 <i>s</i> 以内。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 在50个不标准深蹲视频中, 非标准帧与总帧数的比率" src="Detail/GetImg?filename=images/JSJY201905036_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 在50个不标准深蹲视频中, 非标准帧与总帧数的比率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Ratio of non</i>-<i>standard frames to total frames in</i> 50 <i>non</i>-<i>standard deep squat videos</i></p>

                </div>
                <div class="p1">
                    <p id="97">在计算4个角度特征之前, 应首先保证找出深度图像中所需的人体5个关节点的确切位置。如图7所示, 肩关节、髋关节、膝关节、踝关节和足关节的确切位置可以在本文程序中正确找到, 其中, 图7 (<i>a</i>) 为进行标准深蹲运动时关节点在深度图像的位置, 图7 (<i>b</i>) 为进行非标准深蹲运动时关节点在深度图像的位置。此外, 为保证关节识别的准确性, 必须将<i>Kinect</i>安装在适当的位置。在本文实验中, <i>Kinect</i>与地面的距离为1.4 <i>m</i>;人与<i>Kinect</i>的距离为2.3～3.0 <i>m</i>;实验者 (三个男性和一个女性) 的年龄为20岁～25岁;实验者的身高为1.65～1.82 <i>m</i>;以及实验者的体重为45.6～92.4 <i>kg</i>。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 深蹲时人体骨架中的五个关节点" src="Detail/GetImg?filename=images/JSJY201905036_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 深蹲时人体骨架中的五个关节点  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Five joints in human skeleton during deep squat</i></p>

                </div>
                <div class="p1">
                    <p id="99">由于实验者在深蹲到下降结束后, 关节角度不会急剧变化, 所以, 包含非标准角度的单个或几个帧不能作为判断的基准。例如, 实验者在深蹲期间进行轻微移动以纠正姿势或调节舒适度。因此在实验中, 非标准帧的比率被用来作为判断基准。图8显示了一个典型的非标准深蹲运动视频中角度变化曲线, 其中总的角度帧为128帧, 包含有72个非标准帧和56个标准帧, 非标准帧比率达到43.75%。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 一个典型的非标准深蹲视频中的角度变化曲线图" src="Detail/GetImg?filename=images/JSJY201905036_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 一个典型的非标准深蹲视频中的角度变化曲线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Angle change curves of a typical non</i>-<i>standard deep squat video</i></p>

                </div>
                <div class="p1">
                    <p id="101">在大多数情况下, 非标准的深蹲姿势总是伴随着关节屈伸不足或屈伸过度, 以及严重的身体震颤和移动。图9显示了4种典型的非标准深蹲姿势的角度变化曲线。</p>
                </div>
                <div class="p1">
                    <p id="102">根据图9中的曲线图, 可推断出一般情况下, 深蹲期间髋部角度和躯干角度是相关的。从曲线图中可看出, 当膝部角度是标准的, 并且髋部角度增加时, 躯干角度将随之减小。当这两个角度中的其中一个不标准时, 非标准角度将会影响另一个角度, 因此非标准帧比率相对较高。然而, 其他关节的角度通常是不相关的。此外, 实验者在进行深蹲时, 除非身体摇晃或移动, 关节的角度变化较小, 曲线较平滑。</p>
                </div>
                <div class="p1">
                    <p id="103">本文方法会尽快检测出深蹲姿势是否标准, 因此可以及时协助健身者纠正姿势, 避免因长期使用错误姿势而导致的严重危害。由于仅计算4个特征, 该方法在时间效率方面具有很大优势。图10记录了在深蹲期间每一帧的时间消耗 (从自己收集的深蹲视频中随机选择的一个测试视频) 。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 四种典型的非标准深蹲视频中的角度变化曲线" src="Detail/GetImg?filename=images/JSJY201905036_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 四种典型的非标准深蹲视频中的角度变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 9 <i>Angle change curves of four typical non</i>-<i>standard deep squat videos</i></p>

                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905036_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 深蹲期间每一帧的时间消耗" src="Detail/GetImg?filename=images/JSJY201905036_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 深蹲期间每一帧的时间消耗  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905036_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 10 <i>Time consumption of each frame during deep squat</i></p>

                </div>
                <div class="p1">
                    <p id="106">实验结果表明, 该方法可有效快速地检测非标准深蹲姿势, 并区分非标准和标准深蹲姿势。实验细节如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="107">从表2可以知道, 本文方法可有效检测和判断大多数深蹲姿势。在几种不同类型的深蹲姿势中, 大部分的识别率都在90%以上, 平均识别率达到90.79%, 其中, 识别率最低的为深蹲期间身体移动时, 而这种行为在正常的深蹲运动期间是不常见的。因此, 虽然识别率仅为75%, 但可以预期该方法在实际应用中可以表现得更好。</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="109">本文提出了一种基于计算机视觉技术的非标准深蹲姿势的判断方法。首先, 使用<i>Kinect</i>深度摄像头捕获深度图像并提取人体骨架关节点的三维坐标信息;然后, 利用余弦定理计算深蹲姿势抽象化后的躯干角度、髋部角度、膝部角度和踝部角度四个代表性特征, 并记录其变化值;最后, 深蹲运动结束后计算非标准帧的比率, 并与实验得出的阈值进行对比以判断姿势是否标准。实验结果表明, 该方法可快速有效地检测出不同类型的非标准深蹲姿势, 并具有计算量低、鲁棒性高和时效性好等特点。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit"><b>表</b>2 <b>六种不同类型深蹲姿势的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Experimental results of six different types of deep squat posture</i></p>
                    <p class="img_note"></p>
                    <table id="110" border="1"><tr><td><br />深蹲姿势类型</td><td>检测数目</td><td>正确检测数目</td><td>准确率/%</td></tr><tr><td><br />标准深蹲姿势</td><td>8</td><td>8</td><td>100.00</td></tr><tr><td><br />膝部过度前倾</td><td>12</td><td>11</td><td>91.67</td></tr><tr><td><br />屈膝不足</td><td>13</td><td>12</td><td>92.31</td></tr><tr><td><br />屈膝过度</td><td>14</td><td>13</td><td>92.86</td></tr><tr><td><br />屈髋不足</td><td>13</td><td>13</td><td>100.00</td></tr><tr><td><br />身体移动</td><td>16</td><td>12</td><td>75.00</td></tr><tr><td><br />总计</td><td>76</td><td>69</td><td>90.79</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="121">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sitting back in the squat">

                                <b>[1]</b> CHIU L Z.Sitting back in the squat[J].Strength and Conditioning Journal, 2009, 31 (6) :25-27.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new Kinect approach to judge unhealthy sitting posture based on neck angle and torso angle">

                                <b>[2]</b> YAO L Y, MING W D, Cui H.A new Kinect approach to judge unhealthy sitting posture based on neck angle and torso angle[C]// Proceedings of the 2017 International Conference on Image and Graphics, LNCS 10666.Berlin:Springer-Verlag, 2017:340-350.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB5C3452D9F95DEE48AFB98E9357DF2B0&amp;v=MTU5MTZOaWZPZmNHOWJkTElxbzB4YlowR0NRaE11aElibTBrUFFYZVhwUkV3ZnNiaVI4aWZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh6THErd2FBPQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> FANG B, SUN F C, LIU H P, et al.3D human gesture capturing and recognition by the IMMU-based data glove[J].Neurocomputing, 2017, 277:198-207.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fabric-based wearable band for hand gesture recognition based on filament strain sensors:A preliminary investigation">

                                <b>[4]</b> FERRONE A, JIANG X, MAIOLO L, et al.A fabric-based wearable band for hand gesture recognition based on filament strain sensors:A preliminary investigation[C]// Proceedings of the 2016 IEEE Healthcare Innovation Point-of-Care Technologies Conference.Piscataway, NJ:IEEE, 2016:113-116.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep dynamic neural networks for gesture segmentation and recognition">

                                <b>[5]</b> WU D, SHAO L.Deep dynamic neural networks for gesture segmentation and recognition[C]// Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:552-571.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFD0832E94A28C3E004C8E085CFD682C9&amp;v=MTYzODFjWE1IdG5QcmZwTVlKb05CQTg2dWhZVDdreDFQWC9xcVdGRERiU2NSOG1XQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekxxK3dhQT1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> LI Y, WANG X G, LIU W Y, et al.Deep attention network for joint hand gesture localization and recognition using static RGB-D images[J].Information Sciences, 2018, 441:66-78.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201807041&amp;v=MTk3ODl1WnNGeURuVXI3QUx6N0JiN0c0SDluTXFJOUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 曾星, 孙备, 罗武胜, 等.基于深度传感器的坐姿检测系统[J].计算机科学, 2018, 45 (7) :237-242. (ZENG X, SUN B, LUO W S, et al.Sitting posture detection system based on depth sensor[J].Computer Science, 2018, 45 (7) :237-242.) 
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new approach to fall detection based on the human torso motion model">

                                <b>[8]</b> YAO L Y, MING W D, LU K Q.A new approach to fall detection based on the human torso motion model[J].Applied Sciences, 2017, 7 (10) :993.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sequential deep learning for human action recognition">

                                <b>[9]</b> BACCOUCHE M, MAMALET F, WOLF C, et al.Sequential deep learning for human action recognition[C]// Proceedings of the 2011 International Workshop on Human Behavior Unterstanding, LNCS 7065.Berlin:Springer-Verlag, 2011:29-39.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond short snippets:Deep networks for video classification">

                                <b>[10]</b> NG J Y, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:4694-4702.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806040&amp;v=MjQ2NDNVUjdxZlp1WnNGeURuVXI3QUx6N0JkN0c0SDluTXFZOUJaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 吴亮, 何毅, 梅雪, 等.基于时空兴趣点和概率潜动态条件随机场模型的在线行为识别方法[J].计算机应用, 2018, 38 (6) :1760-1764. (WU L, HE Y, MEI X, et al.Online behavior recognition using space-time interest points and probabilistic latent-dynamic conditional random field model[J].Journal of Computer Applications, 2018, 38 (6) :1760-1764.) 
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201608042&amp;v=MjM4MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyN0FMejdCZDdHNEg5Zk1wNDlCWm8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 姬晓飞, 左鑫孟.基于关键帧特征库统计特征的双人交互行为识别[J].计算机应用, 2016, 36 (8) :2287-2291. (JI X F, ZUO X M.Human interaction recognition based on statistical features of key frame feature library[J].Journal of Computer Applications, 2016, 36 (8) :2287-2291.) 
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conceiving human interaction by visualising depth data of head pose changes and emotion recognition via facial expressions">

                                <b>[13]</b> KALIATAKIS G, STERGIOY A, VIDAKIS N.Conceiving human interaction by visualising depth data of head pose changes and emotion recognition via facial expressions[J].Computers, 2017, 6 (3) :25-37.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122200144633&amp;v=MjEwMDNJS0Z3VmF4bz1OaWZPZmJLOUg5UE9yWTlGWmU4TENuODZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyMw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> MAITI S, REDDY S, RAHEJA J L.View invariant real-time gesture recognition[J].Optik—International Journal for Light and Electron Optics, 2015, 126 (23) :3737-3742.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201802028&amp;v=MTkxMzFzRnlEblVyN0FMejdCZDdHNEg5bk1yWTlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 张全贵, 蔡丰, 李志强.基于耦合多隐马尔可夫模型和深度图像数据的人体动作识别[J].计算机应用, 2018, 38 (2) :454-457. (ZHANG Q G, CAI F, LI Z Q.Human action recognition based on coupled multi-hidden Markov model and depth image data[J].Journal of Computer Applications, 2018, 38 (2) :454-457.) 
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201506060&amp;v=MTk1NzVEWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyN0FMejdCZDdHNEg5VE1xWTk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 谈家谱, 徐文胜.基于Kinect的指尖检测与手势识别方法[J].计算机应用, 2015, 35 (6) :1795-1800. (TAN J P, XU W S.Fingertip detection and gesture recognition method based on Kinect[J].Journal of Computer Applications, 2015, 35 (6) :1795-1800.) 
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning for real time poses classification using Kinect skeleton data">

                                <b>[17]</b> CHOUBIK Y, MAHMOUDI A.Machine learning for real time poses classification using Kinect skeleton data[C]// Proceedings of the 2016 International Conference on Computer Graphics, Imaging and Visualization.Piscataway, NJ:IEEE, 2016:307-311.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SMUD&amp;filename=SMUD15121700000017&amp;v=Mjc5ODRyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRndWYXhvPU5pRGVhcks5SDlQTnFJOUZaT3NQREgwK29CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> WINWOOD P W, CRONIN J B, BROWN S R, et al.A biomechanical analysis of the heavy sprint-style sled pull and comparison with the back squat[J].International Journal of Sports Science and Coaching, 2015, 10 (5) :851-868.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3CE538D4F823191FED4D57BEDE6C8201&amp;v=MDcyNDRZZk9HUWxmQnJMVTA1dHBoekxxK3dhQT1OaWZPZmJETGE5VFBwL3RCRXVNTkQzMHd6bUJtbmpzSlRYaVEyV1pBZjhHY1I3cWVDT052RlNpV1dyN0pJRnBtYUJ1SA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> STEVENS W R Jr, KOKOSZKA A Y, ANDEERSON A M, et al.Automated event detection algorithm for two squatting protocols[J].Gait and Posture, 2018, 59:253-257.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905036" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905036&amp;v=MDYwMDc0SDlqTXFvOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXI3Qkx6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
