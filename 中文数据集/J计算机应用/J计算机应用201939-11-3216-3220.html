<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136445452315000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911019%26RESULT%3d1%26SIGN%3dqZKHu0rjEgT%252bVi5Vj2F0T8DXbWM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911019&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911019&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911019&amp;v=MTY5MDBGckNVUjdxZlp1WnNGeW5uVkw3Skx6N0JkN0c0SDlqTnJvOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="1.1 &lt;b&gt;卷积神经网络&lt;/b&gt;">1.1 <b>卷积神经网络</b></a></li>
                                                <li><a href="#63" data-title="1.2 &lt;b&gt;时间建模方法&lt;/b&gt;">1.2 <b>时间建模方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="2 本文方法 ">2 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="2.1 &lt;b&gt;特征提取&lt;/b&gt;">2.1 <b>特征提取</b></a></li>
                                                <li><a href="#76" data-title="2.2 &lt;b&gt;时空正则化&lt;/b&gt;">2.2 <b>时空正则化</b></a></li>
                                                <li><a href="#86" data-title="2.3 &lt;b&gt;帧级正则化&lt;/b&gt;">2.3 <b>帧级正则化</b></a></li>
                                                <li><a href="#91" data-title="2.4 &lt;b&gt;损失函数&lt;/b&gt;">2.4 <b>损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="3 实验与结果 ">3 实验与结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#102" data-title="3.1 &lt;b&gt;实验环境和参数设置&lt;/b&gt;">3.1 <b>实验环境和参数设置</b></a></li>
                                                <li><a href="#104" data-title="3.2 &lt;b&gt;数据集&lt;/b&gt;">3.2 <b>数据集</b></a></li>
                                                <li><a href="#107" data-title="3.3 &lt;b&gt;评价指标&lt;/b&gt;">3.3 <b>评价指标</b></a></li>
                                                <li><a href="#109" data-title="3.4 &lt;b&gt;在&lt;/b&gt;&lt;i&gt;MARS&lt;/i&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;DukeMTMC&lt;/i&gt;-&lt;i&gt;ReID&lt;/i&gt;&lt;b&gt;数据集中评估&lt;/b&gt;">3.4 <b>在</b><i>MARS</i><b>和</b><i>DukeMTMC</i>-<i>ReID</i><b>数据集中评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="图1 ResNet- 50网络的结构">图1 ResNet- 50网络的结构</a></li>
                                                <li><a href="#66" data-title="图2 基于时间建模方法的原理框图">图2 基于时间建模方法的原理框图</a></li>
                                                <li><a href="#72" data-title="图3 本文方法的整体框图">图3 本文方法的整体框图</a></li>
                                                <li><a href="#111" data-title="图4 在&lt;i&gt;MARS&lt;/i&gt;数据集上行人再识别的结果">图4 在<i>MARS</i>数据集上行人再识别的结果</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;本文方法中各个组成部分的性能比较 &lt;/b&gt;"><b>表</b>1 <b>本文方法中各个组成部分的性能比较 </b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同视频序列长度的性能比较 &lt;/b&gt;"><b>表</b>2 <b>不同视频序列长度的性能比较 </b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;几种方法在&lt;/b&gt;MARS&lt;b&gt;数据集中的性能比较 &lt;/b&gt;"><b>表</b>3 <b>几种方法在</b>MARS<b>数据集中的性能比较 </b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;几种方法在&lt;/b&gt; DukeMTMC-ReID&lt;b&gt;数据集中的性能比较 &lt;/b&gt;"><b>表</b>4 <b>几种方法在</b> DukeMTMC-ReID<b>数据集中的性能比较 </b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="155">


                                    <a id="bibliography_1" title=" 李幼蛟,卓力,张菁,等.行人再识别技术综述[J].自动化学报,2018,44(9):1554-1568.(LI Y J,ZHUO L,ZHANG J,et al.A survey of person re-identification[J].Acta Automatica Sinica,2018,44(9):1554-1568.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201809002&amp;v=MTY5NDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVkw3SktDTGZZYkc0SDluTXBvOUZab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         李幼蛟,卓力,张菁,等.行人再识别技术综述[J].自动化学报,2018,44(9):1554-1568.(LI Y J,ZHUO L,ZHANG J,et al.A survey of person re-identification[J].Acta Automatica Sinica,2018,44(9):1554-1568.)
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_2" title=" MCLAUGHLIN N,DEL RINCON J M,MILLER P.Recurrent convolutional network for video-based person re-identification[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1325-1334." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent convolutional network for video-based person reidentification">
                                        <b>[2]</b>
                                         MCLAUGHLIN N,DEL RINCON J M,MILLER P.Recurrent convolutional network for video-based person re-identification[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1325-1334.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_3" title=" WU Z,WANG X,JIANG Y G,et al.Modeling spatial-temporal clues in a hybrid deep learning framework for video classification[C]// Proceedings of the 23rd ACM International Conference on Multimedia.New York:ACM,2015:461-470." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling spatial-temporal clues in a hybrid deep learning framework for video classification">
                                        <b>[3]</b>
                                         WU Z,WANG X,JIANG Y G,et al.Modeling spatial-temporal clues in a hybrid deep learning framework for video classification[C]// Proceedings of the 23rd ACM International Conference on Multimedia.New York:ACM,2015:461-470.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_4" title=" LIU Y,YAN J,OUYANG W.Quality aware network for set to set recognition[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4694-4703." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quality Aware Network for Set to Set Recognition">
                                        <b>[4]</b>
                                         LIU Y,YAN J,OUYANG W.Quality aware network for set to set recognition[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4694-4703.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_5" title=" ZHOU Z,HUANG Y,WANG W,et al.See the forest for the trees:Joint spatial and temporal recurrent neural networks for video-based person re-identification[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4747-4756." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=See the forest for the trees:joint spatial and temporal recurrent neural networks in video-based person re-identification">
                                        <b>[5]</b>
                                         ZHOU Z,HUANG Y,WANG W,et al.See the forest for the trees:Joint spatial and temporal recurrent neural networks for video-based person re-identification[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4747-4756.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     KARPATHY A,TODERICI G,SHETTY S,et al.Large-scale video classification with convolutional neural networks[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2014:1725-1732.</a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_7" title=" DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database[C]// Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:a large-scale hierarchical image database">
                                        <b>[7]</b>
                                         DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database[C]// Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2009:248-255.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.</a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_9" title=" YOU J,WU A,LI X,et al.Top-push video-based person re-identification[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1345-1353." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Top-push video-based person re-identification">
                                        <b>[9]</b>
                                         YOU J,WU A,LI X,et al.Top-push video-based person re-identification[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1345-1353.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_10" title=" YAN Y,NI B,SONG Z,et al.Person re-identification via recurrent feature aggregation[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:701-716." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person Re-identification via recurrent feature aggregation">
                                        <b>[10]</b>
                                         YAN Y,NI B,SONG Z,et al.Person re-identification via recurrent feature aggregation[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:701-716.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_11" title=" XU K,BA J,KIROS R,et al.Show,attend and tell:Neural image caption generation with visual attention[C]// Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:International Machine Learning Society,2015:2048-2057." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention">
                                        <b>[11]</b>
                                         XU K,BA J,KIROS R,et al.Show,attend and tell:Neural image caption generation with visual attention[C]// Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:International Machine Learning Society,2015:2048-2057.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_12" title=" HERMANS A,BEYR L,LEIBE B.In defense of the triplet loss for person re-identification[EB/OL].[2017- 11- 21].http://arxiv.org/pdf/1703.07737." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In defense of the triplet loss for person re-identification">
                                        <b>[12]</b>
                                         HERMANS A,BEYR L,LEIBE B.In defense of the triplet loss for person re-identification[EB/OL].[2017- 11- 21].http://arxiv.org/pdf/1703.07737.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_13" title=" KINGMA D P,BA J.Adam:a method for stochastic optimization[EB/OL].[2017- 01- 30].http://csce.uark.edu/～mgashler/ml/2018_spring/r3/adam.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">
                                        <b>[13]</b>
                                         KINGMA D P,BA J.Adam:a method for stochastic optimization[EB/OL].[2017- 01- 30].http://csce.uark.edu/～mgashler/ml/2018_spring/r3/adam.pdf.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_14" title=" ZHENG L,BIE Z,SUN Y,et al.Mars:a video benchmark for large-scale person re-identification[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:868-884." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mars:A video benchmark for large-scale person re-identification">
                                        <b>[14]</b>
                                         ZHENG L,BIE Z,SUN Y,et al.Mars:a video benchmark for large-scale person re-identification[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:868-884.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_15" title=" ZHENG Z,ZHENG L,YANG Y.Unlabeled samples generated by GAN improve the person re-identification baseline in vitro[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2017:3754-3762." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unlabeled samples generated by gan improve the person re-identification baseline in vitro">
                                        <b>[15]</b>
                                         ZHENG Z,ZHENG L,YANG Y.Unlabeled samples generated by GAN improve the person re-identification baseline in vitro[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2017:3754-3762.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_16" title=" RISTANI E,SOLERA F,ZOU R,et al.Performance measures and a data set for multi-target,multi-camera tracking[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:17-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance measures and a data set for multi-target,multi-camera tracking">
                                        <b>[16]</b>
                                         RISTANI E,SOLERA F,ZOU R,et al.Performance measures and a data set for multi-target,multi-camera tracking[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:17-35.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_17" title=" LI D,CHEN X,ZHANG Z,et al.Learning deep context-aware features over body and latent parts for person re-identification[C]// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:384-393." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification">
                                        <b>[17]</b>
                                         LI D,CHEN X,ZHANG Z,et al.Learning deep context-aware features over body and latent parts for person re-identification[C]// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:384-393.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_18" title=" XIAO Q,LUO H,ZHANG C.Margin sample mining loss:a deep learning based method for person re-identification[EB/OL].[2017- 10- 07].http://arxiv.org/pdf/1710.00478." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Margin sample mining loss:a deep learning based method for person re-identification">
                                        <b>[18]</b>
                                         XIAO Q,LUO H,ZHANG C.Margin sample mining loss:a deep learning based method for person re-identification[EB/OL].[2017- 10- 07].http://arxiv.org/pdf/1710.00478.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_19" title=" LI S,BAK S,CARR P,et al.Diversity regularized spatiotemporal attention for video-based person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:369-378." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diversity regularized spatiotemporal attention for video-based person re-identification">
                                        <b>[19]</b>
                                         LI S,BAK S,CARR P,et al.Diversity regularized spatiotemporal attention for video-based person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:369-378.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_20" title=" LI W,ZHU X,GONG S.Harmonious attention network for person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2285-2294." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Harmonious Attention Network for Person Re-Identification">
                                        <b>[20]</b>
                                         LI W,ZHU X,GONG S.Harmonious attention network for person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2285-2294.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_21" title=" LIN Y,ZHENG L,ZHENG Z,et al.Improving person re-identification by attribute and identity learning[J].Pattern Recognition,2019,95:151-161." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving person re-identification by attribute and identity learning">
                                        <b>[21]</b>
                                         LIN Y,ZHENG L,ZHENG Z,et al.Improving person re-identification by attribute and identity learning[J].Pattern Recognition,2019,95:151-161.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_22" title=" CHEN D,LI H,XIAO T,et al.Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:1169-1178." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding">
                                        <b>[22]</b>
                                         CHEN D,LI H,XIAO T,et al.Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:1169-1178.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_23" title=" CHANG X,HOSPEDALES T M,XIANG T.Multi-level factorisation net for person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2109-2118." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-Level Factorisation Net for Person Re-Identification">
                                        <b>[23]</b>
                                         CHANG X,HOSPEDALES T M,XIANG T.Multi-level factorisation net for person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2109-2118.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_24" title=" CHEN Y,ZHU X,GONG S.Person re-identification by deep learning multi-scale representations[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2590-2600." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by deep learning multi-scale representations">
                                        <b>[24]</b>
                                         CHEN Y,ZHU X,GONG S.Person re-identification by deep learning multi-scale representations[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2590-2600.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_25" title=" 李姣,张晓晖,朱虹,等.多置信度重排序的行人再识别算法[J].模式识别与人工智能,2017,30(11):995-1002.(LI J,ZHANG X H,ZHU H,et al.Person re-identification via multiple confidences re-ranking[J].Pattern Recognition and Artificial Intelligence,2017,30(11):995-1002.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201711005&amp;v=MTI4MzN5bm5WTDdKS0Q3WWJMRzRIOWJOcm85RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         李姣,张晓晖,朱虹,等.多置信度重排序的行人再识别算法[J].模式识别与人工智能,2017,30(11):995-1002.(LI J,ZHANG X H,ZHU H,et al.Person re-identification via multiple confidences re-ranking[J].Pattern Recognition and Artificial Intelligence,2017,30(11):995-1002.)
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-23 13:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3216-3220 DOI:10.11772/j.issn.1001-9081.2019051084            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于时空正则化的视频序列中行人的再识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E4%BF%9D%E6%88%90&amp;code=43224068&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘保成</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B4%E7%87%95&amp;code=10570781&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朴燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E6%82%A6&amp;code=43224069&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐悦</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%95%BF%E6%98%A5%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0103753&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长春理工大学电子信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于现实复杂情况中各种因素的干扰,行人再识别的过程中可能出现识别错误等问题。为了提高行人再识别的准确性,提出了一种基于时空正则化的行人再识别算法。首先,利用ResNet-50网络对输入的视频序列逐帧进行特征提取,将一系列帧级特征输入到时空正则化网络并产生对应的权重分数;然后,对帧级特征使用加权平均得到视频序列级特征,为避免权重分数聚集在一帧,使用帧级正则化来限制帧间差异;最后,通过最小化损失得到最优结果。在DukeMTMC-ReID和MARS数据集中做了大量的测试,实验结果表明,所提方法与Triplet算法相比能够有效提高行人再识别的平均精度(mAP)和准确率,并且对于人体姿势变化、视角变化和相似外观目标的干扰具有出色的性能表现。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人再识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E9%97%B4%E5%BB%BA%E6%A8%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时间建模;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘保成(1995—),男,吉林白山人,硕士研究生,CCF会员,主要研究方向:机器学习、计算机视觉;;
                                </span>
                                <span>
                                    *朴燕(1965—),女,吉林长春人,教授,博士,主要研究方向:计算机视觉、模式识别,电子邮箱,piaoyan@cust.edu.cn;
                                </span>
                                <span>
                                    唐悦(1994—),女,吉林长春人,硕士研究生,主要研究方向:深度学习、计算机视觉。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>吉林省科技支撑项目(20180201091GX);</span>
                                <span>吉林省科技创新中心项目(20180623039TC);</span>
                    </p>
            </div>
                    <h1><b>Person re-identification in video sequence based on spatial-temporal regularization</b></h1>
                    <h2>
                    <span>LIU Baocheng</span>
                    <span>PIAO Yan</span>
                    <span>TANG Yue</span>
            </h2>
                    <h2>
                    <span>College of Electronic Information Engineering, Changchun University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Due to the interference of various factors in the complex situation of reality, the errors may occur in the person re-identification. To improve the accuracy of person re-identification, a person re-identification algorithm based on spatial-temporal regularization was proposed. Firstly, the ResNet-50 network was used to extract the features of the input video sequence frame by frame, and the series of frame-level features were input into the spatial-temporal regularization network to generate corresponding weight scores. Then the weighted average was performed on the frame-level features to obtain the sequence-level features. To avoid weight scores from being aggregated in one frame, frame-level regularization was used to limit the difference between frames. Finally, the optimal results were obtained by minimizing the losses. A large number of tests were performed on MARS and DukeMTMC-ReID datasets. The experimental results show that the mean Average Precision(mAP) and the accuracy can be effectively improved by the proposed algorithm compared with Triplet algorithm. And the proposed algorithm has excellent performance for human posture variation, viewing angle changes and interference with similar appearance targets.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=person%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">person re-identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=temporal%20modeling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">temporal modeling;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LIU Baocheng, born in 1995, M. S. candidate. His research interests include machine learning, computer vision. ;
                                </span>
                                <span>
                                    PIAO Yan, born in 1965, Ph. D., professor. Her research interests include computer vision, pattern recognition. ;
                                </span>
                                <span>
                                    TANG Yue, born in 1994, M. S. candidate. Her research interests include deep learning, computer vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Science and Technology Support Project of Jilin Province(20180201091GX);</span>
                                <span>the Project of Jilin Provincial Science and Technology Innovation Center(20180623039TC);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="54">近年来,随着智能视频分析的迅速发展和国家对公共安防监控的重视,行人再识别技术已成为视频监控领域中至关重要的一部分<citation id="205" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。行人再识别来源于多摄像机目标跟踪,主要处理非重叠摄像机间重新确定特定行人的问题,即判断在不同时间、不同地点出现在不同摄像机的行人是否为同一个人。</p>
                </div>
                <div class="p1">
                    <p id="55">基于视频的行人再识别是当前研究的热点,现阶段的大多数方法都是基于深度神经网络和时间信息建模: McLaughlin等<citation id="206" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>首先提出通过循环神经网络(Recurrent Neural Network, RNN)对帧之间的时间信息建模; Wu等<citation id="207" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>通过训练卷积网络和循环层,从视频中提取外观特征和时空特征,并构建混合网络融合两种类型的特征; Liu等<citation id="208" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>设计了一个质量感知网络(Quality Aware Network, QAN)用于聚合时序特征; Zhou等<citation id="209" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出用RNN和时间注意方法对行人进行再识别; Karpathy等<citation id="210" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>设计了一个卷积神经网络(Convolutional Neural Network, CNN)来提取特征,并使用时间池化方法来聚合特征。由于不同相机拍摄的视频图像会因光照变化、遮挡或人体姿势变化等因素影响,目标会出现较大的外观变化,使得行人再识别仍然是一个具有挑战性的问题。</p>
                </div>
                <div class="p1">
                    <p id="56">本文针对行人再识别的准确性,提出了一种基于时空正则化的行人再识别算法。利用ResNet- 50网络逐帧进行特征提取,帧级特征经过时空正则化网络产生相应的权重分数,通过加权平均将帧级特征融合为一个序列级特征; 同时使用帧级正则化避免权重分数聚集在一帧,最终通过最小化损失函数找到最佳的识别结果。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="58" name="58">1.1 <b>卷积神经网络</b></h4>
                <div class="p1">
                    <p id="59">最近几年,深度学习在计算机视觉领域取得了出色的成绩。与人工设计特征的方法相比,基于深度学习的方法可以从大量数据中自主学习得到图像的特征信息,更加符合人工智能的要求。</p>
                </div>
                <div class="p1">
                    <p id="60">在计算机视觉领域中,CNN是应用最广泛的深度学习模型之一, CNN通过在卷积层中的非线性叠加可以得到具有高级语义信息的特征,并且其每个卷积层都可以得到输入图像的不同特征表达。在行人再识别的过程中,利用CNN提取目标行人更精准和更具有判别性的特征,可以获得更多关于目标行人的信息,有利于提高识别结果的准确性。</p>
                </div>
                <div class="p1">
                    <p id="61">本文使用在ImageNet数据集<citation id="211" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>上预训练的ResNet- 50<citation id="212" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>卷积神经网络对输入的视频序列进行特征提取。ResNet- 50网络深度为50层,其中包含5个卷积层,即Conv1和4个具有残差模块的Conv2、Conv3、Conv4、Conv5。ResNet- 50网络结构如图1所示。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911019_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 ResNet- 50网络的结构" src="Detail/GetImg?filename=images/JSJY201911019_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 ResNet- 50网络的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911019_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure of ResNet- 50 network</p>

                </div>
                <h4 class="anchor-tag" id="63" name="63">1.2 <b>时间建模方法</b></h4>
                <div class="p1">
                    <p id="64">近年来由于数据集规模不断地扩大,基于视频的行人再识别成为当前研究的主流方向。与基于图像的方法相比,基于视频的方法可以有效地利用视频序列中的时间信息。因为摄像机拍摄的大部分都是时间连续的视频,可以为行人再识别提供更多的信息。</p>
                </div>
                <div class="p1">
                    <p id="65">基于视频的行人再识别方法主要注重时间信息的整合,即通过时间建模的方法将帧级特征聚合为视频序列级特征。首先,将输入的视频序列通过卷积神经网络提取帧级特征{<i><b>f</b></i><sup><i>t</i></sup>},<i>t</i>∈[1,<i>T</i>],其中<i>T</i>表示视频序列的帧数;然后,利用时间建模方法将帧级特征{<i><b>f</b></i><sup><i>t</i></sup>}聚合成单个特征<i><b>f</b></i>,用<i><b>f</b></i>表示视频序列级特征;最后,通过最小化损失得到最优的识别结果。图2展示了基于时间建模方法的原理。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911019_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于时间建模方法的原理框图" src="Detail/GetImg?filename=images/JSJY201911019_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于时间建模方法的原理框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911019_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Block diagram of temporal modeling method</p>

                </div>
                <div class="p1">
                    <p id="67">常用的时间建模方法有三种:时间池化、时间注意和RNN或其改进模型长短期记忆(Long Short-Term Memory, LSTM)网络。在时间池化模型<citation id="213" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>中,主要使用最大池化或平均池化。对于最大池化,<i><b>f</b></i>=max <i><b>f</b></i><sup><i>t</i></sup>; 对于平均池化,<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi mathvariant="bold-italic">f</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup></mrow></math></mathml>。但当视频中目标行人经常出现遮挡时,这种方法通常会失败。</p>
                </div>
                <div class="p1">
                    <p id="68">RNN或LSTM模型中<citation id="214" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,将一系列帧级特征聚合成单个特征主要有两种方法:第一种方法是直接在最后一个步骤采用隐藏状态<i><b>h</b></i><sup><i>T</i></sup>,即<i><b>f</b></i>=<i><b>h</b></i><sup><i>T</i></sup>; 第二种方法是计算RNN的输出{<i>o</i><sup><i>t</i></sup>}的平均值,即<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>o</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup></mrow></math></mathml>。但RNN或LSTM模型通常提取浅层特征,缺少对目标的判别性表达,并且难以在大型数据集中训练。</p>
                </div>
                <div class="p1">
                    <p id="69">在基于时间注意的模型<citation id="215" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>中,主要使用加权平均法将帧级特征聚合为序列级特征,即<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mspace width="0.25em" /><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>t</mi></msup></mrow></math></mathml>,其中<i>α</i><sub><i>t</i></sub>为每帧的权重。基于时间注意的方法可以很好地抑制噪声的干扰(如遮挡等),并且它是现在最主流的方法之一。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">2 本文方法</h3>
                <div class="p1">
                    <p id="71">本文首先使用<i>ResNet</i>- 50网络对输入的视频序列逐帧进行特征提取,将最后一个卷积层(<i>Conv</i>5)的特征输入到时空正则化网络并产生相应的权重分数,通过对所有帧级特征加权平均得到视频序列级特征。为了避免在注意图转换为权重分数时聚焦于一帧而忽略其他帧,使用帧级正则化来限制帧间差异。最后将帧级正则化与三重损失函数、<i>softmax</i>交叉熵损失函数联合起来,用于训练整个网络。本文方法的整体框图如图3所示。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911019_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文方法的整体框图" src="Detail/GetImg?filename=images/JSJY201911019_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文方法的整体框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911019_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Overall block diagram of the proposed method</i></p>

                </div>
                <h4 class="anchor-tag" id="73" name="73">2.1 <b>特征提取</b></h4>
                <div class="p1">
                    <p id="74">本文使用<i>ResNet</i>- 50网络对输入的视频序列进行特征提取。通常,<i>ResNet</i>- 50网络使用一系列层处理图像,其中每个单独的层由卷积、池化和非线性激活函数等步骤组成。为了简化符号,本文将<i>ResNet</i>- 50网络定义为函数<i>f</i><sub><i>c</i></sub>=<i>C</i>(<i>x</i>),其将图像<i><b>x</b></i>作为输入并且产生特征作为输出。</p>
                </div>
                <div class="p1">
                    <p id="75">设<i><b>I</b></i>=<i><b>I</b></i><sub>1</sub>,<i><b>I</b></i><sub>2</sub>,…,<i><b>I</b></i><sub><i>T</i></sub>是由行人图像组成的长度为<i>T</i>的视频序列,其中<i><b>I</b></i><sub><i>t</i></sub>是目标行人在时间<i>t</i>处的图像。每个图像<i><b>I</b></i><sub><i>t</i></sub>通过ResNet- 50网络之后产生帧级特征,即<i><b>f</b></i><sup><i>t</i></sup>=<i>C</i>(<i><b>I</b></i><sub><i>t</i></sub>)。本文将视频序列输入到ResNet- 50网络中并输出一系列帧级特征{<i><b>f</b></i><sup><i>t</i></sup>}(<i>t</i>∈[1,<i>T</i>])。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">2.2 <b>时空正则化</b></h4>
                <div class="p1">
                    <p id="77"><i>ResNet</i>- 50网络中最后一个卷积层(<i>Conv</i>5)的特征图大小为<i>W</i>×<i>H</i>,其维度为<i>D</i>=2 048,<i>H</i>和<i>W</i>是特征图的高度和宽度,<i>H</i>和<i>W</i>的大小取决于输入图像的尺寸。首先将帧级特征<i><b>f</b></i><sup><i>t</i></sup>=(<i><b>f</b></i><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></math></mathml>, <i><b>f</b></i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></math></mathml>,…, <i><b>f</b></i><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>D</mi><mi>t</mi></msubsup></mrow></math></mathml>)作为时空正则化网络的输入,将特征图中的所有元素,针对每个特征通道<i>d</i>进行空间正则化,生成相应的注意图<i><b>g</b></i><sub><i>t</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="78"><i><b>g</b></i><sub><i>t</i></sub>=<i><b>f</b></i><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>t</mi></msubsup></mrow></math></mathml>/‖<i><b>f</b></i><sup><i>t</i></sup>‖<sub>2</sub>      (1)</p>
                </div>
                <div class="p1">
                    <p id="79">其中<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>t</mi></msup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">f</mi><msubsup><mrow></mrow><mi>d</mi><mi>t</mi></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup></mrow></math></mathml>是<i><b>f</b></i><sup><i>t</i></sup>的L2范数。在经过空间正则化之后,每帧都具有一个对应的注意图。然后将每帧注意图中的所有元素针对每个特征通道<i>d</i>使用L1范数以获得相应的空间注意分数:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></munder><mo stretchy="false">∥</mo></mstyle></mrow></mstyle><mi mathvariant="bold-italic">g</mi><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中<i>m</i>和<i>n</i>代表每帧注意图中相应的所有元素。因此,每帧都具有一个对应的空间注意分数<i>s</i><sub><i>t</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="82">本文直接比较来自不同帧的空间注意分数<i>s</i><sub><i>t</i></sub>(<i>t</i>∈[1,<i>T</i>]),并采用Sigmoid函数和L1归一化计算时间注意分数:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>/</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>σ</mi></mstyle><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中<i>σ</i>表示Sigmoid函数。最后,为每帧分配一个特定的权重分数<i>α</i><sub><i>t</i></sub>,通过加权平均得到视频序列级特征<i><b>f</b></i>:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mspace width="0.25em" /><mi mathvariant="bold-italic">f</mi><msup><mrow></mrow><mi>t</mi></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">2.3 <b>帧级正则化</b></h4>
                <div class="p1">
                    <p id="87">对于基于视频的行人再识别而言,来自同一视频序列的行人图像应代表同一人的外观,但是在注意图转换为注意分数时,会出现注意分数集中在一个特定帧上并且在很大程度上忽略其他帧的情况。为了限制帧间差异,避免注意分数聚集在一帧,本文从视频序列的<i>T</i>帧中随机选择两帧<i>i</i>和<i>j</i>,并使用Frobenius范数对帧级注意图进行正则化:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mtext>F</mtext></msub><mo>=</mo></mtd></mtr><mtr><mtd><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">g</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo><mo>-</mo><mi mathvariant="bold-italic">g</mi><msubsup><mrow></mrow><mi>j</mi><mi>d</mi></msubsup><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">其中<i><b>g</b></i><sub><i>i</i></sub>和<i><b>g</b></i><sub><i>j</i></sub>是由式(1)产生的注意图。将所有正则化项<i>F</i><sub><i>i</i></sub><sub>, </sub><sub><i>j</i></sub>乘以一个常数<i>β</i>后加到式(9)中来最小化损失:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mrow><mo>(</mo><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mtable columnalign="left"><mtr><mtd><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mi>i</mi><mo>≠</mo><mi>j</mi></mtd></mtr></mtable><mi>Τ</mi></munderover><mrow></mrow></mstyle><mi>β</mi><mo>⋅</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi></mrow></msub></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">2.4 <b>损失函数</b></h4>
                <div class="p1">
                    <p id="92">本文使用三重损失函数和<i>softmax</i>交叉熵损失函数来训练网络。</p>
                </div>
                <div class="p1">
                    <p id="93">三重损失函数最初是<i>Hermans</i>等<citation id="216" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出的,是原始的三重损失(<i>semi</i>-<i>hard triplet loss</i>)的改进版。本文为每个小批量(<i>mini</i>-<i>batch</i>)随机抽取<i>P</i>个身份,并为每个身份随机抽取<i>K</i>个视频序列(每个序列包含<i>T</i>帧),以满足三重损失函数要求。三重损失函数可以表述如下:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext><mtext>p</mtext><mtext>l</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mover accent="true"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ρ</mi></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow></mrow></mstyle></mrow><mo stretchy="true">⌢</mo></mover></mrow></mstyle><mrow><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext><mspace width="0.25em" /><mtext>a</mtext><mtext>n</mtext><mtext>c</mtext><mtext>h</mtext><mtext>o</mtext><mtext>r</mtext><mtext>s</mtext></mrow></mover><mo stretchy="false">[</mo><mi>α</mi><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mover accent="true"><mrow><mo>+</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Κ</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">f</mi><msubsup><mrow></mrow><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">f</mi><msubsup><mrow></mrow><mi>p</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow><mo stretchy="true">⌢</mo></mover></mrow></mstyle><mrow><mtext>h</mtext><mtext>a</mtext><mtext>r</mtext><mtext>d</mtext><mtext>e</mtext><mtext>s</mtext><mtext>t</mtext><mspace width="0.25em" /><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext><mtext>i</mtext><mtext>t</mtext><mtext>i</mtext><mtext>v</mtext><mtext>e</mtext></mrow></mover><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Κ</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Ρ</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">f</mi><msubsup><mrow></mrow><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">f</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></mstyle><mrow><mtext>h</mtext><mtext>a</mtext><mtext>r</mtext><mtext>d</mtext><mtext>e</mtext><mtext>s</mtext><mtext>t</mtext><mspace width="0.25em" /><mtext>n</mtext><mtext>e</mtext><mtext>g</mtext><mtext>a</mtext><mtext>t</mtext><mtext>i</mtext><mtext>v</mtext><mtext>e</mtext></mrow></munder><mo stretchy="false">]</mo><msub><mrow></mrow><mo>+</mo></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">其中:<i><b>f</b></i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>、 <i><b>f</b></i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>和<i><b>f</b></i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>分别是从目标样本、正样本和负样本中提取的特征; <i>α</i>是用于控制样本内部距离的超参数。正样本和负样本指的是与目标样本具有相同身份和不同身份的行人。</p>
                </div>
                <div class="p1">
                    <p id="96">除了使用三重损失函数以外,本文还采用softmax交叉熵损失进行判别性学习。softmax交叉熵损失函数可以表述如下:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>o</mtext><mtext>f</mtext><mtext>t</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></msub><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mrow><mi>Ρ</mi><mi>Κ</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ρ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>p</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>a</mi></mrow></msub><mspace width="0.25em" /><mrow><mi>lg</mi></mrow><mspace width="0.25em" /><mi>q</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>a</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">其中<i>p</i><sub><i>i</i></sub><sub>,</sub><sub><i>a</i></sub>和<i>q</i><sub><i>i</i></sub><sub>,</sub><sub><i>a</i></sub>是样本{<i>i</i>,<i>a</i>}的真实身份和预测。</p>
                </div>
                <div class="p1">
                    <p id="99">总损失函数<i>L</i><sub>total</sub>是softmax损失和triplet损失的组合,如式(9)所示:</p>
                </div>
                <div class="p1">
                    <p id="100"><i>L</i><sub>total</sub>=<i>L</i><sub>softmax</sub>+<i>L</i><sub>triplet</sub>      (9)</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag">3 实验与结果</h3>
                <h4 class="anchor-tag" id="102" name="102">3.1 <b>实验环境和参数设置</b></h4>
                <div class="p1">
                    <p id="103">本文使用<i>Python</i>语言进行编程,实验环境为<i>pytorch</i>。所有实验都在<i>Windows</i> 10系统,<i>NVIDIA GTX</i> 1060 <i>GPU</i>的电脑上完成。视频序列的大小调整为256×128。首先从输入的视频序列中随机选择T=4帧,然后随机选择P=4个身份对每个小批量(<i>mini</i>-<i>batch</i>)进行采样,并从训练集中为每个身份随机抽取K=4个视频序列,批量大小(<i>batch size</i>)为32。学习率为0.000 3,三重损失函数的<i>margin</i>参数设置为0.3。在训练期间,采用<i>Adam</i><citation id="217" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>优化网络。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">3.2 <b>数据集</b></h4>
                <div class="p1">
                    <p id="105">运动分析和再识别数据集(<i>Motion Analysis and Re</i>-<i>identification Set</i>, <i>MARS</i>)<citation id="218" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>包含1 261个身份和大约20 000个视频序列,是迄今为止最大的视频行人再识别数据集之一。这些序列至少由2个摄像机捕获,最多由6个摄像机捕获,每个身份平均有13.2个序列。此外,数据集固定地分为训练集和测试集,用于训练的身份为625个,用于测试的身份为626个,其中还包含3 248个干扰序列。</p>
                </div>
                <div class="p1">
                    <p id="106"><i>DukeMTMC</i>-<i>ReID</i>数据集<citation id="219" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>源自<i>DukeMTMC</i>数据集<citation id="220" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>,也是一个大规模的行人再识别数据集。它由8个摄像机捕获的1 812个身份组成,其中1 404个身份出现在两个以上的摄像机中,其余的408个是干扰身份。数据集固定地分为训练集和测试集,都有702个身份。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">3.3 <b>评价指标</b></h4>
                <div class="p1">
                    <p id="108">为了评估本文的方法,使用累积匹配特征(<i>Cumulative Matching Characteristic</i>, <i>CMC</i>)曲线和平均精度(<i>mean Average Precision</i>, <i>mAP</i>)作为本实验中的评价指标。<i>CMC</i>曲线表示行人识别的准确性,本文使用<i>Rank</i>-1、<i>Rank</i>- 5、<i>Rank</i>-10和<i>Rank</i>-20的得分代表<i>CMC</i>曲线。当每次识别仅对应视频序列中的一个目标时,<i>CMC</i>指标是有效的, 但是当视频中存在多个目标时,<i>CMC</i>指标是有偏差的。<i>DukeMTMC</i>-<i>ReID</i>和<i>MARS</i>数据集在使用<i>CMC</i>曲线作为评价指标的同时,也采用<i>mAP</i>作为评价指标。相比之下,<i>mAP</i>是一个更具有综合性的指标,非常适合单目标和多目标的再识别。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">3.4 <b>在</b><i>MARS</i><b>和</b><i>DukeMTMC</i>-<i>ReID</i><b>数据集中评估</b></h4>
                <div class="p1">
                    <p id="110">为了验证本文方法的有效性,在<i>MARS</i>数据集中进行了测试与分析。本文选取了4个具有代表性的视频序列,如图4所示,其中:<i>query</i>表示待识别的目标行人; 数字1～10表示<i>Rank</i>-1到<i>Rank</i>-10; 黑色实线框代表正样本(与目标具有相同身份的人),即识别正确; 无框代表负样本(与目标具有不同身份的人),即匹配错误。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911019_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 在MARS数据集上行人再识别的结果" src="Detail/GetImg?filename=images/JSJY201911019_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 在<i>MARS</i>数据集上行人再识别的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911019_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Results of person re</i>-<i>identification on MARS dataset</i></p>

                </div>
                <div class="p1">
                    <p id="112">从图4(<i>a</i>)中可以看出,本文方法成功识别不同视角的所有候选者;在图4(<i>b</i>)中本文方法也成功找到了最高等级的正确候选者;图4(<i>c</i>)受到明显的光照变化的影响;图4(<i>d</i>)包含与待识别目标具有相似外观行人的干扰。实验结果表明本文方法对于人体姿势变化、视角变化、光照变化和相似外观目标的干扰都具有出色的性能表现。</p>
                </div>
                <div class="p1">
                    <p id="113">表1列出了本文方法中各个组成部分的性能比较结果,其中:<i>Baseline</i>对应于在<i>DukeMTMC</i>-<i>ReID</i>和<i>MARS</i>数据集上使用<i>softmax</i>交叉熵损失函数训练的基础的网络模型; <i>Triplet</i>、<i>STR</i>(<i>Spatial</i>-<i>Temporal Regularization</i>)和<i>FLR</i>(<i>Frame</i>-<i>Level Regularization</i>)分别代表三重损失函数、时空正则化和帧级正则化。<i>Baseline</i>+<i>Triplet</i>代表用三重损失函数和<i>softmax</i>交叉熵损失函数训练的网络。在<i>MARS</i>数据集中,与<i>Baseline</i>+<i>Triplet</i>相比,<i>STR</i>在<i>mAP</i>方面提高了2.5个百分点,在<i>Rank</i>-1准确率方面提高了3.3个百分点。与<i>Baseline</i>+<i>Triplet</i>+<i>STR</i>相比,<i>FLR</i>方法在<i>mAP</i>方面提高了1.7个百分点,在<i>Rank</i>-1准确率方面提高了2.7个百分点。在<i>DukeMTMC</i>-<i>ReID</i>数据集中,<i>STR</i>在<i>mAP</i>方面提高了1.7个百分点,在<i>Rank</i>-1准确率方面提高了4.8个百分点。而<i>FLR</i>在<i>mAP</i>方面提高了1.2个百分点,在<i>Rank</i>-1准确率上提高了1.8个百分点。结果表明空间正则化方法有助于提高行人再识别的准确性,帧级正则化方法可以平衡帧间差异,进一步提高整体的性能。</p>
                </div>
                <div class="p1">
                    <p id="115">表2展示了输入不同长度视频序列的性能比较。为了公平比较,本文除了改变视频序列的长度<i>T</i>以外,其他的参数均保持不变。<i>T</i>=1是不使用时间建模方法的单幅图像的模型。从表2中可以看出,随着序列长度<i>T</i>的增加,mAP和Rank准确率得分均有所提高, 这表明时间建模方法对于提高行人再识别的准确性是有效的。当<i>T</i>=4时,本文方法的整体性能表现最佳。<i>T</i>=4时,在MARS数据集中本文方法的Rank-1准确率为82.1%,mAP为72.3%;而在DukeMTMC-ReID数据集中本文方法的Rank-1准确率为80.0%,mAP为61.2%。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表</b>1 <b>本文方法中各个组成部分的性能比较 </b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Performance comparison of various components in the proposed method </p>
                    <p class="img_note">单位:%</p>
                    <table id="116" border="1"><tr><td rowspan="2"><br />模型</td><td colspan="5"><br />MARS数据集</td><td rowspan="2"></td><td colspan="4"><br />DukeMTMC-ReID数据集</td></tr><tr><td><br />mAP</td><td>Rank-1</td><td>Rank- 5</td><td>Rank-10</td><td>Rank-20</td><td><br />mAP</td><td>Rank-1</td><td>Rank- 5</td><td>Rank-20</td></tr><tr><td>Baseline</td><td>54.0</td><td>64.7</td><td>82.1</td><td>87.4</td><td>91.0</td><td></td><td>43.5</td><td>62.3</td><td>70.4</td><td>79.2</td></tr><tr><td><br />Baseline+Triplet</td><td>68.1</td><td>76.1</td><td>86.3</td><td>90.6</td><td>92.9</td><td></td><td>58.3</td><td>73.4</td><td>78.6</td><td>86.7</td></tr><tr><td><br />Baseline+Triplet+STR</td><td>70.6</td><td>79.4</td><td>88.6</td><td>92.1</td><td>94.2</td><td></td><td>60.0</td><td>78.2</td><td>83.5</td><td>90.1</td></tr><tr><td><br />Baseline+Triplet+STR+FLR</td><td>72.3</td><td>82.1</td><td>90.5</td><td>93.1</td><td>95.0</td><td></td><td>61.2</td><td>80.0</td><td>88.8</td><td>93.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表</b>2 <b>不同视频序列长度的性能比较 </b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Performance comparison of different video sequence lengths </p>
                    <p class="img_note">单位:%</p>
                    <table id="117" border="1"><tr><td rowspan="2"><br /><i>T</i></td><td colspan="5"><br />MARS数据集</td><td rowspan="2"></td><td colspan="4"><br />DukeMTMC-ReID数据集</td></tr><tr><td><br />mAP</td><td>Rank-1</td><td>Rank- 5</td><td>Rank-10</td><td>Rank-20</td><td><br />mAP</td><td>Rank-1</td><td>Rank- 5</td><td>Rank-20</td></tr><tr><td><br />1</td><td>62.9</td><td>74.7</td><td>85.2</td><td>88.6</td><td>92.4</td><td></td><td>50.4</td><td>71.5</td><td>77.6</td><td>84.2</td></tr><tr><td><br />2</td><td>68.6</td><td>78.6</td><td>88.4</td><td>90.8</td><td>93.2</td><td></td><td>57.7</td><td>78.1</td><td>83.6</td><td>89.3</td></tr><tr><td><br />4</td><td>72.3</td><td>82.1</td><td>90.5</td><td>93.1</td><td>95.0</td><td></td><td>61.2</td><td>80.0</td><td>88.8</td><td>93.7</td></tr><tr><td><br />8</td><td>72.2</td><td>81.7</td><td>90.4</td><td>93.5</td><td>95.0</td><td></td><td>61.0</td><td>80.0</td><td>88.5</td><td>93.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">为了进行公平的比较,本文使用相同的基础模型与现有的方法进行对比。表3列出了本文方法与MARS中其他方法的比较,其中“—”表示论文作者没有进行对应的实验(下同)。本文方法的mAP为72.3%,与Triplet<citation id="221" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>相比提高了4.6个百分点,与CSACSE(Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding)方法<citation id="222" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>相比提高了2.9个百分点,与MSML(Margin Sample Mining Loss)方法<citation id="223" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>相比提高了0.3个百分点。Rank-1准确率为 82.1%,相对于Triplet 提高了2.3个百分点,相对于CSACSE提高了0.9个百分点。对于Rank- 5和Rank-20而言,本文方法也取得了出色的成绩。在Rank-10方面,准确率为93.1%。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表</b>3 <b>几种方法在</b>MARS<b>数据集中的性能比较 </b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Performance comparison of several methods in MARS dataset </p>
                    <p class="img_note">单位:%</p>
                    <table id="119" border="1"><tr><td><br />方法</td><td>mAP</td><td>Rank-1</td><td>Rank- 5</td><td>Rank-10</td><td>Rank-20</td></tr><tr><td><br />本文方法</td><td>72.3</td><td>82.1</td><td>90.5</td><td>93.1</td><td>95.0</td></tr><tr><td><br />Zheng方法<sup>[14]</sup></td><td>49.3</td><td>68.3</td><td>82.6</td><td>—</td><td>89.4</td></tr><tr><td><br />QAN方法<sup>[4]</sup></td><td>51.7</td><td>73.7</td><td>84.9</td><td>—</td><td>91.6</td></tr><tr><td><br />Triplet方法<sup>[12]</sup></td><td>67.7</td><td>79.8</td><td>91.4</td><td>—</td><td>—</td></tr><tr><td><br />Zhou方法<sup>[5]</sup></td><td>50.7</td><td>70.6</td><td>90.0</td><td>—</td><td>97.6</td></tr><tr><td><br />Li方法<sup>[17]</sup></td><td>56.1</td><td>71.8</td><td>86.6</td><td>—</td><td>93.0</td></tr><tr><td><br />MSML方法<sup>[18]</sup></td><td>72.0</td><td>83.0</td><td>92.6</td><td>—</td><td>—</td></tr><tr><td><br />STAN方法<sup>[19]</sup></td><td>65.8</td><td>82.3</td><td>—</td><td>—</td><td>—</td></tr><tr><td><br />CSACSE方法<sup>[22]</sup></td><td>69.4</td><td>81.2</td><td>92.1</td><td>—</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">表4列出了本文方法与DukeMTMC-ReID中其他方法的比较, 该数据集比MARS更具有挑战性,因为它的相机视域更宽,场景更复杂,行人图像在分辨率和背景方面变化很大。表4中列出了本文方法的mAP和Rank-1准确率分别为61.2%和80.0%,与APR方法相比<citation id="224" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>均提高了9.3个百分点,与其他方法相比并没有明显的提高。但是本文方法的模型更加简单、且易于训练。表4还列出了本文方法的Rank- 5和Rank-20准确率分别为88.8%和93.7%。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表</b>4 <b>几种方法在</b> DukeMTMC-ReID<b>数据集中的性能比较 </b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Performance comparison of several methods in DukeMTMC-ReID dataset </p>
                    <p class="img_note">单位:%</p>
                    <table id="121" border="1"><tr><td><br />方法</td><td>mAP</td><td>Rank- 1</td><td>Rank- 5</td><td>Rank- 20</td></tr><tr><td><br />本文方法</td><td>61.2</td><td>80.0</td><td>88.8</td><td>93.7</td></tr><tr><td><br />HA-CNN方法<sup>[20]</sup></td><td>63.8</td><td>80.5</td><td>—</td><td>—</td></tr><tr><td><br />APR方法<sup>[21]</sup></td><td>51.9</td><td>70.7</td><td>—</td><td>—</td></tr><tr><td><br />MLFN方法<sup>[23]</sup></td><td>62.8</td><td>81.0</td><td>—</td><td>—</td></tr><tr><td><br />DPFL方法<sup>[24]</sup></td><td>60.6</td><td>79.2</td><td>—</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="122" name="122" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="123">本文主要对基于视频的行人再识别进行了分析和研究。实验结果表明,时间建模方法对于提高视频中行人再识别的准确性是有效的。本文还提出了时空正则化和帧级正则化策略,进一步提高了行人再识别的准确性。在<i>DukeMTMC</i>-<i>ReID</i>和<i>MARS</i>数据集上进行实验,实验结果清楚地证明了本文方法的整体有效性。未来的主要工作是将本文方法与目标检测或跟踪算法相结合应用于实际的多摄像机监控环境,实现对目标行人准确的识别和连续、稳定的跟踪。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="155">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201809002&amp;v=MjkwMDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVkw3SktDTGZZYkc0SDluTXBvOUZab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 李幼蛟,卓力,张菁,等.行人再识别技术综述[J].自动化学报,2018,44(9):1554-1568.(LI Y J,ZHUO L,ZHANG J,et al.A survey of person re-identification[J].Acta Automatica Sinica,2018,44(9):1554-1568.)
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent convolutional network for video-based person reidentification">

                                <b>[2]</b> MCLAUGHLIN N,DEL RINCON J M,MILLER P.Recurrent convolutional network for video-based person re-identification[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1325-1334.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling spatial-temporal clues in a hybrid deep learning framework for video classification">

                                <b>[3]</b> WU Z,WANG X,JIANG Y G,et al.Modeling spatial-temporal clues in a hybrid deep learning framework for video classification[C]// Proceedings of the 23rd ACM International Conference on Multimedia.New York:ACM,2015:461-470.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quality Aware Network for Set to Set Recognition">

                                <b>[4]</b> LIU Y,YAN J,OUYANG W.Quality aware network for set to set recognition[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4694-4703.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=See the forest for the trees:joint spatial and temporal recurrent neural networks in video-based person re-identification">

                                <b>[5]</b> ZHOU Z,HUANG Y,WANG W,et al.See the forest for the trees:Joint spatial and temporal recurrent neural networks for video-based person re-identification[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:4747-4756.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 KARPATHY A,TODERICI G,SHETTY S,et al.Large-scale video classification with convolutional neural networks[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2014:1725-1732.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:a large-scale hierarchical image database">

                                <b>[7]</b> DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database[C]// Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2009:248-255.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Top-push video-based person re-identification">

                                <b>[9]</b> YOU J,WU A,LI X,et al.Top-push video-based person re-identification[C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1345-1353.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person Re-identification via recurrent feature aggregation">

                                <b>[10]</b> YAN Y,NI B,SONG Z,et al.Person re-identification via recurrent feature aggregation[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:701-716.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention">

                                <b>[11]</b> XU K,BA J,KIROS R,et al.Show,attend and tell:Neural image caption generation with visual attention[C]// Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:International Machine Learning Society,2015:2048-2057.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In defense of the triplet loss for person re-identification">

                                <b>[12]</b> HERMANS A,BEYR L,LEIBE B.In defense of the triplet loss for person re-identification[EB/OL].[2017- 11- 21].http://arxiv.org/pdf/1703.07737.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">

                                <b>[13]</b> KINGMA D P,BA J.Adam:a method for stochastic optimization[EB/OL].[2017- 01- 30].http://csce.uark.edu/～mgashler/ml/2018_spring/r3/adam.pdf.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mars:A video benchmark for large-scale person re-identification">

                                <b>[14]</b> ZHENG L,BIE Z,SUN Y,et al.Mars:a video benchmark for large-scale person re-identification[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:868-884.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unlabeled samples generated by gan improve the person re-identification baseline in vitro">

                                <b>[15]</b> ZHENG Z,ZHENG L,YANG Y.Unlabeled samples generated by GAN improve the person re-identification baseline in vitro[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington,DC:IEEE Computer Society,2017:3754-3762.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance measures and a data set for multi-target,multi-camera tracking">

                                <b>[16]</b> RISTANI E,SOLERA F,ZOU R,et al.Performance measures and a data set for multi-target,multi-camera tracking[C]// Proceedings of the 14th European Conference on Computer Vision.Berlin:Springer,2016:17-35.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification">

                                <b>[17]</b> LI D,CHEN X,ZHANG Z,et al.Learning deep context-aware features over body and latent parts for person re-identification[C]// Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:384-393.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Margin sample mining loss:a deep learning based method for person re-identification">

                                <b>[18]</b> XIAO Q,LUO H,ZHANG C.Margin sample mining loss:a deep learning based method for person re-identification[EB/OL].[2017- 10- 07].http://arxiv.org/pdf/1710.00478.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diversity regularized spatiotemporal attention for video-based person re-identification">

                                <b>[19]</b> LI S,BAK S,CARR P,et al.Diversity regularized spatiotemporal attention for video-based person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:369-378.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Harmonious Attention Network for Person Re-Identification">

                                <b>[20]</b> LI W,ZHU X,GONG S.Harmonious attention network for person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2285-2294.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving person re-identification by attribute and identity learning">

                                <b>[21]</b> LIN Y,ZHENG L,ZHENG Z,et al.Improving person re-identification by attribute and identity learning[J].Pattern Recognition,2019,95:151-161.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding">

                                <b>[22]</b> CHEN D,LI H,XIAO T,et al.Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:1169-1178.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-Level Factorisation Net for Person Re-Identification">

                                <b>[23]</b> CHANG X,HOSPEDALES T M,XIANG T.Multi-level factorisation net for person re-identification[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2018:2109-2118.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by deep learning multi-scale representations">

                                <b>[24]</b> CHEN Y,ZHU X,GONG S.Person re-identification by deep learning multi-scale representations[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway:IEEE,2017:2590-2600.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201711005&amp;v=MDk2MTFOcm85RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5WTDdKS0Q3WWJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 李姣,张晓晖,朱虹,等.多置信度重排序的行人再识别算法[J].模式识别与人工智能,2017,30(11):995-1002.(LI J,ZHANG X H,ZHU H,et al.Person re-identification via multiple confidences re-ranking[J].Pattern Recognition and Artificial Intelligence,2017,30(11):995-1002.)
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911019" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911019&amp;v=MTY5MDBGckNVUjdxZlp1WnNGeW5uVkw3Skx6N0JkN0c0SDlqTnJvOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
