<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136779646221250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904043%26RESULT%3d1%26SIGN%3dNf6JxHOI19oIXFyo73lY%252fer%252bMgg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904043&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904043&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904043&amp;v=MjMzMjFyQ1VSN3FmWnVac0Z5RGhVTHJMTHo3QmQ3RzRIOWpNcTQ5Qlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 传统级联卷积神经网络特征点定位方法 ">1 传统级联卷积神经网络特征点定位方法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="2 改进级联卷积神经网络特征点定位方法 ">2 改进级联卷积神经网络特征点定位方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="2.1 &lt;b&gt;级联卷积神经网络结构的改进&lt;/b&gt;">2.1 <b>级联卷积神经网络结构的改进</b></a></li>
                                                <li><a href="#79" data-title="2.2 &lt;b&gt;融合&lt;/b&gt;Faster-RCNN&lt;b&gt;区域提取的改进&lt;/b&gt;">2.2 <b>融合</b>Faster-RCNN<b>区域提取的改进</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="3.1 &lt;b&gt;实验数据&lt;/b&gt;">3.1 <b>实验数据</b></a></li>
                                                <li><a href="#100" data-title="3.2 &lt;b&gt;实验结果分析&lt;/b&gt;">3.2 <b>实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="图1 卷积神经网络特征点定位框架">图1 卷积神经网络特征点定位框架</a></li>
                                                <li><a href="#78" data-title="图2 本文级联卷积神经网络结构">图2 本文级联卷积神经网络结构</a></li>
                                                <li><a href="#84" data-title="图3 基于&lt;i&gt;Faster&lt;/i&gt;-&lt;i&gt;RCNN&lt;/i&gt;的区域提取框架">图3 基于<i>Faster</i>-<i>RCNN</i>的区域提取框架</a></li>
                                                <li><a href="#88" data-title="图4 融合Faster-RCNN模型提取区域的特征点定位流程">图4 融合Faster-RCNN模型提取区域的特征点定位流程</a></li>
                                                <li><a href="#89" data-title="图5 标注样例">图5 标注样例</a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;训练中的网络参数&lt;/b&gt;"><b>表</b>1 <b>训练中的网络参数</b></a></li>
                                                <li><a href="#105" data-title="图6 损失和迭代轮次关系">图6 损失和迭代轮次关系</a></li>
                                                <li><a href="#106" data-title="图7 预测效果展示">图7 预测效果展示</a></li>
                                                <li><a href="#109" data-title="图8 传统级联&lt;i&gt;CNN&lt;/i&gt;和本文方法的累积误差分布对比">图8 传统级联<i>CNN</i>和本文方法的累积误差分布对比</a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;传统级联&lt;/b&gt;CNN&lt;b&gt;与本文方法准确度对比&lt;/b&gt;"><b>表</b>2 <b>传统级联</b>CNN<b>与本文方法准确度对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="139">


                                    <a id="bibliography_1" title="NOBLE J A, BOUKERROUI D.Ultrasound image segmentation:a survey[J].IEEE Transactions on Medical Imaging, 2006, 25 (8) :987-1010." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ultrasound image segmentation: a survey">
                                        <b>[1]</b>
                                        NOBLE J A, BOUKERROUI D.Ultrasound image segmentation:a survey[J].IEEE Transactions on Medical Imaging, 2006, 25 (8) :987-1010.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_2" title="冉隆科.一种基于角点检测方法的骨龄图像关键点定位[J].电子设计工程, 2011, 19 (14) :175-177. (RAN L K.A corner detection method based on the image of the skeletal age point positioning[J].Electronic Design Engineering, 2011, 19 (14) :175-177.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GWDZ201114073&amp;v=Mjc2NzVDWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEaFVMckxJanJQZExHNEg5RE5xNDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        冉隆科.一种基于角点检测方法的骨龄图像关键点定位[J].电子设计工程, 2011, 19 (14) :175-177. (RAN L K.A corner detection method based on the image of the skeletal age point positioning[J].Electronic Design Engineering, 2011, 19 (14) :175-177.) 
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_3" title="MAHAPATRA D.Landmark detection in cardiac MRI using learned local image statistics[C]//STACOM 2012:Proceedings of the 2012Statistical Atlases and Computational Models of the Heart-Imaging and Modelling Challenges, LNCS 7746.Berlin:Springer, 2012:115-124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Landmark detection in cardiac MRI using learned local image statistics">
                                        <b>[3]</b>
                                        MAHAPATRA D.Landmark detection in cardiac MRI using learned local image statistics[C]//STACOM 2012:Proceedings of the 2012Statistical Atlases and Computational Models of the Heart-Imaging and Modelling Challenges, LNCS 7746.Berlin:Springer, 2012:115-124.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_4" title="YANG X, JIN J, XU M, et al.Ultrasound common carotid artery segmentation based on active shape model[J].Computational and Mathematical Methods in Medicine, 2013, 2013 (2) :345968." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ultrasound Common Carotid Artery Segmentation Based on Active Shape Model">
                                        <b>[4]</b>
                                        YANG X, JIN J, XU M, et al.Ultrasound common carotid artery segmentation based on active shape model[J].Computational and Mathematical Methods in Medicine, 2013, 2013 (2) :345968.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_5" title="COOTES T F, TAYLOR C J, COOPER D H, et al.Active shape models-their training and application[J].Computer Vision&amp;amp;Image Understanding, 1995, 61 (1) :38-59." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084429&amp;v=MjA1NzlRVE1ud1plWnRGaW5sVXIzSUtGb1hieEU9TmlmT2ZiSzdIdEROcW85RVpPTUxDSDR3b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        COOTES T F, TAYLOR C J, COOPER D H, et al.Active shape models-their training and application[J].Computer Vision&amp;amp;Image Understanding, 1995, 61 (1) :38-59.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_6" title="VARGAS-QUINTERO L, ESCALANTE-RAMREZ B, MARN LC, et al.Left ventricle segmentation in fetal echocardiography using a multi-texture active appearance model based on the steered Hermite transform[J].Computer Methods and Programs in Biomedicine, 2016, 137:231-245." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Left ventricle segmentation in fetal echocardiography using a multi-texture active appearance model based on the steered Hermite transform">
                                        <b>[6]</b>
                                        VARGAS-QUINTERO L, ESCALANTE-RAMREZ B, MARN LC, et al.Left ventricle segmentation in fetal echocardiography using a multi-texture active appearance model based on the steered Hermite transform[J].Computer Methods and Programs in Biomedicine, 2016, 137:231-245.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_7" title="COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active appearance models">
                                        <b>[7]</b>
                                        COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_8" title="DOLLR P, WELINDER P, PERONA P.Cascaded pose regression[C]//Proceedings of the 2010 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2010:1078-1085." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascaded Pose Regression">
                                        <b>[8]</b>
                                        DOLLR P, WELINDER P, PERONA P.Cascaded pose regression[C]//Proceedings of the 2010 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2010:1078-1085.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_9" title="SUN P, ZHOU H, LUNDINE D, et al.Fast segmentation of left ventricle in CT images by explicit shape regression using random pixel difference features[EB/OL].[2018-05-10].https://arxiv.org/pdf/1507.07508." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast segmentation of left ventricle in CT images by explicit shape regression using random pixel difference features">
                                        <b>[9]</b>
                                        SUN P, ZHOU H, LUNDINE D, et al.Fast segmentation of left ventricle in CT images by explicit shape regression using random pixel difference features[EB/OL].[2018-05-10].https://arxiv.org/pdf/1507.07508.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_10" title="SEDAI S, ROY P, GARNAVI R.Segmentation of right ventricle in cardiac mr images using shape regression[C]//MLMI 2015:Proceedings of the 2015 Machine Learning in Medical Imaging.Berlin:Springer, 2015:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segmentation of right ventricle in cardiac mr images using shape regression">
                                        <b>[10]</b>
                                        SEDAI S, ROY P, GARNAVI R.Segmentation of right ventricle in cardiac mr images using shape regression[C]//MLMI 2015:Proceedings of the 2015 Machine Learning in Medical Imaging.Berlin:Springer, 2015:1-8.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_11" title="LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradient-based learning applied to document recognition">
                                        <b>[11]</b>
                                        LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_12" title="PRASOON A, PETERSEN K, IGEL C, et al.Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network[C]//MICCAI 2013:Proceedings of the 2013 Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer, 2013:246-253." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Feature Learning for Knee Cartilage Segmentation Using A Triplanar Convolutional Neural Network">
                                        <b>[12]</b>
                                        PRASOON A, PETERSEN K, IGEL C, et al.Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network[C]//MICCAI 2013:Proceedings of the 2013 Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer, 2013:246-253.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_13" title="YANG D, ZHANG S, YAN Z, et al.Automated anatomical landmark detection ondistal femur surface using convolutional neural network[C]//Proceedings of the 12th IEEE International Symposium on Biomedical Imaging.Piscataway, NJ:IEEE, 2015:17-21." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated anatomical landmark detection ondistal femur surface using convolutional neural network">
                                        <b>[13]</b>
                                        YANG D, ZHANG S, YAN Z, et al.Automated anatomical landmark detection ondistal femur surface using convolutional neural network[C]//Proceedings of the 12th IEEE International Symposium on Biomedical Imaging.Piscataway, NJ:IEEE, 2015:17-21.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_14" title="REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[14]</b>
                                        REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_15" title="SUN Y, WANG X, TANG X.Deep convolutional network cascade for facial point detection[C]//Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2013:3476-3483." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Convolutional Network Cascade for Facial Point Detection">
                                        <b>[15]</b>
                                        SUN Y, WANG X, TANG X.Deep convolutional network cascade for facial point detection[C]//Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2013:3476-3483.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_16" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[EB/OL].[2018-05-10].http://www.nvidia.in/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">
                                        <b>[16]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[EB/OL].[2018-05-10].http://www.nvidia.in/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_17" title="NAIR V, HINTON G E.Rectified linear units improve restricted Boltzmann machines[C]//ICML 2010:Proceedings of the 27th International Conference on International Conference on Machine Learning.Madison, Wisconsin:Omnipress, 2010:807-814." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted boltzmann machines">
                                        <b>[17]</b>
                                        NAIR V, HINTON G E.Rectified linear units improve restricted Boltzmann machines[C]//ICML 2010:Proceedings of the 27th International Conference on International Conference on Machine Learning.Madison, Wisconsin:Omnipress, 2010:807-814.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_18" title="GIRSHICK R, DONAHUE J, DARRELL T, et al.Region-based convolutional networks for accurate object detection and segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (1) :142-158." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Region-Based Convolutional Networks for Accurate Object Detection and Segmentation">
                                        <b>[18]</b>
                                        GIRSHICK R, DONAHUE J, DARRELL T, et al.Region-based convolutional networks for accurate object detection and segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (1) :142-158.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_19" title="HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:346-361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">
                                        <b>[19]</b>
                                        HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:346-361.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_20" title="GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[20]</b>
                                        GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_21" title="LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the 2015IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[21]</b>
                                        LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the 2015IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3431-3440.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_22" title="纪祥虎, 高思聪, 黄志标, 等.基于Centripetal CatmullRom曲线的经食道超声心动图左心室分割方法[J].四川大学学报 (工程科学版) , 2016, 48 (5) :84-90. (JI X H, GAO S C, HUANG Z B, et al.Left ventricle segmentation in transesophageal echocardiography based on Centripetal CatmullRom curve[J].Journal of Sichuan University (Engineering Science Edition) , 2016, 48 (5) :84-90.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SCLH201605013&amp;v=MDQxODFyQ1VSN3FmWnVac0Z5RGhVTHJMTmk3SFpyRzRIOWZNcW85RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        纪祥虎, 高思聪, 黄志标, 等.基于Centripetal CatmullRom曲线的经食道超声心动图左心室分割方法[J].四川大学学报 (工程科学版) , 2016, 48 (5) :84-90. (JI X H, GAO S C, HUANG Z B, et al.Left ventricle segmentation in transesophageal echocardiography based on Centripetal CatmullRom curve[J].Journal of Sichuan University (Engineering Science Edition) , 2016, 48 (5) :84-90.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-29 11:42</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),1201-1207 DOI:10.11772/j.issn.1001-9081.2018091931            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积神经网络的左心室超声图像特征点定位</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E7%8E%89%E9%87%91&amp;code=41473337&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周玉金</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%93%E4%B8%9C&amp;code=15701958&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晓东</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%8A%9B%E6%88%88&amp;code=38429754&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张力戈</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E9%94%B4&amp;code=36173915&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱锴</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E5%AE%87&amp;code=24018508&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚宇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E6%88%90%E9%83%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0211714&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院成都计算机应用研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统级联卷积神经网络 (CNN) 在左心室超声图像中定位准确度较低的问题, 提出一种融合更快速区域卷积神经网络 (Faster-RCNN) 模型提取区域的级联卷积神经网络, 实现对超声图像中左心室心内膜和心外膜轮廓特征点的定位。首先, 采用两级级联的方式改进传统级联卷积神经网络的网络结构, 第一级网络利用一个改进的卷积网络粗略定位左心室心内膜和心外膜联合的特征点, 第二级网络使用四个改进的卷积网络分别对心内膜特征点和心外膜特征点进行位置微调, 之后定位输出左心室心内膜和心外膜联合的轮廓特征点位置;然后, 将改进的级联卷积神经网络与目标区域提取融合, 即利用Faster-RCNN模型提取包含左心室的目标区域并将目标区域送入改进的级联卷积神经网络;最后, 由粗到细对左心室轮廓特征点进行定位。实验结果表明, 与传统级联卷积神经网络相比, 所提方法在左心室超声图像上的定位效果更好, 更逼近真实值, 在均方根误差的评价标准下, 特征点定位准确度提升了32.6个百分点。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%A3%B0%E5%BF%83%E5%8A%A8%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超声心动图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B7%A6%E5%BF%83%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">左心室;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%82%B9%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征点定位;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BA%A7%E8%81%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">级联卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *周玉金 (1993—) , 女, 云南曲靖人, 硕士研究生, 主要研究方向:深度学习、医学图像处理;电子邮箱zhouyujinanna@126.com;
                                </span>
                                <span>
                                    王晓东 (1973—) , 男, 四川乐山人, 研究员, 主要研究方向:地理信息系统、物联网;;
                                </span>
                                <span>
                                    张力戈 (1995—) , 男, 山西太原人, 博士研究生, 主要研究方向:优化算法、深度学习;;
                                </span>
                                <span>
                                    朱锴 (1991—) , 男, 贵州安顺人, 博士研究生, 主要研究方向:机器学习、深度学习;;
                                </span>
                                <span>
                                    姚宇 (1980—) , 男, 四川宜宾人, 研究员, 博士, 主要研究方向:机器学习、数据挖掘。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>四川省科技厅重点研发项目 (2017SZ0010);</span>
                                <span>四川省科技支撑计划项目 (2016JZ0035);</span>
                    </p>
            </div>
                    <h1><b>Feature point localization of left ventricular ultrasound image based on convolutional neural network</b></h1>
                    <h2>
                    <span>ZHOU Yujin</span>
                    <span>WANG Xiaodong</span>
                    <span>ZHANG Lige</span>
                    <span>ZHU Kai</span>
                    <span>YAO Yu</span>
            </h2>
                    <h2>
                    <span>Chengdu Institute of Computer Application, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem that the traditional cascaded Convolutional Neural Network (CNN) has low accuracy of feature point localization in left ventricular ultrasound image, an improved cascaded CNN with region extracted by Faster Region-based CNN (Faster-RCNN) model was proposed to locate the left ventricular endocardial and epicardial feature points in ultrasound images. Firstly, the traditional cascaded CNN was improved by a structure of two-stage cascaded. In the first stage, an improved convolutional network was used to roughly locate the endocardial and epicardial joint feature points. In the second stage, four improved convolutional networks were used to fine-tune the endocardial feature points and the epicardial feature points separately. After that, the positions of joint contour feature points were output. Secondly, the improved cascaded CNN was merged with target region extraction, which means that the target region containing the left ventricle was extracted by the Faster-RCNN model and then was sent into the improved cascaded CNN. Finally, the left ventricular contour feature points were located from coarse to fine. Experimental results show that compared with the traditional cascaded CNN, the proposed method is much more accurate in left ventricle feature point localization, and its prediction points are closer to the actual values. Under the root mean square error evaluation standard, the accuracy of feature point localization is improved by 32.6 percentage points.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=echocardiography&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">echocardiography;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=left%20ventricle&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">left ventricle;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20points%20location&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature points location;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cascaded%20convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cascaded convolutional neural network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHOU Yujin, born in 1993, M. S. candidate. Her research interests include deep learning, medical image processing.;
                                </span>
                                <span>
                                    WANG Xiaodong, born in 1973, research fellow. His research interests include geographic information system, Internet of things.;
                                </span>
                                <span>
                                    ZHANG Lige, born in 1995, Ph. D. candidate. His research interests include optimization algorithm, deep learning.;
                                </span>
                                <span>
                                    ZHU Kai, born in 1991, Ph. D. candidate. His research interests include machine learning, deep learning.;
                                </span>
                                <span>
                                    YAO Yu, born in 1980, Ph. D. research fellow. His research interests include machine learning, data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-17</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Key Research and Development Project of Science and Technology Department of Sichuan Province (2017SZ0010);</span>
                                <span>the Science and Technology Support Project of Sichuan Province (2016JZ0035);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">超声心动图因其便携、成本低廉成为医学中心脏腔室测量可视化最常用的一种医疗成像方式。超声专家们往往关心超声图像上关键部位的生理参数信息, 通常对左心室超声成像进行分析并对收缩末期和舒张末期阶段心室轮廓边界进行检测, 用它来评价心功能的好坏进而预判诊断心脏疾病<citation id="183" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。实现左心室轮廓的准确定位是对左心室功能评价的首要前提, 对计算机辅助诊疗有着重要的价值。</p>
                </div>
                <div class="p1">
                    <p id="49">采用特征点来实现对医学图像上一些生理结构关键部位的定位, 一直以来是医学研究者研究的热点问题。如文献<citation id="184" type="reference">[<a class="sup">2</a>]</citation>采用角点检测的方法在骨龄图像上定位手部指骨特征点;文献<citation id="185" type="reference">[<a class="sup">3</a>]</citation>采用监督学习方法从不同角度在心脏图像上检测左心室特征点。目前, 特征点定位方法大致可以分为两种:一种是基于统计建模的方法, 根据先验特征点分布模型, 通过不断迭代收敛来匹配定位特征点。如:文献<citation id="186" type="reference">[<a class="sup">4</a>]</citation>利用主动形态模型 (Active Shape Model, ASM) <citation id="187" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的点分布模型在超声图像中定位颈总动脉特征点进而对其分割, 文献<citation id="188" type="reference">[<a class="sup">6</a>]</citation>采用主动表观模型 (Active Appearance Model, AAM) <citation id="189" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>在胎儿超声心动图上建立多纹理点分布模型, 根据左心室的特征点轮廓进行分割。第二种是基于级联回归<citation id="190" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的方法, 通过一系列回归器对一个指定的初始值逐步细化, 通过回归器的级联来自动学习特征点向量。如:文献<citation id="191" type="reference">[<a class="sup">9</a>]</citation>直接利用形状回归检测左心室轮廓, 文献<citation id="192" type="reference">[<a class="sup">10</a>]</citation>采用回归的方法分割右心室。这些方法虽然得到了广泛应用, 但有其局限性:一是模型学习过多依赖于人工干预, 初始值位置的设定往往决定着模型预测的好坏;二是需提取人工设计的特征, 对于低水平图像信号中的突变情况, 人为的特征并不能很好地表达图像特征。</p>
                </div>
                <div class="p1">
                    <p id="50">进入了卷积神经网络 (Convolutional Neural Network, CNN) <citation id="193" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>时代后, 卷积网络利用其特有的局部感知和权值共享机制来自动提取更深层次特征, 逐渐代替人工提取特征, 越来越多的研究者将卷积神经网络应用于医学图像处理上, 如:文献<citation id="194" type="reference">[<a class="sup">12</a>]</citation>利用卷积神经网络对医学图像进行分类;文献<citation id="195" type="reference">[<a class="sup">13</a>]</citation>在3D医学图像上采用卷积神经网络, 实现了对股骨特征点的定位。本文进行了左心室超声图像轮廓特征点定位的研究工作, 针对超声图像存在斑点噪声、伪影以及图像对比度较低等影响超声图像上左心室轮廓界限较难确定的一系列难题, 汲取卷积神经网络应用在计算机视觉领域解决问题的经验, 提出一种融合更快速区域卷积神经网络 (Faster Region-based CNN, Faster-RCNN) <citation id="196" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>模型提取区域的级联卷积神经网络对经食管超声图像中左心室轮廓特征点定位的方法。该方法首先采用Faster-RCNN网络模型提取包含左心室的目标区域, 然后将包含左心室的目标区域送入本文设计的两级级联卷积神经网络, 经过第一级网络粗略预测特征点位置和在第二级网络进行位置微调的基础上, 最终准确定位出左心室心内膜与心外膜联合的轮廓特征点, 达到了较为理想的定位效果。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 传统级联卷积神经网络特征点定位方法</h3>
                <div class="p1">
                    <p id="52">Sun等<citation id="197" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>首次提出一种三级级联的卷积神经网络应用于人脸特征点的定位。该算法由三级级联网络组成:第一级网络粗略估计人脸特征点位置, 第二级和第三级网络在以每个特征点为中心截取的小区域中对上一级网络估计的位置进一步地精确调整, 由粗到细地实现人脸特征点的定位。预测的特征点位置表达式为:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">x</mi><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mo>⋯</mo><mo>+</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>l</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow><mrow><mi>l</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>k</mi></munderover><mspace width="0.25em" /></mstyle><mfrac><mrow><mtext>Δ</mtext><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mtext>Δ</mtext><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mo>⋯</mo><mo>+</mo><mtext>Δ</mtext><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">其中:<i>k</i>为级联层级, 共有三个层级;<i>l</i><sub><i>i</i></sub>为第<i>i</i>级网络中所采用的网络的数;<b><i>x</i></b><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>l</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>为第一级第<i>l</i><sub>1</sub>个网络预测的向量, 第一级网络预测中共有<i>l</i><sub>1</sub>个网络预测的平均值作为粗略估计的绝对值与第二级以及第三级网络中共有<i>l</i><sub><i>i</i></sub>个网络预测的平均调整值Δ<b><i>x</i></b><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>相加输出最终特征点的定位位置。</p>
                </div>
                <div class="p1">
                    <p id="57">传统的级联卷积神经网络通过多个卷积神经网络以级联的方式实现特征点定位, 每个卷积神经网络特征点定位框架如图1所示, 由输入层、卷积层、池化层、全连接层以及输出层组成。输入层输入图像数据<b><i>I</i></b> (<i>h</i>, <i>w</i>) , 即输入<i>h</i>×<i>w</i>的灰度图像。经由卷积层, 用卷积核<b><i>C</i></b> (<i>s</i>, <i>n</i>, <i>p</i>, <i>q</i>) 来提取特征, 其中<i>s</i>为卷积核的区域长度, <i>n</i>为特征层数, <i>p</i>为共享权值参数, <i>q</i>为局部共享权值。<i>h</i>、<i>w</i>、<i>m</i>分别为上一层图像的高和宽以及特征图数量, 共有<i>m</i>个卷积核对图像进行卷积, 采用双曲正切 (tanh) 函数作为神经元激活函数来提取特征, 卷积层通过<b><i>C</i></b> (<i>s</i>, <i>n</i>, <i>p</i>, <i>q</i>) 卷积的表达式如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>s</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>s</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>x</mi></mstyle></mrow></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mi>k</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>+</mo><mi>l</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>⋅</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>l</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>r</mi><mo>, </mo><mi>u</mi><mo>, </mo><mi>v</mi><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mi>b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中:<i>i</i>=Δ<i>h</i>·<i>u</i>, Δ<i>h</i>·<i>u</i>+1, …, Δ<i>h</i>·<i>u</i>+Δ<i>h</i>-1, <i>j</i>=Δ<i>w</i>·<i>v</i>, Δ<i>w</i>·<i>v</i>+1, …, Δ<i>w</i>·<i>v</i>+Δ<i>w</i>-1, <i>t</i>=0, 1, …, <i>n</i>-1;并有<mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mi>h</mi><mo>=</mo><mfrac><mrow><mi>h</mi><mo>-</mo><mi>s</mi><mo>+</mo><mn>1</mn></mrow><mi>p</mi></mfrac><mo>, </mo><mtext>Δ</mtext><mi>w</mi><mo>=</mo><mfrac><mrow><mi>w</mi><mo>-</mo><mi>s</mi><mo>+</mo><mn>1</mn></mrow><mi>q</mi></mfrac><mo>, </mo><mi>u</mi><mo>=</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mspace width="0.25em" /><mi>p</mi><mo>-</mo><mn>1</mn></mrow></math></mathml>和<i>v</i>=0, 1, …, <i>q</i>-1。而<i>x</i><sub><i>i</i></sub>和<i>y</i><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>分别为上一层输出以及本层卷积后的特征输出。池化层相当于紧接着卷积层的一个特殊的卷积操作, 有效缩小卷积层特征图尺寸, 减少参数数量。用<b><i>P</i></b> (<i>s</i>) 表示池化层, 通过长度为<i>s</i>的卷积核进行卷积的结果表达式如式 (3) 所示:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false"> (</mo><mi>g</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo>⋅</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mn>0</mn><mo>≤</mo><mi>k</mi><mo>, </mo><mi>l</mi><mo>&lt;</mo><mi>s</mi></mrow></munder><mo stretchy="false">{</mo><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>⋅</mo><mi>s</mi><mo>+</mo><mi>k</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>⋅</mo><mi>s</mi><mo>+</mo><mi>l</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">}</mo><mo>+</mo><mi>b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中:<i>i</i>、 <i>j</i>、<i>t</i>、<i>u</i>、<i>v</i>与式 (2) 中相应参数一致;<i>g</i>为激活函数的增益系数;<i>b</i>为偏置, 通过tanh激活函数对池化后的特征图进行非线性输出。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 卷积神经网络特征点定位框架" src="Detail/GetImg?filename=images/JSJY201904043_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 卷积神经网络特征点定位框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Convolutional neural network feature point localization framework</p>

                </div>
                <div class="p1">
                    <p id="65">全连接层将卷积操作生成的特征图映射成一个固定长度的特征向量, 用<b><i>F</i></b> (<i>n</i>) 表示, 其激活表达式如式 (4) 所示:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>f</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>f</mi></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中: <i>f</i>=0, 1, …, <i>n</i>-1 (<i>n</i>为特征图层数) ;<i>m</i>是特征图数量, <i>n</i>和<i>m</i>即为上一层的神经元数量; <i>y</i><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>f</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>为全连接层输出值, 最终将全连接的总输出值映射至输出层以输出特征点坐标向量。</p>
                </div>
                <div class="p1">
                    <p id="69">该算法特征点定位回归采用的损失函数为欧氏距离损失, 即均方根误差, 计算预测特征点与标记点向量的均方误差。如式 (5) 所示:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中: <b><i>y</i></b><sub><i>i</i></sub>为手工标记特征点组成向量〈<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>, <i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>, …, <i>x</i><sub><i>n</i></sub>, <i>y</i><sub><i>n</i></sub>〉中的第<i>i</i>维特征点坐标向量, <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>为预测特征点向量的第<i>i</i>维特征点坐标向量。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">2 改进级联卷积神经网络特征点定位方法</h3>
                <h4 class="anchor-tag" id="74" name="74">2.1 <b>级联卷积神经网络结构的改进</b></h4>
                <div class="p1">
                    <p id="75">传统级联卷积神经网络框架最初应用于人脸特征点定位并能准确定位出人脸的眉毛、眼睛、鼻子以及嘴等特征点。由于超声图像中左心室轮廓边界较为模糊, 相对于人脸特征点, 左心室轮廓特征点较难确定, 直接采用传统级联卷积神经网络没有充分学习到左心室轮廓的特征, 导致定位效果较差。因此, 本文提出了改进的级联卷积神经网络, 级联方式上与传统级联一致, 采用两级级联方式:第一级网络粗略预测特征点位置, 第二级网络分别选取包含左心室内膜特征点的小块区域和包含左心室外膜特征点的小块区域利用卷积神经网络对第一级网络预测的特征点位置进行细微调整, 两级网络由粗到细地定位左心室轮廓特征点。网络结构方面所采用的卷积神经网络结构与传统级联卷积神经网络采用的网络结构不同, 其网络结构根据左心室超声图像轮廓特征点定位任务, 依照深度卷积神经网络 (<i>AlexNet</i>) <citation id="198" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的网络结构而重新设计, 并使用了修正线性单元 (<i>Rectified Linear Units</i>, <i>ReLU</i>) 作为激活函数。<i>ReLU</i>激活函数相对于<i>tanh</i>激活函数, 梯度下降更快<citation id="199" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 其函数表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mi>x</mi><mo>≤</mo><mn>0</mn></mtd></mtr><mtr><mtd><mi>x</mi><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">本文提出的级联卷积神经网络中第一级网络所采用的卷积神经网络结构如图2 (a) 所示, 输入为256×256的图像, 包含了5层卷积层和3层全连接层。其前2个卷积层中卷积核设为7×7和5×5, 其余3层卷积核尺寸都为3×3, 卷积层后紧跟池化层, 其核尺寸都设为3×3。全连接层将特征图映射为一维向量, 最后1个全连接层输出左心室轮廓34个特征点 (<i>x</i>, <i>y</i>) 坐标组成的68维向量。第二级网络采用的卷积神经网络结构如图2 (b) 所示, 相对于第一级网络结构, 第二级网络输入区域相对较小, 为96×96的目标图像区域, 采用4个卷积层和2个全连接层。第1个卷积层采用7×7的卷积核, 其余3层卷积层以及池化层的核尺寸都设为3×3。连接2层全连接层, 最后1个全连接层输出34维向量。第二级网络共采用4个如图2 (b) 所示的卷积神经网络, 针对包含心内膜特征点和包含心外膜特征点的小块区域, 每个区域分别采用2个卷积神经网络取其输出向量的平均值作为每个区域特征点在第一级网络预测位置的微调, 由粗到细地预测出左心室轮廓特征点位置, 其预测特征点的预测向量与传统级联卷积神经网络预测特征点向量表达一致, 如式 (5) 所示。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文级联卷积神经网络结构" src="Detail/GetImg?filename=images/JSJY201904043_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文级联卷积神经网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Network architectures of the proposed cascaded CNN</p>

                </div>
                <h4 class="anchor-tag" id="79" name="79">2.2 <b>融合</b>Faster-RCNN<b>区域提取的改进</b></h4>
                <div class="p1">
                    <p id="80">对于左心室超声图像轮廓特征点的定位, 在超声图像中本研究所感兴趣的区域是包含左心室的区域。感兴趣区域包含着重要的信息, 背景区域的信息较为次要, 有时候不必要的背景区域难免会影响定位精度。因此, 如何提取感兴趣区域成为特征点定位首先要解决的问题。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">2.2.1 <i>Faster</i>-<i>RCNN</i>模型</h4>
                <div class="p1">
                    <p id="82"><i>Faster</i>-<i>RCNN</i>模型因其较高的检测精度, 在目标检测领域应用广泛, 将其应用在包含左心室的目标区域检测效果比较理想。<i>Faster</i>-<i>RCNN</i>模型由<i>Girshick</i>等<citation id="200" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>在基于区域驱动的区域卷积神经网络 (<i>Region</i>-<i>based CNN</i>, <i>RCNN</i>) 、空间金字塔池化卷积网络 (<i>Spatial Pyramid Pooling convolutional neural network</i>, <i>SPP</i>-<i>net</i>) <citation id="201" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation> 和快速区域卷积神经网络 (<i>Fast region</i>-<i>based CNN</i>, <i>Fast RCNN</i>) <citation id="202" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的基础上提出, 其整个模型是一个统一的目标检测网络, 如图3所示, 包含卷积层、区域生成网络、感兴趣池化和分类四部分。</p>
                </div>
                <div class="p1">
                    <p id="83">1) 卷积层利用一组基础的卷积层、<i>ReLU</i>激活函数和池化层提取图像的特征图, 与第2) 部分的区域生成网络和全连接层共享。2) 区域生成网络是一个全卷积网络 (<i>Full Convolutional Network</i>, <i>FCN</i>) <citation id="203" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 用于生成候选区域, 它以任意大小的图像作为输入, 输出一组带有目标得分的目标候选区, 可以同时在每个位置预测目标边界和目标分数并输入到该层两个子全连接层, 其中分类层通过<i>Softmax</i>判断候选区域为前景目标区域或者背景区域, 回归层修正候选区域获得更精确的候选区域。3) 感兴趣池化对输入的特征图和候选区域信息进行综合后提取候选特征图, 映射至全连接层判定目标类别。4) 分类层利用候选特征图区别候选区域为前景目标区域还是背景区域, 同时再次利用回归层来获得目标区域精确的边界框坐标位置。</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于Faster-RCNN的区域提取框架" src="Detail/GetImg?filename=images/JSJY201904043_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于<i>Faster</i>-<i>RCNN</i>的区域提取框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Region extraction framework based on Faster</i>-<i>RCNN</i></p>

                </div>
                <h4 class="anchor-tag" id="85" name="85">2.2.2 融合<i>Faster</i>-<i>RCNN</i>的级联卷积神经网络</h4>
                <div class="p1">
                    <p id="86">为了提高左心室超声图像上轮廓特征点定位精度, 排除超声图像中影响定位精度的背景信息进一步提升定位准确度, 本文在改进传统级联卷积神经网络的基础上, 引入<i>Faster</i>-<i>RCNN</i>模型, 提出了融合<i>Faster</i>-<i>RCNN</i>模型提取区域的级联卷积神经网络。该方法首先利用<i>Faster</i>-<i>RCNN</i>模型提取包含左心室的目标区域, 然后针对目标区域的图像块采用改进的两级级联卷积神经网络进行左心室轮廓特征点定位, 第一级网络粗略预测左心室心内膜和心外膜联合的轮廓特征点位置, 第二级网络分别针对心内膜特征点和心外膜特征点进行位置微调, 两级网络由粗到细地实现左心室轮廓的特征点定位。</p>
                </div>
                <div class="p1">
                    <p id="87">本文提出的融合<i>Faster</i>-<i>RCNN</i>模型提取区域的级联卷积神经网络应用于左心室超声图像轮廓特征点定位的整个流程如图4所示。第一步, 采用<i>Faster</i>-<i>RCNN</i>模型通过一组基础的卷积神经网络提取特征, 由区域生成网络生成候选区域, 通过对候选区域的边界框精确的回归和分类, 以此获取左心室目标区域, 提取包含左心室的目标区域, 最终输出左心室目标区域边界框的坐标向量 (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>, <i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>) , 分别为边界框的左上角坐标 (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>) 和右下角坐标 (<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>) 。第二步, 根据Faster-RCNN输出的边界框坐标值, 在相应的经食管超声图像中截取出目标区域图像块, 将截取的左心室目标区域送入本文提出的两级级联卷积神经网络, 第一级网络在截取的目标区域内进行左心室心内膜与心外膜轮廓特征点的粗略定位, 第二级网络分别针对左心室心内膜和心外膜小块区域内的特征点进行位置微调, 最终精确定位出左心室心内膜与心外膜联合的轮廓特征点。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 融合Faster-RCNN模型提取区域的特征点定位流程" src="Detail/GetImg?filename=images/JSJY201904043_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 融合Faster-RCNN模型提取区域的特征点定位流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Process of feature point localization with region extracted by Faster-RCNN model</p>

                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 标注样例" src="Detail/GetImg?filename=images/JSJY201904043_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 标注样例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Labeling samples</p>

                </div>
                <h3 id="90" name="90" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="91" name="91">3.1 <b>实验数据</b></h4>
                <div class="p1">
                    <p id="92">本文的实验数据来自四川大学华西医院麻醉科, 由华西医院专家标注的不同心动周期内获取的经食管超声图像, 共有训练集图像400张, 测试集图像186张。每幅图像建立了左心室矩形边界框标注以及左心室心内膜与心外膜联合的轮廓特征点标注, 标注的样例如图5所示。其中左心室的特征点标注同文献<citation id="204" type="reference">[<a class="sup">22</a>]</citation>一致, 文献中<i>Centripetal Catmull</i>-<i>Rom</i>曲线能够在减少特征点数量的同时得到形状一致的特征点, 选用了34个特征点, 内层17个点表示心内膜, 外层17个点表示心外膜, 心外膜和心内膜特征点联合勾勒出左心室轮廓。</p>
                </div>
                <div class="p1">
                    <p id="93">针对本文实验较少的训练数据集进行数据增强以扩充训练数据集。先对训练集中原始图像进行水平翻转, 其翻转表达式如式 (7) 所示, 原始图像中像素 (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>) 变换为 (<i>x</i>, <i>y</i>) , <i>w</i>为原始图像的宽度。经过水平翻转变换后, 将训练数据集扩充至800张。</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mi>w</mi></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mspace width="0.25em" /><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mi>x</mi></mtd></mtr><mtr><mtd><mi>y</mi></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">然后再对训练集中所有图像进行顺时针旋转<i>β</i>=10°, 其旋转表达式如式 (8) 所示, 将原始图像中像素 (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>) 变换为 (<i>x</i>, <i>y</i>) 。旋转变换后训练数据集扩充至1 600张进行训练。</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>cos</mi><mspace width="0.25em" /><mi>β</mi></mtd><mtd><mi>sin</mi><mspace width="0.25em" /><mi>β</mi></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mo>-</mo><mi>sin</mi><mspace width="0.25em" /><mi>β</mi></mtd><mtd><mi>cos</mi><mspace width="0.25em" /><mi>β</mi></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mspace width="0.25em" /><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mi>x</mi></mtd></mtr><mtr><mtd><mi>y</mi></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">本文提出的第一级级联卷积神经网络训练数据有两种:一是直接使用整张图像数据以及标注特征点的训练进行第一级心内膜与心外膜联合的34个特征点粗略定位。二是利用Faster-RCNN模型提取的左心室目标区域图像数据训练, 此步骤首先要利用左心室边界框标注的训练集训练Faster-RCNN网络, 获取边界框坐标后在整张图像上截取左心室目标区域, 同时对标注点坐标进行相应的转换, 针对含有左心室的目标区域图像在第一级网络中进行心内膜与心外膜联合的34个特征点粗略定位。</p>
                </div>
                <div class="area_img" id="98">
                                            <p class="img_tit">
                                                <b>表</b>1 <b>训练中的网络参数</b>
                                                    <br />
                                                Tab. 1 Description of network parameters during training
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201904043_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 训练中的网络参数" src="Detail/GetImg?filename=images/JSJY201904043_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="99">第二级级联卷积神经网络分别对心内膜和心外膜特征点进行微调定位。共4个卷积神经网络, 分别用于心内膜图像区域和心外膜图像区域进行特征点微调。对于心内膜区域训练数据, 以心内膜17个关键点的中心点作为中心, 截取出原始图像宽度的4/10、5/10两种小区域图像数据, 用2个卷积神经网络进行微调。心外膜17个关键点同理用2个卷积神经网络在截取出的原始图像宽度的4/10、5/10图像区域上进行微调, 最终由粗到细定位出心内膜和心外膜联合的轮廓特征点。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">3.2 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="101">本文实验基于<i>Caffe</i>深度学习框架, 利用<i>Python</i> 2.7编写完成, 系统环境为<i>Ubuntu</i>14.04, 显卡为<i>Titan X</i>。</p>
                </div>
                <div class="p1">
                    <p id="102">传统级联卷积神经网络与本文改进的级联卷积神经网络在训练中网络参数变化如表1所示, 传统级联卷积神经网络第一级网络输入的图像为256×256的超声图像, 网络有4个卷积层、2个全连接层以及<i>Euclidean Loss</i>损失层, 网络输出68维坐标向量, 其第二级网络共有3个卷积层、2个全连接层和1个<i>Euclidean Loss</i>损失层, 输入规格为96×96的包含心内膜或心外膜特征点的目标图像区域, 最终输出心内膜或者心外膜34维的微调坐标向量。本文改进的级联卷积神经网络训练时, 第一级网络输入为256×256的超声图像, 经过5个卷积层, 3个全连接层和<i>Euclidean Loss</i>损失层, 最后输出68维左心室心内膜与心外膜联合的坐标向量。其第二级网络输入包含心内膜特征点的96×96规格的图像区域, 经由4个卷积层、2个全连接层和<i>Euclidean Loss</i>损失层后, 输出心内膜微调位置的34维向量, 输入包含心外膜特征点的图像区域, 输出心外膜特征点微调的34维向量。</p>
                </div>
                <div class="p1">
                    <p id="103">本文改进的级联卷积神经网络训练过程中学习率设为1‰, 采用随机梯度下降的优化方法, 模型共训练迭代10万次, 网络训练时损失和迭代轮次的关系如图6所示, 横坐标为训练迭代的前400轮次, 纵坐标为欧氏距离损失。从图6中可看到训练损失和验证损失在迭代过程中迅速下降, 且最终损失接近于零。说明本文所改进的级联卷积神经网络在本研究数据集上的训练是有效的, 并且能够快速收敛。</p>
                </div>
                <div class="p1">
                    <p id="104">基于训练好的网络模型, 进行左心室超声图像的轮廓特征点预测, 传统级联卷积神经网络与本文方法的预测效果如图7所示, 图7 (<i>a</i>) 中<i>A</i>、<i>B</i>、<i>C</i>和<i>D</i>是从测试数据集中挑选的4张经食管超声实例图像, 其中<i>A</i>、<i>D</i>为左心室收缩期的超声切面实例图, <i>B</i>、<i>C</i>为左心室舒张期的超声切面实例图;图7 (<i>b</i>) 展示的是这4张经食管超声实例图的左心室轮廓手工标记特征点;图7 (<i>c</i>) ～ (<i>f</i>) 是针对4张经食管超声实例图采用不同</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 损失和迭代轮次关系" src="Detail/GetImg?filename=images/JSJY201904043_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 损失和迭代轮次关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Relationship between loss and iteration times</i></p>

                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 预测效果展示" src="Detail/GetImg?filename=images/JSJY201904043_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 预测效果展示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Prediction results</i></p>

                </div>
                <div class="p1">
                    <p id="107">方法所预测特征点的效果展示。其中图7 (<i>c</i>) 展示的是传统级联卷积神经网络预测的效果;图7 (<i>d</i>) 展示的是本文所设计的单级卷积神经网络的粗略预测效果;图7 (<i>e</i>) 为本文改进的级联卷积神经网络在实例超声图像上的预测效果;图7 (<i>f</i>) 为本文提出的融合<i>Faster</i>-<i>RCNN</i>模型提取区域的级联卷积神经网络预测的效果。可看出图7 (<i>c</i>) 中传统级联神经网络在实例图<i>A</i>、<i>B</i>、<i>C</i>和<i>D</i>中预测的左心室轮廓特征点已经偏离左心室轮廓的真实标记点。图7 (<i>d</i>) 中本文所设计的单级卷积神经网络粗略的预测出了左心室轮廓的特征点, <i>B</i>、<i>C</i>实例图中预测的特征点较为贴合真实标记点, <i>A</i>、<i>D</i>实例图中左心室心尖部位的轮廓特征点位置预测不太准确。从子图7 (<i>e</i>) 中可看到本文改进的级联卷积神经网络明显修正了<i>A</i>、<i>D</i>实例图中本文所设计的单级卷积神经网络预测不准确的特征点, 较为接近真实的轮廓标记点, 但可看到<i>A</i>、<i>B</i>、<i>C</i>、<i>D</i>实例图中左心室轮廓心外膜上个别的预测点与真实标记点的位置仍有所偏离, 图7 (<i>f</i>) 可看到本文提出的融合<i>Faster</i>-<i>RCNN</i>模型提取区域的级联卷积神经网络的预测点又进一步地修正了本文改进的级联卷积神经网络的预测位置, 更逼近真实标记点的位置, 表现效果最佳。</p>
                </div>
                <div class="p1">
                    <p id="108">将本文方法与传统级联卷积神经网络进行定量分析, 其累计误差分布如图8所示。可看出本文提出的融合<i>Faster</i>-<i>RCNN</i>模型提取区域的级联卷积神经网络效果最优, 本文改进级联卷积神经网络在本文单级卷积神经网络的基础上表现有所提升, 并且本文方法明显优于传统级联卷积神经网络。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904043_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 传统级联CNN和本文方法的累积误差分布对比" src="Detail/GetImg?filename=images/JSJY201904043_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 传统级联<i>CNN</i>和本文方法的累积误差分布对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904043_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Cumulative error distribution comparison between</i><i>traditional cascaded CNN and the proposed method</i></p>

                </div>
                <div class="p1">
                    <p id="110">本文采用式 (5) 的均方根误差评价标准对测试样本进一步定量评估, 本文实验中定义评价标准下误差如果超过5%则认为是预测错误。定义预测样本准确度如式 (9) 所示:</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mi>Τ</mi><mi>Ν</mi></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">其中:<i>T</i>为预测正确的样本数目;<i>N</i>为测试样本总数。</p>
                </div>
                <div class="p1">
                    <p id="113">在评价标准下, 对比结果如表2所示, 在均方根误差测量下, 本文方法比传统级联卷积神经网络获得的误差更小。无论是针对心内膜特征点、心外膜特征点还是心外膜与心内膜联合的轮廓特征点, 本文方法的定位准确度都达到了80%以上。本文改进的级联卷积神经网络的定位准确度比本文单级卷积神经网络的定位准确度高。融合Faster-RCNN模型提取区域的级联卷积神经网络在本文改进的级联卷积神经网络的基础上定位准确度达到了85.0%, 相对于传统级联卷积神经网络, 定位准确度提升了32.6个百分点。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表</b>2 <b>传统级联</b>CNN<b>与本文方法准确度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Accuracy comparison between traditional cascaded CNN and the proposed method</p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td><br />算法</td><td>均方<br />根差</td><td>心内膜<br />准确度/%</td><td>心外膜<br />准确度/%</td><td>平均<br />准确度/%</td></tr><tr><td><br />传统级联CNN</td><td>39.74</td><td>56.0</td><td>48.9</td><td>52.4</td></tr><tr><td><br />本文单级CNN</td><td>28.87</td><td>82.1</td><td>81.5</td><td>81.8</td></tr><tr><td><br />本文级联CNN</td><td>28.14</td><td>84.9</td><td>84.0</td><td>84.5</td></tr><tr><td><br />本文融合区域<br />提取的级联CNN</td><td>27.78</td><td>85.2</td><td>84.7</td><td>85.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="115" name="115" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="116">本文提出了一种融合<i>Faster</i>-<i>RCNN</i>区域提取的级联卷积神经网络实现对左心室超声图像心内膜和心外膜轮廓特征点的定位方法。首先本文针对传统级联卷积神经网络直接照搬应用于经食管超声图像中左心室轮廓特征点定位不理想的情况, 重新设计级联卷积神经网络结构, 提升定位精度。其次针对背景区域影响定位精度的情况, 进一步提出融合<i>Faster</i>-<i>RCNN</i>模型先提取包含左心室的目标区域, 再进行目标区域内两级级联卷积神经网络的左心室轮廓特征点定位。实验结果表明, 本文方法定位准确度更高, 其预测的左心室轮廓特征点更逼近真实标记的特征点。在下一步的研究中, 计划将级联卷积神经网络的特征点定位方法应用于3<i>D</i>超声心动图中实现左心室轮廓的跟踪, 实时捕捉左心室的收缩舒张变化情况, 进一步实现更智能化的计算机辅助诊疗。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="139">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ultrasound image segmentation: a survey">

                                <b>[1]</b>NOBLE J A, BOUKERROUI D.Ultrasound image segmentation:a survey[J].IEEE Transactions on Medical Imaging, 2006, 25 (8) :987-1010.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GWDZ201114073&amp;v=MTI4MTlHRnJDVVI3cWZadVpzRnlEaFVMckxJanJQZExHNEg5RE5xNDlDWjRRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>冉隆科.一种基于角点检测方法的骨龄图像关键点定位[J].电子设计工程, 2011, 19 (14) :175-177. (RAN L K.A corner detection method based on the image of the skeletal age point positioning[J].Electronic Design Engineering, 2011, 19 (14) :175-177.) 
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Landmark detection in cardiac MRI using learned local image statistics">

                                <b>[3]</b>MAHAPATRA D.Landmark detection in cardiac MRI using learned local image statistics[C]//STACOM 2012:Proceedings of the 2012Statistical Atlases and Computational Models of the Heart-Imaging and Modelling Challenges, LNCS 7746.Berlin:Springer, 2012:115-124.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ultrasound Common Carotid Artery Segmentation Based on Active Shape Model">

                                <b>[4]</b>YANG X, JIN J, XU M, et al.Ultrasound common carotid artery segmentation based on active shape model[J].Computational and Mathematical Methods in Medicine, 2013, 2013 (2) :345968.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084429&amp;v=Mjg3NDZadEZpbmxVcjNJS0ZvWGJ4RT1OaWZPZmJLN0h0RE5xbzlFWk9NTENINHdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>COOTES T F, TAYLOR C J, COOPER D H, et al.Active shape models-their training and application[J].Computer Vision&amp;Image Understanding, 1995, 61 (1) :38-59.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Left ventricle segmentation in fetal echocardiography using a multi-texture active appearance model based on the steered Hermite transform">

                                <b>[6]</b>VARGAS-QUINTERO L, ESCALANTE-RAMREZ B, MARN LC, et al.Left ventricle segmentation in fetal echocardiography using a multi-texture active appearance model based on the steered Hermite transform[J].Computer Methods and Programs in Biomedicine, 2016, 137:231-245.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active appearance models">

                                <b>[7]</b>COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascaded Pose Regression">

                                <b>[8]</b>DOLLR P, WELINDER P, PERONA P.Cascaded pose regression[C]//Proceedings of the 2010 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2010:1078-1085.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast segmentation of left ventricle in CT images by explicit shape regression using random pixel difference features">

                                <b>[9]</b>SUN P, ZHOU H, LUNDINE D, et al.Fast segmentation of left ventricle in CT images by explicit shape regression using random pixel difference features[EB/OL].[2018-05-10].https://arxiv.org/pdf/1507.07508.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segmentation of right ventricle in cardiac mr images using shape regression">

                                <b>[10]</b>SEDAI S, ROY P, GARNAVI R.Segmentation of right ventricle in cardiac mr images using shape regression[C]//MLMI 2015:Proceedings of the 2015 Machine Learning in Medical Imaging.Berlin:Springer, 2015:1-8.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradient-based learning applied to document recognition">

                                <b>[11]</b>LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Feature Learning for Knee Cartilage Segmentation Using A Triplanar Convolutional Neural Network">

                                <b>[12]</b>PRASOON A, PETERSEN K, IGEL C, et al.Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network[C]//MICCAI 2013:Proceedings of the 2013 Medical Image Computing and Computer-Assisted Intervention.Berlin:Springer, 2013:246-253.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated anatomical landmark detection ondistal femur surface using convolutional neural network">

                                <b>[13]</b>YANG D, ZHANG S, YAN Z, et al.Automated anatomical landmark detection ondistal femur surface using convolutional neural network[C]//Proceedings of the 12th IEEE International Symposium on Biomedical Imaging.Piscataway, NJ:IEEE, 2015:17-21.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[14]</b>REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Convolutional Network Cascade for Facial Point Detection">

                                <b>[15]</b>SUN Y, WANG X, TANG X.Deep convolutional network cascade for facial point detection[C]//Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2013:3476-3483.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">

                                <b>[16]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[EB/OL].[2018-05-10].http://www.nvidia.in/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted boltzmann machines">

                                <b>[17]</b>NAIR V, HINTON G E.Rectified linear units improve restricted Boltzmann machines[C]//ICML 2010:Proceedings of the 27th International Conference on International Conference on Machine Learning.Madison, Wisconsin:Omnipress, 2010:807-814.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Region-Based Convolutional Networks for Accurate Object Detection and Segmentation">

                                <b>[18]</b>GIRSHICK R, DONAHUE J, DARRELL T, et al.Region-based convolutional networks for accurate object detection and segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (1) :142-158.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">

                                <b>[19]</b>HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//ECCV 2014:Proceedings of the 2014 European Conference on Computer Vision.Berlin:Springer, 2014:346-361.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[20]</b>GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[21]</b>LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the 2015IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3431-3440.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SCLH201605013&amp;v=MTI5ODg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURoVUxyTE5pN0hackc0SDlmTXFvOUVaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>纪祥虎, 高思聪, 黄志标, 等.基于Centripetal CatmullRom曲线的经食道超声心动图左心室分割方法[J].四川大学学报 (工程科学版) , 2016, 48 (5) :84-90. (JI X H, GAO S C, HUANG Z B, et al.Left ventricle segmentation in transesophageal echocardiography based on Centripetal CatmullRom curve[J].Journal of Sichuan University (Engineering Science Edition) , 2016, 48 (5) :84-90.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904043" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904043&amp;v=MjMzMjFyQ1VSN3FmWnVac0Z5RGhVTHJMTHo3QmQ3RzRIOWpNcTQ5Qlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
