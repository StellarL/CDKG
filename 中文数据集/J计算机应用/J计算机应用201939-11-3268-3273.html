<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136448014971250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911027%26RESULT%3d1%26SIGN%3dgHm%252bHVuVYyQr7xnZDTWAZ8F9g04%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911027&amp;v=MzAxNzVuVzd2T0x6N0JkN0c0SDlqTnJvOUhZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="1 递归矩阵求逆的MVDR ">1 递归矩阵求逆的MVDR</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="1.1 &lt;b&gt;双微阵列&lt;/b&gt;">1.1 <b>双微阵列</b></a></li>
                                                <li><a href="#47" data-title="1.2 &lt;b&gt;最小方差无畸变响应&lt;/b&gt;">1.2 <b>最小方差无畸变响应</b></a></li>
                                                <li><a href="#67" data-title="1.3 &lt;b&gt;对角加载&lt;/b&gt;">1.3 <b>对角加载</b></a></li>
                                                <li><a href="#76" data-title="1.4 &lt;b&gt;递归矩阵求逆&lt;/b&gt;">1.4 <b>递归矩阵求逆</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#98" data-title="2 调制域谱减法 ">2 调制域谱减法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="3 卷积神经网络模型结构 ">3 卷积神经网络模型结构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#119" data-title="3.1 &lt;b&gt;卷积层&lt;/b&gt;">3.1 <b>卷积层</b></a></li>
                                                <li><a href="#125" data-title="3.2 &lt;b&gt;池化层&lt;/b&gt;">3.2 <b>池化层</b></a></li>
                                                <li><a href="#129" data-title="3.3 &lt;b&gt;损失函数&lt;/b&gt;">3.3 <b>损失函数</b></a></li>
                                                <li><a href="#134" data-title="3.4 &lt;b&gt;模型训练&lt;/b&gt;">3.4 <b>模型训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#148" data-title="4 实验与分析 ">4 实验与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#161" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="图1 双微阵列结构">图1 双微阵列结构</a></li>
                                                <li><a href="#115" data-title="图2 时域、频域与调制域关系图">图2 时域、频域与调制域关系图</a></li>
                                                <li><a href="#118" data-title="图3 &lt;i&gt;CNN&lt;/i&gt;模型示意图">图3 <i>CNN</i>模型示意图</a></li>
                                                <li><a href="#152" data-title="图4 含噪语音经不同算法处理后的语音时域仿真">图4 含噪语音经不同算法处理后的语音时域仿真</a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同算法在&lt;/b&gt;0 &lt;i&gt;dB&lt;/i&gt;&lt;b&gt;不同噪声环境下运行时间对比 &lt;/b&gt;"><b>表</b>1 <b>不同算法在</b>0 <i>dB</i><b>不同噪声环境下运行时间对比 </b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;i&gt;F&lt;/i&gt;16&lt;b&gt;噪声环境语音识别准确率 &lt;/b&gt;"><b>表</b>2 <i>F</i>16<b>噪声环境语音识别准确率 </b></a></li>
                                                <li><a href="#158" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;i&gt;volvo&lt;/i&gt;&lt;b&gt;噪声环境语音识别准确率 &lt;/b&gt;"><b>表</b>3 <i>volvo</i><b>噪声环境语音识别准确率 </b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;i&gt;babble&lt;/i&gt;&lt;b&gt;噪声环境语音识别准确率 &lt;/b&gt;"><b>表</b>4 <i>babble</i><b>噪声环境语音识别准确率 </b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="204">


                                    <a id="bibliography_1" title=" 韩纪庆,张磊,郑铁然.语音信号处理[M].北京:清华大学出版社,2004:1-4.(HAN J Q,ZHANG L,ZHENG T R.Speech Signal Processing[M].Beijing:Tsinghua University Press,2004:1-4.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302088318002&amp;v=MDcyMTFuS3JpZlplWnZGeW5uVTc3S0tGb1NYRnF6R2JDNEhOSEVwNHhFYk9zUERoTTh6eFVTbURkOVNIN24zeEU5ZmJ2&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         韩纪庆,张磊,郑铁然.语音信号处理[M].北京:清华大学出版社,2004:1-4.(HAN J Q,ZHANG L,ZHENG T R.Speech Signal Processing[M].Beijing:Tsinghua University Press,2004:1-4.)
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_2" title=" 宋知用.Matlab在语音信号分析与合成中的应用[M].北京:北京航空航天大学出版社,2013:176-199.(SONG Z Y.Application of Matlab in Speech Signal Analysis and Synthesis[M].Beijing:Beihang University Press,2013:176-199.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787512412286000&amp;v=MDk0ODVZMU5ZdXNQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5uVTc3S0tGb1NYRnF6R2JhNUhOWE5y&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         宋知用.Matlab在语音信号分析与合成中的应用[M].北京:北京航空航天大学出版社,2013:176-199.(SONG Z Y.Application of Matlab in Speech Signal Analysis and Synthesis[M].Beijing:Beihang University Press,2013:176-199.)
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_3" title=" ZHANG X,WANG Z,WANG D.A speech enhancement algorithm by iterating single- and multi-microphone processing and its application to robust ASR[C]// Proceedings of the 2017 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2017:276-280." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A speech enhancement algorithm by iterating single- and multi-microphone processing and its application to robust ASR">
                                        <b>[3]</b>
                                         ZHANG X,WANG Z,WANG D.A speech enhancement algorithm by iterating single- and multi-microphone processing and its application to robust ASR[C]// Proceedings of the 2017 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2017:276-280.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_4" title=" HIGUCHI T,ITO N,ARAKI S,et al.Online MVDR beamformer based on complex Gaussian mixture model with spatial prior for noise robust ASR[J].IEEE/ACM Transactions on Audio Speech and Language Processing,2017,25(4):780-793." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM591800599ACBC59DEB68BBDC2939CA88&amp;v=MTI3NDBOTEtYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeGJxM3hLND1OaWZJWTdheEg5bk1yNHBNYlpwOGZnODh4bUptbURsMU9nMlczeEE4ZXJ2bg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         HIGUCHI T,ITO N,ARAKI S,et al.Online MVDR beamformer based on complex Gaussian mixture model with spatial prior for noise robust ASR[J].IEEE/ACM Transactions on Audio Speech and Language Processing,2017,25(4):780-793.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_5" title=" PFEIFENBERGER L,SCHRANK T,Z&#214;HRER M,et al.Multi-channel speech processing architectures for noise robust speech recognition:3rd CHiME challenge results [C]// Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway:IEEE,2016:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-channel speech processing architectures for noise robust speech recognition:3rd CHiME challenge results">
                                        <b>[5]</b>
                                         PFEIFENBERGER L,SCHRANK T,Z&#214;HRER M,et al.Multi-channel speech processing architectures for noise robust speech recognition:3rd CHiME challenge results [C]// Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway:IEEE,2016:1-7.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     曾庆宁,卜玉婷,刘伟波.一种适用于噪声环境下的语音识别方法:201910581762.8[P].2019- 06- 30.(ZENG Q N,BU Y T,LIU W B.A speech recognition method suitable for noise environments:201910581762.8[P].2019- 06- 30.)</a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_7" title=" TASESKA M,HABETS E A P.Informed spatial filtering for sound extraction using distributed microphone arrays [J].IEEE/ACM Transactions on Audio,Speech,and Language Processing,2014,22(7):1195-1207." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM4CD82A07749E698B97723A0153219B34&amp;v=MDE0MDR4MlFhN1RoL1N3N2lyUmMyZTdPZE43bWJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4YnEzeEs0PU5pZklZN2ZMYXRuTzNvOUNZKzhHZVhvdw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         TASESKA M,HABETS E A P.Informed spatial filtering for sound extraction using distributed microphone arrays [J].IEEE/ACM Transactions on Audio,Speech,and Language Processing,2014,22(7):1195-1207.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_8" title=" 曾庆宁,肖强,王瑶,等.一种双微阵列语音增强方法[J].电子与信息学报,2018,40(5):1187-1194.(ZENG Q N,XIAO Q,WANG Y,et al.A dual micro-array speech enhancement method[J].Journal of Electronics &amp;amp; Information Technology,2018,40(5):1187-1194.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201805025&amp;v=MDMzMDVyRzRIOW5NcW85SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5XN3ZPSVRmU2Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         曾庆宁,肖强,王瑶,等.一种双微阵列语音增强方法[J].电子与信息学报,2018,40(5):1187-1194.(ZENG Q N,XIAO Q,WANG Y,et al.A dual micro-array speech enhancement method[J].Journal of Electronics &amp;amp; Information Technology,2018,40(5):1187-1194.)
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_9" title=" CAPON J,GREENFIELD R J,KOLKER R J.Multidimensional maximum-likelihood processing of a large aperture seismic array[J].Proceedings of the IEEE,1967,55(2):192-211." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multidimensional maximum-likelihood processing of a large aperture seismic array">
                                        <b>[9]</b>
                                         CAPON J,GREENFIELD R J,KOLKER R J.Multidimensional maximum-likelihood processing of a large aperture seismic array[J].Proceedings of the IEEE,1967,55(2):192-211.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_10" title=" 施荣华,孟秋杰,董健,等.一种基于对角载入的鲁棒MVDR波束形成算法[J].湖南大学学报(自然科学版),2012,39(9):57-61.(SHI R H,MENG Q J,DONG J,et al.A robust adaptive beamforming algorithm based on diagonal loading[J].Journal of Hunan University (Natural Sciences),2012,39(9):57-61.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HNDX201209010&amp;v=MjQxNDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublc3dk9MU1BQZHJHNEg5UE1wbzlFWklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         施荣华,孟秋杰,董健,等.一种基于对角载入的鲁棒MVDR波束形成算法[J].湖南大学学报(自然科学版),2012,39(9):57-61.(SHI R H,MENG Q J,DONG J,et al.A robust adaptive beamforming algorithm based on diagonal loading[J].Journal of Hunan University (Natural Sciences),2012,39(9):57-61.)
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_11" title=" MITRA V,van HOUT J,WANG W,et al.Improving robustness against reverberation for automatic speech recognition[C]// Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway:IEEE,2015:525-532." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving robustness against reverberation for automatic speech recognition">
                                        <b>[11]</b>
                                         MITRA V,van HOUT J,WANG W,et al.Improving robustness against reverberation for automatic speech recognition[C]// Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway:IEEE,2015:525-532.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_12" title=" MITRA V,WANG W,BARTELS C,et al.Articulatory information and multiview features for large vocabulary continuous speech recognition[C]// Proceedings of the 2018 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2018:5634-5638." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Articulatory information and multiview features for large vocabulary continuous speech recognition">
                                        <b>[12]</b>
                                         MITRA V,WANG W,BARTELS C,et al.Articulatory information and multiview features for large vocabulary continuous speech recognition[C]// Proceedings of the 2018 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2018:5634-5638.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_13" title=" 陈紫强,李欣阳,谢跃雷.结合相位谱补偿的调制域谱减法[J].信号处理,2015,31(4):468-473.(CHEN Z Q,LI X Y,XIE Y L.Modulation domain spectral subtraction combined with phase spectrum compensation[J].Journal of Signal Processing,2015,31(4):468-473.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201504012&amp;v=MDkzOTZxQnRHRnJDVVI3cWZadVpzRnlublc3dk9QVFhJWUxHNEg5VE1xNDlFWm9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         陈紫强,李欣阳,谢跃雷.结合相位谱补偿的调制域谱减法[J].信号处理,2015,31(4):468-473.(CHEN Z Q,LI X Y,XIE Y L.Modulation domain spectral subtraction combined with phase spectrum compensation[J].Journal of Signal Processing,2015,31(4):468-473.)
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_14" title=" QIAN Y,TAN T,YU D.Neural network based multi-factor aware joint training for robust speech recognition [J].IEEE/ACM Transactions on Audio Speech &amp;amp; Language Processing,2017,24(12):2231-2240." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM070F465EFD34CB9B3ED6206B98EFDBC1&amp;v=MDMyMzlNQ0E5THhtUVFuMHQ3U24vazNoczlETVRnTjhtZUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhicTN4SzQ9TmlmSVk3Ty9IcWZJcVlvd0VwOA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         QIAN Y,TAN T,YU D.Neural network based multi-factor aware joint training for robust speech recognition [J].IEEE/ACM Transactions on Audio Speech &amp;amp; Language Processing,2017,24(12):2231-2240.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_15" title=" 张晴晴,刘勇,潘接林,等.基于卷积神经网络的连续语音识别[J].工程科学学报,2015,37(9):1212-1217.(ZHANG Q Q,LIU Y,PAN J L,et al.Continuous speech recognition based on convolutional neural networks[J].Chinese Journal of Engineering,2015,37(9):1212-1217.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJKD201509015&amp;v=MDg4OTBUTXBvOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVzd2T0p5ZkFhckc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         张晴晴,刘勇,潘接林,等.基于卷积神经网络的连续语音识别[J].工程科学学报,2015,37(9):1212-1217.(ZHANG Q Q,LIU Y,PAN J L,et al.Continuous speech recognition based on convolutional neural networks[J].Chinese Journal of Engineering,2015,37(9):1212-1217.)
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_16" title=" 周志华.机器学习[M].北京:清华大学出版社,2016:97-140.(ZHOU Z H.Machine Learning[M].Beijing:Tsinghua University Press,2016:97-140.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MTQ2MzE5ZmJ2bktyaWZaZVp2RnlublU3N0tLRm9TWEZxekdiQzRITlhPckkxTlkrc1BEQk04enhVU21EZDlTSDduM3hF&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         周志华.机器学习[M].北京:清华大学出版社,2016:97-140.(ZHOU Z H.Machine Learning[M].Beijing:Tsinghua University Press,2016:97-140.)
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_17" title=" CHAN W,LANE I.Deep convolutional neural networks for acoustic modeling in low resource languages[C]// Proceedings of the 2015 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2015:2056-2060." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep convolutional neural networks for acoustic modeling in low resource languages">
                                        <b>[17]</b>
                                         CHAN W,LANE I.Deep convolutional neural networks for acoustic modeling in low resource languages[C]// Proceedings of the 2015 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2015:2056-2060.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_18" title=" 马金龙,曾庆宁,龙超,等.多噪声环境下可懂度提升的助听器语音增强[J].计算机工程与设计,2016,37(8):2160-2164.(MA J L,ZENG Q N,LONG C,et al.Intelligibility improved speech enhancement for hearing aids in complex noise environment [J].Computer Engineering and Design,2016,37(8):2160-2164.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201608034&amp;v=MjU3NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublc3dk9OaWZZWkxHNEg5Zk1wNDlHWUk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         马金龙,曾庆宁,龙超,等.多噪声环境下可懂度提升的助听器语音增强[J].计算机工程与设计,2016,37(8):2160-2164.(MA J L,ZENG Q N,LONG C,et al.Intelligibility improved speech enhancement for hearing aids in complex noise environment [J].Computer Engineering and Design,2016,37(8):2160-2164.)
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-09-16 10:30</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3268-3273 DOI:10.11772/j.issn.1001-9081.2019050878            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于双微阵列与卷积神经网络的语音识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E4%BC%9F%E6%B3%A2&amp;code=38491594&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘伟波</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%BE%E5%BA%86%E5%AE%81&amp;code=10684072&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曾庆宁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%9C%E7%8E%89%E5%A9%B7&amp;code=42592761&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卜玉婷</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E5%B1%95%E6%81%92&amp;code=15091618&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑展恒</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A1%82%E6%9E%97%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%AD%A6%E9%99%A2&amp;code=0269119&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂林电子科技大学信息与通信学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为解决噪声环境下语音识别率降低以及传统波束形成算法难以处理空间噪声的问题,基于双微阵列结构提出了一种改进的最小方差无畸变响应(MVDR)波束形成方法。首先,采用对角加载提高双微阵列增益,并利用递归矩阵求逆降低计算复杂度;然后,通过后置调制域谱减法对语音作进一步处理,解决了一般谱减法容易产生音乐噪声的问题,有效减小了语音畸变,获得了良好的噪声抑制效果;最后,采用卷积神经网络(CNN)进行语音模型的训练,提取语音深层次的特征,有效地解决了语音信号多样性问题。实验结果表明,提出的方法在经CNN训练的语音识别系统模型中取得了较好的识别效果,在信噪比为10 dB的F16噪声环境下的语音识别率达到了92.3%,具有良好的稳健性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E5%BE%AE%E9%98%B5%E5%88%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双微阵列;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%99%AA%E5%A3%B0%E7%8E%AF%E5%A2%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">噪声环境;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%B3%E5%81%A5%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稳健性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘伟波(1991—),男,河南商丘人,硕士研究生,主要研究方向:语音识别;;
                                </span>
                                <span>
                                    *曾庆宁(1963—),男,广西桂林人,教授,博士,主要研究方向:语音信号处理、图像处理,电子邮箱,1975420119@qq.com;
                                </span>
                                <span>
                                    卜玉婷(1995—),女,湖南益阳人,硕士研究生,主要研究方向:语音信号处理;;
                                </span>
                                <span>
                                    郑展恒(1978—),男,陕西杨凌人,高级实验师,硕士,主要研究方向:语音信号处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61461011);</span>
                                <span>广西自然科学基金重点项目(2016GXNSFDA380018);</span>
                                <span>“认知无线电与信息处理”教育部重点实验室主任基金资助项目(CRKL160107,CRKL170108);</span>
                    </p>
            </div>
                    <h1><b>Speech recognition method based on dual micro-array and convolutional neural network</b></h1>
                    <h2>
                    <span>LIU Weibo</span>
                    <span>ZENG Qingning</span>
                    <span>BU Yuting</span>
                    <span>ZHENG Zhanheng</span>
            </h2>
                    <h2>
                    <span>School of Information and Communication, Guilin University of Electronic Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the low speech recognition rate in noise environment, and the difficulty of traditional beamforming algorithm in dealing with spatial noise problem, an improved Minimum Variance Distortionless Response(MVDR) beamforming method based on dual micro-array was proposed. Firstly, the gain of micro-array was increased by diagonal loading, and the computational complexity was reduced by the inversion of recursive matrix. Then, through the modulation domain spectrum subtraction for further processing, the problem that music noise was easily produced by general spectral subtraction was solved, effectively reducing speech distortion, and well suppressing the noise. Finally, the Convolution Neural Network(CNN) was used to train the speech model and extract the deep features of speech, effectively solve the problem of speech signal diversity. The experimental results show that the proposed method achieves good recognition effect in the CNN trained speech recognition system, and has the speech recognition accuracy of 92.3% in F16 noise environment with 10 dB signal-to-noise ratio, means it has good robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=speech%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">speech recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dual%20micro-array&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dual micro-array;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=noise%20environment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">noise environment;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=robustness&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">robustness;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LIU Weibo, born in 1991, M. S. candidate. His research interests include speech recognition. ;
                                </span>
                                <span>
                                    ZENG Qingning, born in 1963, Ph. D., professor. His research interests include speech signal processing, image processing. ;
                                </span>
                                <span>
                                    BU Yuting, born in 1995, M. S. candidate. Her research interests include speech signal processing. ;
                                </span>
                                <span>
                                    ZHENG Zhanheng, born in 1978, M. S., senior experimentalist. His research interests include speech signal processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-23</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China(61461011);</span>
                                <span>the State Key Program of National Nature Science of Guangxi Zhuang Autonomous Region(2016GXNSFDA380018);</span>
                                <span>the Director Fund of Key Laboratory of Cognitive Radio and Information Processing of Ministry of Education(CRKL160107,CRKL170108);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="40">语音识别技术作为模式识别的一个重要分支,主要目的是让机器理解人类所说内容的含义,从而实现人与机器自然的交流,以摆脱现有的以文本输入形式进行交互的局限性。语音作为目前人机交互最便捷的方式,具有高效、直接、自然的特性,也是人类本身之间最方便快捷的沟通交流方式之一<citation id="240" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。目前人机智能语音交互的最大意义就是可以彻底解放人的双手,降低学习成本。但是,在实际应用时,语音总会受到环境噪声或者传输介质的干扰,导致音质受损,影响其中蕴含的语言信息正常传递,所以,如何对复杂环境下的语音进行处理,减少噪声以及干扰的影响,进而提升语音识别系统的稳健性就显得至关重要<citation id="241" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="41">近年来,研究者在噪声环境下以及远场语音识别应用领域探索了广泛的波束形成实现方法,例如,Zhang等<citation id="242" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出了一种基于深度神经网络的麦克风阵列降噪算法,有效提高了真实噪声环境下的语音识别率;Higuchi等<citation id="243" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>使用基于时频掩蔽的波束形成方法应用到语音识别任务中,有效抑制了噪声干扰并提高了语音识别率;Lukas等<citation id="244" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>研究了广义旁瓣抵消波束形成器的三种变体结构,并应用后置滤波器来进一步增强语音信号的方法在CHiME 国际多通道语音分离与识别大赛中取得了优异的成绩。以上研究成果表明,针对噪声或干扰语音设计的麦克风阵列语音算法可以作为语音识别系统的输入端处理算法,达到提高语音质量的目的,对噪声及远场环境下的语音识别性能有一定的提升。</p>
                </div>
                <div class="p1">
                    <p id="42">本文针对传统波束形成算法难以处理的空间噪声问题,提出了一种综合了最小方差无畸变响应(Minimum Variance Distortionless Response, MVDR)波束形成与对角加载的麦克风阵列语音波束形成方法,并通过后置调制域谱减法对语音进一步处理,获得了较好的降噪效果和识别准确率(该方法已申请发明专利<citation id="245" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>)。该方法中定义了一个加权因子,对语音信号采样协方差矩阵进行对角加载处理,使得环境噪声中非相关性比较强的协方差元素得到抑制,降低噪声增益从而达到提高阵列增益的目的<citation id="246" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。为了降低计算量和复杂度,本文采用了递推算法,减少矩阵求逆运算。另外,通过采用卷积神经网络(Convolutional Neural Network, CNN)从原始语音数据中提取高维隐含的特征,有力地处理数据的可变性和丰富性,减少了神经网络训练的参数。实验结果表明,该方法在噪声抑制和提升语音质量方面均取得了优于传统方法的效果,在噪声环境下的语音识别系统中具有一定的稳健性。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">1 递归矩阵求逆的MVDR</h3>
                <h4 class="anchor-tag" id="44" name="44">1.1 <b>双微阵列</b></h4>
                <div class="p1">
                    <p id="45">图1所示为本文采用的双微阵列,采用KEMAR人工头模拟人耳特性。两侧分别是一个微型阵列,左右阵间距离为模拟人耳的间距,一般为12～18 cm,微型阵内距离为2～4 cm,此时阵间麦克风采集的噪声信号相关性较弱或者是非相关噪声,阵内麦克风采集的噪声相关性较强<citation id="247" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。有研究表明,麦克风间距在12～18 cm的噪声低频带相关性较强,而高频带的噪声相关性较弱,因此利用相关性算法能够达到较好的去噪效果<citation id="248" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911027_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 双微阵列结构" src="Detail/GetImg?filename=images/JSJY201911027_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 双微阵列结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911027_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure of dual micro-array</p>

                </div>
                <h4 class="anchor-tag" id="47" name="47">1.2 <b>最小方差无畸变响应</b></h4>
                <div class="p1">
                    <p id="48">双微麦克风阵列接收到的语音信号模型采用如下形式表示:</p>
                </div>
                <div class="p1">
                    <p id="49"><i><b>y</b></i><sub><i>m</i></sub>(<i>t</i>)=<i><b>x</b></i><sub><i>m</i></sub>(<i>t</i>)+<i><b>n</b></i><sub><i>m</i></sub>(<i>t</i>); <i>m</i>=1,2,…,<i>M</i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="50">其中: <i>t</i>表示时间索引,<i>M</i>表示麦克风数量,<i><b>x</b></i><sub><i>m</i></sub>(<i>t</i>)表示纯净语音信号,<i><b>n</b></i><sub><i>m</i></sub>(<i>t</i>)表示加性噪声和干扰信号,<i><b>y</b></i><sub><i>m</i></sub>(<i>t</i>)表示含噪语音信号。</p>
                </div>
                <div class="p1">
                    <p id="51">对式(1)进行傅里叶变换,得到频域表示形式</p>
                </div>
                <div class="p1">
                    <p id="52"><i><b>Y</b></i>(<i>l</i>,<i>k</i>)=<i><b>X</b></i>(<i>l</i>,<i>k</i>)+<i><b>N</b></i>(<i>l</i>,<i>k</i>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="53">其中: <i>l</i>是时间索引,<i>k</i>是频率索引,下文为简洁起见,适当省略部分索引符号。</p>
                </div>
                <div class="p1">
                    <p id="54">最小方差无畸变响应波束形成器是由Capon首先提出的一种经典的麦克风阵列自适应波束形成方法,满足目标信号无失真的条件下最小化阵列输出的约束,可以有效实现抑制噪声以及非目标方向干扰的目的<citation id="249" type="reference"><link href="220" rel="bibliography" /><link href="222" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。MVDR需满足以下约束优化条件:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>w</mi></munder><mo stretchy="false">{</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">W</mi><mo stretchy="false">}</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mtd></mtr></mtable></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">其中: <i><b>D</b></i>(<i>k</i>)是期望信号导向矢量,<i><b>R</b></i><sub><i>n</i></sub> 是噪声协方差矩阵。采用Lagrange算子,定义函数<citation id="250" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>λ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">w</mi><mo>+</mo><mi>λ</mi><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">将该函数对<i><b>w</b></i>求导,并令该导数为0,得到:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">w</mi></mrow></mfrac><mo>=</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">w</mi><mo>-</mo><mi>λ</mi><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">进行求解得到加权向量:</p>
                </div>
                <div class="p1">
                    <p id="61"><i><b>W</b></i>=<i>λ</i><i><b>R</b></i><sub><i>n</i></sub><sup>-1</sup><i><b>D</b></i>(<i>k</i>)      (6)</p>
                </div>
                <div class="p1">
                    <p id="62">代入前面式(3)的约束条件中,求得:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Η</mtext></msup><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">把式(7)代入式(6)中可求出最优加权向量为:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>V</mtext><mtext>D</mtext><mtext>R</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Η</mtext></msup><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">MVDR波束形成器要求精确的导向矢量和噪声协方差矩阵估计,而在实际应用环境中,由于麦克风接收到的信号的导向矢量存在偏差以及协方差矩阵估计不准确,使得MVDR波束形成器的性能下降,不能有效地抑制干扰。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">1.3 <b>对角加载</b></h4>
                <div class="p1">
                    <p id="68">假设导向矢量和协方差矩阵都没有误差,考虑<i>M</i>个麦克风组成的双微阵列结构,假设语音信号和噪声之间互不相关,一般通过有限次快拍估计的数据协方差矩阵代替噪声的协方差矩阵<citation id="251" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation></p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="bold-italic">R</mi><mo>^</mo><mspace width="0.25em" /></mrow><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi mathvariant="bold-italic">y</mi></mstyle><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中<i>K</i>为快拍数。在实际应用时,阵列结构的导向矢量存在偏差,麦克风接收的数据里面也往往含有一部分的期望信号,这就导致无法得到理想的噪声协方差矩阵。因此本文引入对角加载的方法来抑制协方差矩阵中小特征值扰动造成偏差的输出性能的影响<citation id="252" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="71"><i><b>R</b></i><mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>ε</mi></msubsup></mrow></math></mathml>=<i><b>R</b></i>^ <sub><i>n</i></sub>+<i>ε</i><i><b>I</b></i>      (10)</p>
                </div>
                <div class="p1">
                    <p id="72">其中:<i><b>R</b></i>^ <sub><i>n</i></sub>为进行对角加载前的噪声信号的协方差矩阵;<i><b>R</b></i><mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>ε</mi></msubsup></mrow></math></mathml>为对角加载后协方差矩阵;<i>ε</i>为对角加载量,本文取0.01,<i><b>I</b></i>为单位矩阵。将协方差矩阵对角加载抑制非相关噪声,式(10)中的协方差矩阵为:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mi>n</mi><mi>ε</mi></msubsup><mo>=</mo><mrow><mo>(</mo><mrow><mtable><mtr><mtd><mi>r</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mo>+</mo><mi>ε</mi></mtd><mtd><mi>r</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>r</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>Ν</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>r</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>r</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub><mo>+</mo><mi>ε</mi></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>r</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>Ν</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>r</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mn>1</mn></mrow></msub></mtd><mtd><mi>r</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>r</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mi>Ν</mi></mrow></msub><mo>+</mo><mi>ε</mi></mtd></mtr></mtable></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">得到对角加载后的权向量为:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>V</mtext><mtext>D</mtext><mtext>R</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mi>n</mi><mi>ε</mi></msubsup><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Η</mtext></msup><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mi>n</mi><mi>ε</mi></msubsup><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">1.4 <b>递归矩阵求逆</b></h4>
                <div class="p1">
                    <p id="77"><i>MVDR</i>波束形成器中的加权向量是通过对协方差矩阵进行求逆运算得到的最佳权值矢量,其计算复杂度较高,运算量会随着阵元数目和采样数目的增加逐渐剧增,为了降低计算量和复杂度,本文采用了递推算法,减少矩阵求逆运算量<citation id="253" type="reference"><link href="224" rel="bibliography" /><link href="226" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。假设对经双微阵列结构采集的含噪语音进行处理时,噪声与语音信号相互独立,含噪语音信号的功率谱密度为<i>Φ</i><sub><i>y</i></sub>=<i>E</i>(<i><b>YY</b></i><sup>H</sup>),且满足有<i>Φ</i><sub><i>y</i></sub>=<i>Φ</i><sub><i>x</i></sub>+<i>Φ</i><sub><i>n</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="78">对功率谱密度在时间上求平均进行估计:</p>
                </div>
                <div class="p1">
                    <p id="79"><i>Φ</i><sub><i>n</i></sub>(<i>l</i>)=<i>α</i><sub><i>n</i></sub><i>Φ</i><sub><i>n</i></sub>(<i>l</i>-1)+[1-<i>α</i><sub><i>n</i></sub>]<i><b>Y</b></i>(<i>l</i>)<i><b>Y</b></i>(<i>l</i>)<sup>H</sup>      (13)</p>
                </div>
                <div class="p1">
                    <p id="80"><i>Φ</i><sub><i>x</i></sub><sub>+</sub><sub><i>n</i></sub>(<i>l</i>)=<i>α</i><sub><i>x</i></sub><i>Φ</i><sub><i>x</i></sub><sub>+</sub><sub><i>n</i></sub>(<i>l</i>-1)+[1-<i>α</i><sub><i>x</i></sub>]<i><b>Y</b></i>(<i>l</i>)<i><b>Y</b></i>(<i>l</i>)<sup>H</sup>      (14)</p>
                </div>
                <div class="p1">
                    <p id="81">其中:<i>Φ</i><sub><i>n</i></sub>为噪声的功率谱密度,<i>Φ</i><sub><i>x</i></sub><sub>+</sub><sub><i>n</i></sub>为含噪语音的功率谱密度;<i>α</i><sub><i>n</i></sub>和<i>α</i><sub><i>x</i></sub>为一固定常数,通过式(15)求得:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mn>1</mn><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>x</mi></msub></mtd></mtr><mtr><mtd><mi>α</mi><msub><mrow></mrow><mi>x</mi></msub><mo>=</mo><mn>1</mn><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>α</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">对含噪语音端点检测,检测到语音帧时<i>μ</i><sub><i>x</i></sub>=1,噪声帧时<i>μ</i><sub><i>x</i></sub>=0,其中<mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>α</mi><mo>˜</mo></mover></math></mathml><sub><i>x</i></sub>∈(0,1]本文取固定常数0.95。</p>
                </div>
                <div class="p1">
                    <p id="84">该递推算法的基本流程<citation id="254" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是:</p>
                </div>
                <div class="p1">
                    <p id="85">1)计算初始噪声的相关矩阵即功率谱密度进行求逆运算,得到初始化权值;</p>
                </div>
                <div class="p1">
                    <p id="86">2)开始对初始噪声段进行Woodbury更新;</p>
                </div>
                <div class="p1">
                    <p id="87">3)进入语音段处理,用前一帧的求逆相关矩阵替代当前帧的相关矩阵;</p>
                </div>
                <div class="p1">
                    <p id="88">4)进入噪声段处理,对2)中的相关矩阵进行Woodbury更新;</p>
                </div>
                <div class="p1">
                    <p id="89">5)递推完成整个信号长度的运算。</p>
                </div>
                <div class="p1">
                    <p id="90">根据矩阵求逆引理,相关矩阵求逆运算后可表示为:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Φ</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi>Φ</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mi>Φ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo><mi>Φ</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo><mi>Φ</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">对式(16)进一步简化<citation id="255" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,令:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mi>Φ</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow><mrow><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo><mi>Φ</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">最后递推得到MVDR波束形成器的加权向量为:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>V</mtext><mtext>D</mtext><mtext>R</mtext></mrow></msub><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Η</mtext></msup><mi>Φ</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>v</mi></mrow></msub><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Η</mtext></msup><mi>Φ</mi><msub><mrow></mrow><mi>n</mi></msub><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mi mathvariant="bold-italic">Ι</mi><mo>-</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>n</mi></msub><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>V</mtext><mtext>D</mtext><mtext>R</mtext></mrow></msub><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">将求得的递归矩阵求逆的MVDR的权值矢量系数与含噪双微阵列语音信号的频谱相乘即得到进行波束形成增强后的语音信号。</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>V</mtext><mtext>D</mtext><mtext>R</mtext></mrow></msub><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="98" name="98" class="anchor-tag">2 调制域谱减法</h3>
                <div class="p1">
                    <p id="99">语音增强的目标是提高噪声环境下的语音质量,这就需要在噪声抑制和引起的语音失真之间找到一个平衡点。在许多增强方法中,谱减法是一种简单而有效的方法,适用于平稳或缓慢变化的加性噪声环境,常见的功率谱谱减法,侧重于信号幅度的估计,在信号重构中采用带噪相位,即信号功率谱由含噪信号功率谱的估计值减去噪声功率谱的估计值来估计<citation id="256" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。当信噪比较高时,含噪语音相位接近于纯净语音信号相位,可直接用于纯净语音的估计,但随着信噪比的降低,噪声相位在信号重构中变得不足。同时,传统谱减法引起的“音乐噪声”问题也需要解决,因此,本文引入调制域谱减法进行改善。</p>
                </div>
                <div class="p1">
                    <p id="100">与频域表示的是频率与幅度间关系和时域表示时间和幅度间关系不同,调制域是时间和频率之间的相互关系,三者之间可由图2所示的坐标系表示<citation id="257" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="101">对式(19)求得的语音信号的频谱估计<mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover></math></mathml>(<i>l</i>,<i>k</i>)使用极坐标表示形式:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mi>j</mi><mo>∠</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">式(20)中|<mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover></math></mathml>(<i>l</i>,<i>k</i>)|表示语音幅度谱,∠<mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover></math></mathml>(<i>l</i>,<i>k</i>)表示语音相位谱,对|<mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover></math></mathml>(<i>l</i>,<i>k</i>)|再次进行傅里叶变换,从频域进入调制域</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="bold-italic">X</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="bold-italic">Ρ</mi><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mtext>j</mtext><mo>∠</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">其中,<i>l</i>表示调制帧,<i>u</i>表示调制频率,|<mathml id="171"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover></math></mathml>(<i>l</i>,<i>k</i>,<i>u</i>)|表示调制域幅度谱,∠<mathml id="172"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover></math></mathml>(<i>l</i>,<i>k</i>,<i>u</i>)表示调制域相位谱,<i><b>P</b></i>(<i>l</i>,<i>k</i>,<i>u</i>)表示噪声调制谱,<i><b>X</b></i>^ (<i>l</i>,<i>k</i>,<i>u</i>)是调制域谱减法处理后估计的语音信号。在实际计算中,通常对信号进行端点检测(Voice Activity Detection,VAD),只在噪声段进行估计和更新噪声,或者取信号的前几帧作静音段,取这几帧分幅度平均值作为整体估计出的噪声。调制域谱减法的原理与经典谱减法原理相似,通过下面计算可得到调制域幅度谱<citation id="258" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo stretchy="false">|</mo><mrow><mi mathvariant="bold-italic">X</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mi>η</mi><mo stretchy="false">|</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>,</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>≥</mo><mo stretchy="false">(</mo><mi>λ</mi><mo>+</mo><mi>η</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Ρ</mi><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>,</mo><mtext> </mtext><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">其中:<i>η</i>是过减因子,<i>λ</i>是增益补偿因子,<i><b>P</b></i>^ (<i>l</i>,<i>k</i>,<i>u</i>)是估计的调制噪声谱,由式(24)得到:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mi>γ</mi><mo stretchy="false">|</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>γ</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中<i>γ</i>是平滑系数。得到语音调制域幅度谱后再结合含噪语音调制域相位谱进行傅里叶逆变换可得估计的最终纯净语音信号频域幅度谱:</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="bold-italic">X</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">|</mo><mrow><mi mathvariant="bold-italic">X</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mtext>j</mtext><mo>∠</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112"><i><b>X</b></i>^ (<i>l</i>,<i>k</i>)=<i>IDFT</i>(<i><b>X</b></i>^ (<i>l</i>,<i>k</i>,<i>u</i>))      (26)</p>
                </div>
                <div class="p1">
                    <p id="113">对式(26)得到的语音幅度谱,再结合含噪语音的相位谱进行傅里叶逆变换即得到最终消噪后的信号:</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ι</mi><mi>D</mi><mi>F</mi><mi>Τ</mi><mo stretchy="false">(</mo><mrow><mi mathvariant="bold-italic">X</mi><mo>^</mo><mspace width="0.25em" /></mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911027_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 时域、频域与调制域关系图" src="Detail/GetImg?filename=images/JSJY201911027_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 时域、频域与调制域关系图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911027_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Relation diagram of time domain, frequency domain and modulation domain</p>

                </div>
                <h3 id="116" name="116" class="anchor-tag">3 卷积神经网络模型结构</h3>
                <div class="p1">
                    <p id="117">本文采用<i>CNN</i>进行语音模型的训练,其中卷积层和池化层是其核心也是优势所在,相比全连接神经网络的优势在于可以大幅减少参数的数量。本文卷积神经网络模型结构如图3所示,包含卷积层1、池化层1、卷积层2、池化层2、全连接层1、全连接层2。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 CNN模型示意图" src="Detail/GetImg?filename=images/JSJY201911027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>CNN</i>模型示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Schematic diagram of CNN model</i></p>

                </div>
                <h4 class="anchor-tag" id="119" name="119">3.1 <b>卷积层</b></h4>
                <div class="p1">
                    <p id="120">卷积层能够实现神经网络中的局部连接和权值共享功能,局部连接利用了语音短时时间内语音特征信息相关性强,距离较远的特征相关联性较弱的特点,只对局部特征进行学习,在神经网络的更高层对各局部特征进行综合,相对于全连接层全部神经元都进行连接的特点,<i>CNN</i>层与层间的连接只是上一层神经元节点只与下一层的部分节点连接<citation id="259" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。<i>CNN</i>的卷积操作通过卷积核在特征平面上滑动得到,形成局部连接的区域,卷积核的权重随机初始化得到,通过训练过程不断更新,直到网络达到最优。经卷积核卷积运算后的每张特征图(<i>feature map</i>)就是一种局部的卷积方式,这样可以提取不同特征的局部表示,卷积运算的方式如式(28)<citation id="260" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>所示:</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mi>k</mi></msubsup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>k</mi></msubsup><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>k</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">其中:<i>x</i>表示第<i>k</i>层的输出作为<i>k</i>+1层的输入,<i>y</i>表示第<i>k</i>+1层的输出,<i>σ</i>表示激活函数,<i>w</i><mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>k</mi></msubsup></mrow></math></mathml>表示第<i>k</i>层的第<i>i</i>个神经单元与第<i>k</i>+1层的第<i>j</i>个神经单元之间的权值矩阵,<i>b</i>是第<i>k</i>+1层的偏置项。</p>
                </div>
                <div class="p1">
                    <p id="123">本文上述激活函数采用Leaky relus函数,它是在ReLU激活函数基础上改进的,保留了一部分负值,当输入为负值时乘以较小的系数<i>a</i>,本文中取0.01,其数学表达式<citation id="261" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>为:</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>x</mi><mo>,</mo></mtd><mtd columnalign="left"><mi>x</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mi>a</mi><mi>x</mi><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="125" name="125">3.2 <b>池化层</b></h4>
                <div class="p1">
                    <p id="126">卷积神经网络的池化层处理可以使网络参数大幅减小,而且对说话人差异引起的信号变化具有更好的鲁棒性,实现了对高维度特征的抽象获取。池化层的输入是前一卷积层的局部区域进行下采样等得到,下采样大幅降低了计算复杂度,也提高了模型的泛化能力<citation id="262" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。通常进行最大池化(<i>max pooling</i>)和均值池化(<i>average pooling</i>)操作,最大池化是取池化范围内的最大元素,可以减少卷积层参数误差导致的估计偏差,均值池化是取池化范围内所有元素的算术平均值,在语音识别研究领域,通常采用最大池化。最大池化的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="127" class="code-formula">
                        <mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>=</mo><munderover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mo stretchy="false">(</mo><mi>q</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>,</mo><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mi>s</mi><mo>+</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="128">其中:<i>G</i>表示池化尺寸;<i>s</i>表示步长,决定相邻采样窗口的重叠程度。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">3.3 <b>损失函数</b></h4>
                <div class="p1">
                    <p id="130">损失函数是进行网络模型训练的关键,用来描述预测值与真实值之间的差距,一般有均值平方差(<i>Mean Squared Error</i>,<i>MSE</i>)损失函数和交叉熵(<i>Cross Entropy</i>,<i>CE</i>)损失函数<citation id="263" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。本文训练模型时采用交叉熵函数,它一般用于分类问题,表示预测样本属于某一类别的概率,表达式如式(31)所示:</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>x</mi></munder><mo stretchy="false">[</mo></mstyle><mi>y</mi><mspace width="0.25em" /><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mi>a</mi><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>y</mi><mo stretchy="false">)</mo><mspace width="0.25em" /><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">其中:<i>y</i>表示真实的分类,<i>a</i>表示预测值。</p>
                </div>
                <div class="p1">
                    <p id="133">上述交叉熵的值越小,代表预测结果越准确,模型越好。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">3.4 <b>模型训练</b></h4>
                <div class="p1">
                    <p id="135">卷积神经网络的训练过程分为前向传播与反向传播两个部分,假设已经对所有权值矩阵和偏置进行初始化,已知提取后的输入特征参数矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136">1)前向传播。</h4>
                <div class="p1">
                    <p id="137">①对于卷积层来说,卷积核与输入特征矩阵对应位置相乘后再相加,得到输出矩阵对应位置的值。假设输入矩阵为M*N大小,卷积核为a*b大小,则输出矩阵大小为(M-a+1)*(N-b+1)大小<citation id="264" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="138">②对于池化层来说,前向传播按照选定的池化规则把输入张量进行降维,比如最大池化,只取池化窗口的最大值作为输出值。</p>
                </div>
                <div class="p1">
                    <p id="139">③全连接层的前向传播与一般的前向传播方式一致。</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140">2)反向传播。</h4>
                <div class="p1">
                    <p id="141">卷积神经网络的反向传播由于其卷积层和池化层的特殊性,与一般的反向传播不一样。首先,池化层没有激活函数,这个问题可以容易解决,可以将激活函数看成其本身,则池化层激活函数的导数为1;另外,池化层进行前向传播的过程中对输入进行了压缩降维,所以进行反向传播推到上一层误差时,需要进行上采样; 最后,卷积层是通过卷积后求和得到输出,进行上一层误差的计算方式不同<citation id="265" type="reference"><link href="232" rel="bibliography" /><link href="234" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="142">假设通过前向传播计算后输出层:</p>
                </div>
                <div class="p1">
                    <p id="143"><i>a</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup>=<i>σ</i>(<i>z</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup>)=<i>σ</i>(<i>w</i><sup><i>l</i></sup><i>a</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup><sup>-1</sup>+<i>b</i>)      (32)</p>
                </div>
                <div class="p1">
                    <p id="144">定义损失函数为<i>J</i>(<i>w</i>,<i>b</i>),则输出层的误差可计算为:</p>
                </div>
                <div class="p1">
                    <p id="145"><i>δ</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup>=-(<i>y</i>-<i>a</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup>)·<i>σ</i>′(<i>z</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup>)      (33)</p>
                </div>
                <div class="p1">
                    <p id="146">在反向传播时,首先会把<i>δ</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup>的所有子矩阵还原成进行池化之前的大小:如果是最大池化,则把<i>δ</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup>的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置;如果是均值池化,则把<i>δ</i><sup><i>i</i></sup><sup>,</sup><sup><i>l</i></sup>的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置,这个过程一般叫作upsample<citation id="266" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="147">根据以上前向传播与后向传播训练准则,逐层进行训练,对权值系数矩阵和偏置进行更新,损失函数的值逐渐变小,直到完成模型的训练过程。</p>
                </div>
                <h3 id="148" name="148" class="anchor-tag">4 实验与分析</h3>
                <div class="p1">
                    <p id="149">本文实验所采用的数据通过在安静的实验室环境下使用<i>KEMAR</i>人工头设备以及<i>M</i>-<i>Audio</i>多路音频采集器采集。采用四通道的双微阵列结构录制,其中使用双微阵列采集的纯净语音数据进行模型训练,实验语音数据共30人进行录制,其中男20人,女10人,每人读取25条不同的语音,每条两遍,共1 500条语句,其中60%作为训练集,20%作为验证集,20%的数据作为测试集,测试集的语音加入不同信噪比的噪声,进行后续处理以验证本文算法以及对比算法的效果。</p>
                </div>
                <div class="p1">
                    <p id="150">本文采用的语音特征参数为13维的梅尔频率倒谱系数(<i>Mel Frequency Cepstrum Coefficient</i>, <i>MFCC</i>)及其一阶差分、二阶差分共同组成39维的特征向量,然后拼接当前帧结合前后各5帧的共11帧参数,不足5帧的进行补零,组合好的<i>MFCC</i>参数特征共429维。语音分帧长度为25 <i>ms</i>,帧移10 <i>ms</i>,预加重系数取0.97。由于每条语音长度不一致,本文实验中的语音时间长度均低于2 <i>s</i>,因此假设最多可分为200帧,不足的进行补零,组成200×429的参数矩阵作为网络的输入。将参数进行归一化后,输入到卷积神经网络中进行训练,第一个卷积层采用5×5的卷积核采样窗口,步长为1,32个卷积核抽取特征,把输入和权值向量进行卷积,再加上偏置值,然后输入到激活函数为<i>Leaky</i>_<i>relu</i>函数的激活层;池化层采用2×2大小的最大池化,步长为2,提取对应窗口的最大值;第二个卷积层采用5×5的卷积核采样窗口,64个卷积核抽取特征,同样进行最大池化。最后把池化层2的输出扁平化为1维,有序连接成一个向量作为第一个全连接层的输入,同时,为了防止过拟合引入<i>dropout</i>机制,在不同的训练过程中随机丢弃一部分神经元,提升模型的泛化能力<citation id="267" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。全连接层的节点数为1 024个节点,本文初始权值系数和偏置系数均采用截断正态分布随机数<i>truncated</i>_<i>normal</i>,该函数具有截断功能,可以生成相对比较温和的初始值。优化函数使用<i>Adam</i>,使用交叉熵损失函数,初始学习率为0.000 1,语音标签信息采用<i>one</i>-<i>hot</i>编码,训练时一次取10条数据进行训练。</p>
                </div>
                <div class="p1">
                    <p id="151">验证本文算法对提升噪声环境下语音识别稳健性的可行性,采用三种噪声作对比实验,分别为<i>Noisex</i>-92噪声库中的<i>F</i>16噪声、<i>volvo</i>噪声和<i>babble</i>噪声,信噪比分别为-5 <i>dB</i>、0 <i>dB</i>、5 <i>dB</i>、10 <i>dB</i>,并且进行同种条件下的对比实验,分别采用广义旁瓣抵消(<i>Generalized Sidelobe Canceller</i>,<i>GSC</i>),文献<citation id="268" type="reference">[<a class="sup">18</a>]</citation>中的最小方差无畸变响应波束形成后置改进维纳滤波算法(<i>Minimum Variance Distortionless Response combined with Modify Wiener Filter</i>,<i>MVDR</i>-<i>MWF</i>)<citation id="269" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>和本文算法。图4为<i>F</i>16噪声环境,信噪比为0 <i>dB</i>时,对一段语音采用本文以及对比算法进行实验得到的语音时域波形仿真。</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911027_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 含噪语音经不同算法处理后的语音时域仿真" src="Detail/GetImg?filename=images/JSJY201911027_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 含噪语音经不同算法处理后的语音时域仿真  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911027_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Time domain simulation of</i><i>noisy speech after processing by different algorithms</i></p>

                </div>
                <div class="p1">
                    <p id="153">由图4可以看出,本文算法应用在语音识别系统前,对含噪语音实现了较好的去噪效果,有效抑制了噪声对语音信息的损坏,主要因为本文算法可以充分利用麦克风阵列的方位信息,并且调制域谱减法将幅度谱补偿从频域转移到调制域减少语音畸变的产生,取得了较好的降噪效果。而对比算法的去噪效果并不明显,并且有较多的毛刺存在,经过试听可以听到明显的语音畸变。</p>
                </div>
                <div class="p1">
                    <p id="154">为对比本文采用的递归求逆运算在减少算法计算复杂度方面的优劣,在<i>Matlab</i> 2014实验环境下采用一段不同噪声环境下的信噪比为0 <i>dB</i>的含噪语音进行处理的运行时间对比,对比算法分别为传统<i>MVDR</i>算法以及本文基于递归矩阵求逆与对角加载的改进算法,运行时间情况如表1所示。可以明显看出,本文的改进<i>MVDR</i>波束形成算法相比传统的<i>MVDR</i>算法的运算时间较短,大幅降低了运算复杂度。</p>
                </div>
                <div class="area_img" id="155">
                    <p class="img_tit"><b>表</b>1 <b>不同算法在</b>0 <i>dB</i><b>不同噪声环境下运行时间对比 </b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Running time of different algorithms</i><i>in different noise environments with</i> 0 <i>dB SNR</i></p>
                    <p class="img_note">单位:s</p>
                    <table id="155" border="1"><tr><td><br />算法</td><td><i>F</i>16</td><td><i>volvo</i></td><td><i>babble</i></td></tr><tr><td><br /><i>MVDR</i></td><td>5.736</td><td>4.953</td><td>7.385</td></tr><tr><td><br />本文算法</td><td>1.542</td><td>1.325</td><td>1.679</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="156">为了验证本文算法对噪声环境下语音识别的效果,采用上述三种算法以及不对含噪语音进行任何处理做对比实验,分别得到在<i>F</i>16噪声、<i>volvo</i>噪声、<i>babble</i>噪声环境下经不同算法处理后语音的识别率,如表2～4所示。</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表</b>2 <i>F</i>16<b>噪声环境语音识别准确率 </b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Accuracy of speech recognition</i><i>in F</i>16 <i>noise environment</i></p>
                    <p class="img_note">单位:%</p>
                    <table id="157" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />信噪比/<i>dB</i></td></tr><tr><td><br />-5</td><td>0</td><td>5</td><td>10</td></tr><tr><td><br />未处理</td><td>33.8</td><td>54.3</td><td>53.7</td><td>60.6</td></tr><tr><td><br /><i>GSC</i></td><td>36.4</td><td>47.1</td><td>55.6</td><td>68.3</td></tr><tr><td><br /><i>MVDR</i>-<i>MWF</i></td><td>52.6</td><td>64.5</td><td>72.4</td><td>83.9</td></tr><tr><td><br />本文算法</td><td>65.7</td><td>74.9</td><td>86.6</td><td>92.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="158">
                    <p class="img_tit"><b>表</b>3 <i>volvo</i><b>噪声环境语音识别准确率 </b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Accuracy of speech recognition in</i><i>volvo noise environment</i></p>
                    <p class="img_note">单位:%</p>
                    <table id="158" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />信噪比/<i>dB</i></td></tr><tr><td><br />-5</td><td>0</td><td>5</td><td>10</td></tr><tr><td><br />未处理</td><td>31.0</td><td>40.7</td><td>54.9</td><td>63.5</td></tr><tr><td><br /><i>GSC</i></td><td>40.3</td><td>47.6</td><td>56.8</td><td>65.8</td></tr><tr><td><br /><i>MVDR</i>-<i>MWF</i></td><td>47.2</td><td>65.4</td><td>79.3</td><td>84.6</td></tr><tr><td><br />本文算法</td><td>63.8</td><td>77.5</td><td>84.4</td><td>91.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表</b>4 <i>babble</i><b>噪声环境语音识别准确率 </b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 4 <i>Accuracy of speech recognition in</i><i>babble noise environment</i></p>
                    <p class="img_note">单位:%</p>
                    <table id="159" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />信噪比/<i>dB</i></td></tr><tr><td><br />-5</td><td>0</td><td>5</td><td>10</td></tr><tr><td><br />未处理</td><td>27.4</td><td>34.6</td><td>51.2</td><td>60.3</td></tr><tr><td><br /><i>GSC</i></td><td>39.6</td><td>42.1</td><td>58.5</td><td>63.7</td></tr><tr><td><br /><i>MVDR</i>-<i>MWF</i></td><td>45.2</td><td>61.7</td><td>74.1</td><td>83.2</td></tr><tr><td><br />本文算法</td><td>62.2</td><td>74.6</td><td>87.2</td><td>90.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="160">由表2～4的语音识别对比实验结果可以看出,本文算法对噪声环境下的语音识别率得到了明显的提升,相对于未经处理的含噪语音,经麦克风阵列算法处理后,在<i>CNN</i>模型上,语音识别率得到了明显的改善,在信噪比为10 <i>dB</i>的<i>F</i>16噪声环境下语音识别率达到了92.3%,相对于对比算法和不做处理时的识别率有较大的提升,说明本文在语音识别系统前端使用双微阵列的结构进行消噪处理,来提高语音识别率的方法是切实可行的。另外,卷积神经网络的方法具有独特的优势,主要是<i>CNN</i>担任了特征提取器的角色,可以提取语音深层次的特征,而采用拼帧操作也较好地利用了语音信号的长时相关性。</p>
                </div>
                <h3 id="161" name="161" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="162">本文针对传统波束形成算法难以处理空间噪声问题,提出了一种改进的最小方差无畸变响应波束形成方法,并通过后置调制域谱减法对语音进一步进行处理,获得了较好的降噪效果,另外,本文采用卷积神经网络训练语音模型,实验结果表明,取得了优于对比算法的识别效果,在噪声环境下具有良好的稳健性。</p>
                </div>
                <div class="p1">
                    <p id="163">在下一步的研究工作中,鉴于深度学习在语音识别领域的广泛应用,将在语音模型训练优化方面进行进一步研究,以提升噪声环境下语音识别的稳健性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="204">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302088318002&amp;v=Mjc2MzZGeW5uVTc3S0tGb1NYRnF6R2JDNEhOSEVwNHhFYk9zUERoTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 韩纪庆,张磊,郑铁然.语音信号处理[M].北京:清华大学出版社,2004:1-4.(HAN J Q,ZHANG L,ZHENG T R.Speech Signal Processing[M].Beijing:Tsinghua University Press,2004:1-4.)
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787512412286000&amp;v=MzA0NTN5bm5VNzdLS0ZvU1hGcXpHYmE1SE5YTnJZMU5ZdXNQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZG&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 宋知用.Matlab在语音信号分析与合成中的应用[M].北京:北京航空航天大学出版社,2013:176-199.(SONG Z Y.Application of Matlab in Speech Signal Analysis and Synthesis[M].Beijing:Beihang University Press,2013:176-199.)
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A speech enhancement algorithm by iterating single- and multi-microphone processing and its application to robust ASR">

                                <b>[3]</b> ZHANG X,WANG Z,WANG D.A speech enhancement algorithm by iterating single- and multi-microphone processing and its application to robust ASR[C]// Proceedings of the 2017 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2017:276-280.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM591800599ACBC59DEB68BBDC2939CA88&amp;v=MzA4MDl0cGh4YnEzeEs0PU5pZklZN2F4SDluTXI0cE1iWnA4Zmc4OHhtSm1tRGwxT2cyVzN4QThlcnZuTkxLWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> HIGUCHI T,ITO N,ARAKI S,et al.Online MVDR beamformer based on complex Gaussian mixture model with spatial prior for noise robust ASR[J].IEEE/ACM Transactions on Audio Speech and Language Processing,2017,25(4):780-793.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-channel speech processing architectures for noise robust speech recognition:3rd CHiME challenge results">

                                <b>[5]</b> PFEIFENBERGER L,SCHRANK T,ZÖHRER M,et al.Multi-channel speech processing architectures for noise robust speech recognition:3rd CHiME challenge results [C]// Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway:IEEE,2016:1-7.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 曾庆宁,卜玉婷,刘伟波.一种适用于噪声环境下的语音识别方法:201910581762.8[P].2019- 06- 30.(ZENG Q N,BU Y T,LIU W B.A speech recognition method suitable for noise environments:201910581762.8[P].2019- 06- 30.)
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM4CD82A07749E698B97723A0153219B34&amp;v=MTU1MjRZZk9HUWxmQnJMVTA1dHBoeGJxM3hLND1OaWZJWTdmTGF0bk8zbzlDWSs4R2VYb3d4MlFhN1RoL1N3N2lyUmMyZTdPZE43bWJDT052RlNpV1dyN0pJRnBtYUJ1SA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> TASESKA M,HABETS E A P.Informed spatial filtering for sound extraction using distributed microphone arrays [J].IEEE/ACM Transactions on Audio,Speech,and Language Processing,2014,22(7):1195-1207.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201805025&amp;v=MTgyNTQ5SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bm5XN3ZPSVRmU2RyRzRIOW5NcW8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 曾庆宁,肖强,王瑶,等.一种双微阵列语音增强方法[J].电子与信息学报,2018,40(5):1187-1194.(ZENG Q N,XIAO Q,WANG Y,et al.A dual micro-array speech enhancement method[J].Journal of Electronics &amp; Information Technology,2018,40(5):1187-1194.)
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multidimensional maximum-likelihood processing of a large aperture seismic array">

                                <b>[9]</b> CAPON J,GREENFIELD R J,KOLKER R J.Multidimensional maximum-likelihood processing of a large aperture seismic array[J].Proceedings of the IEEE,1967,55(2):192-211.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HNDX201209010&amp;v=MTk5MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlublc3dk9MU1BQZHJHNEg5UE1wbzlFWkk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 施荣华,孟秋杰,董健,等.一种基于对角载入的鲁棒MVDR波束形成算法[J].湖南大学学报(自然科学版),2012,39(9):57-61.(SHI R H,MENG Q J,DONG J,et al.A robust adaptive beamforming algorithm based on diagonal loading[J].Journal of Hunan University (Natural Sciences),2012,39(9):57-61.)
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving robustness against reverberation for automatic speech recognition">

                                <b>[11]</b> MITRA V,van HOUT J,WANG W,et al.Improving robustness against reverberation for automatic speech recognition[C]// Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Piscataway:IEEE,2015:525-532.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Articulatory information and multiview features for large vocabulary continuous speech recognition">

                                <b>[12]</b> MITRA V,WANG W,BARTELS C,et al.Articulatory information and multiview features for large vocabulary continuous speech recognition[C]// Proceedings of the 2018 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2018:5634-5638.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201504012&amp;v=MzI0NzZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVzd2T1BUWElZTEc0SDlUTXE0OUU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 陈紫强,李欣阳,谢跃雷.结合相位谱补偿的调制域谱减法[J].信号处理,2015,31(4):468-473.(CHEN Z Q,LI X Y,XIE Y L.Modulation domain spectral subtraction combined with phase spectrum compensation[J].Journal of Signal Processing,2015,31(4):468-473.)
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM070F465EFD34CB9B3ED6206B98EFDBC1&amp;v=MTM0MzZZN08vSHFmSXFZb3dFcDhNQ0E5THhtUVFuMHQ3U24vazNoczlETVRnTjhtZUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhicTN4SzQ9TmlmSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> QIAN Y,TAN T,YU D.Neural network based multi-factor aware joint training for robust speech recognition [J].IEEE/ACM Transactions on Audio Speech &amp; Language Processing,2017,24(12):2231-2240.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJKD201509015&amp;v=MTM3OTN5bm5XN3ZPSnlmQWFyRzRIOVRNcG85RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 张晴晴,刘勇,潘接林,等.基于卷积神经网络的连续语音识别[J].工程科学学报,2015,37(9):1212-1217.(ZHANG Q Q,LIU Y,PAN J L,et al.Continuous speech recognition based on convolutional neural networks[J].Chinese Journal of Engineering,2015,37(9):1212-1217.)
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MDU5MDhEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bm5VNzdLS0ZvU1hGcXpHYkM0SE5YT3JJMU5ZK3NQ&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 周志华.机器学习[M].北京:清华大学出版社,2016:97-140.(ZHOU Z H.Machine Learning[M].Beijing:Tsinghua University Press,2016:97-140.)
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep convolutional neural networks for acoustic modeling in low resource languages">

                                <b>[17]</b> CHAN W,LANE I.Deep convolutional neural networks for acoustic modeling in low resource languages[C]// Proceedings of the 2015 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2015:2056-2060.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201608034&amp;v=MDc4ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5uVzd2T05pZllaTEc0SDlmTXA0OUdZSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 马金龙,曾庆宁,龙超,等.多噪声环境下可懂度提升的助听器语音增强[J].计算机工程与设计,2016,37(8):2160-2164.(MA J L,ZENG Q N,LONG C,et al.Intelligibility improved speech enhancement for hearing aids in complex noise environment [J].Computer Engineering and Design,2016,37(8):2160-2164.)
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911027" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911027&amp;v=MzAxNzVuVzd2T0x6N0JkN0c0SDlqTnJvOUhZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
