<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136753660596250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906042%26RESULT%3d1%26SIGN%3ddYJH9o9M6FR4iZmNhCLhjz2TfT8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906042&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906042&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906042&amp;v=MjI1OTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGtXN3pJTHo3QmQ3RzRIOWpNcVk5QlpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#67" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="1 基于变分Retinex的增强方法 ">1 基于变分Retinex的增强方法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#119" data-title="2 光照补偿和细节调整 ">2 光照补偿和细节调整</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#120" data-title="2.1 &lt;b&gt;结合&lt;/b&gt;Sigmoid&lt;b&gt;函数和伽马矫正的光照补偿方法&lt;/b&gt;">2.1 <b>结合</b>Sigmoid<b>函数和伽马矫正的光照补偿方法</b></a></li>
                                                <li><a href="#128" data-title="2.2 &lt;b&gt;改进反锐化掩模的细节调整算法&lt;/b&gt;">2.2 <b>改进反锐化掩模的细节调整算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#140" data-title="3 改进的模拟合成方法 ">3 改进的模拟合成方法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#159" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#173" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="图1 曝光调整">图1 曝光调整</a></li>
                                                <li><a href="#72" data-title="图2 所提方法基本框架">图2 所提方法基本框架</a></li>
                                                <li><a href="#127" data-title="图3 光照补偿对比">图3 光照补偿对比</a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同算法的&lt;/b&gt;LOE&lt;b&gt;测试结果&lt;/b&gt;"><b>表</b>1 <b>不同算法的</b>LOE<b>测试结果</b></a></li>
                                                <li><a href="#171" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同算法的&lt;/b&gt;VIF&lt;b&gt;测试结果&lt;/b&gt;"><b>表</b>2 <b>不同算法的</b>VIF<b>测试结果</b></a></li>
                                                <li><a href="#172" data-title="图4 不同算法各个场景图像增强结果对比">图4 不同算法各个场景图像增强结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="213">


                                    <a id="bibliography_1" title="YUE H J, YANG J Y, SUN X Y, et al.Contrast enhancement based on intrinsic image decomposition[J].IEEE Transactions on Image Processing, 2017, 26 (8) :3981-3994." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contrast enhancement based on intrinsic image decomposition">
                                        <b>[1]</b>
                                        YUE H J, YANG J Y, SUN X Y, et al.Contrast enhancement based on intrinsic image decomposition[J].IEEE Transactions on Image Processing, 2017, 26 (8) :3981-3994.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_2" title="LEE C, LEE C, KIM C S.Contrast enhancement based on layered difference representation of 2D histograms[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5372-5384." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contrast enhancement based on layered difference representation">
                                        <b>[2]</b>
                                        LEE C, LEE C, KIM C S.Contrast enhancement based on layered difference representation of 2D histograms[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5372-5384.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_3" title="CELIK T, TJAHJADI T.Contextual and variational contrast enhancement[J].IEEE Transactions on Image Processing, 2011, 20 (12) :3431-3441." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contextual and Variational Contrast Enhancement">
                                        <b>[3]</b>
                                        CELIK T, TJAHJADI T.Contextual and variational contrast enhancement[J].IEEE Transactions on Image Processing, 2011, 20 (12) :3431-3441.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_4" title="GU K, ZHAI G T, LIN W S, et al.The analysis of image contrast:from quality assessment to automatic enhancement[J].IEEETransactions on Cybernetics, 2016, 46 (1) :284-297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The analysis of image contrast:from quality assessment to automatic enhancement">
                                        <b>[4]</b>
                                        GU K, ZHAI G T, LIN W S, et al.The analysis of image contrast:from quality assessment to automatic enhancement[J].IEEETransactions on Cybernetics, 2016, 46 (1) :284-297.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_5" title="JOBSON D J, RAHMAN Z, WOODELL G A.Properties and performance of a center/surround Retinex[J].IEEE Transactions on Image Processing, 1997, 6 (3) :451-462." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Properties and performance of a center/surround retinex">
                                        <b>[5]</b>
                                        JOBSON D J, RAHMAN Z, WOODELL G A.Properties and performance of a center/surround Retinex[J].IEEE Transactions on Image Processing, 1997, 6 (3) :451-462.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_6" title="刘茜, 卢心红, 李象霖.基于多尺度Retinex的自适应图像增强方法[J].计算机应用, 2009, 29 (8) :2077-2079. (LIU Q, LU XH, LI X L.Adaptive image enhancement method based on multiscale Retinex algorithm[J].Journal of Computer Applications, 2009, 29 (8) :2077-2079.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200908016&amp;v=MTI4ODl1WnNGeURrVzd6SUx6N0JkN0c0SHRqTXA0OUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        刘茜, 卢心红, 李象霖.基于多尺度Retinex的自适应图像增强方法[J].计算机应用, 2009, 29 (8) :2077-2079. (LIU Q, LU XH, LI X L.Adaptive image enhancement method based on multiscale Retinex algorithm[J].Journal of Computer Applications, 2009, 29 (8) :2077-2079.) 
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_7" title="WANG S H, ZHENG J, HU H M, et al.Naturalness preserved enhancement algorithm for non-uniform illumination images[J].IEEETransactions on Image Processing, 2013, 22 (9) :3538-3548." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Naturalness preserved enhancement algorithm for non-uniform illumination images">
                                        <b>[7]</b>
                                        WANG S H, ZHENG J, HU H M, et al.Naturalness preserved enhancement algorithm for non-uniform illumination images[J].IEEETransactions on Image Processing, 2013, 22 (9) :3538-3548.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_8" title="王小明, 黄昶, 李全彬, 等.改进的多尺度Retinex图像增强算法[J].计算机应用, 2010, 30 (8) :2091-2093. (WANG X M, HUANG C, LI Q B, et al.Improved multi-scale Retinex image enhancement algorithm[J].Journal of Computer Applications, 2010, 30 (8) :2091-2093.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201008029&amp;v=MTgwODlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGtXN3pJTHo3QmQ3RzRIOUhNcDQ5SGJZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        王小明, 黄昶, 李全彬, 等.改进的多尺度Retinex图像增强算法[J].计算机应用, 2010, 30 (8) :2091-2093. (WANG X M, HUANG C, LI Q B, et al.Improved multi-scale Retinex image enhancement algorithm[J].Journal of Computer Applications, 2010, 30 (8) :2091-2093.) 
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_9" title="FU X Y, LIAO Y H, ZENG D L, et al.A probabilistic method for image enhancement with simultaneous illumination and reflectance estimation[J].IEEE Transactions on Image Processing, 2015, 24 (12) :4965-4977." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Probabilistic Method for Image Enhancement With Simultaneous Illumination and Reflectance Estimation.">
                                        <b>[9]</b>
                                        FU X Y, LIAO Y H, ZENG D L, et al.A probabilistic method for image enhancement with simultaneous illumination and reflectance estimation[J].IEEE Transactions on Image Processing, 2015, 24 (12) :4965-4977.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_10" title="WANG Y F, WANG H Y, YIN C L, et al.Biologically inspired image enhancement based on Retinex[J].Neurocomputing, 2016, 177 (C) :373-384." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBFC456CACB282B35E553411B2230C2BC&amp;v=MDMwNTg1THpCTm03enArVEg3ajNoQTNlckxuUjhqc0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMbTN3Nmc9TmlmT2ZjSE9iZFhKcWZ3MEY1a05CSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        WANG Y F, WANG H Y, YIN C L, et al.Biologically inspired image enhancement based on Retinex[J].Neurocomputing, 2016, 177 (C) :373-384.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_11" title="GUO X J, LI Y, LING H B.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LIME:Low-light image enhancement via illumination map estimation">
                                        <b>[11]</b>
                                        GUO X J, LI Y, LING H B.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_12" title="FU X Y, ZENG D L, HUANG Y, et al.A weighted variational model for simultaneous reflectance and illumination estimation[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2782-2790." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A weighted variational model for simultaneous reflectanceand illumination estimation">
                                        <b>[12]</b>
                                        FU X Y, ZENG D L, HUANG Y, et al.A weighted variational model for simultaneous reflectance and illumination estimation[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2782-2790.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_13" title="KIMMEL R, ELAD M, SHAKED D, et al.A variational framework for Retinex[J].International Journal of Computer Vision, 2003, 52 (1) :7-23." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830803&amp;v=MTQyMzJ6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFc3N0FJMTQ9Tmo3QmFyTzRIdEhPcDR4RmJPc01ZM2s1&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        KIMMEL R, ELAD M, SHAKED D, et al.A variational framework for Retinex[J].International Journal of Computer Vision, 2003, 52 (1) :7-23.
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_14" title="GOLDSTEIN T, OSHER S.The split bregman method for L1-regularized problems[J].SIAM Journal on Imaging Sciences, 2009, 2 (2) :323-343." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Split Bregman Method for L1-Regularized Problems">
                                        <b>[14]</b>
                                        GOLDSTEIN T, OSHER S.The split bregman method for L1-regularized problems[J].SIAM Journal on Imaging Sciences, 2009, 2 (2) :323-343.
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_15" title="DENG G.A generalized unsharp masking algorithm[J].IEEETransactions on Image Processing, 2011, 20 (5) :1249-1261." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A generalized unsharp masking algorithm">
                                        <b>[15]</b>
                                        DENG G.A generalized unsharp masking algorithm[J].IEEETransactions on Image Processing, 2011, 20 (5) :1249-1261.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_16" title="ANCUTI C O, ANCUTI C.Single image dehazing by multi-scale fusion[J].IEEE Transactions on Image Processing, 2013, 22 (8) :3271-3282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single Image Dehazing by Multi-Scale Fusion">
                                        <b>[16]</b>
                                        ANCUTI C O, ANCUTI C.Single image dehazing by multi-scale fusion[J].IEEE Transactions on Image Processing, 2013, 22 (8) :3271-3282.
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_17" title="ANCUTI C O, ANCUTI C, de VLEESCHOUWER C, et al.Single-scale fusion:an effective approach to merging images[J].IEEE Transactions on Image Processing, 2017, 26 (1) :65-78." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-scale fusion:an effective approach to merging images">
                                        <b>[17]</b>
                                        ANCUTI C O, ANCUTI C, de VLEESCHOUWER C, et al.Single-scale fusion:an effective approach to merging images[J].IEEE Transactions on Image Processing, 2017, 26 (1) :65-78.
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_18" title="ANCUTI C, ANCUTI C O, HABER T, et al.Enhancing underwater images and videos by fusion[C]//Proceedings of the 2012IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:81-88." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhancing Underwater Images and Videos by Fusion">
                                        <b>[18]</b>
                                        ANCUTI C, ANCUTI C O, HABER T, et al.Enhancing underwater images and videos by fusion[C]//Proceedings of the 2012IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:81-88.
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_19" title="ANCUTI C O, ANCUTI C, BEKAERT P.Enhancing by saliencyguided decolorization[C]//CVPR 2011:Proceedings of the 2011IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2011:257-264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhancing by saliency-guided decolorization">
                                        <b>[19]</b>
                                        ANCUTI C O, ANCUTI C, BEKAERT P.Enhancing by saliencyguided decolorization[C]//CVPR 2011:Proceedings of the 2011IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2011:257-264.
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_20" title="CHOI L K, YOU J, BOVIK A C.Referenceless prediction of perceptual fog density and perceptual image defogging[J].IEEETransactions on Image Processing, 2015, 24 (11) :3888-3901." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Referenceless prediction of perceptual fog density and perceptual image defogging">
                                        <b>[20]</b>
                                        CHOI L K, YOU J, BOVIK A C.Referenceless prediction of perceptual fog density and perceptual image defogging[J].IEEETransactions on Image Processing, 2015, 24 (11) :3888-3901.
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_21" title="JOBSON D J, RAHMAN Z, WOODELL G A.A multiscale Retinex for bridging the gap between color images and the human observation of scenes[J].IEEE Transactions on Image Processing, 1997, 6 (7) :965-976." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A multiscale retinex for bridging the gap between color images and the human observation of scenes">
                                        <b>[21]</b>
                                        JOBSON D J, RAHMAN Z, WOODELL G A.A multiscale Retinex for bridging the gap between color images and the human observation of scenes[J].IEEE Transactions on Image Processing, 1997, 6 (7) :965-976.
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_22" title="XU H T, ZHAI G Z, WU X L, et al.Generalized equalization model for image enhancement[J].IEEE Transactions on Multimedia, 2014, 16 (1) :68-82." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized Equalization Model for Image Enhancement">
                                        <b>[22]</b>
                                        XU H T, ZHAI G Z, WU X L, et al.Generalized equalization model for image enhancement[J].IEEE Transactions on Multimedia, 2014, 16 (1) :68-82.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_23" title="SHAN Q, JIA J Y, BROWN M S.Globally optimized linear windowed tone mapping[J].IEEE transactions on Visualization and Computer Graphics, 2010, 16 (4) :663-675." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Globally optimized linear windowed tone mapping">
                                        <b>[23]</b>
                                        SHAN Q, JIA J Y, BROWN M S.Globally optimized linear windowed tone mapping[J].IEEE transactions on Visualization and Computer Graphics, 2010, 16 (4) :663-675.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_24" title="YING Z Q, LI G, REN Y R, et al.A new image contrast enhancement algorithm using exposure fusion framework[C]//CAIP 2017:Proceedings of the 2017 International Conference on Computer Analysis of Images and Patterns, LNCS 10425.Cham:Springer, 2017:36-46." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new image contrast enhancement algorithm using exposure fusion framework">
                                        <b>[24]</b>
                                        YING Z Q, LI G, REN Y R, et al.A new image contrast enhancement algorithm using exposure fusion framework[C]//CAIP 2017:Proceedings of the 2017 International Conference on Computer Analysis of Images and Patterns, LNCS 10425.Cham:Springer, 2017:36-46.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_25" title="LI M D, LIU J Y, YANG W H, et al.Structure-revealing lowlight image enhancement via robust Retinex model[J].IEEETransactions on Image Processing, 2018, 27 (6) :2828-2841." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structure-Revealing Low-Light Image Enhancement Via Robust Retinex Model.">
                                        <b>[25]</b>
                                        LI M D, LIU J Y, YANG W H, et al.Structure-revealing lowlight image enhancement via robust Retinex model[J].IEEETransactions on Image Processing, 2018, 27 (6) :2828-2841.
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_26" title="MA K, ZENG K, WANG Z.Perceptual quality assessment for multi-exposure image fusion[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3345-3356." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual Quality Assessment for Multi-Exposure Image Fusion">
                                        <b>[26]</b>
                                        MA K, ZENG K, WANG Z.Perceptual quality assessment for multi-exposure image fusion[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3345-3356.
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_27" title="SHEIKH H R, BOVIK A C.Image information and visual quality[J].IEEE Transactions on Image Processing, 2006, 15 (2) :430-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image information and visual quality">
                                        <b>[27]</b>
                                        SHEIKH H R, BOVIK A C.Image information and visual quality[J].IEEE Transactions on Image Processing, 2006, 15 (2) :430-444.
                                    </a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_28" title="LI Z G, ZHENG J H, ZHU Z J, et al.Weighted guided image filtering[J].IEEE Transactions on Image Processing, 2015, 24 (1) :120-129." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weighted Guided Image Filtering">
                                        <b>[28]</b>
                                        LI Z G, ZHENG J H, ZHU Z J, et al.Weighted guided image filtering[J].IEEE Transactions on Image Processing, 2015, 24 (1) :120-129.
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_29" title="AYDIN T O, MANTIUK R, MYSZKOWSKI K, et al.Dynamic range independent image quality assessment[C]//SIGGRAPH2008:Proceedings of the 2008 ACM SIGGRAPH.New York:ACM, 2008:Article No.69." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic range independent image quality assessment">
                                        <b>[29]</b>
                                        AYDIN T O, MANTIUK R, MYSZKOWSKI K, et al.Dynamic range independent image quality assessment[C]//SIGGRAPH2008:Proceedings of the 2008 ACM SIGGRAPH.New York:ACM, 2008:Article No.69.
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_30" title="WANG Q H, FU X Y, ZHANG X P, et al.A fusion-based method for single backlit image enhancement[C]//ICIP 2016:Proceedings of the 2016 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2016:4077-4081." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fusion-based method for single backlit image enhancement">
                                        <b>[30]</b>
                                        WANG Q H, FU X Y, ZHANG X P, et al.A fusion-based method for single backlit image enhancement[C]//ICIP 2016:Proceedings of the 2016 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2016:4077-4081.
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_31" title="YING Z Q, LI G, REN Y R, et al.A new low-light image enhancement algorithm using camera response model[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision Workshop.Piscataway, NJ:IEEE, 2017:3015-3022." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new low-light image enhancement algorithm using camera response model">
                                        <b>[31]</b>
                                        YING Z Q, LI G, REN Y R, et al.A new low-light image enhancement algorithm using camera response model[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision Workshop.Piscataway, NJ:IEEE, 2017:3015-3022.
                                    </a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_32" title="王晨, 汤心溢, 高思莉.基于人眼视觉的红外图像增强算法研究[J].激光与红外, 2017, 47 (1) :114-118. (WANG C, TANGX Y, GAO S L.Infrared image enhancement algorithm based on human vision[J].Laser and Infrared, 2017, 47 (1) :114-118.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGHW201701023&amp;v=MzE2Mzc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGtXN3pJTHlyRGViRzRIOWJNcm85SFo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[32]</b>
                                        王晨, 汤心溢, 高思莉.基于人眼视觉的红外图像增强算法研究[J].激光与红外, 2017, 47 (1) :114-118. (WANG C, TANGX Y, GAO S L.Infrared image enhancement algorithm based on human vision[J].Laser and Infrared, 2017, 47 (1) :114-118.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-31 09:50</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1804-1809 DOI:10.11772/j.issn.1001-9081.2018112284            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于模拟多曝光融合的低照度图像增强方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B8%E9%A9%AC%E7%B4%AB%E8%8F%B1&amp;code=41987915&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">司马紫菱</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E5%B3%B0&amp;code=11234746&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡峰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学计算机科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E9%87%8D%E5%BA%86%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6)&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算智能重庆市重点实验室(重庆邮电大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对部分低照度图像整体亮度偏暗、对比度差和视觉信息偏弱等问题, 提出一种基于模拟多曝光融合的低照度图像增强方法。首先, 利用改进的变分Retinex模型和形态学的结合产生基准图来保证曝光图像集中的主体信息;其次, 结合Sigmoid函数和伽马矫正构造新的光照补偿归一化函数, 同时提出了一种基于高斯引导滤波的反锐化掩模算法, 用于调整基准图的细节;最后, 分别从亮度、色调和曝光率设计曝光图集的加权值, 通过多尺度融合得到最终增强结果, 有效地避免了增强结果中的光晕和颜色失真。在不同的公开数据集上的实验结果表明, 与传统的低照度图像增强方法进行相比, 所提方法降低了亮度失真率, 提升了视觉信息保真度。该方法能够有效地保留视觉信息, 有利于实现低照度图像增强的实时性应用。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%8E%E7%85%A7%E5%BA%A6%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">低照度图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Retinex%E7%90%86%E8%AE%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Retinex理论;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9B%9D%E5%85%89%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曝光融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%86%E8%8A%82%E8%B0%83%E6%95%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">细节调整;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像增强;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *司马紫菱 (1996—) , 女, 湖北天门人, 硕士研究生, 主要研究方向:智能信息处理;simazl@ qq. com;
                                </span>
                                <span>
                                    胡峰 (1978—) , 男, 湖北天门人, 教授, 博士, 主要研究方向:数据挖掘、Rough集、粒计算、智能信息处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2018YFC0808305);</span>
                                <span>国家自然科学基金资助项目 (61751312, 61533020, 61309014);</span>
                                <span>重庆市重点产业共性关键技术创新专项 (cstc2017zdcy-zdyfX0001, cstc2017zdcy-zdzx0046);</span>
                                <span>重庆市基础科学与前沿技术研究专项 (cstc2017jcyjAX0408);</span>
                    </p>
            </div>
                    <h1><b>Low-light image enhancement method based on simulating multi-exposure fusion</b></h1>
                    <h2>
                    <span>SIMA Ziling</span>
                    <span>HU Feng</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Technology, Chongqing University of Posts and Telecommunications</span>
                    <span>Chongqing Key Laboratory of Computational Intelligence (Chongqing University of Posts and Telecommunications)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problems of low luminance, low contrast and poor visual information, a low-light image enhancement method based on simulating multi-exposure fusion was proposed. Firstly, the improved variational Retinex model and morphology were combined to generate the reference map to ensure the subject information in the exposed image set. Then, a new illumination compensation normalization function was constructed by combining Sigmoid function and gamma correction. At the same time, an unsharp masking algorithm based on Gaussian guided filtering was proposed to adjust the details of the reference map. Finally, the weighted values of exposed image set were designed from luminance, chromatic information and exposure rate respectively, and the final enhancement result was obtained through multi-scale fusion with effective avoidance of halo phenomenon and color distortion. The experimental results on different public datasets show that, compared with the traditional low-light image enhancement method, the proposed method has reduced the lightness distortion rate and increased the visual information fidelity. The proposed method can effectively preserve visual information, which is conducive to the real-time application of low-light image enhancement.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=low-light%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">low-light image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Retinex%20theory&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Retinex theory;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=exposure%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">exposure fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=detail%20adjustment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">detail adjustment;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20enhancement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image enhancement;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SIMA Ziling, born in 1996, M. S. candidate. Her research interests include intelligent information processing. ;
                                </span>
                                <span>
                                    HU Feng, born in 1978, Ph. D. , professor. His research interests include data mining, Rough set, granular calculation, intelligent information processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-19</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Key Research and Development Program of China (2018YFC0808305);</span>
                                <span>the National Natural Science Foundation of China (61751312, 61533020, 61309014);</span>
                                <span>the Chongqing Special Fund for Common Key Technology Innovations in Key Industries (cstc2017zdcy-zdyfX 0001, cstc2017zdcy-zdzx0046);</span>
                                <span>the Chongqing Research Program of Basic Research and Frontier Technology (cstc2017jcyjA X0408);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="67" name="67" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="68">随着科技的进步与发展, 图像采集的方式越来越丰富, 人们对图像的质量要求也越来越高。然而图像在获取的过程会受到很多因素的影响, 特殊光照环境下, 光学成像设备因为光照不均匀, 从而可能使获得的图像曝光不均匀、场景细节损失、弱小目标识别不清。由于拍摄设备的动态范围是有限度的, 如果仅仅调整设备曝光率, 还是不能解决某些区域出现过度曝光或者过度饱和等问题。从图1可以看出, 随着曝光率的增加, 原来曝光不足的区域趋向于正常显示, 而原来正常的区域趋向于过度曝光, 导致无法正常显示区域信息。针对这个问题, 学者们进行了大量研究。目前主流的方法主要分为两类:基于直方图增强和基于Retinex图像增强<citation id="277" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。基于直方图的方法通过修改直方图的分布来增强图像, 该方法因为简单有效而被广泛应用于各个领域, 然而该方法对噪声敏感, 在其改变图像亮度的同时可能出现过度增强的现象。近年来围绕这一问题, 学者们提出了一系列优化算法。Lee等<citation id="278" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>通过寻找二维直方图分层之间的差异性提出了一种新的对比度增强算法;Celik等<citation id="279" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>尝试寻找一个最大灰度差来重新映射直方图;Gu等<citation id="280" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>将质量评估模型应用于直方图参数的优化中, 有效地处理了过度增强的问题。但这些方法关注的是对比度增强, 并没有充分利用图像的真实亮度, 存在过度增强或者未被增强的风险。</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906042_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 曝光调整" src="Detail/GetImg?filename=images/JSJY201906042_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 曝光调整  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906042_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Exposure adjustment</p>

                </div>
                <div class="p1">
                    <p id="70">基于Retinex的方法将图像看作是由光照分量和反射分量构成。传统的Retinex方法通过对光照分量的估计和移除, 将反射分量看作是最后的增强结果<citation id="284" type="reference"><link href="221" rel="bibliography" /><link href="223" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>, 但是往往会出现增强结果不自然和过度增强等问题。Wang等<citation id="281" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>设计了亮度滤波器对亮度进行估计, 然后使用双对数变换修整亮度, 但是不能很好地处理细节。王小明等<citation id="282" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了利用快速二维卷积和多尺度连续估计的算法, 降低了多尺度Retinex算法的运算复杂度, 但该方法可能会因为光照分量的结构性导致部分增强区域失去自然性。Fu等<citation id="283" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>通过最大后验概率来估计光照分量和反射分量, 将两者进行伽马校正后重新调整图像, 得到最后的增强结果, 虽然增强效果较为理想, 但仍会在纹理丰富的区域丢失细节信息。</p>
                </div>
                <div class="p1">
                    <p id="71">上述方法可以获得较好的主观质量, 但这些结果并不能准确地反映场景的真实信息。因此, 基于单幅低照度图像的增强仍然是一个具有挑战性的问题。为了改善上述情况, 基于Retinex的图像增强方法, 本文提出了一种基于模拟多曝光融合的低照度图像增强方法, 基本框架如图2所示。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906042_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 所提方法基本框架" src="Detail/GetImg?filename=images/JSJY201906042_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 所提方法基本框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906042_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Framework of the proposed method</p>

                </div>
                <div class="p1">
                    <p id="73">由图2可以看出, 该方法由两部分组成:模拟曝光和模拟合成。第一阶段模拟人眼聚焦调整, 产生多幅曝光图像集;第二阶段模拟人脑, 将产生的图像集进行融合来获得最后的增强图像。算法具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="74">1) 首先将原来的低照度图像由RGB (Red, Green, Blue) 颜色模式转化为HSV (Hue, Saturation, Value) 模式, 然后将HSV图像的V通道进行变分Retinex和形态学操作, 得到变分增强后的基准图<i>E</i><sub>1</sub>。</p>
                </div>
                <div class="p1">
                    <p id="75">2) 将变分增强后的基准图<i>E</i><sub>1</sub>通过Sigmoid函数和伽马矫正构造新的归一化函数来实现图像的光照补偿, 并得到基准图<i>E</i><sub>2</sub>。</p>
                </div>
                <div class="p1">
                    <p id="76">3) 再通过基于高斯引导滤波的反锐化掩模算法对基准图<i>E</i><sub>1</sub>进行细节调整, 得到调整后的基准图<i>E</i><sub>3</sub>。</p>
                </div>
                <div class="p1">
                    <p id="77">4) 将三幅基准图<i>E</i><sub>1</sub>、<i>E</i><sub>2</sub>、<i>E</i><sub>3</sub>基于图像的亮度、曝光率和色调设计三幅曝光图的加权值, 为曝光良好的像素分配较大的权值, 曝光不足的像素分配较小的权值, 得到图像集<i>W</i><sub>1</sub>、<i>W</i><sub>2</sub>、<i>W</i><sub>3</sub>。</p>
                </div>
                <div class="p1">
                    <p id="78">5) 最后将加权后的图像集和基准图像集通过多尺度融合的方式结合, 实现最后的增强, 输出增强后的图像。</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag">1 基于变分Retinex的增强方法</h3>
                <div class="p1">
                    <p id="80">在模拟曝光之前, 需要确定曝光图像的数量。为了同时兼顾图像的亮度和细节信息, 本文选择三幅曝光图像, 其中一幅为基准图像, 另外两幅曝光图是在基准图<i>E</i><sub>1</sub>的基础上, 分别基于细节和亮度调整而产生。所以基准图<i>E</i><sub>1</sub>是整个曝光图集的核心, <i>E</i><sub>1</sub>的估计不足将会严重影响着其余两幅图的内容调整。Retinex理论是基于人眼视觉系统所提出的图像增强理论, 因此基于Retinex理论产生基准图, 其数学模拟如下:</p>
                </div>
                <div class="p1">
                    <p id="81"><i>g</i> (<i>x</i>, <i>y</i>) =<i>f</i><sub>R</sub> (<i>x</i>, <i>y</i>) *<i>f</i><sub>L</sub> (<i>x</i>, <i>y</i>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<i>g</i> (<i>x</i>, <i>y</i>) 为观测图像; <i>f</i><sub>R</sub> (<i>x</i>, <i>y</i>) 为反射分量; <i>f</i><sub>L</sub> (<i>x</i>, <i>y</i>) 为光照分量。然而通过一个已知量求解两个未知量, 这在数学上称为病态问题。部分研究者假设光照分量已知并通过低通滤波器去估计, 将最后的反射分量视为最终的增强结果<citation id="286" type="reference"><link href="221" rel="bibliography" /><link href="223" rel="bibliography" /><link href="231" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">10</a>]</sup></citation>。但在实际中, 光照分量是很难估计的, 而且将反射分量看作为最后的增强结果可能会造成部分增强效果的不自然, 不能真实地反映目标场景信息<citation id="285" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="83">为了更加真实地反映目标场景的反射分量和光照分量, 本文采用一种新的变分Retinex模型<citation id="287" type="reference"><link href="229" rel="bibliography" /><link href="235" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">12</a>]</sup></citation>来估计图像的反射分量和光照分量, 模型如下:</p>
                </div>
                <div class="p1">
                    <p id="84">arg min‖<i>f</i><sub>R</sub><i>f</i><sub>L</sub>-<i>g</i>‖<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>+<i>λ</i><sub>1</sub>‖ᐁ<i>f</i><sub>R</sub>‖<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>+<i>λ</i><sub>2</sub>‖ᐁ<i>f</i><sub>L</sub>‖<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>+<i>λ</i><sub>3</sub>‖<i>f</i><sub>L</sub>-<i>f</i><sub><i>L</i><sub>0</sub></sub>‖<mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="89">其中:模型第一项‖<i>f</i><sub>R</sub><i>f</i><sub>L</sub>-<i>g</i>‖<mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>为数据保真项, 用以约束估计的反射分量<i>f</i><sub>R</sub>和光照分量<i>f</i><sub>L</sub>与输入图像<i>g</i>保持一致性, ᐁ为水平方向和垂直方向的差分运算;第二项正则化项‖ᐁ<i>f</i><sub>R</sub>‖<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>是对反射分量求解的约束, 并用参数<i>λ</i><sub>1</sub>调节对整个模型的影响程度;第三项正则化项‖ᐁ<i>f</i><sub>L</sub>‖<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>是对光照分量的求解的约束, 通过参数<i>λ</i><sub>2</sub>进行权重调整;最后一项正则化项‖<i>f</i><sub>L</sub>-<i>f</i><sub><i>L</i><sub>0</sub></sub>‖<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>是经验项, 使光照<i>f</i><sub>L</sub>服从以<i>L</i><sub>0</sub>范数为均值的高斯分布, 防止在计算过程中的尺度发生变化。为了避免求解的光照分量过小, <i>λ</i><sub>3</sub>用来调节其对模型的影响程度;根据文献<citation id="288" type="reference">[<a class="sup">13</a>]</citation>约定: <i>f</i><sub>R</sub>的取值为0到1, 并且<i>g</i>≥<i>f</i><sub>L</sub>。</p>
                </div>
                <div class="p1">
                    <p id="94">可以看出式 (2) 在计算过程中受两个未知变量制约, 且只有一个已知量, 无法用传统梯度下降的方法求解, 本文采用交替迭代的优化策略求解<i>f</i><sub>R</sub>和<i>f</i><sub>L</sub><citation id="289" type="reference"><link href="229" rel="bibliography" /><link href="235" rel="bibliography" /><link href="239" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">12</a>,<a class="sup">14</a>]</sup></citation>。式 (2) 重写为:</p>
                </div>
                <div class="p1">
                    <p id="95">arg min‖<i>f</i><sub>R</sub>-<i>g</i>/<i>f</i><sub>L</sub>‖<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>+<i>λ</i><sub>1</sub>‖ᐁ<i>f</i><sub>R</sub>‖<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="98">可以看出式 (3) 的求解是最小二乘化问题, 通过对反射分量的求导, 可以得到<i>f</i><sub>R</sub>的闭合解:</p>
                </div>
                <div class="p1">
                    <p id="99"><i>f</i><sub>R</sub>=<i>F</i><sup>-1</sup> (<i>F</i> (<i>g</i>/<i>f</i><sub>L</sub>) / (<i>F</i> (1) +<i>λ</i><sub>1</sub> (<i>F</i><sup>*</sup> (ᐁ<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>h</mi><mi>Τ</mi></msubsup></mrow></math></mathml>) <i>F</i> (ᐁ<sub><i>h</i></sub>) +</p>
                </div>
                <div class="p1">
                    <p id="101"><i>F</i><sup>*</sup> (ᐁ<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>v</mi><mi>Τ</mi></msubsup></mrow></math></mathml>) <i>F</i> (ᐁ<sub><i>v</i></sub>) ) ) )      (4) </p>
                </div>
                <div class="p1">
                    <p id="103">其中:<i>F</i>为快速傅里叶变换, <i>F</i><sup>-1</sup>为傅里叶逆变换;<i>F</i><sup>*</sup>是傅里叶共轭转置;<i>h</i>和<i>v</i>分别表示水平和垂直方向的差分运算。</p>
                </div>
                <div class="p1">
                    <p id="104">光照分量的求解原理和反射分量的求解原理相似, 固定反射分量, 消除式 (2) 中与光照分量计算无关的正则化项, 式 (2) 可以重写为:</p>
                </div>
                <div class="p1">
                    <p id="105">arg min‖<i>f</i><sub>L</sub>-<i>g</i>/<i>f</i><sub>R</sub>‖<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>+<i>λ</i><sub>1</sub>‖ᐁ<i>f</i><sub>L</sub>‖<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>+<i>λ</i><sub>3</sub>‖<i>f</i><sub>L</sub>-<i>f</i><sub><i>L</i><sub>0</sub></sub>‖<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="109">对光照分量求解, 可以得到<i>f</i><sub>L</sub>的闭合解:</p>
                </div>
                <div class="p1">
                    <p id="110"><i>f</i><sub>L</sub>=<i>F</i><sup>-1</sup> (<i>F</i> (<i>λ</i><sub>3</sub><i>f</i><sub><i>L</i>0</sub>+<i>g</i>/<i>f</i><sub>R</sub>) / (<i>F</i> (1+<i>λ</i><sub>3</sub>) +</p>
                </div>
                <div class="p1">
                    <p id="111"><i>λ</i><sub>2</sub> (<i>F</i> (ᐁ<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>h</mi><mi>Τ</mi></msubsup></mrow></math></mathml>) <i>F</i> (ᐁ<sub><i>h</i></sub>) +<i>F</i> (ᐁ<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>v</mi><mi>Τ</mi></msubsup></mrow></math></mathml>) <i>F</i> (ᐁ<sub><i>v</i></sub>) ) ) )      (6) </p>
                </div>
                <div class="p1">
                    <p id="114">根据文献<citation id="290" type="reference">[<a class="sup">13</a>]</citation>可知<i>g</i> ≥ <i>f</i><sub>L</sub>, 所以每次交替求解的时候需要对<i>f</i><sub>L</sub>作一次校正, 令<i>f</i><sub>L</sub>=max (<i>g</i>, <i>f</i><sub>L</sub>) , 将<i>f</i><sub>L</sub>作为下一次更新<i>f</i><sub><i>L</i><sub>0</sub></sub>的初始值<citation id="291" type="reference"><link href="229" rel="bibliography" /><link href="235" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="115">由于光照分量饱含图像的亮度信息, 对光照分量非线性地拉伸可以消除其对图像暗区域的影响, 非线性拉伸定义如下:</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>f</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>≤</mo><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>l</mtext><mtext>o</mtext><mtext>w</mtext></mrow></msubsup></mtd></mtr><mtr><mtd><mfrac><mrow><mi>f</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>-</mo><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>l</mtext><mtext>o</mtext><mtext>w</mtext></mrow></msubsup></mrow><mrow><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>h</mtext><mtext>i</mtext><mtext>g</mtext><mtext>h</mtext></mrow></msubsup><mo>-</mo><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>l</mtext><mtext>o</mtext><mtext>w</mtext></mrow></msubsup></mrow></mfrac><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>l</mtext><mtext>o</mtext><mtext>w</mtext></mrow></msubsup><mo>&lt;</mo><mi>f</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>&lt;</mo><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>h</mtext><mtext>i</mtext><mtext>g</mtext><mtext>h</mtext></mrow></msubsup></mtd></mtr><mtr><mtd><mn>2</mn><mn>5</mn><mn>5</mn><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>h</mtext><mtext>i</mtext><mtext>g</mtext><mtext>h</mtext></mrow></msubsup><mo>≤</mo><mi>f</mi><msub><mrow></mrow><mtext>L</mtext></msub></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>l</mtext><mtext>o</mtext><mtext>w</mtext></mrow></msubsup><mo>=</mo><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">) </mo><mo>-</mo><mi>d</mi><mo>*</mo><mi>α</mi></mtd></mtr><mtr><mtd><mi>f</mi><msubsup><mrow></mrow><mtext>L</mtext><mrow><mtext>h</mtext><mtext>i</mtext><mtext>g</mtext><mtext>h</mtext></mrow></msubsup><mo>=</mo><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">) </mo><mo>+</mo><mi>d</mi><mo>*</mo><mi>α</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">其中:mean为取均值;<i>d</i>为截断因子;<i>α</i>为方差;并将拉伸后的<i>f</i><sub>L</sub>和求解的<i>f</i><sub>R</sub>写回图像, 得到最后的基准图:</p>
                </div>
                <div class="p1">
                    <p id="118"><i>g</i><sub>stan</sub>=<i>f</i><sub>R</sub>*<i>f</i><sub>L</sub>      (9) </p>
                </div>
                <h3 id="119" name="119" class="anchor-tag">2 光照补偿和细节调整</h3>
                <h4 class="anchor-tag" id="120" name="120">2.1 <b>结合</b>Sigmoid<b>函数和伽马矫正的光照补偿方法</b></h4>
                <div class="p1">
                    <p id="121">在进行光照补偿中, 将变分Retinex增强后的V通道结果图像作为光照补偿的输入图像。相比直接使用原图像变换到HSV空间来得到V通道, 本文提出的方法更能保持目标场景的主要信息和结构, 根据变分Retinex理论得到的实际信息来进一步来调整光照。在图像的光照补偿中, 本文结合Sigmoid函数和伽马矫正构造新的光照补偿归一化函数, 如式 (10) 所示:</p>
                </div>
                <div class="p1">
                    <p id="122"><i>Y</i><sub>2</sub>=2*arctan (<i>a</i>*<i>Y</i><sub>0</sub>) /π      (10) </p>
                </div>
                <div class="p1">
                    <p id="123">其中:<i>Y</i><sub>0</sub>为RGB转为HSV之后V通道归一化的结果;<i>Y</i><sub>2</sub>为光照补偿后的结果;<i>a</i>为拉伸算子, 用来调整低亮度区域的拉伸程度, 对于不同亮度的图像, 其值也随着变化。<i>a</i>的计算如式 (11) :</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><mo>=</mo><mn>5</mn><mo>+</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>Y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>Y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">其中, mean为取均值, 从式 (11) 得到, 低照度的图像其均值越低, 那么得到的<i>a</i>值就越高, 对应低照度区域的亮度也会拉伸较大, 能提取更多的细节信息。</p>
                </div>
                <div class="p1">
                    <p id="126">本文通过拉伸图像低亮度区域来增加低亮度区域的细节信息, 以此来加强变分Retinex处理过后的V通道分量的细节信息。本文对光照补偿进行仿真实验对比, 结果如图3所示。由图3可以看出, 相比直接利用Retinex理论得到的增强图像, 经过本文方法光照补偿后得到的图像明显优于Retinex增强后的图像。</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906042_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 光照补偿对比" src="Detail/GetImg?filename=images/JSJY201906042_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 光照补偿对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906042_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Light compensation comparison</p>

                </div>
                <h4 class="anchor-tag" id="128" name="128">2.2 <b>改进反锐化掩模的细节调整算法</b></h4>
                <div class="p1">
                    <p id="129">在图像的细节调整中, 同样直接将变分Retinex得到增强后的V通道分量来进行有效的细节调整, 为了突出增强后的细节效果, 需要充分挖掘原图中图像隐藏的细节信息。本文采用了反锐化掩模算法, 其主要思想是:首先图像被分成两个部分, 一是低频的反锐化掩模部分, 可以通过图像的低通滤波如平滑和模糊技术获得, 二是高频部分, 可以通过原图减去反锐化掩模获取;然后高频部分被放大对比度并加入到反锐化掩模中去;最后得到增强后的图像。</p>
                </div>
                <div class="p1">
                    <p id="130">同时本文针对反锐化掩模 (Generalized Unsharp Masking, GUM) 算法<citation id="292" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>作出改进, 采用一般的mask来得到掩模容易引入噪声信息, 所以本文引入高斯引导滤波来得到掩模部分, 如式 (12) 所示;</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo>*</mo><mtext>π</mtext><mo>*</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>*</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>j</mi><mo>-</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mo>*</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">其中:<i>i</i>、 <i>j</i>表示图像中各像素的位置坐标;<i>k</i>是其中间位置;<i>σ</i>表示图像的标准差。</p>
                </div>
                <div class="p1">
                    <p id="133">对掩模部分采用自适应直方图均衡化得到加强后的效果, 接着将原图和均衡化后的图像进行归一化, 如式 (13) 所示:</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>=</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mi>g</mi></mrow><mrow><mi>max</mi><mo stretchy="false"> (</mo><mi>g</mi><mo>, </mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135">其中, <i>g</i>是由单位矩阵<b><i>I</i></b>除以高斯滤波后的图像得到的, 其计算如式 (14) :</p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><mo>=</mo><mfrac><mi mathvariant="bold-italic">Ι</mi><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>F</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>a</mtext><mtext>m</mtext><mtext>a</mtext></mrow></msup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="137">最后结合两个归一化后的图像得到最终的细节调整后的图像, 如式 (15) 所示:</p>
                </div>
                <div class="p1">
                    <p id="138" class="code-formula">
                        <mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><mo>=</mo><mfrac><mi mathvariant="bold-italic">Ι</mi><mrow><mi>max</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>.</mo><mn>1</mn><mo>, </mo><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mo stretchy="false"> (</mo><mi>G</mi><mo>*</mo><mi>Η</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="139">其中, <i>H</i>为<i>Y</i><sub>2</sub>进行自适应直方图均衡化之后, 再进行标准化处理得到的结果。最后得到的<i>V</i>即是通过GUM算法增强后的图像。</p>
                </div>
                <h3 id="140" name="140" class="anchor-tag">3 改进的模拟合成方法</h3>
                <div class="p1">
                    <p id="141">因为模拟曝光的<i>E</i><sub>1</sub>、<i>E</i><sub>2</sub>、<i>E</i><sub>3</sub>来自同一场景, 目标内容高度相关, 但是各自的侧重点不同。因此权重的设计对<i>E</i><sub>1</sub>、<i>E</i><sub>2</sub>、<i>E</i><sub>3</sub>至关重要, 要既能有效地增强低照度图像中未曝光区域, 又能有效地保留原图中曝光良好的区域。因此本文需要给曝光良好的像素值分配较大的权重, 未曝光区域的像素值分配较小的权重。基于这样的考虑, 本文从图像亮度、曝光度和色调对比度三个方面设计权重。</p>
                </div>
                <div class="p1">
                    <p id="142">亮度权重设计与目标场景的可见性有关, 亮度应该分配较大的权重给曝光良好的区域, 其他区域则对应分配较小的权重<citation id="293" type="reference"><link href="227" rel="bibliography" /><link href="243" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">16</a>]</sup></citation>。权重设计基于RGB通道信息, 其定义如下:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>W</mi><msubsup><mrow></mrow><mtext>B</mtext><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><msqrt><mrow><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi>R</mi><msup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo>-</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>G</mi><msup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo>-</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>B</mi><msup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo>-</mo><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><mo>/</mo><mn>3</mn></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">其中, <i>R</i><sup><i>E</i><sub><i>i</i></sub></sup>、<i>G</i><sup><i>E</i><sub><i>i</i></sub></sup>、<i>B</i><sup><i>E</i><sub><i>i</i></sub></sup>分别为<i>E</i><sub><i>i</i></sub>转为RGB空间后的各颜色值。</p>
                </div>
                <div class="p1">
                    <p id="145">曝光度权重从估算像素的曝光程度计算。在大多数情况下, 希望增强图像的像素值尽可能保持一个平均状态, 而不是0和255的两个极端, 所以采用高斯距离模型设计曝光度权重<citation id="294" type="reference"><link href="245" rel="bibliography" /><link href="247" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="146" class="code-formula">
                        <mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><msubsup><mrow></mrow><mtext>e</mtext><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mo>-</mo><mfrac><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>E</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>max</mi></mrow></msubsup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="147">因为最终目的是希望增强后的像素保持一个中间状态, <i>E</i><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>max</mi></mrow></msubsup></mrow></math></mathml>的取值范围为0到1, 所以, 所有<i>E</i><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>max</mi></mrow></msubsup></mrow></math></mathml>的设置为0.5, 标准差<i>σ</i>则采用默认设置0.25<citation id="295" type="reference"><link href="243" rel="bibliography" /><link href="245" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="150">最后一个权重是色调对比度。色调对比度是高质量图像的一个重要特征, 其设计采用文献<citation id="296" type="reference">[<a class="sup">19</a>]</citation>的色调滤波, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="151"><i>W</i><sup><i>E</i><sub><i>i</i></sub></sup><sub>c</sub>=<i>E</i><sub><i>i</i></sub> (1+<i>γ</i> cos (<i>Hκ</i>+ϕ) <i>S</i>)      (18) </p>
                </div>
                <div class="p1">
                    <p id="152">其中:<i>H</i>和<i>S</i>为输入图像在HSV空间的分量;<i>κ</i>用来保持颜色对等, ϕ表示颜色轮盘的偏移角度, <i>γ</i>用来缓解颜色饱和的影响, <i>γ</i>、ϕ和<i>κ</i>为默认设置<citation id="297" type="reference"><link href="249" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。最终的权重<i>W</i>是三者共同的乘积, 三个权重同时约束最终的权重<citation id="298" type="reference"><link href="243" rel="bibliography" /><link href="251" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">20</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="153"><i>W</i><sup><i>E</i><sub><i>i</i></sub></sup>=<i>W</i><sup><i>E</i><sub><i>i</i></sub></sup><sub>B</sub>*<i>W</i><sup><i>E</i><sub><i>i</i></sub></sup><sub>e</sub>*<i>W</i><sup><i>E</i><sub><i>i</i></sub></sup><sub>c</sub>      (19) </p>
                </div>
                <div class="p1">
                    <p id="154">为了得到一致性的结果, 将各曝光图的权重作归一化:</p>
                </div>
                <div class="p1">
                    <p id="155" class="code-formula">
                        <mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>W</mi><mo stretchy="true">¯</mo></mover><msup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo>=</mo><mfrac><mrow><mi>W</mi><msup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>W</mi></mstyle><msup><mrow></mrow><mrow><mi>E</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="156">最后的增强图像<i>E</i><sub>en</sub>通过融合上述的曝光图与权重图获得。若采用传统融合策略, 容易导致最后的增强结果产生离散的光晕效应<citation id="299" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>;因此本文采用多尺度融合策略融合曝光图和权重图, 首先采用拉普拉斯金字塔对曝光图进行不同尺度的分解, 权重图采用高斯金字塔进行不同尺度的分解, 其定义如下:</p>
                </div>
                <div class="p1">
                    <p id="157" class="code-formula">
                        <mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msubsup><mrow></mrow><mrow><mtext>e</mtext><mtext>n</mtext></mrow><mi>l</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>G</mi></mstyle><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">{</mo><mover accent="true"><mi>W</mi><mo stretchy="true">¯</mo></mover><msup><mrow></mrow><mi>Κ</mi></msup><mo stretchy="false">}</mo><mi>L</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">{</mo><mi>E</mi><msup><mrow></mrow><mi>Κ</mi></msup><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="158">其中:<i>G</i>、<i>L</i>分别表示高斯金字塔操作和拉普拉斯操作;根据文献<citation id="300" type="reference">[<a class="sup">17</a>]</citation>, <i>l</i>设定为5。</p>
                </div>
                <h3 id="159" name="159" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="160">本文设计了四组对比实验定性地分析算法的性能, 用于对比的算法如下:反锐化掩模 (<i>GUM</i>) <citation id="301" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、多尺度<i>Retinex</i> (<i>A MultiScale Retinex</i>, <i>AMSR</i>) 算法<citation id="302" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、通用均衡模型 (<i>Generalized Equalization Model</i>, <i>GEM</i>) <citation id="303" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、全局优化的线性窗口色调映射 (<i>Globally Optimized Linear Windowed tone mapping</i>, <i>GOLW</i>) 算法<citation id="304" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、曝光融合方法 (<i>Exposure Fusion Framework</i>, <i>EFF</i>) <citation id="305" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、基于鲁棒<i>Retinex</i>模型 (<i>Robust Retinex Model</i>, <i>RRM</i>) 的图像增强算法<citation id="306" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。测试的低照度图像全部为公开的数据集:<i>DICM</i> (<i>DIgital CaMeras</i>) <citation id="307" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、<i>NPE</i>-<i>Data</i> (<i>Naturalness Preserved Enhancement</i>-<i>Data</i>) <citation id="308" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、<i>LIME</i>-<i>Data</i> (<i>Low</i>-<i>light IMage</i><i>Enhancement</i>-<i>Data</i>) <citation id="309" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、<i>MEF</i> (<i>Multi</i>-<i>Exposure image Fusion</i>) <citation id="310" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>。为了保证实验的公平性, 文章涉及的代码均为<i>Matlab</i>实现, 对比实验涉及的参数全部采用默认参数。本文从客观评价和主观分析两方面分析算法性能。</p>
                </div>
                <div class="p1">
                    <p id="161">客观评价将从亮度失真 (<i>Lightness</i>-<i>Order</i>-<i>Error</i>, <i>LOE</i>) <citation id="311" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、视觉信息失真<citation id="312" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>两个方面对算法进行评估。亮度失真定义如下:</p>
                </div>
                <div class="p1">
                    <p id="162" class="code-formula">
                        <mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>Ο</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>R</mi></mstyle><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="163">其中:<i>RD</i> (<i>x</i>) 表示的是输入图像和增强图像的相对阶差分;<i>x</i>表示的是每个像素。<i>RD</i> (<i>x</i>) 定义为:</p>
                </div>
                <div class="p1">
                    <p id="164" class="code-formula">
                        <mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>U</mi></mstyle><mo stretchy="false"> (</mo><mi>L</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>, </mo><mi>L</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>⊕</mo><mi>U</mi><mo stretchy="false"> (</mo><mi>L</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>, </mo><mi>L</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="165">其中:<i>m</i>为像素个数;⊕为异或操作;<i>L</i> (<i>x</i>) 和<i>L</i><sup>1</sup> (<i>x</i>) 分别表示的是像素<i>x</i>在原始图像和增强图中的最大值。对于<i>U</i> (<i>x</i>, <i>y</i>) , 默认返回值为1;如果<i>x</i> ≥ <i>y</i>, 返回为0。文献<citation id="313" type="reference">[<a class="sup">7</a>,<a class="sup">11</a>]</citation>指出, LOE值越小, 图像的亮度自然性保持越好, 亮度失真率越低。由于计算LOE时间复杂度比较高, 所以本文将所有测试集图像下采样到100×100。在不同数据集上, 将本文算法和其他算法进行综合对比, 结果如表1所示。从表1可以看出, 本文算法的失真率结果最优, 表明本文算法具有很好的自然保留性。</p>
                </div>
                <div class="area_img" id="166">
                    <p class="img_tit"><b>表</b>1 <b>不同算法的</b>LOE<b>测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 LOE test results of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="166" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />数据集</td></tr><tr><td><br />DICM</td><td>NPE-Data</td><td>LIME-Data</td><td>MEF</td></tr><tr><td>AMSR</td><td>1 770.40</td><td>2 036.17</td><td>770.04</td><td>1 492.10</td></tr><tr><td><br />GEM</td><td>524.34</td><td>379.61</td><td>520.02</td><td>258.71</td></tr><tr><td><br />GUM</td><td>380.78</td><td>344.83</td><td>510.71</td><td>288.65</td></tr><tr><td><br />GOLW</td><td>1 080.29</td><td>445.44</td><td>675.37</td><td>809.61</td></tr><tr><td><br />RRM</td><td>360.06</td><td>298.25</td><td>288.71</td><td>274.31</td></tr><tr><td><br />EFF</td><td>338.74</td><td>265.43</td><td>328.46</td><td>250.69</td></tr><tr><td><br />本文算法</td><td>337.11</td><td>279.21</td><td>310.53</td><td>242.55</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="167">图像视觉信息失真是图像质量的一个重要评估指标。本文采用视觉信息保真度 (Visual Information Fidelity, VIF) <sup></sup><citation id="314" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>去评估增强图像的信息失真, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="168" class="code-formula">
                        <mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><mi>Ι</mi><mi>F</mi><mo>=</mo><mfrac><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi>C</mi><mo>, </mo><mi>F</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi>C</mi><mo>, </mo><mi>E</mi><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="169">其中:<i>C</i>为参考图像;<i>F</i>为失真图像;<i>E</i>是人类视觉系统感知的图像;<i>K</i>为模拟大脑感知的交互信息;<i>VIF</i>作为全参考图像质量评估指标, 被用来测试退化图像的质量。然而在图像增强质量评估中, 参考图像为增强图像, 输入图像作为退化图像;VIF值越大, 说明增强图像具有越好的信息保真, 信息失真率比较小。从表2可以看出, 本文的算法在大部分数据集上VIF值都高于其他比较算法, 表明本文的算法具有很好的信息保真度。</p>
                </div>
                <div class="p1">
                    <p id="170">从图4可以看以, GOLW算法在低照度场景上增强结果偏向于白化, 并伴随有噪声扩大现象, 整体亮度较高, 亮度趋向于过度饱和状态, 导致某些区域出现过度曝光;GUM和本文算法实验的结果比较相似, 但是在处理暗度较深的场景上, 本文的算法较为自然;AMSR算法可能会在部分区域产生光晕, GEM在色调较为丰富的区域存在着色彩溢出的问题;EFF算法在图片色调较暗区域不能完整地保留细节信息, RRM算法有噪声扩大现象, 并且物体边缘会存在模糊现象。而本文算法由于对低照度图像进行光照补偿和细节调整, 很好地保留了低照度情况下物体的细节, 最终输出图像具有较高的颜色保真, 而且在提升物体可见度方面效果明显;同时, 在多尺度融合阶段, 添加了自适应的权值因子, 能够很好地保留全局对比度和细节, 避免了光晕伪影, 使融合结果更加丰富多彩。通过实验对比数据, 综合主观评价和客观评价两个图像质量评价指标表明, 相比其他算法, 本文提出的算法能够得到质量更高的增强图像。</p>
                </div>
                <div class="area_img" id="171">
                    <p class="img_tit"><b>表</b>2 <b>不同算法的</b>VIF<b>测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 VIF test results of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="171" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />数据集</td></tr><tr><td><br />DICM</td><td>NPE-Data</td><td>LIME-Data</td><td>MEF</td></tr><tr><td>AMSR</td><td>0.726 719</td><td>0.661 242</td><td>0.511 582</td><td>0.616 510</td></tr><tr><td><br />GEM</td><td>0.697 067</td><td>0.656 962</td><td>0.418 208</td><td>0.666 262</td></tr><tr><td><br />GUM</td><td>0.708 944</td><td>0.631 252</td><td>0.447 517</td><td>0.589 610</td></tr><tr><td><br />GOLW</td><td>0.450 758</td><td>0.457 054</td><td>0.240 416</td><td>0.335 729</td></tr><tr><td><br />RRM</td><td>0.666 262</td><td>0.713 875</td><td>0.541 339</td><td>0.782 309</td></tr><tr><td><br />EFF</td><td>0.734 532</td><td>0.735 729</td><td>0.468 954</td><td>0.880 565</td></tr><tr><td><br />本文算法</td><td>0.748 803</td><td>0.721 886</td><td>0.527 424</td><td>0.898 456</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="172">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906042_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同算法各个场景图像增强结果对比" src="Detail/GetImg?filename=images/JSJY201906042_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同算法各个场景图像增强结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906042_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of image enhancement results of different algorithms in different scenes</p>

                </div>
                <h3 id="173" name="173" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="174">针对低照度图像, 本文提出了一种基于模拟多曝光融合的图像增强方法。首先, 利用形态学和改进的变分模型产生曝光图像集的基准图, 以此保证增强结果的主体信息;为了模拟多曝光, 分别以亮度和细节为目的, 在基准图的基础上产生另外两幅曝光图;然后, 分别基于亮度、曝光度和色调设计曝光图集的加权值, 为曝光良好的像素分配较大的权值, 为曝光不足的区域分配较小的权值;最后, 采取多尺度融合图像集的方式, 避免增强结果中的光晕, 得到了最终增强的图像。将本文的方法和已有的低照度图像增强方法在四个公开的数据集上进行测试对比, 实验结果表明, 本文的方法具有较小的亮度失真和对比度失真, 能够有效地保留图像本身的视觉信息。但是因为本文采取了交替求解策略, 迭代次数增加了算法的复杂度, 因此在接下来的工作中, 如何提高算法的效率将是下一步的研究重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="213">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contrast enhancement based on intrinsic image decomposition">

                                <b>[1]</b>YUE H J, YANG J Y, SUN X Y, et al.Contrast enhancement based on intrinsic image decomposition[J].IEEE Transactions on Image Processing, 2017, 26 (8) :3981-3994.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contrast enhancement based on layered difference representation">

                                <b>[2]</b>LEE C, LEE C, KIM C S.Contrast enhancement based on layered difference representation of 2D histograms[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5372-5384.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contextual and Variational Contrast Enhancement">

                                <b>[3]</b>CELIK T, TJAHJADI T.Contextual and variational contrast enhancement[J].IEEE Transactions on Image Processing, 2011, 20 (12) :3431-3441.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The analysis of image contrast:from quality assessment to automatic enhancement">

                                <b>[4]</b>GU K, ZHAI G T, LIN W S, et al.The analysis of image contrast:from quality assessment to automatic enhancement[J].IEEETransactions on Cybernetics, 2016, 46 (1) :284-297.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Properties and performance of a center/surround retinex">

                                <b>[5]</b>JOBSON D J, RAHMAN Z, WOODELL G A.Properties and performance of a center/surround Retinex[J].IEEE Transactions on Image Processing, 1997, 6 (3) :451-462.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200908016&amp;v=MjgyODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEa1c3eklMejdCZDdHNEh0ak1wNDlFWW9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>刘茜, 卢心红, 李象霖.基于多尺度Retinex的自适应图像增强方法[J].计算机应用, 2009, 29 (8) :2077-2079. (LIU Q, LU XH, LI X L.Adaptive image enhancement method based on multiscale Retinex algorithm[J].Journal of Computer Applications, 2009, 29 (8) :2077-2079.) 
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Naturalness preserved enhancement algorithm for non-uniform illumination images">

                                <b>[7]</b>WANG S H, ZHENG J, HU H M, et al.Naturalness preserved enhancement algorithm for non-uniform illumination images[J].IEEETransactions on Image Processing, 2013, 22 (9) :3538-3548.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201008029&amp;v=MjQwOTlHRnJDVVI3cWZadVpzRnlEa1c3eklMejdCZDdHNEg5SE1wNDlIYllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>王小明, 黄昶, 李全彬, 等.改进的多尺度Retinex图像增强算法[J].计算机应用, 2010, 30 (8) :2091-2093. (WANG X M, HUANG C, LI Q B, et al.Improved multi-scale Retinex image enhancement algorithm[J].Journal of Computer Applications, 2010, 30 (8) :2091-2093.) 
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Probabilistic Method for Image Enhancement With Simultaneous Illumination and Reflectance Estimation.">

                                <b>[9]</b>FU X Y, LIAO Y H, ZENG D L, et al.A probabilistic method for image enhancement with simultaneous illumination and reflectance estimation[J].IEEE Transactions on Image Processing, 2015, 24 (12) :4965-4977.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBFC456CACB282B35E553411B2230C2BC&amp;v=MjY0NDVyTG5SOGpzQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekxtM3c2Zz1OaWZPZmNIT2JkWEpxZncwRjVrTkJINUx6Qk5tN3pwK1RIN2ozaEEzZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>WANG Y F, WANG H Y, YIN C L, et al.Biologically inspired image enhancement based on Retinex[J].Neurocomputing, 2016, 177 (C) :373-384.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LIME:Low-light image enhancement via illumination map estimation">

                                <b>[11]</b>GUO X J, LI Y, LING H B.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A weighted variational model for simultaneous reflectanceand illumination estimation">

                                <b>[12]</b>FU X Y, ZENG D L, HUANG Y, et al.A weighted variational model for simultaneous reflectance and illumination estimation[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2782-2790.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830803&amp;v=Mjg2Mzg5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFc3N0FJMTQ9Tmo3QmFyTzRIdEhPcDR4RmJPc01ZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>KIMMEL R, ELAD M, SHAKED D, et al.A variational framework for Retinex[J].International Journal of Computer Vision, 2003, 52 (1) :7-23.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Split Bregman Method for L1-Regularized Problems">

                                <b>[14]</b>GOLDSTEIN T, OSHER S.The split bregman method for L1-regularized problems[J].SIAM Journal on Imaging Sciences, 2009, 2 (2) :323-343.
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A generalized unsharp masking algorithm">

                                <b>[15]</b>DENG G.A generalized unsharp masking algorithm[J].IEEETransactions on Image Processing, 2011, 20 (5) :1249-1261.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single Image Dehazing by Multi-Scale Fusion">

                                <b>[16]</b>ANCUTI C O, ANCUTI C.Single image dehazing by multi-scale fusion[J].IEEE Transactions on Image Processing, 2013, 22 (8) :3271-3282.
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-scale fusion:an effective approach to merging images">

                                <b>[17]</b>ANCUTI C O, ANCUTI C, de VLEESCHOUWER C, et al.Single-scale fusion:an effective approach to merging images[J].IEEE Transactions on Image Processing, 2017, 26 (1) :65-78.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhancing Underwater Images and Videos by Fusion">

                                <b>[18]</b>ANCUTI C, ANCUTI C O, HABER T, et al.Enhancing underwater images and videos by fusion[C]//Proceedings of the 2012IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:81-88.
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhancing by saliency-guided decolorization">

                                <b>[19]</b>ANCUTI C O, ANCUTI C, BEKAERT P.Enhancing by saliencyguided decolorization[C]//CVPR 2011:Proceedings of the 2011IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2011:257-264.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Referenceless prediction of perceptual fog density and perceptual image defogging">

                                <b>[20]</b>CHOI L K, YOU J, BOVIK A C.Referenceless prediction of perceptual fog density and perceptual image defogging[J].IEEETransactions on Image Processing, 2015, 24 (11) :3888-3901.
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A multiscale retinex for bridging the gap between color images and the human observation of scenes">

                                <b>[21]</b>JOBSON D J, RAHMAN Z, WOODELL G A.A multiscale Retinex for bridging the gap between color images and the human observation of scenes[J].IEEE Transactions on Image Processing, 1997, 6 (7) :965-976.
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized Equalization Model for Image Enhancement">

                                <b>[22]</b>XU H T, ZHAI G Z, WU X L, et al.Generalized equalization model for image enhancement[J].IEEE Transactions on Multimedia, 2014, 16 (1) :68-82.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Globally optimized linear windowed tone mapping">

                                <b>[23]</b>SHAN Q, JIA J Y, BROWN M S.Globally optimized linear windowed tone mapping[J].IEEE transactions on Visualization and Computer Graphics, 2010, 16 (4) :663-675.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new image contrast enhancement algorithm using exposure fusion framework">

                                <b>[24]</b>YING Z Q, LI G, REN Y R, et al.A new image contrast enhancement algorithm using exposure fusion framework[C]//CAIP 2017:Proceedings of the 2017 International Conference on Computer Analysis of Images and Patterns, LNCS 10425.Cham:Springer, 2017:36-46.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structure-Revealing Low-Light Image Enhancement Via Robust Retinex Model.">

                                <b>[25]</b>LI M D, LIU J Y, YANG W H, et al.Structure-revealing lowlight image enhancement via robust Retinex model[J].IEEETransactions on Image Processing, 2018, 27 (6) :2828-2841.
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual Quality Assessment for Multi-Exposure Image Fusion">

                                <b>[26]</b>MA K, ZENG K, WANG Z.Perceptual quality assessment for multi-exposure image fusion[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3345-3356.
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image information and visual quality">

                                <b>[27]</b>SHEIKH H R, BOVIK A C.Image information and visual quality[J].IEEE Transactions on Image Processing, 2006, 15 (2) :430-444.
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weighted Guided Image Filtering">

                                <b>[28]</b>LI Z G, ZHENG J H, ZHU Z J, et al.Weighted guided image filtering[J].IEEE Transactions on Image Processing, 2015, 24 (1) :120-129.
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic range independent image quality assessment">

                                <b>[29]</b>AYDIN T O, MANTIUK R, MYSZKOWSKI K, et al.Dynamic range independent image quality assessment[C]//SIGGRAPH2008:Proceedings of the 2008 ACM SIGGRAPH.New York:ACM, 2008:Article No.69.
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fusion-based method for single backlit image enhancement">

                                <b>[30]</b>WANG Q H, FU X Y, ZHANG X P, et al.A fusion-based method for single backlit image enhancement[C]//ICIP 2016:Proceedings of the 2016 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2016:4077-4081.
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new low-light image enhancement algorithm using camera response model">

                                <b>[31]</b>YING Z Q, LI G, REN Y R, et al.A new low-light image enhancement algorithm using camera response model[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision Workshop.Piscataway, NJ:IEEE, 2017:3015-3022.
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_32" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGHW201701023&amp;v=MDkzOTA1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEa1c3eklMeXJEZWJHNEg5Yk1ybzlIWjRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[32]</b>王晨, 汤心溢, 高思莉.基于人眼视觉的红外图像增强算法研究[J].激光与红外, 2017, 47 (1) :114-118. (WANG C, TANGX Y, GAO S L.Infrared image enhancement algorithm based on human vision[J].Laser and Infrared, 2017, 47 (1) :114-118.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906042" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906042&amp;v=MjI1OTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGtXN3pJTHo3QmQ3RzRIOWpNcVk5QlpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
