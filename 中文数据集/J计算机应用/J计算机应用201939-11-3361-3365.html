<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136450861846250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911043%26RESULT%3d1%26SIGN%3d3GDsyIAYf4%252fML7MLEFomhn0j7%252fo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911043&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911043&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911043&amp;v=MjEyMjU3cWZadVpzRnluZ1ZycklMejdCZDdHNEg5ak5ybzlCWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="2 左心耳自动分割方法 ">2 左心耳自动分割方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="2.1 &lt;b&gt;基于&lt;/b&gt;YOLO&lt;b&gt;模型的自动定位&lt;/b&gt;">2.1 <b>基于</b>YOLO<b>模型的自动定位</b></a></li>
                                                <li><a href="#69" data-title="2.2 &lt;b&gt;基于&lt;/b&gt;C-V&lt;b&gt;模型的自动分割&lt;/b&gt;">2.2 <b>基于</b>C-V<b>模型的自动分割</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="3 实验结果和分析 ">3 实验结果和分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="3.1 &lt;b&gt;实验数据和实验平台&lt;/b&gt;">3.1 <b>实验数据和实验平台</b></a></li>
                                                <li><a href="#79" data-title="3.2 &lt;b&gt;实验设置&lt;/b&gt;">3.2 <b>实验设置</b></a></li>
                                                <li><a href="#85" data-title="3.3 &lt;b&gt;定位结果&lt;/b&gt;">3.3 <b>定位结果</b></a></li>
                                                <li><a href="#88" data-title="3.4 &lt;b&gt;定位误差&lt;/b&gt;">3.4 <b>定位误差</b></a></li>
                                                <li><a href="#93" data-title="3.5 &lt;b&gt;分割精度&lt;/b&gt;">3.5 <b>分割精度</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="图1 左心耳超声图像自动分割的算法流程">图1 左心耳超声图像自动分割的算法流程</a></li>
                                                <li><a href="#51" data-title="图2 基于YOLO网络的LAA自动定位框架图">图2 基于YOLO网络的LAA自动定位框架图</a></li>
                                                <li><a href="#60" data-title="图3 YOLO网络的结构及参数设置图">图3 YOLO网络的结构及参数设置图</a></li>
                                                <li><a href="#87" data-title="图4 不同扫描条件下的自动定位结果">图4 不同扫描条件下的自动定位结果</a></li>
                                                <li><a href="#90" data-title="图5 不同迭代次数下正确和偏差样本数的比例">图5 不同迭代次数下正确和偏差样本数的比例</a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同迭代次数下自动定位结果&lt;/b&gt;"><b>表</b>1 <b>不同迭代次数下自动定位结果</b></a></li>
                                                <li><a href="#95" data-title="图6 C-V模型的自动分割结果">图6 C-V模型的自动分割结果</a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表&lt;/b&gt;2 C-V&lt;b&gt;模型&lt;/b&gt;、LBF&lt;b&gt;模型&lt;/b&gt;、RSF&lt;b&gt;模型自动分割结果评价指标&lt;/b&gt;"><b>表</b>2 C-V<b>模型</b>、LBF<b>模型</b>、RSF<b>模型自动分割结果评价指标</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表&lt;/b&gt;3 C-V&lt;b&gt;模型&lt;/b&gt;、LBF&lt;b&gt;模型&lt;/b&gt;、RSF&lt;b&gt;模型&lt;/b&gt;CPU&lt;b&gt;耗时比较&lt;/b&gt;"><b>表</b>3 C-V<b>模型</b>、LBF<b>模型</b>、RSF<b>模型</b>CPU<b>耗时比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="131">


                                    <a id="bibliography_1" title=" AL-SAADY N M,OBEL O A,CAMM A J.Left atrial appendage:structure,function,and role in thrombo-embolism[J].Heart,1999,82(5):547-554." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Left atrial appendage: structure, function, and role in thromboembolism">
                                        <b>[1]</b>
                                         AL-SAADY N M,OBEL O A,CAMM A J.Left atrial appendage:structure,function,and role in thrombo-embolism[J].Heart,1999,82(5):547-554.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_2" title=" KONG B,LIU Y,HUANG H,et al.Left atrial appendage closure for thrombo-embolism prevention in patients with atrial fibrillation:advances and perspectives[J].Journal of Thoracic Disease.2015,7(2):199-203." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Left atrial appendage closure for thrombo-embolism prevention in patients with atrial fibrillation:advances and perspectives">
                                        <b>[2]</b>
                                         KONG B,LIU Y,HUANG H,et al.Left atrial appendage closure for thrombo-embolism prevention in patients with atrial fibrillation:advances and perspectives[J].Journal of Thoracic Disease.2015,7(2):199-203.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_3" title=" LEAL S,MORENO R,de SOUSA ALMEIDA M,et al.Evidence-based percutaneous closure of the left atrial appendage in patients with atrial fibrillation[J].Current Cardiology Reviews.2012,8(1):37-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SBAD&amp;filename=SBAD120619000029&amp;v=MDY1Njlidm5LcmlmWmVadkZ5bm5VNzdOSlZzWE5pL0thcks2SHRmTnBvOUZaT3NOQlJNOHp4VVNtRGQ5U0g3bjN4RTlm&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         LEAL S,MORENO R,de SOUSA ALMEIDA M,et al.Evidence-based percutaneous closure of the left atrial appendage in patients with atrial fibrillation[J].Current Cardiology Reviews.2012,8(1):37-42.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_4" title=" WHITLOCK R P,HEALEY J S,CONNOLLY S J.Left atrial appendage occlusion does not eliminate the need for warfarin [J].Circulation,2009,120(19):1927-1932." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWK&amp;filename=SJWK12103000037737&amp;v=MTQxOTdpbmxVcjNJSVZzUmJ4RT1OaWZjWmJLNkg5SFByNDlGWk9nSUMzOCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         WHITLOCK R P,HEALEY J S,CONNOLLY S J.Left atrial appendage occlusion does not eliminate the need for warfarin [J].Circulation,2009,120(19):1927-1932.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_5" title=" ABDELMONEIM S S,MULVAGH S L.Techniques to improve left atrial appendage imaging[J].Journal of Atrial Fibrillation,2014,7(1):No.1059." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Techniques to improve left atrial appendage imaging">
                                        <b>[5]</b>
                                         ABDELMONEIM S S,MULVAGH S L.Techniques to improve left atrial appendage imaging[J].Journal of Atrial Fibrillation,2014,7(1):No.1059.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_6" title=" CHAN T F,VESE L A.Active contours without edges[J].IEEE Transactions on Image Processing,2001,10(2):266-277." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active contours without edges">
                                        <b>[6]</b>
                                         CHAN T F,VESE L A.Active contours without edges[J].IEEE Transactions on Image Processing,2001,10(2):266-277.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_7" title=" LI C,KAO C Y,GORE J C,et al.Implicit active contours driven by local binary fitting energy[C]// Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2007:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Implicit Active Contours Driven by Local BinaryFitting Energy">
                                        <b>[7]</b>
                                         LI C,KAO C Y,GORE J C,et al.Implicit active contours driven by local binary fitting energy[C]// Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2007:1-7.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_8" title=" LI C,KAO C Y,GORE J C,et al.Minimization of region-scalable fitting energy for image segmentation[J].IEEE Transactions on Image Processing,2008,17(10):1940-1949." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Minimization of Region-Scalable Fitting Energy for Image Segmentation">
                                        <b>[8]</b>
                                         LI C,KAO C Y,GORE J C,et al.Minimization of region-scalable fitting energy for image segmentation[J].IEEE Transactions on Image Processing,2008,17(10):1940-1949.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     LeCUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444.</a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_10" title=" GIRSHICK R,DONAHUE J,DARRELL T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[10]</b>
                                         GIRSHICK R,DONAHUE J,DARRELL T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2014:580-587.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_11" title=" GIRSHICK R.Fast R-CNN[EB/OL].[2019- 04- 20].https://arxiv.org/pdf/1504.08083.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[11]</b>
                                         GIRSHICK R.Fast R-CNN[EB/OL].[2019- 04- 20].https://arxiv.org/pdf/1504.08083.pdf.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     REN S,HE K,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.</a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_13" title=" GOULD S,GAO T,KOLLER D.Region-based segmentation and object detection[C]// Advances in Neural Information Processing Systems 22.Cambridge,MA:MIT Press,2009:655-663." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Region-based segmentation and objectdetection">
                                        <b>[13]</b>
                                         GOULD S,GAO T,KOLLER D.Region-based segmentation and object detection[C]// Advances in Neural Information Processing Systems 22.Cambridge,MA:MIT Press,2009:655-663.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_14" title=" REDMON J,DIVVALA S,GIRSHICK R,et al.You only look once:unified,real-time object detection[EB/OL].[2019- 04- 20].https://arxiv.org/pdf/1506.02640.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:unified,real-time object detection">
                                        <b>[14]</b>
                                         REDMON J,DIVVALA S,GIRSHICK R,et al.You only look once:unified,real-time object detection[EB/OL].[2019- 04- 20].https://arxiv.org/pdf/1506.02640.pdf.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     REDMON J,FARHADI A.YOLO9000:better,faster,stronger[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6517-6525.</a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_16" title=" REDMON J,FARHADI A.YOLOv3:an incremental improvement[EB/OL].[2018- 05- 25].https://arxiv.org/pdf/1804.02767.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An Incremental Improvement[C/OL]">
                                        <b>[16]</b>
                                         REDMON J,FARHADI A.YOLOv3:an incremental improvement[EB/OL].[2018- 05- 25].https://arxiv.org/pdf/1804.02767.pdf.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_17" title=" MAŠKA M,DANĔK O,GARASA S,et al.Segmentation and shape tracking of whole fluorescent cells based on the Chan-Vese model[J].IEEE Transactions on Medical Imaging,2013,32(6):995-1006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segmentation and shape tracking of whole fluorescent cells based on the chan-vese model">
                                        <b>[17]</b>
                                         MAŠKA M,DANĔK O,GARASA S,et al.Segmentation and shape tracking of whole fluorescent cells based on the Chan-Vese model[J].IEEE Transactions on Medical Imaging,2013,32(6):995-1006.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_18" title=" WANG X,ZHENG C,LI C.Automated CT liver segmentation using improved Chan-Vese model with global shape constrained energy[C]// Proceedings of the 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway:IEEE,2011:3415-3418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated CT liver segmentation using improved Chan-Vese model with global shape constrained energy">
                                        <b>[18]</b>
                                         WANG X,ZHENG C,LI C.Automated CT liver segmentation using improved Chan-Vese model with global shape constrained energy[C]// Proceedings of the 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway:IEEE,2011:3415-3418.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_19" title=" MAAS A L,HANNUN A Y,NG A Y,et al.Rectifier nonlinearities improve neural network acoustic models[EB/OL].[2018- 05- 28].http://robotics.stanford.edu/～amaas/papers/relu_hybrid_icml2013_final.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rectifier nonlinearities improve neural network acoustic models">
                                        <b>[19]</b>
                                         MAAS A L,HANNUN A Y,NG A Y,et al.Rectifier nonlinearities improve neural network acoustic models[EB/OL].[2018- 05- 28].http://robotics.stanford.edu/～amaas/papers/relu_hybrid_icml2013_final.pdf
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-26 09:04</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3361-3365 DOI:10.11772/j.issn.1001-9081.2019040771            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的超声图像左心耳自动分割方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A9%E8%B7%AF%E6%98%93&amp;code=35034872&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩路易</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E9%9F%AB%E6%A0%80&amp;code=26424812&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄韫栀</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%AA%A6%E6%B5%A9%E7%84%B6&amp;code=43224398&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">窦浩然</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%99%BD%E6%96%87%E5%A8%9F&amp;code=15765470&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">白文娟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%A5%87&amp;code=09791005&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘奇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0054367&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川大学电气工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E6%9D%90%E6%96%99%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0147648&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川大学材料科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B7%B1%E5%9C%B3%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0128376&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深圳大学生物医学工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E5%8D%8E%E8%A5%BF%E5%8C%BB%E9%99%A2%E8%B6%85%E5%A3%B0%E5%BF%83%E5%86%85%E7%A7%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川大学华西医院超声心内科</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>从超声图像中分割出左心耳(LAA)是得出临床诊断指标的重要步骤,而准确自动分割的首要步骤和难点就是实现目标的自动定位。针对这一问题,提出了一种结合基于深度学习框架的自动定位和基于模型的分割算法的方法来实现超声图像中LAA的自动分割。首先,训练YOLO模型作为LAA自动定位的网络架构;其次,通过验证集确定最优的权重文件,并预测出LAA的最小包围盒;最后,在正确定位的基础上,将YOLO预测的最小包围盒放大1.5倍作为初始轮廓,利用C-V模型完成LAA的自动分割。分割结果用5项指标加以评价:正确性、敏感性、特异性、阴性、阳性。实验结果表明,所提方法能够实现不同分辨率条件和不同显示模式下LAA的自动定位,小样本数据在1 000次迭代时已经达到最优的定位效果,正确定位率达到72.25%,并且在正确定位的基础上,C-V模型的分割准确率能够达到98.09%。因此,深度学习技术在实现LAA超声图像的自动分割上具备较大的潜力,能够为基于轮廓的分割算法提供良好的初始轮廓。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E5%8A%A8%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自动分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=C-V%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">C-V模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B7%A6%E5%BF%83%E8%80%B3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">左心耳;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%A3%B0%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超声图像;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    韩路易(1995—),男,四川成都人,硕士研究生,主要研究方向:医学图像处理;;
                                </span>
                                <span>
                                    黄韫栀(1989—),女,江苏靖江人,博士研究生,主要研究方向:医学信号处理、医学图像处理;;
                                </span>
                                <span>
                                    窦浩然(1995—),男,河北邢台人,硕士研究生,主要研究方向:医学图像分析;;
                                </span>
                                <span>
                                    白文娟(1982—),山西广灵人,副主任医师,博士研究生,主要研究方向:心脏瓣膜病的动态评估;;
                                </span>
                                <span>
                                    *刘奇(1966—),男,四川内江人,教授,博士,主要研究方向:机器视觉、医学图像处理、医学信息,电子邮箱,liuqi@scu.edu.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>成都市科技局项目(2015-HM01-00525-SF);</span>
                    </p>
            </div>
                    <h1><b>Automatic method for left atrial appendage segmentation from ultrasound images based on deep learning</b></h1>
                    <h2>
                    <span>HAN Luyi</span>
                    <span>HUANG Yunzhi</span>
                    <span>DOU Haoran</span>
                    <span>BAI Wenjuan</span>
                    <span>LIU Qi</span>
            </h2>
                    <h2>
                    <span>College of Electrical Engineering, Sichuan University</span>
                    <span>College of Materials Science and Engineering, Sichuan University</span>
                    <span>School of Biomedical Engineering, Shenzhen University</span>
                    <span>Department of Cardiology, West China Hospital of Sichuan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Segmenting Left Atrial Appendage(LAA) from ultrasound image is an essential step for obtaining the clinical indicators, and the prerequisite and difficulty for automatic and accurate segmentation is locating the target accurately. Therefore, a method combining with automatic location based on deep learning and segmenting algorithm based on model was proposed to accomplish the automatic segmentation of LAA from ultrasound images. Firstly, You Only Look Once(YOLO) model was trained as the network structure for the automatic location of LAA. Secondly, the optimal weight files were determined by the validation set and the bounding box of LAA was predicted. Finally, based on the correct location, the bounding box was magnified 1.5 times as the initial contour, and C-V(Chan-Vese) model was utilized to realize the automatic segmentation of LAA. The performance of automatic segmentation was evaluated by 5 metrics, including accuracy, sensitivity, specificity, positive, and negative. The experimental results show that the proposed method can achieve a good automatic segmentation in different resolutions and visual modes, small samples data achieve the optimal location performance at 1 000 iterations with a correct position rate of 72.25%, and C-V model can reach the accuracy of 98.09% based on the correct location. Therefore, deep learning is a rather promising technique in the automatic segmentation of LAA from ultrasound images, and it can provide a good initial contour for the segmentation algorithm based on contour.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=automatic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">automatic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=C-V(Chan-Vese)%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">C-V(Chan-Vese) model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Left%20Atrial%20Appendage(LAA)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Left Atrial Appendage(LAA);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ultrasound%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ultrasound image;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    HAN Luyi, born in 1995, M. S. candidate. His research interests include medical image processing. ;
                                </span>
                                <span>
                                    HUANG Yunzhi, born in 1989, Ph. D. candidate. Her research interests include medical signal processing, medical image processing.;
                                </span>
                                <span>
                                    DOU Haoran, born in 1995, M. S. candidate. His research interests include medical image analysis. ;
                                </span>
                                <span>
                                    BAI Wenjuan, born in 1982, Ph. D. candidate, associate senior doctor. Her research interests include dynamic evaluation of heart valve disease. ;
                                </span>
                                <span>
                                    LIU Qi, born in 1966, Ph. D., professor. His research interests include machine vision, medical image processing, medical information.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-07</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Project of Chengdu Science and Technology Bureau(2015-HM01-00525-SF);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">房颤患者心房附壁血栓脱落可以导致体循环血栓栓塞,其中脑卒中是最常见和最严重的血栓栓塞事件<citation id="169" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。最近研究显示左心耳(Left Atrial Appendage, LAA)封堵术可以有效预防房颤患者血栓栓塞<citation id="170" type="reference"><link href="133" rel="bibliography" /><link href="135" rel="bibliography" /><link href="137" rel="bibliography" /><link href="139" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>,封堵术成功的关键在于左心耳形状的准确选择。对左心耳图像准确的自动分割是自动计算封堵装置的形状参数以及判断房颤的指标的重要前提,具有重要的临床意义。在分析大量医学数据时,由医生进行人工分割LAA边界繁杂耗时且主观性强, 因此,需要设计重复性强、鲁棒性高的自动分割算法,以实现后续的自动计算并同时减轻医护人员的工作负担。为此,LAA超声图像自动分割模型,需克服两大难点:1)克服图像中其他目标的干扰,实现快速准确地自动定位;2)克服超声图像中的固有斑噪和LAA内部的梳状肌和肌小梁所造成的影响,实现准确的轮廓演化。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="44">目前,活动轮廓模型(Active Contour Model, ACM)已经广泛应用于图像自动分割的方案中,ACM的基本思想是通过演化目标轮廓曲线来求解其能量泛函的最小值。常见的基本形变模型有:1)主动外观模型(Active Appearance Model, AAM);2)主动形状模型(Active Shape Model,ASM);3)模板模型(Atlas Model,AM)。由Chan等<citation id="171" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出的C-V(Chan-Vese)模型是最受欢迎的模型之一,C-V模型在边界较弱的图像上有更好的表现,但不能很好地适用于图像强度不均匀的情况。Li等<citation id="172" type="reference"><link href="143" rel="bibliography" /><link href="145" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>提出由局部二值拟合(Local Binary Fitting,LBF)和局部扩展拟合(Region-Scalable Fitting,RSF)为能量驱动的方法。RSF在模型中引入了图像局部区域的强度信息,从而实现对强度不均匀的图像的有效分割。该类方法能够克服基于图像像素分类的分割算法的缺陷:1)鲁棒性不高;2)需要大量的人工参与以提高准确性。方法的计算量大,且如果初始轮廓选择不当,非凸的能量函数会导致模型陷入局部最小值,甚至分割失败。对于LAA图像,大部分研究采用一些基本几何形状作为初始轮廓,如果无法准确定位会极大地延长收敛时间,甚至收敛出错误的轮廓。因此,本文从提高分割精度和鲁棒性角度出发,要实现自动分割,其首要步骤就是从超声图像中快速、自动定位出LAA。</p>
                </div>
                <div class="p1">
                    <p id="45">基于特征工程的机器学习方法需要人工参与,设计有效良好的特征提取方案,以将原始数据转换成可区分的特征向量。但是人工参与的工程技能和专业知识在量化过程中会有一定主观差异,对于分类结果会有一定的影响。基于卷积神经网络(Convolutional Neural Network,CNN)<citation id="173" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>的深度学习方法可以利用原始图像数据作为输入,通过组合每一层简单的非线性模块,就可以将输入转换成高层的表达输出,且整个表达学习的过程几乎不需要人工参与。在目标检测任务上,深度学习方法主要分为两类:1)基于选择性搜索的区域卷积神经网络(Region-based Convolutional Neural Network,R-CNN)<citation id="176" type="reference"><link href="149" rel="bibliography" /><link href="151" rel="bibliography" /><link href="153" rel="bibliography" /><link href="155" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>;2)实现端到端检测的YOLO(You Only Look Once)<citation id="177" type="reference"><link href="157" rel="bibliography" /><link href="159" rel="bibliography" /><link href="161" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>。基于R-CNN的方法利用聚类方式,对图像进行分割分组,得到多个候选框的层次组。R-CNN计算的候选框数量多且有大量重叠,冗余计算量非常大,为提高运行速度,Fast R-CNN<citation id="174" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>方案中候选框只经过一个CNN,Faster R-CNN<citation id="175" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>则直接利用CNN来计算候选框。然而,基于R-CNN的方案,复杂的流程通常导致计算速度慢且很难实现优化。而YOLO把物体框的选择与识别进行了结合,一步输出,识别速度非常快,达到每秒45～150帧。</p>
                </div>
                <div class="p1">
                    <p id="46">目前针对LAA轮廓提取的研究和文献非常少,现有的LAA定量分析软件是以左室的形态结构作为模型的。为减小所选封堵装置的误差,提高自动分割的运算效率和准确性,本文提出了一种基于深度学习网络和参数活动轮廓模型的全自动LAA提取方法,整个流程如图1所示。首先,利用深度学习网络YOLO训练LAA超声图像得到网络的权值参数,完成LAA的准确自动定位;进而,在只含有LAA单一目标的有限范围内,利用改进的C-V模型<citation id="178" type="reference"><link href="163" rel="bibliography" /><link href="165" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>完成LAA的自动分割。</p>
                </div>
                <div class="area_img" id="47">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911043_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 左心耳超声图像自动分割的算法流程" src="Detail/GetImg?filename=images/JSJY201911043_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 左心耳超声图像自动分割的算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911043_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flowchart of automatic segmentation algorithm for LAA from ultrasound images</p>

                </div>
                <h3 id="48" name="48" class="anchor-tag">2 左心耳自动分割方法</h3>
                <h4 class="anchor-tag" id="49" name="49">2.1 <b>基于</b>YOLO<b>模型的自动定位</b></h4>
                <div class="p1">
                    <p id="50">通常,完成一个心动周期所需要的时间为0.8 s, 由于人眼的视觉暂留效应,当帧率高于每秒24帧时,就认为是连贯的,因此为满足实时定位并识别超声图像中的LAA,采用YOLO的网络架构,即使不采用批处理,利用GPU加速模式,对于物体识别和定位的速度也可以达到每秒45～150帧<citation id="179" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>,能够满足对LAA的处理速度。基于YOLO网络的LAA自动定位架构如图2所示,自动定位架构主要分为训练和测试两大模块:训练时使用由医生标记的训练集对YOLO模型的权值参数进行更新;测试时固定权值参数,对测试集图像进行预测,输出LAA区域的位置估计与类别回归得到的包围盒。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911043_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于YOLO网络的LAA自动定位框架图" src="Detail/GetImg?filename=images/JSJY201911043_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于YOLO网络的LAA自动定位框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911043_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Flowchart of automatic LAA location based on YOLO network</p>

                </div>
                <div class="p1">
                    <p id="52">图2中所使用的YOLO网络的具体结构及参数设置如图3所示,整个卷积神经网络包含了24个卷积层,并连接了2个全连接层。卷积层通过局部连接对图像局部特征进行提取;池化层降低图像尺度同时增加了卷积层的感受野范围;全连接层则整合了全局信息,能够更好地预测LAA区域。YOLO网络的具体参数设置如下:</p>
                </div>
                <div class="p1">
                    <p id="53">1)网络输入缩放成统一大小,且输入图像矩阵分割成<i>s</i>×<i>s</i> 的单元格,通常将<i>s</i>设为7。</p>
                </div>
                <div class="p1">
                    <p id="54">2)每个单元格负责输出<i>b</i>个矩形框,其中共包含两部分信息:①数组(<i>x</i>, <i>y</i>, <i>w</i>, <i>h</i>)表示其位置信息,<i>x</i>、 <i>y</i>表示中心相对于单元格左上角的位置偏移,记单元格左上角为(0, 0),右下角为(1, 1),<i>w</i>、 <i>h</i>表示矩形框的宽与高,都是相对于整个图片的相对值,全幅图片大小为(1, 1)。②概率<i>P</i>(<i>object</i>)表示该框是物体的概率。</p>
                </div>
                <div class="p1">
                    <p id="55">3)每个单元格再负责输出<i>c</i>个类别的概率,用概率<i>P</i>(<i>class</i>|<i>object</i>)表示,最终输出时物体的概率乘以类别概率,才是整体识别到是一个物体的概率,即:</p>
                </div>
                <div class="p1">
                    <p id="56"><i>P</i>(<i>class</i>)=<i>P</i>(<i>class</i>|<i>object</i>)×<i>P</i>(<i>object</i>)</p>
                </div>
                <div class="p1">
                    <p id="57">如果一个物体的中心落入一个单元格,则该单元格上的<i>b</i>个矩形框的位置信息都为该物体的位置信息,<i>c</i>个概率中对应该物体类别值为1,其他为0。由于训练的数据集分为左心耳和非左心耳两类,所以将<i>b</i>、<i>c</i>均设为2。</p>
                </div>
                <div class="p1">
                    <p id="58">4)最终输出层共包含有<i>s</i>×<i>s</i>×(<i>b</i>×5+<i>c</i>)个单元。网络每一层之间的连接,类比神经元的稀疏激活性,输入信号的稀疏特性使得学习网络并不需要很强的线性可分机制,因此在网络训练时,整个网络除了最后一层为线性激活,其他层都采用如式(1)的泄露型线性矫正激活方式<citation id="180" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Φ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>x</mi><mo>,</mo></mtd><mtd columnalign="left"><mi>x</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>.</mo><mn>1</mn><mi>x</mi><mo>,</mo></mtd><mtd columnalign="left"><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911043_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 YOLO网络的结构及参数设置图" src="Detail/GetImg?filename=images/JSJY201911043_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 YOLO网络的结构及参数设置图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911043_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structure and parameter settings of YOLO network</p>

                </div>
                <div class="p1">
                    <p id="61">YOLO每一个单元格能够预测多个包围盒,在训练时,每一个物体只需要一个包围盒,因此,根据当前最高的交并比(Intersection over Union,IoU),指定一个预测的包围盒对应待检测物体。但是,输入图片的大部分单元格中并没有落入物体中心,为增强整个网络的稳定性,引入参数<i>λ</i><sub>coord</sub>和<i>λ</i><sub>noobj</sub>,对类别概率和最小包围盒的误差进行分析。训练过程中,为使包围盒的预测偏差受目标大小的影响较小,最终优化的损失函数如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><mo>=</mo><mi>λ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>o</mtext><mtext>r</mtext><mtext>d</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>s</mi><mo>×</mo><mi>s</mi></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><mo stretchy="false">[</mo></mstyle></mrow></mstyle><mi>Τ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>o</mtext><mtext>r</mtext><mtext>d</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>s</mi><mo>×</mo><mi>s</mi></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><mo stretchy="false">[</mo></mstyle></mrow></mstyle><mi>Τ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mo stretchy="false">(</mo><msqrt><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msqrt><mo>-</mo><msqrt><mrow><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></msqrt><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><msqrt><mrow><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msqrt><mo>-</mo><msqrt><mrow><mover accent="true"><mi>h</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></msqrt><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>s</mi><mo>×</mo><mi>s</mi></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><mi>Τ</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>s</mi><mo>×</mo><mi>s</mi></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><mi>Τ</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>s</mi><mo>×</mo><mi>s</mi></mrow></munderover><mi>Τ</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi></mrow></munder><mo stretchy="false">(</mo></mstyle><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo>-</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中:<i>T</i><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow></math></mathml>表示是否出现在单元格<i>i</i>中,<i>T</i><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow></math></mathml>表示使用第<i>i</i>个单元格中第<i>j</i>个类别的预测器,数组(<i>x</i><sub><i>i</i></sub>,<i>y</i><sub><i>i</i></sub>,<i>w</i><sub><i>i</i></sub>,<i>h</i><sub><i>i</i></sub>)和(<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>,<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>,<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>w</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>,<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>h</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>)分别表示目标在第<i>i</i>个单元格中的筛选框坐标及其估计。针对同一个物体可能识别出来多个选框的情况,YOLO采用非极大值抑制(Non-Maximum Suppression,NMS)去掉重复框。根据每一个框对应的置信率,消除多余框的具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="64">1)按置信率排序由高到低排序;</p>
                </div>
                <div class="p1">
                    <p id="65">2)取最大置信率的框为物体框;</p>
                </div>
                <div class="p1">
                    <p id="66">3)剩余多个框中,去掉与最大置信率框的重叠率大于特定阈值的框;</p>
                </div>
                <div class="p1">
                    <p id="67">4)重复步骤3),直到没有剩余框;</p>
                </div>
                <div class="p1">
                    <p id="68">5)确定最终被标记为物体的框。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">2.2 <b>基于</b>C-V<b>模型的自动分割</b></h4>
                <div class="p1">
                    <p id="70">由于YOLO学习网络能够输出只包含单一目标——LAA的最紧包围盒,非左心耳区域所占比例较小,因此利用C-V模型实现左心耳腔和其余组织的分离。作为几何形变模型,C-V模型并没有用到图像的梯度信息,而是利用了目标和背景的灰度差异,因此,C-V模型的一个优势是能够分割出边界梯度变化不明显的结构,并且能够实现多目标分割,能够满足本实验中不同显示模态和不同扫描条件下LAA的自动分割。C-V模型的能量方程定义如式(3)所示:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo>=</mo><mi>μ</mi><mo>⋅</mo><mi>L</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo><mo>+</mo><mi>υ</mi><mo>⋅</mo><mi>A</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mstyle displaystyle="true"><mrow><mo>∫</mo><mrow></mrow></mrow></mstyle></mrow><msub><mrow></mrow><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo>|</mo><mrow><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo><mi>l</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext>d</mtext><mi>x</mi><mtext>d</mtext><mi>y</mi><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mstyle displaystyle="true"><mrow><mo>∫</mo><mrow></mrow></mrow></mstyle></mrow><msub><mrow></mrow><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>s</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo>|</mo><mrow><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo><mi>l</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext>d</mtext><mi>x</mi><mtext>d</mtext><mi>y</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中:<i>L</i>表示演化轮廓,<i>I</i>表示图像灰度,<i>μ</i>和<i>υ</i>为相应项的非负系数,<i>λ</i><sub>1</sub>和<i>λ</i><sub>2</sub>为相应项的正系数,<i>l</i><sub>1</sub>和<i>l</i><sub>2</sub>分别表示演化轮廓内部和外部所有像素的平均灰度。前两项为演化轮廓的内部能量,用以正则化轮廓的几何特性;后两项为演化轮廓的外部能量,用以将轮廓收敛到正确的位置。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">3 实验结果和分析</h3>
                <h4 class="anchor-tag" id="74" name="74">3.1 <b>实验数据和实验平台</b></h4>
                <h4 class="anchor-tag" id="75" name="75">1)实验数据。</h4>
                <div class="p1">
                    <p id="76">所有的LAA超声图片都是由四川大学华西医院超声心内科提供,医生利用Phillip iE33超声诊断仪并配合经食道的探头X7-2t采获。数据来自不同的患者,一共512例,每一例中有1张LAA图像由医生同时标注包围框和LAA轮廓。其中测试数据集包含192例,训练数据集320例,并从训练集中随机选择192例作为验证集。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">2)实验平台。</h4>
                <div class="p1">
                    <p id="78">硬件环境:Intel Xeon E5-2630 CPU 2.40 GHz,8 GB内存,NIVIDIA GeForce GTX 1080Ti显卡。软件环境:权值参数训练在UBUNTU 14.0.4系统,Python 2.7环境下进行;自动定位和自动分割实验在Windows 10系统,Matlab 2015a环境下进行。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">3.2 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="80">在数据采集阶段,共由3名医生完成对超声图像LAA区域的标记,标记结果为LAA在诊断图像中的最紧包围盒。在训练数据阶段,从采集数据中随机抽取512张,其中320张用于训练YOLO网络的权值,剩余192张作为该训练网络的测试。为保证整个网络架构的迭代收敛,网络参数设置如下:</p>
                </div>
                <div class="p1">
                    <p id="81">1)统一图像输入尺寸为418×418;</p>
                </div>
                <div class="p1">
                    <p id="82">2)由于本次的数据集有限,为了更好地引导网络朝向极值所在的方向学习,因此采用全数据集的形式输入;</p>
                </div>
                <div class="p1">
                    <p id="83">3)为保证学习速度和学习的准确率,最终能够找到极值,YOLO网络的训练使用了自适应动量估计(Adaptive moment estimation,Adam)优化方法,学习率设置为0.001,批处理个数设置为8。对于式(2)所示的损失函数,<i>λ</i><sub>coord</sub>设置为5,<i>λ</i><sub>noobj</sub>设置为0.5。</p>
                </div>
                <div class="p1">
                    <p id="84">训练阶段结束后,导出网络的权重文件,并随机抽取采集样本数据输入到训练网络测试整个学习网络性能。自动定位的输出结果为包围LAA的最小包围盒,其中,定位结果只显示置性度高于0.5的包围盒。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">3.3 <b>定位结果</b></h4>
                <div class="p1">
                    <p id="86">得益于多层卷积网络,和传统的基于灰度信息和机器学习分类的方法相比,基于深度学习的自动定位从图像整体角度出发,从微观到宏观地分析图像中的像素、边、物体,因此能够克服传统超声图像固有的低分辨率缺陷。图4展示了学习网络在不同扫描条件下的定位结果,其中:图4(a)为LAA处于舒张末期,虽然目标明确,但扫描结果分辨率较低情况下自动定位的结果;图4(b)为目标占整幅图像比例较小,但扫描结果分辨率较高情况下的结果;图4(c)为目标清晰,但为双帧显示模式下的定位结果;图4(d)为分辨率中等水平、LAA处于收缩末期时边界不明显情况下的定位结果。因此,利用YOLO网络,可以完成LAA在不同显示模式、不同扫描条件下的自动定位。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911043_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同扫描条件下的自动定位结果" src="Detail/GetImg?filename=images/JSJY201911043_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同扫描条件下的自动定位结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911043_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Automatic location results under different scanning conditions</p>

                </div>
                <h4 class="anchor-tag" id="88" name="88">3.4 <b>定位误差</b></h4>
                <div class="p1">
                    <p id="89">为确定学习网络的最优迭代次数,提取出训练阶段中在不同迭代次数下得到的权重文件进行测试,并将自动定位的结果归为以下4类:1)遗漏、2)错误、3)部分重合、4)完全重合。其中:遗漏是指LAA区域并未出现包围盒;错误是指包围盒出现在非左心耳区域;部分重合是指包围盒未包含完全左心耳,但至少包含了90%的左心耳区域;完全重合是指包围盒和医生标记的完全重合。遗漏和错误归为是偏差(NEGATIVE)定位,而部分重合和重合认为是正确(POSTIVE)的定位。本次实验利用192例验证集样本,对1 000 ～ 7 000次迭代得到的权值进行验证,表1和图5显示了以上4类的统计结果。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911043_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同迭代次数下正确和偏差样本数的比例" src="Detail/GetImg?filename=images/JSJY201911043_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同迭代次数下正确和偏差样本数的比例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911043_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 POSTIVE and NEGATIVE distribution with different iterations</p>

                </div>
                <div class="p1">
                    <p id="91">若仅从完全重合率来看,在5 000次迭代时重叠的样本数是最多的,但相应的错误率也较高,因此,从整体正确定位率可以看出,由于样本数量有限,在1 000次迭代时,自动定位的效果最好。并且从图5中可以看出,不同迭代次数下,统计结果差异变化较小,因此,1 000次迭代后,整个网络都处于一个稳定收敛的状态。值得指出的是,本次实验中正确自动定位的概率不是很高,主要归结为以下3方面原因:1)医学数据的训练样本数有限;2)左心耳形状的个体差异性;3)左心耳具有多样化的病理形态,而训练样本中只包括了一部分病理形态。</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表</b>1 <b>不同迭代次数下自动定位结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Automatic location results with different iterations</p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td><br />迭代次数</td><td>遗漏率</td><td>错误率</td><td>部分重合率</td><td>完全重合率</td></tr><tr><td><br />1 000</td><td>0.265 3</td><td>0.012 2</td><td>0.342 9</td><td>0.379 6</td></tr><tr><td><br />2 000</td><td>0.269 7</td><td>0.059 9</td><td>0.273 4</td><td>0.397 0</td></tr><tr><td><br />3 000</td><td>0.196 2</td><td>0.111 5</td><td>0.330 8</td><td>0.361 5</td></tr><tr><td><br />4 000</td><td>0.181 8</td><td>0.146 2</td><td>0.229 2</td><td>0.442 7</td></tr><tr><td><br />5 000</td><td>0.152 0</td><td>0.148 0</td><td>0.184 0</td><td>0.516 0</td></tr><tr><td><br />6 000</td><td>0.224 5</td><td>0.089 8</td><td>0.334 7</td><td>0.351 0</td></tr><tr><td><br />7 000</td><td>0.219 1</td><td>0.111 6</td><td>0.262 9</td><td>0.410 4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">3.5 <b>分割精度</b></h4>
                <div class="p1">
                    <p id="94">针对192例测试样本中完成正确定位的数据,利用C-V模型的自动分割结果如图6所示。其中,图6(a)为在目标定位结果与实际位置完全重合的情况下得到的分割结果,图6(b)则为在目标定位结果与实际位置部分重合的情况下得到的分割结果。为保证部分重合能够包含所有的区域,在自动分割之前,将包围盒自动扩增1.5倍。可以看出,正确定位后的数据均能够得到较好的分割结果。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911043_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 C-V模型的自动分割结果" src="Detail/GetImg?filename=images/JSJY201911043_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 C-V模型的自动分割结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911043_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Automatic segmentation results of C-V model</p>

                </div>
                <div class="p1">
                    <p id="96">将医生勾勒的轮廓作为“金标准”,采用5个指标:准确性(Accuracy)、敏感性(Sensitivity)、特异性(Specificity)、阳性预测值(Positive)、阴性预测值(Negative)<citation id="181" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>为指标,评价基于YOLO网络自动定位的左心耳自动分割方案:</p>
                </div>
                <div class="p1">
                    <p id="97"><i>Accuracy</i>=(<i>TP</i>+<i>TN</i>)/(<i>TP</i>+<i>TN</i>+<i>FP</i>+<i>FN</i>)</p>
                </div>
                <div class="p1">
                    <p id="98"><i>Sensitivity</i>=<i>TP</i>/(<i>TP</i>+<i>FN</i>)</p>
                </div>
                <div class="p1">
                    <p id="99"><i>Specificity</i>=<i>TN</i>/(<i>TN</i>+<i>FP</i>)</p>
                </div>
                <div class="p1">
                    <p id="100"><i>Positive</i>=<i>TP</i>/(<i>TP</i>+<i>TN</i>)</p>
                </div>
                <div class="p1">
                    <p id="101"><i>Negtive</i>=<i>FP</i>/(<i>FP</i>+<i>FN</i>)</p>
                </div>
                <div class="p1">
                    <p id="102">从概率上讲,<i>TP</i>(True Positive)表示正样例分类成正样例;<i>TN</i>(True Negative)表示负样例分类成负样例;<i>FP</i>(False Positive)表示负样例分类成正样例;<i>FN</i>(False Negative)表示正样例分类成负样例。</p>
                </div>
                <div class="p1">
                    <p id="103">针对1 000次迭代权重下,192例测试样本中正确定位的数据(共计138例),表2在各项指标下对比了C-V、LBF和RSF三种模型的分割效果。可以看出,在YOLO网络提供自动定位的初始轮廓的情况下,三种模型的分割准确率都非常高,且各项指标均表现优秀。表3中所示为三种模型的CPU耗时比较,C-V模型耗时最少。因此,在三种模型精确度相近的情况下,C-V模型更适用于左心耳分割任务。但是,当左心耳接近收缩末期时,整个分割结果会受到其内部的梳状肌和肌小梁影响,因而仍会导致漏检。在未来的工作中,将会为进一步提高全自动分割的准确性,结合其他技术和信息,对左心耳超声图像进行分析。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表</b>2 C-V<b>模型</b>、LBF<b>模型</b>、RSF<b>模型自动分割结果评价指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Evaluation indicator for segmentation results of C-V model, LBF model and RSF model</p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td><br />评价指标</td><td>C-V</td><td>LBF</td><td>RSF</td></tr><tr><td><br />准确性</td><td>0.980 9±0.034 9</td><td>0.976 8±0.048 4</td><td>0.980 0±0.041 2</td></tr><tr><td><br />敏感性</td><td>0.906 5±0.066 3</td><td>0.898 3±0.079 2</td><td>0.904 8±0.075 2</td></tr><tr><td><br />特异性</td><td>0.965 6±0.128 1</td><td>0.964 4±0.167 7</td><td>0.959 2±0.159 5</td></tr><tr><td><br />阳性预测值</td><td>0.983 0±0.033 7</td><td>0.979 2±0.041 3</td><td>0.970 2±0.052 9</td></tr><tr><td><br />阴性预测值</td><td>0.907 1±0.074 3</td><td>0.900 5±0.068 2</td><td>0.907 7±0.073 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="105">
                                            <p class="img_tit">
                                                <b>表</b>3 C-V<b>模型</b>、LBF<b>模型</b>、RSF<b>模型</b>CPU<b>耗时比较</b>
                                                    <br />
                                                Tab. 3 CPU time for C-V model, LBF model and RSF model
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911043_10500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201911043_10500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911043_10500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 C-V模型、LBF模型、RSF模型CPU耗时比较" src="Detail/GetImg?filename=images/JSJY201911043_10500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="106" name="106" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="107">尽管超声设备目前的应用广泛,但是超声图像的分割一直受限于图像本身的质量,虽然近年来已有相关的硬件改善,但是影响成像质量仍受病人状态、医生经验等多种因素制约。相较于目前大量的关于超声图像自动分割的研究,几乎都是基于图像本身的灰度、梯度、相位信息,或是解剖结构的先验信息构建出基于边界或是区域的能量模型,通过求解得到使能量最小的值来完成分割工作,但是模型受限于目标区域的局部特征,并且需要经过较多的预处理步骤才能在特定条件的图像上取得较为满意的效果。</p>
                </div>
                <div class="p1">
                    <p id="108">为尽量克服图像质量和个体差异带来的自动分割的困难,并提高从超声图像中全自动分割出左心耳的准确性,基于YOLO模型自动定位的自动分割方法主要的创新如下:</p>
                </div>
                <div class="p1">
                    <p id="109">1)依托于深度学习的理念,应用CNN结构,根据YOLO搭建出的学习网络,实现了左心耳超声图像中目标的自动定位。基于YOLO模型的自动定位架构能够较好地克服超声成像固有的低分辨率、斑点噪声的干扰,成功定位出心动周期内各种状态下的左心耳,同时,能够完成不同分辨率条件和不同显示模式结果下的自动定位。</p>
                </div>
                <div class="p1">
                    <p id="110">2)在定位准确的基础上,左心耳已经被锁定包含单一目标的矩形框内,利用C-V模型能够较为准确分割出目标。</p>
                </div>
                <div class="p1">
                    <p id="111">由于医学样本量通常受限于患者意愿,因此测试数据量相对较小,在以后的工作中会进一步收集样本,扩大训练和测试样本数量,进一步使得自动定位和分割的结果具有更强的鲁棒性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="131">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Left atrial appendage: structure, function, and role in thromboembolism">

                                <b>[1]</b> AL-SAADY N M,OBEL O A,CAMM A J.Left atrial appendage:structure,function,and role in thrombo-embolism[J].Heart,1999,82(5):547-554.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Left atrial appendage closure for thrombo-embolism prevention in patients with atrial fibrillation:advances and perspectives">

                                <b>[2]</b> KONG B,LIU Y,HUANG H,et al.Left atrial appendage closure for thrombo-embolism prevention in patients with atrial fibrillation:advances and perspectives[J].Journal of Thoracic Disease.2015,7(2):199-203.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SBAD&amp;filename=SBAD120619000029&amp;v=MTYzNDR6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3N05KVnNYTmkvS2FySzZIdGZOcG85RlpPc05CUk04&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> LEAL S,MORENO R,de SOUSA ALMEIDA M,et al.Evidence-based percutaneous closure of the left atrial appendage in patients with atrial fibrillation[J].Current Cardiology Reviews.2012,8(1):37-42.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWK&amp;filename=SJWK12103000037737&amp;v=MDY1MzRLNkg5SFByNDlGWk9nSUMzOCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJVnNSYnhFPU5pZmNaYg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> WHITLOCK R P,HEALEY J S,CONNOLLY S J.Left atrial appendage occlusion does not eliminate the need for warfarin [J].Circulation,2009,120(19):1927-1932.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Techniques to improve left atrial appendage imaging">

                                <b>[5]</b> ABDELMONEIM S S,MULVAGH S L.Techniques to improve left atrial appendage imaging[J].Journal of Atrial Fibrillation,2014,7(1):No.1059.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active contours without edges">

                                <b>[6]</b> CHAN T F,VESE L A.Active contours without edges[J].IEEE Transactions on Image Processing,2001,10(2):266-277.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Implicit Active Contours Driven by Local BinaryFitting Energy">

                                <b>[7]</b> LI C,KAO C Y,GORE J C,et al.Implicit active contours driven by local binary fitting energy[C]// Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2007:1-7.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Minimization of Region-Scalable Fitting Energy for Image Segmentation">

                                <b>[8]</b> LI C,KAO C Y,GORE J C,et al.Minimization of region-scalable fitting energy for image segmentation[J].IEEE Transactions on Image Processing,2008,17(10):1940-1949.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 LeCUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[10]</b> GIRSHICK R,DONAHUE J,DARRELL T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2014:580-587.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[11]</b> GIRSHICK R.Fast R-CNN[EB/OL].[2019- 04- 20].https://arxiv.org/pdf/1504.08083.pdf.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 REN S,HE K,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Region-based segmentation and objectdetection">

                                <b>[13]</b> GOULD S,GAO T,KOLLER D.Region-based segmentation and object detection[C]// Advances in Neural Information Processing Systems 22.Cambridge,MA:MIT Press,2009:655-663.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:unified,real-time object detection">

                                <b>[14]</b> REDMON J,DIVVALA S,GIRSHICK R,et al.You only look once:unified,real-time object detection[EB/OL].[2019- 04- 20].https://arxiv.org/pdf/1506.02640.pdf.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 REDMON J,FARHADI A.YOLO9000:better,faster,stronger[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6517-6525.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An Incremental Improvement[C/OL]">

                                <b>[16]</b> REDMON J,FARHADI A.YOLOv3:an incremental improvement[EB/OL].[2018- 05- 25].https://arxiv.org/pdf/1804.02767.pdf.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segmentation and shape tracking of whole fluorescent cells based on the chan-vese model">

                                <b>[17]</b> MAŠKA M,DANĔK O,GARASA S,et al.Segmentation and shape tracking of whole fluorescent cells based on the Chan-Vese model[J].IEEE Transactions on Medical Imaging,2013,32(6):995-1006.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated CT liver segmentation using improved Chan-Vese model with global shape constrained energy">

                                <b>[18]</b> WANG X,ZHENG C,LI C.Automated CT liver segmentation using improved Chan-Vese model with global shape constrained energy[C]// Proceedings of the 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society.Piscataway:IEEE,2011:3415-3418.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rectifier nonlinearities improve neural network acoustic models">

                                <b>[19]</b> MAAS A L,HANNUN A Y,NG A Y,et al.Rectifier nonlinearities improve neural network acoustic models[EB/OL].[2018- 05- 28].http://robotics.stanford.edu/～amaas/papers/relu_hybrid_icml2013_final.pdf
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911043" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911043&amp;v=MjEyMjU3cWZadVpzRnluZ1ZycklMejdCZDdHNEg5ak5ybzlCWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
