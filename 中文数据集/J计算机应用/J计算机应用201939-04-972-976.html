<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775414033750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904007%26RESULT%3d1%26SIGN%3dMxQI%252f1Nmfss5MsjG8J2CjZSZ4LE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904007&amp;v=MjQ4NzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjczT0x6N0JkN0c0SDlqTXE0OUZZNFFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="1 CLSM ">1 CLSM</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="2 基于CLSM的改进模型 ">2 基于CLSM的改进模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="3.1 &lt;b&gt;实验数据和评测指标&lt;/b&gt;">3.1 <b>实验数据和评测指标</b></a></li>
                                                <li><a href="#98" data-title="3.2 &lt;b&gt;实验设计&lt;/b&gt;">3.2 <b>实验设计</b></a></li>
                                                <li><a href="#106" data-title="3.3 &lt;b&gt;结果分析&lt;/b&gt;">3.3 <b>结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#114" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="图1 CLSM结构">图1 CLSM结构</a></li>
                                                <li><a href="#64" data-title="图2 实体关注层结构">图2 实体关注层结构</a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;问答对数据示例&lt;/b&gt;"><b>表</b>1 <b>问答对数据示例</b></a></li>
                                                <li><a href="#89" data-title="图3 语义匹配网络模型">图3 语义匹配网络模型</a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;实体类型示例&lt;/b&gt;"><b>表</b>2 <b>实体类型示例</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;与传统语义模型的对比实验结果&lt;/b&gt; %"><b>表</b>3 <b>与传统语义模型的对比实验结果</b> %</a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;与传统翻译模型的对比实验结果&lt;/b&gt; %"><b>表</b>4 <b>与传统翻译模型的对比实验结果</b> %</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;与深度神经网络模型的对比实验结果&lt;/b&gt; %"><b>表</b>5 <b>与深度神经网络模型的对比实验结果</b> %</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="133">


                                    <a id="bibliography_1" title=" AHN D D, JIJKOUN V, MISHNE G A, et al. Using Wikipedia at the TREC QA track[EB/OL]. [2018- 05- 10]. http://staff.science.uva.nl/～mdr/Publications/Files/uams-trec-2004-final-qa.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using Wikipedia at the TREC QA track">
                                        <b>[1]</b>
                                         AHN D D, JIJKOUN V, MISHNE G A, et al. Using Wikipedia at the TREC QA track[EB/OL]. [2018- 05- 10]. http://staff.science.uva.nl/～mdr/Publications/Files/uams-trec-2004-final-qa.pdf.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_2" title=" 汤庸, 林鹭贤, 罗烨敏, 等. 基于自动问答系统的信息检索技术研究进展[J]. 计算机应用, 2008, 28 (11) : 2745-2748. (TANG Y, LIN L X, LUO Y M, et al. Survey on information retrieval system based on question answering system[J]. Journal of Computer Applications, 2008, 28 (11) : 2745-2748.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200811005&amp;v=MDEzNzc3M09MejdCZDdHNEh0bk5ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         汤庸, 林鹭贤, 罗烨敏, 等. 基于自动问答系统的信息检索技术研究进展[J]. 计算机应用, 2008, 28 (11) : 2745-2748. (TANG Y, LIN L X, LUO Y M, et al. Survey on information retrieval system based on question answering system[J]. Journal of Computer Applications, 2008, 28 (11) : 2745-2748.) 
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_3" title=" GAO J F, HE X D, YIH W T, et al. Learning continuous phrase representations for translation modeling [EB/OL]. [2018- 05- 10]. http://www.aclweb.org/anthology/P14-1066." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning continuous phrase representations for translation modeling">
                                        <b>[3]</b>
                                         GAO J F, HE X D, YIH W T, et al. Learning continuous phrase representations for translation modeling [EB/OL]. [2018- 05- 10]. http://www.aclweb.org/anthology/P14-1066.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_4" title=" HUANG P S, HE X D, GAO J F, et al. Learning deep structured semantic models for Web search using clickthrough data[C]// CIKM 2013: Proceedings of the 22nd ACM International Conference on Information &amp;amp; Knowledge Management. New York: ACM, 2013: 2333-2338." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep structured semantic models for web search using clickthrough data">
                                        <b>[4]</b>
                                         HUANG P S, HE X D, GAO J F, et al. Learning deep structured semantic models for Web search using clickthrough data[C]// CIKM 2013: Proceedings of the 22nd ACM International Conference on Information &amp;amp; Knowledge Management. New York: ACM, 2013: 2333-2338.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_5" title=" SHEN Y, HE X D, GAO J F, et al. A latent semantic model with convolutional-pooling structure for information retrieval[C]// CIKM 2014: Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, New York: ACM, 2014: 101-110." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval">
                                        <b>[5]</b>
                                         SHEN Y, HE X D, GAO J F, et al. A latent semantic model with convolutional-pooling structure for information retrieval[C]// CIKM 2014: Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, New York: ACM, 2014: 101-110.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_6" title=" HE X, GAO J, DENG L, et al. Convolutional latent semantic models and their applications: US 9477654B2[P]. 2015- 10- 01." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional latent semantic models and their applications">
                                        <b>[6]</b>
                                         HE X, GAO J, DENG L, et al. Convolutional latent semantic models and their applications: US 9477654B2[P]. 2015- 10- 01.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_7" title=" GAO J, PANTEL P, GAMON M, et al. Modeling interestingness with deep neural networks[EB/OL]. [2018- 05- 10]. http://www.aclweb.org/anthology/D14-1002." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling interestingness with deep neural networks">
                                        <b>[7]</b>
                                         GAO J, PANTEL P, GAMON M, et al. Modeling interestingness with deep neural networks[EB/OL]. [2018- 05- 10]. http://www.aclweb.org/anthology/D14-1002.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_8" title=" BELLOS C C, PAPADOPOULOS A, ROSSO R, et al. Identification of COPD patients&#39; health status using an intelligent system in the CHRONIOUS wearable platform[J]. IEEE Journal of Biomedical and Health Informatics, 2014, 18 (3) : 731-738." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identication of COPD patients&amp;#39;&amp;#39;health status using an intelligent system in the CHRONIOUS wearable platform">
                                        <b>[8]</b>
                                         BELLOS C C, PAPADOPOULOS A, ROSSO R, et al. Identification of COPD patients&#39; health status using an intelligent system in the CHRONIOUS wearable platform[J]. IEEE Journal of Biomedical and Health Informatics, 2014, 18 (3) : 731-738.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_9" title=" BUSA-FEKETE R, SZARVAS G, &#201;LTETHO T, et al. An apple-to-apple comparison of Learning-to-rank algorithms in terms of normalized discounted cumulative gain[C]// ECAI 2012: Proceedings of the 20th European Conference on Artificial Intelligence. Montpellier, France: IOS Press, 2012:16." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An apple-to-apple comparison of Learning-to-rank algorithms in terms of normalized discounted cumulative gain">
                                        <b>[9]</b>
                                         BUSA-FEKETE R, SZARVAS G, &#201;LTETHO T, et al. An apple-to-apple comparison of Learning-to-rank algorithms in terms of normalized discounted cumulative gain[C]// ECAI 2012: Proceedings of the 20th European Conference on Artificial Intelligence. Montpellier, France: IOS Press, 2012:16.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_10" title=" GAO J F, TOUTANOVA K, YIH W T. Clickthrough-based latent semantic models for Web search[C]// SIGIR 2011: Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval. New York: ACM, 2011: 675-684." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clickthrough-based latent semantic models for web search">
                                        <b>[10]</b>
                                         GAO J F, TOUTANOVA K, YIH W T. Clickthrough-based latent semantic models for Web search[C]// SIGIR 2011: Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval. New York: ACM, 2011: 675-684.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_11" title=" 徐佳俊, 杨飏, 姚天昉, 等. 基于LDA模型的论坛热点话题识别和追踪[J]. 中文信息学报, 2016, 30 (1) : 43-49. (XU J J, YANG Y, YAO T F, et al. LDA based hot topic detection and tracking for the forum[J]. Journal of Chinese Information Processing, 2016, 30 (1) : 43-49.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS201601007&amp;v=MDI4ODc0SDlmTXJvOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjczT0tDallmYkc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         徐佳俊, 杨飏, 姚天昉, 等. 基于LDA模型的论坛热点话题识别和追踪[J]. 中文信息学报, 2016, 30 (1) : 43-49. (XU J J, YANG Y, YAO T F, et al. LDA based hot topic detection and tracking for the forum[J]. Journal of Chinese Information Processing, 2016, 30 (1) : 43-49.) 
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_12" title=" LU Z D, LI H. A deep architecture for matching short texts[EB/OL]. [2018- 05- 10]. http://papers.nips.cc/paper/5019-a-deep-architecture-for-matching-short-texts.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A deep architecture for matching short texts">
                                        <b>[12]</b>
                                         LU Z D, LI H. A deep architecture for matching short texts[EB/OL]. [2018- 05- 10]. http://papers.nips.cc/paper/5019-a-deep-architecture-for-matching-short-texts.pdf.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_13" title=" GAO J, HE X, NIE J Y. Clickthrough-based translation models for Web search: from word models to phrase models[C]// CIKM 2010: Proceedings of the 19th ACM International Conference on Information and Knowledge Management. New York: ACM, 2010: 1139-1148." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clickthrough-based translation models for Web search: from word models to phrase models">
                                        <b>[13]</b>
                                         GAO J, HE X, NIE J Y. Clickthrough-based translation models for Web search: from word models to phrase models[C]// CIKM 2010: Proceedings of the 19th ACM International Conference on Information and Knowledge Management. New York: ACM, 2010: 1139-1148.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_14" title=" 刘红光, 魏小敏. Bag of Words算法框架的研究[J]. 舰船电子工程, 2011, 31 (9) : 125-128. (LIU H G, WEI X M. Research on frame of bag of words algorithm[J]. Ship Electronic Engineering, 2011, 31 (9) : 125-128.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCGC201109036&amp;v=MTI0MjRiYkc0SDlETXBvOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjczT0x5N00=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         刘红光, 魏小敏. Bag of Words算法框架的研究[J]. 舰船电子工程, 2011, 31 (9) : 125-128. (LIU H G, WEI X M. Research on frame of bag of words algorithm[J]. Ship Electronic Engineering, 2011, 31 (9) : 125-128.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),972-976 DOI:10.11772/j.issn.1001-9081.2018081691            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于注意力机制的改进CLSM检索式匹配问答方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E9%87%8D%E9%87%8D&amp;code=06296625&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于重重</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E5%B8%85&amp;code=38470372&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹帅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%BD%98%E5%8D%9A&amp;code=34361715&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">潘博</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E9%9D%92%E5%B7%9D&amp;code=36727078&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张青川</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E4%B8%96%E7%92%87&amp;code=11607548&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐世璇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E5%B7%A5%E5%95%86%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0163176&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京工商大学计算机与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%E9%99%A2%E6%B0%91%E6%97%8F%E5%AD%A6%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0006210&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国社会科学院民族学与人类学研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对检索式匹配问答模型对中文语料适应性弱和句子语义信息被忽略的问题, 提出一种基于卷积神经网络潜在语义模型 (CLSM) 的中文文本语义匹配模型。首先, 在传统CLSM基础上进行改进, 去掉单词和字母的<i>N</i>元模型层, 以增强模型对中文语料的适应性;其次, 采用注意力机制算法, 针对输入的中文词向量信息建立实体关注层模型, 以加强句中核心词的权重信息;最后, 通过卷积神经网络 (CNN) 有效地捕获输入句子上下文结构方面信息, 并通过池化层对获取的语义信息进行降维。基于医疗问答对数据集, 将改进模型与传统语义模型、传统翻译模型、深度神经网络模型进行对比, 实验结果显示所提模型在归一化折现累积增益 (NDCG) 方面有4～10个百分点的提升, 优于对比模型。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">潜在语义模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%8C%B9%E9%85%8D%E9%97%AE%E7%AD%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">检索式匹配问答;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *于重重 (1971—) , 女, 北京人, 教授, 博士, CCF会员, 主要研究方向:人工智能、模式识别、机器学习;电子邮箱chongzhy@vip.sina.com;
                                </span>
                                <span>
                                    曹帅 (1993—) , 女, 甘肃庆阳人, 硕士研究生, 主要研究方向:人工智能、自然语言处理;;
                                </span>
                                <span>
                                    潘博 (1992—) , 男, 广西南宁人, 硕士研究生, 主要研究方向:人工智能、自然语言处理;;
                                </span>
                                <span>
                                    张青川 (1982—) , 男, 北京人, 讲师, 博士研究生, 主要研究方向:自然语言处理、机器学习、数据挖掘;;
                                </span>
                                <span>
                                    徐世璇 (1954—) , 女, 浙江宁波人, 研究员, 博士, 主要研究方向:少数民族语言。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>教育部人文社会科学研究规划基金资助项目 (16YJAZH072);</span>
                                <span>国家社会科学基金重大项目 (14ZDB156);</span>
                    </p>
            </div>
                    <h1><b>Retrieval matching question and answer method based on improved CLSM with attention mechanism</b></h1>
                    <h2>
                    <span>YU Chongchong</span>
                    <span>CAO Shuai</span>
                    <span>PAN Bo</span>
                    <span>ZHANG Qingchuan</span>
                    <span>XU Shixuan</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information Engineering, Beijing Technology and Business University</span>
                    <span>Institute of Ethnology and Anthropology, Chinese Academy of Social Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Focusing on the problem that the Retrieval Matching Question and Answer (RMQA) model has weak adaptability to Chinese corpus and the neglection of semantic information of the sentence, a Chinese text semantic matching model based on Convolutional neural network Latent Semantic Model (CLSM) was proposed. Firstly, the word-<i>N</i>-gram layer and letter-<i>N</i>-gram layer of CLSM were removed to enhance the adaptability of the model to Chinese corpus. Secondly, with the focus on vector information of input Chinese words, an entity attention layer model was established based on the attention mechanism algorithm to strengthen the weight information of the core words in sentence. Finally, Convolutional Neural Network (CNN) was used to capture the input sentence context structure information effectively and the pool layer was used to reduce the dimension of semantic information. In the experiments based on a medical question and answer dataset, compared with the traditional semantic models, traditional translation models and deep neural network models, the proposed model has 4-10 percentage points improvement in Normalized Discount Cumulative Gain (NDCG) .</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Latent%20Semantic%20Model%20(CLSM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Latent Semantic Model (CLSM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Retrieval%20Matching%20Question%20and%20Answer%20(RMQA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Retrieval Matching Question and Answer (RMQA) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YU Chongchong, born in 1971, Ph. D. , professor. Her research interests include artificial intelligence, pattern recognition, machine learning;
                                </span>
                                <span>
                                    CAO Shuai, born in 1993, M. S. candidate. Her research interests include artificial intelligence, natural language processing.;
                                </span>
                                <span>
                                    PAN Bo, born in 1992, M. S. candidate. His research interests include artificial intelligence, natural language processing.;
                                </span>
                                <span>
                                    ZHANG Qingchuan, born in 1982, Ph. D. candida, lecturer. His research interests include natural language processing, machine learning, data mining.;
                                </span>
                                <span>
                                    XU Shixuan, born in 1954, Ph. D. , research fellow. Her research interests include minority language.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Ministry of Education Humanities and Social Sciences Research Planning Fund Project (16YJAZH072);</span>
                                <span>the National Social Science Foundation major Project (14ZDB156);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="34">检索式匹配问答系统的研究伴随搜索引擎技术的发展不断推进。1999年, 随着文本信息检索会议中自动问答任务 (Text REtrieval Conference (Question &amp; Answering track) , TREC (QA) ) <citation id="161" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>的发起, 检索式匹配问答系统迎来了真正的研究进展。TREC (QA) 的任务是给定特定Web数据集, 从中找到能够回答问题的答案。这类方法是以检索和答案抽取为核心的问答过程, 具体过程包括问题分析、篇章检索和答案抽取<citation id="162" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="35">根据答案抽取方法的不同, 现有的检索式匹配问答系统可以分为两类<citation id="163" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>:第一类是基于模式匹配和统计文本信息抽取的问答方法。该方法需要人工线下设定好各类问题答案的模式, 需要构建大量的问答模式, 代价高。第二类是利用神经网络模型对文本作语义表示后进行语义匹配的方法。2013年Huang等<citation id="164" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>针对搜索引擎中问答之间的语义匹配问题, 提出了基于多层感知器的深度语义表示模型 (Deep Structured Semantic Model, DSSM) 。在DSSM中, 输入层是基于文本的词袋向量, 该模型忽略了文本中的词法、句法和语法信息, 将其仅仅看作词的多种组合, 这样就无法捕捉句子的上下文信息。基于此, 2014年Shen等<citation id="165" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>对DSSM作出改进, 提出了一种基于卷积神经网络的隐语义模型 (Convolutional Latent Semantic Model, CLSM) 。该模型将文本的语义信息加入到检索问答过程中, 有效地提高了检索式问答模型的精确度。2016年CLSM得到了广泛应用, 如用它来捕捉目标语言在局部上下文或者全局上下文中的含义<citation id="166" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 也有人用它来构建推荐系统<citation id="167" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="36">CLSM是基于英文文本作出的改进:其中字母<i>N</i>元组合模型主要是针对大量不同的英文单词, 从中提取出多种字母组合, 对其进行特征提取。然而中文汉字的个数远远超过了英文字母的个数, 汉字的组合数更是远远超过了英文字母的组合数, 因此通过原始CLSM很难对中文文本进行语义特征提取。本文结合中文语料的特点, 提出了一种基于注意力机制的改进CLSM检索式匹配问答模型, 该模型一方面解决了由于中文汉字远远多于英文字母而导致的文本特征难提取问题;另一方面加入了基于命名实体的注意力机制, 能有效提高匹配问答结果的准确率。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag">1 CLSM</h3>
                <div class="p1">
                    <p id="38">如图1所示, CLSM主要是将一个潜在语义空间中任意长度的词语序列所包含的语义信息映射成一个低维向量。</p>
                </div>
                <div class="area_img" id="39">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904007_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 CLSM结构" src="Detail/GetImg?filename=images/JSJY201904007_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 CLSM结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904007_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 CLSM architecture</p>

                </div>
                <div class="p1">
                    <p id="40">图1中:90k表示在不包含下划线和标点符号的情况下;字母的<i>N</i>元模型层维度是90k;卷积层以及池化层的维度均为300;最终输出语义信息的维度是128。</p>
                </div>
                <div class="p1">
                    <p id="41">CLSM主要由五个部分构成:</p>
                </div>
                <div class="p1">
                    <p id="42">1) 单词的<i>N</i>元模型层。此部分的作用是通过一个自定义大小的滑动窗口将输入的文本序列划分为多个固定长度的单词组合。</p>
                </div>
                <div class="p1">
                    <p id="43">2) 字母的<i>N</i>元模型层。此部分的作用是将第1) 步中的单词组合转换为由字母组合的向量表示。<b><i>W</i></b><sub><b><i>f</i></b></sub>是转移矩阵。对于单词<i>N</i>元模型层中的第<i>t</i>个<i>N</i>元单词组合, 其对应的<i>N</i>元字母组合表示为:</p>
                </div>
                <div class="p1">
                    <p id="44"><b><i>l</i></b><sub><i>t</i></sub>=[<b><i>f</i></b><mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mi>d</mi></mrow><mtext>Τ</mtext></msubsup></mrow></math></mathml>, …, <b><i>f</i></b><mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml>, …, <b><i>f</i></b><mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>d</mi></mrow><mtext>Τ</mtext></msubsup></mrow></math></mathml>]<sup>T</sup>;<i>t</i>=1, 2, …, <i>T</i>      (1) </p>
                </div>
                <div class="p1">
                    <p id="48">其中: <b><i>f</i></b><sub><i>t</i></sub>是第<b><i>W</i></b><sub><b><i>f</i></b></sub>个单词的<i>N</i>元字母组合形式;上下文窗口大小为<i>n</i>=2<i>d</i>+1。</p>
                </div>
                <div class="p1">
                    <p id="49">3) 卷积层。该层通过自定义大小的滑动窗口将基于每个单词的<i>N</i>元字母特征<b><i>l</i></b><sub><i>t</i></sub>转换为其在上下文中的特征向量<b><i>h</i></b><sub><i>t</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="50"><b><i>h</i></b><sub><i>t</i></sub>=tanh (<b><i>W</i></b><sub><i>c</i></sub>·<b><i>l</i></b><sub><i>t</i></sub>) ; <i>t</i>=1, 2, …, <i>T</i>      (2) </p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>tanh</mi></mrow><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mn>2</mn><mi>x</mi></mrow></msup></mrow><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mn>2</mn><mi>x</mi></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">其中:<b><i>W</i></b><sub><i>c</i></sub>是特征变换矩阵;<i>tanh</i>是激活函数。</p>
                </div>
                <div class="p1">
                    <p id="53">4) 最大池化层。通过最大池化将单词在上下文中的特征向量<b><i>h</i></b><sub><i>t</i></sub>转化为一个固定长度的句子级特征向量<b><i>v</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="54"><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">v</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>t</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Τ</mi></mrow></munder><mo stretchy="false">{</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo></mrow></math></mathml>; <i>i</i>=1, 2, …, <i>K</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="56">其中:<b><i>v</i></b> (<i>i</i>) 是最大池化层<b><i>v</i></b>中的第<i>i</i>个元素;<b><i>h</i></b><sub><i>t</i></sub> (<i>i</i>) 是第<i>t</i>个特征向量<b><i>h</i></b><sub><i>t</i></sub>中的第<i>i</i>个元素;<i>K</i>是最大池化层的向量维度, 它与上下文特征向量<b><i>h</i></b><sub><i>t</i></sub>的维度相同。</p>
                </div>
                <div class="p1">
                    <p id="57">5) 语义层。为输入的单词序列提取高级特征语义向量<b><i>y</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="58"><b><i>y</i></b>=tanh (<b><i>W</i></b><sub><i>s</i></sub>·<b><i>v</i></b>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="59">其中:<b><i>v</i></b>是通过最大池化层提取的句子级特征向量;<b><i>W</i></b><sub><i>s</i></sub>是特征变换矩阵;<i>tanh</i>是激活函数。</p>
                </div>
                <div class="p1">
                    <p id="60">CLSM是基于英文文本提出的语义特征提取模型, 其单词<i>N</i>元模型层和字母<i>N</i>元模型层都是为了提取英文文本中的特征信息, 然而中文文本中汉字的组合数量远远超过了英文字母的组合数量, 因此通过<i>N</i>元模型不能有效提取其重要特征。基于此, 本文对CLSM进行了改进。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag">2 基于CLSM的改进模型</h3>
                <div class="p1">
                    <p id="62">在<i>CLSM</i>中, <i>CNN</i>网络扮演了极其重要的作用, 对文本序列进行字母N元组合卷积。由于本文处理的是中文语料, 其汉字组合总数远多于字母组合总数, 因此需要将中文语料进行分词, 将英文字母组合替换成中文词组, 用中文词组作为文本序列基本单元。</p>
                </div>
                <div class="p1">
                    <p id="63">除此之外, 经过分词之后的中文词组种类繁多, 其中含有大量的虚词以及不相关的实词, 这些词会严重影响模型的精度。因此本文首先通过命名实体识别方法得出文本序列中相关实体, 然后构建实体关注层 (<i>Entity</i>-<i>attention Layer</i>) , 目的就是为了更好地解析句子语义, 提高模型的精确度。实体关注层的结构如图2所示。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 实体关注层结构" src="Detail/GetImg?filename=images/JSJY201904007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 实体关注层结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Entity attention layer architecture</i></p>

                </div>
                <div class="p1">
                    <p id="65">对于每个实体类型, 有其特定的权重张量<b><i>W</i></b><sub>entity</sub>。下面描述句子权重矢量<i>α</i><sup>*</sup>以及实体关注层的输出<b><i>W</i></b><sub><i>i</i></sub>的计算过程。</p>
                </div>
                <div class="area_img" id="129">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904007_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="70">其中:<b><i>W</i></b><sub>entity</sub> (<i>t</i>+<i>i</i>) 表示实体类型<b><i>word</i></b> (<i>t</i>+<i>i</i>) 的权重向量;<b><i>v</i></b> (<b><i>word</i></b> (<i>t</i>+<i>i</i>) ) 为实体类型<b><i>word</i></b> (<i>t</i>+<i>i</i>) 的向量表示。</p>
                </div>
                <div class="p1">
                    <p id="71">基于<i>α</i>归一化权重张量<i>α</i><sup>*</sup>:</p>
                </div>
                <div class="p1">
                    <p id="72"><i>α</i><sup>*</sup>={<i>α</i><sup>*</sup><sub>1</sub>, <i>α</i><sup>*</sup><sub>2</sub>, …, <i>α</i><sup>*</sup><sub><i>k</i></sub>, <i>α</i><sup>*</sup><sub><i>k</i>+1</sub>, …, <i>α</i><sup>*</sup><sub><i>n</i>-1</sub>, <i>α</i><sup>*</sup><sub><i>n</i></sub>}      (7) </p>
                </div>
                <div class="p1">
                    <p id="73">其中<i>α</i><sup>*</sup><sub><i>k</i></sub>的计算如式 (8) 所示:</p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">α</mi><msubsup><mrow></mrow><mi>k</mi><mo>*</mo></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>; <i>k</i>∈[1, <i>n</i>]      (8) </p>
                </div>
                <div class="p1">
                    <p id="76">实体关注层的输出<b><i>W</i></b><sub><i>i</i></sub>如式 (9) 所示:</p>
                </div>
                <div class="p1">
                    <p id="77"><b><i>W</i></b><sub><i>i</i></sub>=<b><i>v</i></b> (<b><i>word</i></b> (<i>t</i>+<i>i</i>) ) * (<i>α</i><sup>*</sup><sub><i>i</i></sub>) <sup>T</sup>      (9) </p>
                </div>
                <div class="p1">
                    <p id="78">将<b><i>W</i></b><sub><i>i</i></sub>输入CNN之后, 模型训练与CLSM相同。下面给出数据结构的形式化定义:</p>
                </div>
                <div class="p1">
                    <p id="79"><b>定义</b>1 实体类型所对应的权重向量<b><i>W</i></b><sub>entity</sub> (<i>t</i>+<i>i</i>) 定义为一个和输入词向量<b><i>v</i></b> (<b><i>word</i></b> (<i>t</i>+<i>i</i>) ) 维度相同的向量。</p>
                </div>
                <div class="p1">
                    <p id="80"><b>定义</b>2 实体关注层的输出<b><i>W</i></b><sub><i>i</i></sub>指的是输入词向量矩阵<b><i>V</i></b><sub><i>x</i></sub>经过实体关注层后的输出矩阵, 包含句子本身的信息及其权重信息。</p>
                </div>
                <div class="p1">
                    <p id="81"><b>定义</b>3 表示层的输出<b><i>y</i></b>即为最终的语义输出, 其大小定义为128维。</p>
                </div>
                <div class="area_img" id="82">
                    <p class="img_tit"><b>表</b>1 <b>问答对数据示例</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Question and answer data examples</p>
                    <p class="img_note"></p>
                    <table id="82" border="1"><tr><td><br />问题</td><td>答案</td></tr><tr><td><br />感冒一月有余, 吃药打针不见好, 怀疑是否感冒引起的鼻炎, 症状:鼻塞, 流鼻涕 (清浓交替) , 喉咙痒, 咳嗽引着耳痒。我该怎么治疗?</td><td>问题分析:您好, 过敏性鼻炎咽炎主要与自身抵抗力下降、接触过敏物质等因素有关。冬春季节多发, 在受凉、感冒后、或过度疲劳后接触致敏物质可迅速产生过敏反应, 可表现为鼻塞、大量清水样涕, 咽痒、刺激性干咳、剧烈咳嗽, 可伴有喘憋现象。夜间发作明显, 时间久了, 可引发支气管肺炎等症。意见建议:建议您可口服抗过敏及镇静止咳药物, 局部使用皮质激素类药物喷鼻, 可配合雾化吸入治疗, 适量口服中药止咳糖浆。户外可戴口罩防护, 避免接触过敏物质, 祝您早日康复!</td></tr><tr><td><br />双腿无力, 走路超过2公里时, 就会疲劳, 休息可以缓解。神经内科医生看过, 说我有点高血压, 同时肌酐高达150, 不能吃阿司匹林, 就开了波立维药片, 副作用较小。但是波立维药片可能使得肌酐升高。我不敢吃, 咋办?</td><td>问题分析:您好, 您目前是低血压, 站的时间长了或是坐的时间长了, 会使脑供血不足, 导致头晕、疲劳症状, 平躺就不会出现上述症状。意见建议:建议经常去医院测血压、多吃些咸菜, 在家炒菜的时候多吃盐, 多喝水, 增加血容量, 改善脑供血、升高血压。多吃盐1个月, 再试试还有上述症状吗, 试试有没有减轻。</td></tr><tr><td><br />我脸上的色斑越来越多。体检过一次, 有小三阳, 中度脂肪肝, 高血压90～130, 肾有钙盐结晶, 前列腺有增大。我应该怎样治疗?</td><td>一般可以吃点碳酸氢钠片碱化尿液, 同时可以吃点苯溴马隆或者别嘌醇片等, 建议平时饮食上面需要注意, 进啤酒、海鲜、豆制品等, 多喝水促进尿酸代谢。</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="83">模型的算法如下:</p>
                </div>
                <div class="p1">
                    <p id="84">步骤1 构建输入词向量矩阵<b><i>V</i></b><sub><i>x</i></sub>。设<b><i>word</i></b> (<i>t</i>) 为输入的句子所对应的词组单元, 其所对应的词向量为<b><i>v</i></b> (<b><i>word</i></b> (<i>t</i>) ) , 则模型的输入可以表示为<b><i>V</i></b><sub><i>x</i></sub>={<b><i>v</i></b> (<b><i>word</i></b> (0) ) , <b><i>v</i></b> (<b><i>word</i></b> (1) ) , …, <b><i>v</i></b> (<b><i>word</i></b> (<i>i</i>) ) , …}, <i>i</i>∈[0, <i>len</i> (<i>seq</i>_<i>src</i>) ], 其中<i>len</i> (<i>seq</i>_<i>src</i>) 代表输入序列的长度。</p>
                </div>
                <div class="p1">
                    <p id="85">步骤2 将<b><i>V</i></b><sub><i>x</i></sub>作为实体关注层的输入, 设其输出为<b><i>W</i></b><sub><i>i</i></sub>。则<b><i>W</i></b><sub><i>i</i></sub>可以表示为<b><i>v</i></b> (<b><i>word</i></b> (<i>t</i>+<i>i</i>) ) *<i>softmax</i> (<i>α</i>) , 其中<i>α</i>的计算如式 (6) 所示。</p>
                </div>
                <div class="p1">
                    <p id="86">步骤3 构建CLSM表示层。该层的输入为步骤2中计算获得的突出命名实体信息的矩阵<b><i>W</i></b><sub><i>i</i></sub>, 通过与<i>win</i>=1×3进行卷积运算之后得到初步得到语义信息<b><i>h</i></b><sub><i>t</i></sub>, 最终输出为<b><i>y</i></b>=tanh (<b><i>W</i></b><sub><i>s</i></sub>·max (<b><i>h</i></b><sub><i>t</i></sub>) ) 。</p>
                </div>
                <div class="p1">
                    <p id="87">步骤4 最后借助语义匹配模型, 通过计算向量余弦相似度来获得问答对匹配度<i>sim</i> (<b><i>y</i></b><sub>src</sub>, <b><i>y</i></b><sub>tgt</sub>) , 其中<b><i>y</i></b><sub>src</sub>代表问题的语义信息, <b><i>y</i></b><sub>tgt</sub>代表回答的语义信息。</p>
                </div>
                <div class="p1">
                    <p id="88">模型结构如图3所示。依据经验, 本文将词向量的维度设为300;卷积层和最大池化层的维度分别设置为300和128;学习效率初始值为0.01;卷积层的Filter的大小设为1×3, 这样既包含了上下文信息, 也去除了冗余信息。其输出是可变长度序列, 长度与输入序列的长度成比例。 在输入序列的开始和结尾分别添加一个特殊的“填充”单词〈s〉, 目的是形成词组序列中任何位置的单词的完整窗口。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904007_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 语义匹配网络模型" src="Detail/GetImg?filename=images/JSJY201904007_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 语义匹配网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904007_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Semantic matching network model</p>

                </div>
                <h3 id="90" name="90" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="91" name="91">3.1 <b>实验数据和评测指标</b></h4>
                <div class="p1">
                    <p id="92">本文采用寻医问药网站上大规模的在线问答对数据作为训练评估模型的数据集。问答对数据经过分词和标签替换等预处理后输入模型。问答对有47万对。数据分为三个部分:训练集40万对、验证集4万对、测试集3万对。每个问答对中问句和答句的平均长度分别为25词和50词。问答对数据形式如表1所示。实体代表的是医学领域的核心词汇, 比如疾病、症状的名称等。如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="93">神经网络的初始权重通过随机初始化获取。模型采用基于小批量随机梯度下降的方法进行训练, 每个最小批次包括1 024个训练样本。实验使用双重交叉验证方法。模型的评估都是通过<i>NDCG</i> (<i>Normalized Discounted Cumulative Gain</i>) 方法<citation id="168" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>来衡量的。<i>NDCG</i>常用于作为对排序结果的评价指标, 当通过模型得出某些元素的顺序时, 便可以通过<i>NDCG</i>来测评这个排序结果的准确度, 其计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ν</mi><mi>D</mi><mi>C</mi><mi>G</mi><mo>@</mo><mi>Κ</mi><mo>=</mo><mfrac><mrow><mi>D</mi><mi>C</mi><mi>G</mi><mo>@</mo><mi>Κ</mi></mrow><mrow><mi>i</mi><mi>D</mi><mi>C</mi><mi>G</mi><mo>@</mo><mi>Κ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>D</mi><mi>C</mi><mi>G</mi><mo>@</mo><mi>Κ</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mspace width="0.25em" /></mstyle><mfrac><mrow><mn>2</mn><msup><mrow></mrow><mrow><mi>r</mi><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo>-</mo><mn>1</mn></mrow><mrow><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">其中:<i>NDCG</i>@<i>K</i>表示前<i>K</i>个位置累计得到的效益;lb (<i>i</i>+1) 表示第<i>i</i>个位置上答案的影响因子的倒数;<i>r</i> (<i>l</i>) 表示第<i>l</i>个答案的相关度等级, 如3表示非常相关, 2表示较相关, 1表示相关, 0表示无关, -1表示垃圾文件。</p>
                </div>
                <div class="p1">
                    <p id="96">NDCG其实是由DCG的值计算得出的, 由式 (10) 可看出, 分子为模型计算出的Ranking的DCG值, 分母为理想情况下的DCG值。</p>
                </div>
                <div class="area_img" id="97">
                    <p class="img_tit"><b>表</b>2 <b>实体类型示例</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Entity type samples</p>
                    <p class="img_note"></p>
                    <table id="97" border="1"><tr><td><br />问题</td><td>实体</td></tr><tr><td><br />感冒一月有余, 吃药打针不见好, 怀疑是否感冒引起的鼻炎, 症状:鼻塞, 流鼻涕 (清浓交替) , 喉咙痒, 咳嗽引着耳痒。我该怎么治疗?</td><td>感冒、鼻炎、鼻塞、流鼻涕、咳嗽、喉咙痒、耳痒</td></tr><tr><td><br />我脸上的色斑越来越多。体检过一次, 有小三阳, 中度脂肪肝, 高血压90～130, 肾有钙盐结晶, 前列腺有增大。我应该怎样治疗?</td><td>小三阳、中度脂肪肝、高血压、钙盐结晶、前列腺有增大</td></tr><tr><td><br />问题分析:您好, 过敏性鼻炎咽炎主要与自身抵抗力下降、接触过敏物质等因素有关。冬春季节多发, 在受凉、感冒后、或过度疲劳后接触致敏物质可迅速产生过敏反应, 可表现为鼻塞、大量清水样涕, 咽痒、刺激性干咳、剧烈咳嗽, 可伴有喘憋现象。夜间发作明显, 时间久了, 可引发支气管肺炎等症。意见建议:建议您可口服抗过敏及镇静止咳药物, 局部使用皮质激素类药物喷鼻, 可配合雾化吸入治疗, 适量口服中药止咳糖浆。户外可戴口罩防护, 避免接触过敏物质, 祝您早日康复!</td><td>过敏性鼻炎、鼻塞、咽痒、干咳、剧烈咳嗽、重要止咳糖浆、戴口罩防护</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">3.2 <b>实验设计</b></h4>
                <div class="p1">
                    <p id="99">本文设计了三组对比实验:</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132">1) 与传统语义模型的对比实验。</h4>
                <div class="p1">
                    <p id="100">为了验证本文模型的优越性, 设计了在实验数据集与模型参数均相同的情况下, 本文模型与两组传统潜在语义模型的对比实验。潜在语义模型只能以监督学习方式或非监督学习方式在文档中学习, 其中双语主题模型 (<i>Bilingual Topic Model</i>, <i>BLTM</i>) <citation id="169" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>是监督学习, 而概率潜在语义分析模型 (<i>Probabilistic Latent Semantic Analysis</i>, <i>PLSA</i>) <citation id="170" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和文档主题生成模型 (<i>Latent Dirichlet allocation</i>, <i>LDA</i>) <citation id="171" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>是非监督学习。实验结果如表3所示。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表</b>3 <b>与传统语义模型的对比实验结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Experimental results compared with</i><i>traditional semantic models</i> %</p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td><br />模型</td><td><i>NDCG</i>@1</td><td><i>NDCG</i>@2</td><td><i>NDCG</i>@3</td></tr><tr><td><br /><i>PLSA</i></td><td>38.15</td><td>47.64</td><td>38.15</td></tr><tr><td><br /><i>LDA</i></td><td>40.10</td><td>50.40</td><td>40.10</td></tr><tr><td><br /><i>BLTM</i></td><td>39.50</td><td>47.90</td><td>39.50</td></tr><tr><td><br /><i>CLSM</i>+<i>entity</i>_<i>attention</i></td><td><b>49.20</b></td><td><b>56.70</b></td><td><b>50.30</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="131" name="131">2) 与传统翻译模型的对比实验。</h4>
                <div class="p1">
                    <p id="102">目前, 很多学者将问答对看成源语言和目标语言, 通过用翻译模型计算二者的短语与短语之间的对齐关系来建立简单的问答匹配模型。因此, 为了验证本文的改进模型较传统翻译模型的优越性, 设计了一个基于短语的翻译模型 (Phrase-based Translation Model, PTM) <citation id="172" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和一个基于词的翻译模型 (Word-based Translation Model, WTM) <citation id="173" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的对比实验, PTM旨在直接模拟多词短语中的上下文信息。而WTM实质上是PTM的一个特例。实验结果如表4所示。</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表</b>4 <b>与传统翻译模型的对比实验结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Experimental results compared with traditional translation models %</p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><br />模型</td><td>NDCG@1</td><td>NDCG@2</td><td>NDCG@3</td></tr><tr><td><br />WTM</td><td>35.6</td><td>46.41</td><td>35.6</td></tr><tr><td><br />PTM</td><td>47.9</td><td>54.4</td><td>47.9</td></tr><tr><td><br />CLSM+entity_attention</td><td><b>49.2</b></td><td><b>56.7</b></td><td><b>50.3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="130" name="130">3) 与深度神经网络模型的对比实验。</h4>
                <div class="p1">
                    <p id="104">为了验证改进之后模型的准确性, 设计了基于深度神经网络模型的对比实验。本文模型是检索式问答模型, 也就是说从众多候选答案中通过某种方法筛选出最符合要求的答案, 理论上可以看作是对候选答案进行分类, 因此加入一定数量的负样本可以增强和验证模型的检索匹配的能力。在进行与深度神将网络模型的对比实验之前, 本文按照以往的经验分别将负样本数设置为0～100, 发现负样本数为50时训练效果最好, 其次是4。因此, 分别将<i>J</i> (代表负样本数量) 设置为50和4进行了对比实验。实验结果如表5所示。</p>
                </div>
                <div class="area_img" id="105">
                    <p class="img_tit"><b>表</b>5 <b>与深度神经网络模型的对比实验结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Experimental results compared with deep neural network models %</p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td><br />模型</td><td><i>J</i></td><td>NDCG@1</td><td>NDCG@2</td><td>NDCG@3</td></tr><tr><td rowspan="2"><br />DSSM</td><td><br />4</td><td>44.3</td><td>51.4</td><td>44.3</td></tr><tr><td><br />50</td><td>41.1</td><td>52.8</td><td>41.1</td></tr><tr><td rowspan="2"><br />CLSM</td><td><br />4</td><td>44.3</td><td>53.1</td><td>44.3</td></tr><tr><td><br />50</td><td>48.4</td><td>55.3</td><td>48.4</td></tr><tr><td rowspan="2"><br />CLSM+<br />entity_attention</td><td><br />4</td><td><b>48.8</b></td><td><b>55.9</b></td><td><b>49.1</b></td></tr><tr><td><br />50</td><td><b>49.2</b></td><td><b>56.7</b></td><td><b>50.3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">3.3 <b>结果分析</b></h4>
                <h4 class="anchor-tag" id="107" name="107">1) 与传统语义模型的对比实验。</h4>
                <div class="p1">
                    <p id="108">从表3可看出, CLSM+entity_attention模型明显优于传统的潜在语义模型:在NDCG@1方面较PLSA提升了11个百分点, 在NDCG@3方面较LDA提升了12个百分点, 与BLTM相比, 模型的平均精度提高了10个百分点。PLSA和LDA均采用了词袋 (bag of words) 的方法将每一篇文档视为一个词频向量, 从而将文本信息转化为了易于建模的数字信息, 导致词与词之间的顺序信息丢失, 这不仅简化了问题的复杂性, 也降低了模型的精度。本文提出的改进模型中通过卷积神经层恰当地解决了这一问题。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">2) 与传统翻译模型的对比实验。</h4>
                <div class="p1">
                    <p id="110">从表4可看出, 在检索式问答场景下, PTM优于WTM;而CLSM+entity_attention模型与PTM和WTM相比, 均有一定幅度的提升。近几年来, 在统计机器翻译领域, 基于短语的翻译模型的性能优于基于词的翻译模型;但对于句子中非连续的固定搭配等问题仍然没有得到有效的解决。本文通过自定义大小的滑动窗口来抽取句子中基于词的上下文信息, 然后通过最大池化层进行信息筛选, 从而提升了模型的优越性。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">3) 与深度神经网络模型的对比实验。</h4>
                <div class="p1">
                    <p id="112">从表5可看出:CLSM+entity_attention模型较CLSM在NDCG方面有4个百分点的提升, 说明实体关注层对模型的精确度提高具有极其重要的作用。在负样本数量<i>J</i>的设置方面, 分别进行了三组对比实验:DSSM、CLSM以及CLSM+entity_attention, <i>J</i>=50时三者各自的NDCG@1和NDCG@2比<i>J</i>=4时均有一定幅度的提升, 因此本文将<i>J</i>设置为50。</p>
                </div>
                <div class="p1">
                    <p id="113">综上所述, CLSM+entity_attention能够通过实体关注层加强核心词的信息, 同时利用卷积神经网络有效地捕获语义匹配有用的上下文结构方面信息, 从而提升检索式匹配问答的准确率。</p>
                </div>
                <h3 id="114" name="114" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="115">CLSM的新型深度学习架构主要由CNN的卷积结构支撑, 一般通过卷积层来提取句子级别的特征, 通过最大池化层来提取<i>N</i>-gram级别的局部上下文特征。本文在此基础上进行调整, 加入了基于实体类型的关注层, 同时与几种最先进的语义模型进行比较, 发现在大规模真实问答数据集上, 改进的CLSM检索式匹配问答模型可进一步提高模型对句子的语义理解能力, 在NDCG方面有4%以上的提升。不过该模型仍然存在不足, 即用于训练模型的中文语料句子复杂度是不同的, 本文主要针对简单句进行了实验, 因此未来的工作将在原有基础上加入基于知识图谱的推理式方法, 以提高模型应用的广泛性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="133">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using Wikipedia at the TREC QA track">

                                <b>[1]</b> AHN D D, JIJKOUN V, MISHNE G A, et al. Using Wikipedia at the TREC QA track[EB/OL]. [2018- 05- 10]. http://staff.science.uva.nl/～mdr/Publications/Files/uams-trec-2004-final-qa.pdf.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200811005&amp;v=MTIyNDhIdG5Ocm85RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWNzNPTHo3QmQ3RzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 汤庸, 林鹭贤, 罗烨敏, 等. 基于自动问答系统的信息检索技术研究进展[J]. 计算机应用, 2008, 28 (11) : 2745-2748. (TANG Y, LIN L X, LUO Y M, et al. Survey on information retrieval system based on question answering system[J]. Journal of Computer Applications, 2008, 28 (11) : 2745-2748.) 
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning continuous phrase representations for translation modeling">

                                <b>[3]</b> GAO J F, HE X D, YIH W T, et al. Learning continuous phrase representations for translation modeling [EB/OL]. [2018- 05- 10]. http://www.aclweb.org/anthology/P14-1066.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep structured semantic models for web search using clickthrough data">

                                <b>[4]</b> HUANG P S, HE X D, GAO J F, et al. Learning deep structured semantic models for Web search using clickthrough data[C]// CIKM 2013: Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management. New York: ACM, 2013: 2333-2338.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval">

                                <b>[5]</b> SHEN Y, HE X D, GAO J F, et al. A latent semantic model with convolutional-pooling structure for information retrieval[C]// CIKM 2014: Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, New York: ACM, 2014: 101-110.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional latent semantic models and their applications">

                                <b>[6]</b> HE X, GAO J, DENG L, et al. Convolutional latent semantic models and their applications: US 9477654B2[P]. 2015- 10- 01.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling interestingness with deep neural networks">

                                <b>[7]</b> GAO J, PANTEL P, GAMON M, et al. Modeling interestingness with deep neural networks[EB/OL]. [2018- 05- 10]. http://www.aclweb.org/anthology/D14-1002.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identication of COPD patients&amp;#39;&amp;#39;health status using an intelligent system in the CHRONIOUS wearable platform">

                                <b>[8]</b> BELLOS C C, PAPADOPOULOS A, ROSSO R, et al. Identification of COPD patients' health status using an intelligent system in the CHRONIOUS wearable platform[J]. IEEE Journal of Biomedical and Health Informatics, 2014, 18 (3) : 731-738.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An apple-to-apple comparison of Learning-to-rank algorithms in terms of normalized discounted cumulative gain">

                                <b>[9]</b> BUSA-FEKETE R, SZARVAS G, ÉLTETHO T, et al. An apple-to-apple comparison of Learning-to-rank algorithms in terms of normalized discounted cumulative gain[C]// ECAI 2012: Proceedings of the 20th European Conference on Artificial Intelligence. Montpellier, France: IOS Press, 2012:16.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clickthrough-based latent semantic models for web search">

                                <b>[10]</b> GAO J F, TOUTANOVA K, YIH W T. Clickthrough-based latent semantic models for Web search[C]// SIGIR 2011: Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval. New York: ACM, 2011: 675-684.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS201601007&amp;v=MjY4NDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y3M09LQ2pZZmJHNEg5Zk1ybzlGWTRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 徐佳俊, 杨飏, 姚天昉, 等. 基于LDA模型的论坛热点话题识别和追踪[J]. 中文信息学报, 2016, 30 (1) : 43-49. (XU J J, YANG Y, YAO T F, et al. LDA based hot topic detection and tracking for the forum[J]. Journal of Chinese Information Processing, 2016, 30 (1) : 43-49.) 
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A deep architecture for matching short texts">

                                <b>[12]</b> LU Z D, LI H. A deep architecture for matching short texts[EB/OL]. [2018- 05- 10]. http://papers.nips.cc/paper/5019-a-deep-architecture-for-matching-short-texts.pdf.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clickthrough-based translation models for Web search: from word models to phrase models">

                                <b>[13]</b> GAO J, HE X, NIE J Y. Clickthrough-based translation models for Web search: from word models to phrase models[C]// CIKM 2010: Proceedings of the 19th ACM International Conference on Information and Knowledge Management. New York: ACM, 2010: 1139-1148.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCGC201109036&amp;v=MTk1NDNVUjdxZlp1WnNGeURnVjczT0x5N01iYkc0SDlETXBvOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 刘红光, 魏小敏. Bag of Words算法框架的研究[J]. 舰船电子工程, 2011, 31 (9) : 125-128. (LIU H G, WEI X M. Research on frame of bag of words algorithm[J]. Ship Electronic Engineering, 2011, 31 (9) : 125-128.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904007" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904007&amp;v=MjQ4NzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVjczT0x6N0JkN0c0SDlqTXE0OUZZNFFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
