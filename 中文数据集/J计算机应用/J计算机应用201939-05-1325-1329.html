<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136763315596250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905014%26RESULT%3d1%26SIGN%3d7icHqdX%252bLNimhvaXaBaUPWjyFfA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905014&amp;v=Mjk5MzFzRnlEbVY3ek9MejdCZDdHNEg5ak1xbzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="1 基于序列学习的端到端语音合成 ">1 基于序列学习的端到端语音合成</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="1.1 Encoder-Decoder&lt;b&gt;模型&lt;/b&gt;">1.1 Encoder-Decoder<b>模型</b></a></li>
                                                <li><a href="#68" data-title="1.2 &lt;b&gt;注意力机制&lt;/b&gt;">1.2 <b>注意力机制</b></a></li>
                                                <li><a href="#77" data-title="1.3 Griffin-Lim&lt;b&gt;算法&lt;/b&gt;">1.3 Griffin-Lim<b>算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="2 基于WaveNet的端到端语音合成 ">2 基于WaveNet的端到端语音合成</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="2.1 &lt;b&gt;前端处理&lt;/b&gt;">2.1 <b>前端处理</b></a></li>
                                                <li><a href="#86" data-title="2.2 &lt;b&gt;特征选择&lt;/b&gt;">2.2 <b>特征选择</b></a></li>
                                                <li><a href="#88" data-title="2.3 &lt;i&gt;CBHG&lt;/i&gt;&lt;b&gt;模块&lt;/b&gt;">2.3 <i>CBHG</i><b>模块</b></a></li>
                                                <li><a href="#91" data-title="2.4 &lt;i&gt;WaveNet&lt;/i&gt;&lt;b&gt;网络架构&lt;/b&gt;">2.4 <i>WaveNet</i><b>网络架构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="3 实验结果 ">3 实验结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#102" data-title="3.1 &lt;b&gt;实验数据&lt;/b&gt;">3.1 <b>实验数据</b></a></li>
                                                <li><a href="#104" data-title="3.2 &lt;b&gt;实验配置&lt;/b&gt;">3.2 <b>实验配置</b></a></li>
                                                <li><a href="#109" data-title="3.3 &lt;b&gt;实验评估&lt;/b&gt;">3.3 <b>实验评估</b></a></li>
                                                <li><a href="#115" data-title="3.4 &lt;b&gt;对比实验&lt;/b&gt;">3.4 <b>对比实验</b></a></li>
                                                <li><a href="#119" data-title="3.5 &lt;b&gt;系统对比&lt;/b&gt;">3.5 <b>系统对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="图1 Encoder-Decoder模型结构">图1 Encoder-Decoder模型结构</a></li>
                                                <li><a href="#70" data-title="图2 注意力机制结构">图2 注意力机制结构</a></li>
                                                <li><a href="#83" data-title="图3 基于&lt;i&gt;WaveNet&lt;/i&gt;的端到端语音合成系统">图3 基于<i>WaveNet</i>的端到端语音合成系统</a></li>
                                                <li><a href="#94" data-title="图4 &lt;i&gt;WaveNet&lt;/i&gt;网络架构">图4 <i>WaveNet</i>网络架构</a></li>
                                                <li><a href="#107" data-title="图5 特征预测情况">图5 特征预测情况</a></li>
                                                <li><a href="#112" data-title="图6 主观配对测试">图6 主观配对测试</a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同语言的语音合成样本&lt;/b&gt;&lt;i&gt;MOS&lt;/i&gt;&lt;b&gt;比较&lt;/b&gt;"><b>表</b>1 <b>不同语言的语音合成样本</b><i>MOS</i><b>比较</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同特征合成语音&lt;/b&gt;&lt;i&gt;MOS&lt;/i&gt;&lt;b&gt;比较&lt;/b&gt;"><b>表</b>2 <b>不同特征合成语音</b><i>MOS</i><b>比较</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同系统合成语音&lt;/b&gt;&lt;i&gt;MOS&lt;/i&gt;&lt;b&gt;比较&lt;/b&gt;"><b>表</b>3 <b>不同系统合成语音</b><i>MOS</i><b>比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" &lt;i&gt;FUNG P&lt;/i&gt;, &lt;i&gt;SCHULTZ T&lt;/i&gt;.&lt;i&gt;Multilingual spoken language processing&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Signal Processing Magazine&lt;/i&gt;, 2008, 25 (3) :89-97." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multilingual spoken language processing">
                                        <b>[1]</b>
                                         &lt;i&gt;FUNG P&lt;/i&gt;, &lt;i&gt;SCHULTZ T&lt;/i&gt;.&lt;i&gt;Multilingual spoken language processing&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Signal Processing Magazine&lt;/i&gt;, 2008, 25 (3) :89-97.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" &lt;i&gt;HUNT A J&lt;/i&gt;, &lt;i&gt;BLACK A W&lt;/i&gt;.&lt;i&gt;Unit selection in a concatenative speech synthesis system using a large speech database&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 1996 &lt;i&gt;IEEE International Conference on Acoustics&lt;/i&gt;, &lt;i&gt;Speech&lt;/i&gt;, &lt;i&gt;and Signal Processing&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 1996:373-376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unit selection in a concatenative speech synthesis system using a large speech database">
                                        <b>[2]</b>
                                         &lt;i&gt;HUNT A J&lt;/i&gt;, &lt;i&gt;BLACK A W&lt;/i&gt;.&lt;i&gt;Unit selection in a concatenative speech synthesis system using a large speech database&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 1996 &lt;i&gt;IEEE International Conference on Acoustics&lt;/i&gt;, &lt;i&gt;Speech&lt;/i&gt;, &lt;i&gt;and Signal Processing&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 1996:373-376.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" &lt;i&gt;CAMPBELL N&lt;/i&gt;, &lt;i&gt;BLACK A W&lt;/i&gt;.&lt;i&gt;Prosody and the selection of source units for concatenative synthesis&lt;/i&gt; [&lt;i&gt;M&lt;/i&gt;]// &lt;i&gt;Progress in Speech Synthesis&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;, 1997:279-292." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=＂Prosody and the Selection of Source Units for Concatenative Synthesis,＂">
                                        <b>[3]</b>
                                         &lt;i&gt;CAMPBELL N&lt;/i&gt;, &lt;i&gt;BLACK A W&lt;/i&gt;.&lt;i&gt;Prosody and the selection of source units for concatenative synthesis&lt;/i&gt; [&lt;i&gt;M&lt;/i&gt;]// &lt;i&gt;Progress in Speech Synthesis&lt;/i&gt;.&lt;i&gt;New York&lt;/i&gt;:&lt;i&gt;Springer&lt;/i&gt;, 1997:279-292.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" &lt;i&gt;ZE H&lt;/i&gt;, &lt;i&gt;SENIOR A&lt;/i&gt;, &lt;i&gt;SCHUSTER M&lt;/i&gt;.&lt;i&gt;Statistical parametric speech synthesis using deep neural networks&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2013 &lt;i&gt;IEEE International Conference on Acoustics&lt;/i&gt;, &lt;i&gt;Speech and Signal Processing&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2013:7962-7966." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Statistical parametric speech synthesis using neuralnetworks">
                                        <b>[4]</b>
                                         &lt;i&gt;ZE H&lt;/i&gt;, &lt;i&gt;SENIOR A&lt;/i&gt;, &lt;i&gt;SCHUSTER M&lt;/i&gt;.&lt;i&gt;Statistical parametric speech synthesis using deep neural networks&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2013 &lt;i&gt;IEEE International Conference on Acoustics&lt;/i&gt;, &lt;i&gt;Speech and Signal Processing&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2013:7962-7966.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" &lt;i&gt;TOKUDA K&lt;/i&gt;, &lt;i&gt;NANKAKU Y&lt;/i&gt;, &lt;i&gt;TODA T&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Speech synthesis&lt;/i&gt;&lt;i&gt;based on hidden Markov models&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Proceedings of the IEEE&lt;/i&gt;, 2013, 101 (5) :1234-1252." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech synthesisbased on hidden Markov models">
                                        <b>[5]</b>
                                         &lt;i&gt;TOKUDA K&lt;/i&gt;, &lt;i&gt;NANKAKU Y&lt;/i&gt;, &lt;i&gt;TODA T&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Speech synthesis&lt;/i&gt;&lt;i&gt;based on hidden Markov models&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Proceedings of the IEEE&lt;/i&gt;, 2013, 101 (5) :1234-1252.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" &lt;i&gt;ZEN H&lt;/i&gt;, &lt;i&gt;TOKUDA K&lt;/i&gt;, &lt;i&gt;BLACK A W&lt;/i&gt;.&lt;i&gt;Statistical parametric speech synthesis&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Speech Communication&lt;/i&gt;, 2009, 51 (11) :1039-1064." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300465445&amp;v=MDc3ODYvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGMFFhUlE9TmlmT2ZiSzdIdERPckk5RllPMEtDSGc4b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         &lt;i&gt;ZEN H&lt;/i&gt;, &lt;i&gt;TOKUDA K&lt;/i&gt;, &lt;i&gt;BLACK A W&lt;/i&gt;.&lt;i&gt;Statistical parametric speech synthesis&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Speech Communication&lt;/i&gt;, 2009, 51 (11) :1039-1064.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" &lt;i&gt;OORD A V D&lt;/i&gt;, &lt;i&gt;DIELEMAN&lt;/i&gt;, &lt;i&gt;ZEN H&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;WaveNet&lt;/i&gt;:&lt;i&gt;a generative model for raw audio&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2016, 2016:&lt;i&gt;arXiv&lt;/i&gt;:1609.03499 (2016- 09- 12) [2016- 09- 19].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1609.03499." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Wave Net:A Generative Model for Raw Audio&amp;quot;">
                                        <b>[7]</b>
                                         &lt;i&gt;OORD A V D&lt;/i&gt;, &lt;i&gt;DIELEMAN&lt;/i&gt;, &lt;i&gt;ZEN H&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;WaveNet&lt;/i&gt;:&lt;i&gt;a generative model for raw audio&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2016, 2016:&lt;i&gt;arXiv&lt;/i&gt;:1609.03499 (2016- 09- 12) [2016- 09- 19].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1609.03499.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" &lt;i&gt;ARIK S O&lt;/i&gt;, &lt;i&gt;CHRZANOWSKI M&lt;/i&gt;, &lt;i&gt;COATES A&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Deep Voice&lt;/i&gt;:&lt;i&gt;real&lt;/i&gt;-&lt;i&gt;time neural text&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;speech&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2017, 2017:&lt;i&gt;arXiv&lt;/i&gt;:1702.07825 (2017- 02- 25) [2017- 03- 07].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1702.07825." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Voice:real-time neural text-to-speech">
                                        <b>[8]</b>
                                         &lt;i&gt;ARIK S O&lt;/i&gt;, &lt;i&gt;CHRZANOWSKI M&lt;/i&gt;, &lt;i&gt;COATES A&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Deep Voice&lt;/i&gt;:&lt;i&gt;real&lt;/i&gt;-&lt;i&gt;time neural text&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;speech&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2017, 2017:&lt;i&gt;arXiv&lt;/i&gt;:1702.07825 (2017- 02- 25) [2017- 03- 07].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1702.07825.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" &lt;i&gt;SOTELO J&lt;/i&gt;, &lt;i&gt;MEHRI S&lt;/i&gt;, &lt;i&gt;KUMAR K&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Char&lt;/i&gt;2&lt;i&gt;Wav&lt;/i&gt;:&lt;i&gt;end&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;end speech synthesis&lt;/i&gt; [&lt;i&gt;EB&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].[2018- 06- 20].&lt;i&gt;http&lt;/i&gt;://&lt;i&gt;mila&lt;/i&gt;.&lt;i&gt;umontreal&lt;/i&gt;.&lt;i&gt;ca&lt;/i&gt;/&lt;i&gt;wp&lt;/i&gt;-&lt;i&gt;content&lt;/i&gt;/&lt;i&gt;uploads&lt;/i&gt;/2017/02/&lt;i&gt;end&lt;/i&gt;-&lt;i&gt;end&lt;/i&gt;-&lt;i&gt;speech&lt;/i&gt;.&lt;i&gt;pdf&lt;/i&gt;." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Char2Wav:end-to-end speech synthesis">
                                        <b>[9]</b>
                                         &lt;i&gt;SOTELO J&lt;/i&gt;, &lt;i&gt;MEHRI S&lt;/i&gt;, &lt;i&gt;KUMAR K&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Char&lt;/i&gt;2&lt;i&gt;Wav&lt;/i&gt;:&lt;i&gt;end&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;end speech synthesis&lt;/i&gt; [&lt;i&gt;EB&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].[2018- 06- 20].&lt;i&gt;http&lt;/i&gt;://&lt;i&gt;mila&lt;/i&gt;.&lt;i&gt;umontreal&lt;/i&gt;.&lt;i&gt;ca&lt;/i&gt;/&lt;i&gt;wp&lt;/i&gt;-&lt;i&gt;content&lt;/i&gt;/&lt;i&gt;uploads&lt;/i&gt;/2017/02/&lt;i&gt;end&lt;/i&gt;-&lt;i&gt;end&lt;/i&gt;-&lt;i&gt;speech&lt;/i&gt;.&lt;i&gt;pdf&lt;/i&gt;.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" &lt;i&gt;WANG Y&lt;/i&gt;, &lt;i&gt;SKERRY&lt;/i&gt;-&lt;i&gt;RYAN R&lt;/i&gt;, &lt;i&gt;STANTON D&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Tacotron&lt;/i&gt;:&lt;i&gt;towards end&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;end speech synthesis&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2017, 2017:&lt;i&gt;arXiv&lt;/i&gt;:1703.10135 (2017- 03- 29) [2017- 04- 06].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1703.10135." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tacotron:towards end-to-end speech synthesis">
                                        <b>[10]</b>
                                         &lt;i&gt;WANG Y&lt;/i&gt;, &lt;i&gt;SKERRY&lt;/i&gt;-&lt;i&gt;RYAN R&lt;/i&gt;, &lt;i&gt;STANTON D&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Tacotron&lt;/i&gt;:&lt;i&gt;towards end&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;end speech synthesis&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2017, 2017:&lt;i&gt;arXiv&lt;/i&gt;:1703.10135 (2017- 03- 29) [2017- 04- 06].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1703.10135.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     &lt;i&gt;GRIFFIN D&lt;/i&gt;, &lt;i&gt;LIM J S&lt;/i&gt;.&lt;i&gt;Signal estimation from modified short&lt;/i&gt;-&lt;i&gt;time Fourier transform&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Acoustics Speech and Signal Processing&lt;/i&gt;, 1984, 32 (2) :236-243.</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" &lt;i&gt;CHOROWSKI J K&lt;/i&gt;, &lt;i&gt;BAHDANAU D&lt;/i&gt;, &lt;i&gt;SERDYUK D&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Attention&lt;/i&gt;-&lt;i&gt;based models for speech recognition&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 28&lt;i&gt;th International Conference on Neural Information Processing Systems&lt;/i&gt;.&lt;i&gt;Cambridge&lt;/i&gt;, &lt;i&gt;MA&lt;/i&gt;:&lt;i&gt;MIT Press&lt;/i&gt;, 2015:577-585." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention-based models for speech recognition">
                                        <b>[12]</b>
                                         &lt;i&gt;CHOROWSKI J K&lt;/i&gt;, &lt;i&gt;BAHDANAU D&lt;/i&gt;, &lt;i&gt;SERDYUK D&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Attention&lt;/i&gt;-&lt;i&gt;based models for speech recognition&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 28&lt;i&gt;th International Conference on Neural Information Processing Systems&lt;/i&gt;.&lt;i&gt;Cambridge&lt;/i&gt;, &lt;i&gt;MA&lt;/i&gt;:&lt;i&gt;MIT Press&lt;/i&gt;, 2015:577-585.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" &lt;i&gt;BAHDANAU D&lt;/i&gt;, &lt;i&gt;CHOROWSKI J&lt;/i&gt;, &lt;i&gt;SERDYUK D&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;End&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;end attention&lt;/i&gt;-&lt;i&gt;based large vocabulary speech recognition&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2016 &lt;i&gt;IEEE International Conference on Acoustics&lt;/i&gt;, &lt;i&gt;Speech and Signal Processing&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2016:4945-4949." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end attention-based large vocabulary speech recognition">
                                        <b>[13]</b>
                                         &lt;i&gt;BAHDANAU D&lt;/i&gt;, &lt;i&gt;CHOROWSKI J&lt;/i&gt;, &lt;i&gt;SERDYUK D&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;End&lt;/i&gt;-&lt;i&gt;to&lt;/i&gt;-&lt;i&gt;end attention&lt;/i&gt;-&lt;i&gt;based large vocabulary speech recognition&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2016 &lt;i&gt;IEEE International Conference on Acoustics&lt;/i&gt;, &lt;i&gt;Speech and Signal Processing&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2016:4945-4949.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" &lt;i&gt;CHAN W&lt;/i&gt;, &lt;i&gt;JAITLY N&lt;/i&gt;, &lt;i&gt;LE Q&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Listen&lt;/i&gt;, &lt;i&gt;attend and spell&lt;/i&gt;:&lt;i&gt;a neural network for large vocabulary conversational speech recognition&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2016 &lt;i&gt;IEEE International Conference on Acoustics&lt;/i&gt;, &lt;i&gt;Speech and Signal Processing&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2016:4960-4964." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Listen,attend and spell:a neural network for large vocabulary conversational speech recognition">
                                        <b>[14]</b>
                                         &lt;i&gt;CHAN W&lt;/i&gt;, &lt;i&gt;JAITLY N&lt;/i&gt;, &lt;i&gt;LE Q&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Listen&lt;/i&gt;, &lt;i&gt;attend and spell&lt;/i&gt;:&lt;i&gt;a neural network for large vocabulary conversational speech recognition&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2016 &lt;i&gt;IEEE International Conference on Acoustics&lt;/i&gt;, &lt;i&gt;Speech and Signal Processing&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2016:4960-4964.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" &lt;i&gt;VINYALS O&lt;/i&gt;, &lt;i&gt;TOSHEV A&lt;/i&gt;, &lt;i&gt;BENGIO S&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Show and tell&lt;/i&gt;:&lt;i&gt;a neural image caption generator&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2015 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2015:3156-3164." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show and Tell:A Neural Image Caption Generator">
                                        <b>[15]</b>
                                         &lt;i&gt;VINYALS O&lt;/i&gt;, &lt;i&gt;TOSHEV A&lt;/i&gt;, &lt;i&gt;BENGIO S&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Show and tell&lt;/i&gt;:&lt;i&gt;a neural image caption generator&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2015 &lt;i&gt;IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2015:3156-3164.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" &lt;i&gt;VINYALS O&lt;/i&gt;, &lt;i&gt;KAISER L&lt;/i&gt;, &lt;i&gt;KOO T&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Grammar as a foreign language&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 28&lt;i&gt;th International Conference on Neural Information Processing Systems&lt;/i&gt;.&lt;i&gt;Cambridge&lt;/i&gt;, &lt;i&gt;MA&lt;/i&gt;:&lt;i&gt;MIT Press&lt;/i&gt;, 2014:2773-2781." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Grammar as a foreign language">
                                        <b>[16]</b>
                                         &lt;i&gt;VINYALS O&lt;/i&gt;, &lt;i&gt;KAISER L&lt;/i&gt;, &lt;i&gt;KOO T&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Grammar as a foreign language&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 28&lt;i&gt;th International Conference on Neural Information Processing Systems&lt;/i&gt;.&lt;i&gt;Cambridge&lt;/i&gt;, &lt;i&gt;MA&lt;/i&gt;:&lt;i&gt;MIT Press&lt;/i&gt;, 2014:2773-2781.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" &lt;i&gt;LEE J&lt;/i&gt;, &lt;i&gt;CHO K&lt;/i&gt;, &lt;i&gt;HOFMANN T&lt;/i&gt;.&lt;i&gt;Fully character&lt;/i&gt;-&lt;i&gt;level neural machine translation without explicit segmentation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2017, 2017:&lt;i&gt;arXiv&lt;/i&gt;:1610.03017 (2016- 10- 10) [2017- 05- 13].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1610.03017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully Character-Level Neural Machine Translation without Explicit Segmentation[Z/OL]">
                                        <b>[17]</b>
                                         &lt;i&gt;LEE J&lt;/i&gt;, &lt;i&gt;CHO K&lt;/i&gt;, &lt;i&gt;HOFMANN T&lt;/i&gt;.&lt;i&gt;Fully character&lt;/i&gt;-&lt;i&gt;level neural machine translation without explicit segmentation&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2017, 2017:&lt;i&gt;arXiv&lt;/i&gt;:1610.03017 (2016- 10- 10) [2017- 05- 13].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1610.03017.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" &lt;i&gt;SRIVASTAVA R K&lt;/i&gt;, &lt;i&gt;GREFF K&lt;/i&gt;, &lt;i&gt;SCHMIDHUBER J&lt;/i&gt;.&lt;i&gt;Highway networks&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2015, 2015:&lt;i&gt;arXiv&lt;/i&gt;:1505.00387 (2015- 03- 03) [2015- 11- 03].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1505.00387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Highway networks">
                                        <b>[18]</b>
                                         &lt;i&gt;SRIVASTAVA R K&lt;/i&gt;, &lt;i&gt;GREFF K&lt;/i&gt;, &lt;i&gt;SCHMIDHUBER J&lt;/i&gt;.&lt;i&gt;Highway networks&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;/&lt;i&gt;OL&lt;/i&gt;].&lt;i&gt;arXiv Preprint&lt;/i&gt;, 2015, 2015:&lt;i&gt;arXiv&lt;/i&gt;:1505.00387 (2015- 03- 03) [2015- 11- 03].&lt;i&gt;https&lt;/i&gt;://&lt;i&gt;arxiv&lt;/i&gt;.&lt;i&gt;org&lt;/i&gt;/&lt;i&gt;abs&lt;/i&gt;/1505.00387.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" &lt;i&gt;ERRO D&lt;/i&gt;, &lt;i&gt;SAINZ I&lt;/i&gt;, &lt;i&gt;NAVAS E&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Harmonics plus noise model based vocoder for statistical parametric speech synthesis&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Journal of Selected Topics in Signal Processing&lt;/i&gt;, 2014, 8 (2) :184-194." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Harmonics plus noise model based vocoder for statistical parametric speech synthesis">
                                        <b>[19]</b>
                                         &lt;i&gt;ERRO D&lt;/i&gt;, &lt;i&gt;SAINZ I&lt;/i&gt;, &lt;i&gt;NAVAS E&lt;/i&gt;, &lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Harmonics plus noise model based vocoder for statistical parametric speech synthesis&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Journal of Selected Topics in Signal Processing&lt;/i&gt;, 2014, 8 (2) :184-194.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" &lt;i&gt;AOKI N&lt;/i&gt;.&lt;i&gt;Development of a rule&lt;/i&gt;-&lt;i&gt;based speech synthesis system for the Japanese language using a MELP vocoder&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2000 10&lt;i&gt;th European Signal Processing Conference&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2000:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Development of a rule-based speech synthesis system for the Japanese language using a MELP vocoder">
                                        <b>[20]</b>
                                         &lt;i&gt;AOKI N&lt;/i&gt;.&lt;i&gt;Development of a rule&lt;/i&gt;-&lt;i&gt;based speech synthesis system for the Japanese language using a MELP vocoder&lt;/i&gt; [&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Proceedings of the&lt;/i&gt; 2000 10&lt;i&gt;th European Signal Processing Conference&lt;/i&gt;.&lt;i&gt;Piscataway&lt;/i&gt;, &lt;i&gt;NJ&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;, 2000:1-4.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" &lt;i&gt;GUNDUZHAN E&lt;/i&gt;, &lt;i&gt;MOMTAHAN K&lt;/i&gt;.&lt;i&gt;Linear prediction based packet loss concealment algorithm for PCM coded speech&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Speech and Audio Processing&lt;/i&gt;, 2001, 9 (8) :778-785." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Linear prediction based packet loss concealment algorithm for PCM coded speech">
                                        <b>[21]</b>
                                         &lt;i&gt;GUNDUZHAN E&lt;/i&gt;, &lt;i&gt;MOMTAHAN K&lt;/i&gt;.&lt;i&gt;Linear prediction based packet loss concealment algorithm for PCM coded speech&lt;/i&gt; [&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Speech and Audio Processing&lt;/i&gt;, 2001, 9 (8) :778-785.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 09:47</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1325-1329 DOI:10.11772/j.issn.1001-9081.2018102131            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于WaveNet的端到端语音合成方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B1%E6%B3%BD%E5%AE%87&amp;code=41746648&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邱泽宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B1%88%E4%B8%B9&amp;code=39524093&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">屈丹</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E8%BF%9E%E6%B5%B7&amp;code=40431351&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张连海</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%88%98%E7%95%A5%E6%94%AF%E6%8F%B4%E9%83%A8%E9%98%9F%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1702647&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">战略支援部队信息工程大学信息系统工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对端到端语音合成系统中Griffin-Lim算法恢复相位信息合成语音保真度较低、人工处理痕迹明显的问题, 提出了一种基于WaveNet网络架构的端到端语音合成方法。以序列映射Seq2Seq结构为基础, 首先将输入文本转化为one-hot向量, 然后引入注意力机制获取梅尔声谱图, 最后利用WaveNet后端处理网络重构语音信号的相位信息, 从而将梅尔频谱特征逆变换为时域波形样本。实验的测试语料为LJSpeech-1.0和THchs-30, 针对英语、汉语两个语种进行了实验, 实验结果表明平均意见得分 (MOS) 分别为3.31、3.02, 在合成自然度方面优于采用Griffin-Lim算法的端到端语音合成系统以及参数式语音合成系统。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音合成;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%AF%E5%88%B0%E7%AB%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">端到端;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Seq2Seq&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Seq2Seq;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Griffin-Lim%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Griffin-Lim算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=WaveNet&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">WaveNet;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *邱泽宇 (1995—) , 男, 河南驻马店人, 硕士研究生, 主要研究方向:智能信息处理、语音合成;电子邮箱qiu_hank@163.com;
                                </span>
                                <span>
                                    屈丹 (1974—) , 女, 吉林长春人, 副教授, 博士, 主要研究方向:语音信号处理、智能信息处理、人工智能、信号分析;;
                                </span>
                                <span>
                                    张连海 (1971—) , 男, 山东菏泽人, 副教授, 硕士, 主要研究方向:语音信号处理、智能信息处理、人工智能、信号分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61673395);</span>
                    </p>
            </div>
                    <h1><b>End-to-end speech synthesis based on WaveNet</b></h1>
                    <h2>
                    <span>QIU Zeyu</span>
                    <span>QU Dan</span>
                    <span>ZHANG Lianhai</span>
            </h2>
                    <h2>
                    <span>College of Information Systems Engineering, PLA Strategic Force Information Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Griffin-Lim algorithm is widely used in end-to-end speech synthesis with phase estimation, which always produces obviously artificial speech with low fidelity. Aiming at this problem, a system for end-to-end speech synthesis based on WaveNet network architecture was proposed. Based on Seq2 Seq (Sequence-to-Sequence) structure, firstly the input text was converted into a one-hot vector, then, the attention mechanism was introduced to obtain a Mel spectrogram, finally WaveNet network was used to reconstruct phase information to generate time-domain waveform samples from the Mel spectrogram features. Aiming at English and Chinese, the proposed method achieves a Mean Opinion Score (MOS) of 3.31 on LJSpeech-1.0 corpus and 3.02 on THchs-30 corpus, which outperforms the end-to-end systems based on Griffin-Lim algorithm and parametric systems in terms of naturalness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=speech%20syhthesis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">speech syhthesis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=end-to-end&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">end-to-end;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sequence-to-Sequence%20(Seq2Seq)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sequence-to-Sequence (Seq2Seq) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Griffin-Lim%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Griffin-Lim algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=WaveNet&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">WaveNet;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    QIU Zeyu, born in 1995, M. S. candidate. His research interests include intelligent information processing, speech synthesis. ;
                                </span>
                                <span>
                                    QU Dan, born in 1974, Ph. D. , associate professor. Her research interests include speech signal processing, intelligent information processing, artificial intelligence, signal analysis. ;
                                </span>
                                <span>
                                    ZHANG Lianhai, born in 1971, M. S. , associate professor. His research interests include speech signal processing, intelligent information processing, artificial intelligence, signal analysis.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-23</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61673395);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="46">语音合成 (Speech Synthesis) , 又称文语转换 (Text To Speech, TTS) 技术是指计算机通过分析将任意文本转化为流畅语音的技术。语音合成作为实现人机语音交互系统的核心技术之一<citation id="126" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 是语音处理技术中一个重要的方向, 其应用价值越来越受到重视。</p>
                </div>
                <div class="p1">
                    <p id="47">语音合成领域的主导技术随着时代的发展不断更迭。基于波形拼接的语音合成方法, 是一项把预先录制的语音波形片段拼接在一起的技术, 是目前语音合成领域常用方法之一<citation id="127" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。受到语料库内容的限制, 这种方法对拼接算法的优化、存储配置的调整等方面有较大的要求, 对于语料库之外的其他说话人、其他文本内容起不到任何作用。</p>
                </div>
                <div class="p1">
                    <p id="48">随着基于统计参数的语音合成方法日益成熟, 这种方法被逐渐应用到语音合成中<citation id="128" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。基于统计参数的语音合成方法的基本思想是, 通过对输入的训练语音进行参数分解, 然后对声学参数建模, 并构建参数化训练模型, 生成训练模型库, 最后在模型库的指导下, 预测待合成文本的语音参数, 将参数输入声码器合成目标语音, 这种方法解决了拼接式合成方法中边界人工痕迹很多的问题。然而由这些方法构造的系统需要大量的专业领域知识, 因而设计困难, 并且所需模块通常是单独训练, 产生自每个模块的错误会有叠加效应, 生成的语音与人类语音相比, 经常模糊不清并且不自然。</p>
                </div>
                <div class="p1">
                    <p id="49">随着人工智能技术的快速发展, 语音合成领域有了新的技术支持。深度学习可以将内部模块统一到一个模型中, 并直接连接输入和输出, 减少了基于特定领域知识的密集工程参数模型, 这种技术被称为“端到端”学习。设计一个能在已标注的 (文本、语音) 配对数据集上训练的端到端的语音合成系统, 会带来诸多优势: 第一, 这样的系统可以基于各种属性进行多样化的调节, 比如不同说话人、不同语言, 或者像语义这样的高层特征;第二, 与存在错误叠加效应的多阶段模型相比, 单一模型更鲁棒。</p>
                </div>
                <div class="p1">
                    <p id="50">近年来端到端的语音合成系统引起了广泛的研究, WaveNet<citation id="129" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>是一个强大的语音生成模型, 它在TTS中表现良好, 但样本级自回归的特性导致其速度较慢, 需要一个复杂的前端文本分析系统, 因此不是端到端语音合成系统。Deep Voice<citation id="130" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>将传统TTS系统流水线中的每一个模块分别用神经网络架构代替, 然而它的每个模块都是单独训练的, 要把系统改成端到端的方式比较困难。Char2Wav<citation id="131" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>是一个独立开发的可以在字符数据上训练的端到端模型, 但是它需要传统的声码器参数作为中间特征表达, 不能直接预测输出频谱特征。Tacotron<citation id="132" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>是一个从字符序列生成幅度谱的Seq2Seq (Sequence-to-Sequence) 架构, 它仅用输入数据训练出一个单一的神经网络, 用于替代语言学和声学特征的生成模块, 使用Griffin-Lim算法<citation id="133" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>估计相位, 施加短时傅里叶变换合成语音, 从而简化了传统语音合成的流水线, 然而Griffin-Lim算法会产生特有的人工痕迹并且合成的语音保真度较低, 因此需要替换成神经网络架构。</p>
                </div>
                <div class="p1">
                    <p id="51">本文针对目前端到端系统中Griffin-Lim算法还原语音信号自然度较低的问题, 提出了一种基于WaveNet网络架构的端到端语音合成方法, 采用基于注意力机制的Seq2Seq架构作为特征预测网络, 将输入文本转化为梅尔声谱图, 结合WaveNet架构实现了多语种的语音合成。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">1 基于序列学习的端到端语音合成</h3>
                <div class="p1">
                    <p id="53">基于注意力机制<citation id="134" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的Seq2Seq模型主要由引入注意力机制的编码—解码 (Encoder-Decoder) 特征预测网络模型组成。目前已经在机器翻译、语音识别<citation id="136" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>和计算机视觉<citation id="135" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>等领域得到了应用。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54">1.1 Encoder-Decoder<b>模型</b></h4>
                <div class="p1">
                    <p id="55">Encoder-Decoder模型是深度学习中常见的一个模型框架, 该框架最显著的特征就是它是一个端到端学习的算法。所谓编码, 就是将输入序列转化成一个固定长度的向量;解码, 就是将之前生成的固定向量再转化成输出序列。编码器和解码器部分可以是任意的文字、语音、图像、视频数据, 基于Encoder-Decoder架构, 将模型运用到语音合成中, 其网络结构如图1所示。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905014_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Encoder-Decoder模型结构" src="Detail/GetImg?filename=images/JSJY201905014_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Encoder-Decoder模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905014_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Architecture of Encoder-Decoder model</p>

                </div>
                <div class="p1">
                    <p id="57">编码和解码常采用循环神经网络 (Recurrent Neural Network, RNN) 的组合。在RNN中, 当前时刻隐层状态<b><i>h</i></b><sub><i>t</i></sub>是由上一时刻的隐层状态<b><i>h</i></b><sub><i>t</i>-1</sub>和当前时刻的输入<i>x</i><sub><i>t</i></sub>决定的, 即:</p>
                </div>
                <div class="p1">
                    <p id="58"><b><i>h</i></b><sub><i>t</i></sub>=<i>f</i> (<b><i>h</i></b><sub><i>t</i>-1</sub>, <i>x</i><sub><i>t</i></sub>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="59">获得了各个时刻的隐层状态以后, 再将信息汇总, 生成最后的语义编码 (上下文向量) <b><i>c</i></b>:</p>
                </div>
                <div class="p1">
                    <p id="60"><b><i>c</i></b>=<i>q</i> ({<i>f</i><sub><i>h</i><sub>1</sub></sub>, <i>f</i><sub><i>h</i><sub>2</sub></sub>, …, <b><i>h</i></b><sub><i>T</i><sub><i>x</i></sub></sub>})      (2) </p>
                </div>
                <div class="p1">
                    <p id="61">其中<i>q</i>表示非线性函数。实际上在基本的RNN或LSTM (Long Short-Term Memory) 网络中, 当前时刻计算完后便看不到前面时刻的隐层状态了, 所以就用最后一个时刻的隐层状态作为语义编码<b><i>c</i></b>, 即:</p>
                </div>
                <div class="p1">
                    <p id="62"><b><i>c</i></b>=<b><i>h</i></b><sub><i>T</i><sub><i>x</i></sub></sub>      (3) </p>
                </div>
                <div class="p1">
                    <p id="63">解码过程要根据给定的语义编码<b><i>c</i></b>和已经生成的输出序列<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>t</i>-1</sub>来预测下一个输出序列<i>y</i><sub><i>t</i></sub>, 实际上就是把待生成语音<b><i>y</i></b>={<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>t</i></sub>}的联合概率分解成按顺序的条件概率:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mo stretchy="false">{</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo><mo>, </mo><mi mathvariant="bold-italic">c</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">而每一个输出序列的条件概率即为:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>p</i> (<i>y</i><sub><i>t</i></sub>|{<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>t</i>-1</sub>}, <b><i>c</i></b>) =<i>g</i> (<i>y</i><sub><i>t</i>-1</sub>, <i>s</i><sub><i>t</i></sub>, <b><i>c</i></b>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="67">其中:<i>s</i><sub><i>t</i></sub>是输出RNN中的隐藏层状态, <b><i>c</i></b>代表之前提到的语义编码, <i>y</i><sub><i>t</i>-1</sub>表示上个时刻的输出。<i>g</i>表示一种非线性变换, 可以输出<i>y</i><sub><i>t</i></sub>的概率。这种Encoder-Decoder架构的好处在于, 它的输入序列长度不需要和输出序列长度保持一致。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">1.2 <b>注意力机制</b></h4>
                <div class="p1">
                    <p id="69"><i>Encoder</i>-<i>Decoder</i>架构虽然解决了输入序列和输出序列长度不一致的问题, 但模型在每次生成语音的时候, 所使用的语义编码都是相同的。然而, 在每次生成语音时所关注的重点是不一样的。为了弥补上述<i>Encoder</i>-<i>Decoder</i>模型的局限性<citation id="137" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 需要引入注意力机制。注意力机制的结构如图2所示。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905014_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 注意力机制结构" src="Detail/GetImg?filename=images/JSJY201905014_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 注意力机制结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905014_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Architecture of attention mechanism</i></p>

                </div>
                <div class="p1">
                    <p id="71">引入注意力机制后的语义编码<b><i>c</i></b>的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">c</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi mathvariant="bold-italic">α</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">其中:<b><i>h</i></b><sub><i>t</i></sub>为编码器输出的特征向量, <i>α</i><sub><i>t</i></sub>为权重向量。权重向量在编码器每次进行预测时都不一样, 计算方法如下:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mrow><mo stretchy="false"> (</mo><mi>e</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo></mrow></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mrow><mo stretchy="false"> (</mo><mi>e</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">) </mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中<i>e</i><sub><i>ij</i></sub>代表了编码器的第<i>j</i>个输入与解码器的第<i>i</i>个输出的匹配程度。</p>
                </div>
                <div class="p1">
                    <p id="76">在Encoder-Decoder结构中, 在编码器把输入单词转化成向量的表示之后, 每进行一次预测时, 对编码器的输出赋予不同的权重, 进行加权求和, 再把其输入到编码器中。这样也间接使<b><i>c</i></b>成为了可变长度的一个序列。训练基于注意力机制的Seq2Seq模型, 可以获得待合成语音的频谱特征。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">1.3 Griffin-Lim<b>算法</b></h4>
                <div class="p1">
                    <p id="78"><i>Griffin</i>-<i>Lim</i>算法是把<i>Seq</i>2<i>Seq</i>的输出转化成被合成为波形的目标表达, 使得估计得到的信号傅里叶变换的幅度值与原始信号傅里叶变换的幅度值的平方误差达到最小。</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mo>-</mo><mi>∞</mi></mrow><mi>∞</mi></munderover><mi>ω</mi></mstyle><mo stretchy="false"> (</mo><mi>m</mi><mi>s</mi><mo>-</mo><mi>n</mi><mo stretchy="false">) </mo><mrow><mstyle displaystyle="true"><mrow><mo>∫</mo><mrow></mrow></mrow></mstyle></mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mtext>π</mtext></mrow><mtext>π</mtext></msubsup><mrow><mi>X</mi><mo>^</mo><mspace width="0.25em" /></mrow><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>e</mtext><msup><mrow></mrow><mrow><mi>j</mi><mi>ω</mi><mi>n</mi></mrow></msup><mtext>d</mtext><mi>ω</mi></mrow><mrow><mn>2</mn><mtext>π</mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mo>-</mo><mi>∞</mi></mrow><mi>∞</mi></munderover><mi>ω</mi></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false"> (</mo><mi>m</mi><mi>s</mi><mo>-</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">通过迭代重构信号的相位信息和已知的幅度信息, 得到语音信号的估算值。比较估算的语音信号傅里叶变换的幅度值与原始信号幅度值的差值, 其中<i>ω</i> (<i>ms</i>-<i>n</i>) 为一个窗序列, <i>s</i>表示窗移的长度, <i>x</i><sup><i>i</i></sup> (<i>n</i>) 是第<i>i</i>次迭代后重构的语音信号。当两者的差值达到一个比较小的值后, 则停止迭代, 就认为第<i>i</i>次迭代后所获得的语音信号是从信号傅里叶变换幅度值重构的原始语音信号。</p>
                </div>
                <h3 id="81" name="81" class="anchor-tag">2 基于WaveNet的端到端语音合成</h3>
                <div class="p1">
                    <p id="82">尽管<i>Griffin</i>-<i>Lim</i>算法可以在不破坏左右相邻的幅度谱和自身幅度谱的情况下, 求一个近似相位, 但其会产生特有的人工痕迹并且合成的语音保真度较低, 因此需要换成神经网络合成器来恢复信号的相位信息。本文基于预测的梅尔频谱图, 采用<i>WaveNet</i>网络架构, 来学习产生不同语言的时域波形样本, 对基于注意力机制的<i>Seq</i>2<i>Seq</i>端到端语音合成系统进行改进。图3描绘了该系统架构。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905014_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于WaveNet的端到端语音合成系统" src="Detail/GetImg?filename=images/JSJY201905014_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于<i>WaveNet</i>的端到端语音合成系统  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905014_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>End</i>-<i>to</i>-<i>end speech synthesis system based on WaveNet</i></p>

                </div>
                <h4 class="anchor-tag" id="84" name="84">2.1 <b>前端处理</b></h4>
                <div class="p1">
                    <p id="85">在训练模型时, 因为纯文本数据是没法作为深度学习输入的, 所以首先要把文本转化为一个个对应的向量。不同语言有着不同的标注方法, 比如英语可以直接用26个字母加上标点符号作为标注, 也就是直接使用英文内容本身;韩语有一套独立的字母表, 每个字母可以使用<i>Unicode</i>代码作为标注字符;而汉字本身有2～3万个, 还有很多同音字, 所以使用汉语拼音作为字符标注是一种可行方案。通过调整字符的表征方式, 使系统实现多语言合成成为可能。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">2.2 <b>特征选择</b></h4>
                <div class="p1">
                    <p id="87">对于音频, 主要是提取出它的频谱特征。梅尔频率声谱图与线性频率声谱图 (短时傅里叶变换的振幅) 是相关的。从对人类听觉系统的响应测试中得到启发, 梅尔频谱是对短时傅里叶变换的频率轴施加一个非线性变换, 用较少的维度对频率范围进行压缩变换得到的, 比波形样本更平滑, 并且由于其每一帧都是对相位不变的, 所以使用这样一个表征, 更容易用均方误差 (<i>Mean Squared Error</i>, <i>MSE</i>) 损失进行训练。对比传统声码器中使用的语言学和声学特征, 梅尔声谱图作为音频信号的更低层次的声学表征, 因此在梅尔声谱图上训练语音合成应该更直接。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">2.3 <i>CBHG</i><b>模块</b></h4>
                <div class="p1">
                    <p id="89">系统输入的每个字符都是一个<i>one</i>-<i>hot</i>向量并被嵌入一个连续向量中, 通过带<i>dropout</i>瓶颈层的预处理<i>pre</i>-<i>net</i>网络, 对每个字符向量施加一组非线性变换后的序列输出输入到<i>CBHG</i> (<i>Convolutional Bank with Highway networks and Grated recurrent unit</i>) 模块<citation id="138" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="90"><i>CBHG</i>模块包含一个一维卷积滤波器组, 对局部上下文信息进行建模;然后接一个多层高速公路网络 (<i>highway network</i>) , 用来提取高层特征<citation id="139" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>;最后通过一个双向门控循环单元 (<i>Gated Recurrent Unit</i>, <i>GRU</i>) 循环神经网络, 用来前后双向提取特征序列。<i>CBHG</i>模块使用非因果卷积、批标准化、残差连接以及步长为1的最大池化处理, 将<i>pre</i>-<i>net</i>的输出变换成编码器的最终表达, 提高模型的泛化能力。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">2.4 <i>WaveNet</i><b>网络架构</b></h4>
                <div class="p1">
                    <p id="92">传统声码器是基于分析窗口中静态过程的假设完成的, 所以原始语音中的这种相位信息的详细时间结构将或多或少地丢失<citation id="140" type="reference"><link href="39" rel="bibliography" /><link href="41" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>, 并且与原始语音相比, 所产生的语音失去自然性或清晰度。线性声谱图抛弃了相位信息, 而像<i>Griffin</i>-<i>Lim</i>算法可以对抛弃的相位信息进行估计, 用一个短时傅里叶逆变换就可以把线性声谱图转换成时域波形, 但人工合成的痕迹较重。</p>
                </div>
                <div class="p1">
                    <p id="93">本文采用<i>WaveNet</i>架构作为语音生成器, 提升合成语音的质量, 所提出的方法不涉及传统声码器中通过激励信号来驱动关节式过滤器, 并且不需要对诸如高斯性的数据进行任何数学假设, 其网络结构如图4所示。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905014_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 WaveNet网络架构" src="Detail/GetImg?filename=images/JSJY201905014_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 <i>WaveNet</i>网络架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905014_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Architecture of WaveNet network</i></p>

                </div>
                <div class="p1">
                    <p id="95"><i>WaveNet</i>是一个直接产生音频信号的神经网络, 将预测的一系列声谱图样本或者语言信息输入网络, 利用<i>WaveNet</i>架构的自回归特性, 恢复由各种现有声码器丢失的详细相位信息后, 可得到语音波形。<i>WaveNet</i>主要由称为扩张因果卷积层的一维卷积层组成。输入通过这些卷积层和门控激活函数, 最后<i>softmax</i>函数输出由μ-<i>law</i>算法编码的波形采样值的后验概率<citation id="141" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。门控激活函数的具体形式由式 (9) 给出:</p>
                </div>
                <div class="p1">
                    <p id="96"><b><i>z</i></b>=tanh (<i>W</i><sub><i>f</i></sub>*<b><i>y</i></b>) ⊙<i>σ</i> (<i>W</i><sub><i>g</i></sub>*<b><i>y</i></b>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="97">其中:<b><i>y</i></b>和<b><i>z</i></b>分别是激活的输入和输出;符号*代表卷积操作;<i>σ</i> (·) 表示sigmoid函数;<i>W</i>表示卷积权重; <i>f</i>和<i>g</i>分别表示滤波器和门。</p>
                </div>
                <div class="p1">
                    <p id="98">从生成模型的角度来看, 可以写出波形采样点<b><i>y</i></b>={<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>T</i></sub>}的联合概率:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">|</mo><mi>λ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow></mrow></mstyle><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>λ</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">给定模型参数<i>λ</i>。可以认为, WaveNet 过考虑过去的波形样本的因果卷积和门控激活函数的非线性运算来近似计算上述联合概率。语音波形的合成可以通过重复等式 (10) 所需的次数来实现。 在这种情况下, 网络输入是过去产生的波形采样, 通过WaveNet网络结构, 重构损失的相位信息, 把梅尔频谱特征逆变换为时域波形样本。</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag">3 实验结果</h3>
                <h4 class="anchor-tag" id="102" name="102">3.1 <b>实验数据</b></h4>
                <div class="p1">
                    <p id="103">为了测试模型音频建模的性能, 本文在两个不同语种的语料库上对其进行评估。实验语种类别分别为英语、汉语, 采用<i>LJSpeech</i>-1.0英语语料库和<i>THchs</i>-30汉语语料库训练所有的模型: <i>LJSpeech</i>-1.0英语语料库时长约24 <i>h</i>, 共13 100句话, 每句平均17个单词, 平均时长6.6 <i>s</i>, 由一名专业女性播讲; <i>THchs</i>-30汉语语料库时长约33.5 <i>h</i>, 共13 388句话, 每句平均20个字, 平均时长9 <i>s</i>, 由30个会讲流利普通话的大学生录制。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">3.2 <b>实验配置</b></h4>
                <div class="p1">
                    <p id="105">特征提取和预处理:语音信号采样频率是22 050 <i>Hz</i>, 采样位是16 <i>b</i>, 使用<i>Hamming</i>窗处理, 帧长50 <i>ms</i>, 帧移12.5 <i>ms</i>, 预加重系数0.97。语料库中的所有音频都被与文本对应, 英语直接用26个字母加上标点符号作为字符标注, 比如“6”标注为“<i>six</i>”; 汉字则用拼音作为字符标注, “春天”标注为“<i>chun</i>1 <i>tian</i>1” (1代表第一声) , 即所有的模型都是在经过预标准化处理过的数据上训练的。</p>
                </div>
                <div class="p1">
                    <p id="106">实验训练过程包括:首先训练特征预测<i>Seq</i>2<i>Seq</i>网络, 用于从输入的字符序列预测梅尔频谱的帧序列。嵌入字符为256维, 普通的<i>Seq</i>2<i>Seq</i>模型对字符输入效果不好, 本文在嵌入层后添加一个<i>pre</i>-<i>net</i>模块, 它有两个隐藏层, 层与层之间的连接均是全连接。第一层的隐藏单元数目与输入单元数目一致, 第二层的隐藏单元数目为第一层的一半。两个隐藏层采用的均为<i>ReLu</i> (<i>Rectified Linear Unit</i>) 激活函数, 并使用0.5的<i>dropout</i>进行正则化处理, 编码器和解码器均采用2层残差<i>RNN</i>, 每层包含256个<i>GRU</i>单元, 来提高模型的泛化能力。批次规模 (<i>batch size</i>) 为32, 使用<i>Adam</i>优化器并指定参数。一阶矩估计的指数衰减率<i>β</i><sub>1</sub>=0.9, 二阶矩估计的指数衰减率<i>β</i><sub>2</sub>=0.999, 初始学习率为0.002, 提高训练效率, 使模型参数尽快收敛, 训练500k (k为1 000) 步后降低到0.000 5。经过训练, 模型学习到了清晰平滑的对齐, 如图5 (a) 所示, 并预测出梅尔声谱图, 如图5 (b) 所示。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905014_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 特征预测情况" src="Detail/GetImg?filename=images/JSJY201905014_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 特征预测情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905014_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Situation of feature prediction</p>

                </div>
                <div class="p1">
                    <p id="108">然后基于特征预测网络的输出梅尔声谱图, 来训练有30个扩大卷积层, 分3个循环进行的WaveNet网络架构, 学习产生时域波形样本。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">3.3 <b>实验评估</b></h4>
                <div class="p1">
                    <p id="110">为了评估模型的性能, 本文做了主观配对测试。在主观配对比较测试中, 测试者均为该语种专业的大学生, 测试者听完一个样本, 会选择他们更喜欢哪一个样本, 如果没有倾向可以选择中立。测试结果如图6所示。</p>
                </div>
                <div class="p1">
                    <p id="111">本实验中, 通过调整字符嵌入的构造方式, 模型实现了不同语言的合成。从图6中可以看出, 英语语音合成的样本受欢迎程度高于汉语语音样本。同时还做了平均意见得分 (<i>Mean Opinion Score</i>, <i>MOS</i>) 测试, 在<i>MOS</i>测试中, 评分者听完每个合成结果, 会对合成语音的自然度进行5分制打分 (1为很差, 2为差, 3为一般, 4为好, 5为很好) , 打分间隔为0.5, 对不同训练阶段的样本进行测试, 测试结果如表1所示。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905014_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 主观配对测试" src="Detail/GetImg?filename=images/JSJY201905014_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 主观配对测试  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905014_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Subjective preference scores</i></p>

                </div>
                <div class="area_img" id="113">
                    <p class="img_tit"><b>表</b>1 <b>不同语言的语音合成样本</b><i>MOS</i><b>比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Comparison of MOS for synthesis</i><i>speech samples in different languages</i></p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td>语种</td><td>训练<br />步数/<i>k</i></td><td><i>MOS</i></td><td></td><td>语种</td><td>训练<br />步数/<i>k</i></td><td><i>MOS</i></td></tr><tr><td rowspan="5"><br />英文</td><td><br />50</td><td>1.94</td><td rowspan="5"></td><td rowspan="5">中文</td><td><br />50</td><td>1.77</td></tr><tr><td><br />100</td><td>2.82</td><td><br />100</td><td>2.46</td></tr><tr><td><br />150</td><td>3.15</td><td><br />150</td><td>2.79</td></tr><tr><td><br />200</td><td>3.28</td><td><br />200</td><td>2.96</td></tr><tr><td><br />250</td><td>3.31</td><td><br />250</td><td>3.02</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="114">表1展示了随着训练步数的增加, 两种不同语言样本合成质量的变化。当训练步数达到200<i>k</i>步时, 系统性能趋于稳定, 合成质量提升甚微。结合图7的实验结果, 可以看出英语语音合成质量要好于汉语语音合成质量, 这可能跟汉语韵律结构复杂, 特征表达困难有较大的关系。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">3.4 <b>对比实验</b></h4>
                <div class="p1">
                    <p id="116">为了与梅尔声谱图的效果作对比, 训练特征预测网络使其预测线性频率声谱图, 这样就可以用<i>Griffin</i>-<i>Lim</i>算法对声谱图进行逆变换, 实验结果如表2所示。</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表</b>2 <b>不同特征合成语音</b><i>MOS</i><b>比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Comparison of MOS for speech synthesized from different features</i></p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />模型</td><td><i>MOS</i></td></tr><tr><td><br />线性预测幅度谱+<i>Griffin</i>-<i>Lim</i></td><td>3.03</td></tr><tr><td><br />线性预测幅度谱+<i>WaveNet</i></td><td>3.28</td></tr><tr><td><br />梅尔声谱图+<i>WaveNet</i></td><td>3.31</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">实验结果表明, <i>WaveNet</i>比<i>Griffin</i>-<i>Lim</i>算法恢复相位信息的性能更强, 生成的语音质量高很多。由于梅尔声谱图是一个更紧凑的特征表达, 所以使用梅尔声谱图作为预测特征效果更好。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119">3.5 <b>系统对比</b></h4>
                <div class="p1">
                    <p id="120">为了进一步验证端到端语音合成系统的实用性, 本文又与基于深度神经网络 (<i>Deep Neural Network</i>, <i>DNN</i>) 参数式语音合成系统, 和目前已投入商业应用的拼接式语音系统进行比较, 实验结果如表3所示。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表</b>3 <b>不同系统合成语音</b><i>MOS</i><b>比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Comparison of MOS for speech synthesized from different systems</i></p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br />系统</td><td><i>MOS</i></td></tr><tr><td><br />端到端合成系统</td><td>3.31</td></tr><tr><td><br />拼接式合成系统</td><td>3.68</td></tr><tr><td><br />基于<i>DNN</i>参数式合成系统</td><td>2.91</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="122">实验结果表明, 端到端系统合成的语音优于基于<i>DNN</i>参数式合成系统, 和商业应用广泛的拼接式合成系统还有一定的差距, 但端到端系统模型结构更鲁棒, 调节灵活, 所以有很好的研究意义。</p>
                </div>
                <h3 id="123" name="123" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="124">本文主要介绍的端到端语音合成系统, 首先用基于注意力机制的<i>Seq</i>2<i>Seq</i>模型训练一个特征预测网络, 然后获取待合成语音的梅尔声谱图, 利用<i>WaveNet</i>架构恢复损失的相位信息来实现语音合成。在实验中, 采用<i>WaveNet</i>架构的系统性能优于采用<i>Griffin</i>-<i>Lim</i>算法作为波形转换器的系统。实验中, 随着训练步数的增加, 系统的性能提高, 迭代至200<i>k</i>次后趋于稳定。调整字符的表征方式, 可以实现不同语言的合成。由于中文特征表达以及韵律结构较为复杂, 所以合成自然度不如英文语音。</p>
                </div>
                <div class="p1">
                    <p id="125">本次实验中采用的<i>Seq</i>2<i>Seq</i>架构主要为<i>RNN</i>的组合。在后续的研究中会探讨其他网络组合对合成质量的影响, 对<i>WaveNet</i>网络结构进行修订以提升收敛速度也是一个值得研究的课题。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multilingual spoken language processing">

                                <b>[1]</b> <i>FUNG P</i>, <i>SCHULTZ T</i>.<i>Multilingual spoken language processing</i> [<i>J</i>].<i>IEEE Signal Processing Magazine</i>, 2008, 25 (3) :89-97.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unit selection in a concatenative speech synthesis system using a large speech database">

                                <b>[2]</b> <i>HUNT A J</i>, <i>BLACK A W</i>.<i>Unit selection in a concatenative speech synthesis system using a large speech database</i>[<i>C</i>]// <i>Proceedings of the</i> 1996 <i>IEEE International Conference on Acoustics</i>, <i>Speech</i>, <i>and Signal Processing</i>.<i>Piscataway</i>, <i>NJ</i>:<i>IEEE</i>, 1996:373-376.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=＂Prosody and the Selection of Source Units for Concatenative Synthesis,＂">

                                <b>[3]</b> <i>CAMPBELL N</i>, <i>BLACK A W</i>.<i>Prosody and the selection of source units for concatenative synthesis</i> [<i>M</i>]// <i>Progress in Speech Synthesis</i>.<i>New York</i>:<i>Springer</i>, 1997:279-292.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Statistical parametric speech synthesis using neuralnetworks">

                                <b>[4]</b> <i>ZE H</i>, <i>SENIOR A</i>, <i>SCHUSTER M</i>.<i>Statistical parametric speech synthesis using deep neural networks</i> [<i>C</i>]// <i>Proceedings of the</i> 2013 <i>IEEE International Conference on Acoustics</i>, <i>Speech and Signal Processing</i>.<i>Piscataway</i>, <i>NJ</i>:<i>IEEE</i>, 2013:7962-7966.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech synthesisbased on hidden Markov models">

                                <b>[5]</b> <i>TOKUDA K</i>, <i>NANKAKU Y</i>, <i>TODA T</i>, <i>et al</i>.<i>Speech synthesis</i><i>based on hidden Markov models</i>[<i>J</i>].<i>Proceedings of the IEEE</i>, 2013, 101 (5) :1234-1252.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300465445&amp;v=Mjk2ODhlcnFRVE1ud1plWnRGaW5sVXIzSUtGMFFhUlE9TmlmT2ZiSzdIdERPckk5RllPMEtDSGc4b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> <i>ZEN H</i>, <i>TOKUDA K</i>, <i>BLACK A W</i>.<i>Statistical parametric speech synthesis</i> [<i>J</i>].<i>Speech Communication</i>, 2009, 51 (11) :1039-1064.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Wave Net:A Generative Model for Raw Audio&amp;quot;">

                                <b>[7]</b> <i>OORD A V D</i>, <i>DIELEMAN</i>, <i>ZEN H</i>, <i>et al</i>.<i>WaveNet</i>:<i>a generative model for raw audio</i>[<i>J</i>/<i>OL</i>].<i>arXiv Preprint</i>, 2016, 2016:<i>arXiv</i>:1609.03499 (2016- 09- 12) [2016- 09- 19].<i>https</i>://<i>arxiv</i>.<i>org</i>/<i>abs</i>/1609.03499.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Voice:real-time neural text-to-speech">

                                <b>[8]</b> <i>ARIK S O</i>, <i>CHRZANOWSKI M</i>, <i>COATES A</i>, <i>et al</i>.<i>Deep Voice</i>:<i>real</i>-<i>time neural text</i>-<i>to</i>-<i>speech</i> [<i>J</i>/<i>OL</i>].<i>arXiv Preprint</i>, 2017, 2017:<i>arXiv</i>:1702.07825 (2017- 02- 25) [2017- 03- 07].<i>https</i>://<i>arxiv</i>.<i>org</i>/<i>abs</i>/1702.07825.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Char2Wav:end-to-end speech synthesis">

                                <b>[9]</b> <i>SOTELO J</i>, <i>MEHRI S</i>, <i>KUMAR K</i>, <i>et al</i>.<i>Char</i>2<i>Wav</i>:<i>end</i>-<i>to</i>-<i>end speech synthesis</i> [<i>EB</i>/<i>OL</i>].[2018- 06- 20].<i>http</i>://<i>mila</i>.<i>umontreal</i>.<i>ca</i>/<i>wp</i>-<i>content</i>/<i>uploads</i>/2017/02/<i>end</i>-<i>end</i>-<i>speech</i>.<i>pdf</i>.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tacotron:towards end-to-end speech synthesis">

                                <b>[10]</b> <i>WANG Y</i>, <i>SKERRY</i>-<i>RYAN R</i>, <i>STANTON D</i>, <i>et al</i>.<i>Tacotron</i>:<i>towards end</i>-<i>to</i>-<i>end speech synthesis</i> [<i>J</i>/<i>OL</i>].<i>arXiv Preprint</i>, 2017, 2017:<i>arXiv</i>:1703.10135 (2017- 03- 29) [2017- 04- 06].<i>https</i>://<i>arxiv</i>.<i>org</i>/<i>abs</i>/1703.10135.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 <i>GRIFFIN D</i>, <i>LIM J S</i>.<i>Signal estimation from modified short</i>-<i>time Fourier transform</i> [<i>J</i>].<i>IEEE Transactions on Acoustics Speech and Signal Processing</i>, 1984, 32 (2) :236-243.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention-based models for speech recognition">

                                <b>[12]</b> <i>CHOROWSKI J K</i>, <i>BAHDANAU D</i>, <i>SERDYUK D</i>, <i>et al</i>.<i>Attention</i>-<i>based models for speech recognition</i> [<i>C</i>]// <i>Proceedings of the</i> 28<i>th International Conference on Neural Information Processing Systems</i>.<i>Cambridge</i>, <i>MA</i>:<i>MIT Press</i>, 2015:577-585.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end attention-based large vocabulary speech recognition">

                                <b>[13]</b> <i>BAHDANAU D</i>, <i>CHOROWSKI J</i>, <i>SERDYUK D</i>, <i>et al</i>.<i>End</i>-<i>to</i>-<i>end attention</i>-<i>based large vocabulary speech recognition</i> [<i>C</i>]// <i>Proceedings of the</i> 2016 <i>IEEE International Conference on Acoustics</i>, <i>Speech and Signal Processing</i>.<i>Piscataway</i>, <i>NJ</i>:<i>IEEE</i>, 2016:4945-4949.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Listen,attend and spell:a neural network for large vocabulary conversational speech recognition">

                                <b>[14]</b> <i>CHAN W</i>, <i>JAITLY N</i>, <i>LE Q</i>, <i>et al</i>.<i>Listen</i>, <i>attend and spell</i>:<i>a neural network for large vocabulary conversational speech recognition</i> [<i>C</i>]// <i>Proceedings of the</i> 2016 <i>IEEE International Conference on Acoustics</i>, <i>Speech and Signal Processing</i>.<i>Piscataway</i>, <i>NJ</i>:<i>IEEE</i>, 2016:4960-4964.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show and Tell:A Neural Image Caption Generator">

                                <b>[15]</b> <i>VINYALS O</i>, <i>TOSHEV A</i>, <i>BENGIO S</i>, <i>et al</i>.<i>Show and tell</i>:<i>a neural image caption generator</i>[<i>C</i>]// <i>Proceedings of the</i> 2015 <i>IEEE Conference on Computer Vision and Pattern Recognition</i>.<i>Piscataway</i>, <i>NJ</i>:<i>IEEE</i>, 2015:3156-3164.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Grammar as a foreign language">

                                <b>[16]</b> <i>VINYALS O</i>, <i>KAISER L</i>, <i>KOO T</i>, <i>et al</i>.<i>Grammar as a foreign language</i>[<i>C</i>]// <i>Proceedings of the</i> 28<i>th International Conference on Neural Information Processing Systems</i>.<i>Cambridge</i>, <i>MA</i>:<i>MIT Press</i>, 2014:2773-2781.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully Character-Level Neural Machine Translation without Explicit Segmentation[Z/OL]">

                                <b>[17]</b> <i>LEE J</i>, <i>CHO K</i>, <i>HOFMANN T</i>.<i>Fully character</i>-<i>level neural machine translation without explicit segmentation</i>[<i>J</i>/<i>OL</i>].<i>arXiv Preprint</i>, 2017, 2017:<i>arXiv</i>:1610.03017 (2016- 10- 10) [2017- 05- 13].<i>https</i>://<i>arxiv</i>.<i>org</i>/<i>abs</i>/1610.03017.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Highway networks">

                                <b>[18]</b> <i>SRIVASTAVA R K</i>, <i>GREFF K</i>, <i>SCHMIDHUBER J</i>.<i>Highway networks</i> [<i>J</i>/<i>OL</i>].<i>arXiv Preprint</i>, 2015, 2015:<i>arXiv</i>:1505.00387 (2015- 03- 03) [2015- 11- 03].<i>https</i>://<i>arxiv</i>.<i>org</i>/<i>abs</i>/1505.00387.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Harmonics plus noise model based vocoder for statistical parametric speech synthesis">

                                <b>[19]</b> <i>ERRO D</i>, <i>SAINZ I</i>, <i>NAVAS E</i>, <i>et al</i>.<i>Harmonics plus noise model based vocoder for statistical parametric speech synthesis</i> [<i>J</i>].<i>IEEE Journal of Selected Topics in Signal Processing</i>, 2014, 8 (2) :184-194.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Development of a rule-based speech synthesis system for the Japanese language using a MELP vocoder">

                                <b>[20]</b> <i>AOKI N</i>.<i>Development of a rule</i>-<i>based speech synthesis system for the Japanese language using a MELP vocoder</i> [<i>C</i>]// <i>Proceedings of the</i> 2000 10<i>th European Signal Processing Conference</i>.<i>Piscataway</i>, <i>NJ</i>:<i>IEEE</i>, 2000:1-4.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Linear prediction based packet loss concealment algorithm for PCM coded speech">

                                <b>[21]</b> <i>GUNDUZHAN E</i>, <i>MOMTAHAN K</i>.<i>Linear prediction based packet loss concealment algorithm for PCM coded speech</i> [<i>J</i>].<i>IEEE Transactions on Speech and Audio Processing</i>, 2001, 9 (8) :778-785.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905014" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905014&amp;v=Mjk5MzFzRnlEbVY3ek9MejdCZDdHNEg5ak1xbzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
