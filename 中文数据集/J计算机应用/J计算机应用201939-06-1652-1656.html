<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136674893596250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906017%26RESULT%3d1%26SIGN%3dRgcI36hEBtFJS5ksq9Nw%252f9lggSU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906017&amp;v=MDg1ODF6N0JkN0c0SDlqTXFZOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3IvTkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="1 双重注意力孪生网络算法 ">1 双重注意力孪生网络算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="1.1 &lt;b&gt;基于孪生网络的跟踪算法&lt;/b&gt;">1.1 <b>基于孪生网络的跟踪算法</b></a></li>
                                                <li><a href="#57" data-title="1.2 &lt;b&gt;双重注意力孪生网络框架&lt;/b&gt;">1.2 <b>双重注意力孪生网络框架</b></a></li>
                                                <li><a href="#59" data-title="1.3 &lt;b&gt;双重注意力机制算法&lt;/b&gt;">1.3 <b>双重注意力机制算法</b></a></li>
                                                <li><a href="#72" data-title="1.4 &lt;b&gt;数据集和网络训练细节&lt;/b&gt;">1.4 <b>数据集和网络训练细节</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="2 实验结果及分析 ">2 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="2.1 &lt;b&gt;在&lt;/b&gt;&lt;i&gt;OTB&lt;/i&gt;2013&lt;b&gt;和&lt;/b&gt;&lt;i&gt;OTB&lt;/i&gt;100&lt;b&gt;上的评估&lt;/b&gt;">2.1 <b>在</b><i>OTB</i>2013<b>和</b><i>OTB</i>100<b>上的评估</b></a></li>
                                                <li><a href="#84" data-title="2.2 &lt;b&gt;基于&lt;/b&gt;OTB100&lt;b&gt;属性的分析&lt;/b&gt;">2.2 <b>基于</b>OTB100<b>属性的分析</b></a></li>
                                                <li><a href="#87" data-title="2.3 &lt;b&gt;在&lt;/b&gt;&lt;i&gt;VOT&lt;/i&gt;2017&lt;b&gt;实时挑战上的结果&lt;/b&gt;">2.3 <b>在</b><i>VOT</i>2017<b>实时挑战上的结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="图1 DASiam算法原理图">图1 DASiam算法原理图</a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;双重注意力孪生网络参数&lt;/b&gt;"><b>表</b>1 <b>双重注意力孪生网络参数</b></a></li>
                                                <li><a href="#63" data-title="图2 双重注意力模块">图2 双重注意力模块</a></li>
                                                <li><a href="#83" data-title="图3 不同视频基准库上成功率对比">图3 不同视频基准库上成功率对比</a></li>
                                                <li><a href="#86" data-title="图4 &lt;i&gt;OTB&lt;/i&gt;100上不同属性的成功率对比">图4 <i>OTB</i>100上不同属性的成功率对比</a></li>
                                                <li><a href="#89" data-title="图5 &lt;i&gt;VOT&lt;/i&gt;2017实时平均期望重叠率排名">图5 <i>VOT</i>2017实时平均期望重叠率排名</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="110">


                                    <a id="bibliography_1" title="HENRIQUES J F, CASEIRO R, MARTINS P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">
                                        <b>[1]</b>
                                        HENRIQUES J F, CASEIRO R, MARTINS P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                                    </a>
                                </li>
                                <li id="112">


                                    <a id="bibliography_2" title="熊昌镇, 车满强, 王润玲.基于稀疏卷积特征和相关滤波的实时视觉跟踪算法[J].计算机应用, 2018, 38 (8) :2175-2179, 2223. (XIONG C Z, CHE M Q, WANG R L.Real-time visual tracking algorithm based on correlation filters and sparse convolutional features[J].Journal of Computer Applications, 2018, 38 (8) :2175-2179, 2223.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201808007&amp;v=MTM0OTdyL05MejdCZDdHNEg5bk1wNDlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        熊昌镇, 车满强, 王润玲.基于稀疏卷积特征和相关滤波的实时视觉跟踪算法[J].计算机应用, 2018, 38 (8) :2175-2179, 2223. (XIONG C Z, CHE M Q, WANG R L.Real-time visual tracking algorithm based on correlation filters and sparse convolutional features[J].Journal of Computer Applications, 2018, 38 (8) :2175-2179, 2223.) 
                                    </a>
                                </li>
                                <li id="114">


                                    <a id="bibliography_3" title="樊佳庆, 宋慧慧, 张开华.通道稳定性加权补充学习的实时视觉跟踪算法[J].计算机应用, 2018, 38 (6) :1751-1754. (FAN JQ, SONG H H, ZHANG K H.Real-time visual tracking algorithm via channel stability weighted complementary learning[J].Journal of Computer Applications, 2018, 38 (6) :1751-1754.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806038&amp;v=MjYzNDBMejdCZDdHNEg5bk1xWTlHYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyL04=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        樊佳庆, 宋慧慧, 张开华.通道稳定性加权补充学习的实时视觉跟踪算法[J].计算机应用, 2018, 38 (6) :1751-1754. (FAN JQ, SONG H H, ZHANG K H.Real-time visual tracking algorithm via channel stability weighted complementary learning[J].Journal of Computer Applications, 2018, 38 (6) :1751-1754.) 
                                    </a>
                                </li>
                                <li id="116">


                                    <a id="bibliography_4" title="朱明敏, 胡茂海.基于相关滤波器的长时视觉目标跟踪方法[J].计算机应用, 2017, 37 (5) :1466-1470. (ZHU M M, HU M H.Long-term visual object tracking algorithm based on correlation filter[J].Journal of Computer Applications, 2017, 37 (5) :1466-1470.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201705045&amp;v=MTIyMTdCdEdGckNVUjdxZlp1WnNGeS9oV3IvTkx6N0JkN0c0SDliTXFvOUJZWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        朱明敏, 胡茂海.基于相关滤波器的长时视觉目标跟踪方法[J].计算机应用, 2017, 37 (5) :1466-1470. (ZHU M M, HU M H.Long-term visual object tracking algorithm based on correlation filter[J].Journal of Computer Applications, 2017, 37 (5) :1466-1470.) 
                                    </a>
                                </li>
                                <li id="118">


                                    <a id="bibliography_5" title="BERTINETTO L, VALMADRE J, HENRIQUES J F, et al.Fullyconvolutional Siamese networks for object tracking[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision, LNCS 9914.Cham:Springer, 2016:850-865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">
                                        <b>[5]</b>
                                        BERTINETTO L, VALMADRE J, HENRIQUES J F, et al.Fullyconvolutional Siamese networks for object tracking[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision, LNCS 9914.Cham:Springer, 2016:850-865.
                                    </a>
                                </li>
                                <li id="120">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                    KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.</a>
                                </li>
                                <li id="122">


                                    <a id="bibliography_7" title="TAO R, GAVVES E, SMEULDERS A W M.Siamese instance search for tracking[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1420-1429." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Siamese Instance Search for Tracking">
                                        <b>[7]</b>
                                        TAO R, GAVVES E, SMEULDERS A W M.Siamese instance search for tracking[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1420-1429.
                                    </a>
                                </li>
                                <li id="124">


                                    <a id="bibliography_8" title="HUANG C, LUCEY S, RAMANAN D.Learning policies for adaptive tracking with deep feature cascades[C]//Proceedings of the2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:105-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Policies for Adaptive Tracking With Deep Feature Cascades">
                                        <b>[8]</b>
                                        HUANG C, LUCEY S, RAMANAN D.Learning policies for adaptive tracking with deep feature cascades[C]//Proceedings of the2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:105-114.
                                    </a>
                                </li>
                                <li id="126">


                                    <a id="bibliography_9" title="VALMADRE J, BERTINETTO L, HENRIQUES J, et al.End-toend representation learning for correlation filter based tracking[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5000-5008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Representation Learning for Correlation Filter Based Tracking">
                                        <b>[9]</b>
                                        VALMADRE J, BERTINETTO L, HENRIQUES J, et al.End-toend representation learning for correlation filter based tracking[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5000-5008.
                                    </a>
                                </li>
                                <li id="128">


                                    <a id="bibliography_10" title="GUO Q, FENG W, ZHOU C, et al.Learning dynamic Siamese network for visual object tracking[C]//Proceedings of the 2017IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:1781-1789." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Dynamic Siamese Network for Visual Object Tracking">
                                        <b>[10]</b>
                                        GUO Q, FENG W, ZHOU C, et al.Learning dynamic Siamese network for visual object tracking[C]//Proceedings of the 2017IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:1781-1789.
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_11" title="HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[11]</b>
                                        HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_12" title="SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-10-16].http://www.cs.cmu.edu/~jeanoh/16-785/papers/simonyan-iclr2015-vgg.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[12]</b>
                                        SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-10-16].http://www.cs.cmu.edu/~jeanoh/16-785/papers/simonyan-iclr2015-vgg.pdf.
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_13" title="RUSSAKOVSKY O, DENG J, SU H, et al.Image Net large scale visual recognition challenge[J].International Journal of Computer Vision, 2015, 115 (3) :211-252." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:Large scale visual recognition challenge">
                                        <b>[13]</b>
                                        RUSSAKOVSKY O, DENG J, SU H, et al.Image Net large scale visual recognition challenge[J].International Journal of Computer Vision, 2015, 115 (3) :211-252.
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_14" title="ABADI M, BARHAM P, CHEN J M, et al.Tensor Flow:a system for large-scale machine learning[C]//OSDI 2016:Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation.Berkeley, CA:USENIX Association, 2016:265-283." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tensor Flow:a system for large-scale machine learning">
                                        <b>[14]</b>
                                        ABADI M, BARHAM P, CHEN J M, et al.Tensor Flow:a system for large-scale machine learning[C]//OSDI 2016:Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation.Berkeley, CA:USENIX Association, 2016:265-283.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_15" title="WU Y, LIM J, YANG M H.Online object tracking:a benchmark[C]//CVPR 2013:Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">
                                        <b>[15]</b>
                                        WU Y, LIM J, YANG M H.Online object tracking:a benchmark[C]//CVPR 2013:Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2013:2411-2418.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_16" title="WU Y, LIM J, YANG M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[16]</b>
                                        WU Y, LIM J, YANG M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_17" title="KRISTAN M, LEONARDIS A, MATAS J, et al.The visual object tracking VOT2017 challenge results[C]//ICCVW 2017:Proceedings of the 2017 IEEE International Conference on Computer Vision Workshop.Piscataway, NJ:IEEE, 2017:1949-1972." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Visual Object Tracking VOT2017 Challenge Results">
                                        <b>[17]</b>
                                        KRISTAN M, LEONARDIS A, MATAS J, et al.The visual object tracking VOT2017 challenge results[C]//ICCVW 2017:Proceedings of the 2017 IEEE International Conference on Computer Vision Workshop.Piscataway, NJ:IEEE, 2017:1949-1972.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_18" title="DANELLJAN M, HGER G, KHAN F, et al.Accurate scale estimation for robust visual tracking[C]//Proceedings of the 2014British Machine Vision Conference.Durham, UK:BMVA Press, 2014:65.1-65.11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">
                                        <b>[18]</b>
                                        DANELLJAN M, HGER G, KHAN F, et al.Accurate scale estimation for robust visual tracking[C]//Proceedings of the 2014British Machine Vision Conference.Durham, UK:BMVA Press, 2014:65.1-65.11.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_19" title="DANELLJAN M, HAGER G, KHAN F S, et al.Learning spatially regularized correlation filters for visual tracking[C]//ICCV2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:4310-4318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[19]</b>
                                        DANELLJAN M, HAGER G, KHAN F S, et al.Learning spatially regularized correlation filters for visual tracking[C]//ICCV2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:4310-4318.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-15 14:58</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1652-1656 DOI:10.11772/j.issn.1001-9081.2018112419            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于双重注意力孪生网络的实时视觉跟踪</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%BA%B7&amp;code=30364808&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨康</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%8B%E6%85%A7%E6%85%A7&amp;code=37754420&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宋慧慧</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%BC%80%E5%8D%8E&amp;code=35449591&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张开华</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%8B%8F%E7%9C%81%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%8D%97%E4%BA%AC%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6)&amp;code=0151773&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江苏省大数据分析技术重点实验室(南京信息工程大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A7%E6%B0%94%E7%8E%AF%E5%A2%83%E4%B8%8E%E8%A3%85%E5%A4%87%E6%8A%80%E6%9C%AF%E5%8D%8F%E5%90%8C%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83(%E5%8D%97%E4%BA%AC%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6)&amp;code=0151773&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">大气环境与装备技术协同创新中心(南京信息工程大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了解决全卷积孪生网络 (SiamFC) 跟踪算法在跟踪目标经历剧烈的外观变化时容易发生模型漂移从而导致跟踪失败的问题, 提出了一种双重注意力机制孪生网络 (DASiam) 去调整网络模型并且不需要在线更新。首先, 主干网络使用修改后表达能力更强的并适用于目标跟踪任务的VGG网络;然后, 在网络的中间层加入一个新的双重注意力机制去动态地提取特征, 这种机制由通道注意机制和空间注意机制组成, 分别对特征图的通道维度和空间维度进行变换得到双重注意特征图;最后, 通过融合两个注意机制的特征图进一步提升模型的表征能力。在三个具有挑战性的跟踪基准库即OTB2013、OTB100和2017年视觉目标跟踪库 (VOT2017) 实时挑战上进行实验, 实验结果表明, 以40 frame/s的速度运行时, 所提算法在OTB2013和OTB100上的成功率指标比基准SiamFC分别高出3.5个百分点和3个百分点, 并且在VOT2017实时挑战上面超过了2017年的冠军SiamFC, 验证了所提出算法的有效性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孪生网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    杨康 (1993—) , 男, 江苏徐州人, 硕士研究生, 主要研究方向:目标跟踪;;
                                </span>
                                <span>
                                    *宋慧慧 (1986—) , 女, 山东聊城人, 教授, 博士, 主要研究方向:遥感图像处理;songhuihui@ nuist. edu. cn;
                                </span>
                                <span>
                                    张开华 (1983—) , 男, 山东日照人, 教授, 博士, CCF会员, 主要研究方向:图像分割、目标跟踪。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61872189, 61876088);</span>
                                <span>江苏省自然科学基金资助项目 (BK20170040);</span>
                                <span>江苏省研究生科研与实践创新计划项目 (SJCX19_0311);</span>
                    </p>
            </div>
                    <h1><b>Real-time visual tracking based on dual attention siamese network</b></h1>
                    <h2>
                    <span>YANG Kang</span>
                    <span>SONG Huihui</span>
                    <span>ZHANG Kaihua</span>
            </h2>
                    <h2>
                    <span>Jiangsu Key Laboratory of Big Data Analysis Technology (Nanjing University of Information Science and Technology)</span>
                    <span>Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (Nanjing University of Information Science and Technology)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem that Fully-Convolutional Siamese network (SiamFC) tracking algorithm is prone to model drift and results in tracking failure when the tracking target suffers from dramatic appearance changes, a new Dual Attention Siamese network (DASiam) was proposed to adapt the network model without online updating. Firstly, a modified Visual Geometry Group (VGG) network which was more expressive and suitable for the target tracking task was used as the backbone network. Then, a novel dual attention mechanism was added to the middle layer of the network to dynamically extract features. This mechanism was consisted of a channel attention mechanism and a spatial attention mechanism. The channel dimension and the spatial dimension of the feature maps were transformed to obtain the double attention feature maps. Finally, the feature representation of the model was further improved by fusing the feature maps of the two attention mechanisms. The experiments were conducted on three challenging tracking benchmarks: OTB2013, OTB100 and 2017 Visual-Object-Tracking challenge (VOT2017) real-time challenges. The experimental results show that, running at the speed of 40 frame/s, the proposed algorithm has higher success rates on OTB2013 and OTB100 than the baseline SiamFC by the margin of 3.5 percentage points and 3 percentage points respectively, and surpass the 2017 champion SiamFC in the VOT2017 real-time challenge, verifying the effectiveness of the proposed algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=siamese%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">siamese network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YANG Kang, born in 1993, M. S. candidate. His research interests include object tracking. ;
                                </span>
                                <span>
                                    SONG Huihui, born in 1986, Ph. D. , professor. Her research interests include remote sensing image processing. ;
                                </span>
                                <span>
                                    ZHANG Kaihua, born in 1983, Ph. D. , professor. His research interests include image segmentation, object tracking.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-07</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61872189, 61876088);</span>
                                <span>the Natural Science Foundation of Jiangsu Province (BK20170040);</span>
                                <span>the Postgraduate Research&amp;Practice Innovation Program of Jiangsu Province (SJCX19_0311);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">视觉目标跟踪在计算机视觉领域是一个基础性但充满挑战的研究方向, 被应用于各种视觉领域, 比如无人驾驶、人机交互和视频监控等。由于存在目标发生剧烈的外观变化、目标遮挡、光照变换等干扰因素, 除此之外, 还要考虑实时的因素, 所以尽管最近几年目标跟踪算法研究取得了显著性的提升, 但到目前为止仍然是一个极具挑战性的任务。</p>
                </div>
                <div class="p1">
                    <p id="43">基于相关滤波的跟踪器可以通过一个循环矩阵在傅里叶域快速求解来实现快速目标跟踪, 出现了很多速度快且简单的跟踪器<citation id="154" type="reference"><link href="110" rel="bibliography" /><link href="112" rel="bibliography" /><link href="114" rel="bibliography" /><link href="116" rel="bibliography" /><link href="118" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。最近几年, 深度卷积神经网络在计算机视觉领域取得了显著的成功, 比如分类任务、目标检测等任务。所以也有很多研究者将深度学习应用到目标跟踪任务上去, 其中取得突破性的且能够达到实时要求的算法就是全卷积孪生网络 (Fully-Convolutional Siamese network, SiamFC) <citation id="148" type="reference"><link href="118" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, SiamFC把目标跟踪任务当作相似性匹配任务, 即利用外部训练数据训练一个修改后的AlexNet<citation id="149" type="reference"><link href="120" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>卷积网络作为通用的匹配函数, 再把匹配函数作为目标跟踪的图像特征提取器, 如果匹配函数能够学习更好的特征表达能力, 那么对于提升跟踪器的性能是有帮助的。孪生实例搜索跟踪 (Siamese Instance Search Tracking, SINT) <citation id="150" type="reference"><link href="122" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将跟踪任务看作是一个验证任务并利用光流进一步提升性能表现, 但是速度只有4 frame/s, 很难应用到现实场景中;提前停止跟踪 (Early-Stopping Tracker, EAST) <citation id="151" type="reference"><link href="124" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>主要判断低级的特征, 如果能够跟踪到目标时就停止特征提取进行加速;相关滤波网络 (Correlation Filter Network, CFNet) 跟踪<citation id="152" type="reference"><link href="126" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>将相关滤波作为一个可微的层加入低层的网络特征中去学习目标变换, 大大降低了网络参数量的同时仍然保持很好的跟踪性能。动态孪生网跟踪 (Dynamic Siamese Network, DSiam) <citation id="153" type="reference"><link href="128" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>尝试在线学习目标的外观变化去进一步提升孪生网络的表征能力。</p>
                </div>
                <div class="p1">
                    <p id="44">尽管基于孪生网络的跟踪算法取得了显著的进步, 但是这种孪生网络框架仍然有一些问题没有解决。首先, 用于孪生网络的框架一般都是比较浅层的AlexNet网络, 在深度学习任务中, 已经证明了更深的网络具有更强的信息表征能力<citation id="155" type="reference"><link href="130" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>;其次, 在目标发生剧烈的变化时, 由于孪生网络缺少动态的调节模型机制, 只能等价地对待每一个特征图和特征空间, 没有重点关注的目标区域, 这样限制了模型丰富的表征能力。</p>
                </div>
                <div class="p1">
                    <p id="45">针对基于孪生网络的跟踪器出现的上述问题, 本文在SiamFC的跟踪算法框架之下, 把特征提取网络换成了修改过的且适用于目标跟踪任务的VGG (Visual Geometry Group) <citation id="156" type="reference"><link href="132" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>网络, 在此基础之上, 为了进一步增强网络模型的判别能力, 提出了一种新的双重注意力机制去调节模型。最后为了验证该算法的有效性, 在三个具有挑战性的视频库上进行详尽的实验, 并与几个经典的跟踪算法进行比较, 实验结果表明所提方法得到了很有竞争力的结果。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag">1 双重注意力孪生网络算法</h3>
                <div class="p1">
                    <p id="47">为了实现高效的视觉跟踪任务, 本文提出了一种新的基于双重注意孪生 (Dual Attention Siamese network, DASiam) 网络的视觉跟踪算法, 如图1所示。该算法由一个修改后的深度卷积神经网络VGG和一个双重注意模块组成, 其中双重注意模块包括通道注意模块和空间注意模块, 最后将提取到的模板图像和搜索图像的高维语义信息特征进行相关操作得到最终的目标位置。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906017_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 DASiam算法原理图" src="Detail/GetImg?filename=images/JSJY201906017_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 DASiam算法原理图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906017_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic diagram of DASiam algorithm</p>

                </div>
                <h4 class="anchor-tag" id="49" name="49">1.1 <b>基于孪生网络的跟踪算法</b></h4>
                <div class="p1">
                    <p id="50">最近几年在目标跟踪领域的开创性工作是全卷积孪生网络 (SiamFC) 目标跟踪算法, 如图1所示, 孪生网络的输入是从视频第一帧 (目标) 和后续帧进行裁剪的一堆图像, 分别用<b><i>Z</i></b>和<b><i>X</i></b>表示, 其中<b><i>Z</i></b>∈<b>R</b><sup><i>W</i><sub><i>t</i></sub>×<i>H</i><sub><i>t</i></sub>×3</sup>且<b><i>X</i></b>∈<b>R</b><sup><i>W</i><sub><i>s</i></sub>×<i>H</i><sub><i>s</i></sub>×3</sup>, 然后通过一个离线训练的匹配函数<i>F</i> (<b><i>Z</i></b>, <b><i>X</i></b>) 在模板图像<b><i>Z</i></b>和搜索图像<b><i>X</i></b>进行相关运算得到一个相似性响应得分图, 响应得分最大的位置就是新的目标位置, 其中用于特征提取的卷积网络关于搜索图像<b><i>X</i></b>是全卷积的, 这样就可以输入不同尺度大小的搜索图像以便选择合适的尺度作为新的预测框。相似性响应得分图可以由式 (1) 得到:</p>
                </div>
                <div class="p1">
                    <p id="51"><i>F</i> (<b><i>Z</i></b>, <b><i>X</i></b>;<i>θ</i>) =<i>φ</i> (<b><i>Z</i></b>;<i>θ</i>) *<i>φ</i> (<b><i>X</i></b>;<i>θ</i>) +<i>b</i>·1      (1) </p>
                </div>
                <div class="p1">
                    <p id="52">其中:“*”表示相关运算;<i>θ</i>是网络的参数且<i>b</i>是一个偏置项。最后的输出是一个定义在有限的网格区域中的具有空间结构的得分图而不是一个链式向量或者标量, 其中网络中最优的参数是采用随机梯度下降从头开始训练而得到的, 具体而言, 从视频目标检测数据集 (ImageNet Large Scale Visual Recognition Challenge, ILSVRC) <citation id="157" type="reference"><link href="134" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>获得大量的图像对 (<b><i>Z</i></b><sub><i>i</i></sub>, <b><i>X</i></b><sub><i>i</i></sub>) , 给定相应的标签响应图<b><i>Y</i></b><sub><i>i</i></sub>∈{+1, -1}, 然后最小化如下的逻辑回归损失函数<i>L</i> (·) :</p>
                </div>
                <div class="p1">
                    <p id="53"><mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">θ</mi></munder><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">{</mo></mstyle><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Y</mi><mo>, </mo><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ζ</mi><mo>, </mo><mi mathvariant="bold-italic">X</mi></mrow></math></mathml>;<i>θ</i>) }      (2) </p>
                </div>
                <div class="p1">
                    <p id="55">其中, <i>N</i>表示训练样本数。</p>
                </div>
                <div class="p1">
                    <p id="56">虽然SiamFC取得了很好的结果, 但是与现有的相关滤波跟踪器的结果有着很大的差距, 这是因为SiamFC用于特征提取的全卷积网络是使用修改后的AlexNet, 由于AlexNet层数较浅, 学到的特征表征能力有限, 当跟踪目标发生剧烈形变时模型容易发生漂移, 导致跟踪失败。本文采用更深的修改后的适用于目标跟踪任务的VGG网络模型作为新的特征提取网络, 并在网络中加入双重注意力机制调整模型的特征提取过程, 进而选择性地强调有用的信息而抑制不太有用的信息, 而不是等价地对待所有的特征信息。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57">1.2 <b>双重注意力孪生网络框架</b></h4>
                <div class="p1">
                    <p id="58">图1展示了本文算法的基础框架, 由修改后的<i>VGG</i>网络作为主干网络, 除了最后一个卷积 (<i>Convolutional</i>, <i>Conv</i>) 层, 每一层卷积之后立即加入批归一化 (<i>Batch Normalization</i>, <i>BN</i>) 层, 然后再经过非线性激活函数 (<i>Rectified Linear Unit</i>, <i>ReLU</i>) 层, 没有填充, 并且在网络的第10层后面加入一个注意力调节机制, 具体的网络参数如表1所示。由于深度卷积网络中高语义信息对于目标的外观变化具有很强的鲁棒性, 但是当出现相似性目标时, 由于高级语义信息缺少判别性, 就容易导致模型出现漂移。所以为了增强网络的判别能力, 在网络的中间层加入一个动态的特征调节机制, 这个机制由双重注意力机制实现, 包括通道注意机制和空间注意机制, 在后面将详细介绍双重注意力机制算法, 所有的网络参数在训练完成后都是固定的, 不需要在线微调从而满足实时性的要求。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.3 <b>双重注意力机制算法</b></h4>
                <div class="p1">
                    <p id="60">注意力机制在图像领域取得了很大的成功, 因为它参考了人类的一个习惯:当我们看到一张图片的时候并不是一次性能看到所有的信息, 而是仅仅关注某个被选定的位置, 然后再向四周蔓延。神经网络在处理图像的时候, 每次网络的关注点可能只是图像中的某个小部分, 因此如果能在网络模型关注图像某个部分时都能够强调这个部分的话, 这样对于模型的特征表达能力是有提升的。为此, 本文设计了一种适用于目标跟踪任务的双重注意力机制, 当目标发生剧烈形变的时候, 网络能够通过注意力机制关注目标的主要部分, 从而提升模型的鲁棒性。</p>
                </div>
                <div class="area_img" id="61">
                    <p class="img_tit"><b>表</b>1 <b>双重注意力孪生网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Dual attention siamese network parameters</i></p>
                    <p class="img_note"></p>
                    <table id="61" border="1"><tr><td><br />卷积层</td><td>卷积核大小<br /> (<i>w</i>, <i>h</i>, <i>in</i>, <i>out</i>) </td><td>步长</td><td>模板图像<br />127×127</td><td>搜索图像<br />255×255</td></tr><tr><td><br />Conv1</td><td>3×3×3×96</td><td>1</td><td>125×125</td><td>253×253</td></tr><tr><td><br />Conv2</td><td>3×3×96×96</td><td>1</td><td>123×123</td><td>251×251</td></tr><tr><td><br />Pool1</td><td>3×3</td><td>2</td><td>61×61</td><td>125×25</td></tr><tr><td><br />Conv3</td><td>3×3×96×128</td><td>1</td><td>59×59</td><td>123×123</td></tr><tr><td><br />Conv4</td><td>3×3×128×128</td><td>1</td><td>57×57</td><td>121×121</td></tr><tr><td><br />Pool2</td><td>3×3</td><td>2</td><td>28×28</td><td>60×60</td></tr><tr><td><br />Conv5</td><td>3×3×128×256</td><td>1</td><td>26×26</td><td>58×58</td></tr><tr><td><br />Conv6</td><td>3×3×256×256</td><td>1</td><td>24×24</td><td>56×56</td></tr><tr><td><br />Conv7</td><td>3×3×256×256</td><td>1</td><td>22×22</td><td>54×54</td></tr><tr><td><br />Conv8</td><td>3×3×256×256</td><td>1</td><td>20×20</td><td>52×52</td></tr><tr><td><br />Attention</td><td>—</td><td>—</td><td>—</td><td>—</td></tr><tr><td><br />Pool3</td><td>2×2</td><td>2</td><td>10×10</td><td>26×26</td></tr><tr><td><br />Conv9</td><td>3×3×256×256</td><td>1</td><td>8×8</td><td>24×24</td></tr><tr><td><br />Conv10</td><td>3×3×256×512</td><td>1</td><td>6×6</td><td>22×22</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="62">本文所提出的双重注意力机制分别对通道维度和空间维度上的语义特征进行建模, 如图2所示。通道依赖性由通道注意模块得到, 由于高级特征图的每个通道可以被视为对特定类的响应, 并且不同的语义特征响应彼此之间相互关联, 利用通道特征图之间的相互依赖关系, 如可以通过增强特征图之间的相互依赖关系去提高特征的表达能力;对于空间注意模块, 通过引入自我注意机制来建立特征图中任意两个位置之间的联系, 对于某个位置的特征可以通过加权求和所有位置的特征信息来更新, 最后把通道注意特征与空间位置特征进行元素相加来进一步加强网络的特征表征能力。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906017_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 双重注意力模块" src="Detail/GetImg?filename=images/JSJY201906017_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 双重注意力模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906017_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Dual attention module</p>

                </div>
                <div class="p1">
                    <p id="64">具体而言, 通道注意模块是以特征图为单位, 对每一个通道都配一个权值, 如图2中通道注意模块所示。它由一个多层感知器实现, 输入特征<b><i>M</i></b>∈<b>R</b><sup><i>w</i>×<i>h</i>×<i>d</i></sup>首先经过一个全局平均池化层得到一个特征向量<b><i>m</i></b>= (<i>m</i><sub>1</sub>, <i>m</i><sub>2</sub>, …, <i>m</i><sub><i>d</i></sub>) 作为全连接层的输入, 其中<i>m</i><sub><i>i</i></sub>∈<b>R</b>, 经过一个隐藏层, 再经过一个非线性激活函数Sigmoid层得到输出向量<i>β</i>= (<i>β</i><sub>1</sub>, <i>β</i><sub>2</sub>, …, <i>β</i><sub><i>d</i></sub>) , 其中<i>β</i><sub><i>i</i></sub>∈<b>R</b>, 然后将激活向量<i>β</i>与输入特征<b><i>M</i></b>进行元素相乘, 最终生成通道注意特征图<b><i>U</i></b>∈<b>R</b><sup><i>w</i>×<i>h</i>×<i>d</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="65">对于空间注意模块来说, 它是以特征图中的每个像素点为单位的, 对特征图中的每个像素点都配一个权重, 以便建立空间信息的结构依赖关系去增强模型的特征表达能力。如图2中空间注意模块所示, 给定一个输入特征图<b><i>A</i></b>∈<b>R</b><sup><i>d</i>×<i>W</i>×<i>H</i></sup>, 首先经过三个变换函数<i>h</i>、<i>f</i>、<i>g</i>得到变换后的特征图<b><i>B</i></b>、<b><i>C</i></b>、<b><i>D</i></b>, 其中{<b><i>B</i></b>, <b><i>C</i></b>, <b><i>D</i></b>}∈<b>R</b><sup><i>d</i>×<i>W</i>×<i>H</i></sup>, 变换函数包括1×1的卷积层, BN层和ReLU层, 然后把<b><i>C</i></b>、<b><i>D</i></b>变换为<b>R</b><sup><i>d</i>×<i>WH</i></sup>, 用<b><i>C</i></b>的转置乘<b><i>D</i></b>, 再经过一个Sigmoid激活函数计算得到空间注意图, 计算式为:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo>⋅</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><mi>h</mi></mrow></munderover><mrow><mi>exp</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">C</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo>⋅</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中<i>s</i><sub><i>i</i>, <i>j</i></sub> 表示第<i>i</i>个区域与第<i>j</i>个区域之间的权重, 与此同时, 特征图<b><i>B</i></b>也变换为<b>R</b><sup><i>d</i>×<i>WH</i></sup>, 然后再将<b><i>B</i></b>与<b><i>S</i></b>的转置进行矩阵相乘并且将得到的结果重新变换为<b>R</b><sup><i>d</i>×<i>W</i>×<i>H</i></sup>, 由式 (4) 计算得到最终的空间注意特征输出:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>W</mi><mi>Η</mi></mrow></munderover><mi>s</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中, <i>λ</i>是一个可学习的变量因子, 初始化为0, 然后渐渐赋予更大的权重, 这样可以允许网络首先学习简单的任务然后再慢慢增加学习任务的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="70">最终双重注意力机制的输出是将通道注意特征和空间注意特征进行元素相加, 以便获得更好的特征表征信息。</p>
                </div>
                <div class="p1">
                    <p id="71"><b><i>O</i></b><sub><i>i</i></sub>=<b><i>U</i></b><sub><i>i</i></sub>+<b><i>V</i></b><sub><i>i</i></sub>; <i>i</i>=1, 2, …, <i>d</i>      (5) </p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">1.4 <b>数据集和网络训练细节</b></h4>
                <div class="p1">
                    <p id="73">本文的网络是在视频目标检测数据集<i>ILSVRC</i>上使用彩色图像离线训练的, 其中包含了4 500个视频序列且有大约有130万个人工标注的边界框, 最近被广泛应用在跟踪领域。采用动量为0.9的随机梯度下降最优化网络并设置权重衰减为0.000 5, 学习率以指数衰减方式从10<sup>-2</sup>到10<sup>-5</sup>, 训练周期大约为65个周期且每次小批量训练样本数为16。最后为了解决尺度变换问题, 在搜索图像上采用三个不同的尺度缩放因子{<i>q</i><sup><i>s</i></sup>|<i>q</i>=1.025, <i>s</i>=-1, 0, 1}去搜索图像, 通过一个因子为0.35的线性插值去更新当前目标的尺度。</p>
                </div>
                <div class="p1">
                    <p id="74">本文所提出的网络模型是在TensorFlow 1.4.1框架<citation id="158" type="reference"><link href="136" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>上训练的, 且实验评估是在一台配置为英特尔i7- 8700K CPU 和显卡GTX1080Ti电脑上进行的, 平均帧率是40 frame/s。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">2 实验结果及分析</h3>
                <div class="p1">
                    <p id="76">为了评估本文所提算法的有效性, 在三个具有挑战性并且被广泛使用的视频基准库上进行实验, 分别是:<i>OTB</i>2013<citation id="159" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、<i>OTB</i>100<citation id="160" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、2017年视觉目标跟踪库 (2017 <i>Visual</i>-<i>Object</i>-<i>Tracking challenge</i>, <i>VOT</i>2017) <citation id="161" type="reference"><link href="142" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>实时挑战, 并且与基准算法<i>SiamFC</i>和几个经典的算法进行对比实验。</p>
                </div>
                <div class="p1">
                    <p id="77">在本文实验中, 选择了三个具有代表性的跟踪器进行对比, 包括本文算法基准<i>SiamFC</i>和经典的相关滤波算法判别尺度空间跟踪器 (<i>Discriminative Scale Space Tracker</i>, <i>DSST</i>) <citation id="162" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、核化相关滤波跟踪 (<i>Kernelized Correlation Filter</i>, <i>KCF</i>) <citation id="163" type="reference"><link href="110" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、空间正则判别相关滤波跟踪 (<i>Spatially Regularized Discriminative Correlation Filter</i>, <i>SRDCF</i>) <citation id="164" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">2.1 <b>在</b><i>OTB</i>2013<b>和</b><i>OTB</i>100<b>上的评估</b></h4>
                <div class="p1">
                    <p id="79"><i>OTB</i>2013和<i>OTB</i>100是视觉跟踪领域广泛使用的基准库, 分别包含了51个和100个人工标注的视频帧, 并且包含了11个不同的属性, 例如尺度变换、光照变化、平面内旋转、快速运动等。算法的性能由两个性能指标衡量:成功率和精确率。成功率表明重合率得分超过某个阈值的帧的个数占视频总帧数的百分比, 精确率表明了中心位置误差在一个特定阈值内的视频帧数占总帧数的百分比。重合率计算如下:</p>
                </div>
                <div class="p1">
                    <p id="80"><i>os</i>=|<b><i>G</i></b><sub>rec</sub>∩<b><i>P</i></b><sub>rec</sub>|/|<b><i>G</i></b><sub>rec</sub>∪<b><i>P</i></b><sub>rec</sub>|      (6) </p>
                </div>
                <div class="p1">
                    <p id="81">其中, <b><i>G</i></b><sub>rec</sub>、<b><i>P</i></b><sub>rec</sub>分别表示人工标定的边界框和跟踪器预测的边界框。</p>
                </div>
                <div class="p1">
                    <p id="82">OTB2013和OTB100基准库上不同算法的成功率对比结果如图3所示。从图3 (a) 可以看出, 在视频数据集OTB2013上, 本文算法DASiam能够排在第一并且比基准算法SiamFC提高了3.5个百分点 , 显著地提高了跟踪性能;由图3 (b) 可以看出, 在更具有挑战性的100个视频数据集OTB100上, DASiam也比SiamFC高出了3个百分点, 很好地验证了本文跟踪算法的有效性。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906017_083.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同视频基准库上成功率对比" src="Detail/GetImg?filename=images/JSJY201906017_083.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同视频基准库上成功率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906017_083.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Comparison of success rates on different video benchmarks</p>

                </div>
                <h4 class="anchor-tag" id="84" name="84">2.2 <b>基于</b>OTB100<b>属性的分析</b></h4>
                <div class="p1">
                    <p id="85">本文在<i>OTB</i>100上对所提出的跟踪器进行了11种不同属性的对比分析实验。图4 (<i>a</i>) 、4 (<i>b</i>) 分别展示了当目标经历了运动模糊和平面内旋转两种属性的成功率, 这两种属性表明了跟踪的目标经历了比较大的外观变化, 与给定的第一帧的目标外观变化差别较大。由图4可以看出, 在运动模糊的属性下本文算法取得了62.4%的得分, 比基准算法<i>SiamFC</i>高出7.4个百分点;同时, 本文算法在平面内旋转的属性下也取得了较好的表现。在目标经历了运动模糊或者旋转导致目标外观发生变化的时候, <i>SiamFC</i>的跟踪成功率得分比较低, 表明该算法的鲁棒性较低;而本文的<i>DASiam</i>加入了双重注意力机制能够很好地建立通道和空间的联系, 充分利用目标的有用信息而抑制周围的干扰因素, 从而提升了算法的鲁棒性, 并且充分利用深度网络的优势进一步提取表达能力更强的特征。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906017_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 OTB100上不同属性的成功率对比" src="Detail/GetImg?filename=images/JSJY201906017_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 <i>OTB</i>100上不同属性的成功率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906017_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Comparison of success rates of different attributes on OTB</i>100</p>

                </div>
                <h4 class="anchor-tag" id="87" name="87">2.3 <b>在</b><i>VOT</i>2017<b>实时挑战上的结果</b></h4>
                <div class="p1">
                    <p id="88">在<i>VOT</i>2017数据库中包含了60个更精细的人工标注的视频序列并且更具有挑战性, 最近几年在跟踪领域中也被广泛采用, 除此之外, <i>VOT</i>2017还包含了一项新的实时实验, 要求所有的跟踪器必须以超过实时的25 <i>frame</i>/<i>s</i>的速度处理视频流, 这就意味着跟踪器如果达不到实时, 评估器将以上一帧的预测结果作为当前帧的跟踪结果, 这就很容易导致跟踪器跟踪失败。图5给出了本文算法<i>DASiam</i>和其他5个实时的跟踪器在<i>VOT</i>2017实时实验上的排名, 其中基准<i>SiamFC</i>是2017年实时挑战赛上的冠军。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 VOT2017实时平均期望重叠率排名" src="Detail/GetImg?filename=images/JSJY201906017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>VOT</i>2017实时平均期望重叠率排名  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906017_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Average expected overlapping ranking for VOT</i>2017 <i>real</i>-<i>time challenge</i></p>

                </div>
                <div class="p1">
                    <p id="90">由图5可以看出, 本文算法的性能已经超过了<i>SiamFC</i>排到了第一, 具体而言超过基准<i>SiamFC</i>大约0.8个百分点, 更好地验证了本文的双重注意力机制孪生网络能够很好地适用于基于孪生网络的跟踪器。</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="92">本文在全卷积孪生网络 (<i>SiamFC</i>) 跟踪的基础上改进了用于特征提取的卷积神经网络, 提出了双重注意力机制孪生网络跟踪器 (<i>DASiam</i>) , 通过在修改后的<i>VGG</i>网络中嵌入了通道注意模块和空间注意模块提升网络模型的判别能力, 去解决目标外观变化等问题。本文方法能够在跟踪标准测试集<i>OTB</i>2013和<i>OTB</i>100上取得很有竞争力的实验结果, 在<i>VOT</i>2017实时挑战上的性能表现甚至超过了2017年实时的冠军<i>SiamFC</i>, 表明本文方法能够在实际场景中, 如无人驾驶、智能安防等, 可以实现更好的跟踪效果以满足实际要求。但是, 本文方法对于强烈光照变化、尺度变化较大等其他干扰因素出现时, 跟踪结果不太理想, 接下来将针对强烈光照变化、尺度变化较大等问题进行进一步研究改进。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="110">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">

                                <b>[1]</b>HENRIQUES J F, CASEIRO R, MARTINS P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="112">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201808007&amp;v=MDM4ODRSN3FmWnVac0Z5L2hXci9OTHo3QmQ3RzRIOW5NcDQ5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>熊昌镇, 车满强, 王润玲.基于稀疏卷积特征和相关滤波的实时视觉跟踪算法[J].计算机应用, 2018, 38 (8) :2175-2179, 2223. (XIONG C Z, CHE M Q, WANG R L.Real-time visual tracking algorithm based on correlation filters and sparse convolutional features[J].Journal of Computer Applications, 2018, 38 (8) :2175-2179, 2223.) 
                            </a>
                        </p>
                        <p id="114">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806038&amp;v=MzE1MzdCdEdGckNVUjdxZlp1WnNGeS9oV3IvTkx6N0JkN0c0SDluTXFZOUdiSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>樊佳庆, 宋慧慧, 张开华.通道稳定性加权补充学习的实时视觉跟踪算法[J].计算机应用, 2018, 38 (6) :1751-1754. (FAN JQ, SONG H H, ZHANG K H.Real-time visual tracking algorithm via channel stability weighted complementary learning[J].Journal of Computer Applications, 2018, 38 (6) :1751-1754.) 
                            </a>
                        </p>
                        <p id="116">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201705045&amp;v=MTIxNDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXci9OTHo3QmQ3RzRIOWJNcW85QllZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>朱明敏, 胡茂海.基于相关滤波器的长时视觉目标跟踪方法[J].计算机应用, 2017, 37 (5) :1466-1470. (ZHU M M, HU M H.Long-term visual object tracking algorithm based on correlation filter[J].Journal of Computer Applications, 2017, 37 (5) :1466-1470.) 
                            </a>
                        </p>
                        <p id="118">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">

                                <b>[5]</b>BERTINETTO L, VALMADRE J, HENRIQUES J F, et al.Fullyconvolutional Siamese networks for object tracking[C]//ECCV2016:Proceedings of the 2016 European Conference on Computer Vision, LNCS 9914.Cham:Springer, 2016:850-865.
                            </a>
                        </p>
                        <p id="120">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="122">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Siamese Instance Search for Tracking">

                                <b>[7]</b>TAO R, GAVVES E, SMEULDERS A W M.Siamese instance search for tracking[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1420-1429.
                            </a>
                        </p>
                        <p id="124">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Policies for Adaptive Tracking With Deep Feature Cascades">

                                <b>[8]</b>HUANG C, LUCEY S, RAMANAN D.Learning policies for adaptive tracking with deep feature cascades[C]//Proceedings of the2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:105-114.
                            </a>
                        </p>
                        <p id="126">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Representation Learning for Correlation Filter Based Tracking">

                                <b>[9]</b>VALMADRE J, BERTINETTO L, HENRIQUES J, et al.End-toend representation learning for correlation filter based tracking[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5000-5008.
                            </a>
                        </p>
                        <p id="128">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Dynamic Siamese Network for Visual Object Tracking">

                                <b>[10]</b>GUO Q, FENG W, ZHOU C, et al.Learning dynamic Siamese network for visual object tracking[C]//Proceedings of the 2017IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:1781-1789.
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[11]</b>HE K M, ZHANG X Y, REN S Q, et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[12]</b>SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-10-16].http://www.cs.cmu.edu/~jeanoh/16-785/papers/simonyan-iclr2015-vgg.pdf.
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:Large scale visual recognition challenge">

                                <b>[13]</b>RUSSAKOVSKY O, DENG J, SU H, et al.Image Net large scale visual recognition challenge[J].International Journal of Computer Vision, 2015, 115 (3) :211-252.
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tensor Flow:a system for large-scale machine learning">

                                <b>[14]</b>ABADI M, BARHAM P, CHEN J M, et al.Tensor Flow:a system for large-scale machine learning[C]//OSDI 2016:Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation.Berkeley, CA:USENIX Association, 2016:265-283.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">

                                <b>[15]</b>WU Y, LIM J, YANG M H.Online object tracking:a benchmark[C]//CVPR 2013:Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2013:2411-2418.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[16]</b>WU Y, LIM J, YANG M H.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1834-1848.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Visual Object Tracking VOT2017 Challenge Results">

                                <b>[17]</b>KRISTAN M, LEONARDIS A, MATAS J, et al.The visual object tracking VOT2017 challenge results[C]//ICCVW 2017:Proceedings of the 2017 IEEE International Conference on Computer Vision Workshop.Piscataway, NJ:IEEE, 2017:1949-1972.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">

                                <b>[18]</b>DANELLJAN M, HGER G, KHAN F, et al.Accurate scale estimation for robust visual tracking[C]//Proceedings of the 2014British Machine Vision Conference.Durham, UK:BMVA Press, 2014:65.1-65.11.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[19]</b>DANELLJAN M, HAGER G, KHAN F S, et al.Learning spatially regularized correlation filters for visual tracking[C]//ICCV2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:4310-4318.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906017" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906017&amp;v=MDg1ODF6N0JkN0c0SDlqTXFZOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3IvTkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
