<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637138991434947500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201903003%26RESULT%3d1%26SIGN%3dEuqs0QuwvXeKTR2b%252b5MbzG4jOtU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903003&amp;v=MTY0MTMzenFxQnRHRnJDVVI3cWZadVpwRmlEbVU3L05MejdCZDdHNEg5ak1ySTlGWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="1 相关基础知识 ">1 相关基础知识</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="1.1 AdaBoost&lt;b&gt;算法简介&lt;/b&gt;">1.1 AdaBoost<b>算法简介</b></a></li>
                                                <li><a href="#77" data-title="1.2 &lt;b&gt;代价敏感集成算法&lt;/b&gt;">1.2 <b>代价敏感集成算法</b></a></li>
                                                <li><a href="#106" data-title="1.3 SMOTE&lt;b&gt;算法简介&lt;/b&gt;">1.3 SMOTE<b>算法简介</b></a></li>
                                                <li><a href="#111" data-title="1.4 &lt;b&gt;核距离&lt;/b&gt;">1.4 <b>核距离</b></a></li>
                                                <li><a href="#118" data-title="1.5 NKSMOTE&lt;b&gt;算法简介&lt;/b&gt;">1.5 NKSMOTE<b>算法简介</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#132" data-title="2 NIBoost算法基本原理 ">2 NIBoost算法基本原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#134" data-title="2.1 NIBoost&lt;b&gt;原理介绍&lt;/b&gt;">2.1 NIBoost<b>原理介绍</b></a></li>
                                                <li><a href="#164" data-title="2.2 NIBoost&lt;b&gt;的时间复杂度分析&lt;/b&gt;">2.2 NIBoost<b>的时间复杂度分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#166" data-title="3 实验设计与结果分析 ">3 实验设计与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#168" data-title="3.1 &lt;b&gt;数据集&lt;/b&gt;">3.1 <b>数据集</b></a></li>
                                                <li><a href="#171" data-title="3.2 &lt;b&gt;评价标准&lt;/b&gt;">3.2 <b>评价标准</b></a></li>
                                                <li><a href="#192" data-title="3.3 &lt;b&gt;实验结果及分析&lt;/b&gt;">3.3 <b>实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#197" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#170" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;非平衡数据集的基本信息&lt;/b&gt;"><b>表</b>1 <b>非平衡数据集的基本信息</b></a></li>
                                                <li><a href="#173" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;混淆矩阵&lt;/b&gt;"><b>表</b>2 <b>混淆矩阵</b></a></li>
                                                <li><a href="#199" data-title="&lt;b&gt;表&lt;/b&gt;3 4&lt;b&gt;个算法在&lt;/b&gt;5&lt;b&gt;个数据集上的&lt;/b&gt;F-value&lt;b&gt;值&lt;/b&gt;、G-mean&lt;b&gt;值&lt;/b&gt;、AUC&lt;b&gt;值的比较&lt;/b&gt;"><b>表</b>3 4<b>个算法在</b>5<b>个数据集上的</b>F-value<b>值</b>、G-mean<b>值</b>、AUC<b>值的比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="215">


                                    <a id="bibliography_1" title="WEISS G M, ZADROZNY B, SAAR M.Guest editorial:special issue on utility-based data mining[J].Data Mining and Knowledge Discovery, 2008, 17 (2) :129-135." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD00000971069&amp;v=MjkxNjhaK1p1Rmkva1c3eklJRnM9TmozYWFyTzRIdEhNcG9oRVpPMEdZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3Fk&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        WEISS G M, ZADROZNY B, SAAR M.Guest editorial:special issue on utility-based data mining[J].Data Mining and Knowledge Discovery, 2008, 17 (2) :129-135.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_2" title="del CASTILLO M D, SERRANO J I.A multistrategy approach for digital text categorization from imbalanced documents[J].ACMSIGKDD Explorations Newsletter, 2004, 6 (1) :70-79." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000085401&amp;v=MDA5MDBaT01LQ0h3NG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSktGMFVhaGM9TmlmSVk3SzdIdGpOcjQ5Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        del CASTILLO M D, SERRANO J I.A multistrategy approach for digital text categorization from imbalanced documents[J].ACMSIGKDD Explorations Newsletter, 2004, 6 (1) :70-79.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_3" title="WEI W, LI J, CAO L.Effective detection of sophisticated online banking fraud on extremely imbalanced data[J].World Wide Web, 2013, 16 (4) :449-475." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13062800015698&amp;v=MzI1OTFLQ25VeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSktGMFVhaGM9Tmo3QmFySzdIdGZPcDQ5RlpPbw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        WEI W, LI J, CAO L.Effective detection of sophisticated online banking fraud on extremely imbalanced data[J].World Wide Web, 2013, 16 (4) :449-475.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_4" title="江颉, 王卓芳, GONG R S, 等.不平衡数据分类方法及其在入侵检测中的应用研究[J].计算机科学, 2013, 40 (4) :131-135. (JIANG J, WANG Z F, GONG R S, et al.Imbalanced data classification method and its application research for intrusion detection[J].Computer Science, 2013, 40 (4) :131-135.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201304029&amp;v=MTg3NjJDVVI3cWZadVpwRmlEbVU3L05MejdCYjdHNEg5TE1xNDlIYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        江颉, 王卓芳, GONG R S, 等.不平衡数据分类方法及其在入侵检测中的应用研究[J].计算机科学, 2013, 40 (4) :131-135. (JIANG J, WANG Z F, GONG R S, et al.Imbalanced data classification method and its application research for intrusion detection[J].Computer Science, 2013, 40 (4) :131-135.) 
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_5" title="KUBAT M, HOLTE RC, MATWIN S.Machine learning for the detection of oil spills in satellite radar images[J].Machine Learning, 1998, 30 (2) :195-215." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339718&amp;v=MDE2NTVrVzd6SUlGcz1OajdCYXJPNEh0SE5ySXhNWStvSFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rmkv&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        KUBAT M, HOLTE RC, MATWIN S.Machine learning for the detection of oil spills in satellite radar images[J].Machine Learning, 1998, 30 (2) :195-215.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_6" title="SCHAEFER G, NAKASHIMA T.Strategies for addressing class imbalance in ensemble classification of thermography breast cancer features[C]//Proceedings of the 2015 IEEE Congress on Evolutionary Computation.Piscataway, NJ:IEEE, 2015:2362-2367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Strategies for addressing class imbalance in ensemble classification of thermography breast cancer features">
                                        <b>[6]</b>
                                        SCHAEFER G, NAKASHIMA T.Strategies for addressing class imbalance in ensemble classification of thermography breast cancer features[C]//Proceedings of the 2015 IEEE Congress on Evolutionary Computation.Piscataway, NJ:IEEE, 2015:2362-2367.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                    CHAWLA N V, BOWYER K W, HALL L O.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.</a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_8" title="QIAN Y, LIANG Y, LI M.A resampling ensemble algorithm for classification of imbalance problems[J].Neurocomputing, 2014, 143:57-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700313588&amp;v=MTc0MTBmT2ZiSzhIOURNcUk5Rlorb01DWFF4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpKS0YwVWFoYz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        QIAN Y, LIANG Y, LI M.A resampling ensemble algorithm for classification of imbalance problems[J].Neurocomputing, 2014, 143:57-67.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_9" title="DOUZAS G, BACAO F, LAST F.Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE[J].Information Sciences, 2018, 465:1-20." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE">
                                        <b>[9]</b>
                                        DOUZAS G, BACAO F, LAST F.Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE[J].Information Sciences, 2018, 465:1-20.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_10" title="GALAR M, FERNANDEZ A, BARRENECHEA E.Orderingbased pruning for improving the performance of ensembles of classifiers in the framework of imbalanced datasets[J].Information Sciences, 2016, 354:178-196." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ordering-based pruning for improving the performance of ensembles of classifiers in the framework of imbalanced data sets">
                                        <b>[10]</b>
                                        GALAR M, FERNANDEZ A, BARRENECHEA E.Orderingbased pruning for improving the performance of ensembles of classifiers in the framework of imbalanced datasets[J].Information Sciences, 2016, 354:178-196.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_11" title="ZHANG Y, ZHANG D, MI G.Using ensemble methods to deal with imbalanced data in predicting protein-protein interactions[J].Computational Biology and Chemistry, 2012, 36 (2) :36-41." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300467873&amp;v=MDYzNTlpclJkR2VycVFUTW53WmVadEZpbmxVcmpKS0YwVWFoYz1OaWZPZmJLN0h0RE5ySTlGWU8wSUJIczZvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        ZHANG Y, ZHANG D, MI G.Using ensemble methods to deal with imbalanced data in predicting protein-protein interactions[J].Computational Biology and Chemistry, 2012, 36 (2) :36-41.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                    KIM M J, KANG D K, KIM H B.Geometric mean based boosting algorithm with over-sampling to resolve data imbalance problem for值的比较ms on five datasets NBst Rare Boost NIBoost6 0.548 6 0.644 62 0.674 6 0.741 08 0.948 8 0.965 02 0.616 6 0.659 48 0.746 2 0.771 47 0.707 0 0.756 32 0.676 8 0.879 82 0.751 1 0.863 44 0.965 6 0.977 08 0.695 2 0.734 66 0.812 4 0.852 80 0.780 2 0.861 56 0.727 4 0.888 88 0.776 7 0.865 04 0.965 6 0.977 00 0.711 1 0.735 24 0.828 7 0.861 42 0.801 9 0.865 5bankruptcy prediction[J].Expert Systems with Applications, 2015, 42 (3) :1074-1082.</a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_13" title="李雄飞, 李军, 董元方, 等.一种新的不平衡数据学习算法PCBoost[J].计算机学报, 2012, 35 (2) :202-209. (LI X F, LI J, DONG Y F, et al.A new learning algorithm for imbalanced data PCBoost[J].Chinese Journal of Computers, 2012, 35 (2) :202-209.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201202003&amp;v=Mjc2MjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbVU3L05MejdCZHJHNEg5UE1yWTlGWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        李雄飞, 李军, 董元方, 等.一种新的不平衡数据学习算法PCBoost[J].计算机学报, 2012, 35 (2) :202-209. (LI X F, LI J, DONG Y F, et al.A new learning algorithm for imbalanced data PCBoost[J].Chinese Journal of Computers, 2012, 35 (2) :202-209.) 
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_14" title="HE H, ZHANG W, ZHANG S.A novel ensemble method for credit scoring:adaption of different imbalance ratios[J].Expert Systems with Applications, 2018, 98:105-117." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A novel ensemble method for credit scoring:adaption of different imbalance ratios.&amp;quot;">
                                        <b>[14]</b>
                                        HE H, ZHANG W, ZHANG S.A novel ensemble method for credit scoring:adaption of different imbalance ratios[J].Expert Systems with Applications, 2018, 98:105-117.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_15" title="付忠良.多标签代价敏感分类集成学习算法[J].自动化学报, 2014, 40 (6) :1075-1085. (FU Z L.Cost-sensitive ensemble learning algorithm for multi-label classification problems[J].Acta Automatica Sinica, 2014, 40 (6) :1075-1085.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201406005&amp;v=MjA1MzdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRG1VNy9OS0NMZlliRzRIOVhNcVk5Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        付忠良.多标签代价敏感分类集成学习算法[J].自动化学报, 2014, 40 (6) :1075-1085. (FU Z L.Cost-sensitive ensemble learning algorithm for multi-label classification problems[J].Acta Automatica Sinica, 2014, 40 (6) :1075-1085.) 
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_16" title="FAN W, STOLFO S J, ZHANG J, et al.Ada Cost:misclassification cost-sensitive boosting[C]//ICML&#39;99:Proceedings of the 16th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann Publishers, 1999:97-105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adacost:Misclassification Cost-Sensitive Boosting">
                                        <b>[16]</b>
                                        FAN W, STOLFO S J, ZHANG J, et al.Ada Cost:misclassification cost-sensitive boosting[C]//ICML&#39;99:Proceedings of the 16th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann Publishers, 1999:97-105.
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_17" title="SUN Y, KAMEL M S, WONG A K C.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739373&amp;v=MjE1NDhLRjBVYWhjPU5pZk9mYks3SHRETnFZOUZZK2dHRDNzNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        SUN Y, KAMEL M S, WONG A K C.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378.
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_18" title="JOSHI M V, KUMAR V, AGARWAL R.Evaluating boosting algorithms to classify rare classes:comparison and improvements[C]//Proceedings of the 2001 IEEE International Conference on Data Mining.Piscataway, NJ:IEEE, 2001:257-264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluating boosting algo-rithms to classify rare classes:comparison and improvements">
                                        <b>[18]</b>
                                        JOSHI M V, KUMAR V, AGARWAL R.Evaluating boosting algorithms to classify rare classes:comparison and improvements[C]//Proceedings of the 2001 IEEE International Conference on Data Mining.Piscataway, NJ:IEEE, 2001:257-264.
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_19" title="SIERS M J, ISLAM M Z.Software defect prediction using a cost sensitive decision forest and voting, and a potential solution to the class imbalance problem[J].Information Systems, 2015, 51:62-71." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122600211100&amp;v=MDY0ODM1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpKS0YwVWFoYz1OaWZPZmJLOUg5UE9xWTlGWnVvT0RYdw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        SIERS M J, ISLAM M Z.Software defect prediction using a cost sensitive decision forest and voting, and a potential solution to the class imbalance problem[J].Information Systems, 2015, 51:62-71.
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_20" title="ZHANG Y, WANG D.A cost-sensitive ensemble method for classimbalanced data sets[J].Abstract and Applied Analysis, 2013, 2013:Article ID 196256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Cost-Sensitive Ensemble Method for Class-Imbalanced Datasets">
                                        <b>[20]</b>
                                        ZHANG Y, WANG D.A cost-sensitive ensemble method for classimbalanced data sets[J].Abstract and Applied Analysis, 2013, 2013:Article ID 196256.
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_21" title="AODHA O M, BROSTOW G J.Revisiting example dependent cost-sensitive learning with decision trees[C]//Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2013:193-200." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Revisiting example dependent cost-sensitive learning with decision trees">
                                        <b>[21]</b>
                                        AODHA O M, BROSTOW G J.Revisiting example dependent cost-sensitive learning with decision trees[C]//Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2013:193-200.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_22" title="GALAR M, FERNANDEZ A, BARRENECHEA E.EUSBoost:enhancing ensembles for highly imbalanced data-sets by evolutionary undersampling[J].Pattern Recognition, 2013, 46 (12) :3460-3471." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300161907&amp;v=Mjc1OTVsVXJqSktGMFVhaGM9TmlmT2ZiSzlIOVBPckk5RlplME9CWHcrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        GALAR M, FERNANDEZ A, BARRENECHEA E.EUSBoost:enhancing ensembles for highly imbalanced data-sets by evolutionary undersampling[J].Pattern Recognition, 2013, 46 (12) :3460-3471.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_23" title="ROY N K S, ROSSI B.Cost-sensitive strategies for data imbalance in bug severity classification:experimental results[C]//Proceedings of the 2017 43rd Euromicro Conference on Software Engineering and Advanced Applications.Washington, DC:IEEE Computer Society, 2017:426-429." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cost-sensitive strategies for data imbalance in bug severity classification:experimental results">
                                        <b>[23]</b>
                                        ROY N K S, ROSSI B.Cost-sensitive strategies for data imbalance in bug severity classification:experimental results[C]//Proceedings of the 2017 43rd Euromicro Conference on Software Engineering and Advanced Applications.Washington, DC:IEEE Computer Society, 2017:426-429.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_24" title="LEE H K, KIM S B.An overlap-sensitive margin classifier for imbalanced and overlapping data[J].Expert Systems with Applications, 2018, 98:72-83." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An overlap-sensitive margin classifier for imbalanced and overlapping data">
                                        <b>[24]</b>
                                        LEE H K, KIM S B.An overlap-sensitive margin classifier for imbalanced and overlapping data[J].Expert Systems with Applications, 2018, 98:72-83.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(03),629-633 DOI:10.11772/j.issn.1001-9081.2018071598            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>新的基于代价敏感集成学习的非平衡数据集分类方法NIBoost</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%8E%89&amp;code=09161776&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王莉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%BA%A2%E6%A2%85&amp;code=17456369&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈红梅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%94%9F%E6%AD%A6&amp;code=41275237&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王生武</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0218487&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南交通大学信息科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现实生活中存在大量的非平衡数据, 大多数传统的分类算法假定类分布平衡或者样本的错分代价相同, 因此在对这些非平衡数据进行分类时会出现少数类样本错分的问题。针对上述问题, 在代价敏感的理论基础上, 提出了一种新的基于代价敏感集成学习的非平衡数据分类算法——NIBoost (New Imbalanced Boost) 。首先, 在每次迭代过程中利用过采样算法新增一定数目的少数类样本来对数据集进行平衡, 在该新数据集上训练分类器;其次, 使用该分类器对数据集进行分类, 并得到各样本的预测类标及该分类器的分类错误率;最后, 根据分类错误率和预测的类标计算该分类器的权重系数及各样本新的权重。实验采用决策树、朴素贝叶斯作为弱分类器算法, 在UCI数据集上的实验结果表明, 当以决策树作为基分类器时, 与RareBoost算法相比, F-value最高提高了5.91个百分点、G-mean最高提高了7.44个百分点、AUC最高提高了4.38个百分点;故该新算法在处理非平衡数据分类问题上具有一定的优势。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非平衡数据集;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">代价敏感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%87%E9%87%87%E6%A0%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">过采样;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Adaboost%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Adaboost算法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王莉 (1992—) , 女, 山东菏泽人, 硕士研究生, CCF会员, 主要研究方向:数据挖掘;;
                                </span>
                                <span>
                                    *陈红梅 (1971—) , 女, 四川成都人, 教授, 博士, CCF会员, 主要研究方向:智能信息处理、数据挖掘;电子邮箱hmchen@swjtu.edu.cn;
                                </span>
                                <span>
                                    王生武 (1995—) , 男, 安徽芜湖人, 硕士研究生, CCF会员, 主要研究方向:数据挖掘。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-31</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61572406);</span>
                    </p>
            </div>
                    <h1><b>NIBoost: new imbalanced dataset classification method based on cost sensitive ensemble learning</b></h1>
                    <h2>
                    <span>WANG Li</span>
                    <span>CHEN Hongmei</span>
                    <span>WANG Shengwu</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Technology, Southwest Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The problem of misclassification of minority class samples appears frequently when classifying massive amount of imbalanced data in real life with traditional classification algorithms, because most of these algorithms only suit balanced class distribution or samples with same misclassification cost. To overcome this problem, a classification algorithm for imbalanced dataset based on cost sensitive ensemble learning and oversampling — New Imbalanced Boost (NIBoost) was proposed. Firstly, the oversampling algorithm was used to add a certain number of minority samples to balance the dataset in each iteration, and the classifier was trained on the new dataset. Secondly, the classifier was used to classify the dataset to obtain the predicted class label of each sample and the classification error rate of the classifier. Finally, the weight coefficient of the classifier and new weight of each sample were calculated according to the classification error rate and the predicted class labeles. Experimental results on UCI datasets with decision tree and Naive Bayesian used as weak classifier algorithm show that when decision tree was used as the base classifier of NIBoost, compared with RareBoost algorithm, the F-value is increased up to 5.91 percentage points, the G-mean is increased up to 7.44 percentage points, and the AUC is increased up to 4.38 percentage points. The experimental results show that the proposed algorithm has advantages on imbalanced data classification problem.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=imbalanced%20dataset&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">imbalanced dataset;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cost%20sensitive&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cost sensitive;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=over-sampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">over-sampling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Adaboost%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Adaboost algorithm;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Li, born in 1992, M. S. candidate. Her research interests include data mining.;
                                </span>
                                <span>
                                    CHEN Hongmei, born in 1971, Ph. D. , professor. Her research interests include intelligent information processing, data mining.;
                                </span>
                                <span>
                                    WANG Shengwu, born in 1995, M. S. candidate. His research interests include data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-07-31</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61572406);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="51">非平衡数据集的分类是指在各类样本数目不相等的情况下的分类问题<citation id="263" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在实际应用中非平衡数据集分布广泛, 比如医疗诊断、金融诈骗、网络入侵检测、电信用户检测、石油勘探等<citation id="276" type="reference"><link href="217" rel="bibliography" /><link href="219" rel="bibliography" /><link href="221" rel="bibliography" /><link href="223" rel="bibliography" /><link href="225" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。传统的分类算法大部分都以提高分类器总体分类精度为目标, 然而当传统的分类算法应用在非平衡数据集上时很容易造成少数类的错分。国内外学者通过不断的研究, 从算法层面和数据层面提出了许多改进优化的算法。数据层面主要是通过采样技术对数据集进行重构, 以降低非平衡度进而提高分类准确率。常见的采样方法有两种:过采样和欠采样。欠采样是通过减少多数类样本个数使数据集分布相对平衡。过采样是通过增加少数类样本个数使数据集分布相对平衡。Chawla等<citation id="264" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>于2002年提出合成少数类过采样技术 (Synthetic Minority Oversampling TEchnique, SMOTE) 算法, 其基本思想是在对样本集中所有的少数类样本, 分别求出与其距离较近的少数类样本, 然后在它们之间通过线性插值的方式来生成新的少数类样本, 以降低数据集的非平衡度。算法层面主要有集成学习、代价敏感和单类别学习等算法。集成学习法是通过对多个基分类器进行集成从而获得更强的泛化性能。Qian等<citation id="265" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出将采样方法与集成学习理论相结合, 其中运用的采样方法有欠采样和SMOTE算法。Douzas等<citation id="266" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出一种基于<i>k</i>-means聚类和SMOTE的简单、有效的过采样方法, 该方法设法仅在安全区域进行过采样来避免产生噪声。Galar等<citation id="267" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出在集成学习中通过对基分类器进行选择, 以提高其分类效率。Zhang等<citation id="268" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出通过将少数类与多数类多次组合, 得到不同的训练样本集, 然后再运用集成学习思想进行分类, 其结果提高了少数类的分类精度。Kim等<citation id="269" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出GMBoost (Geometric Mean based Boosting) 算法将弱分类器的评价标准由错误率改为多数类样本错误率与少数类样本错误率的几何均值, 然后与采样算法相结合, 以解决非平衡分类问题。李雄飞等<citation id="270" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出的PCBoost (Perturbation Correction Boosting) 算法, 该算法每次迭代开始时通过增加少数类样本的个数, 平衡训练数据集, 在子分类器形成后, 删除分类错误的合成样本, 然后通过多个基分类器的组合来解决非平衡的问题, 其方法具有很好的泛化能力。He等<citation id="271" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了一种新颖的集成模型, 该模型拓展了BalanceCascade方法, 且用随机森林 (Random Forest, RF) 和XGBoost (eXtreme Gradient Boosting) 两种树型分类器作为基分类器, 具有很高的性能和稳健性。代价敏感学习 (Cost Sensitive Learning, SCL) <citation id="272" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>用于处理非平衡分类的基本思路是在非平衡分类中正确识别出少数类样本的价值比正确识别出多数类样本的价值要高, 因此在分类中应赋予不同的损失代价。Fan等<citation id="273" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出的AdaCost算法是一种基于代价敏感的Boosting方法, 该算法给予分错的少数类样本与多数类样本不同的代价因子, 实验证明当以精度和召回率衡量分类结果时, AdaCost的性能优于其他版本的Boosting算法。Sun等<citation id="274" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>根据样本权重更新方式的不同, 提出三种不同的代价敏感Boosting算法。Joshi等<citation id="275" type="reference"><link href="249" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出的RareBoost算法按照每次迭代之后的TP/FP (True Positive/False Positive) 与TN/FN (True Negative/False Negative) 的比值分别为预测为正类的样本和预测为负类的样本进行权值更新。代价敏感学习也可以与传统的分类算法相结合, 进而使传统分类算法具有代价敏感性<citation id="277" type="reference"><link href="251" rel="bibliography" /><link href="253" rel="bibliography" /><link href="255" rel="bibliography" /><link href="257" rel="bibliography" /><link href="259" rel="bibliography" /><link href="261" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="52">本文提出一种将代价敏感集成学习算法与过采样技术相结合的非平衡数据分类算法——NIBoost (New Imbalanced Boost) 算法。其基本思想是首先采用GMBoost算法中多数类样本错误率与少数类样本错误率的几何均值作为分类器的评价标准。然后在每次迭代中融入NKSMOTE (New Kernel Synthetic Minority Over-Sampling TEchnique) 算法, 即通过增加少数类样本平衡数据集;然后在该数据集上训练分类器, 随后根据TP/FP与TN/FN的比值分别为预测为正类的样本和预测为负类的样本进行权值更新, 使得分类器在训练中更加关注被分错的样本。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag">1 相关基础知识</h3>
                <div class="p1">
                    <p id="54">本章对相关的基础知识及对本文所提NIBoost算法中迭代过程需要用到的过采样算法 (NKSMOTE) 的基本原理进行介绍。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">1.1 AdaBoost<b>算法简介</b></h4>
                <div class="p1">
                    <p id="56">AdaBoost基本思想是首先为数据集中的每个样本赋予一个初始权值, 然后在该数据集上训练基分类器, 最后根据基分类器的分类结果对样本权值进行更新, 使得错分样本在后续训练时可以得到更多的关注。最后组合分类器的判决结果是所有弱分类器判决结果的加权和。AdaBoost算法流程如下。</p>
                </div>
                <div class="p1">
                    <p id="57">输入:数据集<b><i>S</i></b>:{ (<b><i>x</i></b><sub>1</sub>, <i>y</i><sub>1</sub>) , (<b><i>x</i></b><sub>2</sub>, <i>y</i><sub>2</sub>) , …, (<b><i>x</i></b><sub><i>n</i></sub>, <i>y</i><sub><i>n</i></sub>) };迭代次数<i>T</i>, 弱学习算法WeakLearn;</p>
                </div>
                <div class="p1">
                    <p id="58">输出:组合分类器。</p>
                </div>
                <div class="p1">
                    <p id="59">1) 数据集<b><i>S</i></b>中样本权值的初始化<i>w</i><sub>1</sub> (<i>i</i>) =1/<i>n</i></p>
                </div>
                <div class="p1">
                    <p id="61">2) For <i>t</i>=1, 2, …, <i>T</i></p>
                </div>
                <div class="p1">
                    <p id="63">在数据集上训练得到弱分类器<i>h</i><sub><i>t</i></sub> (<i>x</i>) , 计算分类器的错误率<i>ε</i><sub><i>t</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="64"><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>i</mi></mrow></msub></mrow></math></mathml>; <i>h</i><sub><i>t</i></sub> (<i>x</i><sub><i>i</i></sub>) ≠<i>y</i><sub><i>i</i></sub>      (1) </p>
                </div>
                <div class="p1">
                    <p id="66">If <i>ε</i><sub><i>t</i></sub>&gt;0.5 then break</p>
                </div>
                <div class="p1">
                    <p id="67">按照式 (2) 更新样本权值。</p>
                </div>
                <div class="p1">
                    <p id="68"><i>w</i><sub><i>t</i>+1</sub> (<i>i</i>) = (<i>w</i><sub><i>t</i></sub> (<i>i</i>) exp (-<i>α</i><sub><i>t</i></sub><i>y</i><sub><i>i</i></sub><i>h</i><sub><i>t</i></sub> (<i>x</i><sub><i>i</i></sub>) ) ) /<i>Z</i><sub><i>t</i></sub>;</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mspace width="0.25em" /><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mi>ε</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>ε</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中<i>Z</i><sub><i>t</i></sub>是规范化因子</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ζ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mo>-</mo><mi>α</mi><msubsup><mrow></mrow><mi>t</mi><mrow></mrow></msubsup><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">End For</p>
                </div>
                <div class="p1">
                    <p id="73">3) 输出组合分类器。</p>
                </div>
                <div class="p1">
                    <p id="75"><i>H</i> (<i>x</i>) =sign<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">1.2 <b>代价敏感集成算法</b></h4>
                <div class="p1">
                    <p id="78">非平衡数据集的分类问题中, 少数类往往是分类的重点, 即正确识别出少数类比识别出多数类样本具有更大的价值。代价敏感学习就是基于该思想设计, 给错分的少数类样本赋予较高的错分代价。</p>
                </div>
                <div class="p1">
                    <p id="79">AdaCost算法是AdaBoost算法的一种变体, 即将AdaBoost算法中的样本权值的更新公式修改为:<i>w</i><sub><i>t</i>+1</sub> (<i>i</i>) =<i>w</i><sub><i>t</i></sub> (<i>i</i>) exp (-<i>α</i><sub><i>t</i></sub><i>y</i><sub><i>i</i></sub><i>h</i><sub><i>t</i></sub> (<i>x</i><sub><i>i</i></sub>) <i>β</i><sub>∂</sub>) , 其中<i>β</i><sub>∂</sub>为代价调整函数, <i>β</i><sub>∂</sub>=sgn (<i>h</i><sub><i>t</i></sub> (<i>x</i><sub><i>i</i></sub>) , <i>y</i><sub><i>i</i></sub>) 。若分类正确即sgn (<i>h</i><sub><i>t</i></sub> (<i>x</i><sub><i>i</i></sub>) , <i>y</i><sub><i>i</i></sub>) =1, <i>β</i><sub>+</sub>=-0.5<i>C</i><sub><i>i</i></sub>+0.5;若分类错误即sgn (<i>h</i><sub><i>t</i></sub> (<i>x</i><sub><i>i</i></sub>) , <i>y</i><sub><i>i</i></sub>) =-1, <i>β</i><sub>-</sub>=0.5<i>C</i><sub><i>i</i></sub>+0.5。<i>C</i><sub><i>i</i></sub>为错误分类第<i>i</i>个样本的代价。使得被错分的少数类样本权值增加的幅度大于被错分的多数类样本权值增加的幅度。但是存在的问题是在实际应用中代价函数不容易定义, 针对该问题许多研究者提出了新算法, 如Joshi等提出的RareBoost算法。</p>
                </div>
                <div class="p1">
                    <p id="80">RareBoost算法的基本思想是通过改变AdaBoost算法中的分类器权重<i>α</i><sub><i>t</i></sub>的计算方式, 来处理非平衡数据集分类问题。RareBoost算法首先按照式 (5) 和式 (6) 计算<i>α</i><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ρ</mi></msubsup></mrow></math></mathml>, <i>α</i><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup></mrow></math></mathml>, 然后按照式 (7) 对分类结果为正类的样本和分类结果为负类的样本分别进行权值更新。RareBoost算法的流程如下。</p>
                </div>
                <div class="p1">
                    <p id="83">输入:数据集<b><i>S</i></b>:{ (<b><i>x</i></b><sub>1</sub>, <i>y</i><sub>1</sub>) , (<b><i>x</i></b><sub>2</sub>, <i>y</i><sub>2</sub>) , …, (<b><i>x</i></b><sub><i>n</i></sub>, <i>y</i><sub><i>n</i></sub>) };迭代次数<i>T</i>;</p>
                </div>
                <div class="p1">
                    <p id="84">输出:组合分类器。</p>
                </div>
                <div class="p1">
                    <p id="85">1) 数据集<b><i>S</i></b>中样本权值的初始化<i>w</i><sub>1</sub> (<i>i</i>) =1/<i>n</i></p>
                </div>
                <div class="p1">
                    <p id="87">2) For <i>t</i>=1, 2, …, <i>T</i></p>
                </div>
                <div class="p1">
                    <p id="89">在数据集上训练得到弱分类器<i>h</i><sub><i>t</i></sub> (<i>x</i>) </p>
                </div>
                <div class="p1">
                    <p id="90">分别计算<i>α</i><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ρ</mi></msubsup></mrow></math></mathml>, <i>α</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="93"><i>α</i><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ρ</mi></msubsup></mrow></math></mathml>=ln (<i>TP</i><sub><i>t</i></sub>/<i>FP</i><sub><i>t</i></sub>) /2      (5) </p>
                </div>
                <div class="p1">
                    <p id="95"><i>α</i><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup></mrow></math></mathml>=ln (<i>TN</i><sub><i>t</i></sub>/<i>FN</i><sub><i>t</i></sub>) /2      (6) </p>
                </div>
                <div class="p1">
                    <p id="97">其中:<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mi>Ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>&gt;</mo><mn>0</mn><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&gt;</mo><mn>0</mn></mrow></munder><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>, </mo><mi>F</mi><mi>Ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>&gt;</mo><mn>0</mn><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mn>0</mn></mrow></munder><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>, </mo><mi>Τ</mi><mi>Ν</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mn>0</mn></mrow></munder><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>, </mo><mi>F</mi><mi>Ν</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&gt;</mo><mn>0</mn></mrow></munder><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="99">按照式 (7) 更新样本权值。</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mi>α</mi><msubsup><mrow></mrow><mi>t</mi><mi>Ρ</mi></msubsup><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>/</mo><mi>Ζ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mtext> </mtext><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>≥</mo><mn>0</mn></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mi>α</mi><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>/</mo><mi>Ζ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">End For</p>
                </div>
                <div class="p1">
                    <p id="102">3) 输出组合分类器。</p>
                </div>
                <div class="p1">
                    <p id="104"><i>H</i> (<i>x</i><sub><i>i</i></sub>) =sign<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn><mo>, </mo><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>&gt;</mo><mn>0</mn></mrow><mi>Τ</mi></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mi>t</mi><mi>Ρ</mi></msubsup><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn><mo>, </mo><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mn>0</mn></mrow><mi>Τ</mi></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">1.3 SMOTE<b>算法简介</b></h4>
                <div class="p1">
                    <p id="107">SMOTE基本思想是通过人工合成新的少数类样本来改变样本分布。其基本原理是在相距较近的少数类样本之间进行线性插值, 从而生成新的少数类样本。下面对SMOTE关键步骤进行简单的介绍。</p>
                </div>
                <div class="p1">
                    <p id="108">SMOTE算法是对少数类样本进行过采样的算法。对于某个少数类样本<b><i>x</i></b>, 首先找到距其最近的<i>K</i>个少数类样本, 若向上采样倍率为<i>N</i>, 则从其<i>K</i>个少数类样本近邻中随机选择<i>N</i>个少数类样本, 记为<b><i>y</i></b><sub>1</sub>, <b><i>y</i></b><sub>2</sub>, …, <b><i>y</i></b><sub><i>N</i></sub>。最后, 在<b><i>x</i></b>和<i>N</i>个少数类样本之间进行随机线性插值, 生成<i>N</i>个新的少数类样本, 记为<b><i>x</i></b><sub><i>new</i>1</sub>, <b><i>x</i></b><sub><i>new</i>2</sub>, …, <b><i>x</i></b><sub><i>newN</i></sub>。如式 (9) 所示:</p>
                </div>
                <div class="p1">
                    <p id="109"><b><i>x</i></b><sub><i>newj</i></sub>=<b><i>x</i></b>+rand (0, 1) * (<b><i>y</i></b><sub><i>j</i></sub>-<b><i>x</i></b>) ; <i>j</i>=1, 2, …, <i>N</i>      (9) </p>
                </div>
                <div class="p1">
                    <p id="110">其中:rand (0, 1) 是指区间 (0, 1) 的一个随机数。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">1.4 <b>核距离</b></h4>
                <div class="p1">
                    <p id="112">将样本通过非线性映射函数映射到核空间, 在核空间中度量的样本之间的距离称为核距离。NKSMOTE算法中涉及核距离的计算, 对于核空间中任意两个样本点<i>φ</i> (<b><i>x</i></b>) 和<i>φ</i> (<b><i>y</i></b>) , 其核距离计算如下:</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>d</mi><mo stretchy="false"> (</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>, </mo><mi>φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mo stretchy="false">∥</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>-</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>=</mo></mtd></mtr><mtr><mtd><msqrt><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo>, </mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>-</mo><mn>2</mn><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo></mrow></msqrt><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">其中:<i>φ</i> () 是非线性映射函数, <i>K</i> (, ) 是核函数。</p>
                </div>
                <div class="p1">
                    <p id="115">目前常用的核函数有高斯核函数、多项式核函数、<i>S</i>型核函数等。</p>
                </div>
                <div class="p1">
                    <p id="116">本文采用的核函数是高斯函数, 公式如下</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mo>-</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">1.5 NKSMOTE<b>算法简介</b></h4>
                <div class="p1">
                    <p id="119">1) 对任意少数类样本<b><i>x</i></b>在核空间上寻找距其最近的<i>K</i>个样本。根据其<i>K</i>个样本近邻中少数类样本的个数与多数类样本的个数的比例, 将少数类样本<b><i>x</i></b>分为安全样本, 危险样本, 噪声样本。若少数类样本的个数多于多数类样本的个数, 则少数类样本<b><i>x</i></b>为安全样本;若少数类样本的个数少于多数类样本的个数, 且存在少数类样本, 则少数类样本<b><i>x</i></b>为危险样本;若全是多数类样本则该少数类样本<b><i>x</i></b>为噪声样本。</p>
                </div>
                <div class="p1">
                    <p id="120">2) 若少数类<b><i>x</i></b>不是噪声样本, 则从其<i>K</i>近邻中随机选择两个样本, 在三个样本之间按照一定的合成规则合成<i>N</i>个新样本, 其中<i>N</i>值是向上采样倍率。若选中的两个样本<b><i>y</i></b><sub>1</sub>, <b><i>y</i></b><sub>2</sub>均是多数类样本, 则利用式 (12) 和 (13) 得到新的少数类样本。</p>
                </div>
                <div class="p1">
                    <p id="121">①在<b><i>y</i></b><sub>1</sub>, <b><i>y</i></b><sub>2</sub>之间进行随机线性插值, 生成<i>N</i>个临时样本<i>t</i><sub><i>newj</i></sub> (<i>j</i>=1, 2, …, <i>N</i>) :</p>
                </div>
                <div class="p1">
                    <p id="122"><i>t</i><sub><i>newj</i></sub>=<b><i>y</i></b><sub>1</sub>+rand (0, 0.5) * (<b><i>y</i></b><sub>2</sub>-<b><i>y</i></b><sub>1</sub>) ; <i>j</i>=1, 2, …, <i>N</i>      (12) </p>
                </div>
                <div class="p1">
                    <p id="123">②在<i>t</i><sub><i>newj</i></sub>, <b><i>x</i></b>之间进行随机线性插值, 构造新的少数类样本<b><i>x</i></b><sub><i>newj</i></sub> (<i>j</i>=1, 2, …, <i>N</i>) :</p>
                </div>
                <div class="p1">
                    <p id="124"><b><i>x</i></b><sub><i>newj</i></sub>=<b><i>x</i></b>+rand (0, 1) * (<i>t</i><sub><i>newj</i></sub>-<b><i>x</i></b>) ; <i>j</i>=1, 2, …, <i>N</i>      (13) </p>
                </div>
                <div class="p1">
                    <p id="125">其中:rand (0, 1) 表示区间 (0, 1) 内的一个随机数;rand (0, 0.5) 表示区间 (0, 0.5) 内的一个随机数。</p>
                </div>
                <div class="p1">
                    <p id="126">若选中的两个样本<b><i>y</i></b><sub>1</sub>, <b><i>y</i></b><sub>2</sub>中有一个是少数类, 则利用式 (14) 和 (13) 得到新的少数类样本。</p>
                </div>
                <div class="p1">
                    <p id="127">①在<b><i>y</i></b><sub>1</sub>, <b><i>y</i></b><sub>2</sub>之间进行随机线性插值, 生成<i>N</i>个临时样本<i>t</i><sub><i>newj</i></sub> (<i>j</i>=1, 2, …, <i>N</i>) :</p>
                </div>
                <div class="p1">
                    <p id="128"><i>t</i><sub><i>newj</i></sub>=<b><i>y</i></b><sub>1</sub>+rand (0, 1) * (<b><i>y</i></b><sub>2</sub>-<b><i>y</i></b><sub>1</sub>) ; <i>j</i>=1, 2, …, <i>N</i>      (14) </p>
                </div>
                <div class="p1">
                    <p id="129">②在<i>t</i><sub><i>newj</i></sub>, <b><i>x</i></b>之间利用式 (11) 进行随机线性插值, 构造新的少数类样本<b><i>x</i></b><sub><i>newj</i></sub> (<i>j</i>=1, 2, …, <i>N</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="130">3) 若少数类样本<b><i>x</i></b>是噪声样本, 对其进行过采样时有给数据集引入噪声的风险, 但是噪声样本又有其积极作用, 为了使风险降到最低, 设定其向上采样倍率<i>N</i>为1。在其余少数类样本中随机选择一个少数类样本<b><i>y</i></b>, 在<b><i>x</i></b>与<b><i>y</i></b>之间进行随机线性插值, 其中增量因子是一个在 (0.5, 1) 区间上服从均匀分布的随机数, 使新的少数类样本更靠近样本<b><i>y</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="131"><b><i>x</i></b><sub>new</sub>=<b><i>x</i></b>+rand (0.5, 1) * (<b><i>y</i></b>-<b><i>x</i></b>)      (15) </p>
                </div>
                <h3 id="132" name="132" class="anchor-tag">2 NIBoost算法基本原理</h3>
                <div class="p1">
                    <p id="133">本文所提出的NIBoost算法是一种结合了代价敏感和过采样技术的Boosting算法。它能在每次迭代的过程中逐渐地增加少数类样本的数目来平衡数据集和调整错分样本的权重, 使得最终训练出来的强分类器对非平衡数据集有很高的分类性能。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">2.1 NIBoost<b>原理介绍</b></h4>
                <div class="p1">
                    <p id="135">本文提出一种将代价敏感思想与过采样技术相结合的非平衡数据分类算法——NIBoost算法。其基本思想是在每次迭代中融入过采样算法 (NKSMOTE) , 即通过增加少数类样本平衡数据集;然后在该数据集上训练分类器。随后根据样本原始的类标以及分类的结果分别进行权值调整。算法分为4个阶段:1) 对非平衡数据集<b><i>S</i></b>中的样本进行权值初始化。2) 调用过采样算法 (NKSMOTE) , 生成新的少数类样本与原始数据集组成新的训练集。3) 在训练集上训练弱分类器, 分别计算该分类器的<i>e</i><sub><i>t</i></sub>值及<i>α</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ρ</mi></msubsup></mrow></math></mathml>、<i>α</i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup></mrow></math></mathml>, 根据<i>α</i><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ρ</mi></msubsup></mrow></math></mathml>、<i>α</i><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup></mrow></math></mathml>对分类结果为正类的样本和分类结果为负类的样本分别进行权值更新。4) 重复2) , 3) , 若<i>e</i><sub><i>t</i></sub>大于0.5则终止循环。</p>
                </div>
                <div class="p1">
                    <p id="140">算法1 NIBoost。</p>
                </div>
                <div class="p1">
                    <p id="141">输入:数据集<b><i>S</i></b>:{ (<b><i>x</i></b><sub>1</sub>, <i>y</i><sub>1</sub>) , (<b><i>x</i></b><sub>2</sub>, <i>y</i><sub>2</sub>) , …, (<b><i>x</i></b><sub><i>n</i></sub>, <i>y</i><sub><i>n</i></sub>) };样本总数<i>n</i>;多数类样本个数<i>n</i><sup>+</sup>;少数类样本个数<i>n</i><sup>-</sup>;迭代次数<i>T</i>;弱学习算法WeakLearn;</p>
                </div>
                <div class="p1">
                    <p id="142">输出:组合分类器。</p>
                </div>
                <div class="p1">
                    <p id="143">1) 数据集<b><i>S</i></b>中样本权值的初始化<i>w</i><sub>1</sub> (<i>i</i>) =1/<i>n</i></p>
                </div>
                <div class="p1">
                    <p id="145">2) For <i>t</i>=1, 2, …, <i>T</i></p>
                </div>
                <div class="p1">
                    <p id="147">①调用过采样算法 (NKSMOTE) , 新增<i>m</i>个少数类样本以平衡数据集, 按照式 (18) 更新权值:</p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>/</mo><mi>n</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mtd></mtr><mtr><mtd><mi>w</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>o</mtext><mtext>l</mtext><mtext>d</mtext></mrow></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>×</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi>m</mi></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∉</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149">其中:<i>S</i><sub><i>t</i></sub>表示第<i>t</i>次迭代新增加的少数类集合, <i>w</i><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>o</mtext><mtext>l</mtext><mtext>d</mtext></mrow></msubsup></mrow></math></mathml> (<i>i</i>) 和<i>w</i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msubsup></mrow></math></mathml> (<i>i</i>) 分别表示第<i>t</i>次迭代时样本 (<b><i>x</i></b><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 的权值。</p>
                </div>
                <div class="p1">
                    <p id="152">②在新数据集上训练分类器, 计算得到该分类器的分类错误率<i>e</i><sub><i>t</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="153" class="code-formula">
                        <mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>e</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><msqrt><mrow><mi>e</mi><msubsup><mrow></mrow><mi>t</mi><mo>+</mo></msubsup><mo>⋅</mo><mi>e</mi><msubsup><mrow></mrow><mi>t</mi><mo>-</mo></msubsup></mrow></msqrt><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="154">其中:</p>
                </div>
                <div class="p1">
                    <p id="155" class="code-formula">
                        <mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>e</mi><msubsup><mrow></mrow><mi>t</mi><mo>+</mo></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msup><mrow></mrow><mo>+</mo></msup></mrow></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>/</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msup><mrow></mrow><mo>+</mo></msup></mrow></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>e</mi><msubsup><mrow></mrow><mi>t</mi><mo>-</mo></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msup><mrow></mrow><mo>-</mo></msup></mrow></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>/</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msup><mrow></mrow><mo>-</mo></msup></mrow></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="156">; <i>h</i> (<i>x</i><sub><i>i</i></sub>) ≠<i>y</i><sub><i>i</i></sub></p>
                </div>
                <div class="p1">
                    <p id="157">If <i>e</i><sub><i>t</i></sub>&gt;0.5 Then break</p>
                </div>
                <div class="p1">
                    <p id="158">③根据式 (5) 、 (6) 计算得到该分类器的系数<i>α</i><mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ρ</mi></msubsup></mrow></math></mathml>, <i>α</i><mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup></mrow></math></mathml>, 并按照式 (7) 再次更新样本的权值。</p>
                </div>
                <div class="p1">
                    <p id="161">End For</p>
                </div>
                <div class="p1">
                    <p id="162">3) 根据式 (8) 输出组合分类器<i>H</i> (<i>x</i><sub><i>i</i></sub>) 。</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164">2.2 NIBoost<b>的时间复杂度分析</b></h4>
                <div class="p1">
                    <p id="165">由于所提出的算法需要用到NKSMOTE算法来平衡训练数据集, 故NIBoost算法的时间复杂度要高于一般的Boost算法。若各变量符号如之前的算法流程所示, 数据集的大小为<i>n</i>, 算法的迭代次数为<i>T</i>, 则在数据的预处理阶段, 需要为各个样本初始化权重, 时间复杂度为<i>O</i> (<i>n</i>) ;然后调用NKSMOTE算法来生成新的少数类, 当计算少数类与其他样本的距离时, 时间复杂度为<i>O</i> (<i>f</i>·<i>n</i><sub><i>i</i></sub>· (<i>m</i>+<i>n</i>) ) , 再将距离排序, 其时间复杂度为<i>O</i> (<i>n</i> log (<i>n</i>) ) , 其中: <i>f</i>是样本的特征个数, <i>n</i><sub><i>i</i></sub>表示在第<i>i</i>次迭代过程中少数类样本的个数, <i>m</i>为之前迭代过程中新增的少数类样本个数总和。最后更新样本权重的时间复杂度也为<i>O</i> (<i>n</i>+<i>m</i>) 。因为少数类样本的数量<i>n</i><sub><i>i</i></sub>远小于样本个数<i>n</i>, 新增的少数类样本个数<i>m</i>也不会超过<i>n</i>, 算法共要迭代<i>T</i>次, 故NIBoost的整体时间复杂度可简化为<i>O</i> (2·<i>T</i>·<i>f</i>·<i>n</i><sup>2</sup>) 。</p>
                </div>
                <h3 id="166" name="166" class="anchor-tag">3 实验设计与结果分析</h3>
                <div class="p1">
                    <p id="167">为了验证本文所提算法的有效性, 在实际的非平衡数据集上用NIBoost算法对其分类。对实验结果的分析证明, NIBoost算法在处理非平衡数据集时具有一定的优势。</p>
                </div>
                <h4 class="anchor-tag" id="168" name="168">3.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="169">实验所用数据集来自UCI数据库, 表1列出了这些非平衡数据集的基本信息, 包括数据集名称、样本数、属性数、少数类数以及非平衡度 (Imbalanced Rate, IR) 。</p>
                </div>
                <div class="area_img" id="170">
                    <p class="img_tit"><b>表</b>1 <b>非平衡数据集的基本信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Basic information of imbalanced datasets</p>
                    <p class="img_note"></p>
                    <table id="170" border="1"><tr><td><br />名称</td><td>样本数</td><td>属性数</td><td>少数类样本数</td><td>非平衡度</td></tr><tr><td>ecoli3</td><td>336</td><td>7</td><td>35</td><td>8.60</td></tr><tr><td><br />ecoli1</td><td>336</td><td>7</td><td>77</td><td>3.36</td></tr><tr><td><br />wisconsin</td><td>683</td><td>9</td><td>239</td><td>1.86</td></tr><tr><td><br />pima</td><td>768</td><td>8</td><td>268</td><td>1.87</td></tr><tr><td><br />yeast3</td><td>1 484</td><td>8</td><td>163</td><td>8.10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="171" name="171">3.2 <b>评价标准</b></h4>
                <div class="p1">
                    <p id="172">在传统分类学习方法中, 一般采用分类精度作为评价指标, 然而对于非平衡数据集而言, 用分类精度来评价分类器的性能是不合理的。在机器学习领域中对于非平衡数据分类的常用评价标准有ROC (Receiver Operating Characteristic) 曲线、AUC (Area Under Curve) 以及基于混淆矩阵的F-value, G-mean。在非平衡数据学习中, 少数类对应为正类, 多数类对应为负类, 表2给出了二分类问题的混淆矩阵。</p>
                </div>
                <div class="area_img" id="173">
                    <p class="img_tit"><b>表</b>2 <b>混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Confusion matrix</p>
                    <p class="img_note"></p>
                    <table id="173" border="1"><tr><td><br />类型</td><td>预测正类</td><td>预测负类</td></tr><tr><td><br />实际正类</td><td>TP</td><td>FN</td></tr><tr><td><br />实际负类</td><td>FP</td><td>TN</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="175">根据混淆矩阵可以得到以下评价指标。</p>
                </div>
                <div class="p1">
                    <p id="176">真正率:</p>
                </div>
                <div class="p1">
                    <p id="177"><i>TP</i><sub>rate</sub>=<i>TP</i>/ (<i>TP</i>+<i>FN</i>)      (18) </p>
                </div>
                <div class="p1">
                    <p id="178">真负率:</p>
                </div>
                <div class="p1">
                    <p id="179"><i>TN</i><sub>rate</sub>=<i>TN</i>/ (<i>TN</i>+<i>FP</i>)      (19) </p>
                </div>
                <div class="p1">
                    <p id="180">假正率:</p>
                </div>
                <div class="p1">
                    <p id="181"><i>FP</i><sub>rate</sub>=<i>FP</i>/ (<i>TN</i>+<i>FP</i>)      (20) </p>
                </div>
                <div class="p1">
                    <p id="182">正类预测值:</p>
                </div>
                <div class="p1">
                    <p id="183"><i>PP</i><sub>value</sub>=<i>TP</i>/ (<i>TP</i>+<i>FP</i>)      (21) </p>
                </div>
                <div class="p1">
                    <p id="184">其中:真正率<i>TP</i><sub>rate</sub>又称为查全率<i>recall</i>, 正类预测值<i>PP</i><sub>value</sub>又称为查准率<i>precision</i>。</p>
                </div>
                <div class="p1">
                    <p id="185" class="code-formula">
                        <mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>-</mo><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>⋅</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo>⋅</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>+</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">) </mo></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="186">其中:<i>β</i>用于调节<i>precision</i>和<i>recall</i>的相对重要度, 通常取为1。</p>
                </div>
                <div class="p1">
                    <p id="187">如果同时关注两个类的性能, 可以使用G-mean评价算法在两个类上的性能:</p>
                </div>
                <div class="p1">
                    <p id="188" class="code-formula">
                        <mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>=</mo><msqrt><mrow><mi>Τ</mi><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msub><mo>×</mo><mi>Τ</mi><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msub></mrow></msqrt><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="189">ROC曲线是<i>TP</i><sub>rate</sub>与<i>FP</i><sub>rate</sub>关系的可视化表示, 其中<i>TP</i><sub>rate</sub>是纵坐标, <i>FP</i><sub>rate</sub>是横坐标, 因此曲线越靠近左上角表示分类器的性能越好。当两条ROC曲线相交时很难判断出哪条曲线更好, 所以通过计算ROC曲线下方的面积AUC作为评价指标。</p>
                </div>
                <div class="p1">
                    <p id="190"><i>AUC</i>= (1+<i>TP</i><sub>rate</sub>-<i>FP</i><sub>rate</sub>) /2      (24) </p>
                </div>
                <div class="p1">
                    <p id="191">本文采用F-value、G-mean、AUC作为评价度量。</p>
                </div>
                <h4 class="anchor-tag" id="192" name="192">3.3 <b>实验结果及分析</b></h4>
                <div class="p1">
                    <p id="193">实验用Java实现SMOTE、RareBoost、PCBoost、NIBoost算法, 采用决策树 (J48) 、朴素贝叶斯 (Naive Bayesian, NB) 算法为基分类器算法。实验采用五折交叉验证。表3展示了上述4个算法在5个数据集上的F-value值、G-mean值、AUC值的比较结果。</p>
                </div>
                <div class="p1">
                    <p id="194">通过对表3的观察可以发现, 当以J48作为基分类算法, 用F-value作为评价度量时, 在所有数据集上, NIBoost均比PCBoost、RareBoost效果好, 比PCBoost最高提高了2.3个百分点, 比RareBoost最高提高了5.91个百分点;除了在数据集ecoli3上略逊于SMOTE外, 在其他数据集上NIBoost均比SMOTE算法效果好, 最高提高了2个百分点。当用G-mean作为评价度量时, 在所有数据集上, NIBoost均比RareBoost、SMOTE效果好, 比RareBoost最高提高了7.44个百分点;除了在数据集ecoli1上略逊于PCBoost外, 在其他数据集上NIBoost也都比PCBoost效果好。当用AUC作为评价度量时, 在所有数据集上, NIBoost均比SMOTE、RareBoost效果好, 比SMOTE最高提高了1.54个百分点, 比RareBoost最高提高了4.38个百分点;除了在数据集ecoli3、yeast3上略逊于PCBoost外, 在其他数据集上NIBoost均比PCBoost效果好。</p>
                </div>
                <div class="p1">
                    <p id="195">当以NB作为基分类算法, 用F-value作为评价度量时, 除了在数据集ecoli1上略逊于SMOTE外, 在其他数据集上NIBoost均比SMOTE算法效果好, 最高提高了3.9个百分点;除了在数据集ecoli1上NIBoost略逊于PCBoost外, 在其他数据集上NIBoost均比PCBoost效果好, 最高提高了5.22个百分点;在所有数据集上, NIBoost均比RareBoost效果要好, 最高提高了9.6个百分点。当用G-mean作为评价度量时, 在所有数据集上, NIBoost均比SMOTE、RareBoost效果好, 比SMOTE最高提高了2.3个百分点, 比RareBoost最高提高了20.3个百分点;除了在数据集ecoli1上NIBoost略逊于PCBoost外, 在其他数据集上NIBoost均比PCBoost效果好。当用AUC作为评价度量时, 除了在数据集ecoli1、yeast3上略逊于SMOTE外, 在其他数据集上NIBoost均比SMOTE算法效果好, 最高提高了1.74个百分点;除了在数据集ecoli1上略逊于PCBoost外, 在其他数据集上NIBoost均比PCBoost效果好, 最高提高了4.1个百分点;在所有数据集上, NIBoost均比RareBoost算法效果好, 最高提高了16.14个百分点。</p>
                </div>
                <div class="p1">
                    <p id="196">所以总的来说, NIBoost算法在处理非平衡数据时具有一定的优势。</p>
                </div>
                <h3 id="197" name="197" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="198">针对非平衡数据集的分类问题, 本文从算法层面入手, 结合RareBoost算法和GMBoost算法的思想, 给出一种将代价敏感思想与过采样技术相结合的非平衡数据分类算法——NIBoost算法。该算法是在每次迭代中融入过采样算法 (NKSMOTE) , 即通过增加少数类样本逐步平衡数据集;然后在该数据集上训练分类器。随后根据样本的原始类标与分类结果分别进行权值调整。实验结果表明NIBoost算法具有更好的分类性能。进一步的研究工作是将欠采样和集成学习的思想融合起来以解决非平衡数据分类困难的问题。</p>
                </div>
                <div class="area_img" id="199">
                                            <p class="img_tit">
                                                <b>表</b>3 4<b>个算法在</b>5<b>个数据集上的</b>F-value<b>值</b>、G-mean<b>值</b>、AUC<b>值的比较</b>
                                                    <br />
                                                Tab. 3 Comparison of F-value, G-mean and AUC values of four algorithms on five datasets
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903003_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201903003_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903003_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 4个算法在5个数据集上的F-value值、G-mean值、AUC值的比较" src="Detail/GetImg?filename=images/JSJY201903003_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="215">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD00000971069&amp;v=MDQwNDF0SE1wb2hFWk8wR1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rmkva1c3eklJRnM9TmozYWFyTzRI&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>WEISS G M, ZADROZNY B, SAAR M.Guest editorial:special issue on utility-based data mining[J].Data Mining and Knowledge Discovery, 2008, 17 (2) :129-135.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000085401&amp;v=MDM2MDJjPU5pZklZN0s3SHRqTnI0OUZaT01LQ0h3NG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSktGMFVhaA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>del CASTILLO M D, SERRANO J I.A multistrategy approach for digital text categorization from imbalanced documents[J].ACMSIGKDD Explorations Newsletter, 2004, 6 (1) :70-79.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13062800015698&amp;v=MjQzNTBLN0h0Zk9wNDlGWk9vS0NuVXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyakpLRjBVYWhjPU5qN0Jhcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>WEI W, LI J, CAO L.Effective detection of sophisticated online banking fraud on extremely imbalanced data[J].World Wide Web, 2013, 16 (4) :449-475.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201304029&amp;v=MTAyOTE0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURtVTcvTkx6N0JiN0c0SDlMTXE0OUhiWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>江颉, 王卓芳, GONG R S, 等.不平衡数据分类方法及其在入侵检测中的应用研究[J].计算机科学, 2013, 40 (4) :131-135. (JIANG J, WANG Z F, GONG R S, et al.Imbalanced data classification method and its application research for intrusion detection[J].Computer Science, 2013, 40 (4) :131-135.) 
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339718&amp;v=MzA4MzZGaS9rVzd6SUlGcz1OajdCYXJPNEh0SE5ySXhNWStvSFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>KUBAT M, HOLTE RC, MATWIN S.Machine learning for the detection of oil spills in satellite radar images[J].Machine Learning, 1998, 30 (2) :195-215.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Strategies for addressing class imbalance in ensemble classification of thermography breast cancer features">

                                <b>[6]</b>SCHAEFER G, NAKASHIMA T.Strategies for addressing class imbalance in ensemble classification of thermography breast cancer features[C]//Proceedings of the 2015 IEEE Congress on Evolutionary Computation.Piscataway, NJ:IEEE, 2015:2362-2367.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                CHAWLA N V, BOWYER K W, HALL L O.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700313588&amp;v=MDkzNzRqSktGMFVhaGM9TmlmT2ZiSzhIOURNcUk5Rlorb01DWFF4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>QIAN Y, LIANG Y, LI M.A resampling ensemble algorithm for classification of imbalance problems[J].Neurocomputing, 2014, 143:57-67.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE">

                                <b>[9]</b>DOUZAS G, BACAO F, LAST F.Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE[J].Information Sciences, 2018, 465:1-20.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ordering-based pruning for improving the performance of ensembles of classifiers in the framework of imbalanced data sets">

                                <b>[10]</b>GALAR M, FERNANDEZ A, BARRENECHEA E.Orderingbased pruning for improving the performance of ensembles of classifiers in the framework of imbalanced datasets[J].Information Sciences, 2016, 354:178-196.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300467873&amp;v=MTEwMjBmYks3SHRETnJJOUZZTzBJQkhzNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSktGMFVhaGM9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>ZHANG Y, ZHANG D, MI G.Using ensemble methods to deal with imbalanced data in predicting protein-protein interactions[J].Computational Biology and Chemistry, 2012, 36 (2) :36-41.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                KIM M J, KANG D K, KIM H B.Geometric mean based boosting algorithm with over-sampling to resolve data imbalance problem for值的比较ms on five datasets NBst Rare Boost NIBoost6 0.548 6 0.644 62 0.674 6 0.741 08 0.948 8 0.965 02 0.616 6 0.659 48 0.746 2 0.771 47 0.707 0 0.756 32 0.676 8 0.879 82 0.751 1 0.863 44 0.965 6 0.977 08 0.695 2 0.734 66 0.812 4 0.852 80 0.780 2 0.861 56 0.727 4 0.888 88 0.776 7 0.865 04 0.965 6 0.977 00 0.711 1 0.735 24 0.828 7 0.861 42 0.801 9 0.865 5bankruptcy prediction[J].Expert Systems with Applications, 2015, 42 (3) :1074-1082.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201202003&amp;v=MDQ1OTZxQnRHRnJDVVI3cWZadVpwRmlEbVU3L05MejdCZHJHNEg5UE1yWTlGWjRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>李雄飞, 李军, 董元方, 等.一种新的不平衡数据学习算法PCBoost[J].计算机学报, 2012, 35 (2) :202-209. (LI X F, LI J, DONG Y F, et al.A new learning algorithm for imbalanced data PCBoost[J].Chinese Journal of Computers, 2012, 35 (2) :202-209.) 
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A novel ensemble method for credit scoring:adaption of different imbalance ratios.&amp;quot;">

                                <b>[14]</b>HE H, ZHANG W, ZHANG S.A novel ensemble method for credit scoring:adaption of different imbalance ratios[J].Expert Systems with Applications, 2018, 98:105-117.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201406005&amp;v=MjQwMjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbVU3L05LQ0xmWWJHNEg5WE1xWTlGWVlRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>付忠良.多标签代价敏感分类集成学习算法[J].自动化学报, 2014, 40 (6) :1075-1085. (FU Z L.Cost-sensitive ensemble learning algorithm for multi-label classification problems[J].Acta Automatica Sinica, 2014, 40 (6) :1075-1085.) 
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adacost:Misclassification Cost-Sensitive Boosting">

                                <b>[16]</b>FAN W, STOLFO S J, ZHANG J, et al.Ada Cost:misclassification cost-sensitive boosting[C]//ICML'99:Proceedings of the 16th International Conference on Machine Learning.San Francisco, CA:Morgan Kaufmann Publishers, 1999:97-105.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739373&amp;v=MjczNDhxWTlGWStnR0QzczZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyakpLRjBVYWhjPU5pZk9mYks3SHRETg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>SUN Y, KAMEL M S, WONG A K C.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378.
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluating boosting algo-rithms to classify rare classes:comparison and improvements">

                                <b>[18]</b>JOSHI M V, KUMAR V, AGARWAL R.Evaluating boosting algorithms to classify rare classes:comparison and improvements[C]//Proceedings of the 2001 IEEE International Conference on Data Mining.Piscataway, NJ:IEEE, 2001:257-264.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122600211100&amp;v=MjMxOTlPcVk5Rlp1b09EWHc1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpKS0YwVWFoYz1OaWZPZmJLOUg5UA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>SIERS M J, ISLAM M Z.Software defect prediction using a cost sensitive decision forest and voting, and a potential solution to the class imbalance problem[J].Information Systems, 2015, 51:62-71.
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Cost-Sensitive Ensemble Method for Class-Imbalanced Datasets">

                                <b>[20]</b>ZHANG Y, WANG D.A cost-sensitive ensemble method for classimbalanced data sets[J].Abstract and Applied Analysis, 2013, 2013:Article ID 196256.
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Revisiting example dependent cost-sensitive learning with decision trees">

                                <b>[21]</b>AODHA O M, BROSTOW G J.Revisiting example dependent cost-sensitive learning with decision trees[C]//Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2013:193-200.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300161907&amp;v=MTk2NjhaZTBPQlh3K29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSktGMFVhaGM9TmlmT2ZiSzlIOVBPckk5Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>GALAR M, FERNANDEZ A, BARRENECHEA E.EUSBoost:enhancing ensembles for highly imbalanced data-sets by evolutionary undersampling[J].Pattern Recognition, 2013, 46 (12) :3460-3471.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cost-sensitive strategies for data imbalance in bug severity classification:experimental results">

                                <b>[23]</b>ROY N K S, ROSSI B.Cost-sensitive strategies for data imbalance in bug severity classification:experimental results[C]//Proceedings of the 2017 43rd Euromicro Conference on Software Engineering and Advanced Applications.Washington, DC:IEEE Computer Society, 2017:426-429.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An overlap-sensitive margin classifier for imbalanced and overlapping data">

                                <b>[24]</b>LEE H K, KIM S B.An overlap-sensitive margin classifier for imbalanced and overlapping data[J].Expert Systems with Applications, 2018, 98:72-83.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201903003" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903003&amp;v=MTY0MTMzenFxQnRHRnJDVVI3cWZadVpwRmlEbVU3L05MejdCZDdHNEg5ak1ySTlGWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
