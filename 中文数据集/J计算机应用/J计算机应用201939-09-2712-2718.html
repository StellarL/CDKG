<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136481957315000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909039%26RESULT%3d1%26SIGN%3dZSHQ4s5mFxPT%252bJ9K4Nvr5cb5e8k%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909039&amp;v=MDU0MzZxQnRHRnJDVVI3cWZadVpzRnlqblZMM0lMejdCZDdHNEg5ak1wbzlHYllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#55" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="1 衰减式生成对抗网络 ">1 衰减式生成对抗网络</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="2 网络构成细节 ">2 网络构成细节</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="2.1 &lt;b&gt;敏感因子引导的衰减器&lt;/b&gt;">2.1 <b>敏感因子引导的衰减器</b></a></li>
                                                <li><a href="#85" data-title="2.2 &lt;b&gt;多语境嵌入的生成器和判别器&lt;/b&gt;">2.2 <b>多语境嵌入的生成器和判别器</b></a></li>
                                                <li><a href="#93" data-title="2.3 &lt;b&gt;网络训练细节&lt;/b&gt;">2.3 <b>网络训练细节</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="3 实验结果分析 ">3 实验结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="图1 衰减式生成网络训练流程">图1 衰减式生成网络训练流程</a></li>
                                                <li><a href="#82" data-title="图2 阴影衰减图像">图2 阴影衰减图像</a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;衰减器内部结构&lt;/b&gt;"><b>表</b>1 <b>衰减器内部结构</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;判别器内部结构&lt;/b&gt;"><b>表</b>2 <b>判别器内部结构</b></a></li>
                                                <li><a href="#102" data-title="图3 综合筛选数据集示例">图3 综合筛选数据集示例</a></li>
                                                <li><a href="#106" data-title="图4 两种方法生成的阴影蒙版与无阴影结果">图4 两种方法生成的阴影蒙版与无阴影结果</a></li>
                                                <li><a href="#109" data-title="图5 不同方法的阴影去除结果">图5 不同方法的阴影去除结果</a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各个方法在不同区域的实验结果比较&lt;/b&gt;"><b>表</b>3 <b>各个方法在不同区域的实验结果比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="177">


                                    <a id="bibliography_1" title=" GUO R,DAI Q,HOIEM D.Paired regions for shadow detection and removal [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(12):2956-2967." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Paired regions for shadow detection and removal">
                                        <b>[1]</b>
                                         GUO R,DAI Q,HOIEM D.Paired regions for shadow detection and removal [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(12):2956-2967.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_2" title=" KHAN S H,BENNAMOUN M,SOHEL F,et al.Automatic shadow detection and removal from a single image [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(3):431-446." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic Shadow Detection and Removal from a Single Image">
                                        <b>[2]</b>
                                         KHAN S H,BENNAMOUN M,SOHEL F,et al.Automatic shadow detection and removal from a single image [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(3):431-446.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_3" title=" 张营,朱岱寅,俞翔,等.一种VideoSAR动目标阴影检测方法[J].电子与信息学报,2017,39(9):2197-2202.(ZHANG Y,ZHU D Y,YU X,et al.Approach to moving targets shadow detection for VideoSAR [J].Journal of Electronics and Information Technology,2017,39(9):2197-2202.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201709022&amp;v=MzIzOTFzRnlqblZMM0lJVGZTZHJHNEg5Yk1wbzlIWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         张营,朱岱寅,俞翔,等.一种VideoSAR动目标阴影检测方法[J].电子与信息学报,2017,39(9):2197-2202.(ZHANG Y,ZHU D Y,YU X,et al.Approach to moving targets shadow detection for VideoSAR [J].Journal of Electronics and Information Technology,2017,39(9):2197-2202.)
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_4" title=" ZHANG L,ZHANG Q,XIAO C.Shadow remover:image shadow removal based on illumination recovering optimization [J].IEEE Transactions on Image Processing,2015,24(11):4623-4636." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shadow remover:Image shadow removal based on illumination recovering optimization">
                                        <b>[4]</b>
                                         ZHANG L,ZHANG Q,XIAO C.Shadow remover:image shadow removal based on illumination recovering optimization [J].IEEE Transactions on Image Processing,2015,24(11):4623-4636.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_5" title=" ZHANG L,ZHU Y,LIAO B,et al.Video shadow removal using spatio-temporal illumination transfer [J].Computer Graphics Forum,2017,36(7):125-134." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Video shadow removal using spatio-temporal illumination transfer">
                                        <b>[5]</b>
                                         ZHANG L,ZHU Y,LIAO B,et al.Video shadow removal using spatio-temporal illumination transfer [J].Computer Graphics Forum,2017,36(7):125-134.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_6" title=" SHEN L,CHUA T W,LEMAN K.Shadow optimization from structured deep edge detection [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:2067-2074." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shadow optimization from structured deep edge detection">
                                        <b>[6]</b>
                                         SHEN L,CHUA T W,LEMAN K.Shadow optimization from structured deep edge detection [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:2067-2074.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_7" title=" LIU F,GLEICHER M.Texture-consistent shadow removal [C]// Proceedings of the 2008 European Conference on Computer Vision,LNCS 5305.Berlin:Springer,2008:437-450." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Texture-consistent shadow removal">
                                        <b>[7]</b>
                                         LIU F,GLEICHER M.Texture-consistent shadow removal [C]// Proceedings of the 2008 European Conference on Computer Vision,LNCS 5305.Berlin:Springer,2008:437-450.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_8" title=" FINLAYSON G D,HORDLEY S D,DREW M S.Removing shadows from images [C]// Proceedings of the 2002 European Conference on Computer Vision,LNCS 2353.Berlin:Springer,2002:823-836." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Removing Shadows from Images,&amp;quot;">
                                        <b>[8]</b>
                                         FINLAYSON G D,HORDLEY S D,DREW M S.Removing shadows from images [C]// Proceedings of the 2002 European Conference on Computer Vision,LNCS 2353.Berlin:Springer,2002:823-836.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_9" title=" FINLAYSON G D,HORDLEY S D,LU C,et al.On the removal of shadows from images [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(1):59-68." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the removal of shadows from images">
                                        <b>[9]</b>
                                         FINLAYSON G D,HORDLEY S D,LU C,et al.On the removal of shadows from images [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(1):59-68.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_10" title=" MOHAN A,TUMBLIN J,CHOUDHURY P.Editing soft shadows in a digital photograph [J].IEEE Computer Graphics and Applications,2007,27(2):23-31." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Editing Soft Shadows in a Digital Photograph">
                                        <b>[10]</b>
                                         MOHAN A,TUMBLIN J,CHOUDHURY P.Editing soft shadows in a digital photograph [J].IEEE Computer Graphics and Applications,2007,27(2):23-31.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_11" title=" CHUANG Y,CURLESS B,SALESIN D H,et al.A Bayesian approach to digital matting [C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2001,2:264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Bayesian approach to digital matting">
                                        <b>[11]</b>
                                         CHUANG Y,CURLESS B,SALESIN D H,et al.A Bayesian approach to digital matting [C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2001,2:264.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_12" title=" ARBEL E,HEL-OR H.Shadow removal using intensity surfaces and texture anchor points [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2011,33(6):1202-1216." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shadow removal using intensity surfaces and texture anchor points">
                                        <b>[12]</b>
                                         ARBEL E,HEL-OR H.Shadow removal using intensity surfaces and texture anchor points [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2011,33(6):1202-1216.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_13" title=" WANG J,LI X,HUI L,et al.Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal [C]// Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:1788-1797." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal">
                                        <b>[13]</b>
                                         WANG J,LI X,HUI L,et al.Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal [C]// Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:1788-1797.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_14" title=" BARRON J T,MALIK J.Shape,illumination,and reflectance from shading [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1670-1687." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape, illumination, and reflectance from shading">
                                        <b>[14]</b>
                                         BARRON J T,MALIK J.Shape,illumination,and reflectance from shading [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1670-1687.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_15" title=" QU L,TIAN J,HE S,et al.DeshadowNet:a multi-context embedding deep network for shadow removal [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017:2308-2316." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeshadowNet:A Multi-context Embedding Deep Network for Shadow Removal">
                                        <b>[15]</b>
                                         QU L,TIAN J,HE S,et al.DeshadowNet:a multi-context embedding deep network for shadow removal [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017:2308-2316.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_16" title=" ZHU J,SAMUEL K G G,MASOOD S Z,et al.Learning to recognize shadows in monochromatic natural images [C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2010:223-230." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to recognize shadows in monochromatic natural images">
                                        <b>[16]</b>
                                         ZHU J,SAMUEL K G G,MASOOD S Z,et al.Learning to recognize shadows in monochromatic natural images [C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2010:223-230.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_17" title=" LALONDE J,EFROS A A,NARASIMHAN S G.Detecting ground shadows in outdoor consumer photographs [C]// Proceedings of the 2010 European Conference on Computer Vision,LNCS 6312.Berlin:Springer,2010:322-335." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting ground shadows in outdoor consumerphotographs">
                                        <b>[17]</b>
                                         LALONDE J,EFROS A A,NARASIMHAN S G.Detecting ground shadows in outdoor consumer photographs [C]// Proceedings of the 2010 European Conference on Computer Vision,LNCS 6312.Berlin:Springer,2010:322-335.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_18" title=" FINLAYSON G D,DREW M S,LU C.Entropy minimization for shadow removal [J].International Journal of Computer Vision,2009,85(1):35-57." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003546640&amp;v=MTQ2MDRxb3REWXU4UFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFU3M1BJbDQ9Tmo3QmFyTzRIdEhQ&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         FINLAYSON G D,DREW M S,LU C.Entropy minimization for shadow removal [J].International Journal of Computer Vision,2009,85(1):35-57.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_19" title=" GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets [C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014,2:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[19]</b>
                                         GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets [C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014,2:2672-2680.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_20" title=" MIRZA M,OSINDERO S.Conditional generative adversarial nets [C]// Proceedings of the 2014 Advances in Neural Information Processing Systems.Montreal,Canada:[s.n.],2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[20]</b>
                                         MIRZA M,OSINDERO S.Conditional generative adversarial nets [C]// Proceedings of the 2014 Advances in Neural Information Processing Systems.Montreal,Canada:[s.n.],2014:2672-2680.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_21" title=" ISOLA P,ZHU J,ZHOU T,et al.Image-to-image translation with conditional adversarial networks [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:5967-5976." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image-to-Image Translation with Conditional Adversarial Networks">
                                        <b>[21]</b>
                                         ISOLA P,ZHU J,ZHOU T,et al.Image-to-image translation with conditional adversarial networks [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:5967-5976.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_22" title=" LE H,VICENTE T F Y,NGUYEN V,et al.A+D net:training a shadow detector with adversarial shadow attenuation [C]// Proceedings of the 2018 European Conference on Computer Vision,LNCS 11206.Berlin:Springer,2018:680-696." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A+D net:training a shadow detector with adversarial shadow attenuation">
                                        <b>[22]</b>
                                         LE H,VICENTE T F Y,NGUYEN V,et al.A+D net:training a shadow detector with adversarial shadow attenuation [C]// Proceedings of the 2018 European Conference on Computer Vision,LNCS 11206.Berlin:Springer,2018:680-696.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_23" title=" 施文娟,孙彦景,左海维,等.基于视频自然统计特性的无参考移动终端视频质量评价[J].电子与信息学报,2018,40(1):143-150.(SHI W J,SUN Y J,ZUO H W,et al.No-reference mobile video quality assessment based on video natural statistics [J].Jounal of Electronics and Information Technology,2018,40(1):143-150." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201801020&amp;v=MDM5MDJmU2RyRzRIOW5Ncm85SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5am5WTDNJSVQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         施文娟,孙彦景,左海维,等.基于视频自然统计特性的无参考移动终端视频质量评价[J].电子与信息学报,2018,40(1):143-150.(SHI W J,SUN Y J,ZUO H W,et al.No-reference mobile video quality assessment based on video natural statistics [J].Jounal of Electronics and Information Technology,2018,40(1):143-150.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_24" title=" IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// Proceedings of the 32nd International Conference on International Conference on Machine Learning.Lille,France:[s.n.],2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[24]</b>
                                         IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// Proceedings of the 32nd International Conference on International Conference on Machine Learning.Lille,France:[s.n.],2015:448-456.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_25" title=" JOHNSON J,ALAHI A,LI F.Perceptual losses for real-time style transfer and super-resolution [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:694-711." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual losses for real-time style transfer and superresolution">
                                        <b>[25]</b>
                                         JOHNSON J,ALAHI A,LI F.Perceptual losses for real-time style transfer and super-resolution [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:694-711.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_26" title=" WONGSUPHASAWAT K,SMILKOV D,WEXLER J,et al.Visualizing dataflow graphs of deep learning models in TensorFlow [J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):1-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing dataflow graphs of deep learning models in tensor Flow">
                                        <b>[26]</b>
                                         WONGSUPHASAWAT K,SMILKOV D,WEXLER J,et al.Visualizing dataflow graphs of deep learning models in TensorFlow [J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):1-12.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-31 11:45</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2712-2718 DOI:10.11772/j.issn.1001-9081.2019020321            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于衰减式生成对抗网络的单幅图像阴影去除</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BB%96%E6%96%8C&amp;code=22439678&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">廖斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%AD%E9%81%93%E5%BC%BA&amp;code=42779658&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谭道强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%96%87&amp;code=41133387&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴文</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B9%96%E5%8C%97%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0234810&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">湖北大学计算机与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>图像中的阴影是投影物体的重要视觉信息,但也会对计算机视觉任务造成影响。现有的单幅图像阴影去除方法因鲁棒阴影特征的缺乏或训练样本数据的不足与误差等原因,无法得到好的阴影去除结果。为了准确生成用于描述阴影区域光照衰减程度的蒙版图像,进而获得高质量的无阴影图像,提出了一种基于衰减式生成对抗网络的单幅图像阴影去除方法。首先,敏感因子引导的衰减器被用来提升训练样本数据,为后续的生成器与判别器提供符合物理光照模型的阴影样本图像。其次,生成器将结合感知损失,并在判别器的督促下得到最终阴影蒙版。与相关研究工作比较,所提方法能有效恢复阴影区域的光照信息,可以得到更为逼真、阴影边界过渡更加自然的无阴影图像。利用客观指标评价阴影去除结果。实验结果表明,该方法能在多个真实场景下有效去除阴影,去阴影结果视觉一致性良好。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%98%B4%E5%BD%B1%E5%8E%BB%E9%99%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">阴影去除;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%B0%E5%87%8F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">衰减器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%89%E7%85%A7%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光照模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *廖斌(1979—),男,湖北襄阳人,教授,博士,主要研究方向:计算机图像视频处理;电子邮箱bliao@hubu.edu.cn;
                                </span>
                                <span>
                                    谭道强(1995—),男,湖北咸宁人,硕士研究生,主要研究方向:计算机图像处理;;
                                </span>
                                <span>
                                    吴文(1994—),男,湖北武汉人,硕士研究生,主要研究方向:图像视频处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61300125);</span>
                    </p>
            </div>
                    <h1><b>Single image shadow removal based on attenuated generative adversarial networks</b></h1>
                    <h2>
                    <span>LIAO Bin</span>
                    <span>TAN Daoqiang</span>
                    <span>WU Wen</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Information Engineering, Hubei University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Shadow in an image is important visual information of the projective object, but it affects computer vision tasks. Existing single image shadow removal methods cannot obtain good shadow-free results due to the lack of robust shadow features or insufficiency of and errors in training sample data. In order to generate accurately the shadow mask image for describing the illumination attenuation degree and obtain the high quality shadow-free image, a single image shadow removal method based on attenuated generative adversarial network was proposed. Firstly, an attenuator guided by the sensitive parameters was used to augment the training sample data in order to provide shadow sample images agreed with physical illumination model for a subsequent generator and discriminator. Then, with the supervision from the discriminator, the generator combined perceptual loss function to generate the final shadow mask. Compared with related works, the proposed method can effectively recover the illumination information of shadow regions and obtain the more realistic shadow-free image with natural transition of shadow boundary. Shadow removal results were evaluated using objective metric. Experimental results show that the proposed method can remove shadow effectively in various real scenes with a good visual consistency.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=shadow%20removal&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">shadow removal;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generative%20adversarial%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generative adversarial network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attenuator&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attenuator;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=illumination%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">illumination model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LIAO Bin, born in 1979, Ph. D. , professor. His research interests include image and video processing.;
                                </span>
                                <span>
                                    TAN Daoqiang, born in 1995, M. S. candidate. His research interests include computer image processing. ;
                                </span>
                                <span>
                                    WU Wen, born in 1994, M. S. candidate. His research interests include image video processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-01</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61300125);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="55" name="55" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="56">阴影虽能为图像深度和物体几何形状预估等计算机视觉研究工作提供重要依据,但其存在也会加大物体检测、目标跟踪等任务的难度。因此,在诸多计算机视觉应用中需要去除图像中的阴影。单幅图像阴影去除一直是计算机视觉和图像处理领域的一个研究热点。</p>
                </div>
                <div class="p1">
                    <p id="57">阴影去除常包含阴影定位和去阴影这两个步骤。在阴影定位过程中,文献<citation id="237" type="reference">[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</citation>中使用阴影检测的方法,文献<citation id="238" type="reference">[<a class="sup">4</a>,<a class="sup">5</a>]</citation>中使用人工交互标注的方法确定阴影的位置,然后再分别使用自定义的特征对全阴影和半阴影区域进行重建。然而,阴影检测本身就十分困难。传统基于物理模型的方法仅仅在处理高清图像时才能得到不错的效果。基于统计学习的方法则过度依赖自定义的阴影特征<citation id="229" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。用户交互虽然能准确地定位阴影,但处理大数据量图像时非常耗时耗力。利用卷积神经网络表征学习的能力,文献<citation id="230" type="reference">[<a class="sup">2</a>]</citation>和文献<citation id="231" type="reference">[<a class="sup">6</a>]</citation>中的深度网络能够学习阴影检测所需要的特征,但受到数据集规模的限制,这两个网络层次都较浅。此外,这两种方法以区域块的形式运用卷积神经网络,后续还要进行全局优化以保持图像的连贯性。在去阴影过程中,已有的阴影去除方法大致可分为以下两类。文献<citation id="239" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</citation>中的方法均基于梯度域去阴影,而文献<citation id="240" type="reference">[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">4</a>,<a class="sup">11</a>,<a class="sup">12</a>]</citation>中的方法均基于图像颜色域去阴影。文献<citation id="241" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>中通过比较光照不变图和原始RGB图像进行阴影检测,然后提出一系列基于梯度的方法进行阴影去除。但这些基于梯度的方法仅仅只改变阴影边界或半阴影区域的梯度变量,因此对于全阴影区域的光照变量并不适用。基于颜色域的阴影去除方法通常采用分类<citation id="242" type="reference"><link href="177" rel="bibliography" /><link href="179" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>和用户交互<citation id="243" type="reference"><link href="197" rel="bibliography" /><link href="199" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>的方法区分全阴影和半阴影区域。其中,文献<citation id="232" type="reference">[<a class="sup">2</a>]</citation>中使用两个独立的卷积神经网络对阴影进行检测并区分全阴影和半阴影区域,然后基于贝叶斯公式的方法提取描述阴影区域光照衰减程度的蒙版图像。文献<citation id="233" type="reference">[<a class="sup">12</a>]</citation>中在全阴影和半阴影区域分别使用不同的低级特征,使用马尔可夫随机场找出半阴影区域,然后使用强度表面恢复法去除阴影。文献<citation id="234" type="reference">[<a class="sup">13</a>]</citation>中基于栈式条件生成对抗网络提出了一种多任务学习的方法,将阴影检测与阴影去除并行,能得到不错的效果。文献<citation id="235" type="reference">[<a class="sup">14</a>]</citation>中绕开了阴影检测环节,通过图像本征分解得到无阴影的图像。但是,基于本征图像的方法可能会改变非阴影区域的颜色。文献<citation id="236" type="reference">[<a class="sup">15</a>]</citation>中提出了一种基于卷积神经网络的方法,结合图像中阴影的外表模型以及语义模型构建多语境结构,为阴影图像生成阴影蒙版,从而达到阴影去除的效果,但因为没有从根源上选择误差较低的训练集,使得阴影去除图像的像素在全局均会发生变化,这并不符合阴影去除的初衷。</p>
                </div>
                <div class="p1">
                    <p id="58">基于以上问题和不足,本文基于一种新的生成对抗网络进行单幅图像的阴影去除,能为待处理的阴影图像生成更为准确的阴影蒙版图像,从而得到更为真实的无阴影图像。根据文献<citation id="244" type="reference">[<a class="sup">7</a>]</citation>可知,阴影图像可由无阴影图像和阴影蒙版图像在像素级别上相乘得到,因此得到了阴影蒙版图像就能得到阴影去除图像。所提网络由衰减器、生成器和判别器等3个部分构成。首先,为了减小样本存在的误差,筛选训练样本。利用带敏感因子的衰减器生成额外的样本训练网络以提高所提模型的拟合能力。衰减器的输入为样本阴影图像和标注阴影区域的阴影二值掩膜构成的四通道图像,旨在生成新的符合物理光照模型的阴影图像。这些新的数据一方面能扩大数据集增加模型的可靠性以及拟合能力,另一方面生成的图像与经过筛选的训练样本一样,与它们对应的真实无阴影图像具有较小的误差,这能提高模型训练的精度。其次,在衰减器提供充足训练样本的前提下,生成器结合感知损失函数,旨在生成准确的阴影蒙版;而判别器不断更新,并给予生成器反馈。最终判别器与衰减器、生成器达到平衡。然后直接将待处理阴影图像作为生成器的输入就能直接得到其对应的阴影蒙版图像。</p>
                </div>
                <div class="p1">
                    <p id="59">本文的主要贡献:1)提出一种基于衰减式生成对抗网络的阴影去除框架,能为待处理的阴影图像生成对应的阴影蒙版,从而实现阴影去除;2)提出了一种带敏感因子的衰减器,能够生成不同衰减程度且符合物理光照模型的阴影衰减图像,以作为额外可靠的数据集;3)与传统的生成器不同,本文将结合感知损失,并与判别器进行对抗训练,以大幅度提高生成蒙版的准确性,得到更自然的无阴影图像,在多个测试数据集上均能得到良好的结果。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">1 衰减式生成对抗网络</h3>
                <div class="p1">
                    <p id="61">传统基于阴影蒙版的阴影去除方法在估算阴影蒙版时需要提前定位阴影的位置。而定位阴影常常需要做阴影检测<citation id="253" type="reference"><link href="177" rel="bibliography" /><link href="179" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>或者使用用户交互的方式<citation id="254" type="reference"><link href="183" rel="bibliography" /><link href="185" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>标注出阴影的位置。因阴影检测的过程本身就十分具有挑战性,且这一过程常常因为缺乏鲁棒的阴影特征<citation id="255" type="reference"><link href="177" rel="bibliography" /><link href="207" rel="bibliography" /><link href="209" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>造成阴影检测不够准确或只在处理高清图像时才能得到较好的结果<citation id="245" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。基于用户交互的方式虽然能更为准确地定位阴影的位置,但整个检测过程缺乏全自动性,大批量处理图像时,需要较多的资源。生成对抗网络<citation id="256" type="reference"><link href="213" rel="bibliography" /><link href="215" rel="bibliography" /><link href="217" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>作为一种图像生成模型无需提前定位阴影的位置,也可应用于生成阴影蒙版。文献<citation id="246" type="reference">[<a class="sup">19</a>]</citation>提出的生成对抗网络由一个生成器G和一个判别器D组成。生成器接收一个随机噪声从而生成一幅逼真的图像。判别器从训练集中学习从而判断生成器生成的图像是真还是假。这两个部分是博弈关系。生成器生成图像旨在让判别器难以辨别真假,从而从训练集中学习出数据的分布。文献<citation id="247" type="reference">[<a class="sup">20</a>]</citation>则进一步拓展文献<citation id="248" type="reference">[<a class="sup">19</a>]</citation>,允许在生成器和判别器中引入额外的条件变量。文献<citation id="249" type="reference">[<a class="sup">21</a>]</citation>基于文献<citation id="250" type="reference">[<a class="sup">20</a>]</citation>,使用一幅输入图像作为约束条件,训练生成对抗网络以生成另一幅图像。利用深度神经网络自动学习特征的能力,可以大幅减小因人为选定特征而对结果造成的误差。由此,根据文献<citation id="251" type="reference">[<a class="sup">21</a>]</citation>,将样本阴影图像作为约束条件,旨在让生成器生成逼真的阴影蒙版,以达到当前判别器无法分辨的程度;而判别器在提高自己判别能力的同时也激励着生成器能力的提升;直至判别器无法辨认生成器生成的图像的真假之后,使用式(1)<citation id="252" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>计算得到无阴影图像:</p>
                </div>
                <div class="p1">
                    <p id="62"><b><i>x</i></b>=<b><i>y</i></b>·<b><i>x</i></b>′      (1)</p>
                </div>
                <div class="p1">
                    <p id="63">其中:<b><i>x</i></b>为阴影图像;<b><i>y</i></b>为阴影蒙版,表示由阴影造成的光照衰减效应;<b><i>x</i></b>′为无阴影图像。</p>
                </div>
                <div class="p1">
                    <p id="64">文献<citation id="257" type="reference">[<a class="sup">21</a>]</citation>对于阴影图像中非阴影区中偏暗的区域、光照衰减程度多样化的阴影区域以及阴影边界处理能力较差,导致无法生成准确的阴影蒙版,从而无法得到真实的无阴影图像。受到文献<citation id="258" type="reference">[<a class="sup">22</a>]</citation>的启发,该文使用一个卷积神经网络为主体进行阴影检测,可以提供额外的训练数据,用以提高网络的泛化能力,从而使得网络能够准确地区分非阴影区域中颜色偏暗区域和真实的阴影的区域。同样,本文在生成对抗网络的基础之上,引入一个衰减器<i>A</i>实现类似于文献<citation id="259" type="reference">[<a class="sup">22</a>]</citation>的功能,以提高整体网络的泛化能力。但文献<citation id="260" type="reference">[<a class="sup">22</a>]</citation>只应用于阴影检测,方法仍然具有较大的局限,虽然基于光照模型,能生成较为逼真的衰减阴影图像,但其在训练阶段,每次仅能为样本阴影图像生成一幅衰减的阴影图像,而且衰减的程度不可控制,有些样本中的阴影衰减程度较高,而还有很多样本几乎不衰减。基于以上这些问题,本文进一步提出敏感因子引导的衰减器辅以生成对抗网络进行训练,每次训练通过调整步长来控制敏感因子的取值,从而来生成相对于输入图像不同衰减程度的阴影图像。在本文中将所提网络称为衰减式生成对抗网络。通过增大可靠的衰减阴影图像样本,所提网络的处理能力得到大幅度增强,其训练流程如图1所示。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909039_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 衰减式生成网络训练流程" src="Detail/GetImg?filename=images/JSJY201909039_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 衰减式生成网络训练流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909039_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Training process of attenuated generative adversarial networks</p>

                </div>
                <div class="p1">
                    <p id="66">衰减式生成对抗网络由一个衰减器、一个生成器以及一个判别器组成。为了减小样本存在的误差,使用像素的均方根误差(Root Mean Square Error,RMSE)<citation id="261" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>筛选客观数据集作为所提网络的训练样本。样本训练集中每组图像将由样本阴影图像、标识阴影位置的阴影二值掩膜、真实的无阴影图像、样本阴影图像的真实阴影蒙版这四幅图像组成。如图1所示,训练过程中,衰减器的输入为训练样本中的阴影图像及其阴影检测的二值掩膜结果,其中⊕表示将三通道的RGB阴影图像和单通道的二值阴影检测结果堆叠成为四通道的输入,而受到敏感因子的引导与差异损失的约束,衰减器能够生成多幅符合物理光照模型的图像。生成器使用衰减器的输出作为额外的训练数据,结合感知损失,旨在生成准确的阴影蒙版。判别器判断衰减器产生的阴影衰减图像和生成器生成的阴影蒙版是否为真,同时给予衰减器以及生成器信息反馈,以达到相互促进相互学习的目的。网络训练完毕后,便可舍弃衰减器与判别器,直接将待处理的阴影图像作为生成器的输入,即可得到其对应的阴影蒙版,结合式(1)便能得到阴影去除结果。</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag">2 网络构成细节</h3>
                <h4 class="anchor-tag" id="68" name="68">2.1 <b>敏感因子引导的衰减器</b></h4>
                <div class="p1">
                    <p id="69">衰减器的引入可以为后续生成器提供符合物理光照模型的训练数据集,而使用敏感因子作为引导则可以控制原始阴影图像中阴影区域的衰减程度以及数量,这将极大程度地提高模型对于多种场景下不同程度阴影的处理能力。敏感因子引导的衰减器加强了非阴影区域中颜色偏暗区域的识别能力,从而生成更加合理、准确的阴影蒙版。在训练过程中,为衰减器构造合适的损失项并形成损失目标函数以达到以上目的。</p>
                </div>
                <div class="p1">
                    <p id="70">设<i>M</i>(<b><i>x</i></b>)是阴影图像<b><i>x</i></b>的阴影掩膜,<i>A</i>(<b><i>x</i></b>)为衰减器<i>A</i>关于<b><i>x</i></b>和<i>M</i>(<b><i>x</i></b>)的输出。首先,为了让衰减器的输出在非阴影区域的像素值保持不变,构建损失项<i>L</i><sub>1</sub>旨在抑制<b><i>x</i></b>在非阴影区域中像素的改变,如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∉</mo><mi>Μ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo></mrow></munder><mo stretchy="false">(</mo></mstyle><mi>A</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中<i>i</i>表示的是像素的序号。</p>
                </div>
                <div class="p1">
                    <p id="73">其次,衰减器生成的图像应该遵守物理光照模型。生成图像在阴影区域中的所有像素改变的幅度应该尽可能一致,如式(3)所示:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mo stretchy="false">{</mo><mtext>R</mtext><mo>,</mo><mtext>G</mtext><mo>,</mo><mtext>B</mtext><mo stretchy="false">}</mo></mrow></munder><mi>V</mi></mstyle><mo stretchy="false">(</mo><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo>-</mo><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中:<i>L</i><sub>2</sub>是物理损失,确保生成器生成的图像是真实可信的。(·)<sup><i>c</i></sup>表示一个像素在<i>c</i>通道上的像素值,<i>c</i>∈{R,G,B}。<i>V</i>(·)为求方差运算,其衡量了各个像素变化量的离散程度。</p>
                </div>
                <div class="p1">
                    <p id="76">仅仅使用上述的两种条件作为约束并不能生成衰减程度一致的衰减图像,每次衰减也只能得到一幅随机衰减程度的图像,因此,衰减器还应该有控制其衰减敏感度的敏感因子参数<i>μ</i>。衰减器虽然仍然受限于输入的场景图像,但通过调节<i>μ</i>能生成衰减程度不同的阴影图像用于衰减式生成对抗网络的训练。同时,参数<i>μ</i>也权衡生成图像与样本阴影图像在阴影区域之间的差异。如式(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Μ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>,</mo><mi>μ</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></munder><mo stretchy="false">(</mo></mstyle><mi>μ</mi><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>A</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中<i>L</i><sub>3</sub>是敏感损失。当<i>μ</i>接近于1的时候,优化目标函数时,会使得阴影内部像素在满足物理约束的基础上,不会改变太多。通过改变<i>μ</i>的大小。就可以产生衰减不同程度的阴影图像。在实验过程中, <i>μ</i>在[0,1]中采样,步长为0.2,则可将原始训练集扩大5倍。</p>
                </div>
                <div class="p1">
                    <p id="79">最终,衰减器<i>A</i>的损失目标函数由损失项<i>L</i><sub>1</sub>、<i>L</i><sub>2</sub>、<i>L</i><sub>3</sub>加权得到,如式(5)所示:</p>
                </div>
                <div class="p1">
                    <p id="80"><i>L</i><sub><i>A</i></sub>(<b><i>x</i></b>)=<i>λ</i><sub>1</sub><i>L</i><sub>1</sub>(<b><i>x</i></b>)+<i>λ</i><sub>2</sub><i>L</i><sub>2</sub>(<b><i>x</i></b>)+<i>λ</i><sub>3</sub><i>L</i><sub>3</sub>(<b><i>x</i></b>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="81">如图2所示,给出了不同敏感因子取值所得到的阴影衰减图像。图2(a)为样本阴影图像。图2(b)是通过文献<citation id="262" type="reference">[<a class="sup">22</a>]</citation>生成的衰减图像,衰减程度不可控,且只能生成单幅衰减图像。图2(c)、图2(d)、图2(e)分别为3个不同的敏感因子控制的图像。通过比较,较大的<i>μ</i>得到的图像,其阴影区域的颜色深度越接近样本阴影图像;反之,较小的<i>μ</i>将会得到阴影较弱的图像,值得注意的是,受到式(2)的约束,衰减器生成的阴影图像在非阴影区域是不会改变的。使用不同衰减程度图像训练所提网络中的生成器,将使得生成器的能力显著提升。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909039_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 阴影衰减图像" src="Detail/GetImg?filename=images/JSJY201909039_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 阴影衰减图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909039_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Attenuated shadow image</p>

                </div>
                <div class="p1">
                    <p id="83">衰减器的内部结构如表1所示。其中,卷积层5(3)表示第5个卷积层重复3次;ReLU为激活函数;BN为批标准化<citation id="263" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>;4/4表示衰减器的输入为一幅四通道图像,分别由三通道的RGB阴影图像和单通道的阴影二值掩膜组成,其输出是一幅在阴影区域存在衰减效果的阴影图像,该图像同样为RGB三个通道,表示为3/3。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表</b>1 <b>衰减器内部结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Internal structure of attenuator</p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td>层级<br />结构</td><td>卷积层1</td><td>卷积层2</td><td>卷积层3</td><td>卷积层4</td><td>卷积<br />层5(3)</td><td>卷积层6</td><td>卷积层7</td><td>卷积<br />层8(3)</td><td>卷积层9</td><td>卷积层10</td><td>卷积层11</td><td>卷积层12</td></tr><tr><td>输入</td><td>4/4,3/3</td><td>64</td><td>128</td><td>256</td><td>512</td><td>512</td><td>512</td><td>1 024</td><td>1 024</td><td>512</td><td>256</td><td>128</td></tr><tr><td><br />输出</td><td>64</td><td>128</td><td>256</td><td>512</td><td>512</td><td>512</td><td>512</td><td>512</td><td>256</td><td>128</td><td>64</td><td>3/3,3/3</td></tr><tr><td><br />前邻</td><td>—</td><td>ReLU</td><td>ReLU</td><td>ReLU</td><td>ReLU</td><td>ReLU</td><td>ReLU</td><td>ReLU</td><td>ReLU</td><td>ReLU</td><td>ReLU</td><td>ReLU</td></tr><tr><td><br />后接</td><td>—</td><td>BN</td><td>BN</td><td>BN</td><td>BN</td><td>—</td><td>BN</td><td>BN</td><td>BN</td><td>BN</td><td>BN</td><td>—</td></tr><tr><td><br />跳跃</td><td>卷积层12</td><td>卷积层11</td><td>卷积层10</td><td>卷积层9</td><td>卷积层8</td><td>—</td><td>—</td><td>卷积层5</td><td>卷积层4</td><td>卷积层3</td><td>卷积层2</td><td>卷积层1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">2.2 <b>多语境嵌入的生成器和判别器</b></h4>
                <div class="p1">
                    <p id="86">在衰减式生成对抗网络中,衰减器<i>A</i>输出的图像<i>A</i>(<b><i>x</i></b>)(包括生成的衰减图像和原始的样本图像)作为生成器<i>G</i>的输入。生成器<i>G</i>将<i>A</i>(<b><i>x</i></b>)作为条件变量,旨在生成阴影蒙版<i>G</i>(<i>A</i>(<b><i>x</i></b>))。<i>G</i>试图模拟由输入图像<i>A</i>(<b><i>x</i></b>)和样本真实阴影蒙版<b><i>y</i></b>组成的训练数据的分布。判别器<i>D</i>的输入为(<i>A</i>(<b><i>x</i></b>),<b><i>y</i></b>)或(<i>A</i>(<b><i>x</i></b>),<i>G</i>(<i>A</i>(<b><i>x</i></b>))),并且需要判断该组图像是否是真的训练数据。构建目标函数如式(6)所示:</p>
                </div>
                <div class="p1">
                    <p id="87"><i>G</i><sup>*</sup>=<i>E</i><sub><b><i>x</i></b>,<b><i>y</i></b>～<i>p</i><sub><i>data</i></sub>(<b><i>x</i></b>,<b><i>y</i></b>)</sub>[ln <i>D</i>(<i>A</i>(<b><i>x</i></b>),<b><i>y</i></b>)]+<i>E</i><sub><b><i>x</i></b>～<i>p</i><sub><i>data</i>(<b><i>x</i></b>)</sub></sub>[ln(1-<i>D</i>(<i>A</i>(<b><i>x</i></b>),<i>G</i>(<i>A</i>(<b><i>x</i></b>))))+<i>λ</i><sub>4</sub><font face="EU-HT">L</font><sub><i>L</i><sub>1</sub></sub>(<i>G</i>)      (6)</p>
                </div>
                <div class="p1">
                    <p id="88"><font face="EU-HT">L</font><sub><i>L</i><sub>1</sub></sub>(<i>G</i>)=<i>E</i><sub><b><i>x</i></b>,<b><i>y</i></b></sub>[‖<b><i>y</i></b>-<i>G</i>(<i>A</i>(<b><i>x</i></b>))‖]      (7)</p>
                </div>
                <div class="p1">
                    <p id="89">其中:<b><i>x</i></b>～<i>p</i><sub><i>data</i></sub>(<b><i>x</i></b>)为<b><i>x</i></b>服从<i>p</i><sub><i>data</i></sub>(<b><i>x</i></b>)分布,<i>E</i>为求期望运算。</p>
                </div>
                <div class="p1">
                    <p id="90">针对如何在去除阴影过程中提高阴影边缘的清晰度这一问题,为鼓励生成图像<i>G</i>(<i>A</i>(<b><i>x</i></b>))与真实阴影蒙版<b><i>y</i></b>差距更小,在实验过程中增加1个额外的感知损失,<font face="EU-HT">L</font><sub><i>L</i><sub>1</sub></sub>距离(真实图像和生成图像)。此时判别器的损失不变,生成器的损失变了。加入<font face="EU-HT">L</font><sub><i>L</i><sub>1</sub></sub>后可在阴影边缘局部减少阴影模糊程度,提高清晰度。<font face="EU-HT">L</font><sub><i>L</i><sub>1</sub></sub>一方面在鼓励生成图像要尽量靠近目标图像,一方面使用感知损失协助生成图像在语义上与目标图像更为相近。</p>
                </div>
                <div class="p1">
                    <p id="91">生成器的内部结构与衰减器,而判别器的内部结构如表2所示,其本质同样是一个卷积神经网络,其中,LReLU为激活函数。6/6表示判别器的输入是一个图像对,分别是阴影图像和阴影蒙版图像。将RGB阴影图像和RGB的阴影蒙版图像堆叠成六通道作为判别器的输入。阴影图像为衰减器的输出,其中包括样本阴影图像与阴影衰减图像。而阴影蒙版图像有可能来源于样本真实阴影蒙版或来源于生成器为样本阴影图像生成的阴影蒙版图像。判别器的输出为图像对是否为真的概率,即阴影蒙版是该样本阴影图像对应的阴影蒙版的可能性。</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表</b>2 <b>判别器内部结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Internal structure of discriminator</p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td><br />层次结构</td><td>卷积层1</td><td>卷积层2</td><td>卷积层3</td><td>卷积层4</td><td>卷积层5</td></tr><tr><td><br />输入</td><td>6/6</td><td>64</td><td>128</td><td>256</td><td>512</td></tr><tr><td><br />输出</td><td>64</td><td>128</td><td>256</td><td>512</td><td>1</td></tr><tr><td><br />前邻</td><td>—</td><td>LReLU</td><td>LReLU</td><td>LReLU</td><td>LReLU</td></tr><tr><td><br />后接</td><td>—</td><td>BN</td><td>BN</td><td>BN</td><td>Sigmoid</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">2.3 <b>网络训练细节</b></h4>
                <div class="p1">
                    <p id="94">衰减式生成对抗网络的训练可视为衰减器、生成器与判别器之间的一场博弈,并可形式化为求解目标函数,如式(8)所示:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>A</mi><mo>,</mo><mi>G</mi></mrow></munder><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mi>A</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>G</mi><msup><mrow></mrow><mo>*</mo></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">如式(8)所示,衰减器与生成器一组与判别器进行对抗训练,判别器旨在最大化目标函数,而衰减器和生成器却试图最小化目标函数。</p>
                </div>
                <div class="p1">
                    <p id="97">为了求解目标函数(8),达到局部最优解,在选取激活函数时,应该注意如下问题。衰减器/生成器的学习率通常较小,可以使用ReLU 加快训练速度。在判别器输出层使用 Sigmoid。这是因为Sigmoid在特征相差明显时的效果很好,在循环过程中会不断地增强特征效果。而判别器的其他层均选用收敛速度快又不易使神经元坏死的 LReLU。这是因为ReLU 函数虽然收敛速度快,但其最大的缺点是当输入小于零时的梯度为 0,这样就导致负值的梯度被设置为 0,此时神经元会处于失活状态,不会对任何数据有所反应,尤其是当学习率很大时,神经元会大面积坏死 ,</p>
                </div>
                <div class="p1">
                    <p id="98">使用Tensorflow<citation id="264" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>实现本文网络,LReLU的斜率设置为0.2。使用了Adam优化方法和一个交替梯度更新策略求解式(8)。首先,固定衰减器和生成器中的权值,采用梯度下降更新判别器中的权值;然后同理,固定判别器中的权值不变,使用梯度下降更新生成器和衰减器中的权值。而在更新生成器中的权重时,优先训练外表结构分支和语义分支,然后再将这两个分支通过一个卷积层连接到一起,则可完成生成器的一次迭代。按照均值为0、方差为0.2的正态分布初始化衰减器、生成器和判别器中所有的卷积层和反卷积层的权重,设置偏置值为0。数据增大采用图像裁剪的方法,将原始286×286的图像随机裁剪成多个256×256的子图像,得到4张子图,然后进行水平翻转得8张子图进一步增大数据集。通过大量实验,当<i>λ</i><sub>1</sub>=0.2,<i>λ</i><sub>2</sub>=0.3,<i>λ</i><sub>3</sub>=0.3,<i>λ</i><sub>4</sub>=0.2时可以得到较好的结果。</p>
                </div>
                <h3 id="99" name="99" class="anchor-tag">3 实验结果分析</h3>
                <div class="p1">
                    <p id="100">本文实验中使用<i>Ubantu</i> 18.04操作系统,内存为8 <i>GB</i>,双核<i>Inter CPU I</i>5,图像处理器(<i>GPU</i>)采用<i>NVIDIA GTX</i> 1080<i>Ti</i>。在数据集部分,挑选<i>UIUC</i>(<i>University of Illinois at Urbana</i>-<i>Champaign</i>)<citation id="265" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、<i>ISTD</i>(<i>a new Dataset with Image Shadow Triplets</i>)<citation id="266" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和<i>SRD</i>(<i>a new Dataset for Shadow Removal</i>)<citation id="267" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>这3个有代表性的数据集进行测试。其中:<i>SRD</i><citation id="268" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>包含3 088组阴影和无阴影图像;<i>UIUC</i><citation id="269" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>包含76组阴影和无阴影图像;<i>ISTD</i><citation id="270" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>包含1 870组阴影、阴影掩膜和无阴影图像。本文经过一系列预处理,便于提供给所提网络学习。具体地,首先,筛选上述3个数据集中一共5 034组图像,使用<i>RMSE</i>计算阴影图像和真实无阴影图像在非阴影区域的误差,选取误差最少且场景多样的800组图像。</p>
                </div>
                <div class="p1">
                    <p id="101">然后,将这800组图像中阴影图像和无阴影图像相减,在3个通道上设置阈值为20,再进行形态学滤波,最后人工调整错误的阴影像素标签,为数据集得到相应的阴影二值掩膜作为其阴影检测的结果,其中来自于<i>ISTD</i>的数据因自带了阴影检测结果则跳过这一步骤。最后为每组图像补充上真实阴影蒙版构成每四幅图像一组的综合筛选数据集。抽取综合筛选数据集中20%的数据用于本文方法的可行性检验。挑选综合筛选数据集中的几组图像作为示例,如图3所示。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909039_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 综合筛选数据集示例" src="Detail/GetImg?filename=images/JSJY201909039_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 综合筛选数据集示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909039_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Examples of synthesized filtered data sets</i></p>

                </div>
                <div class="p1">
                    <p id="103">图3(<i>a</i>)为综合筛选数据集中样本阴影图像。图3(<i>b</i>)为阴影图像对应的真实无阴影图像。图3(<i>c</i>)为通过阈值化处理以及人工标注修正的阴影二值掩膜。图3(<i>d</i>)真实阴影蒙版,其体现了阴影造成的光照衰减效应,为<i>RGB</i>三通道图像。</p>
                </div>
                <div class="p1">
                    <p id="104">在综合筛选数据集中选取四幅图像,从获取阴影蒙版角度,将本文方法与文献<citation id="271" type="reference">[<a class="sup">15</a>]</citation>的方法进行比较,文献<citation id="272" type="reference">[<a class="sup">15</a>]</citation>提出了一种基于卷积神经网络的方法,其构建多语境结构,为阴影图像生成阴影蒙版,该方法十分具有代表性,而且也能得到不错的结果。分别使用文献<citation id="273" type="reference">[<a class="sup">15</a>]</citation>与本文方法为阴影图像生成阴影蒙版,结果如图4所示。</p>
                </div>
                <div class="p1">
                    <p id="105">图4(<i>c</i>)中因为文献<citation id="274" type="reference">[<a class="sup">15</a>]</citation>的方法使用的训练数据集存在较大误差,使得后续的阴影去除结果在非阴影区域也会存在大量的变化。如图4(<i>f</i>)所示,无论是海报上的人物、草地上、墙上以及地面上的阴影,在恢复之后明显会出现过亮的情况,导致阴影边界尤为明显。而观察图4(<i>d</i>),本文方法能够得到更加准确的阴影蒙版。比较图4(<i>b</i>)和图4(<i>d</i>),可以发现,在阴影以外的区域,本文方法几乎不会存在虚假的蒙版信息,这使得本文方法在后续的无阴影图像中,在非阴影区域几乎不会发生变化,结果更加合理。如图4(<i>g</i>)所示,可以发现这四组实验中,本文方法因为能够得到准确的阴影蒙版,使得无阴影结果在非阴影区域得到了完整的信息保留,相比文献<citation id="275" type="reference">[<a class="sup">15</a>]</citation>方法,阴影边界过渡更加自然。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909039_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 两种方法生成的阴影蒙版与无阴影结果" src="Detail/GetImg?filename=images/JSJY201909039_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 两种方法生成的阴影蒙版与无阴影结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909039_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Shadow masks and shadowless results generated by two methods</i></p>

                </div>
                <div class="p1">
                    <p id="107">在综合筛选数据集中挑选四幅不同场景的阴影图像,从阴影去除的角度,将本文方法与文献<citation id="276" type="reference">[<a class="sup">1</a>]</citation>和文献<citation id="277" type="reference">[<a class="sup">13</a>]</citation>的方法进行比较。这两种方法非常具有代表性:文献<citation id="278" type="reference">[<a class="sup">1</a>]</citation>的方法基于传统机器学习的方法,使用自定义的阴影特征,先将阴影检测出来,然后基于物理光照模型将阴影去除;文献<citation id="279" type="reference">[<a class="sup">13</a>]</citation>的方法利用基于条件生成对抗网络的多任务学习方法,能够同时进行阴影检测和阴影去除任务。阴影去除的结果如图5所示。</p>
                </div>
                <div class="p1">
                    <p id="108">从图5可以看出,文献<citation id="280" type="reference">[<a class="sup">1</a>]</citation>的方法因使用缺乏鲁棒性人为定义的阴影特征,从而导致阴影检测的不精准,最终无法得到令人满意的阴影去除结果。如第一行中草地上遮阳伞的部分阴影、第二行中水泥地面上较浅的阴影、第三行中大面积地贴砖、第四行中手持砖块的人物阴影均无法得到准确的检测,从而导致这些区域均未经处理。文献<citation id="281" type="reference">[<a class="sup">13</a>]</citation>的方法相比文献<citation id="282" type="reference">[<a class="sup">1</a>]</citation>的方法能够得到更好的结果,但是复原区域存在明显的亮度偏高、轻度模糊的情况,图像整体过渡不均匀,例如第一行到第三行中依然能够看到遮阳伞的轮廓,第四行中人物的躯干,这说明深度网络对于阴影的检测与识别能力已经远远超过了基于自定义特征阴影检测能力,但是在去除能力上仍然具有较大的上升空间。样本场景的不足、精度的不够以及没有结合感知损失等原因造成深度网络阴影去除能力的不足。本文方法继承了深度神经网络的特征学习能力,为阴影图像直接生成阴影蒙版。衰减器生成的衰减的阴影图像作为强大的数据支撑,使得模型对于任意程度的阴影都有较为鲁棒的识别能力,拓展了模型的泛化能力。引入感知损失的生成器能够生成更为精准的阴影蒙版图像,从而能够得到好的阴影去除结果。本文方法处理这4幅图像时,还原的阴影区域的亮度信息更为准确,阴影边界过渡更为自然的同时减少对非阴影区域像素的改变,图像整体更为均匀。在综合筛选数据集上,使用<i>RMSE</i>和结构相似度(<i>Structural SIMilarity index</i>, <i>SSIM</i>)<citation id="283" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>作为衡量指标,将本文方法与上述具体代表性的单幅图像阴影去除方法进行客观数据比较。其中<i>RMSE</i>计算了去阴影图像和真实无阴影图像对应的每个像素的误差,而<i>SSIM</i>反映了结构信息,其更符合人类视觉的感知。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909039_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同方法的阴影去除结果" src="Detail/GetImg?filename=images/JSJY201909039_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同方法的阴影去除结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909039_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Shadow removal results of different methods</i></p>

                </div>
                <div class="p1">
                    <p id="110">将本文方法与文献<citation id="290" type="reference">[<a class="sup">1</a>,<a class="sup">13</a>,<a class="sup">15</a>]</citation>的方法进行量化评估,如表3所示。测试图像为综合筛选数据集的20%,这些图像均具有相应的真实无阴影图像、阴影检测结果。将阴影去除图像与真实无阴影图像从阴影区域、非阴影区域以及整体区域进行量化分析,分别计算测试图像的<i>RMSE</i>与<i>SSIM</i>值的平均值。<i>RMSE</i>值越低说明阴影去除图像与真实无阴影图像之间的误差越小,<i>SSIM</i>值越大说明阴影去除图像从结构上与真实无阴影图像之间更为相似。因为文献<citation id="284" type="reference">[<a class="sup">1</a>]</citation>的方法基于传统机器学习方法,对阴影区域缺乏鲁棒的特征导致阴影检测十分不准确,从而导致不能拥有较好的阴影去除结果,所以在<i>RMSE</i>上具有较大的误差,在<i>SSIM</i>上相似度也偏低。文献<citation id="285" type="reference">[<a class="sup">15</a>]</citation>的方法基于卷积神经网络提出一种端到端的阴影蒙版生成方式,但因其受到训练样本的限制,在深浅程度不同的阴影场景下,不能得到较好的阴影去除结果,其<i>RMSE</i>和<i>SSIM</i>上的表现虽然相比文献<citation id="286" type="reference">[<a class="sup">1</a>]</citation>的方法有了较大提升,但仍然具有较大的上升空间。文献<citation id="287" type="reference">[<a class="sup">13</a>]</citation>的方法基于多任务学习使用两个联合的生成对抗网络分别完成阴影检测和阴影去除的工作,同样受到训练数据集的约束且并未考虑生成图像和目标图像之间的感知损失。因此其结果虽优于文献<citation id="288" type="reference">[<a class="sup">15</a>]</citation>的方法,但仍然没有本文方法好。本文方法在该误差评估上具有最低的误差值以及最高的结构相似度。基于传统学习方法的文献<citation id="289" type="reference">[<a class="sup">1</a>]</citation>方法消耗时间较长,文献<citation id="291" type="reference">[<a class="sup">13</a>,<a class="sup">15</a>]</citation>的方法和本文方法均基于深度学习,在训练好模型后进行预测,在算法时间上有明显优势。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表</b>3 <b>各个方法在不同区域的实验结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Experimental results comparison of each method in different regions</i></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td rowspan="2">区域类型</td><td colspan="3"><br />文献[1]方法</td><td rowspan="2"></td><td colspan="3"><br />文献[15]方法</td><td rowspan="2"></td><td colspan="3"><br />文献[13]方法</td><td rowspan="2"></td><td colspan="3"><br />本文方法</td></tr><tr><td><br /><i>RMSE</i></td><td><i>SSIM</i></td><td><i>TIME</i>/<i>s</i></td><td><br /><i>RMSE</i></td><td><i>SSIM</i></td><td><i>TIME</i>/<i>s</i></td><td><br /><i>RMSE</i></td><td><i>SSIM</i></td><td><i>TIME</i>/<i>s</i></td><td><br /><i>RMSE</i></td><td><i>SSIM</i></td><td><i>TIME</i>/<i>s</i></td></tr><tr><td>阴影</td><td>25.65</td><td>0.934 8</td><td>120</td><td></td><td>11.62</td><td>0.973 4</td><td>5</td><td></td><td>10.57</td><td>0.975 1</td><td>6</td><td></td><td>9.72</td><td>0.976 2</td><td>4</td></tr><tr><td><br />非阴影</td><td>4.74</td><td>0.982 3</td><td>120</td><td></td><td>4.02</td><td>0.983 1</td><td>5</td><td></td><td>3.56</td><td>0.985 2</td><td>6</td><td></td><td>3.28</td><td>0.986 9</td><td>4</td></tr><tr><td><br />整体</td><td>6.22</td><td>0.967 2</td><td>120</td><td></td><td>5.48</td><td>0.973 6</td><td>5</td><td></td><td>5.18</td><td>0.978 2</td><td>6</td><td></td><td>4.62</td><td>0.982 3</td><td>4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="112" name="112" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="113">本文为得到体现阴影图像中光照衰减效应的阴影蒙版图像,从而得到更加真实、自然的无阴影结果,提出一种新的阴影去除框架。所提框架由一个带敏感因子的衰减器、一个生成器以及一个判别器组成。带有敏感因子的衰减器能够以不同的程度生成符合物理光照模型的阴影图像,这大大地增加了训练数据的数量,同时也增加了模型对于不同程度阴影的识别与处理能力。生成器使用衰减器的输出作为额外的训练数据用于训练,结合了感知损失,旨在生成逼真的阴影蒙版。判别器判断衰减器产生的阴影衰减图像和生成器生成的阴影蒙版是否为真,同时给予生成器信息反馈,使其能与生成器共同进步。在客观数据集上,对所提方法进行了诸多实验,在像素的均方根误差以及结构相似性上均能得到较好的成绩。下一步,将会把所提框架应用于视频去阴影领域,同时解决无阴影视频结果在帧与帧之间的时空连贯性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="177">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Paired regions for shadow detection and removal">

                                <b>[1]</b> GUO R,DAI Q,HOIEM D.Paired regions for shadow detection and removal [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(12):2956-2967.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic Shadow Detection and Removal from a Single Image">

                                <b>[2]</b> KHAN S H,BENNAMOUN M,SOHEL F,et al.Automatic shadow detection and removal from a single image [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(3):431-446.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201709022&amp;v=MDAxNTMzenFxQnRHRnJDVVI3cWZadVpzRnlqblZMM0lJVGZTZHJHNEg5Yk1wbzlIWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 张营,朱岱寅,俞翔,等.一种VideoSAR动目标阴影检测方法[J].电子与信息学报,2017,39(9):2197-2202.(ZHANG Y,ZHU D Y,YU X,et al.Approach to moving targets shadow detection for VideoSAR [J].Journal of Electronics and Information Technology,2017,39(9):2197-2202.)
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shadow remover:Image shadow removal based on illumination recovering optimization">

                                <b>[4]</b> ZHANG L,ZHANG Q,XIAO C.Shadow remover:image shadow removal based on illumination recovering optimization [J].IEEE Transactions on Image Processing,2015,24(11):4623-4636.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Video shadow removal using spatio-temporal illumination transfer">

                                <b>[5]</b> ZHANG L,ZHU Y,LIAO B,et al.Video shadow removal using spatio-temporal illumination transfer [J].Computer Graphics Forum,2017,36(7):125-134.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shadow optimization from structured deep edge detection">

                                <b>[6]</b> SHEN L,CHUA T W,LEMAN K.Shadow optimization from structured deep edge detection [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:2067-2074.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Texture-consistent shadow removal">

                                <b>[7]</b> LIU F,GLEICHER M.Texture-consistent shadow removal [C]// Proceedings of the 2008 European Conference on Computer Vision,LNCS 5305.Berlin:Springer,2008:437-450.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Removing Shadows from Images,&amp;quot;">

                                <b>[8]</b> FINLAYSON G D,HORDLEY S D,DREW M S.Removing shadows from images [C]// Proceedings of the 2002 European Conference on Computer Vision,LNCS 2353.Berlin:Springer,2002:823-836.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the removal of shadows from images">

                                <b>[9]</b> FINLAYSON G D,HORDLEY S D,LU C,et al.On the removal of shadows from images [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(1):59-68.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Editing Soft Shadows in a Digital Photograph">

                                <b>[10]</b> MOHAN A,TUMBLIN J,CHOUDHURY P.Editing soft shadows in a digital photograph [J].IEEE Computer Graphics and Applications,2007,27(2):23-31.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Bayesian approach to digital matting">

                                <b>[11]</b> CHUANG Y,CURLESS B,SALESIN D H,et al.A Bayesian approach to digital matting [C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2001,2:264.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shadow removal using intensity surfaces and texture anchor points">

                                <b>[12]</b> ARBEL E,HEL-OR H.Shadow removal using intensity surfaces and texture anchor points [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2011,33(6):1202-1216.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal">

                                <b>[13]</b> WANG J,LI X,HUI L,et al.Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal [C]// Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:1788-1797.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape, illumination, and reflectance from shading">

                                <b>[14]</b> BARRON J T,MALIK J.Shape,illumination,and reflectance from shading [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(8):1670-1687.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeshadowNet:A Multi-context Embedding Deep Network for Shadow Removal">

                                <b>[15]</b> QU L,TIAN J,HE S,et al.DeshadowNet:a multi-context embedding deep network for shadow removal [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017:2308-2316.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to recognize shadows in monochromatic natural images">

                                <b>[16]</b> ZHU J,SAMUEL K G G,MASOOD S Z,et al.Learning to recognize shadows in monochromatic natural images [C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2010:223-230.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting ground shadows in outdoor consumerphotographs">

                                <b>[17]</b> LALONDE J,EFROS A A,NARASIMHAN S G.Detecting ground shadows in outdoor consumer photographs [C]// Proceedings of the 2010 European Conference on Computer Vision,LNCS 6312.Berlin:Springer,2010:322-335.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003546640&amp;v=MDAyOTJyeG94Y01IN1I3cWRaK1p1RmlybFU3M1BJbDQ9Tmo3QmFyTzRIdEhQcW90RFl1OFBZM2s1ekJkaDRqOTlTWHFS&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> FINLAYSON G D,DREW M S,LU C.Entropy minimization for shadow removal [J].International Journal of Computer Vision,2009,85(1):35-57.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[19]</b> GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets [C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014,2:2672-2680.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[20]</b> MIRZA M,OSINDERO S.Conditional generative adversarial nets [C]// Proceedings of the 2014 Advances in Neural Information Processing Systems.Montreal,Canada:[s.n.],2014:2672-2680.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image-to-Image Translation with Conditional Adversarial Networks">

                                <b>[21]</b> ISOLA P,ZHU J,ZHOU T,et al.Image-to-image translation with conditional adversarial networks [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:5967-5976.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A+D net:training a shadow detector with adversarial shadow attenuation">

                                <b>[22]</b> LE H,VICENTE T F Y,NGUYEN V,et al.A+D net:training a shadow detector with adversarial shadow attenuation [C]// Proceedings of the 2018 European Conference on Computer Vision,LNCS 11206.Berlin:Springer,2018:680-696.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201801020&amp;v=MTIzODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpuVkwzSUlUZlNkckc0SDluTXJvOUhaSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 施文娟,孙彦景,左海维,等.基于视频自然统计特性的无参考移动终端视频质量评价[J].电子与信息学报,2018,40(1):143-150.(SHI W J,SUN Y J,ZUO H W,et al.No-reference mobile video quality assessment based on video natural statistics [J].Jounal of Electronics and Information Technology,2018,40(1):143-150.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[24]</b> IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// Proceedings of the 32nd International Conference on International Conference on Machine Learning.Lille,France:[s.n.],2015:448-456.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual losses for real-time style transfer and superresolution">

                                <b>[25]</b> JOHNSON J,ALAHI A,LI F.Perceptual losses for real-time style transfer and super-resolution [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9906.Berlin:Springer,2016:694-711.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing dataflow graphs of deep learning models in tensor Flow">

                                <b>[26]</b> WONGSUPHASAWAT K,SMILKOV D,WEXLER J,et al.Visualizing dataflow graphs of deep learning models in TensorFlow [J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):1-12.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909039" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909039&amp;v=MDU0MzZxQnRHRnJDVVI3cWZadVpzRnlqblZMM0lMejdCZDdHNEg5ak1wbzlHYllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
