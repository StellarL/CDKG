<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775229033750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904003%26RESULT%3d1%26SIGN%3d8e85i1p27UfrmzB0V4Z2Oqsh4fk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904003&amp;v=MDI3Nzc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWNy9CTHo3QmQ3RzRIOWpNcTQ5Rlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="1.1 &lt;b&gt;支持向量引导的字典学习模型&lt;/b&gt;">1.1 <b>支持向量引导的字典学习模型</b></a></li>
                                                <li><a href="#84" data-title="1.2 &lt;b&gt;支持向量机及其泛化误差界&lt;/b&gt;">1.2 <b>支持向量机及其泛化误差界</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="2 泛化误差界指导的字典学习模型 ">2 泛化误差界指导的字典学习模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="2.1 &lt;b&gt;模型&lt;/b&gt;">2.1 <b>模型</b></a></li>
                                                <li><a href="#127" data-title="2.2 &lt;b&gt;求解模型&lt;/b&gt;">2.2 <b>求解模型</b></a></li>
                                                <li><a href="#186" data-title="2.3 &lt;b&gt;分类方法&lt;/b&gt;">2.3 <b>分类方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#194" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#196" data-title="3.1 &lt;b&gt;模型参数对识别率的影响&lt;/b&gt;">3.1 <b>模型参数对识别率的影响</b></a></li>
                                                <li><a href="#200" data-title="3.2 &lt;b&gt;标准数据介绍和实验设置及其结果&lt;/b&gt;">3.2 <b>标准数据介绍和实验设置及其结果</b></a></li>
                                                <li><a href="#219" data-title="3.3 &lt;b&gt;模型收敛性讨论&lt;/b&gt;">3.3 <b>模型收敛性讨论</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#222" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#190" data-title="图2 SVGDL和本文算法的三个不同可变参数 &lt;i&gt;λ&lt;/i&gt;、&lt;i&gt;τ&lt;/i&gt;和&lt;i&gt;θ&lt;/i&gt;在Extended Yale B上的识别率">图2 SVGDL和本文算法的三个不同可变参数 <i>λ</i>、<i>τ</i>和<i>θ</i>在Extended Yale B上的识别率</a></li>
                                                <li><a href="#193" data-title="图1 GEBGDL算法流程">图1 GEBGDL算法流程</a></li>
                                                <li><a href="#199" data-title="&lt;b&gt;表&lt;/b&gt;1 7&lt;b&gt;个数据集的参数设置&lt;/b&gt;"><b>表</b>1 7<b>个数据集的参数设置</b></a></li>
                                                <li><a href="#204" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同算法在&lt;/b&gt;&lt;i&gt;USPS&lt;/i&gt;&lt;b&gt;数据集上的平均识别率&lt;/b&gt; %"><b>表</b>2 <b>不同算法在</b><i>USPS</i><b>数据集上的平均识别率</b> %</a></li>
                                                <li><a href="#206" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同算法在&lt;/b&gt;Extended Yale B&lt;b&gt;数据集上的平均识别率和标准差&lt;/b&gt; %"><b>表</b>3 <b>不同算法在</b>Extended Yale B<b>数据集上的平均识别率和标准差</b> %</a></li>
                                                <li><a href="#209" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同算法在&lt;/b&gt;AR&lt;b&gt;数据集上的平均识别率和标准差&lt;/b&gt; %"><b>表</b>4 <b>不同算法在</b>AR<b>数据集上的平均识别率和标准差</b> %</a></li>
                                                <li><a href="#210" data-title="&lt;b&gt;表&lt;/b&gt;7 &lt;b&gt;不同算法在&lt;/b&gt;Caltech101&lt;b&gt;数据集上的平均识别率和标准差&lt;/b&gt; %"><b>表</b>7 <b>不同算法在</b>Caltech101<b>数据集上的平均识别率和标准差</b> %</a></li>
                                                <li><a href="#212" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同算法在&lt;/b&gt;ORL&lt;b&gt;数据集上的平均识别率和标准差&lt;/b&gt; %"><b>表</b>5 <b>不同算法在</b>ORL<b>数据集上的平均识别率和标准差</b> %</a></li>
                                                <li><a href="#214" data-title="&lt;b&gt;表&lt;/b&gt;6 ORL&lt;b&gt;数据集上降维前后算法效率及识别率、标准差和耗时&lt;/b&gt;"><b>表</b>6 ORL<b>数据集上降维前后算法效率及识别率、标准差和耗时</b></a></li>
                                                <li><a href="#217" data-title="&lt;b&gt;表&lt;/b&gt;8 &lt;b&gt;不同算法在&lt;/b&gt;COIL20&lt;b&gt;数据集上的平均识别率和标准差&lt;/b&gt; %"><b>表</b>8 <b>不同算法在</b>COIL20<b>数据集上的平均识别率和标准差</b> %</a></li>
                                                <li><a href="#218" data-title="&lt;b&gt;表&lt;/b&gt;9 &lt;b&gt;不同算法在&lt;/b&gt;COIL100&lt;b&gt;数据集上的平均识别率和标准差&lt;/b&gt; %"><b>表</b>9 <b>不同算法在</b>COIL100<b>数据集上的平均识别率和标准差</b> %</a></li>
                                                <li><a href="#221" data-title="图3 本文算法在3个数据集的收敛曲线">图3 本文算法在3个数据集的收敛曲线</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="277">


                                    <a id="bibliography_1" title="WANG Z, LIU D, YANG J, et al.Deep networks for image superresolution with sparse prior[C]//ICCV 2015:Proceedings of the2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2016:370-378." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Networks for Image Super-Resolution with Sparse Prior">
                                        <b>[1]</b>
                                        WANG Z, LIU D, YANG J, et al.Deep networks for image superresolution with sparse prior[C]//ICCV 2015:Proceedings of the2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2016:370-378.
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_2" title="LUO Y, XU Y, JI H.Removing rain from a single image via discriminative sparse coding[C]//ICCV 2015:Proceedings of the2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:3397-3405." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Removing Rain From a Single Image via Discriminative Sparse Coding">
                                        <b>[2]</b>
                                        LUO Y, XU Y, JI H.Removing rain from a single image via discriminative sparse coding[C]//ICCV 2015:Proceedings of the2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:3397-3405.
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_3" title="WRIGHT J, YANG A Y, GANESH A, et al.Robust face recognition via sparse representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2009, 31 (2) :210-227." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust face recognition via sparse representation">
                                        <b>[3]</b>
                                        WRIGHT J, YANG A Y, GANESH A, et al.Robust face recognition via sparse representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2009, 31 (2) :210-227.
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_4" title="LIU Q, LIU C.A novel locally linear KNN model for visual recognition[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1329-1337." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel locally linear KNN model for visual recognition">
                                        <b>[4]</b>
                                        LIU Q, LIU C.A novel locally linear KNN model for visual recognition[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1329-1337.
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_5" title="AHARON M, ELAD M, BRUCKSTEIN A.K-SVD:an algorithm for designing overcomplete dictionaries for sparse representation[J].IEEE Transactions on Signal Processing, 2006, 54 (11) :4311-4322." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation">
                                        <b>[5]</b>
                                        AHARON M, ELAD M, BRUCKSTEIN A.K-SVD:an algorithm for designing overcomplete dictionaries for sparse representation[J].IEEE Transactions on Signal Processing, 2006, 54 (11) :4311-4322.
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_6" title="ZHANG Q, LI B.Discriminative K-SVD for dictionary learning in face recognition[C]//Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:2691-2698." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative K-SVD for dictionary learningin face recognition">
                                        <b>[6]</b>
                                        ZHANG Q, LI B.Discriminative K-SVD for dictionary learning in face recognition[C]//Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:2691-2698.
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_7" title="JIANG Z, LIN Z, DAVIS L S.Label consistent K-SVD:learning a discriminative dictionary for recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2651-2664." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Label consistent KSVD:Learning a discriminative dictionary for recognition">
                                        <b>[7]</b>
                                        JIANG Z, LIN Z, DAVIS L S.Label consistent K-SVD:learning a discriminative dictionary for recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2651-2664.
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_8" title="LI Z, LAI Z, XU Y, et al.A locality-constrained and label embedding dictionary learning algorithm for image classification[J].IEEETransactions on Neural Networks and Learning Systems, 2017, 28 (2) :278-293." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A locality-constrained and label embedding dictionary learning algorithm for image classification">
                                        <b>[8]</b>
                                        LI Z, LAI Z, XU Y, et al.A locality-constrained and label embedding dictionary learning algorithm for image classification[J].IEEETransactions on Neural Networks and Learning Systems, 2017, 28 (2) :278-293.
                                    </a>
                                </li>
                                <li id="293">


                                    <a id="bibliography_9" title="YANG M, ZANG L, FENG X, et al.Sparse representation based Fisher discrimination dictionary learning for image classification[J].International Journal of Computer Vision, 2014, 109 (3) :209-232." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14081200000835&amp;v=MTAwNzE4SHRuTnJZOUZaT3NQQkg4OG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGc1FhaHM9Tmo3QmFySw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        YANG M, ZANG L, FENG X, et al.Sparse representation based Fisher discrimination dictionary learning for image classification[J].International Journal of Computer Vision, 2014, 109 (3) :209-232.
                                    </a>
                                </li>
                                <li id="295">


                                    <a id="bibliography_10" title="CAI S, ZUO W, ZHANG L, et al.Support vector guided dictionary learning[C]//ECCV 2014:Proceedings of the 13th European Conference on Computer Vision, LNCS 8692.Berlin:Springer, 2014:624-639." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support Vector Guided Dictionary Learning">
                                        <b>[10]</b>
                                        CAI S, ZUO W, ZHANG L, et al.Support vector guided dictionary learning[C]//ECCV 2014:Proceedings of the 13th European Conference on Computer Vision, LNCS 8692.Berlin:Springer, 2014:624-639.
                                    </a>
                                </li>
                                <li id="297">


                                    <a id="bibliography_11" title="RIGAMONTI R, BRWON M A, LEPTIT V.Are sparse representations really relevant for image classification?[C]//Proceedings of the CVPR 2011.Washington, DC:IEEE Computer Society, 2011:1545-1552." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Are sparse representations really relevant for image classification?">
                                        <b>[11]</b>
                                        RIGAMONTI R, BRWON M A, LEPTIT V.Are sparse representations really relevant for image classification?[C]//Proceedings of the CVPR 2011.Washington, DC:IEEE Computer Society, 2011:1545-1552.
                                    </a>
                                </li>
                                <li id="299">


                                    <a id="bibliography_12" title="ZHANG L, YANG M, FENG X.Sparse representation or collaborative representation:Which helps face recognition?[C]//Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:471-478." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse representation or collaborative representation:which helps face recognition">
                                        <b>[12]</b>
                                        ZHANG L, YANG M, FENG X.Sparse representation or collaborative representation:Which helps face recognition?[C]//Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:471-478.
                                    </a>
                                </li>
                                <li id="301">


                                    <a id="bibliography_13" title="MEHTA N A, GRAY A G.Sparsity-based generalization bounds for predictive sparse coding[EB/OL].[2018-05-10].http://www.jmlr.org/proceedings/papers/v28/mehta13.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparsity-based generalization bounds for predictive sparse coding">
                                        <b>[13]</b>
                                        MEHTA N A, GRAY A G.Sparsity-based generalization bounds for predictive sparse coding[EB/OL].[2018-05-10].http://www.jmlr.org/proceedings/papers/v28/mehta13.pdf.
                                    </a>
                                </li>
                                <li id="303">


                                    <a id="bibliography_14" title="CAI S, ZHANG L, ZUO W, et al.A probabilistic collaborative representation based approach for pattern classification[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:2950-2959." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Probabilistic Collaborative Representation Based Approach for Pattern Classification">
                                        <b>[14]</b>
                                        CAI S, ZHANG L, ZUO W, et al.A probabilistic collaborative representation based approach for pattern classification[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:2950-2959.
                                    </a>
                                </li>
                                <li id="305">


                                    <a id="bibliography_15" title="PLATT J C.1 2 Fast training of support vector machines using sequential minimal optimization[M]//SOENTPIET R.Advances in Kernel Methods:Support Vector Learning.Cambridge, MA:MIT Press, 1999:185-208." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast training of support vector machines using sequential minimal optimization">
                                        <b>[15]</b>
                                        PLATT J C.1 2 Fast training of support vector machines using sequential minimal optimization[M]//SOENTPIET R.Advances in Kernel Methods:Support Vector Learning.Cambridge, MA:MIT Press, 1999:185-208.
                                    </a>
                                </li>
                                <li id="307">


                                    <a id="bibliography_16" title="MAIRAL J, BACH F, PONCE J.Task-driven dictionary learning[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (4) :791-804." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Task-Driven Dictionary Learning">
                                        <b>[16]</b>
                                        MAIRAL J, BACH F, PONCE J.Task-driven dictionary learning[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (4) :791-804.
                                    </a>
                                </li>
                                <li id="309">


                                    <a id="bibliography_17" title="RAMIREZ I, SPRECHMANN P, SAPIRO G.Classification and clustering via dictionary learning with structured incoherence and shared features[C]//Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:3501-3508." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification and clustering via dictionary learning with structured incoherence and shared features">
                                        <b>[17]</b>
                                        RAMIREZ I, SPRECHMANN P, SAPIRO G.Classification and clustering via dictionary learning with structured incoherence and shared features[C]//Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:3501-3508.
                                    </a>
                                </li>
                                <li id="311">


                                    <a id="bibliography_18" title="GAO S, TSANG W H, MA Y.Learning category-specific dictionary and shared dictionary for fine-grained image categorization[J].IEEE Transactions on Image Processing, 2013, 23 (2) :623-634." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning category-specific dictionary and shared dictionary for fine-grained image categorization">
                                        <b>[18]</b>
                                        GAO S, TSANG W H, MA Y.Learning category-specific dictionary and shared dictionary for fine-grained image categorization[J].IEEE Transactions on Image Processing, 2013, 23 (2) :623-634.
                                    </a>
                                </li>
                                <li id="313">


                                    <a id="bibliography_19" title="ZHOU N.Learning inter-related visual dictionary for object recognition[J]//Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2012:3490-3497." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning inter-related visual dictionary for object recognition">
                                        <b>[19]</b>
                                        ZHOU N.Learning inter-related visual dictionary for object recognition[J]//Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2012:3490-3497.
                                    </a>
                                </li>
                                <li id="315">


                                    <a id="bibliography_20" title="KONG S, WANG D.A dictionary learning approach for classification:separating the particularity and the commonality[C]//ECCV2012:Proceedings of the 12th European Conference on Computer Vision, LNCS 7572.Berlin:Springer, 2012:186-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A dictionary learning approach for classification:separating the particularity and the commonality">
                                        <b>[20]</b>
                                        KONG S, WANG D.A dictionary learning approach for classification:separating the particularity and the commonality[C]//ECCV2012:Proceedings of the 12th European Conference on Computer Vision, LNCS 7572.Berlin:Springer, 2012:186-199.
                                    </a>
                                </li>
                                <li id="317">


                                    <a id="bibliography_21" title="VAPNIK V, CHAPPLE O.Bounds on error expectation for support vector machines[J].Neural Computation, 2000, 12 (9) :2013-2036." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500011870&amp;v=MDg3NDhUTW53WmVadEZpbmxVcjNJS0ZzUWFocz1OaWZKWmJLOUh0ak1xbzlGWk9vT0JIczVvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        VAPNIK V, CHAPPLE O.Bounds on error expectation for support vector machines[J].Neural Computation, 2000, 12 (9) :2013-2036.
                                    </a>
                                </li>
                                <li id="319">


                                    <a id="bibliography_22" title="LIAN X C, LI Z, LU B L, et al.Max-margin dictionary learning for multiclass image categorization[C]//ECCV 2010:Proceedings of the 11th European Conference on Computer Vision, LNCS 6314.Berlin:Springer, 2010:157-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Max-margin dictionary learning for multiclass image categorization">
                                        <b>[22]</b>
                                        LIAN X C, LI Z, LU B L, et al.Max-margin dictionary learning for multiclass image categorization[C]//ECCV 2010:Proceedings of the 11th European Conference on Computer Vision, LNCS 6314.Berlin:Springer, 2010:157-170.
                                    </a>
                                </li>
                                <li id="321">


                                    <a id="bibliography_23" title="SCHOLKOPF B, PLATT J, HOFMANN T.Efficient sparse coding algorithms[EB/OL].[2018-05-10].http://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient sparse coding algorithms">
                                        <b>[23]</b>
                                        SCHOLKOPF B, PLATT J, HOFMANN T.Efficient sparse coding algorithms[EB/OL].[2018-05-10].http://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms.pdf.
                                    </a>
                                </li>
                                <li id="323">


                                    <a id="bibliography_24" title="LEE K C, HO J, KRIEGMAN D J.Acquiring linear subspaces for face recognition under variable lighting[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (5) :684-698." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Acquiring Linear Subspaces for Face Recognition under Variable Lighting">
                                        <b>[24]</b>
                                        LEE K C, HO J, KRIEGMAN D J.Acquiring linear subspaces for face recognition under variable lighting[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (5) :684-698.
                                    </a>
                                </li>
                                <li id="325">


                                    <a id="bibliography_25" title="MARTINEZA M, BENAVENTE R.The AR face database, TR#24[R].Barcelona, Spain:Computer Vision Center, 1998." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The AR face database">
                                        <b>[25]</b>
                                        MARTINEZA M, BENAVENTE R.The AR face database, TR#24[R].Barcelona, Spain:Computer Vision Center, 1998.
                                    </a>
                                </li>
                                <li id="327">


                                    <a id="bibliography_26" title="LI F F, FERGUS R, PERONA P.Learning generative visual models from few training examples:an incremental Bayesian approach tested on 101 object categories[C]//Proceedings of the 2004 Conference on Computer Vision and Pattern Recognition Workshop.Washington, DC:IEEE Computer Society, 2004:178." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning generative visual models from few training examples:an incremental bayesian approach tested on 101 object categories">
                                        <b>[26]</b>
                                        LI F F, FERGUS R, PERONA P.Learning generative visual models from few training examples:an incremental Bayesian approach tested on 101 object categories[C]//Proceedings of the 2004 Conference on Computer Vision and Pattern Recognition Workshop.Washington, DC:IEEE Computer Society, 2004:178.
                                    </a>
                                </li>
                                <li id="329">


                                    <a id="bibliography_27" title="XU Y, ZHANG Z, LU G, et al.Approximately symmetrical face images for image preprocessing in face recognition and sparse representation based classification[J].Pattern Recognition, 2016, 54:68-82." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAB1BB342FF799289569E22EBC17A8155&amp;v=MTAxNjYrYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMMjd3S0U9TmlmT2ZjTEtINk8rckl0SEVwMElCWFU3eHg4VzdEWUlTbjJYM21FMGZzT2NSTA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                        XU Y, ZHANG Z, LU G, et al.Approximately symmetrical face images for image preprocessing in face recognition and sparse representation based classification[J].Pattern Recognition, 2016, 54:68-82.
                                    </a>
                                </li>
                                <li id="331">


                                    <a id="bibliography_28" title="WANG D, KONG S.A classification-oriented dictionary learning model:explicitly learning the particularity and commonality across categories[J].Pattern Recognition, 2014, 47 (2) :885-898." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300161988&amp;v=MTY0MjhlcnFRVE1ud1plWnRGaW5sVXIzSUtGc1FhaHM9TmlmT2ZiSzlIOVBPckk5RlplME9CWFF4b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                        WANG D, KONG S.A classification-oriented dictionary learning model:explicitly learning the particularity and commonality across categories[J].Pattern Recognition, 2014, 47 (2) :885-898.
                                    </a>
                                </li>
                                <li id="333">


                                    <a id="bibliography_29" title="ZHANG Z, XU Y, SHAO L, et al.Discriminative block-diagonal representation learning for image recognition[J].IEEE Transactions on Neural Networks&amp;amp;Learning Systems, 2017, 29 (7) :3111-3125." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative block-diagonal representation learning for image recognition">
                                        <b>[29]</b>
                                        ZHANG Z, XU Y, SHAO L, et al.Discriminative block-diagonal representation learning for image recognition[J].IEEE Transactions on Neural Networks&amp;amp;Learning Systems, 2017, 29 (7) :3111-3125.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-17 16:48</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),940-948 DOI:10.11772/j.issn.1001-9081.2018081785            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>泛化误差界指导的鉴别字典学习</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E6%B6%9B&amp;code=09198274&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐涛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%93%E6%98%8E&amp;code=25311164&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晓明</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%8E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0265106&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西华大学计算机与软件工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%8E%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西华大学机器人研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在提高字典鉴别能力的过程中, 最大间隔字典学习忽视了利用重新获得的数据构建分类器的泛化性能, 不仅与最大间隔原理有关, 还与包含数据的最小包含球 (MEB) 半径有关。针对这一事实, 提出泛化误差界指导的鉴别字典学习算法GEBGDL。首先, 利用支持向量机 (SVM) 的泛化误差上界理论对支持向量引导的字典学习算法 (SVGDL) 的鉴别条件进行改进;然后, 利用SVM大间隔分类原理和MEB半径作为鉴别约束项, 促使不同类编码向量间的间隔最大化, 并减小包含所有编码向量的MEB半径;最后, 为了更充分考虑分类器的泛化性能, 采用交替优化策略分别更新字典、编码系数和分类器, 进而获得编码向量相对间隔更大的分类器, 从而促使字典更好地学习, 提升字典鉴别能力。在USPS手写数字数据集, Extended Yale B、AR、ORL三个人脸集, Caltech101、COIL20、COIL100物体数据集中进行实验, 讨论了超参数和数据维度对识别率的影响。实验结果表明, 在七个图像数据集中, 多数情况下所提算法的识别率优于类标签一致<i>K</i>奇异值分解 (LC-KSVD) 、局部特征和类标嵌入约束字典学习 (LCLE-DL) 算法、Fisher鉴别字典学习 (FDDL) 和SVGDL等算法;且在七个数据集中, 该算法也取得了比基于稀疏表示的分类 (SRC) 、基于协作表示的分类 (CRC) 和SVM更高的识别率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%97%E5%85%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">字典学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%E7%95%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">泛化误差界;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">支持向量机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%B0%8F%E5%8C%85%E5%90%AB%E7%90%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最小包含球;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数字图像分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    徐涛 (1987—) , 男, 四川盐亭人, 硕士研究生, 主要研究方向:模式识别、图像处理;;
                                </span>
                                <span>
                                    *王晓明 (1977—) , 男, 四川简阳人, 副教授, 博士, 主要研究方向:模式识别、机器学习、图像处理、计算机视觉。电子邮箱wxmwm@aliyun.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61532009);</span>
                                <span>教育部春晖计划项目 (Z2015102);</span>
                                <span>四川省教育厅自然科学重点项目 (11ZA004);</span>
                    </p>
            </div>
                    <h1><b>Generalization error bound guided discriminative dictionary learning</b></h1>
                    <h2>
                    <span>XU Tao</span>
                    <span>WANG Xiaoming</span>
            </h2>
                    <h2>
                    <span>School of Computer and Software Engineering, Xihua University</span>
                    <span>Robotics Research Center, Xihua University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the process of improving discriminant ability of dictionary, max-margin dictionary learning methods ignore that the generalization of classifiers constructed by reacquired data is not only in relation to the principle of maximum margin, but also related to the radius of Minimum Enclosing Ball (MEB) containing all the data. Aiming at the fact above, Generalization Error Bound Guided discriminative Dictionary Learning (GEBGDL) algorithm was proposed. Firstly, the discriminant condition of Support Vector Guided Dictionary Learning (SVGDL) algorithm was improved based on the upper bound theory of about the generalization error of Support Vector Machine (SVM) . Then, the SVM large margin classification principle and MEB radius were used as constraint terms to maximize the margin between different classes of coding vectors, and to minimum the MEB radius containing all coding vectors. Finally, as the generalization of classifier being better considered, the dictionary, coding coefficients and classifiers were updated respectively by alternate optimization strategy, obtaining the classifiers with larger margin between the coding vectors, making the dictionary learn better to improve dictionary discriminant ability. The experiments were carried out on a handwritten digital dataset USPS, face datasets Extended Yale B, AR and ORL, object dataset Caltech 101, COIL20 and COIL100 to discuss the influence of hyperparameters and data dimension on recognition rate. The experimental results show that in most cases, the recognition rate of GEBGDL is higher than that of Label Consistent <i>K</i>-means-based Singular Value Decomposition (LC-KSVD) , Locality Constrained and Label Embedding Dictionary Learning (LCLE-DL) , Fisher Discriminative Dictionary Learning (FDDL) and SVGDL algorithm, and is also higher than that of Sparse Representation based Classifier (SRC) , Collaborative Representation based Classifier (CRC) and SVM.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dictionary%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dictionary learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generalization%20error%20bound&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generalization error bound;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Support%20Vector%20Machine%20(SVM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Support Vector Machine (SVM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Minimum%20Enclosing%20Ball%20(MEB)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Minimum Enclosing Ball (MEB) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=digital%20image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">digital image classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XU Tao, born in 1987, M. S. candidate. His research interests include pattern recognition, image processing.;
                                </span>
                                <span>
                                    WANG Xiaoming, born in 1977, Ph. D. , associate professor. His research interests include pattern recognition, machine learning, image processing, computer vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61532009);</span>
                                <span>the Scientific Research Project &quot;Chun hui plan&quot; of Ministry of Education (Z2015102);</span>
                                <span>the Key Scientific Research Foundation of Sichuan Provincial Department of Education (11ZA004);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="64">由于自然图像可以表示为字典原子稀疏的线性组合, 字典在稀疏表示应用中起着非常重要的作用。字典学习 (Dictionary Learning, DL) <citation id="337" type="reference"><link href="277" rel="bibliography" /><link href="279" rel="bibliography" /><link href="281" rel="bibliography" /><link href="283" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>是当前研究的热点之一, 是计算机视觉和图像处理领域的有效工具 (如:超分辨率<citation id="335" type="reference"><link href="277" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、图像降噪<citation id="336" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>和图像分类<citation id="338" type="reference"><link href="281" rel="bibliography" /><link href="283" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>等) , 并取得了较好的效果。在计算机视觉领域, 为特殊的视觉任务构建恰当高效的字典并不容易;在图像分类领域, 通过真实样本图像学习一个理想的字典, 可以恰当并有效地完成图像分类任务。</p>
                </div>
                <div class="p1">
                    <p id="65">面向分类的字典学习方法被称为鉴别性字典学习 (Discriminative DL, DDL) <citation id="345" type="reference"><link href="285" rel="bibliography" /><link href="287" rel="bibliography" /><link href="289" rel="bibliography" /><link href="291" rel="bibliography" /><link href="293" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。目前, 字典学习大致可分为无监督字典学习和监督字典学习方法。无监督字典学习通过最小化目标函数的重构误差获得一个恰当的字典, 直接使得字典具有鉴别性, 并利用重构误差准则分类。监督字典学习方法从监督学习和数据的鉴别信息出发, 同时学习字典和分类器, 并在字典的优化过程中充分解释数据的鉴别信息使得表示系数具有鉴别性, 该方法使用表示系数作为新的特征进行分类。<i>K</i>奇异值分解 (<i>K</i>-means-based Singular Value Decomposition, KSVD) 方法<citation id="339" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>通过对<i>K</i>邻域求解距离获得鉴别字典;Zhang等<citation id="340" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在KSVD的基础上通过同时学习字典和分类器提出鉴别性D-KSVD (Discriminative KSVD) 方法。Jiang等<citation id="341" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>在D-KSVD基础上通过施加鉴别信息对其进行了扩展, 进一步提出了标签一致LC-KSVD (Label Consistent KSVD) 方法;但LC-KSVD方法忽略了原子间的相似特征, 鉴别性能提高有限。为此, Li等<citation id="342" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用局部特征和原子的类标构造嵌入约束的字典学习 (Locality Constrained and Label Embedding DL, LCLE-DL) 算法。Yang等<citation id="343" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的FDDL (Fisher DDL) 方法通过对字典学习模型施加Fisher鉴别准则来提高字典的鉴别性能;该方法提高了鉴别能力, 但是算法在处理大规模的分类问题上时间花费巨大。相对于FDDL算法, Cai等<citation id="344" type="reference"><link href="295" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>通过引入固定参变量提出支持向量引导的字典学习 (Support Vector Guided DL, SVGDL) 方法, 证明了FDDL的固定权值分配策略仅是SVGDL自由分权模式的特例。</p>
                </div>
                <div class="p1">
                    <p id="66">上述大部分鉴别字典学习方法都采用<i>l</i><sub>0</sub>或<i>l</i><sub>1</sub>范数作为稀疏控制条件, 但是<i>l</i><sub>0</sub>或<i>l</i><sub>1</sub>范数计算开销非常巨大。最近, 文献<citation id="349" type="reference">[<a class="sup">11</a>,<a class="sup">12</a>]</citation>的研究工作中讨论了稀疏性在分类任务中对模型识别率的影响。Mehta等<citation id="346" type="reference"><link href="301" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>进一步分析了稀疏系数的工作机制和泛化误差界, 认为稀疏性在DDL模型中是必要的。由于使用<i>l</i><sub>1</sub>范数正则化稀疏系数的求解复杂度远远高于使用<i>l</i><sub>2</sub>范数, 尤其是当训练样本和字典原子巨大时, <i>l</i><sub>1</sub>范数正则化稀疏系数的DDL无效性增加。故此, SVGDL方法为了获得更快的运算速度和良好的分类识别率采用<i>l</i><sub>2</sub>范数作为该模型的稀疏控制条件, 通过真实数据分析了<i>l</i><sub>1</sub>和<i>l</i><sub>2</sub>范数对分类模型识别率的影响, 模型的分类识别率对<i>l</i><sub>2</sub>和<i>l</i><sub>1</sub>范数并不敏感, 并把这种不敏感性归结为只有少量的支持向量自动的引导字典学习。最近, Cai等<citation id="347" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的基于协作表示的分类 (Collaborative Representation based Classifier, CRC) 方法的模式分类模型中采用<i>l</i><sub>2</sub>范数作为稀疏系数控制条件, 并通过理论和分类实验验证, 在模式分类任务中, 相对于基于稀疏表示的分类 (Sparse Representation based Classifier, SRC) <citation id="348" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>方法, CRC能取得良好的分类精度和节约大量算法的训练时间。</p>
                </div>
                <div class="p1">
                    <p id="67">SVGDL采用标准的支持向量机 (Support Vector Machine, SVM) <citation id="350" type="reference"><link href="305" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>作为DDL模型的鉴别项, 仅考虑了最大化间隔原理, 忽视了SVM利用重新获得数据所构建的分类器的泛化性能不仅与大间隔原理有关, 还与包含数据的最小包含球 (Minimum Enclosing Ball, MEB) 的半径有关的基本事实。</p>
                </div>
                <div class="p1">
                    <p id="68">针对这个基本事实, 泛化误差界指导的鉴别字典学习 (Generalization Error Bound Guided Discriminative DL, GEBGDL) 将SVM的泛化误差上界作为基于协作表示 (Collaborative Representation, CR) 的鉴别字典学习方法的鉴别条件, 控制编码系数的更新。由于SVGDL编码向量更新过程中没有考虑到MEB半径的变化限制了模型的泛化能力的提高。为进一步降低模型的泛化误差界, GEBGDL算法在模型的更新中充分考虑MEB半径的变化情况, 同时在训练分类器的过程中减小相似编码向量间的距离, 扩大异类编码向量的分类间隔, 从而获得更加具有鉴别能力的字典, 更好地完成分类任务。为进一步提高鉴别字典的分类性能, 本文提出GEBGDL算法利用SVM的泛化误差上界理论, 扩大相异编码向量间的间隔, 同时减小包含所有编码向量MEB的半径;采用交替优化策略分别更新字典、编码系数和分类器, 充分考虑分类器的泛化性能, 进而获得编码向量相对间隔更大的分类器促使字典更好地学习, 减小了多分类任务中二分类器的泛化误差界, 提升字典的鉴别能力;最后在手写数字识别、人脸识别、物体识别等数据集上验证算法的鉴别性能。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="70">鉴别字典学习的目的是通过样本训练字典提高表示系数的鉴别能力。本章的目标是为数字图像分类建立一个有效的鉴别字典学习方法。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">1.1 <b>支持向量引导的字典学习模型</b></h4>
                <div class="p1">
                    <p id="72">鉴别字典的分类问题是利用表示系数的鉴别性进行分类。假设<b><i>X</i></b>=[<b><i>X</i></b><sub>1</sub>, <b><i>X</i></b><sub>2</sub>, …, <b><i>X</i></b><sub><i>C</i></sub>]∈<b>R</b><sup><i>m</i>×<i>n</i></sup>是具有<i>C</i>个类别, 且包含<i>n</i>个训练样本的训练集。<b><i>X</i></b><sub><i>c</i></sub>∈<b>R</b><sup><i>m</i>×<i>n</i><sub><i>c</i></sub></sup> 是第<i>c</i>类别包含<i>n</i><sub><i>c</i></sub>个训练样本的子集<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>n</mi></mstyle><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></math></mathml>。一个通用的鉴别字典学习模型可表述为:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup><mo>+</mo><mi>τ</mi><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中:‖<b><i>X</i></b>-<b><i>DS</i></b>‖<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow></math></mathml>是重构项;<b><i>D</i></b>∈<b>R</b><sup><i>m</i>×<i>K</i></sup>是学习字典, <i>K</i>是字典原子的个数;<b><i>S</i></b>=[<b><i>s</i></b><sub>1</sub>, <b><i>s</i></b><sub>2</sub>, …, <b><i>s</i></b><sub><i>n</i></sub>]∈<b>R</b><sup><i>K</i>×<i>n</i></sup>是训练样本<b><i>X</i></b>作用于字典<b><i>D</i></b>的表示系数矩阵;<b><i>Y</i></b>是类标签组成的向量或矩阵;如果<b><i>x</i></b><sub><i>i</i></sub>属于<i>c</i> (<i>c</i>=1, 2, …, <i>C</i>) 类, 则<i>y</i><sub><i>i</i></sub>=<i>c</i> (<i>i</i>=1, 2, …, <i>n</i>) ;‖·‖<sup><i>p</i></sup><sub><i>p</i></sub>是作用于<b><i>S</i></b>的<i>l</i><sub><i>P</i></sub>范数 (例如:<i>l</i><sub>1</sub>或<i>l</i><sub>2</sub>) ;<i>λ</i>≥0和<i>τ</i>≥0是权衡参数;<i>L</i> (<b><i>D</i></b>, <b><i>S</i></b>, <b><i>Y</i></b>) 是鉴别项, 用以确保字典<b><i>D</i></b>和编码系数<b><i>S</i></b>的鉴别能力。</p>
                </div>
                <div class="p1">
                    <p id="77">由于字典<b><i>D</i></b>的结构特性, 当前的鉴别字典学习方法可以分为三种类型:1) 共享字典学习 (KSVD、D-KSVD、LC-KSVD、SVGDL和任务驱动字典学习 (Task-DDL, TDDL) <citation id="351" type="reference"><link href="307" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>;2) 类属性字典学习 (FDDL<citation id="352" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、结构化DDL<citation id="353" type="reference"><link href="309" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和类属性DDL<citation id="354" type="reference"><link href="311" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>) ;3) 混合字典学习 (Fisher-hybrid DDL<citation id="355" type="reference"><link href="313" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和区分共性和特性鉴别字典学习 (Separating the Particularity and the Commonality DDL, CPDDL) <citation id="356" type="reference"><link href="315" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>) 。</p>
                </div>
                <div class="p1">
                    <p id="78">支持向量引导的字典学习方法 (SVGDL) 是一种改进的鉴别字典学习算法。假定字典学习鉴别项中所有权值都是通过引入变量获得, 参数化定义为一个函数, 并通过理论证明鉴别项是SVM分类器。同时采用了SVM的最大化间隔思想学习得到鉴别字典。SVGDL算法模型可表述为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>〈</mo><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi><mo>〉</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>τ</mi><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">Y</mi><mo>, </mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中:‖·‖<mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow></math></mathml>的<i>p</i>=2;<b><i>W</i></b>是标准SVM分类法矢量组成的矩阵;<b><i>b</i></b>是与法矢量相互对应的偏置数值组成的向量;<b><i>Y</i></b>=[<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>n</i></sub>]是类别标签向量;<i>L</i> (<b><i>S</i></b>, <b><i>Y</i></b>, <b><i>W</i></b>, <b><i>b</i></b>) 是一个标准的SVM<citation id="357" type="reference"><link href="305" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>被定义为:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">Y</mi><mo>, </mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>θ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>l</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中:<i>l</i> (<b><i>s</i></b><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>, <b><i>w</i></b>, <b><i>b</i></b>) 是用以训练分类器的Hinge损失函数;<i>θ</i>是一个固定常数;〈<b><i>w</i></b>, <b><i>b</i></b>〉可用表示系数 (支持向量) 的线性组合表示。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">1.2 <b>支持向量机及其泛化误差界</b></h4>
                <div class="p1">
                    <p id="85"><i>SVM</i>是典型的大间隔分类器, 其基本的分类问题是二分类问题, 进而采用不同策略扩展为多分类问题。</p>
                </div>
                <div class="p1">
                    <p id="86">目前, <i>SVM</i>模式选择的研究主要集中于交叉验证技术、优化核函数评估标准和最小化期望风险上界三个方面。<i>Vapink</i>等<citation id="358" type="reference"><link href="317" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出的留一法误差 (<i>Leave</i>-<i>One</i>-<i>Out Error</i>, <i>LOO</i><sub>Error</sub>) 风险上界估计方法是分类器真实错误率的无偏估计, 寻优效果准确。<i>LOO</i><sub>Error</sub>数学化描述如下:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>Ο</mi><mi>Ο</mi><msub><mrow></mrow><mrow><mtext>E</mtext><mtext>r</mtext><mtext>r</mtext><mtext>o</mtext><mtext>r</mtext></mrow></msub><mo>≤</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">其中:<i>R</i><sup>2</sup>是最小包含球的半径<i>R</i>的平方;‖<b><i>w</i></b>‖<sup>2</sup>是分类间隔平方的倒数1/<i>γ</i><sup>2</sup>, 即‖<b><i>w</i></b>‖<sup>2</sup>=1/<i>γ</i><sup>2</sup>, 可以通过式 (3) 求得;<i>n</i>是训练样本的总数。等式右边即是泛化误差界上界的估计, 与之对应的SVM在全文中称之为R-SVM (Radius-margin SVM) 。</p>
                </div>
                <div class="p1">
                    <p id="89"><i>R</i><sup>2</sup>可通过求解如下优化问题得到:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>R</mi><mo>∈</mo><mi mathvariant="bold">R</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>o</mi></msub><mo>∈</mo><mi mathvariant="bold">R</mi><msup><mrow></mrow><mi>k</mi></msup></mrow></munder><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>o</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>≤</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>;</mo><mspace width="0.25em" /><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">其中:<b><i>s</i></b><sub><i>o</i></sub>∈<b>R</b><sup><i>k</i></sup>是最小包含球的球心向量。</p>
                </div>
                <h3 id="92" name="92" class="anchor-tag">2 泛化误差界指导的字典学习模型</h3>
                <div class="p1">
                    <p id="93">本章中首先建立二分类目标模型, 并扩展为多类字典学习模型;其次给出模型的优化求解过程;最后给出模型的分类方法。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94">2.1 <b>模型</b></h4>
                <div class="p1">
                    <p id="95">支持向量引导的字典学习模型采用标准的<i>SVM</i><citation id="359" type="reference"><link href="305" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>作为新获得表示系数的分类器。监督学习方面, <i>SVM</i>作为大间隔分类器, 在理论和实践中替代线性分类器取得了良好的分类效果。字典学习方面, <i>SVM</i>与<i>DDL</i>模型相结合的方法<citation id="360" type="reference"><link href="295" rel="bibliography" /><link href="319" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">22</a>]</sup></citation>已获得了良好的结果。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">2.1.1 二分类模型</h4>
                <div class="p1">
                    <p id="97">首先, 使用基于半径/间隔界的<i>R</i>-<i>SVM</i>替换支持向量引导的字典学习模型 (2) 的鉴别条件。其数学化描述如下:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>〈</mo><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mover accent="true"><mi>B</mi><mo>^</mo></mover><mo>〉</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mover accent="true"><mi>B</mi><mo>^</mo></mover><mo>, </mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mn>2</mn><mi>τ</mi><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mover accent="true"><mi>B</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>≥</mo><mn>1</mn><mo>-</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>o</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>≤</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>;</mo><mspace width="0.25em" /><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">其中:模型中采用<i>L</i><sub>2</sub>-SVM<citation id="361" type="reference"><link href="317" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>作为分类器, 并给出模式 (6) 的等价形式如下:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>〈</mo><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mover accent="true"><mi>B</mi><mo>^</mo></mover><mo>〉</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mover accent="true"><mi>B</mi><mo>^</mo></mover><mo>, </mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>τ</mi><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mrow><mo> (</mo><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>θ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>m</mtext></mstyle><mtext>a</mtext><mtext>x</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mover accent="true"><mi>B</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>o</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>≤</mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>;</mo><mspace width="0.25em" /><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">其中:<b><i>D</i></b>是字典矩阵;<b><i>S</i></b>是编码系数矩阵;<b><i>w</i></b>和<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>B</mi><mo>^</mo></mover></math></mathml>分别是SVM的分类超平面的发矢量和偏置;<i>R</i>是最小包含球包含球半径;<b><i>s</i></b><sub><i>o</i></sub>是最小包含球的球心; <i>y</i><sub><i>i</i></sub>是样本的类标签, <i>y</i><sub><i>i</i></sub>=1表示<b><i>s</i></b><sub><i>i</i></sub>属于正样本, <i>y</i><sub><i>i</i></sub>=-1表示<b><i>s</i></b><sub><i>i</i></sub>属于负样本;<i>θ</i>是SVM和MEB惩罚系数, 控制错分样本的惩罚程度;<i>λ</i>和<i>τ</i>是平衡参数, 旨在控制鉴别条件和<i>l</i><sub>2</sub>范数间的关系。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">2.1.2 多分类模型</h4>
                <div class="p1">
                    <p id="104">针对多类别分类问题, 借鉴文献<citation id="362" type="reference">[<a class="sup">10</a>]</citation>的方式采用一对多策略分别更新<i>SVM</i>和<i>MEB</i>的法矢量<b><i>W</i></b>矩阵和半径<i>R</i>。假设训练数据假设<b><i>X</i></b>∈<b>R</b><sup><i>m</i>×<i>n</i></sup>有<i>C</i>种类别, 与训练数据对应的类别标签<b><i>Y</i></b><sup><i>c</i></sup>=[<i>y</i><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>c</mi></msubsup></mrow></math></mathml>, <i>y</i><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>c</mi></msubsup></mrow></math></mathml>, …, <i>y</i><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>c</mi></msubsup></mrow></math></mathml>]。<b><i>W</i></b>=[<b><i>w</i></b><sub>1</sub>, <b><i>w</i></b><sub>2</sub>, …, <b><i>w</i></b><sub><i>C</i></sub>]是<i>C</i>个分类超面发矢量构成的矩阵, <b><i>b</i></b>=[<i>b</i><sub>1</sub>, <i>b</i><sub>2</sub>, …, <i>b</i><sub><i>C</i></sub>]是由<i>C</i>个偏置的值构成的向量, 其中<b><i>w</i></b><sub><i>c</i></sub>和<i>b</i><sub><i>c</i></sub>是SVM关于第<i>c</i>类的分类超平面发矢量和偏置。由于一对多的策略, 模型获得与发矢量矩阵<b><i>W</i></b>相对应的最小包含球半径组成的<b><i>R</i></b><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>C</mi><mn>2</mn></msubsup></mrow></math></mathml>=[<i>R</i><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup></mrow></math></mathml>, <i>R</i><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>, …, <i>R</i><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>C</mi><mn>2</mn></msubsup></mrow></math></mathml>]向量, 其包含<i>C</i>个半径平方数据的向量。由此扩展为多分类模型如下:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>〈</mo><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi><mo>〉</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi><mo>, </mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mn>2</mn><mi>τ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>R</mi></mstyle><msubsup><mrow></mrow><mi>c</mi><mn>2</mn></msubsup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>θ</mi><mi>τ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>R</mi></mstyle><msubsup><mrow></mrow><mi>c</mi><mn>2</mn></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>max</mi></mrow></mstyle><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">其中:一对多策略的条件下, 最小包含球总是包含所有的训练数据<b><i>s</i></b><sub><i>i</i></sub>, 不难发现<b><i>R</i></b><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>C</mi><mn>2</mn></msubsup></mrow></math></mathml>=[<i>R</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup></mrow></math></mathml>, <i>R</i><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>, …, <i>R</i><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>C</mi><mn>2</mn></msubsup></mrow></math></mathml>]中的所有半径相等, 即:<i>R</i><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup></mrow></math></mathml>=<i>R</i><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>=…=<i>R</i><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>C</mi><mn>2</mn></msubsup></mrow></math></mathml>。因此对于多类别模型中, 为了减小求解二次规划问题的计算开销。使用<i>R</i><sup>2</sup>代替<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>R</mi></mstyle><msubsup><mrow></mrow><mi>c</mi><mn>2</mn></msubsup></mrow></math></mathml>。最终建立本文的多类别模型如下:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>〈</mo><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi><mo>〉</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi mathvariant="bold-italic">b</mi><mo>, </mo><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mstyle><mrow><mtext>重</mtext><mtext>构</mtext><mtext>项</mtext><mtext>和</mtext><mtext>稀</mtext><mtext>疏</mtext><mtext>控</mtext><mtext>制</mtext><mtext>项</mtext></mrow></munder><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>τ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mstyle><mrow><mtext>泛</mtext><mtext>化</mtext><mtext>误</mtext><mtext>差</mtext><mtext>界</mtext></mrow></munder><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>θ</mi><mi>τ</mi><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>m</mtext></mstyle></mrow></mstyle><mtext>a</mtext><mtext>x</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mstyle><mrow><mtext>分</mtext><mtext>类</mtext><mtext>条</mtext><mtext>件</mtext></mrow></munder><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">其中:当<i>y</i><sub><i>i</i></sub>=<i>c</i>时, <i>y</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup></mrow></math></mathml>=1;否则<i>y</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup></mrow></math></mathml>=-1。</p>
                </div>
                <div class="p1">
                    <p id="126">标准SVM将间隔平方<i>γ</i><sup>2</sup>的倒数‖<b><i>w</i></b>‖<sup>2</sup>作为正则项。然而, SVM的泛化误差界依赖于<i>R</i><sup>2</sup>/<i>γ</i><sup>2</sup> (即:<i>R</i><sup>2</sup>‖<b><i>w</i></b>‖<sup>2</sup>) , 这也是GEBGDL采用泛化误差上界改进SVGDL算法的动机。当训练样本固定, MEB的半径<i>R</i>是一个固定常数。然而, 在DDL模型的更新过程中, 由模型产生的表示系数<b><i>S</i></b>在迭代的过程中不断变化, 即包含所有表示系数<b><i>s</i></b><sub><i>i</i></sub>的MEB的半径<i>R</i>也随之改变。SVGDL算法简单地为表示系数空间乘以一个固定的常数可以增加正负样本间的间隔, 但这并不是最真实分类效果。因此, 为了获得真实的分类效果, SVM泛化误差上界理论对SVGDL模型分类器的改进提供了理论依据。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">2.2 <b>求解模型</b></h4>
                <div class="p1">
                    <p id="128">由于多类字典模型 (9) 对于所有的变量并非联合的凸优化问题, 因此, 借鉴文献<citation id="363" type="reference">[<a class="sup">10</a>]</citation>所采用的交替优化策略对多类字典模型涉及到的所有变量进行交替优化。详细过程可以分为三个部分:固定<b><i>S</i></b>、〈<b><i>W</i></b>, <b><i>b</i></b>〉和<i>R</i><sup>2</sup>, 更新字典<b><i>D</i></b>;固定<b><i>D</i></b>、〈<b><i>W</i></b>, <b><i>b</i></b>〉和<i>R</i><sup>2</sup>, 更新表示系数<b><i>S</i></b>;固定<b><i>D</i></b>和<b><i>S</i></b>, 更新〈<b><i>W</i></b>, <b><i>b</i></b>〉和<i>R</i><sup>2</sup>可以转变成求解SVM和MEB最小包含球的两个子过程。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">2.2.1 更新表示系数<b><i>S</i></b></h4>
                <div class="p1">
                    <p id="130">借鉴文献<citation id="364" type="reference">[<a class="sup">10</a>]</citation>的表示系数更新方法, 固定<b><i>D</i></b>、〈<b><i>W</i></b>, <b><i>b</i></b>〉和<i>R</i><sup>2</sup>, 逐列对表示系数<b><i>S</i></b>更新。模型转变成如下方式进行描述:</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">S</mi></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mn>2</mn><mi>τ</mi><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>L</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">其中:</p>
                </div>
                <div class="p1">
                    <p id="133" class="code-formula">
                        <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>θ</mi><mi>l</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>l</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>m</mtext></mstyle><mtext>a</mtext><mtext>x</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="134">但是, 优化求解二次Hinge损失函数<i>l</i> (<b><i>w</i></b><sub><i>c</i></sub>, <i>b</i><sub><i>c</i></sub>, <i>y</i><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup></mrow></math></mathml>, <b><i>s</i></b><sub><i>i</i></sub>) 非常困难, 在整体的优化迭代过程中, 本文采用近似求解方法将<i>l</i> (<b><i>w</i></b><sub><i>c</i></sub>, <i>b</i><sub><i>c</i></sub>, <i>y</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup></mrow></math></mathml>, <b><i>s</i></b><sub><i>i</i></sub>) 转变成如下的方式求解:</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo stretchy="false">∥</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo>&gt;</mo><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">当<i>y</i><sub><i>i</i></sub> (<b><i>w</i></b><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><b><i>s</i></b><sub><i>i</i></sub>+<i>b</i><sub><i>c</i></sub>) &gt;1时, 式 (12) 转化成如下形式:</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">S</mi></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>τ</mi><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mi>θ</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>φ</mi></mrow></munder><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">∥</mo></mstyle></mrow></mstyle><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141">对于式 (13) 的第三项化简:</p>
                </div>
                <div class="p1">
                    <p id="142" class="code-formula">
                        <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">∥</mo></mstyle><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mi>c</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mi>n</mi><mo>+</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="bold-italic">s</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">s</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">) </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="bold-italic">s</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">) </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>-</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mn>2</mn></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">s</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">) </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mi>c</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mi>n</mi><mo>+</mo><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">S</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">) </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">E</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mo>-</mo><mn>2</mn><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="143">其中:<b><i>E</i></b>= (1, 1, …, 1) <sup>T</sup>;<b><i>Y</i></b>= (<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>n</i></sub>) <sup>T</sup>。</p>
                </div>
                <div class="p1">
                    <p id="144">去除与<b><i>S</i></b>无关项后, 简化目标函数为:</p>
                </div>
                <div class="area_img" id="274">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904003_27400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="274">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904003_27401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="147">该目标函数的梯度为:</p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>∇</mo><mi>J</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mn>2</mn><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo>+</mo><mn>2</mn><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo>+</mo><mn>2</mn><mi>λ</mi><mi mathvariant="bold-italic">S</mi><mo>+</mo></mtd></mtr><mtr><mtd><mn>2</mn><mi>τ</mi><mi>θ</mi><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>φ</mi></mrow></munder><mn>2</mn></mstyle><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">S</mi><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mi mathvariant="bold-italic">E</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>-</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>c</mi></msub><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149">该目标函数的Hessian矩阵为:</p>
                </div>
                <div class="p1">
                    <p id="150" class="code-formula">
                        <mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><msup><mrow></mrow><mn>2</mn></msup><mi>J</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mo>=</mo><mn>2</mn><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">D</mi><mo>+</mo><mn>2</mn><mo stretchy="false"> (</mo><mi>λ</mi><mi mathvariant="bold-italic">Ι</mi><mo>+</mo><mi>τ</mi><mi>θ</mi><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>φ</mi></mrow></munder><mi mathvariant="bold-italic">w</mi></mstyle><msub><mrow></mrow><mi>c</mi></msub><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">) </mo><mo>⨂</mo><mi mathvariant="bold-italic">Ι</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="151">其中:<i>φ</i>={<i>c</i>|1≤<i>c</i>≤<i>C</i>, <i>y</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup></mrow></math></mathml> (<b><i>w</i></b><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><b><i>s</i></b><sub><i>i</i></sub>+<i>b</i><sub><i>c</i></sub>) -1&gt;0}, 可见该目标函数是Hessian矩阵半正定。故此, 该目标函数<i>J</i> (<b><i>S</i></b>) 为凸函数, 通常采用梯度下降法或者拟牛顿法便可求得最优解。</p>
                </div>
                <div class="p1">
                    <p id="154">当<i>y</i><sub><i>i</i></sub> (<b><i>w</i></b><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><b><i>s</i></b><sub><i>i</i></sub>+<i>b</i><sub><i>c</i></sub>) ≤1时, 式 (11) 转换为如下形式:</p>
                </div>
                <div class="p1">
                    <p id="156" class="code-formula">
                        <mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">S</mi></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="157">式 (17) 仅<b><i>S</i></b>是待求解的变量, 其解是封闭的解析解。该目标函数的梯度为:</p>
                </div>
                <div class="p1">
                    <p id="158">ᐁ<i>J</i><sub>1</sub> (<b><i>S</i></b>) =-2<b><i>D</i></b><sup>T</sup><b><i>X</i></b>+2<b><i>D</i></b><sup>T</sup><b><i>DS</i></b>+2<i>λ</i><b><i>S</i></b>      (18) </p>
                </div>
                <div class="p1">
                    <p id="159">令梯度ᐁ<i>J</i><sub>1</sub> (<b><i>S</i></b>) =0便可求得式 (17) 的解:</p>
                </div>
                <div class="p1">
                    <p id="160"><b><i>S</i></b>= (<b><i>D</i></b><sup>T</sup><b><i>D</i></b>+<i>λ</i><b><i>I</i></b>) <sup>-1</sup><b><i>D</i></b><sup>T</sup><b><i>X</i></b>      (19) </p>
                </div>
                <h4 class="anchor-tag" id="161" name="161">2.2.2 更新字典<b><i>D</i></b></h4>
                <div class="p1">
                    <p id="162">当表示系数<b><i>S</i></b>、〈<b><i>W</i></b>, <b><i>b</i></b>〉和<i>R</i><sup>2</sup>固定后, 更新字典<b><i>D</i></b>。模型关于<b><i>D</i></b>的更新问题转变成式 (21) 的形式:</p>
                </div>
                <div class="area_img" id="275">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904003_27500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="165">该目标函数可通过文献<citation id="365" type="reference">[<a class="sup">23</a>]</citation>的拉格朗日对偶问题求得。首先, 给出<i>J</i><sub>3</sub> (<b><i>D</i></b>) 的Lagrange函数:</p>
                </div>
                <div class="p1">
                    <p id="166" class="code-formula">
                        <mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><msub><mrow></mrow><mrow><mn>3</mn><mi>L</mi></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">β</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>Τ</mtext><mtext>r</mtext><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>d</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mo>.</mo><mi>j</mi></mrow><mn>2</mn></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo>;</mo><mo>∀</mo><mi>j</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Κ</mi><mo stretchy="false">}</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="167">其中 <i>β</i>是Lagrange乘子, 可求得其对偶问题:</p>
                </div>
                <div class="area_img" id="276">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904003_27600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="171"><b>其中</b>:<i>Λ</i>=diag (<i>β</i>) 。<b>通过求其对偶问题, 并将所得到最优解</b><i>β</i><b>代入</b><b><i>D</i></b><sup>T</sup>= (<b><i>SS</i></b><sup>T</sup>+<i>Λ</i>) <sup>-1</sup> (<b><i>DS</i></b><sup>T</sup>) <sup>T</sup>, <b>就可得到字典</b><b><i>D</i></b><b>的最优解</b>。</p>
                </div>
                <h4 class="anchor-tag" id="172" name="172">2.2.3 更新〈<b><i>W</i></b>, <b><i>b</i></b>〉和<i>R</i><sup>2</sup></h4>
                <div class="p1">
                    <p id="173"><b>当</b><b><i>D</i></b><b>和</b><b><i>S</i></b><b>固定后, 更新</b>〈<b><i>W</i></b>, <b><i>b</i></b>〉<b>和</b><i>R</i><sup>2</sup>。<b>模型的更新问题转变成求解多类</b>SVM<b>和</b>MEB<b>最小包含球的两个子问题。通过多次求解二次规划问题获得其解。借鉴文献</b><citation id="366" type="reference">[<a class="sup">21</a>]</citation><b>方法, 分别求得</b>〈<b><i>W</i></b>, <b><i>b</i></b>〉<b>和</b><i>R</i><sup>2</sup>。</p>
                </div>
                <div class="p1">
                    <p id="174"><b>综合以上模型的求解步骤, 概括</b>GEBGDL<b>算法的训练过程如下</b>:</p>
                </div>
                <div class="p1">
                    <p id="175">输入 训练样本<b><i>X</i></b>∈<b>R</b><sup><i>m</i>×<i>n</i></sup>, 初始化权衡参数<i>λ</i>和<i>τ</i>, 惩罚系数<i>θ</i>, 训练数据的类标签<b><i>Y</i></b>∈<b>R</b><sup><i>n</i></sup>, 最大迭代次数<i>T</i>和收敛阈值。</p>
                </div>
                <div class="p1">
                    <p id="176">输出 字典<b><i>D</i></b>和多类R-SVM的〈<b><i>W</i></b>, <b><i>b</i></b>〉。</p>
                </div>
                <div class="p1">
                    <p id="177">1) 初始化字典<b><i>D</i></b>, 表示系数<b><i>S</i></b>, 采用交替优化策略更新。</p>
                </div>
                <div class="p1">
                    <p id="178">2) 固定<b><i>D</i></b>, {<b><i>W</i></b>, <b><i>b</i></b>}和<i>R</i><sup>2</sup>使用式 (10) 更新表示系数<b><i>S</i></b>:</p>
                </div>
                <div class="p1">
                    <p id="179">a) 当迭代次数<i>t</i>=1和<i>y</i><sub><i>i</i></sub> (<b><i>w</i></b><mathml id="180"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><b><i>s</i></b><sub><i>i</i></sub>+<i>b</i><sub><i>c</i></sub>) ≤1时, 使用式 (19) 更新<b><i>S</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="181">b) 当迭代次数<i>t</i>≥2和<i>y</i><sub><i>i</i></sub> (<b><i>w</i></b><mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><b><i>s</i></b><sub><i>i</i></sub>+<i>b</i><sub><i>c</i></sub>) &gt;1时, 使用式 (14) 更新<b><i>S</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="183">3) 固定<b><i>S</i></b>, {<b><i>W</i></b>, <b><i>b</i></b>}和<i>R</i><sup>2</sup>, 使用式 (20) 更新字典<b><i>D</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="184">4) 固定<b><i>D</i></b>和<b><i>S</i></b>, 使用式 (3) 和 (4) 分别更新{<b><i>W</i></b>, <b><i>b</i></b>}和<i>R</i><sup>2</sup>。</p>
                </div>
                <div class="p1">
                    <p id="185">5) 若相邻两次迭代后目标函数值之差小于收敛阈值, 则迭代终止;否则返回2) 继续执行, 直到收敛或达到最大迭代次数<i>T</i>终止。</p>
                </div>
                <h4 class="anchor-tag" id="186" name="186">2.3 <b>分类方法</b></h4>
                <div class="p1">
                    <p id="187">模型训练完成得到字典<b><i>D</i></b>和R-SVM的最优分类超平面以及偏置{<b><i>W</i></b>, <b><i>b</i></b>}。当输入一个新的测试样本<b><i>x</i></b>, 首先将<b><i>x</i></b>通过固定的矩阵<b><i>M</i></b>进行投影编码, 映射成编码向量矩阵的列向量<b><i>s</i></b>=<b><i>Mx</i></b>, 矩阵<b><i>M</i></b>= (<b><i>D</i></b><sup>T</sup><b><i>D</i></b>+<i>λ</i><b><i>I</i></b>) <sup>-1</sup><b><i>D</i></b><sup>T</sup>。然后, 利用<i>C</i>个线性分类器{<b><i>w</i></b><sub><i>c</i></sub>, <i>b</i><sub><i>c</i></sub>}作用于编码向量<b><i>s</i></b>, 通过式 (23) 预测样本<b><i>x</i></b>类别标签<i>y</i>。</p>
                </div>
                <div class="p1">
                    <p id="188" class="code-formula">
                        <mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi>c</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>C</mi></mrow></munder><mspace width="0.25em" /><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mo>⋅</mo><mi mathvariant="bold-italic">s</mi><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="189">结合算法的训练过程和测试过程, 图1为GEBGDL算法的流程。</p>
                </div>
                <div class="area_img" id="190">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904003_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 SVGDL和本文算法的三个不同可变参数 λ、τ和θ在Extended Yale B上的识别率" src="Detail/GetImg?filename=images/JSJY201904003_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 SVGDL和本文算法的三个不同可变参数 <i>λ</i>、<i>τ</i>和<i>θ</i>在Extended Yale B上的识别率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904003_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Recognition rates of SVGDL and the proposed method with three different variable parameters of <i>λ</i>, <i>τ</i> and <i>θ</i> on Extended Yale B</p>

                </div>
                <div class="p1">
                    <p id="191">GEBGDL算法的训练过程采用交替优化策略分别更新字典<b><i>D</i></b>、编码系数<b><i>S</i></b>, 支持向量机的分类法矢量<b><i>W</i></b>和偏置<b><i>b</i></b>以及最小包含球的半径的平方<i>R</i><sup>2</sup>。测试过程通过训练所得到的字典<b><i>D</i></b>获得映射矩阵<b><i>M</i></b>, 将测试样本<b><i>x</i></b>映射成测试样本的编码系数<b><i>s</i></b>, 最后通过支持向量机的分类法矢量和偏置预测测试样本的类别, 获得分类结果。</p>
                </div>
                <div class="p1">
                    <p id="192">GEBGDL训练过程的时间开销主要由3个部分组成:更新学习字典<b><i>D</i></b>的时间复杂度是<i>O</i> (<i>K</i><sup>3</sup><i>mn</i>) , 更新编码系数的时间复杂度是<i>O</i> (<i>K</i><sup>3</sup><i>mn</i>) , 更新线性SVM和MEB的时间复杂度分别是<i>O</i> (<i>Cmn</i>) 和<i>O</i> (<i>mn</i>) 。由于目标优化函数是非凸函数, 并不能获得全局最小值, 经验上可以通过目标函数值的递减变化获得一个理想的字典和线性分类器。分类测试环节的时间复杂度是<i>O</i> (<i>Km</i>) 。其中:<i>m</i>是样本维度, <i>n</i>是样本个数, <i>K</i>是字典原子个数 (或编码系数矩阵逐列向量的维度) , <i>C</i>是样本所包含的类别数。</p>
                </div>
                <div class="area_img" id="193">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904003_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 GEBGDL算法流程" src="Detail/GetImg?filename=images/JSJY201904003_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 GEBGDL算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904003_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of GEBGDL algorithm</p>

                </div>
                <h3 id="194" name="194" class="anchor-tag">3 实验与结果分析</h3>
                <div class="p1">
                    <p id="195">为了对本文提出的鉴别字典学习方法进行定量评价, 实验在7个标准数据集 (包括<i>USPS</i>、<i>Extended Yale B</i><citation id="367" type="reference"><link href="323" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、<i>AR</i><citation id="368" type="reference"><link href="325" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、<i>ORL</i>和<i>Caltech</i>101<citation id="369" type="reference"><link href="327" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、<i>COIL</i>20和<i>COIL</i>100) 上进行仿真实验。实验平台为处理器<i>Intel i</i>3-4130 @3.40 <i>GHz</i>的64位<i>Windows</i> 7旗舰版, <i>Matlab R</i>2013<i>b</i>。</p>
                </div>
                <h4 class="anchor-tag" id="196" name="196">3.1 <b>模型参数对识别率的影响</b></h4>
                <div class="p1">
                    <p id="197"><i>GEBGDL</i>模型涉及3个主要参数, 分别为平衡参数λ和τ, 以及<i>SVM</i>和<i>MEB</i>的惩罚系数θ。为防止过拟合的产生, 平衡参数均采用交叉验证自动选取。为保证实验的公平性, 实验结果为独立运行20次后的平均识别率。为了讨论多个参数变量的综合影响, 实验中对3个主要参数进行了综合测试, 首先, 在<i>Extended Yale B</i>数据集上固定选取每一类别20个样本作为训练数据, 同时固定训练数据和测试数据。图2是在<i>Extended Yale B</i>数据集控制其中一个变量后, 其余两个可变参数对识别率影响的曲面。</p>
                </div>
                <div class="p1">
                    <p id="198">图2依次给出三个可变参数对<i>SVGDL</i>和本文算法在平均识别率上的影响, 为模型的综合参数选择提供了实验依据。当两种算法参数设置一致时, 通过图2中的颜色标签可看出, 本文<i>GEBGDL</i>算法的识别率优于<i>SVGDL</i>, 这说明本文算法采用<i>SVM</i>的泛化误差上界理论的改进可以提高字典的鉴别能力。图2的实验结论同时为交叉验证方法快速的参数选择提供了合理的参考范围。表1给出了通过交叉验证方法自动选取参数后模型对于7个标准数据集的参数设置。</p>
                </div>
                <div class="area_img" id="199">
                    <p class="img_tit"><b>表</b>1 7<b>个数据集的参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Parameters setting on seven datasets</i></p>
                    <p class="img_note"></p>
                    <table id="199" border="1"><tr><td><br />数据集</td><td>λ</td><td>τ</td><td>θ</td></tr><tr><td><br /><i>USPS</i></td><td>1<i>E</i>-2</td><td>1<i>E</i>-5</td><td>10</td></tr><tr><td><br /><i>Extended Yale B</i></td><td>2<i>E</i>-3</td><td>1<i>E</i>-6</td><td>20</td></tr><tr><td><br /><i>AR</i></td><td>2<i>E</i>-3</td><td>1<i>E</i>-6</td><td>20</td></tr><tr><td><br /><i>ORL</i></td><td>2<i>E</i>-3</td><td>1<i>E</i>-6</td><td>20</td></tr><tr><td><br /><i>Caltech</i>101</td><td>5<i>E</i>-2</td><td>2<i>E</i>-5</td><td>30</td></tr><tr><td><br /><i>COIL</i>20</td><td>1<i>E</i>-2</td><td>1<i>E</i>-5</td><td>20</td></tr><tr><td><br /><i>COIL</i>100</td><td>1<i>E</i>-2</td><td>1<i>E</i>-5</td><td>20</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="200" name="200">3.2 <b>标准数据介绍和实验设置及其结果</b></h4>
                <div class="p1">
                    <p id="201">本文采用7个不同的标准数据集对方法<i>SVM</i><citation id="370" type="reference"><link href="305" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、<i>CRC</i><citation id="371" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、<i>SRC</i><citation id="372" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 近似对称人脸图像人脸识别和稀疏表示分类方法<i>ASF</i>-<i>SRC</i><citation id="373" type="reference"><link href="329" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>和其余几种不同的字典学习方法 (<i>D</i>-<i>KSVD</i><citation id="374" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、<i>LC</i>-<i>KSVD</i><citation id="375" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、<i>LCLE</i>-<i>DL</i><citation id="376" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、<i>DLSPC</i><citation id="377" type="reference"><link href="331" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>、<i>FDDL</i><citation id="378" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、<i>SVGDL</i><citation id="379" type="reference"><link href="295" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和<i>BDLRR</i><citation id="380" type="reference"><link href="333" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>) 进行对比实验, 包含手写数字识别、人脸识别、物体识别。</p>
                </div>
                <div class="p1">
                    <p id="202">数据集和实验设计如下:</p>
                </div>
                <div class="p1">
                    <p id="203">1) <i>USPS</i>手写数字数据集收集了数字0～9的9 298张图像, 每一张是16×16像素。由于本文算法采用二范数的正则项作为稀疏控制条件和使用了<i>SVM</i>的分类理论作为鉴别性条件, 故在<i>USPS</i>手写数据集上将本文算法和<i>CRC</i>、<i>SRC</i>、<i>SVM</i>、<i>SVGDL</i>进行对比。借鉴<i>http</i>://<i>www</i>.<i>cad</i>.<i>zju</i>.<i>edu</i>.<i>cn</i>的<i>USPS</i>数据集的实验设置方式, 选择7 291张图像作为训练数据, 剩余的2 007张作为测试数据, 并固定字典原子。表2是不同算法的平均识别率, 相对于<i>CRC</i>、<i>SRC</i>、<i>SVM</i>, 本文算法的平均识别率提高了2～3个百分点, <i>SVGDL</i>提高了1～2个百分点, 这说明将<i>SVM</i>作为基于协作表示的字典学习的鉴别条件或者基于稀疏表达的鉴别条件, 可以增强字典模型的鉴别能力, 提高分类性能。</p>
                </div>
                <div class="area_img" id="204">
                    <p class="img_tit"><b>表</b>2 <b>不同算法在</b><i>USPS</i><b>数据集上的平均识别率</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Average recognition rate of</i><i>different algorithms on USPS dataset</i> %</p>
                    <p class="img_note"></p>
                    <table id="204" border="1"><tr><td><br />算法</td><td>平均识别率</td><td>算法</td><td>平均识别率</td></tr><tr><td><br /><i>CRC</i></td><td>89.43</td><td><i>SVM</i></td><td>90.14</td></tr><tr><td><br /><i>SRC</i></td><td>89.72</td><td><i>SVGDL</i></td><td>91.77</td></tr><tr><td><br />本文算法</td><td><b>92.45</b></td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="205">2) Extended Yale B人脸数据集收集了包含38个类别的2 414张正脸图像。每一类别分别有192×168像素的64张正脸。本数据集的识别难点在于多变光照条件和丰富的面部表情。借鉴文献<citation id="381" type="reference">[<a class="sup">6</a>]</citation>的实验布局方式, 对Extended Yale B人脸集随机地为每一类别选择5、10、15、20张图像作为训练数据, 其余的作为测试数据, 每张图像裁剪为32×32大小, 为了保证实验的公平性, 借鉴文献<citation id="382" type="reference">[<a class="sup">6</a>]</citation>方法对所有算法的实验数据利用过主成分分析法 (Principal Component Analysis, PCA) 降低特征维度到<i>m</i>=300。从表3可看出, 本文算法的平均识别率比SVGDL算法提高了1.5个百分点, 比其他方法提高2～8个百分点。</p>
                </div>
                <div class="area_img" id="206">
                    <p class="img_tit"><b>表</b>3 <b>不同算法在</b>Extended Yale B<b>数据集上的平均识别率和标准差</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Average recognition rate and standard deviation of different algorithms on Extended Yale B dataset %</p>
                    <p class="img_note"></p>
                    <table id="206" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />训练数据中的图像数</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td></tr><tr><td><br />SVM</td><td>62.9±1.6</td><td>73.3±1.4</td><td>82.1±1.3</td><td>89.5±1.0</td></tr><tr><td><br />SRC</td><td>56.4±1.2</td><td>78.6±2.2</td><td>86.1±1.5</td><td>86.7±0.9</td></tr><tr><td><br />KSVD</td><td>77.3±1.4</td><td>88.3±0.9</td><td>92.9±1.0</td><td>93.1±0.9</td></tr><tr><td><br />D-KSVD</td><td>74.2±2.1</td><td>80.3±1.1</td><td>82.4±1.9</td><td>93.1±0.9</td></tr><tr><td><br />LC-KSVD1</td><td>73.3±1.4</td><td>88.3±0.7</td><td>79.1±2.8</td><td>92.6±1.0</td></tr><tr><td><br />LC-KSVD2</td><td>73.3±1.4</td><td>88.4±0.6</td><td>79.2±2.7</td><td>92.8±1.1</td></tr><tr><td><br />DLSPC</td><td>71.7±1.8</td><td>85.2±1.3</td><td>90.2±1.0</td><td>92.4±0.6</td></tr><tr><td><br />FDDL</td><td>63.4±1.8</td><td>87.6±1.2</td><td>91.7±0.9</td><td>93.1±0.7</td></tr><tr><td><br />SVGDL</td><td>76.7±2.2</td><td>88.8±1.2</td><td>91.4±1.0</td><td>96.2±0.8</td></tr><tr><td><br />BDLRR</td><td>77.1±1.4</td><td><b>90.1±1.0</b></td><td><b>94.7±0.6</b></td><td>96.9±0.6</td></tr><tr><td><br />LCLE-DL</td><td>74.8±1.6</td><td>88.4±1.1</td><td>93.4±0.7</td><td>95.5±0.5</td></tr><tr><td><br />ASF-SRC</td><td>64.3±2.3</td><td>78.4±1.0</td><td>91.4±0.8</td><td>93.5±0.6</td></tr><tr><td><br />本文算法</td><td><b>77.7±1.6</b></td><td>89.8±1.1</td><td>92.9±0.9</td><td><b>97.8±0.7</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="207">3) AR人脸数据集包含126个类别的超过4 000张人脸图像。与Extended Yale B所不同的是, 它具有更加丰富的面部表情 (微笑、生气和尖叫等) 和光照变化。借鉴文献<citation id="383" type="reference">[<a class="sup">6</a>]</citation>的实验布局方式, 对于AR人脸集, 分别选择50位男性和50位女性的共2 600张图像 (每人26张) 作为实验数据;每一类别选取3、5、7张作为训练数据, 剩余的作为测试数据, 并利用PCA降低特征维度到<i>m</i>=300。从表4可看出, 本文算法的平均识别率相对于其他算法提高了1～10个百分点, 比典型的字典学习方法提高了1～3个百分点, 比SVGDL提高了1.4个百分点。</p>
                </div>
                <div class="p1">
                    <p id="208">4) ORL人脸数据集 (http://www.cl.cam.ac.uk) 包含了40个类别的400张人脸图像, 每一个类别10张图像。该数据包含每一个人不同时期、不同视角、多变的面部表情和不同的面部细节。为了公平, 所有实验采用64×64像素的图像作为训练数据, 并随机选择2、4、6张图像作为训练数据, 剩余的作为测试数据。</p>
                </div>
                <div class="area_img" id="209">
                    <p class="img_tit"><b>表</b>4 <b>不同算法在</b>AR<b>数据集上的平均识别率和标准差</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Average recognition rate and standard deviation of different algorithms on AR dataset %</p>
                    <p class="img_note"></p>
                    <table id="209" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="3"><br />训练数据中的图像数</td></tr><tr><td><br />3</td><td>5</td><td>7</td></tr><tr><td><br />SVM</td><td>78.6±1.5</td><td>90.4±0.9</td><td>94.7±0.9</td></tr><tr><td><br />SRC</td><td>69.3±1.4</td><td>81.3±1.5</td><td>88.1±1.3</td></tr><tr><td><br />KSVD</td><td>89.1±1.0</td><td>91.2±1.5</td><td>95.7±0.6</td></tr><tr><td><br />D-KSVD</td><td>84.2±1.4</td><td>86.0±2.4</td><td>87.7±3.0</td></tr><tr><td><br />LC-KSVD1</td><td>89.1±1.0</td><td>94.5±0.9</td><td>95.4±0.9</td></tr><tr><td><br />LC-KSVD2</td><td>90.1±1.0</td><td>94.8±0.9</td><td>95.8±0.8</td></tr><tr><td><br />DLSPC</td><td>70.1±2.1</td><td>86.5±1.2</td><td>92.5±1.2</td></tr><tr><td><br />FDDL</td><td>94.9±0.8</td><td>97.7±0.4</td><td>97.5±0.8</td></tr><tr><td><br />SVGDL</td><td>94.6±0.9</td><td>97.7±0.4</td><td>97.4±0.5</td></tr><tr><td><br />BDLRR</td><td>90.8±0.8</td><td>96.2±0.6</td><td>97.7±0.4</td></tr><tr><td><br />LCLE-DL</td><td>90.3±0.8</td><td>95.7±0.7</td><td>97.2±0.7</td></tr><tr><td><br />ASF-SRC</td><td>72.7±1.4</td><td>82.9±1.3</td><td>88.6±1.2</td></tr><tr><td><br />本文算法</td><td><b>95.0±0.8</b></td><td><b>97.7±0.3</b></td><td><b>98.6±0.3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="210">
                    <p class="img_tit"><b>表</b>7 <b>不同算法在</b>Caltech101<b>数据集上的平均识别率和标准差</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 7 Average recognition rate and standard deviation of different algorithms on Caltech101 dataset %</p>
                    <p class="img_note"></p>
                    <table id="210" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="6"><br />训练数据中的图像数</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td><td>25</td><td>30</td></tr><tr><td><br />SRC</td><td>47.7±1.1</td><td>59.4±0.8</td><td>64.6±0.6</td><td>66.4±0.3</td><td>69.1±0.3</td><td>70.5±0.3</td></tr><tr><td><br />KSVD</td><td>48.2±1.1</td><td>59.1±0.6</td><td>65.0±0.6</td><td>68.5±0.3</td><td>70.7±0.5</td><td>73.3±0.3</td></tr><tr><td><br />D-KSVD</td><td>48.8±1.2</td><td>59.1±0.8</td><td>65.1±0.1</td><td>68.4±0.4</td><td>70.8±0.2</td><td>73.1±0.2</td></tr><tr><td><br />LC-KSVD</td><td>54.0±1.2</td><td>59.1±0.8</td><td>66.7±0.9</td><td>69.7±1.2</td><td>72.1±0.6</td><td>73.4±0.4</td></tr><tr><td><br />FDDL</td><td>53.6±0.6</td><td>63.2±0.5</td><td>65.6±0.7</td><td>68.6±0.2</td><td>71.7±0.1</td><td>73.1±0.1</td></tr><tr><td><br />SVGDL</td><td><b>54.7±0.6</b></td><td><b>64.2±0.1</b></td><td>68.3±0.8</td><td>72.1±0.9</td><td>75.2±0.2</td><td>76.5±0.3</td></tr><tr><td><br />BDLRR</td><td>54.7±1.2</td><td>64.0±0.7</td><td>68.7±0.5</td><td>71.9±0.6</td><td>74.0±0.5</td><td>75.9±0.4</td></tr><tr><td><br />LCLE-DL</td><td>52.9±1.4</td><td>62.2±0.6</td><td>66.4±0.6</td><td>69.6±0.5</td><td>72.1±0.5</td><td>74.0±0.3</td></tr><tr><td><br />本文算法</td><td>53.8±0.8</td><td>63.7±0.7</td><td><b>69.4±0.5</b></td><td><b>73.8±0.7</b></td><td><b>76.7±0.5</b></td><td><b>77.6±0.4</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="211">从表5可看出, 本文算法的平均识别率比SVGDL算法提高了约1个百分点, 比其他算法提高了约3个百分点。</p>
                </div>
                <div class="area_img" id="212">
                    <p class="img_tit"><b>表</b>5 <b>不同算法在</b>ORL<b>数据集上的平均识别率和标准差</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Average recognition rate and standard deviation of different algorithms on ORL dataset %</p>
                    <p class="img_note"></p>
                    <table id="212" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="3"><br />训练数据中的图像数</td></tr><tr><td><br />2</td><td>4</td><td>6</td></tr><tr><td><br />SVM</td><td>78.5±3.2</td><td>89.4±1.6</td><td>94.7±0.9</td></tr><tr><td><br />SRC</td><td>75.4±2.9</td><td>86.6±1.9</td><td>94.8±1.3</td></tr><tr><td><br />KSVD</td><td>79.1±2.8</td><td>91.2±1.5</td><td>95.3±1.8</td></tr><tr><td><br />D-KSVD</td><td>79.0±2.6</td><td>86.0±2.4</td><td>87.7±3.0</td></tr><tr><td><br />LC-KSVD1</td><td>79.1±2.8</td><td>91.2±1.5</td><td>95.3±1.8</td></tr><tr><td><br />LC-KSVD2</td><td>79.1±2.8</td><td>91.5±1.5</td><td>95.3±1.8</td></tr><tr><td><br />DLSPC</td><td>83.2±2.1</td><td>92.1±1.4</td><td>95.7±1.8</td></tr><tr><td><br />FDDL</td><td>84.6±2.6</td><td>94.1±1.2</td><td>97.3±1.2</td></tr><tr><td><br />SVGDL</td><td>84.1±2.2</td><td>94.0±1.1</td><td>97.2±1.2</td></tr><tr><td><br />BDLRR</td><td>81.7±1.9</td><td>92.6±2.3</td><td>96.6±1.1</td></tr><tr><td><br />LCLE-DL</td><td>82.2±2.1</td><td>91.8±1.2</td><td>95.8±1.4</td></tr><tr><td><br />ASF-SRC</td><td>84.2±2.5</td><td>93.0±2.0</td><td>95.8±1.4</td></tr><tr><td><br />本文算法</td><td><b>85.2±2.3</b></td><td><b>94.7±1.0</b></td><td><b>97.3±1.1</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="213">实验中, 为了说明采用PCA对裁剪后的图像降维前后对识别率和算法效率的影响, 随机地为每一类别选择8张图像降维和不降维的实验, 并在表6中给出了降维前后算法效率及识别率、标准差和耗时。从表6可看出:采用PCA降维前后算法的平均识别率没有显著的变化, 但是, 降维后大幅降低了算法的运行时间。</p>
                </div>
                <div class="area_img" id="214">
                                            <p class="img_tit">
                                                <b>表</b>6 ORL<b>数据集上降维前后算法效率及识别率、标准差和耗时</b>
                                                    <br />
                                                Tab. 6 Recognition rate, standard deviation and time consuming before and after dimensionality reduction on ORL database
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904003_21400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201904003_21400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904003_21400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表6 ORL数据集上降维前后算法效率及识别率、标准差和耗时" src="Detail/GetImg?filename=images/JSJY201904003_21400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="215">5) Caltech101物体数据集包含了101个类别物体的29 780张图像, 每一个类别超过80张图像。借鉴文献<citation id="384" type="reference">[<a class="sup">6</a>]</citation>的实验布局和设置方式, 由于物体数据集的固有特性, 公平起见使用与该文献相同特征维度<i>m</i>=3 000的图像作为训练和测试数据, 并为每一类别物体选取5、10、15、20、25和30张不同图像作为训练数据, 剩余部分作为测试数据, 分别固定字典的原子数量<i>K</i>。从表7可看出:本文算法在增加训练数据、固定原子个数<i>K</i>的条件下识别率明显提高, 说明了模型足够稀疏;模型在小样本5和10的情况下, 识别率略低于SVGDL算法, 但是随着训练样本的增加, 本文模型表现出了更好的分类性能。</p>
                </div>
                <div class="p1">
                    <p id="216">6) COIL20数据集包含了20个类别从不同角度拍摄的1 440张图像, 每一类别72张图像;COIL100数据集包含了100个类别从不同角度拍摄的7 200张图像, 每一类别72张图像, 均并裁剪为32×32像素。借鉴http://www.cad.zju.edu.cn的实验数据设置, 每一张采用32×32像素的图像。为了公平每次随机选取每一类别的5、10、15和20张图像作为训练数据, 剩余的作为测试数据。每一张图像的特征维度<i>m</i>=1 024, 并分别为COIL20和COIL100固定字典原子。从表8～9可看出:在多类别不同物体的识别上, 本文算法识别率相对于FDDL和SVM并没有显著提高, 但是相对于SVGDL方法平均识别率提高了1个百分点, 说明了本文采用SVM泛化误差上界理论对SVGDL方法的改进是有效的, 对新构建分类器的泛化性能上考虑是必要的。</p>
                </div>
                <div class="area_img" id="217">
                    <p class="img_tit"><b>表</b>8 <b>不同算法在</b>COIL20<b>数据集上的平均识别率和标准差</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 8 Average recognition rate and standard deviation of different algorithmson COIL20 dataset %</p>
                    <p class="img_note"></p>
                    <table id="217" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />训练数据中的图像数</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td></tr><tr><td><br />CRC</td><td>71.2±1.4</td><td>81.4±1.5</td><td>85.8±1.3</td><td>89.3±1.0</td></tr><tr><td><br />SVM</td><td><b>82.4±2.6</b></td><td>90.2±1.6</td><td>93.9±1.3</td><td>95.1±0.7</td></tr><tr><td><br />KSVD</td><td>79.4±2.4</td><td>87.7±1.4</td><td>89.7±1.2</td><td>94.4±1.1</td></tr><tr><td><br />D-KSVD</td><td>76.9±2.6</td><td>87.8±1.4</td><td>90.5±1.5</td><td>94.5±1.0</td></tr><tr><td><br />LC-KSVD1</td><td>79.4±2.4</td><td>86.9±1.5</td><td>90.4±1.0</td><td>93.9±0.9</td></tr><tr><td><br />LC-KSVD2</td><td>80.1±2.4</td><td>88.0±1.3</td><td>91.4±1.0</td><td>94.8±0.9</td></tr><tr><td><br />FDDL</td><td>81.4±2.7</td><td>90.1±1.4</td><td>93.4±0.9</td><td>95.1±0.8</td></tr><tr><td><br />SVGDL</td><td>80.4±2.7</td><td>90.0±1.5</td><td>92.6±0.9</td><td>94.4±1.0</td></tr><tr><td><br />BDLLRR</td><td>81.3±1.7</td><td>89.9±1.2</td><td>93.5±1.2</td><td>95.4±1.2</td></tr><tr><td><br />LCLE-DL</td><td>80.1±1.8</td><td>88.7±1.8</td><td>92.7±1.1</td><td>94.8±0.8</td></tr><tr><td><br />本文算法</td><td>81.5±2.3</td><td><b>91.3±1.4</b></td><td><b>94.2±1.1</b></td><td><b>95.4±0.9</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="218">
                    <p class="img_tit"><b>表</b>9 <b>不同算法在</b>COIL100<b>数据集上的平均识别率和标准差</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 9 Average recognition rate and standard deviation of different algorithms on COIL100 dataset %</p>
                    <p class="img_note"></p>
                    <table id="218" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />训练数据中的图像数</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td></tr><tr><td><br />CRC</td><td>55.4±0.7</td><td>66.0±0.7</td><td>73.4±0.4</td><td>78.3±0.4</td></tr><tr><td><br />SVM</td><td><b>68.8±0.7</b></td><td>76.2±0.7</td><td>80.8±0.6</td><td>82.0±0.3</td></tr><tr><td><br />KSVD</td><td>57.9±0.5</td><td>73.6±0.8</td><td>76.3±0.4</td><td>69.6±0.4</td></tr><tr><td><br />D-KSVD</td><td>66.5±0.9</td><td>73.3±0.7</td><td>76.0±0.6</td><td>69.6±0.5</td></tr><tr><td><br />LC-KSVD1</td><td>67.7±0.7</td><td>73.6±1.2</td><td>77.4±0.6</td><td>69.0±0.4</td></tr><tr><td><br />LC-KSVD2</td><td>67.7±0.7</td><td>74.7±1.1</td><td>78.3±0.6</td><td>69.5±0.4</td></tr><tr><td><br />FDDL</td><td>67.8±0.8</td><td>76.4±0.9</td><td>81.7±0.6</td><td>82.8±0.4</td></tr><tr><td><br />SVGDL</td><td>76.4±0.9</td><td>76.4±0.9</td><td>81.7±0.6</td><td>82.1±0.6</td></tr><tr><td><br />BDLRR</td><td>68.1±0.7</td><td>77.5±0.6</td><td>82.1±0.5</td><td><b>87.4±0.4</b></td></tr><tr><td><br />LCLE-DL</td><td>66.7±0.8</td><td>77.2±0.8</td><td>82.2±0.6</td><td>86.5±0.4</td></tr><tr><td><br />本文算法</td><td>68.7±0.7</td><td><b>77.7±1.0</b></td><td><b>82.4±0.5</b></td><td>83.4±0.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="219" name="219">3.3 <b>模型收敛性讨论</b></h4>
                <div class="p1">
                    <p id="220">为了说明本文算法的收敛性质, 给出优化目标函数 (9) 的数值的变化曲线。目标函数是一个单调递减的函数, 迭代优化的过程中其值在每一轮迭代中呈现出递减趋势。图3为本文算法在3个数据集的收敛曲线, 从中可明显看出:3个数据集上本文算法在5次迭代后迅速收敛到极小值, 并在20次迭代范围内可以迅速收敛达到目标函数最小最优化目标, 充分说明了本文算法的收敛性。</p>
                </div>
                <div class="area_img" id="221">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904003_221.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法在3个数据集的收敛曲线" src="Detail/GetImg?filename=images/JSJY201904003_221.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法在3个数据集的收敛曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904003_221.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Convergence curves of the proposed algorithm on three datasets</p>

                </div>
                <h3 id="222" name="222" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="223">为了提高字典的鉴别性能, 在SVGDL算法的基础上充分考虑到重新获得数据构建的分类器的泛化性能, 提出了泛化误差界指导的鉴别学习算法。在实际情况下, 支持向量引导字典学习方法, 通过学习后的字典映射到系数空间进行分类。与SVGDL方法所不同的是, GEBGDL不仅考虑到了不同类别间的编码系数空间的大间隔分类, 同时使得同类别间的分布更加紧凑, 促使同类原子之间尽可能地反映类别信息, 增强字典的鉴别能力。实验结果表明本文GEBGDL算法的识别率在大多数情况不仅比直接训练原始的SVM、CRC和SRC算法高, 而且比D-KSVD、LC-KSVD、LCLE-DL、FDDL、BDLRR和SVGDL典型的字典学习算法高, 人脸识别方面也比融合图像预处理的ASF-SRC的识别率高。因此, 在模型的训练中考虑分类项的泛化性能, 促使了字典鉴别能力的提高。由于在求解SVM和MEB的问题上时间花销巨大, 下一步的研究方向是如何提高分类器的训练速度, 减小字典训练过程的时间开销。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="277">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Networks for Image Super-Resolution with Sparse Prior">

                                <b>[1]</b>WANG Z, LIU D, YANG J, et al.Deep networks for image superresolution with sparse prior[C]//ICCV 2015:Proceedings of the2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2016:370-378.
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Removing Rain From a Single Image via Discriminative Sparse Coding">

                                <b>[2]</b>LUO Y, XU Y, JI H.Removing rain from a single image via discriminative sparse coding[C]//ICCV 2015:Proceedings of the2015 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2015:3397-3405.
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust face recognition via sparse representation">

                                <b>[3]</b>WRIGHT J, YANG A Y, GANESH A, et al.Robust face recognition via sparse representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2009, 31 (2) :210-227.
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel locally linear KNN model for visual recognition">

                                <b>[4]</b>LIU Q, LIU C.A novel locally linear KNN model for visual recognition[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1329-1337.
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation">

                                <b>[5]</b>AHARON M, ELAD M, BRUCKSTEIN A.K-SVD:an algorithm for designing overcomplete dictionaries for sparse representation[J].IEEE Transactions on Signal Processing, 2006, 54 (11) :4311-4322.
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative K-SVD for dictionary learningin face recognition">

                                <b>[6]</b>ZHANG Q, LI B.Discriminative K-SVD for dictionary learning in face recognition[C]//Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:2691-2698.
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Label consistent KSVD:Learning a discriminative dictionary for recognition">

                                <b>[7]</b>JIANG Z, LIN Z, DAVIS L S.Label consistent K-SVD:learning a discriminative dictionary for recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2651-2664.
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A locality-constrained and label embedding dictionary learning algorithm for image classification">

                                <b>[8]</b>LI Z, LAI Z, XU Y, et al.A locality-constrained and label embedding dictionary learning algorithm for image classification[J].IEEETransactions on Neural Networks and Learning Systems, 2017, 28 (2) :278-293.
                            </a>
                        </p>
                        <p id="293">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14081200000835&amp;v=MDY2NTBubFVyM0lLRnNRYWhzPU5qN0Jhcks4SHRuTnJZOUZaT3NQQkg4OG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>YANG M, ZANG L, FENG X, et al.Sparse representation based Fisher discrimination dictionary learning for image classification[J].International Journal of Computer Vision, 2014, 109 (3) :209-232.
                            </a>
                        </p>
                        <p id="295">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support Vector Guided Dictionary Learning">

                                <b>[10]</b>CAI S, ZUO W, ZHANG L, et al.Support vector guided dictionary learning[C]//ECCV 2014:Proceedings of the 13th European Conference on Computer Vision, LNCS 8692.Berlin:Springer, 2014:624-639.
                            </a>
                        </p>
                        <p id="297">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Are sparse representations really relevant for image classification?">

                                <b>[11]</b>RIGAMONTI R, BRWON M A, LEPTIT V.Are sparse representations really relevant for image classification?[C]//Proceedings of the CVPR 2011.Washington, DC:IEEE Computer Society, 2011:1545-1552.
                            </a>
                        </p>
                        <p id="299">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse representation or collaborative representation:which helps face recognition">

                                <b>[12]</b>ZHANG L, YANG M, FENG X.Sparse representation or collaborative representation:Which helps face recognition?[C]//Proceedings of the 2011 International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2011:471-478.
                            </a>
                        </p>
                        <p id="301">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparsity-based generalization bounds for predictive sparse coding">

                                <b>[13]</b>MEHTA N A, GRAY A G.Sparsity-based generalization bounds for predictive sparse coding[EB/OL].[2018-05-10].http://www.jmlr.org/proceedings/papers/v28/mehta13.pdf.
                            </a>
                        </p>
                        <p id="303">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Probabilistic Collaborative Representation Based Approach for Pattern Classification">

                                <b>[14]</b>CAI S, ZHANG L, ZUO W, et al.A probabilistic collaborative representation based approach for pattern classification[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:2950-2959.
                            </a>
                        </p>
                        <p id="305">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast training of support vector machines using sequential minimal optimization">

                                <b>[15]</b>PLATT J C.1 2 Fast training of support vector machines using sequential minimal optimization[M]//SOENTPIET R.Advances in Kernel Methods:Support Vector Learning.Cambridge, MA:MIT Press, 1999:185-208.
                            </a>
                        </p>
                        <p id="307">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Task-Driven Dictionary Learning">

                                <b>[16]</b>MAIRAL J, BACH F, PONCE J.Task-driven dictionary learning[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (4) :791-804.
                            </a>
                        </p>
                        <p id="309">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification and clustering via dictionary learning with structured incoherence and shared features">

                                <b>[17]</b>RAMIREZ I, SPRECHMANN P, SAPIRO G.Classification and clustering via dictionary learning with structured incoherence and shared features[C]//Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:3501-3508.
                            </a>
                        </p>
                        <p id="311">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning category-specific dictionary and shared dictionary for fine-grained image categorization">

                                <b>[18]</b>GAO S, TSANG W H, MA Y.Learning category-specific dictionary and shared dictionary for fine-grained image categorization[J].IEEE Transactions on Image Processing, 2013, 23 (2) :623-634.
                            </a>
                        </p>
                        <p id="313">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning inter-related visual dictionary for object recognition">

                                <b>[19]</b>ZHOU N.Learning inter-related visual dictionary for object recognition[J]//Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2012:3490-3497.
                            </a>
                        </p>
                        <p id="315">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A dictionary learning approach for classification:separating the particularity and the commonality">

                                <b>[20]</b>KONG S, WANG D.A dictionary learning approach for classification:separating the particularity and the commonality[C]//ECCV2012:Proceedings of the 12th European Conference on Computer Vision, LNCS 7572.Berlin:Springer, 2012:186-199.
                            </a>
                        </p>
                        <p id="317">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500011870&amp;v=MDE5NjNJS0ZzUWFocz1OaWZKWmJLOUh0ak1xbzlGWk9vT0JIczVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyMw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>VAPNIK V, CHAPPLE O.Bounds on error expectation for support vector machines[J].Neural Computation, 2000, 12 (9) :2013-2036.
                            </a>
                        </p>
                        <p id="319">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Max-margin dictionary learning for multiclass image categorization">

                                <b>[22]</b>LIAN X C, LI Z, LU B L, et al.Max-margin dictionary learning for multiclass image categorization[C]//ECCV 2010:Proceedings of the 11th European Conference on Computer Vision, LNCS 6314.Berlin:Springer, 2010:157-170.
                            </a>
                        </p>
                        <p id="321">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient sparse coding algorithms">

                                <b>[23]</b>SCHOLKOPF B, PLATT J, HOFMANN T.Efficient sparse coding algorithms[EB/OL].[2018-05-10].http://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms.pdf.
                            </a>
                        </p>
                        <p id="323">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Acquiring Linear Subspaces for Face Recognition under Variable Lighting">

                                <b>[24]</b>LEE K C, HO J, KRIEGMAN D J.Acquiring linear subspaces for face recognition under variable lighting[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (5) :684-698.
                            </a>
                        </p>
                        <p id="325">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The AR face database">

                                <b>[25]</b>MARTINEZA M, BENAVENTE R.The AR face database, TR#24[R].Barcelona, Spain:Computer Vision Center, 1998.
                            </a>
                        </p>
                        <p id="327">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning generative visual models from few training examples:an incremental bayesian approach tested on 101 object categories">

                                <b>[26]</b>LI F F, FERGUS R, PERONA P.Learning generative visual models from few training examples:an incremental Bayesian approach tested on 101 object categories[C]//Proceedings of the 2004 Conference on Computer Vision and Pattern Recognition Workshop.Washington, DC:IEEE Computer Society, 2004:178.
                            </a>
                        </p>
                        <p id="329">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAB1BB342FF799289569E22EBC17A8155&amp;v=MjQzNzNMMjd3S0U9TmlmT2ZjTEtINk8rckl0SEVwMElCWFU3eHg4VzdEWUlTbjJYM21FMGZzT2NSTCthQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b>XU Y, ZHANG Z, LU G, et al.Approximately symmetrical face images for image preprocessing in face recognition and sparse representation based classification[J].Pattern Recognition, 2016, 54:68-82.
                            </a>
                        </p>
                        <p id="331">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300161988&amp;v=MDAyNDJkR2VycVFUTW53WmVadEZpbmxVcjNJS0ZzUWFocz1OaWZPZmJLOUg5UE9ySTlGWmUwT0JYUXhvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b>WANG D, KONG S.A classification-oriented dictionary learning model:explicitly learning the particularity and commonality across categories[J].Pattern Recognition, 2014, 47 (2) :885-898.
                            </a>
                        </p>
                        <p id="333">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative block-diagonal representation learning for image recognition">

                                <b>[29]</b>ZHANG Z, XU Y, SHAO L, et al.Discriminative block-diagonal representation learning for image recognition[J].IEEE Transactions on Neural Networks&amp;Learning Systems, 2017, 29 (7) :3111-3125.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904003" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904003&amp;v=MDI3Nzc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWNy9CTHo3QmQ3RzRIOWpNcTQ5Rlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
