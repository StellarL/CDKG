<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637138991624166250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201903007%26RESULT%3d1%26SIGN%3dxmCmhTMhFqtp1FRGSR0z9DZmZjY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903007&amp;v=Mjc3NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3pMTHo3QmQ3RzRIOWpNckk5Rlk0UUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#27" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#33" data-title="1 相关问题 ">1 相关问题</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#35" data-title="1.1 Word2vec&lt;b&gt;模型&lt;/b&gt;">1.1 Word2vec<b>模型</b></a></li>
                                                <li><a href="#38" data-title="1.2 &lt;b&gt;网络表示学习&lt;/b&gt;">1.2 <b>网络表示学习</b></a></li>
                                                <li><a href="#42" data-title="1.3 &lt;b&gt;基于&lt;/b&gt;Word2vec&lt;b&gt;的网络表示学习&lt;/b&gt;">1.3 <b>基于</b>Word2vec<b>的网络表示学习</b></a></li>
                                                <li><a href="#46" data-title="1.4 RW&lt;b&gt;及其有偏性&lt;/b&gt;">1.4 RW<b>及其有偏性</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#53" data-title="2 本文方法 ">2 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="2.1 MHRW">2.1 MHRW</a></li>
                                                <li><a href="#68" data-title="2.2 RLP-MHRW">2.2 RLP-MHRW</a></li>
                                                <li><a href="#77" data-title="2.3 &lt;b&gt;算法实施&lt;/b&gt;">2.3 <b>算法实施</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="3 仿真与性能分析 ">3 仿真与性能分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="3.1 &lt;b&gt;实验数据及数据处理&lt;/b&gt;">3.1 <b>实验数据及数据处理</b></a></li>
                                                <li><a href="#112" data-title="3.2 &lt;b&gt;基准方法&lt;/b&gt;">3.2 <b>基准方法</b></a></li>
                                                <li><a href="#117" data-title="3.3 &lt;b&gt;算法评价&lt;/b&gt;">3.3 <b>算法评价</b></a></li>
                                                <li><a href="#123" data-title="3.4 &lt;b&gt;实验环境及参数设置&lt;/b&gt;">3.4 <b>实验环境及参数设置</b></a></li>
                                                <li><a href="#127" data-title="3.5 &lt;b&gt;实验及结果分析&lt;/b&gt;">3.5 <b>实验及结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#37" data-title="图1 CBOW模型与Skip-gram模型">图1 CBOW模型与Skip-gram模型</a></li>
                                                <li><a href="#44" data-title="图2 基于Word2vec的网络表示学习框架">图2 基于Word2vec的网络表示学习框架</a></li>
                                                <li><a href="#71" data-title="图3 MHRW引入的高自环率">图3 MHRW引入的高自环率</a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;从节点表示到边表示的&lt;/b&gt;4&lt;b&gt;种方法&lt;/b&gt;"><b>表</b>1 <b>从节点表示到边表示的</b>4<b>种方法</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;链路预测的&lt;/b&gt;AUC&lt;b&gt;值&lt;/b&gt;"><b>表</b>2 <b>链路预测的</b>AUC<b>值</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表&lt;/b&gt;3 MHRW&lt;b&gt;和&lt;/b&gt;RLP-MHRW 10&lt;b&gt;次迭代采样时间比较&lt;/b&gt;"><b>表</b>3 MHRW<b>和</b>RLP-MHRW 10<b>次迭代采样时间比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="208">


                                    <a id="bibliography_1" title="HASAN M A, ZAKI M J.A survey of link prediction in social networks[M]//AGGARWAL C C.Social Network Data Analytics.Berlin:Springer, 2011:243-275." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of link prediction in social networks">
                                        <b>[1]</b>
                                        HASAN M A, ZAKI M J.A survey of link prediction in social networks[M]//AGGARWAL C C.Social Network Data Analytics.Berlin:Springer, 2011:243-275.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                    GROVER A, LESKOVEC J.Node2vec:scalable feature learning for networks[C]//KDD&#39;16:Proceedings of the 22nd ACMSIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2016:855-864.</a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_3" title="LU Y L, ZHOU T.Link prediction in complex networks:a survey[J].Physica A:Statistical Mechanics and Its Applications, 2011, 390 (6) :1150-1170." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100530194&amp;v=MTIzODJLN0h0RE9ybzlGWWVnUERYVTlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyakpLRjRjYVJFPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        LU Y L, ZHOU T.Link prediction in complex networks:a survey[J].Physica A:Statistical Mechanics and Its Applications, 2011, 390 (6) :1150-1170.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_4" title="涂存超, 杨成, 刘知远, 等.网络表示学习综述[J].中国科学:信息科学, 2017 (8) :980-996. (TU C C, YANG C, LIU Z Y, et al.Network representation learning:an overview[J].SCIENTIA SINI-CA Informationis, 2017 (8) :980-996.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201708003&amp;v=MjE5NzVGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3ekxOVGZBZHJHNEg5Yk1wNDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        涂存超, 杨成, 刘知远, 等.网络表示学习综述[J].中国科学:信息科学, 2017 (8) :980-996. (TU C C, YANG C, LIU Z Y, et al.Network representation learning:an overview[J].SCIENTIA SINI-CA Informationis, 2017 (8) :980-996.) 
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_5" title="ZHANG D, YIN J, ZHU X.Network representation learning:a survey[J].IEEE Transactions on Big Data, 2017, 99:1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network representation learning:a survey">
                                        <b>[5]</b>
                                        ZHANG D, YIN J, ZHU X.Network representation learning:a survey[J].IEEE Transactions on Big Data, 2017, 99:1.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                    MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[C]//NIPS&#39;13:Proceedings of the 26th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates, 2013, 2:3111-3119.</a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                    PEROZZI B, AL-RFOU R, SKIENA S.Deep Walk:online learning of social representations[C]//Proceedings of the 20th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2014:701-710.</a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                    TANG J, QU M, WANG M, et al.LINE:large-scale information network embedding[C]//WWW&#39;15:Proceedings of the 24th International Conference on World Wide Web.Geneva, Switzerland:International World Wide Web Conferences Steering Committee, 2015:1067-1077.</a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_9" title="CHIB S, GREENBERG E.Understanding the metropolis-hastings algorithm[J].Americian Statistician, 1995, 49 (4) :327-335." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding the metropolis-hastings algorithm">
                                        <b>[9]</b>
                                        CHIB S, GREENBERG E.Understanding the metropolis-hastings algorithm[J].Americian Statistician, 1995, 49 (4) :327-335.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_10" title="GJOKA M, KURANT M, BUTTS C T, et al.Walking in Facebook:a case study of unbiased sampling of OSNs[C]//INFOCOM&#39;10:Proceedings of the 29th Conference on Information Communications.Piscataway, NJ:IEEE, 2010:2498-2506." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Walking in Facebook:A case study of unbiased sampling of OSNs">
                                        <b>[10]</b>
                                        GJOKA M, KURANT M, BUTTS C T, et al.Walking in Facebook:a case study of unbiased sampling of OSNs[C]//INFOCOM&#39;10:Proceedings of the 29th Conference on Information Communications.Piscataway, NJ:IEEE, 2010:2498-2506.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_11" title="王栋, 李振宇, 谢高岗.在线社会网络无偏采样技术[J].计算机研究与发展, 2016, 53 (5) :949-967. (WANG D, LI Z Y, XIE GG.Unbiased sampling technologies on online social network[J].Journal of Computer Research and Development, 2016, 53 (5) :949-967.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201605001&amp;v=MjMwNjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3pMTHl2U2RMRzRIOWZNcW85RlpZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        王栋, 李振宇, 谢高岗.在线社会网络无偏采样技术[J].计算机研究与发展, 2016, 53 (5) :949-967. (WANG D, LI Z Y, XIE GG.Unbiased sampling technologies on online social network[J].Journal of Computer Research and Development, 2016, 53 (5) :949-967.) 
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_12" title="LESKOVEC J, KREVL A.SNAP datasets:Stanford large network dataset collection[DB/OL].[2017-07-01].http://snap.stanford.edu/data." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SNAP datasets">
                                        <b>[12]</b>
                                        LESKOVEC J, KREVL A.SNAP datasets:Stanford large network dataset collection[DB/OL].[2017-07-01].http://snap.stanford.edu/data.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-09-29 09:51</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(03),651-655 DOI:10.11772/j.issn.1001-9081.2018071509            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进随机游走的网络表示学习算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%96%87%E6%B6%9B&amp;code=10167615&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王文涛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E7%83%A8&amp;code=37856031&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄烨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%B7%8B%E6%B6%9B&amp;code=41275238&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴淋涛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9F%AF%E7%92%87&amp;code=41275239&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">柯璇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E8%8F%80&amp;code=10479249&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐菀</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%8D%97%E6%B0%91%E6%97%8F%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0185453&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中南民族大学计算机科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有的基于Word2vec的网络表示学习 (NRL) 算法使用随机游走 (RW) 来生成节点序列, 针对随机游走倾向于选择具有较大度的节点, 生成的节点序列不能很好地反映网络结构信息, 从而影响表示学习性能的问题, 提出了基于改进随机游走的网络表示学习算法。首先, 使用RLP-MHRW算法生成节点序列, 它在生成节点序列时不会偏向大度节点, 得到的节点序列能更好地反映网络结构信息;然后, 将节点序列投入到Skip-gram模型得到节点表示向量;最后, 利用链路预测任务来测度表示学习性能。在4个真实网络数据集上进行了实验。在论文合作网络arXiv ASTRO-PH上与LINE和node2vec算法相比, 链路预测的AUC值分别提升了8.9%和3.5%, 其他数据集上也均有提升。实验结果表明, RLP-MHRW能有效提高基于Word2vec的网络表示学习算法的性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络表示学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">随机游走;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">链路预测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E5%81%8F%E9%87%87%E6%A0%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无偏采样;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王文涛 (1967—) , 男, 河北邯郸人, 副教授, 博士, 主要研究方向:计算机网络与控制;;
                                </span>
                                <span>
                                    *黄烨 (1993—) , 男, 湖北大悟人, 硕士研究生, 主要研究方向:知识表示、神经网络;电子邮箱2016110265@mail.scuec.edu.cn;
                                </span>
                                <span>
                                    吴淋涛 (1994—) , 男, 湖南茶陵人, 硕士研究生, 主要研究方向:数据挖掘;;
                                </span>
                                <span>
                                    柯璇 (1994—) , 女, 湖北阳新人, 硕士研究生, 主要研究方向:人工智能;;
                                </span>
                                <span>
                                    唐菀 (1974—) , 女, 贵州都匀人, 教授, 博士, 主要研究方向:光/无线网络协议、网络安全。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61103248);</span>
                                <span>中南民族大学中央高校基本科研业务费专项 (CZY18014) ;中南民族大学研究生创新基金资助项目 (2018sycxjj269);</span>
                    </p>
            </div>
                    <h1><b>Network representation learning algorithm based on improved random walk</b></h1>
                    <h2>
                    <span>WANG Wentao</span>
                    <span>HUANG Ye</span>
                    <span>WU Lintao</span>
                    <span>KE Xuan</span>
                    <span>TANG Wan</span>
            </h2>
                    <h2>
                    <span>College of Computer Science, South-Central University for Nationalities</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Existing Word2 vec-based Network Representation Learning (NRL) algorithms use a Random Walk (RW) to generate node sequence. The RW tends to select nodes with larger degrees, so that the node sequence can not reflect the network structure information well, decreasing the performance of the algorithm. To solve the problem, a new network representation learning algorithm based on improved random walk was proposed. Firstly, RLP-MHRW (Remove self-Loop Probability for Metropolis-Hastings Random Walk) was used to generate node sequence. This algorithm would not favor nodes with larger degrees while forming a node sequence, so that the obtained sequence can efficiently reflect the network structure information. Then, the node sequence was put into Skip-gram model to obtain the node representation vector. Finally, the performance of the network representation learning algorithm was measured by a link prediction task. Contrast experiment has been performed in four real network datasets. Compared with LINE (Large-scale Information Network Embedding) and node2 vec on arXiv ASTRO-PH, the AUC (Area Under Curve) value of link prediction has increased by 8.9% and 3.5% respectively, and so do the other datasets. Experimental results show that RLP-MHRW can effectively improve the performance of the network representation learning algorithm based on Word2 vec.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Network%20Representation%20Learning%20(NRL)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Network Representation Learning (NRL) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Random%20Walk%20(RW)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Random Walk (RW) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=link%20prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">link prediction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=unbiased%20sampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">unbiased sampling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Machine%20Learning%20(ML)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Machine Learning (ML) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Wentao, born in 1967, Ph. D. , associate professor. His research interests include computer network and control.;
                                </span>
                                <span>
                                    HUANG Ye, born in 1993, M. S. candidate. His research interests include knowledge representation, neural network.;
                                </span>
                                <span>
                                    WU Lintao, born in 1994, M. S. candidate. His research interests include data mining.;
                                </span>
                                <span>
                                    KE Xuan, born in 1994, M. S. candidate. Her research interests include artificial intelligence.;
                                </span>
                                <span>
                                    TANG Wan, born in 1974, Ph. D. , professor. Her research interests include optical/wireless network protocol, network security.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-07-23</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61103248);</span>
                                <span>the Fundamental Research Funds for the Central Universities of South-CentralUniversityforNationalities (CZY18014) ;the Innovative Research Program for Graduates of SouthCentral University for Nationalities (2018sycxjj269);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="27" name="27" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="28">有在现实世界中, 网络可以很好地描述一些关系系统, 比如社会、生物和信息系统。系统中的每个实体都映射到网络中的一个节点, 实体之间的关系被映射到网络中的边。网络结构有助于更直观地了解现实世界中的事物关系, 例如, 利用网络结构表示的微博数据, 能很清楚地表示微博用户的关注和被关注信息;在蛋白质相互作用网络里连边展示了蛋白质间能否相互反应。但是, 想要进一步了解网络的深层信息就需要用到网络分析和数据挖掘的技术。网络分析任务通常涉及节点和边的信息预测, 在一个典型的链路预测任务中, 通常试图根据观察到的链接和节点的属性来预测两个节点之间是否有连接<citation id="232" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。链路预测被广泛地应用于各个领域<citation id="233" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 越来越多的国内外学者开始研究复杂网络中的链接预测问题<citation id="234" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="29">随着机器学习算法的盛行, 越来越多的机器学习算法被设计出来, 其性能也远超传统算法。但是, 网络结构数据不能直接作为机器学习算法的输入, 一个直观的想法就是将网络结构特征提取出来并嵌入到向量中, 将这个特征向量作为机器学习算法的输入实现网络分析任务。这个向量必须要尽可能地包含原始网络结构的信息, 且维度不能太大, 网络表示学习 (Network Representation Learning, NRL) 就正好满足了这一需求。网络表示学习算法将网络信息转换为低维实值向量<citation id="235" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 在学习到低维向量之后, 就可以使用现有的机器学习方法来简单高效地执行网络分析任务, 这不仅避免了传统方法的复杂计算, 还提高了算法性能<citation id="236" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="30">Mikolov等在文献<citation id="237" type="reference">[<a class="sup">6</a>]</citation>中提出的Word2vec神经网络语言模型在词表示上取得良好效果, Perozzi等受此文启发在文献<citation id="238" type="reference">[<a class="sup">7</a>]</citation>提出DeepWalk算法, 第一次将深度学习中的技术引入到网络表示学习领域, 利用随机游走 (Random Walk, RW) 从网络中生产节点序列类比于“句子”, 将节点类比于“词”投入到Word2vec模型中得到节点表示向量, 在网络分析任务上取得较大的性能提升。随后又出现了基于简单神经网络的LINE (Large-scale Information Network Embedding) <citation id="239" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和改进DeepWalk的node2vec<citation id="240" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等算法。LINE算法没有延用DeepWalk的思想, 而是对所有的第一级相似度和第二级相似度节点对进行概率建模, 最小化概率分布和经验分布的距离来得到表示结果, 有着较好的表示效果;node2vec算法在生成节点序列时没有像DeepWalk那样均匀地随机选取下一节点, 而是引入<i>p</i>、<i>q</i>两个超参数来均衡采样过程的采样深度和宽度, 提升了算法的扩展性, 同时还提高了节点表示的性能。</p>
                </div>
                <div class="p1">
                    <p id="31">这些基于Word2vec的网络表示学习算法, 使用随机游走 (RW) 或其改进算法来得到节点序列, 然而, RW算法在采样时会偏向大度节点, 会导致采样样本不能正确反映完整的网络拓扑信息<citation id="241" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>;所以, 使用RW算法生成节点序列的网络表示学习算法, 其性能也会受到节点采样的影响。</p>
                </div>
                <div class="p1">
                    <p id="32">针对这一问题, 分别使用MHRW (Metropolis-Hastings Random Walk) <citation id="242" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和改进后的MHRW代替RW生成节点序列, 旨在使采样得到的节点序列能更好地保留原网络的结构特征。为了测度网络表示学习算法的性能, 本文使用常见的网络分析任务——链路预测来衡量网络表示学习算法的性能。最终实验结果显示, 相比基准对比算法, 本文所提算法有更好的链路预测效果, 即本文所提网络表示学习算法有更好的表示性能。</p>
                </div>
                <h3 id="33" name="33" class="anchor-tag">1 相关问题</h3>
                <div class="p1">
                    <p id="34">本章主要介绍与本文工作相关的定义、模型和问题。</p>
                </div>
                <h4 class="anchor-tag" id="35" name="35">1.1 Word2vec<b>模型</b></h4>
                <div class="p1">
                    <p id="36">Mikolov在文献<citation id="243" type="reference">[<a class="sup">6</a>]</citation>中提出Word2vec神经网络语言模型, 该模型能通过给定文本快速地学习词向量表示。Word2vec中包含CBOW (Continuous Bag-Of-Words model) 和Skip-gram (continuous skip-gram model) 两种语言模型。前者利用词的上下文来预测中间词;后者则利用当前词来预测其上下文。在使用Word2vec时可以通过设置相应的参数来选择所要使用的模型。这两个模型都是包含输入层、投影层和输出层的三层神经网络, 如图1所示。</p>
                </div>
                <div class="area_img" id="37">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903007_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 CBOW模型与Skip-gram模型" src="Detail/GetImg?filename=images/JSJY201903007_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 CBOW模型与Skip-gram模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903007_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 CBOW model and Skip-gram model</p>

                </div>
                <h4 class="anchor-tag" id="38" name="38">1.2 <b>网络表示学习</b></h4>
                <div class="p1">
                    <p id="39">网络表示学习 (NRL) 定义:给定网络<i>G</i> (<i>V</i>, <i>E</i>) , 对任意节点<i>v</i>∈<i>V</i>, 学习低维向量表示<b><i>r</i></b><sub><i>v</i></sub>∈<b>R</b><sup><i>d</i></sup>, <b><i>r</i></b><sub><i>v</i></sub>是一个稠密的低维实数向量, 并且满足<i>d</i>远小于|<i>V</i>|。</p>
                </div>
                <div class="p1">
                    <p id="40">传统机器学习结果的好坏极度依赖人工特征设计, 这一过程繁琐且精度不高。信息化时代导致数据量激增, 在庞杂的数据中寻找有价值的特征无疑是一项耗时耗力的工作, 最终还很难找到令人满意的特征。表示学习的出现就解决了这一问题, 它的目标是从原始数据中自动地学习到有意义的特征表示。</p>
                </div>
                <div class="p1">
                    <p id="41">网络表示学习算法负责从网络数据中学习得到网络中每个节点的向量表示, 之后这些节点表示就可以作为节点的特征应用于后续的网络应用任务, 如节点分类、链接预测和可视化等<citation id="244" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="42" name="42">1.3 <b>基于</b>Word2vec<b>的网络表示学习</b></h4>
                <div class="p1">
                    <p id="43">Word2vec神经网络语言模型最初是用来学习词表示向量的, Perozzi等在DeepWalk中第一次把Word2vec应用到网络表示学习中来, 他们把在网络中利用RW采样得到的节点序列类比“句子”, 把节点类比“词”, 投入到Word2vec模型中得到了节点的表示向量, 并获得了很好的表示效果。算法的整体框架如图2所示。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903007_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于Word2vec的网络表示学习框架" src="Detail/GetImg?filename=images/JSJY201903007_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于Word2vec的网络表示学习框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903007_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 NRL framework based on Word2vec</p>

                </div>
                <div class="p1">
                    <p id="45">图2中, <i>k</i>表示节点序列的长度, <i>d</i>表示表示向量的维度。从图2可以看出, 采样算法得到的节点序列是影响后续产生的节点表示特征向量的关键步骤之一, 所以采样算法的性能也将直接影响到整个模型的性能。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46">1.4 RW<b>及其有偏性</b></h4>
                <div class="p1">
                    <p id="47">RW算法的思想是从当前节点出发以等概率选取其邻居节点作为下一个采样节点。经典的随机游走采样的转移概率表达式如式 (1) :</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mrow><mtext>R</mtext><mtext>W</mtext></mrow></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>/</mo><mi>k</mi><msub><mrow></mrow><mi>u</mi></msub><mo>, </mo><mtext> </mtext><mi>v</mi><mspace width="0.25em" /><mtext>i</mtext><mtext>s</mtext><mspace width="0.25em" /><mtext>n</mtext><mtext>e</mtext><mtext>i</mtext><mtext>g</mtext><mtext>h</mtext><mtext>b</mtext><mtext>o</mtext><mtext>r</mtext><mspace width="0.25em" /><mtext>o</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>u</mi><mo>;</mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中, <i>P</i><mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mrow><mtext>R</mtext><mtext>W</mtext></mrow></msubsup></mrow></math></mathml>是从节点<i>u</i>转移到节点<i>v</i>的概率, <i>k</i><sub><i>u</i></sub>是节点<i>u</i>的度。</p>
                </div>
                <div class="p1">
                    <p id="51">对于RW算法, <i>G</i> (<i>V</i>, <i>E</i>) 中任意节点<i>v</i>被采样到的概率是<mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>π</mi><msub><mrow></mrow><mi>v</mi></msub><mo>=</mo><mfrac><mrow><mi>k</mi><msub><mrow></mrow><mi>v</mi></msub></mrow><mrow><mn>2</mn><mrow><mo>|</mo><mi>E</mi><mo>|</mo></mrow></mrow></mfrac><msup><mrow></mrow><mrow><mo stretchy="false">[</mo><mn>1</mn><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow></math></mathml>, 其中|<i>E</i>|是目标采样图的边数, 很容易看出RW采样时会偏向于大度节点。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag">2 本文方法</h3>
                <div class="p1">
                    <p id="54">这一章里, 首先介绍了MHRW算法的思想并在推导过程中说明了MHRW采样的无偏性, 随后就MHRW算法的缺点进行了讨论和改进。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">2.1 MHRW</h4>
                <div class="p1">
                    <p id="56">文献<citation id="245" type="reference">[<a class="sup">10</a>]</citation>中提出的MHRW算法是一种无偏采样算法, 对于MHRW算法, <i>G</i> (<i>V</i>, <i>E</i>) 中任意节点<i>v</i>被采样到的概率是<i>π</i><sub><i>v</i></sub>=1/|<i>V</i>|, 即任意节点被采样到的概率是相等的。MHRW算法是借鉴了MH (Metropolis-Hasting) <citation id="246" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>算法的思想而形成的。</p>
                </div>
                <div class="p1">
                    <p id="57">MH算法的思想是, 假设从概率密度函数为<i>P</i><sub><i>x</i>, <i>y</i></sub>的目标中采样, MH算法会构建一个马尔可夫链, 使得该马尔可夫链的平稳状态分布<i>π</i>是所期望的分布。构建的马尔可夫链的转移概率<i>q</i><sub><i>x</i>, <i>y</i></sub>表达式如式 (2) :</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>q</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>p</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow></msub><mo>⋅</mo><mi>min</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>μ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mi>p</mi><msub><mrow></mrow><mrow><mi>y</mi><mo>, </mo><mi>x</mi></mrow></msub></mrow><mrow><mi>μ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mi>p</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow></msub></mrow></mfrac><mo>, </mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mi>x</mi><mo>≠</mo><mi>y</mi></mtd></mtr><mtr><mtd><mn>1</mn><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>z</mi><mo>≠</mo><mi>x</mi></mrow></munder><mi>q</mi></mstyle><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>z</mi></mrow></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>x</mi><mo>=</mo><mi>y</mi></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中:<mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>μ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mi>p</mi><msub><mrow></mrow><mrow><mi>y</mi><mo>, </mo><mi>x</mi></mrow></msub></mrow><mrow><mi>μ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mi>p</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow></msub></mrow></mfrac><mo>, </mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></mathml>是MH算法的核心, 它是一个接受概率, 在构建马尔可夫链时就以这个概率来确定是否接受新产生的状态, <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>z</mi><mo>≠</mo><mi>x</mi></mrow></munder><mi>q</mi></mstyle><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>z</mi></mrow></msub></mrow></math></mathml>是不接受新状态的概率和, 即停留在当前节点的概率, <i>μ</i> (·) 是所期望分布的平稳状态。</p>
                </div>
                <div class="p1">
                    <p id="62">MHRW算法就是在RW采样的基础上引入MH算法, 从RW的概率密度函数中采样, 构建一个平稳分布状态为均匀分布的马尔可夫链。结合式 (2) 就得到了MHRW算法的转移概率表达式如式 (3) :</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>p</mi><msubsup><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mrow><mtext>R</mtext><mtext>W</mtext></mrow></msubsup><mo>⋅</mo><mi>min</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>μ</mi><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>v</mi><mo>, </mo><mi>u</mi></mrow><mrow><mtext>R</mtext><mtext>W</mtext></mrow></msubsup></mrow><mrow><mi>μ</mi><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mrow><mtext>R</mtext><mtext>W</mtext></mrow></msubsup></mrow></mfrac><mo>, </mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mi>u</mi><mo>≠</mo><mi>v</mi><mo>;</mo></mtd></mtr><mtr><mtd><mn>1</mn><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>u</mi></mrow></munder><mi>Ρ</mi></mstyle><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>i</mi></mrow></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>u</mi><mo>=</mo><mi>v</mi></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">因为<i>μ</i> (·) 是均匀分布, 所以<i>μ</i> (<i>v</i>) =<i>μ</i> (<i>u</i>) 再将式 (1) 代入, 可得到MHRW算法的概率转移表达式如式 (4) :</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mi>u</mi></msub></mrow></mfrac><mo>⋅</mo><mi>min</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>k</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mrow><mi>k</mi><msub><mrow></mrow><mi>v</mi></msub></mrow></mfrac><mo>, </mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>, </mo><mtext> </mtext><mi>v</mi><mspace width="0.25em" /><mtext>i</mtext><mtext>s</mtext><mspace width="0.25em" /><mtext>n</mtext><mtext>e</mtext><mtext>i</mtext><mtext>g</mtext><mtext>h</mtext><mtext>b</mtext><mtext>o</mtext><mtext>r</mtext><mspace width="0.25em" /><mtext>o</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>u</mi><mo>;</mo></mtd></mtr><mtr><mtd><mn>1</mn><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>u</mi></mrow></munder><mi>Ρ</mi></mstyle><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>i</mi></mrow></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>v</mi><mo>=</mo><mi>u</mi><mo>;</mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66"><i>P</i><sub><i>u</i>, <i>v</i></sub>是节点<i>u</i>转移到节点<i>v</i>的概率, <i>k</i><sub><i>u</i></sub>是节点<i>u</i>的度。</p>
                </div>
                <div class="p1">
                    <p id="67">从式 (4) 可以看出, MHRW算法的节点转移概率是由当前节点<i>u</i>的度和其邻居节点<i>v</i>的度共同决定的, 当<i>v</i>=<i>u</i>时, 表示节点停留在当前节点。从MHRW算法的公式推导过程容易看出, MHRW采样算法是等概率采样任意节点, 即MHRW是无偏的。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">2.2 RLP-MHRW</h4>
                <div class="p1">
                    <p id="69">MHRW在节点采样时会有一定概率停留在当前节点, 为了更好地理解这个停留概率这里用自环率<i>C</i><sub><i>i</i></sub>来表示节点的停留概率即<i>C</i><sub><i>i</i></sub>=<i>P</i><sub><i>i</i>, <i>i</i></sub>∈[0, 1) , MHRW导致自环率如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="70">图3中节点上的数字表示节点的度, 边上的数字 (没有画出全部的边) 表示利用式 (4) 计算出来的节点间的转移概率, 为了表示自环率图3在节点自身上加了一条自环边。可以看出节点<i>x</i>和节点<i>z</i>的自环率高达0.74, 节点<i>e</i>的自环率更是高达0.983, 这意味着当游走到这些节点时将会长时间地停留在这些节点, 导致采样算法收敛慢。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903007_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 MHRW引入的高自环率" src="Detail/GetImg?filename=images/JSJY201903007_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 MHRW引入的高自环率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903007_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 High self-loop probabilities generated by MHRW</p>

                </div>
                <div class="p1">
                    <p id="72">在网络表示学习中, 节点序列的采样希望能尽可能地发现节点的邻域结构信息, 类似与图3的高自环率会导致采样算法有限的采样步长内丢失较多的邻域节点;并且, 转移概率的严格对称也并不合理, 例如, 图3中节点<i>e</i>的度为1, 只有一个邻居节点<i>f</i>, 那么从节点<i>e</i>只能转移到节点<i>f</i>, 转移概率应该为1, 而不是与<i>f</i>到<i>e</i>对等的0.017。</p>
                </div>
                <div class="p1">
                    <p id="73">所以, 本文进一步将MHRW算法产生的自环率去掉, 得到RLP-MHRW (Remove self-Loop Probability for MHRW) , 并且将转移概率变为有向, 同时在去自环率的过程中保持MHRW算法原有的节点转移概率分布比例, 并且保证转移概率矩阵行和为1。具体做法是先用MHRW算法计算出各节点转移概率, 然后将节点<i>u</i>的自环率<i>C</i><sub><i>u</i></sub> (非0) 按照其转移到邻居节点的概率<i>P</i><sub><i>u</i>, <i>v</i></sub>的比例划分, 并将划分得到的概率对应追加给<i>P</i><sub><i>u</i>, <i>v</i></sub>, 数学化定义如式 (5) :</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mi>c</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub><mo>⋅</mo><mfrac><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>u</mi></msub></mrow></munder><mi>Ρ</mi></mstyle><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>i</mi></mrow></msub></mrow></mfrac><mo>+</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mtext> </mtext><mi>v</mi><mspace width="0.25em" /><mtext>i</mtext><mtext>s</mtext><mspace width="0.25em" /><mtext>n</mtext><mtext>e</mtext><mtext>i</mtext><mtext>g</mtext><mtext>h</mtext><mtext>b</mtext><mtext>o</mtext><mtext>r</mtext><mspace width="0.25em" /><mtext>o</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>u</mi><mo>;</mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中:<i>P</i><sub><i>u</i>, <i>v</i></sub>是使用MHRW算法计算得到的转移概率, 可以用式 (4) 来计算得到;<i>C</i><sub><i>u</i></sub>就是节点<i>u</i>的自环率, <i>C</i><sub><i>u</i></sub>=<i>P</i><sub><i>u</i>, <i>u</i></sub>;<i>N</i><sub><i>u</i></sub>是节点<i>u</i>的邻居节点集合。从中可以看出, 节点<i>u</i>转移到邻居节点的概率被同时放大了<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>u</mi></msub></mrow></munder><mi>Ρ</mi></mstyle><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>i</mi></mrow></msub></mrow></mfrac><mo>+</mo><mn>1</mn></mrow></math></mathml>倍, 在去自环的同时保留了原算法的转移概率比。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">2.3 <b>算法实施</b></h4>
                <div class="p1">
                    <p id="78">针对RW算法在图采样上偏向于大度节点的缺点, 使用本文提到的MHRW算法和RLP-MHRW算法来进行网络采样生成节点序列, 然后, 分别利用Skip-gram模型训练得到节点表示向量, 算法定义如下。</p>
                </div>
                <div class="p1">
                    <p id="79">算法1 改进随机游走的网络表示学习算法。</p>
                </div>
                <div class="p1">
                    <p id="80">输入:<i>G</i>= (<i>V</i>, <i>E</i>) , 向量维度<i>d</i>, 每个节点生成节点序列的次数<i>γ</i>, 生成节点序列长度<i>l</i>, 窗口大小<i>ω</i>;</p>
                </div>
                <div class="p1">
                    <p id="81">输出:节点表示向量矩阵<b><i>H</i></b>∈<b>R</b><sup>|<i>V</i>|*<i>d</i></sup>。</p>
                </div>
                <div class="area_img" id="206">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201903007_20600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="91">算法2 SequenceSample (<i>G</i>, <i>v</i>, <i>l</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="92">输入:<i>G</i>= (<i>V</i>, <i>E</i>) , 起始节点<i>v</i>, 序列长度<i>l</i>;</p>
                </div>
                <div class="p1">
                    <p id="93">输出:节点采样序列<i>walk</i>。</p>
                </div>
                <div class="area_img" id="207">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201903007_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="103">算法2里的Sample () 函数会依据给定的概率来选择返回的节点, 利用式 (4) 计算<i>P</i>表示该算法为MHRW, 利用式 (5) 则为RLP-MHRW。在给定网络<i>G</i>= (<i>V</i>, <i>E</i>) 和相关的参数后, 算法1将会计算出<i>G</i>中所有节点的表示向量。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">3 仿真与性能分析</h3>
                <h4 class="anchor-tag" id="105" name="105">3.1 <b>实验数据及数据处理</b></h4>
                <div class="p1">
                    <p id="106">本文实验数据为4个不同大小、不同领域、具有代表性的真实网络数据集, 均为无向无权网络, 细节分别如下:</p>
                </div>
                <div class="p1">
                    <p id="107">Facebook<citation id="247" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> 一个美国的社交网络, 数据中节点表示用户, 边表示用户间的朋友关系, 网络中有4 039个节点, 88 234条边;</p>
                </div>
                <div class="p1">
                    <p id="108">arXiv ASTRO-PH<citation id="248" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> (arXiv Astro Physics collaboration network) 一个从arXiv电子出版论文中生成的论文合作网络, 其中节点表示科学家, 边表示科学家合作过论文, 网络中有18 722个节点, 198 110条边;</p>
                </div>
                <div class="p1">
                    <p id="109">USAir (http://vlado.fmf.uni-lj.si/pub/networks/data/) 一个航空路线网络, 其中节点表示机场, 边表示机场间的航线, 网络中有332个节点, 2 126条边;</p>
                </div>
                <div class="p1">
                    <p id="110">Metabolic (http://www.linkprediction.org/index.php/link/resource/data/) 一个生物代谢网络, 其中节点表示线虫代谢物, 表示代谢物之间能直接参与酶催化反应, 网络中有453个节点, 2 025条边。</p>
                </div>
                <div class="p1">
                    <p id="111">为了评估算法, 本文将数据集随机地划分为训练集和测试集, 50%的边被抽取作为训练集, 50%的边作为测试集。在随机划分过程中要保证训练集网络的连通性, 数据集的标签依据原网络中边的有无来定, 原网络中存在的边标签为1, 其他为0。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">3.2 <b>基准方法</b></h4>
                <div class="p1">
                    <p id="113">为了展示算法性能, 本文选用以下算法作为对比:</p>
                </div>
                <div class="p1">
                    <p id="114">DeepWalk 该方法是第一个将深度学习使用到网络表示学习中的算法, 它使用均匀随机游走来生成节点序列, 利用Skip-gram模型来学习节点向量表示;</p>
                </div>
                <div class="p1">
                    <p id="115">LINE 该方法将学习<i>d</i>维的特征表示分为两部分:第一部分, 在直接邻居节点上模拟BFS (Breadth-First Search) 来学习<i>d</i>/2维的特征;第二部分, 严格的从距离源节点2-hop的节点中采样学习剩下的<i>d</i>/2为特征;</p>
                </div>
                <div class="p1">
                    <p id="116">node2vec node2vec是一个半监督的NRL模型, 它由<i>p</i>、<i>q</i>两个超参数来分别控制随机游走的深度和广度, 这两个超参数需要额外的训练。当<i>p</i>=<i>q</i>=1时, node2vec等价于DeepWalk。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">3.3 <b>算法评价</b></h4>
                <div class="p1">
                    <p id="118">本文网络表示学习算法得到的是节点表示向量, 要利用链路预测任务来测度算法的性能就需要得到边的特征表示向量, 这里直接引用文献<citation id="249" type="reference">[<a class="sup">2</a>]</citation>中的映射操作, 如表1所示。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表</b>1 <b>从节点表示到边表示的</b>4<b>种方法</b><citation id="250" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation><sup></sup> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Four ways to edge represent from node represent<citation id="251" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation><sup></sup></p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td><br />标记</td><td>映射</td><td>操作符</td><td>定义</td></tr><tr><td><br />a</td><td>Average</td><td>⊕</td><td>[<b><i>f</i></b> (<i>u</i>) ⊕<b><i>f</i></b> (<i>v</i>) ]<sub><i>i</i></sub>= (<i>f</i><sub><i>i</i></sub> (<i>u</i>) +<i>f</i><sub><i>i</i></sub> (<i>v</i>) ) /2</td></tr><tr><td><br />b</td><td>Hadamard</td><td>⨂</td><td>[<b><i>f</i></b> (<i>u</i>) ⨂<b><i>f</i></b> (<i>v</i>) ]<sub><i>i</i></sub>=<i>f</i><sub><i>i</i></sub> (<i>u</i>) *<i>f</i><sub><i>i</i></sub> (<i>v</i>) </td></tr><tr><td><br />c</td><td>Weighted-L1</td><td>‖·‖<sub>1</sub></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">f</mi><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi mathvariant="bold-italic">f</mi><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msub><mrow></mrow><mrow><mover accent="true"><mn>1</mn><mo stretchy="true">¯</mo></mover><mspace width="0.25em" /></mrow></msub><msub><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow></msub><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><msub><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></math></td></tr><tr><td><br />d</td><td>Weighted-L2</td><td>‖·‖<sub>2</sub></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">f</mi><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi mathvariant="bold-italic">f</mi><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msub><mrow></mrow><mrow><mover accent="true"><mn>2</mn><mo stretchy="true">¯</mo></mover><mspace width="0.25em" /></mrow></msub><msub><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow></msub><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><msub><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></td></tr><tr><td colspan="4"><br />注: <b><i>f</i></b> (·) 表示节点特征向量。</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">链路预测的结果度量使用AUC (Area Under the receiver operating characteristic Curve) 值作为算法评价指标, AUC是常用的二值分类评价标准, 从整体上评价算法精度。利用预测算法计算出所有节点间边存在的分值, AUC值可以看作是在测试集中随机选取一条存在边的分值大于随机一条不存在的边分值的概率。一般AUC值会大于0.5, 值越高算法精度越高, 最大不超过1。公式化的定义如下:</p>
                </div>
                <div class="p1">
                    <p id="121"><i>AUC</i>= (<i>N</i><sub><i>p</i></sub>+0.5<i>N</i><sub><i>n</i></sub>) /<i>N</i>      (6) </p>
                </div>
                <div class="p1">
                    <p id="122">其中:<i>N</i>为抽样次数, <i>N</i><sub><i>p</i></sub>为存在边的分数大于不存在边分数的次数, <i>N</i><sub><i>n</i></sub>为存在边的分数小于不存在边分数的次数。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123">3.4 <b>实验环境及参数设置</b></h4>
                <div class="p1">
                    <p id="124">实验硬件环境为:Intel Core i7-4790 (3.6 GHz*8) CPU, 32 GB内存;</p>
                </div>
                <div class="p1">
                    <p id="125">软件环境为:Ubuntun16.04系统, 使用python 2.7版本, 主要python第三方包及版本有:用于科学计算的numpy 1.12.1版, 处理网络数据的networkx 1.11版和包含Word2vec的gensim 2.2.0版;</p>
                </div>
                <div class="p1">
                    <p id="126">实验参数:采样迭代次数<i>r</i>=10, 节点序列长度<i>l</i>=80, Skip-gram模型窗口设为10, 节点向量维度为<i>d</i>=128。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.5 <b>实验及结果分析</b></h4>
                <div class="p1">
                    <p id="128">为了更方便地将本文方法与相关基线进行比较, 本文将链接预测作为所有方法的下游任务。对于所有的模型, 使用逻辑回归来预测。为了减弱随机性, 将本文所提算法在3.1节提到的数据集上独立重复实验20次, 以20次实验的AUC平均值作为最终结果。表2为各算法在预测结果上AUC值的对比, 表3为MHRW和RLP-MHRW生成节点序列的执行时间的对比。</p>
                </div>
                <div class="area_img" id="129">
                                            <p class="img_tit">
                                                <b>表</b>2 <b>链路预测的</b>AUC<b>值</b>
                                                    <br />
                                                Tab. 2 AUC scores for link prediction
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903007_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201903007_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903007_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 链路预测的AUC值" src="Detail/GetImg?filename=images/JSJY201903007_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>
                                <p class="img_note">注:4种操作分别表示表1中节点向量表示到边向量表示的4种映射, 带下划线数字是对应数据下AUC值的最大值。</p>

                </div>
                <div class="p1">
                    <p id="130">从表2的AUC结果可以看出, 使用MHRW后, 与对比算法相比, 链路预测的AUC值在相应的条件下均有提高, 去掉自环率后算法性能进一步提高。引入无偏采样技术后, 算法性能的提升可能原因是, 无偏采样算法采集到的节点序列能更好地保留原网络的结构特征信息, 进而提高了下游链路预测任务的精度;RLP-MHRW算法, 去掉了自环率使得算法不是严格无偏, 却又保留无偏采样时节点将转移概率的比例, 同时同比例放大了节点转移到邻居节点的概率, 使得采样算法能更好地发现节点邻域结构, 但是这一提升仅仅体现在有自环率的节点上, 所以, RLP-MHRW算法比MHRW算法只有较小的AUC值提升。但是, 结合表2和表3来看, RLP-MHRW算法不仅在预测性能上有所提升, 在采样效率上也远高于原始MHRW算法, 而且网络规模越大时提升越大, 所以在处理大规模网络时改进后的算法将会有更大的优势。</p>
                </div>
                <div class="p1">
                    <p id="131">表3中的数据是MHRW和RLP-MHRW在4个网络数据中完成10次迭代采样 (生成节点序列) 所用时间。</p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表</b>3 MHRW<b>和</b>RLP-MHRW 10<b>次迭代采样时间比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Time comparison for iterative sampling by MHRW and RLP-MHRW </p>
                    <p class="img_note">s</p>
                    <table id="132" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="4"><br />数据集</td></tr><tr><td><br />Facebook</td><td>arXiv ASTRO-PH</td><td>USAir</td><td>Metabolic</td></tr><tr><td>MHRW</td><td>886.18</td><td>4377.30</td><td>138.81</td><td>148.66</td></tr><tr><td><br />RLP-MHRW</td><td>30.88</td><td>106.29</td><td>1.20</td><td>1.27</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="133">DeepWalk<citation id="252" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和node2vec<citation id="253" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>都利用RW来生产节点序列, 虽然node2vec引入<i>p</i>、<i>q</i>两个超参数来指导随机游走的方向, 但是没有解决RW算法偏向于大度节点的问题。LINE<citation id="254" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的限制在于只是探索了目标节点1-hop和2-hop的邻居节点, 没有探索更深一步的邻居, 导致丢失部分节点邻域信息。本文所提的MHRW和RLP-MHRW不仅能探索更远的邻居节点, 还克服了RW算法的有偏性, 最终取得了优于以上算法的性能。</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="135">在基于Word2vec的网络表示学习算法中, 在生成节点序列时节点采样算法保留原网络结构信息的能力越强, 表示学习算法的性能就越好。本文针对现有基于Word2vec的网络表示学习算法中节点序列生成算法的缺陷, 提出了基于改进随机游走的网络表示学习算法。该算法在生成节点序列时利用MHRW采样算法来更好地保留原网络的结构信息, 使得Skip-Gram模型学习到的节点表示向量包含更多的原网络信息。实验结果显示, 使用MHRW生成节点序列的表示学习算法, 有效地提升了算法学习特征的能力;使用RLP-MHRW则进一步提升了算法能力, 并在生成节点序列的环节能有效地提升算法效率。但所提算法只能处理无向网络, 且没有考虑节点的属性信息。今后的研究中将会考虑有向图的网络表示学习算法, 并引入节点属性信息进一步提高网络表示学习的性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="208">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of link prediction in social networks">

                                <b>[1]</b>HASAN M A, ZAKI M J.A survey of link prediction in social networks[M]//AGGARWAL C C.Social Network Data Analytics.Berlin:Springer, 2011:243-275.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                GROVER A, LESKOVEC J.Node2vec:scalable feature learning for networks[C]//KDD'16:Proceedings of the 22nd ACMSIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2016:855-864.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100530194&amp;v=MjE0MzFPcm85RlllZ1BEWFU5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpKS0Y0Y2FSRT1OaWZPZmJLN0h0RA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>LU Y L, ZHOU T.Link prediction in complex networks:a survey[J].Physica A:Statistical Mechanics and Its Applications, 2011, 390 (6) :1150-1170.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201708003&amp;v=MjYyMDl1WnBGaURsVzd6TE5UZkFkckc0SDliTXA0OUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>涂存超, 杨成, 刘知远, 等.网络表示学习综述[J].中国科学:信息科学, 2017 (8) :980-996. (TU C C, YANG C, LIU Z Y, et al.Network representation learning:an overview[J].SCIENTIA SINI-CA Informationis, 2017 (8) :980-996.) 
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network representation learning:a survey">

                                <b>[5]</b>ZHANG D, YIN J, ZHU X.Network representation learning:a survey[J].IEEE Transactions on Big Data, 2017, 99:1.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                MIKOLOV T, SUTSKEVER I, CHEN K, et al.Distributed representations of words and phrases and their compositionality[C]//NIPS'13:Proceedings of the 26th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates, 2013, 2:3111-3119.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                PEROZZI B, AL-RFOU R, SKIENA S.Deep Walk:online learning of social representations[C]//Proceedings of the 20th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM, 2014:701-710.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                TANG J, QU M, WANG M, et al.LINE:large-scale information network embedding[C]//WWW'15:Proceedings of the 24th International Conference on World Wide Web.Geneva, Switzerland:International World Wide Web Conferences Steering Committee, 2015:1067-1077.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding the metropolis-hastings algorithm">

                                <b>[9]</b>CHIB S, GREENBERG E.Understanding the metropolis-hastings algorithm[J].Americian Statistician, 1995, 49 (4) :327-335.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Walking in Facebook:A case study of unbiased sampling of OSNs">

                                <b>[10]</b>GJOKA M, KURANT M, BUTTS C T, et al.Walking in Facebook:a case study of unbiased sampling of OSNs[C]//INFOCOM'10:Proceedings of the 29th Conference on Information Communications.Piscataway, NJ:IEEE, 2010:2498-2506.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201605001&amp;v=MTQyODhIOWZNcW85RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3pMTHl2U2RMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>王栋, 李振宇, 谢高岗.在线社会网络无偏采样技术[J].计算机研究与发展, 2016, 53 (5) :949-967. (WANG D, LI Z Y, XIE GG.Unbiased sampling technologies on online social network[J].Journal of Computer Research and Development, 2016, 53 (5) :949-967.) 
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SNAP datasets">

                                <b>[12]</b>LESKOVEC J, KREVL A.SNAP datasets:Stanford large network dataset collection[DB/OL].[2017-07-01].http://snap.stanford.edu/data.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201903007" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903007&amp;v=Mjc3NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3pMTHo3QmQ3RzRIOWpNckk5Rlk0UUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
