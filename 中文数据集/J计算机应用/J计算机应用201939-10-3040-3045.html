<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136463557783750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910041%26RESULT%3d1%26SIGN%3dFJ4Pvq7vWThnAsGuAETIHDrJMgQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910041&amp;v=MDA5NDl1WnNGeWprVjd2SUx6N0JkN0c0SDlqTnI0OUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#71" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="1 锚定邻域回归算法 ">1 锚定邻域回归算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="2 本文算法 ">2 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="2.1 &lt;b&gt;训练样本的选择&lt;/b&gt;">2.1 <b>训练样本的选择</b></a></li>
                                                <li><a href="#99" data-title="2.2 &lt;b&gt;图像块聚类&lt;/b&gt;">2.2 <b>图像块聚类</b></a></li>
                                                <li><a href="#101" data-title="2.3 &lt;b&gt;投影矩阵计算&lt;/b&gt;">2.3 <b>投影矩阵计算</b></a></li>
                                                <li><a href="#105" data-title="2.4 &lt;b&gt;高分辨率图像块重建&lt;/b&gt;">2.4 <b>高分辨率图像块重建</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#109" data-title="3.1 &lt;b&gt;数据集&lt;/b&gt;">3.1 <b>数据集</b></a></li>
                                                <li><a href="#111" data-title="3.2 &lt;b&gt;参数设置&lt;/b&gt;">3.2 <b>参数设置</b></a></li>
                                                <li><a href="#113" data-title="3.3 &lt;b&gt;参数灵敏度&lt;/b&gt;">3.3 <b>参数灵敏度</b></a></li>
                                                <li><a href="#122" data-title="3.4 &lt;b&gt;与先进算法的比较&lt;/b&gt;">3.4 <b>与先进算法的比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#130" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="图1 本文算法框架">图1 本文算法框架</a></li>
                                                <li><a href="#116" data-title="图2 聚类数量对重建效果的影响">图2 聚类数量对重建效果的影响</a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;数据集&lt;/b&gt;&lt;i&gt;Set&lt;/i&gt;5&lt;b&gt;中每幅图像的&lt;/b&gt;&lt;i&gt;PSNR&lt;/i&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;SSIM&lt;/i&gt;&lt;b&gt;在放大倍数为&lt;/b&gt;3&lt;b&gt;时的性能对比&lt;/b&gt;"><b>表</b>1 <b>数据集</b><i>Set</i>5<b>中每幅图像的</b><i>PSNR</i><b>和</b><i>SSIM</i><b>在放大倍数为</b>3<b>时的性能对比</b></a></li>
                                                <li><a href="#121" data-title="图3 邻域数量对重建效果的影响">图3 邻域数量对重建效果的影响</a></li>
                                                <li><a href="#124" data-title="图4 与先进算法在速度上的比较">图4 与先进算法在速度上的比较</a></li>
                                                <li><a href="#128" data-title="图5 不同图像在放大倍数为3时的视觉定性评估">图5 不同图像在放大倍数为3时的视觉定性评估</a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;数据集&lt;/b&gt;Set14&lt;b&gt;中部分图像的&lt;/b&gt;PSNR&lt;b&gt;和&lt;/b&gt;SSIM&lt;b&gt;在放大倍数为&lt;/b&gt;3&lt;b&gt;时的性能对比&lt;/b&gt;"><b>表</b>2 <b>数据集</b>Set14<b>中部分图像的</b>PSNR<b>和</b>SSIM<b>在放大倍数为</b>3<b>时的性能对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="168">


                                    <a id="bibliography_1" title="JIANG J,CHEN C,MA J,et al.SRLSP:a face image super-resolution algorithm using smooth regression with local structure prior[J].IEEE Transactions on Multimedia,2016,19(1):27-40." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SRLSP:A Face Image Super-Resolution Algorithm Using Smooth Regression With Local Structure Prior">
                                        <b>[1]</b>
                                        JIANG J,CHEN C,MA J,et al.SRLSP:a face image super-resolution algorithm using smooth regression with local structure prior[J].IEEE Transactions on Multimedia,2016,19(1):27-40.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_2" title="WU L,WANG Y.The process of criminal investigation based on grey hazy set[C]//Proceedings of the 2010 IEEE International Conference on Systems,Man and Cybernetics.Piscataway:IEEE,2010:26-28." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The process of criminal investigation based on grey hazy set">
                                        <b>[2]</b>
                                        WU L,WANG Y.The process of criminal investigation based on grey hazy set[C]//Proceedings of the 2010 IEEE International Conference on Systems,Man and Cybernetics.Piscataway:IEEE,2010:26-28.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_3" title="EARLY D S,LONG D G.Image reconstruction and enhanced resolution imaging from irregular samples[J].IEEE Transactions on Geoscience and Remote Sensing,2001,39(2):291-302." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image reconstruction and enhanced resolution imaging from irregular samples">
                                        <b>[3]</b>
                                        EARLY D S,LONG D G.Image reconstruction and enhanced resolution imaging from irregular samples[J].IEEE Transactions on Geoscience and Remote Sensing,2001,39(2):291-302.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_4" title="GREENSPAN H.Super-resolution in medical imaging[J].The Computer Journal,2009,52(1):43-63." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution in medical imaging">
                                        <b>[4]</b>
                                        GREENSPAN H.Super-resolution in medical imaging[J].The Computer Journal,2009,52(1):43-63.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_5" title="WEI Z,MA K.Contrast-guided image interpolation[J].IEEETransactions on Image Processing,2013,22(11):4271-4285." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contrast-guided image interpolation">
                                        <b>[5]</b>
                                        WEI Z,MA K.Contrast-guided image interpolation[J].IEEETransactions on Image Processing,2013,22(11):4271-4285.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_6" title="YANG W,LIU J,LI M,et al.Isophote-constrained autoregressive model with adaptive window extension for image interpolation[J].IEEETransactions on Circuits and Systems for Video Technology,2018,28(5):1071-1086." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Isophote-Constrained autoregressive model with adaptive window extension for image interpolation">
                                        <b>[6]</b>
                                        YANG W,LIU J,LI M,et al.Isophote-constrained autoregressive model with adaptive window extension for image interpolation[J].IEEETransactions on Circuits and Systems for Video Technology,2018,28(5):1071-1086.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_7" title="ROMANO Y,PROTTER M,ELAD M.Single image interpolation via adaptive nonlocal sparsity-based modeling[J].IEEE Transactions on Image Processing,2014,23(7):3085-3098." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single image interpolation via adaptive non-local sparsity-based modeling">
                                        <b>[7]</b>
                                        ROMANO Y,PROTTER M,ELAD M.Single image interpolation via adaptive nonlocal sparsity-based modeling[J].IEEE Transactions on Image Processing,2014,23(7):3085-3098.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_8" title="YE W,MA K.Convolutional edge diffusion for fast contrast-guided image interpolation[J].IEEE Signal Processing Letters,2016,23(9):1260-1264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional edge diffusion for fast contrast-guided image interpolation">
                                        <b>[8]</b>
                                        YE W,MA K.Convolutional edge diffusion for fast contrast-guided image interpolation[J].IEEE Signal Processing Letters,2016,23(9):1260-1264.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_9" title="CAO F,CAI M,TAN Y.Image interpolation via low-rank matrix completion and recovery[J].IEEE Transactions on Circuits and Systems for Video Technology,2015,25(8):1261-1270." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Interpolation via Low-Rank Matrix Completion and Recovery">
                                        <b>[9]</b>
                                        CAO F,CAI M,TAN Y.Image interpolation via low-rank matrix completion and recovery[J].IEEE Transactions on Circuits and Systems for Video Technology,2015,25(8):1261-1270.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_10" title="HUANG J,SIU W,LIU T.Fast image interpolation via random forests[J].IEEE Transactions on Image Processing,2015,24(10):3232-3245." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Image interpolation via Random Forests">
                                        <b>[10]</b>
                                        HUANG J,SIU W,LIU T.Fast image interpolation via random forests[J].IEEE Transactions on Image Processing,2015,24(10):3232-3245.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_11" title="ZHU S,ZENG B,ZENG L,et al.Image interpolation based on non-local geometric similarities and directional gradients[J].IEEE Transactions on Multimedia,2016,18(9):1707-1719." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image interpolation based on non-local geometric similarities and directional gradients">
                                        <b>[11]</b>
                                        ZHU S,ZENG B,ZENG L,et al.Image interpolation based on non-local geometric similarities and directional gradients[J].IEEE Transactions on Multimedia,2016,18(9):1707-1719.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_12" title="MARQUINA A,OSHER S J.Image super-resolution by TV-regularization and Bregman iteration[J].Journal of Scientific Computing,2008,37(3):367-382." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003464871&amp;v=MjY0OTg3QmFyTzRIdEhQcTRsQmJPd09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpcmxVNzdNSkY0PU5q&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        MARQUINA A,OSHER S J.Image super-resolution by TV-regularization and Bregman iteration[J].Journal of Scientific Computing,2008,37(3):367-382.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_13" title="FERNANDEZ-GRANDA C,CANDS E J.Super-resolution via transform-invariant group-sparse regularization[C]//Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway:IEEE,2013:3336-3343." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution via transform-invariant groupsparse regularization">
                                        <b>[13]</b>
                                        FERNANDEZ-GRANDA C,CANDS E J.Super-resolution via transform-invariant group-sparse regularization[C]//Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway:IEEE,2013:3336-3343.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_14" title="DONG W,ZHANG L,SHI G,et al.Image deblurring and superresolution by adaptive sparse domain selection and adaptive regularization[J].IEEE Transactions on Image Processing,2011,20(7):1838-1857." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization">
                                        <b>[14]</b>
                                        DONG W,ZHANG L,SHI G,et al.Image deblurring and superresolution by adaptive sparse domain selection and adaptive regularization[J].IEEE Transactions on Image Processing,2011,20(7):1838-1857.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_15" title="DONG W,ZHANG L,SHI G,et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing,2013,22(4):1620-1630." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nonlocally Centralized Sparse Representation for Image Restoration">
                                        <b>[15]</b>
                                        DONG W,ZHANG L,SHI G,et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing,2013,22(4):1620-1630.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_16" title="SUN J,XU Z,SHUM H Y.Image super-resolution using gradient profile prior[C]//Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2008:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using gradient profile prior">
                                        <b>[16]</b>
                                        SUN J,XU Z,SHUM H Y.Image super-resolution using gradient profile prior[C]//Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2008:1-8.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_17" title="WANG L,XIANG S,MENG G,et al.Edge-directed single-image super-resolution via adaptive gradient magnitude self-interpolation[J].IEEE Transactions on Circuits and Systems for Video Technology,2013,23(8):1289-1299." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Edge-Directed Single Image Super-Resolution via Adaptive Gradient Magnitude Self-Interpolation">
                                        <b>[17]</b>
                                        WANG L,XIANG S,MENG G,et al.Edge-directed single-image super-resolution via adaptive gradient magnitude self-interpolation[J].IEEE Transactions on Circuits and Systems for Video Technology,2013,23(8):1289-1299.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_18" title="GAO X,ZHANG K,TAO D,et al.Image super-resolution with sparse neighbor embedding[J].IEEE Transactions on Image Processing,2012,21(7):3194-3205." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution With Sparse Neighbor Embedding">
                                        <b>[18]</b>
                                        GAO X,ZHANG K,TAO D,et al.Image super-resolution with sparse neighbor embedding[J].IEEE Transactions on Image Processing,2012,21(7):3194-3205.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_19" title="YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">
                                        <b>[19]</b>
                                        YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_20" title="TIMOFTE R,de SMET V,van GOOL L.Anchored neighborhood regression for fast example-based super-resolution[C]//Proceedings of the IEEE 2013 International Conference on Computer Vision.Piscataway:IEEE,2013:1920-1927." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anchored Neighborhood Regression for Fast Example-Based Super Resolution">
                                        <b>[20]</b>
                                        TIMOFTE R,de SMET V,van GOOL L.Anchored neighborhood regression for fast example-based super-resolution[C]//Proceedings of the IEEE 2013 International Conference on Computer Vision.Piscataway:IEEE,2013:1920-1927.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_21" title="ZHAO J,HU H,CAO F.Image super-resolution via adaptive sparse representation[J].Knowledge-Based Systems,2017,124:23-33." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESABF8DEC168DAB95F36127B5DDC1052BC&amp;v=MjM1ODFENS9UdzNuMkdaR2VMS1JSOGpzQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeExtN3hLZz1OaWZPZmNMS2FObTQydnhFWXVON2ZRNHd5bUFRNw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        ZHAO J,HU H,CAO F.Image super-resolution via adaptive sparse representation[J].Knowledge-Based Systems,2017,124:23-33.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_22" title="HUANG K,HU R,JIANG J,et al.Face image super-resolution through improved neighbor embedding[C]//Proceedings of the2016 International Conference on Multimedia Modeling,LNCS9516.Cham:Springer,2016:409-420." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face image super-resolution through improved neighbor embedding">
                                        <b>[22]</b>
                                        HUANG K,HU R,JIANG J,et al.Face image super-resolution through improved neighbor embedding[C]//Proceedings of the2016 International Conference on Multimedia Modeling,LNCS9516.Cham:Springer,2016:409-420.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_23" title="PARK J S,SOH J W,CHO N I.High dynamic range and superresolution imaging from a single image[J].IEEE Access,2018,6:10966-10978." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High dynamic range and superresolution imaging from a single image">
                                        <b>[23]</b>
                                        PARK J S,SOH J W,CHO N I.High dynamic range and superresolution imaging from a single image[J].IEEE Access,2018,6:10966-10978.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_24" title="CHANG H,YEUNG D Y,XIONG Y.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2004:I-I." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution throughneighbor embedding">
                                        <b>[24]</b>
                                        CHANG H,YEUNG D Y,XIONG Y.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2004:I-I.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_25" title="ZHANG K,TAO D,GAO X,et al.Learning multiple linear mappings for efficient single image super-resolution[J].IEEE Transactions on Image Processing,2015,24(3):846-861." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multiple linear mappings for efficient single image super-resolution">
                                        <b>[25]</b>
                                        ZHANG K,TAO D,GAO X,et al.Learning multiple linear mappings for efficient single image super-resolution[J].IEEE Transactions on Image Processing,2015,24(3):846-861.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_26" title="JIANG J,MA X,CHEN C,et al.Single image super-resolution via locally regularized anchored neighborhood regression and nonlocal means[J].IEEE Transactions on Multimedia,2017,19(1):15-26." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single Image Super-Resolution via Locally Regularized Anchored Neighborhood Regression and Nonlocal Means">
                                        <b>[26]</b>
                                        JIANG J,MA X,CHEN C,et al.Single image super-resolution via locally regularized anchored neighborhood regression and nonlocal means[J].IEEE Transactions on Multimedia,2017,19(1):15-26.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_27" title="TIMOFTE R,ROTHE R,van GOOL L.Seven ways to improve example-based single image super resolution[C]//Proceedings of the IEEE 2016 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1865-1873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Seven ways to improv example-based single image super resolution">
                                        <b>[27]</b>
                                        TIMOFTE R,ROTHE R,van GOOL L.Seven ways to improve example-based single image super resolution[C]//Proceedings of the IEEE 2016 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1865-1873.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_28" title="DOU Q,WEI S,YANG X,et al.Medical image super-resolution via minimum error regression model selection using random forest[J].Sustainable Cities and Society,2018,42:1-12." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA99A69BC31C6938895AE94E3E46BEE85&amp;v=MjMxNTlIWWZPR1FsZkJyTFUwNXRwaHhMbTd4S2c9TmlmT2ZjS3hGNkRLcHYwMlorcDhDblU2eHg0YTcwNElRWHVYcjJjeGY4RGhNTEthQ09OdkZTaVdXcjdKSUZwbWFCdQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                        DOU Q,WEI S,YANG X,et al.Medical image super-resolution via minimum error regression model selection using random forest[J].Sustainable Cities and Society,2018,42:1-12.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_29" title="ZEYDE R,ELAD M,PROTTER M.On single image scale-up using sparse-representations[C]//Proceedings of the 2010 International Conference on Curves and Surfaces,LNCS 6920.Berlin:Springer,2010:711-730." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On single image scale-up using sparse representations">
                                        <b>[29]</b>
                                        ZEYDE R,ELAD M,PROTTER M.On single image scale-up using sparse-representations[C]//Proceedings of the 2010 International Conference on Curves and Surfaces,LNCS 6920.Berlin:Springer,2010:711-730.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_30" title="BEVILACQUA M,ROUMY A,GUILLEMOT C,et al.Low-complexity single-image super-resolution based on nonnegative neighbor embedding[EB/OL].[2019-01-12].http://people.rennes.inria.fr/Aline.Roumy/publi/12bmvc_Bevilacqua_lowComplexity SR.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Low-complexity single-image super-resolution based on nonnegative neighbor embedding">
                                        <b>[30]</b>
                                        BEVILACQUA M,ROUMY A,GUILLEMOT C,et al.Low-complexity single-image super-resolution based on nonnegative neighbor embedding[EB/OL].[2019-01-12].http://people.rennes.inria.fr/Aline.Roumy/publi/12bmvc_Bevilacqua_lowComplexity SR.pdf.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_31" title="DONG C,LOY C C,HE K,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution Using Deep Convolutional Networks">
                                        <b>[31]</b>
                                        DONG C,LOY C C,HE K,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-19 09:48</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),3040-3045 DOI:10.11772/j.issn.1001-9081.2019040760            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于自适应锚定邻域回归的图像超分辨率算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E5%8F%8C&amp;code=42897047&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶双</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%99%93%E6%95%8F&amp;code=09800851&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨晓敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%A5%E6%96%8C%E5%AE%87&amp;code=08736216&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">严斌宇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0054367&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川大学电子信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在基于字典的图像超分辨率(SR)算法中,锚定邻域回归超分辨率(ANR)算法由于其优越的重建速度和质量引起了人们的广泛关注。然而,ANR算法的锚定邻域投影并不稳定,以致于不足以涵盖各种样式的映射关系。因此提出一种基于自适应锚定邻域回归的图像SR算法,根据样本分布自适应地计算邻域中心从而以更精确的邻域来预计算投影矩阵。首先,以图像块为中心,运用<i>K</i>均值聚类算法将训练样本聚类成不同的簇;然后,用每个簇的聚类中心替换字典原子来计算相应的邻域;最后,运用这些邻域来预计算从低分辨率(LR)空间到高分辨率(HR)空间的映射矩阵。实验结果表明,所提算法在Set14上平均重建效果以31.56 dB的峰值信噪比(PSNR)及0.871 2的结构相似性(SSIM)优于其他基于字典的先进算法,甚至胜过超分辨率卷积神经网络(SRCNN)算法。同时,在主观表现上看,所提算法恢复出了尖锐的图像边缘且产生的伪影较少。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E9%82%BB%E5%9F%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应邻域;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K均值聚类算法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    叶双(1994—),女,四川自贡人,硕士研究生,主要研究方向:图像的超分辨率、机器学习;;
                                </span>
                                <span>
                                    杨晓敏(1980—),女,四川成都人,副教授,博士,主要研究方向:图像处理、机器学习;;
                                </span>
                                <span>
                                    *严斌宇(1975—),男,四川成都人,副教授,博士,主要研究方向:通信与信息系统。电子邮箱yby@scu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>四川省科学技术厅重点研发项目(2018GZ0178);</span>
                    </p>
            </div>
                    <h1><b>Image super-resolution algorithm based on adaptive anchored neighborhood regression</b></h1>
                    <h2>
                    <span>YE Shuang</span>
                    <span>YANG Xiaomin</span>
                    <span>YAN Bin'yu</span>
            </h2>
                    <h2>
                    <span>College of Electronics and Information Engineering, Sichuan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Among the dictionary-based Super-Resolution(SR) algorithms, the Anchored Neighborhood Regression(ANR) algorithm has been attracted widely attention due to its superior reconstruction speed and quality. However, the anchored neighborhood projections of ANR are unstable to cover varieties of mapping relationships. Aiming at the problem, an image SR algorithm based on adaptive anchored neighborhood regression was proposed, which adaptively calculated the neighborhood center based on the distribution of samples in order to pre-estimate the projection matrix based on more accurate neighborhood. Firstly, <i>K</i>-means clustering algorithm was used to cluster the training samples into different clusters with the image patches as centers. Then, the dictionary atoms were replaced with the cluster centers to calculate the corresponding neighborhoods. Finally, the neighborhoods were applied to pre-compute the projection matrix from LR space to HR space. Experimental results show that the average reconstruction performance of the proposed algorithm on Set14 is better than that of other state-of-the-art dictionary-based algorithms with 31.56 dB of Peak Signal-to-Noise Ratio(PSNR) and 0.871 2 of Structural SIMilarity index(SSIM), and even is superior to the Super-Resolution Convolutional Neural Network(SRCNN) algorithm. At the same time, in terms of the subjective performance, the proposed algorithm produces sharp edges in reconstruction results with little artifacts.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20super-resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image super-resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20neighborhood&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive neighborhood;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3EK%3C%2Fi%3E-means%20clustering%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>K</i>-means clustering algorithm;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YE Shuang,born in 1994,M.S.candidate.Her research interests include images super-resolution,machine learning.;
                                </span>
                                <span>
                                    YANG Xiaomin,born in 1980,Ph.D.,associate professor.Her research interests include images processing,machine learning.;
                                </span>
                                <span>
                                    YAN Bin&amp;apos;yu,born in 1975,Ph.D.,associate professor.His research interests include communication and information system.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-05</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Key Research and Development Project of Sichuan Science and Technology Department(2018GZ0178);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="71" name="71" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="72">图像的超分辨率(Super-Resolution, SR)技术是一种由软件实现的,以提高图像分辨率,丰富图像细节信息为目的的技术。目前图像超分辨率技术已广泛应用于很多领域中,如安全监视<citation id="230" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、刑事调查<citation id="231" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、卫星遥感<citation id="232" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和医学诊断<citation id="233" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等。</p>
                </div>
                <div class="p1">
                    <p id="73">由于高分辨率(High-Resolution, HR)图像可以提供比低分辨率(Low-Resolution, LR)图像更多的细节信息,因此在实际生活中被迫切需要。然而,由于低分辨率图像产生的复杂性,且从低分辨率图像映射到高分辨率图像的过程在大多数情况下都是不可逆的,因此如何由已知的低分辨率图像得到未知的高分辨率图像是一个高度病态求解的问题。近年来,人们针对这一问题广泛研究了许多图像超分辨率算法,这些算法大致可以分为如下三类:基于插值的图像超分辨率算法<citation id="234" type="reference"><link href="176" rel="bibliography" /><link href="178" rel="bibliography" /><link href="180" rel="bibliography" /><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><link href="186" rel="bibliography" /><link href="188" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>、基于重建的图像超分辨率算法<citation id="235" type="reference"><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><link href="194" rel="bibliography" /><link href="196" rel="bibliography" /><link href="198" rel="bibliography" /><link href="200" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>和基于学习的图像超分辨率算法<citation id="236" type="reference"><link href="202" rel="bibliography" /><link href="204" rel="bibliography" /><link href="206" rel="bibliography" /><link href="208" rel="bibliography" /><link href="210" rel="bibliography" /><link href="212" rel="bibliography" /><link href="214" rel="bibliography" /><link href="216" rel="bibliography" /><link href="218" rel="bibliography" /><link href="220" rel="bibliography" /><link href="222" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>,<a class="sup">24</a>,<a class="sup">25</a>,<a class="sup">26</a>,<a class="sup">27</a>,<a class="sup">28</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="74">基于插值的超分辨率算法通常采用某些插值函数来根据已知的低分辨率像素值估计相应的高分辨率像素值。这一类算法不需要借助额外的数据进行重建,且实现非常简单。虽然传统的插值算法如双三次或双线性插值由于其计算复杂度低而被广泛应用,但其恢复出来的高分辨率图像的细节相对模糊。并且,由于这一类算法仅仅只能利用有限的信息来估计未知信息而不能显著增加图像信息,因此随着放大系数的增加,估计出的重建图像效果会越来越差。</p>
                </div>
                <div class="p1">
                    <p id="75">为了能利用更多的图像信息,基于重建的图像超分辨率算法使用了同一场景的多幅低分辨率图像来重建高分辨率图像。与基于插值的算法相比,基于重建的图像超分辨率算法实质上增加了重建中所需的信息,因此也实现了相对基于插值的超分辨率算法更好的结果。但是,该类算法的缺点也很明显:首先,人们难以获得多个在相同场景下的低分辨率图像,这从很大程度上限制了该类算法的应用;其次,基于重建的算法的重建性能通常在一定放大倍数后会受到制约;第三,由于相同场景下不同低分辨率图像间存在像素偏移,因此在进行特征预提取时需要先进行图像配准,而图像配准的过程对一些失真严重的图像来说十分困难。</p>
                </div>
                <div class="p1">
                    <p id="76">基于学习的图像超分辨率算法利用图像的先验知识来重建高分辨率图像。该类算法在学习阶段预先学习好高分辨率图像块与相应的低分辨率图像块之间的映射关系,在重建阶段通过学习到的映射关系生成具有更精细细节的高分辨率图像。为了有效利用图像间的不同假设和先验知识,研究人员先后提出了几种基于学习的图像超分辨率算法:基于邻域嵌入(Neighbor Embedding, NE)的超分辨率算法假设每个输入的低分辨率图像块及其对应的高分辨率图像块位于具有相似局部几何的低维非线性流形上<citation id="237" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>,进而通过流形关系进行重建。基于局部线性嵌入(Locally Linear Embedding, LLE)的超分辨率算法假设不同放大尺寸的图像块共享相似的流形几何结构,因此高分辨率图像块可以通过流形关系所匹配到的低分辨率图像块来进行重建<citation id="238" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。然而,流形假设往往在实践中并不精确,因此重建结果通常并不理想。为了克服这个缺点,Yang等<citation id="239" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了一种基于稀疏编码(Sparse Coding,SC)的图像超分辨率算法,通过学习到的高低分辨率词典的稀疏系数来构建高低分辨率空间中图像块之间的对应关系,进而重建出高分辨率图像。基于稀疏编码的图像超分辨率算法主要瓶颈是其较高的计算复杂度,目前,研究人员针对这一问题提出了许多降低计算复杂度的解决方案<citation id="242" type="reference"><link href="206" rel="bibliography" /><link href="208" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>。文献<citation id="240" type="reference">[<a class="sup">29</a>]</citation>将改进的<i>K</i>-SVD(<i>K</i>-Single Value Decomposition)和OMP(Orthogonal Matching Pursuit)算法引入基于稀疏编码的图像超分辨率算法中,在维持一定重建性能的同时,对训练词典的过程以及重建过程进行加速。随后,Timofte等<citation id="241" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出了锚定邻域回归(Anchored Neighbor Regression, ANR)算法,由于从低分辨率到高分辨率图像的锚定线性回归量(投影矩阵)可以离线计算,一旦学习了映射关系,就可以实时重建高分辨率图像。因此,ANR算法在取得优越的重建效果的同时也实现了极快的重建速度。ANR算法仅关注于从过完备字典中实现锚定邻域的投影,然而,一组过完备字典并不能覆盖各种形式的映射关系,因此在一定程度上限制了ANR算法的应用。</p>
                </div>
                <div class="p1">
                    <p id="77">为解决ANR算法中邻域不精确问题,本文提出了一种基于自适应锚定邻域回归的图像超分辨率算法,根据样本分布来自适应地选择邻域。本文算法的大致流程简述如下:首先,模糊并下采样高分辨率图像块以得到高、低分辨率训练样本对;然后,应用<i>K</i>均值聚类算法将高、低分辨率图像块分成不同的簇组,再由各簇组的聚类中心和高低分辨率字典原子的关系来构建邻域;最后,将这些邻域用于预计算精确的锚邻域投影(回归量),并用于重建高分辨率图像。本文主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="78">1)使用<i>K</i>均值聚类算法将图像块聚类成几个不同的簇来重新分配样本空间,然后用聚类中心替代ANR算法中的字典作为锚点来计算邻域。</p>
                </div>
                <div class="p1">
                    <p id="79">2)用欧氏距离来重新计算每个聚类中心的邻域,然后将这些邻域应用于计算离线存储的投影矩阵(回归量)。由于聚类中心是根据样本分布形成的,因此本文算法可以实现邻域的自适应。</p>
                </div>
                <h3 id="80" name="80" class="anchor-tag">1 锚定邻域回归算法</h3>
                <div class="p1">
                    <p id="81">ANR算法的核心是假设字典中的每个原子都可以由其最近的邻域线性表示,因此,相同的表示系数可用来重建其高分辨率图像。ANR算法使用与文献<citation id="243" type="reference">[<a class="sup">29</a>]</citation>中相同的一组外部字典,且称低分辨率稀疏词典原子为锚点。然后,使用式(1)从一组学习字典中稀疏地重建高分辨率图像:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>h</mi></msub><mo>,</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>h</mi></msub><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>l</mi></msub><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi mathvariant="bold-italic">φ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ζ</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中:<i><b>D</b></i><sub>l</sub>和<i><b>D</b></i><sub>h</sub>是联合学习的稀疏字典;<i><b>Y</b></i>和<i><b>X</b></i>是待训练的高低分辨率图像块对;<i><b>Z</b></i>为稀疏表示系数;<i>φ</i>为加权因子。字典<i><b>D</b></i>大小固定,不会随着训练数据库的增加而产生很大变化。</p>
                </div>
                <div class="p1">
                    <p id="84">ANR算法首先将超分辨率问题定义为一种岭回归问题,进而由岭回归的闭式解可以求解出低分辨率图像邻域与高分辨率图像邻域之间的相互关系,进而重建出高分辨率图像。具体来说,对于每个字典原子,ANR算法首先计算离其最近的<i>K</i>样本图像块作为其邻域。然后,通过优化低分辨率图像块来获得相应的高分辨率表示系数。上述过程可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">Ν</mi><msub><mrow></mrow><mtext>l</mtext></msub><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi mathvariant="bold-italic">λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中:<i><b>N</b></i><sub>l</sub>表示根据样本图像块计算得到的邻域,其对应于邻域嵌入中的输入图像块<i><b>Y</b></i>的<i>K</i>个最近邻,以及稀疏编码和全局回归(Global Regression, GR)<citation id="244" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>中的低分辨率字典<i><b>D</b></i><sub>l</sub>;<i>λ</i>为正则项;<i>α</i>是用于重建高分辨率图像块的系数向量。式(2)的闭式解为:</p>
                </div>
                <div class="p1">
                    <p id="87"><i>α</i>=(<i><b>N</b></i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><i><b>N</b></i><sub><i>l</i></sub>+<i>λ</i><i><b>I</b></i>)<sup>-1</sup><i><b>N</b></i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><i><b>Y</b></i>      (3)</p>
                </div>
                <div class="p1">
                    <p id="88">求解得到的系数<i>α</i>可进一步用来计算高分辨率图像块:</p>
                </div>
                <div class="p1">
                    <p id="89"><i><b>X</b></i>=<i><b>N</b></i><sub>h</sub><i>α</i>      (4)</p>
                </div>
                <div class="p1">
                    <p id="90">其中:<i><b>N</b></i><sub>h</sub>表示对应于<i><b>N</b></i><sub>l</sub>的高分辨率图像块邻域。由式(3)～(4)可以归纳出重建阶段所使用的投影矩阵。由于投影矩阵与重建过程中的低分辨率图像无关,因此投影矩阵可以离线计算:</p>
                </div>
                <div class="p1">
                    <p id="91"><i><b>P</b></i><sub><i>G</i></sub>=<i><b>N</b></i><sub><i>h</i></sub>(<i><b>N</b></i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><i><b>N</b></i><sub><i>l</i></sub>+<i>λ</i><i><b>I</b></i>)<sup>-1</sup><i><b>N</b></i><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="92">整个ANR算法的重建过程只需通过计算一组矩阵乘法就可以得到解决:将预先计算好的回归量<i><b>P</b></i><sub>G</sub>与低分辨率输入图像块<i><b>Y</b></i>相乘,就可以得到其对应的高分辨率图像块<i><b>X</b></i>。</p>
                </div>
                <div class="p1">
                    <p id="93">根据上述推导,ANR算法可以分别从训练阶段和重建阶段来确定锚点。在训练阶段,ANR算法将每个字典原子作为划分邻域的中心,使用余弦距离计算离字典原子(即划分邻域的中心)最近的样本来作为其邻域。在重建阶段,对于输入的低分辨率样本块,ANR算法根据相关性匹配出与其最相关的字典原子,并使用该原子存储的投影矩阵来生成高分辨率图像块。然而,由于字典原子并不能准确地由其固定尺寸的邻域来进行表示,导致样本空间的不精确划分。因此,本文提出了一种自适应方式的算法来确定邻域并用自适应邻域重建高分辨率图像。具体地,对于每个训练样本块,计算离它最近的<i>K</i>个图像块作为其邻域,并且将每个图像块本身作为其邻域分组的中心。</p>
                </div>
                <h3 id="94" name="94" class="anchor-tag">2 本文算法</h3>
                <div class="p1">
                    <p id="95">本文算法包含四个关键步骤:1)选择<i>Yang</i>算法<citation id="245" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>中用于训练稀疏字典对的样本作为本文的训练样本,并根据样本的自相似性将训练样本聚类成若干簇;2)对于每个簇的聚类中心,找到离其最近的K个图像块作为其邻域;3)使用目标邻域离线计算投影矩阵;4)利用预先计算的投影矩阵重建高分辨率图像块。本文框架如图1所示。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910041_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法框架" src="Detail/GetImg?filename=images/JSJY201910041_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910041_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.1 <i>Framework of the proposed algorithm</i></p>

                </div>
                <h4 class="anchor-tag" id="97" name="97">2.1 <b>训练样本的选择</b></h4>
                <div class="p1">
                    <p id="98">本文选取<i>Yang</i>算法<citation id="246" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>中用于训练稀疏字典的训练样本来进行聚类,由于本文使用聚类中心来替换<i>ANR</i>算法中的字典原子作为中心进而来计算邻域,因此在本文算法中不需要用到字典。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">2.2 <b>图像块聚类</b></h4>
                <div class="p1">
                    <p id="100">与<i>SC</i>和<i>GR</i>方法相同,本文选择整个字典来获取最终的高分辨率图像块。然而,原子周围的局部流形更能由内部的密集样品而非一组外部临近原子来表示。因此,用聚类中心的邻域替换整个词典来重建高分辨率图像块是很有必要的。特别地,本文将训练样本图像块聚类成不同的簇,用聚类中心作为邻域中心来重新分割样本空间。具体而言,对于第<i>i</i>个聚类中心<i>m</i><sub><i>i</i></sub>,本文选择离其最近的<i>K</i>个图像块作为其邻域<i><b>N</b></i><sub><i>il</i></sub>,相应地,对应的高分辨率邻域<i><b>N</b></i><sub><i>ih</i></sub>也可以获得。低分辨率邻域<i><b>N</b></i><sub><i>il</i></sub>及其高分辨率邻域<i><b>N</b></i><sub><i>ih</i></sub>构成第<i>i</i>个聚类对,并代表低、高分辨率邻域之间的对应关系。由于本文算法是根据训练样本的分布自适应地获得聚类中心,因此可以更精确地划分样本空间,从而提升重建效果。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">2.3 <b>投影矩阵计算</b></h4>
                <div class="p1">
                    <p id="102">如第1章所述,ANR算法在重建速度方面取得了显着成就,因此,在本文算法中将继续沿用这一优势。对于第<i>i</i>个低分辨率聚类中心<i>m</i><sub><i>i</i></sub>,本文使用式(6)计算投影矩阵<i><b>P</b></i><sub><i>i</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="103"><i><b>P</b></i><sub><i>i</i></sub>=<i><b>N</b></i><sub><i>ih</i></sub>+<i><b>N</b></i><sub><i>il</i></sub>)<sup>T</sup><i><b>N</b></i><sub><i>il</i></sub>+<i>λ</i><i><b>I</b></i><sup>-1</sup><i><b>N</b></i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow><mtext>Τ</mtext></msubsup></mrow></math></mathml>      (6)</p>
                </div>
                <div class="p1">
                    <p id="104">其中:<i><b>N</b></i><sub><i>il</i></sub>表示第<i>i</i>个聚类中心在低分辨率空间的邻域;<i><b>N</b></i><sub><i>ih</i></sub>是其对应在高分辨率空间的邻域。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">2.4 <b>高分辨率图像块重建</b></h4>
                <div class="p1">
                    <p id="106">在重建阶段,对于给定的低分辨率图像块<i><b>y</b></i><sub><i>i</i></sub>,在训练样本库中选择离其最近的聚类中心<i>m</i><sub><i>i</i></sub>并计算相应的投影矩阵<i><b>P</b></i><sub><i>i</i></sub>。然后,待重建的高分辨率图像块<i><b>x</b></i><sub><i>i</i></sub>可以通过与ANR算法相同的方式获得,<i><b>x</b></i><sub><i>i</i></sub>= <i><b>y</b></i><sub><i>i</i></sub><i><b>P</b></i><sub><i>i</i></sub>。因为已经在训练阶段获得并存储了所有锚点及其投影矩阵,所以超分辨率过程的计算成本可以得到显著降低。</p>
                </div>
                <div class="p1">
                    <p id="107">本文算法通过用聚类中心替换ANR算法中的字典原子作为锚点来获得邻域,并且基于聚类中心来搜索适当的邻域大小以获得最终精确的高分辨率图像块。邻域大小(数量)对重建效果的影响将通过实验进行分析讨论。简而言之,本文算法能自适应地选择基于样本的邻域,更合理地选取锚点从而更精确地表示图像,因此可以获得优于其他先进算法的重建效果。</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">3 实验与分析</h3>
                <h4 class="anchor-tag" id="109" name="109">3.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="110">在本节中,通过与先进算法进行比较来验证所提出的图像超分辨率算法的有效性。为公平比较,本文选择与<i>Yang</i>算法<citation id="247" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和<i>Zeyde</i>算法<citation id="248" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>相同的训练集,该训练集包含91张高分辨率图像。本文使用下采样系数×3对高分辨率图像进行下采样,并且通过双三次插值生成具有相同大小的低分辨率图像以构建高、低分辨率字典对,该字典对应用于所有参与比较的基于稀疏编码的超分辨率算法。本文选择了两个常见的数据集<i>Set</i>5<citation id="249" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>和<i>Set</i>14<citation id="250" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>用来测试,分别包含5和14张<i>RGB</i>图像。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">3.2 <b>参数设置</b></h4>
                <div class="p1">
                    <p id="112">本文的参数沿用<i>ANR</i>算法中的设置:锚点数为1 024,低分辨率和高分辨率图像的训练样本数为500 000,放大因子为3,归一化因子<i>λ</i>为0.1。此外,本文将聚类簇数量设置为1024。所有实验都在具有Intel Core i5-6400和8 GB RAM的Windows 7(64位)上进行。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113">3.3 <b>参数灵敏度</b></h4>
                <h4 class="anchor-tag" id="114" name="114">3.3.1 聚类数量对重建效果的影响</h4>
                <div class="p1">
                    <p id="115">本文设置了不同的聚类数量来进行广泛的实验,以此来验证本文算法在不同簇数下的性能。图2显示了不同聚类数量下本文算法在数据集<i>Set</i>5上的平均峰值信噪比(<i>Peak Signal</i>-<i>to</i>-<i>Noise Ratio</i>, <i>PSNR</i>)值。从图2可以看出:当放大簇数直到值达到峰值256时,<i>PSNR</i>值增加,当簇数超过256后<i>PSNR</i>值开始减小。因此,对本文算法256为最佳聚类数量。为了公平地与其他先进方法算法进行比较,本文在实验中沿用<i>ANR</i>算法中的字典大小1 024作为聚类数量。越大的聚类数量可以涵盖更多图像隐含的映射关系,进而可以越精确地对重建图像块进行越精细的表征。但是,在一定的训练图像规模下,随着聚类数量的增多,每一簇所含的样本量会逐渐地减少,进而在重建时对图像的表征不是很完全而导致重建性能的降低。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 聚类数量对重建效果的影响" src="Detail/GetImg?filename=images/JSJY201910041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 聚类数量对重建效果的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.2 <i>Effect of the number of clusters on reconstruction performance</i></p>

                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表</b>1 <b>数据集</b><i>Set</i>5<b>中每幅图像的</b><i>PSNR</i><b>和</b><i>SSIM</i><b>在放大倍数为</b>3<b>时的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.1 <i>PSNR and SSIM of each image in Set</i>5 <i>dataset with magnification of</i> 3</p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />图像</td><td><i>Baby</i></td><td><i>Bird</i></td><td><i>Butterfly</i></td><td><i>Head</i></td><td><i>Woman</i></td><td>均值</td></tr><tr><td><br /><i>Bicubic</i></td><td>33.90(0.903 7)</td><td>32.62(0.926 2)</td><td>24.04(0.821 4)</td><td>32.88(0.800 2)</td><td>28.57(0.889 5)</td><td>30.40(0.868 2)</td></tr><tr><td><br /><i>Yang</i></td><td>34.29(0.904 2)</td><td>34.18(0.939 8)</td><td>25.68(0.861 5)</td><td>33.18(0.802 3)</td><td>29.98(0.904 0)</td><td>31.46(0.882 4)</td></tr><tr><td><br /><i>Zeyde</i></td><td>35.07(0.920 8)</td><td>34.62(0.948 1)</td><td>25.96(0.876 9)</td><td>33.56(0.820 3)</td><td>30.37(0.917 5)</td><td>31.92(0.896 7)</td></tr><tr><td><br /><i>NE</i>+<i>LS</i></td><td>34.94(0.919 2)</td><td>34.42(0.946 5)</td><td>25.85(0.876 0)</td><td>33.53(0.819 9)</td><td>30.21(0.916 3)</td><td>31.79(0.895 6)</td></tr><tr><td><br /><i>NE</i>+<i>NNLS</i></td><td>34.76(0.916 7)</td><td>34.31(0.943 7)</td><td>25.64(0.866 4)</td><td>33.46(0.818 2)</td><td>29.90(0.910 8)</td><td>31.61(0.891 2)</td></tr><tr><td><br /><i>NE</i>+<i>LLE</i></td><td>35.05(0.920 7)</td><td>34.62(0.948 1)</td><td>25.77(0.870 4)</td><td>33.60(0.822 6)</td><td>30.22(0.916 0)</td><td>31.85(0.895 6)</td></tr><tr><td><br /><i>GR</i></td><td>34.92(0.921 3)</td><td>34.00(0.941 5)</td><td>25.06(0.831 8)</td><td>33.49(0.822 4)</td><td>29.76(0.905 2)</td><td>31.45(0.884 4)</td></tr><tr><td><br /><i>ANR</i></td><td><b>35.13(0.922</b><b>3</b>)</td><td>34.66(0.949 3)</td><td>25.92(0.871 6)</td><td><u>33.64(0.823 9)</u></td><td>30.33(0.916 7)</td><td>31.94(0.896 8)</td></tr><tr><td><br />SRCNN</td><td><u>35.01(0.921 0)</u></td><td><u>34.90(0.949 4)</u></td><td><b>27.58(0.901</b><b>2</b>)</td><td>33.55(0.821 5)</td><td><u>30.91(0.923 6)</u></td><td><b>32.39(0.903</b><b>3</b>)</td></tr><tr><td><br />本文算法</td><td>34.99(0.920 4)</td><td><b>35.23(0.951</b><b>9</b>)</td><td><u>26.89(0.900 8)</u></td><td><b>33.71(0.825</b><b>2)</b></td><td><b>30.99(0.925</b><b>7</b>)</td><td><u>32.36(0.904 8)</u></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:A(B)表示PSNR(SSIM),PSNR的单位为dB;加粗数值表示各算法中最优客观指标值;带下划线数值表示各算法中次优客观指标值。</p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">3.3.2 邻域数量对重建效果的影响</h4>
                <div class="p1">
                    <p id="119">由之前的陈述可知,从理论上讲,邻域数量越多重建高分辨率图像的结果就越准确,因此,随着邻域数量的增加,将获得更高质量的高分辨率图像。在训练阶段,本文通过设置不同的邻域数量来证明这一假设,然后分析所有测试图像的PSNR,实验结果如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="120">从图3所示的曲线中可以清楚地看到,当邻域数量从2增加到256时,PSNR值大致上显着提升,代表重建质量的显著提高。然而,当邻域大小从256继续增加到4 096时,PSNR开始急剧下降,这意味着重建质量正在下降。因此,对于簇数设置为1 024而得到的每个聚类中心,本文为其选择最佳邻域大小256,并用于计算式(6)中的投影矩阵。这一实验说明越多的邻域数量可以对待估计的高分辨率图像块进行更精确的表征,而由于训练样本有限,在邻域数量达到一定的量后,可能由于先前并不属于当前邻域的无关样本参与到了重建过程中,因此重建性能会稍有下滑。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910041_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 邻域数量对重建效果的影响" src="Detail/GetImg?filename=images/JSJY201910041_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 邻域数量对重建效果的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910041_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Effect of the number of neighborhoods on reconstruction performance</p>

                </div>
                <h4 class="anchor-tag" id="122" name="122">3.4 <b>与先进算法的比较</b></h4>
                <div class="p1">
                    <p id="123">对比算法包括Bicubic、Yang算法<citation id="251" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、Zeyde算法<citation id="252" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>、GR(Global Regression)<citation id="253" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、ANR<citation id="254" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、NE+NNLS(Neighbor Embedding with Non-negative Least Squares)<citation id="255" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>、NE+LLE(Neighbor Embedding with Locally Linear Embedding)<citation id="256" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>和超分辨率卷积神经网络(Super-Resolution Convolutional Neural Network, SRCNN)<citation id="257" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>。本文使用无约束最小二乘法来求解NE+LS(Neighbor Embedding with Least Squares)<citation id="258" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>回归。为了处理彩色图像,本文将训练集转换到YCbCr颜色空间,超分辨率处理仅在灰度空间Y上执行,颜色空间Cb和Cr将直接进行双三次插值。图4展示了本文算法和对比算法之间的运行时间对比。从图4中可以看出,本文算法可以在较短的运行时间内获得最佳的重建性能,表明了本文算法的实用性。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 与先进算法在速度上的比较" src="Detail/GetImg?filename=images/JSJY201910041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 与先进算法在速度上的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Speed comparison with state-of-the-art algorithms</p>

                </div>
                <div class="p1">
                    <p id="125">此外,本文分别通过客观和主观手段,在两个常见图像数据集Set5和Set14上验证了所提算法的有效性,其客观定量结果显示在表1～2中。</p>
                </div>
                <div class="p1">
                    <p id="126">从表1～2可以看出,本文算法在PSNR和结构相似性(Structure SIMilarity index, SSIM)上都明显优于对比算法,甚至在部分图像的客观指标比较中优于近期火热的基于深度学习的图像超分辨率算法SRCNN。</p>
                </div>
                <div class="p1">
                    <p id="127">主观视觉结果如图5所示,从中可以看出:Yang算法<citation id="259" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和Zeyde算法<citation id="260" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>获得的重建结果的边缘和纹理过度平滑,NE+LS和NE+NNLS由于其不准确的邻域引入了一些伪像。由于本文算法的邻域选择更精确,因此可以有效地避免模糊伪影,从而可以获得更加真实的高分辨率图像。与其他现有基于学习的技术相比,本文算法显示出更好的定性和定量结果,同时保持相对快速的重建速度。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910041_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同图像在放大倍数为3时的视觉定性评估" src="Detail/GetImg?filename=images/JSJY201910041_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同图像在放大倍数为3时的视觉定性评估  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910041_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Visual qualitative assessment of different images with magnification=3</p>

                </div>
                <div class="area_img" id="129">
                    <p class="img_tit"><b>表</b>2 <b>数据集</b>Set14<b>中部分图像的</b>PSNR<b>和</b>SSIM<b>在放大倍数为</b>3<b>时的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.2 PSNR and SSIM of part images in Set14 dataset with magnification of 3</p>
                    <p class="img_note"></p>
                    <table id="129" border="1"><tr><td><br />图像</td><td>Comic</td><td>Face</td><td>Flowers</td><td>Foreman</td><td>Lena</td><td>Monarch</td><td>Pepper</td><td>均值</td></tr><tr><td><br />Bicubic  </td><td>23.14(0.699 9)</td><td>32.82(0.798 3)</td><td>27.27(0.802 5)</td><td>31.20(0.905 7)</td><td>31.68(0.857 9)</td><td>29.43(0.919 4)</td><td>32.47(0.871 8)</td><td>29.72(0.836 5)</td></tr><tr><td><br />Yang  </td><td>23.96(0.757 6)</td><td>33.12(0.801 0)</td><td>28.33(0.831 0)</td><td>32.06(0.912 9)</td><td>32.66(0.864 6)</td><td>30.80(0.928 8)</td><td>33.43(0.868 8)</td><td>30.62(0.852 1)</td></tr><tr><td><br />Zeyde  </td><td>23.99(0.756 1)</td><td>33.53(0.819 5)</td><td>28.48(0.838 6)</td><td>33.24(0.929 1)</td><td>33.00(0.877 8)</td><td>31.12(0.937 9)</td><td>34.15(0.887 9)</td><td>31.07(0.863 8)</td></tr><tr><td><br />NE+LS  </td><td>23.95(0.754 2)</td><td>33.50(0.818 6)</td><td>28.40(0.836 3)</td><td>33.25(0.929 5)</td><td>32.98(0.877 4)</td><td>30.96(0.936 2)</td><td>33.99(0.886 4)</td><td>31.00(0.862 7)</td></tr><tr><td><br />NE+NNLS  </td><td>23.87(0.750 1)</td><td>33.46(0.817 9)</td><td>28.27(0.833 2)</td><td>32.92(0.924 1)</td><td>32.82(0.875 1)</td><td>30.78(0.933 5)</td><td>33.65(0.883 0)</td><td>30.82(0.859 6)</td></tr><tr><td><br />NE+LLE  </td><td>24.01(0.758 4)</td><td>33.57(0.821 5)</td><td>28.44(0.838 8)</td><td>33.26(0.929 2)</td><td>33.02(0.878 7)</td><td>30.96(0.935 9)</td><td>33.89(0.886 1)</td><td>31.02(0.864 0)</td></tr><tr><td><br />GR  </td><td>23.85(0.750 2)</td><td>33.46(0.821 9)</td><td>28.19(0.833 3)</td><td>32.34(0.916 4)</td><td>32.67(0.875 4)</td><td>30.47(0.928 7)</td><td>33.29(0.880 1)</td><td>30.61(0.858 0)</td></tr><tr><td><br />ANR  </td><td>24.08(0.762 1)</td><td><u>33.62(0.823 2)</u></td><td>28.55(0.841 3)</td><td>33.28(0.930 0)</td><td>33.09(0.880 1)</td><td>31.11(0.937 4)</td><td>33.91(0.886 8)</td><td>31.09(0.865 8)</td></tr><tr><td><br />SRCNN  </td><td><b>24.39(0.777</b><b>8</b>)</td><td>33.58(0.821 4)</td><td><b>28.97(0.847</b><b>5</b>)</td><td><u>33.40(0.932 1)</u></td><td><b>33.39(0.882</b><b>7)</b></td><td><b>32.39(0.945</b><b>0</b>)</td><td><u>34.34(0.887 6)</u></td><td><u>31.49(0.870 6)</u></td></tr><tr><td><br />本文算法 </td><td><u>24.27(0.771 8)</u></td><td><b>33.68(0.824</b><b>6</b>)</td><td><u>28.89(0.847 3)</u></td><td><b>34.20(0.937</b><b>6</b>)</td><td><u>33.37(0.882 0)</u></td><td><u>31.89(0.943 5)</u></td><td><b>34.62(0.891</b><b>3)</b></td><td><b>31.56(0.871</b><b>2</b>)</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:A(B)表示PSNR(SSIM),PSNR的单位为dB;加粗数值表示各算法中最优客观指标值;带下划线数值表示各算法中次优客观指标值。</p>
                    <p class="img_note"></p>
                </div>
                <h3 id="130" name="130" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="131">本文提出了一种基于自适应邻域的锚定邻域回归图像超分辨率算法。本文以ANR算法为基础,从如下几个方面进行了改进:首先,本文使用<i>K</i>均值聚类算法将训练样本聚类成以图像块为中心的簇来重新划分样本空间。此外,使用欧氏距离重新计算基于聚类中心而不是字典原子的邻域。由于用于计算邻域的回归量可以在训练过程中离线存储,因此可以大幅降低计算成本;并且,由于邻域中心是根据样本分布的自适应获得的,因此可以实现邻域的自适应性。实验结果表明,本文算法不仅在客观和主观评价上优于其他先进算法,同时在重建速度上也具有一定的竞争力。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="168">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SRLSP:A Face Image Super-Resolution Algorithm Using Smooth Regression With Local Structure Prior">

                                <b>[1]</b>JIANG J,CHEN C,MA J,et al.SRLSP:a face image super-resolution algorithm using smooth regression with local structure prior[J].IEEE Transactions on Multimedia,2016,19(1):27-40.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The process of criminal investigation based on grey hazy set">

                                <b>[2]</b>WU L,WANG Y.The process of criminal investigation based on grey hazy set[C]//Proceedings of the 2010 IEEE International Conference on Systems,Man and Cybernetics.Piscataway:IEEE,2010:26-28.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image reconstruction and enhanced resolution imaging from irregular samples">

                                <b>[3]</b>EARLY D S,LONG D G.Image reconstruction and enhanced resolution imaging from irregular samples[J].IEEE Transactions on Geoscience and Remote Sensing,2001,39(2):291-302.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution in medical imaging">

                                <b>[4]</b>GREENSPAN H.Super-resolution in medical imaging[J].The Computer Journal,2009,52(1):43-63.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contrast-guided image interpolation">

                                <b>[5]</b>WEI Z,MA K.Contrast-guided image interpolation[J].IEEETransactions on Image Processing,2013,22(11):4271-4285.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Isophote-Constrained autoregressive model with adaptive window extension for image interpolation">

                                <b>[6]</b>YANG W,LIU J,LI M,et al.Isophote-constrained autoregressive model with adaptive window extension for image interpolation[J].IEEETransactions on Circuits and Systems for Video Technology,2018,28(5):1071-1086.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single image interpolation via adaptive non-local sparsity-based modeling">

                                <b>[7]</b>ROMANO Y,PROTTER M,ELAD M.Single image interpolation via adaptive nonlocal sparsity-based modeling[J].IEEE Transactions on Image Processing,2014,23(7):3085-3098.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional edge diffusion for fast contrast-guided image interpolation">

                                <b>[8]</b>YE W,MA K.Convolutional edge diffusion for fast contrast-guided image interpolation[J].IEEE Signal Processing Letters,2016,23(9):1260-1264.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Interpolation via Low-Rank Matrix Completion and Recovery">

                                <b>[9]</b>CAO F,CAI M,TAN Y.Image interpolation via low-rank matrix completion and recovery[J].IEEE Transactions on Circuits and Systems for Video Technology,2015,25(8):1261-1270.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Image interpolation via Random Forests">

                                <b>[10]</b>HUANG J,SIU W,LIU T.Fast image interpolation via random forests[J].IEEE Transactions on Image Processing,2015,24(10):3232-3245.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image interpolation based on non-local geometric similarities and directional gradients">

                                <b>[11]</b>ZHU S,ZENG B,ZENG L,et al.Image interpolation based on non-local geometric similarities and directional gradients[J].IEEE Transactions on Multimedia,2016,18(9):1707-1719.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003464871&amp;v=MjIxMDJrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXJsVTc3TUpGND1OajdCYXJPNEh0SFBxNGxCYk93T1kz&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>MARQUINA A,OSHER S J.Image super-resolution by TV-regularization and Bregman iteration[J].Journal of Scientific Computing,2008,37(3):367-382.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution via transform-invariant groupsparse regularization">

                                <b>[13]</b>FERNANDEZ-GRANDA C,CANDS E J.Super-resolution via transform-invariant group-sparse regularization[C]//Proceedings of the 2013 IEEE International Conference on Computer Vision.Piscataway:IEEE,2013:3336-3343.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization">

                                <b>[14]</b>DONG W,ZHANG L,SHI G,et al.Image deblurring and superresolution by adaptive sparse domain selection and adaptive regularization[J].IEEE Transactions on Image Processing,2011,20(7):1838-1857.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nonlocally Centralized Sparse Representation for Image Restoration">

                                <b>[15]</b>DONG W,ZHANG L,SHI G,et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing,2013,22(4):1620-1630.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using gradient profile prior">

                                <b>[16]</b>SUN J,XU Z,SHUM H Y.Image super-resolution using gradient profile prior[C]//Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2008:1-8.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Edge-Directed Single Image Super-Resolution via Adaptive Gradient Magnitude Self-Interpolation">

                                <b>[17]</b>WANG L,XIANG S,MENG G,et al.Edge-directed single-image super-resolution via adaptive gradient magnitude self-interpolation[J].IEEE Transactions on Circuits and Systems for Video Technology,2013,23(8):1289-1299.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution With Sparse Neighbor Embedding">

                                <b>[18]</b>GAO X,ZHANG K,TAO D,et al.Image super-resolution with sparse neighbor embedding[J].IEEE Transactions on Image Processing,2012,21(7):3194-3205.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">

                                <b>[19]</b>YANG J,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anchored Neighborhood Regression for Fast Example-Based Super Resolution">

                                <b>[20]</b>TIMOFTE R,de SMET V,van GOOL L.Anchored neighborhood regression for fast example-based super-resolution[C]//Proceedings of the IEEE 2013 International Conference on Computer Vision.Piscataway:IEEE,2013:1920-1927.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESABF8DEC168DAB95F36127B5DDC1052BC&amp;v=MDcwNzhtN3hLZz1OaWZPZmNMS2FObTQydnhFWXVON2ZRNHd5bUFRN0Q1L1R3M24yR1pHZUxLUlI4anNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>ZHAO J,HU H,CAO F.Image super-resolution via adaptive sparse representation[J].Knowledge-Based Systems,2017,124:23-33.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face image super-resolution through improved neighbor embedding">

                                <b>[22]</b>HUANG K,HU R,JIANG J,et al.Face image super-resolution through improved neighbor embedding[C]//Proceedings of the2016 International Conference on Multimedia Modeling,LNCS9516.Cham:Springer,2016:409-420.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High dynamic range and superresolution imaging from a single image">

                                <b>[23]</b>PARK J S,SOH J W,CHO N I.High dynamic range and superresolution imaging from a single image[J].IEEE Access,2018,6:10966-10978.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution throughneighbor embedding">

                                <b>[24]</b>CHANG H,YEUNG D Y,XIONG Y.Super-resolution through neighbor embedding[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2004:I-I.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multiple linear mappings for efficient single image super-resolution">

                                <b>[25]</b>ZHANG K,TAO D,GAO X,et al.Learning multiple linear mappings for efficient single image super-resolution[J].IEEE Transactions on Image Processing,2015,24(3):846-861.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single Image Super-Resolution via Locally Regularized Anchored Neighborhood Regression and Nonlocal Means">

                                <b>[26]</b>JIANG J,MA X,CHEN C,et al.Single image super-resolution via locally regularized anchored neighborhood regression and nonlocal means[J].IEEE Transactions on Multimedia,2017,19(1):15-26.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Seven ways to improv example-based single image super resolution">

                                <b>[27]</b>TIMOFTE R,ROTHE R,van GOOL L.Seven ways to improve example-based single image super resolution[C]//Proceedings of the IEEE 2016 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:1865-1873.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA99A69BC31C6938895AE94E3E46BEE85&amp;v=MDIwMTVoeExtN3hLZz1OaWZPZmNLeEY2REtwdjAyWitwOENuVTZ4eDRhNzA0SVFYdVhyMmN4ZjhEaE1MS2FDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b>DOU Q,WEI S,YANG X,et al.Medical image super-resolution via minimum error regression model selection using random forest[J].Sustainable Cities and Society,2018,42:1-12.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On single image scale-up using sparse representations">

                                <b>[29]</b>ZEYDE R,ELAD M,PROTTER M.On single image scale-up using sparse-representations[C]//Proceedings of the 2010 International Conference on Curves and Surfaces,LNCS 6920.Berlin:Springer,2010:711-730.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Low-complexity single-image super-resolution based on nonnegative neighbor embedding">

                                <b>[30]</b>BEVILACQUA M,ROUMY A,GUILLEMOT C,et al.Low-complexity single-image super-resolution based on nonnegative neighbor embedding[EB/OL].[2019-01-12].http://people.rennes.inria.fr/Aline.Roumy/publi/12bmvc_Bevilacqua_lowComplexity SR.pdf.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution Using Deep Convolutional Networks">

                                <b>[31]</b>DONG C,LOY C C,HE K,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910041" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910041&amp;v=MDA5NDl1WnNGeWprVjd2SUx6N0JkN0c0SDlqTnI0OUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
