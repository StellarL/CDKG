<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637138991812916250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201903013%26RESULT%3d1%26SIGN%3dMr2LgMmcVBUSC5qFL9Zj7%252b%252bi9Ao%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201903013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903013&amp;v=MTc1ODU3RzRIOWpNckk5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3JJTHo3QmQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="1 视觉定位检测系统模型设计 ">1 视觉定位检测系统模型设计</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="2 行人检测网络 ">2 行人检测网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="2.1 &lt;b&gt;锚点框选取&lt;/b&gt;">2.1 <b>锚点框选取</b></a></li>
                                                <li><a href="#67" data-title="2.2 &lt;b&gt;网络结构&lt;/b&gt;">2.2 <b>网络结构</b></a></li>
                                                <li><a href="#71" data-title="2.3 &lt;b&gt;损失函数&lt;/b&gt;">2.3 <b>损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="3 基于行人检测的实时测距方法 ">3 基于行人检测的实时测距方法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="4 实验 ">4 实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="4.1 &lt;b&gt;数据集扩增&lt;/b&gt;">4.1 <b>数据集扩增</b></a></li>
                                                <li><a href="#108" data-title="4.2 &lt;b&gt;井下巷道行人检测实验&lt;/b&gt;">4.2 <b>井下巷道行人检测实验</b></a></li>
                                                <li><a href="#134" data-title="4.3 &lt;b&gt;测距实验结果&lt;/b&gt;">4.3 <b>测距实验结果</b></a></li>
                                                <li><a href="#144" data-title="4.4 &lt;b&gt;实验结果展示&lt;/b&gt;">4.4 <b>实验结果展示</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#147" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="图1 视觉定位示意图">图1 视觉定位示意图</a></li>
                                                <li><a href="#43" data-title="图2 系统模型">图2 系统模型</a></li>
                                                <li><a href="#56" data-title="图3 网络结构模型">图3 网络结构模型</a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;锚点尺寸&lt;/b&gt;"><b>表</b>1 <b>锚点尺寸</b></a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;网络参数&lt;/b&gt;"><b>表</b>2 <b>网络参数</b></a></li>
                                                <li><a href="#87" data-title="图4 摄像机投影模型">图4 摄像机投影模型</a></li>
                                                <li><a href="#107" data-title="图5 数据集扩增">图5 数据集扩增</a></li>
                                                <li><a href="#112" data-title="图6 损失函数曲线">图6 损失函数曲线</a></li>
                                                <li><a href="#127" data-title="图7 不同算法召回率-准确率对比">图7 不同算法召回率-准确率对比</a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同算法性能对比&lt;/b&gt;"><b>表</b>3 <b>不同算法性能对比</b></a></li>
                                                <li><a href="#133" data-title="图8 不同算法每张图片误检测数-漏检率对比">图8 不同算法每张图片误检测数-漏检率对比</a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;部分拟合数据&lt;/b&gt;"><b>表</b>4 <b>部分拟合数据</b></a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;测距结果&lt;/b&gt;"><b>表</b>5 <b>测距结果</b></a></li>
                                                <li><a href="#146" data-title="图9 不同场景下的检测结果">图9 不同场景下的检测结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="162">


                                    <a id="bibliography_1" title="乔维高, 徐学进.无人驾驶汽车的发展现状及方向[J].上海汽车, 2007 (7) :40-43. (QIAO W G, XU X J.The development situation and direction of the driverless vehicle[J].Shanghai Auto, 2007 (7) :40-43.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SHQC200707013&amp;v=MzA5MTNJOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURsVzdySU5pWGFiYkc0SHRiTXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        乔维高, 徐学进.无人驾驶汽车的发展现状及方向[J].上海汽车, 2007 (7) :40-43. (QIAO W G, XU X J.The development situation and direction of the driverless vehicle[J].Shanghai Auto, 2007 (7) :40-43.) 
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_2" title="李晓明, 郎文辉, 马忠磊, 等.基于图像处理的井下机车行人检测技术[J].煤矿机械, 2017, 38 (4) :167-170. (LI X M, LANG WH, MA Z L, et al.Pedestrian detection technology for mine locomotive based on image processing[J].Coal Mine Machinery, 2017, 38 (4) :167-170.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MKJX201704059&amp;v=MTk2MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3cklLQ2JCZHJHNEg5Yk1xNDlBYlk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        李晓明, 郎文辉, 马忠磊, 等.基于图像处理的井下机车行人检测技术[J].煤矿机械, 2017, 38 (4) :167-170. (LI X M, LANG WH, MA Z L, et al.Pedestrian detection technology for mine locomotive based on image processing[J].Coal Mine Machinery, 2017, 38 (4) :167-170.) 
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_3" title="LIU T, FU H Y, WEN Q, et al.Extended faster R-CNN for long distance human detection:finding pedestrians in UAV images[C]//Proceedings of the 2018 IEEE International Conference on Consumer Electronics.Piscataway, NJ:IEEE, 2018:1-2." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extended faster R-CNN for long distance human detection:finding pedestrians in UAV images">
                                        <b>[3]</b>
                                        LIU T, FU H Y, WEN Q, et al.Extended faster R-CNN for long distance human detection:finding pedestrians in UAV images[C]//Proceedings of the 2018 IEEE International Conference on Consumer Electronics.Piscataway, NJ:IEEE, 2018:1-2.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_4" title="GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[4]</b>
                                        GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_5" title="HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition">
                                        <b>[5]</b>
                                        HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_6" title="GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[6]</b>
                                        GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_7" title="REDMON J, FARHADI A.YOLO9000:better, faster, stronger[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6517-6525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better Faster,Stronger">
                                        <b>[7]</b>
                                        REDMON J, FARHADI A.YOLO9000:better, faster, stronger[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6517-6525.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_8" title="郑嘉祺.基于DCNN的井下行人检测系统的研究与设计[D].西安:西安科技大学, 2017:84-87. (ZHENG J Q.Research and design on pedestrian detection system under the mine based on DCNN[D].Xi&#39;an:Xi&#39;an University of Science and Technology, 2017:84-87.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017725396.nh&amp;v=MzA5NDJDVVI3cWZadVpwRmlEbFc3cklWRjI2R2JTNkc5TEZxWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        郑嘉祺.基于DCNN的井下行人检测系统的研究与设计[D].西安:西安科技大学, 2017:84-87. (ZHENG J Q.Research and design on pedestrian detection system under the mine based on DCNN[D].Xi&#39;an:Xi&#39;an University of Science and Technology, 2017:84-87.) 
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_9" title="REDMON J, DIWALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified real-time object detection">
                                        <b>[9]</b>
                                        REDMON J, DIWALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:779-788.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                    王琳, 卫晨, 李伟山, 等.结合金字塔池化模块的YOLOv2的井下行人检测[EB/OL].[2018-05-21].https://www.doc88.com/p-0714870779937.html. (WANG L, WEI C, LI W S, et al.Pedestrian detection based on YOLOv2 with pyramid pooling module in underground coal mine[EB/OL].[2018-05-21].https://www.doc88.com/p-0714870779937.html.) </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                    李伟山, 卫晨, 王琳.改进的Faster R-CNN煤矿井下行人检测算法[EB/OL].[2018-07-15].http://kns.cnki.net/kcms/detail/11.2127.TP.20180522.0944.002.html. (LI W S, WEI C, WANG L.An improved faster R-CNN approach for pedestrian detection in underground coal mine[EB/OL].[2018-07-15].http://kns.cnki.net/kcms/detail/11.2127.TP.20180522.0944.002.html.) </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_12" title="REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2015, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">
                                        <b>[12]</b>
                                        REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2015, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_13" title="沈彤, 刘文波, 王京.基于双目立体视觉的目标测距系统[J].电子测量技术, 2015, 38 (4) :52-54. (SHEN T, LIU W B, WANGJ.Distance measurement system based on binocular stereo vision[J].Electronic Measurement Technology, 2015, 38 (4) :52-54.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201504013&amp;v=MjEyNTFNcTQ5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3JJSVRmSVlyRzRIOVQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        沈彤, 刘文波, 王京.基于双目立体视觉的目标测距系统[J].电子测量技术, 2015, 38 (4) :52-54. (SHEN T, LIU W B, WANGJ.Distance measurement system based on binocular stereo vision[J].Electronic Measurement Technology, 2015, 38 (4) :52-54.) 
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_14" title="郭磊, 徐友春, 李克强, 等.基于单目视觉的实时测距方法研究[J].中国图象图形学报, 2006, 11 (1) :74-81. (GUO L, XU YC, LI K Q, et al.Study on real-time distance detection based on monocular vision technique[J].Journal of Image and Graphics, 2006, 11 (1) :74-81.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200601011&amp;v=MjU3Njg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURsVzdySVB5cmZiTEc0SHRmTXJvOUVaWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        郭磊, 徐友春, 李克强, 等.基于单目视觉的实时测距方法研究[J].中国图象图形学报, 2006, 11 (1) :74-81. (GUO L, XU YC, LI K Q, et al.Study on real-time distance detection based on monocular vision technique[J].Journal of Image and Graphics, 2006, 11 (1) :74-81.) 
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_15" title="BAO D, WANG P.Vehicle distance detection based on monocular vision[C]//Proceedings of the 2016 International Conference on Progress in Informatics and Computing.Piscataway, NJ:IEEE, 2016:187-191." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vehicle distance detection based on monocular vision">
                                        <b>[15]</b>
                                        BAO D, WANG P.Vehicle distance detection based on monocular vision[C]//Proceedings of the 2016 International Conference on Progress in Informatics and Computing.Piscataway, NJ:IEEE, 2016:187-191.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_16" title="LIN T Y, GOYAL P, GIRSHICK R, et al.Focal loss for dense object detection[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2017:2999-3007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focal Loss for Dense Object Detection">
                                        <b>[16]</b>
                                        LIN T Y, GOYAL P, GIRSHICK R, et al.Focal loss for dense object detection[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2017:2999-3007.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-09-29 09:52</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(03),688-694 DOI:10.11772/j.issn.1001-9081.2018071501            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的井下巷道行人视觉定位算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A9%E6%B1%9F%E6%B4%AA&amp;code=00003928&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩江洪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E7%A8%BC%E8%BD%A9&amp;code=41275243&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁稼轩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%AB%E6%98%9F&amp;code=03082577&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卫星</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%86%E9%98%B3&amp;code=07068981&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陆阳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%88%E8%82%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0083575&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合肥工业大学计算机与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%89%E5%85%A8%E5%85%B3%E9%94%AE%E5%B7%A5%E4%B8%9A%E6%B5%8B%E6%8E%A7%E6%8A%80%E6%9C%AF%E6%95%99%E8%82%B2%E9%83%A8%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=1505528&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安全关键工业测控技术教育部工程研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>自主驾驶矿井机车需要实时检测和定位行驶前方的巷道行人, 激光雷达等非视觉类方法成本高昂, 而传统基于特征提取视觉类方法无法解决井下光照差且光线不均匀的问题。提出一种基于深度学习的井下巷道行人视觉定位算法。首先给出基于深度学习网络的系统整体结构;其次, 搭建目标检测多层卷积神经网络 (CNN) , 生成自主驾驶机车前方视野范围内行人的二维坐标及边界框的尺寸;再次, 通过多项式拟合计算出图像中行人到机车之间的第三维距离;最后通过真实样本集实施模型训练、验证与测试。实验结果表明, 所提算法的检测准确率达94%, 速度达每秒25帧, 测距误差小于4%, 实现了实时高效的巷道行人视觉定位。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B7%B7%E9%81%93%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">巷道行人检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉定位;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像处理;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    韩江洪 (1954—) , 男, 安徽庐江人, 教授, 博士生导师, 硕士, 主要研究方向:计算机网络与通信、计算机控制、汽车电子;;
                                </span>
                                <span>
                                    *袁稼轩 (1994—) , 女, 新疆阿拉尔人, 硕士研究生, 主要研究方向:DSP及嵌入式系统;电子邮箱yuanjiaxuan94@163.com;
                                </span>
                                <span>
                                    卫星 (1980—) , 男, 安徽合肥人, 副教授, 博士, CCF会员, 主要研究方向:物联网工程、离散事件动态系统;;
                                </span>
                                <span>
                                    陆阳 (1967—) , 男, 安徽合肥人, 教授, 博士生导师, 博士, CCF高级会员, 主要研究方向:分布式控制技术、无线通信网络、工业物联网。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划专项 (2016YFC0801800;2016YFC0801405);</span>
                    </p>
            </div>
                    <h1><b>Pedestrian visual positioning algorithm for underground roadway based on deep learning</b></h1>
                    <h2>
                    <span>HAN Jianghong</span>
                    <span>YUAN Jiaxuan</span>
                    <span>WEI Xing</span>
                    <span>LU Yang</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information, Hefei University of Technology</span>
                    <span>Engineering Research Center of Safety Critical Industry Measure and Control Technology, Ministry of Education</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The self-driving mine locomotive needs to detect and locate pedestrians in front of it in the underground roadway in real-time. Non-visual methods such as laser radar are costly, while traditional visual methods based on feature extraction cannot solve the problem of poor illumination and uneven light in the laneway. To solve the problem, a pedestrian visual positioning algorithm for underground roadway based on deep learning was proposed. Firstly, the overall structure of the system based on deep learning network was given. Secondly, a multi-layer Convolutional Neural Network (CNN) for object detection was built to calculate the two-dimensional coordinates and the size of bounding box of pedestrians in visual field of the self-driving locomotive. Thirdly, the third-dimensional distance between the pedestrian in the image and the locomotive was calculated by polynomial fitting. Finally, the model was trained, verified and tested through real sample sets. Experimental results show that the accuracy of the proposed algorithm reaches 94%, the speed achieves 25 frames per second, and the distance detection error is less than 4%, thus efficient and real-time laneway pedestrian visual positioning is realized.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=laneway%20pedestrian%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">laneway pedestrian detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20positioning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual positioning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image processing;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    HAN Jianghong, born in 1954, M. S. , professor. His research interests include computer network and communication, computer control, automotive electronics.;
                                </span>
                                <span>
                                    YUAN Jiaxuan, born in 1994, M. S. candidate. Her research interests include DSP and embedded system;
                                </span>
                                <span>
                                    WEI Xing, born in 1980, Ph. D. , associate professor. His research interests include Internet of things engineering, discrete event dynamic system.;
                                </span>
                                <span>
                                    LU Yang, born in 1967, Ph. D. , professor. His research interests include distributed control technology, wireless communication network, industrial Internet of things.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-07-19</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research Development Program of China (2016YFC0801800, 2016YFC0801405);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="36">近年来, 随着市场对驾驶安全和智能化需求的不断提高, 无人驾驶巨大的社会和经济价值越发凸显<citation id="194" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。无人驾驶系统在民用、科学研究、军事、工业等方面获得广泛应用。其中在工业方面则针对具有繁重的运输任务、有事故风险的井下工作环境来代替人工来完成采矿、运输等任务。不同于一般的驾驶环境, 工业轨道运输环境受井下空间和运输矿物的影响, 容易导致事故的发生, 且一旦发生事故极易造成人员伤亡或引发爆炸等严重后果。因此, 为了从根本上减少机车运行事故的发生, 杜绝人员伤亡现象的出现, 有必要对无人矿井机车前方行人进行动态感知、识别分析处理, 做到及时启/停和提前预警。</p>
                </div>
                <div class="p1">
                    <p id="37">行人检测技术多用于地面、街道等交通场景, 以方向梯度直方图 (Histogram of Oriented Gradients, HOG) 、可变形部件模型 (Deformable Part Model, DPM) 、决策森林 (Decision Forest, DF) 为例, 主要服务于智能交通和地面无人驾驶。由于井下环境照明条件恶劣、灰尘大、光线不均匀等, 无法将地面检测方法照搬。井下传统的图像处理技术有李晓明等<citation id="195" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>在传统Hough变化的基础上提出了极角极径约束法, 标定出轨道线, 在此基础上标定出感兴趣区域, 利用HOG特征结合支持向量机 (Support Vector Machine, SVM) 进行行人检测。这种基于传统图像处理的技术需要人工设计不同的特征提取算子, 并且这些特征提取算子需要靠资深专家进行手工设计, 更新迭代速度较慢且对行人的多样性变化没有很强的鲁棒性。然而, 深度学习却在此领域取得了突破性的进展, 行人检测作为目标检测的一个重要分支, 成为研究的热点之一<citation id="201" type="reference"><link href="166" rel="bibliography" /><link href="168" rel="bibliography" /><link href="170" rel="bibliography" /><link href="172" rel="bibliography" /><link href="174" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。同时。在井下行人检测方面也得到应用。如郑嘉祺<citation id="196" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了一种基于深度卷积神经网络 (Deep Convolutional Neural Network, DCNN) 的矿井下行人检测技术, 利用YOLO (You Only Look Once) <citation id="197" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>目标检测算法, 并针对井下特殊环境的特点对其进行改进, 提高了检测速度;王琳等<citation id="198" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了以YOLO网络为基础, 结合金字塔场景解析的网络中的金字塔池化模块的检测技术, 利用了图片的上下文信息, 对井下行人进行实时检测;李伟山等<citation id="199" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出了改进的Faster R-CNN (Region Convolutional Neural Network, R-CNN) <citation id="200" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>煤矿井下行人检测方法, 一定程度上提高了井下行人的多尺度的检测效果;这些方法未能充分利用井下行人数据的特点, 本质上均是在二维平面对行人进行了识别、定位, 并且其检测结果未能对井下机车安全行驶带来很好的保障作用, 具有应用的局限性。</p>
                </div>
                <div class="p1">
                    <p id="38">为此, 本文将行人检测与视觉测距算法相结合, 进行测距得到巷道行人距离机车的实际距离。当前视觉测距的方法有单目测距和双目测距。双目测距<citation id="202" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>需要使用两个不同角度的相机从不同角度获取两幅图像, 然后通过物体匹配和几何原理, 得到三维信息;然而, 双目视觉测距成像速度慢, 系统复杂, 不能实现实时计算, 于是, 单目视觉测距<citation id="203" type="reference"><link href="188" rel="bibliography" /><link href="190" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>凭借自身原理简单、检测速度快的特点, 成为视觉测距的主流技术。但是, 目前还未有成熟且公认的高精度井下行人视觉测距算法, 所以, 本文提出了一种基于深度学习的井下巷道行人视觉定位算法, 实时检测机车前方视野范围内行人, 并计算出行人与机车之间的距离, 旨在从根本上减少机车运行事故的发生, 杜绝人员伤亡现象的发生。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag">1 视觉定位检测系统模型设计</h3>
                <div class="p1">
                    <p id="40">井下巷道行人的视觉定位检测结构如图1所示, 机车视频图像处理器作为无人自主驾驶机车感知部件, 需要动态地对拍摄到的轨道正前方视野画面进行识别得到行人信息, 并传递给控制部件完成启/停、加减速等操作, 达到无人自主驾驶的目的。上述图像识别器的关键作用为视觉定位, 图像识别器要动态获知视野范围内有无行人, 若有则给出其“三维坐标”, 即 (<i>x</i>, <i>y</i>, <i>z</i>) 。如图1:<i>b</i>处于安全区域内, 行车通过无影响;<i>c</i>处于预警区域内, 需鸣笛示警;<i>a</i>处于危险区域, 应立即停车, 进而提高无人驾驶机车的安全性能。</p>
                </div>
                <div class="area_img" id="41">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 视觉定位示意图" src="Detail/GetImg?filename=images/JSJY201903013_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 视觉定位示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic diagram of visual positioning</p>

                </div>
                <div class="p1">
                    <p id="42">为实现上述目标, 本文将行人检测算法与测距算法相结合, 设计整体算法系统模型 (如图2所示) , 系统主要分为井下巷道行人检测模型与测距模型, 行人检测模型要解决的核心问题是, 检测出图像中任何位置出现的不同尺度大小和各种姿态的行人, 并确定他们在二维图像上的位置及大小, 整个检测流程舍去了候选框提取分支, 直接将特征提取、候选框, 回归和分类在同一个无分支的卷积网络中完成, 使得网络结构变得简单;测距模块负责将行人检测模块得到的检测结果中的行人的宽, 高输入到实验训练时已经拟合完成的距离曲线中, 计算出图像中机车前方行人距离机车的实际距离。最后输出一张标有目标行人及行人距离机车的实际距离的图片, 实现巷道行人检测的“三维定位”。</p>
                </div>
                <div class="area_img" id="43">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 系统模型" src="Detail/GetImg?filename=images/JSJY201903013_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 系统模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 System model</p>

                </div>
                <h3 id="44" name="44" class="anchor-tag">2 行人检测网络</h3>
                <div class="p1">
                    <p id="45">井下巷道行人检测算法借鉴YOLO的基本思想, 从而实现自己的检测效果。YOLO的核心思想是系统将输入图像分成<i>S</i>×<i>S</i>的网格。如果目标的中心落入某个网格单元中, 那么该网格单元就负责检测该目标。每个网格单元都会预测<i>B</i>个边界框和这些框的置信度分数。这些置信度分数反映了该模型对那个框内是否包含目标的概率, 以及它对自己的预测的准确度的估量。在形式上, 将置信度定义如下:</p>
                </div>
                <div class="area_img" id="160">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201903013_16000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="161">IOU<sup>truth</sup><sub>pred</sub> (Intersection-Over-Union, IOU) 为预测框 (predict box) 与真实框 (ground truth) 的交集面积与并集面积之比:</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>Ο</mi><mi>U</mi><msubsup><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>t</mtext><mtext>h</mtext></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>b</mi><mi>o</mi><mi>x</mi><mo stretchy="false"> (</mo><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>t</mtext><mtext>h</mtext><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>b</mi></mstyle><mi>o</mi><mi>x</mi><mo stretchy="false"> (</mo><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>b</mi><mi>o</mi><mi>x</mi><mo stretchy="false"> (</mo><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>t</mtext><mtext>h</mtext><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∪</mo><mi>b</mi></mstyle><mi>o</mi><mi>x</mi><mo stretchy="false"> (</mo><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中<i>P</i><sub><i>r</i></sub> (<i>Object</i>) 表示是否有目标物落入候选框对应的单元格中, 其定义如下:</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false"> (</mo><mi>Ο</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext>无</mtext><mtext>目</mtext><mtext>标</mtext><mtext>物</mtext></mtd></mtr><mtr><mtd><mn>1</mn><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>有</mtext><mtext>目</mtext><mtext>标</mtext><mtext>物</mtext></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">如果该单元格中不存在目标, 则置信度分数应为零。否则, 置信度分数等于预测框与真实标签框之间联合部分的交集。每个边界框包含5个预测:<i>x</i>、<i>y</i>、<i>w</i>、<i>h</i>和置信度。 (<i>x</i>, <i>y</i>) 坐标表示边界框的中心相对于网格单元的边界的值, 而宽度和高度则是相对于整张图像来预测的。置信度预测表示预测框与任意实际边界框之间的IOU。每个网格单元还预测<i>C</i>个条件类别概率<i>P</i><sub><i>r</i></sub> (<i>Class</i>|<i>Object</i>) , 这些概率以包含目标的网格单元为条件。在测试时, 将条件类别概率与每个框的预测的置信度值相乘即可得到每个框的特定类别的置信分数:</p>
                </div>
                <div class="p1">
                    <p id="52"><i>P</i><sub><i>r</i></sub> (<i>Class</i>|<i>Object</i>) *<i>P</i><sub><i>r</i></sub> (<i>Object</i>) *<i>IOU</i><mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>t</mtext><mtext>h</mtext></mrow></msubsup></mrow></math></mathml>=</p>
                </div>
                <div class="p1">
                    <p id="54"><i>P</i><sub><sub><i>r</i></sub></sub> (<i>Class</i>) *<i>IOU</i><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>d</mtext></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>t</mtext><mtext>h</mtext></mrow></msubsup></mrow></math></mathml>      (4) </p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 网络结构模型" src="Detail/GetImg?filename=images/JSJY201903013_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 网络结构模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Model of network structure</p>

                </div>
                <div class="p1">
                    <p id="57">这些分数体现了该类出现在框中的概率以及预测框拟合目标的程度。</p>
                </div>
                <h4 class="anchor-tag" id="58" name="58">2.1 <b>锚点框选取</b></h4>
                <div class="p1">
                    <p id="59">在井下巷道行人检测网络训练过程中, 图像中的锚点尺寸对于行人检测的位置预测影响重大, 故选择出符合数据集的锚点框尤为关键。相对于手工挑选锚点框尺寸, 本文在数据集边界框上运行<i>k</i>-means聚类算法, 让网络自动找到最好的锚点框, 提高收敛速度, 提高行人检测的位置精度。随着迭代次数的不断增加, 网络学习到行人特征, 预测框参数得到不断调整, 最终逼近真实框。分析井下特定的图像数据, 井下行人服饰相对统一, 相对于路面或者生活中的人, 行走姿态相对单一, 得到与图像中行人边界框较好的先验的9种锚点尺寸。</p>
                </div>
                <div class="p1">
                    <p id="60"><i>K</i>-means聚类采用欧氏距离衡量两点之间的距离。聚类的目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mn>1</mn><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mi>Ι</mi><mi>Ο</mi><mi>U</mi></mrow></msub><mo stretchy="false"> (</mo><mi>B</mi><mi>o</mi><mi>x</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>, </mo><mi>Τ</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中, <i>i</i>为聚类的类别数, <i>j</i>为样本集数量, <i>Box</i>[<i>i</i>]表示聚类得到的预测框<i>i</i>的尺寸, <i>Truth</i>[<i>j</i>]表示样本<i>j</i>中行人定位框尺寸, 运行<i>k</i>-means聚类算法并进行尺寸变换后得到如表1所示。</p>
                </div>
                <div class="area_img" id="63">
                    <p class="img_tit"><b>表</b>1 <b>锚点尺寸</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Sizes of anchor boxes</p>
                    <p class="img_note"></p>
                    <table id="63" border="1"><tr><td><br />锚点窗口</td><td>尺寸</td><td></td><td>锚点窗口</td><td>尺寸</td></tr><tr><td><br />anchor box1</td><td> (22×65) </td><td></td><td>anchor box6</td><td> (58×186) </td></tr><tr><td><br />anchor box2</td><td> (30×91) </td><td></td><td>anchor box7</td><td> (75×252) </td></tr><tr><td><br />anchor box3</td><td> (35×121) </td><td></td><td>anchor box8</td><td> (113×340) </td></tr><tr><td><br />anchor box4</td><td> (44×169) </td><td></td><td>anchor box9</td><td> (201×398) </td></tr><tr><td><br />anchor box5</td><td> (53×137) </td><td></td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="64">图片的每个单元格所对应的9个不同的锚点窗口, 然后预测边界框的4个偏移值{<i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, <i>t</i><sub><i>w</i></sub>, <i>t</i><sub><i>h</i></sub>}, 其中<i>t</i><sub><i>x</i></sub>, <i>t</i><sub><i>y</i></sub>, <i>t</i><sub><i>w</i></sub>, <i>t</i><sub><i>h</i></sub>分别表示边界框偏移值的中心点的横纵坐标与边界框的宽和高, 逆向推导得到边界框相对于整张图片的位置和大小{<i>b</i><sub><i>x</i></sub>, <i>b</i><sub><i>y</i></sub>, <i>b</i><sub><i>w</i></sub>, <i>b</i><sub><i>h</i></sub>}, 其中<i>b</i><sub><i>w</i></sub>, <i>b</i><sub><i>h</i></sub>分别表示对边界框宽和高, <i>b</i><sub><i>x</i></sub>, <i>b</i><sub><i>y</i></sub>分别表示边界框中心点的坐标。边界框满足如下公式:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>x</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub></mtd></mtr><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>y</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub></mtd></mtr><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>w</mi></msub><mo>=</mo><mi>p</mi><msub><mrow></mrow><mi>w</mi></msub><mtext>e</mtext><msup><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></msup></mtd></mtr><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>h</mi></msub><mo>=</mo><mi>p</mi><msub><mrow></mrow><mi>h</mi></msub><mtext>e</mtext><msup><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></msup></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">其中<i>p</i><sub><i>w</i></sub>, <i>p</i><sub><i>h</i></sub>分别表示锚点框的宽和高。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">2.2 <b>网络结构</b></h4>
                <div class="p1">
                    <p id="68">本文提出的基于深度学习的井下巷道行人视觉定位算法中行人检测模型 (如图3所示) , 网络模型由53个卷积层, 2个上采样层和一个检测层组成。模型前端是用来提取图片特征向量的卷积神经网络 (Convolutional Neural Network, CNN) , 其中运用了残差网络更好地提取特征。后端通过卷积生成不同大小的特征图, 采用3种尺度预测, 将之前生成的9种聚类, 按照大小分给3个尺度, 这样做充分利用不同卷积层得到的特征图分辨率不同的特性, 即低卷积层分辨率高, 有利于检测小目标, 高卷积层感受野大, 有利于检测大目标。同时, 在卷积层利用1×1、3×3卷积核, 提取细节特征, 更有利于检测小目标行人。具体网络参数如表2。</p>
                </div>
                <div class="area_img" id="69">
                    <p class="img_tit"><b>表</b>2 <b>网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Network parameters</p>
                    <p class="img_note"></p>
                    <table id="69" border="1"><tr><td><br />编号</td><td>Layer</td><td>Filters</td><td>Size/Stripe</td><td>Output</td></tr><tr><td><br />1</td><td>conv</td><td>32</td><td>3×3 / 1</td><td>416×416×32</td></tr><tr><td><br />2</td><td>conv</td><td>64</td><td>3×3 / 2</td><td>208×208×64</td></tr><tr><td><br />3</td><td>conv</td><td>32</td><td>1×1 / 1</td><td>208×208×32</td></tr><tr><td><br />4</td><td>conv</td><td>64</td><td>3×3 / 1</td><td>208×208×64</td></tr><tr><td><br />5</td><td>res</td><td></td><td></td><td>208×208×64</td></tr><tr><td><br />6</td><td>conv</td><td>128</td><td>3×3 / 2</td><td>104×104×128</td></tr><tr><td><br />7</td><td>conv</td><td>64</td><td>1×1 / 1</td><td>104×104×64</td></tr><tr><td><br />8</td><td>conv</td><td>128</td><td>3×3 / 1</td><td>104×104×128</td></tr><tr><td><br />9</td><td>res</td><td></td><td></td><td>104×104×128</td></tr><tr><td><br />10</td><td>conv</td><td>256</td><td>3×3 / 2</td><td>52×52×256</td></tr><tr><td><br />11</td><td>conv</td><td>128</td><td>1×1 / 1</td><td>52×52×128</td></tr><tr><td><br />12</td><td>conv</td><td>256</td><td>3×3 / 1</td><td>52×52×256</td></tr><tr><td><br />13</td><td>res</td><td></td><td></td><td>52×52×256</td></tr><tr><td><br />14</td><td>conv</td><td>215</td><td>3×3 / 2</td><td>26×26×512</td></tr><tr><td><br />15</td><td>conv</td><td>256</td><td>1×1 / 1</td><td>26×26×256</td></tr><tr><td><br />16</td><td>conv</td><td>512</td><td>3×3 / 1</td><td>26×26×512</td></tr><tr><td><br />17</td><td>res</td><td></td><td></td><td>26×26×512</td></tr><tr><td><br />18</td><td>conv</td><td>1 024</td><td>3×3 / 2</td><td>13×13×1 024</td></tr><tr><td><br />19</td><td>conv</td><td>512</td><td>1×1 / 1</td><td>13×13×512</td></tr><tr><td><br />20</td><td>conv</td><td>1 024</td><td>3×3 / 1</td><td>13×13×1 024</td></tr><tr><td><br />21</td><td>res</td><td></td><td></td><td>13×13×1 024</td></tr><tr><td><br />22</td><td>conv</td><td>512</td><td>1×1 / 1</td><td>13×13×512</td></tr><tr><td><br />23</td><td>conv</td><td>1 024</td><td>3×3 / 1</td><td>13×13×1 024</td></tr><tr><td><br />24</td><td>conv</td><td>512</td><td>1×1 / 1</td><td>13×13×512</td></tr><tr><td><br />25</td><td>conv</td><td>1 024</td><td>3×3 / 1</td><td>13×13×1 024</td></tr><tr><td><br />26</td><td>conv</td><td>512</td><td>1×1 / 1</td><td>13×13×512</td></tr><tr><td><br />27</td><td>conv</td><td>1 024</td><td>3×3 / 1</td><td>13×13×1 024</td></tr><tr><td><br />28</td><td>conv</td><td>255</td><td>1×1 / 1</td><td>13×13×255</td></tr><tr><td><br />29</td><td>conv</td><td>256</td><td>1×1 / 1</td><td>13×13×256</td></tr><tr><td><br />30</td><td>upsample</td><td></td><td>2×</td><td>26×26×256</td></tr><tr><td><br />31</td><td>conv</td><td>256</td><td>1×1 / 1</td><td>26×26×256</td></tr><tr><td><br />32</td><td>conv</td><td>512</td><td>3×3 / 1</td><td>26×26×512</td></tr><tr><td><br />33</td><td>conv</td><td>256</td><td>1×1 / 1</td><td>26×26×256</td></tr><tr><td><br />34</td><td>conv</td><td>512</td><td>3×3 / 1</td><td>26×26×512</td></tr><tr><td><br />35</td><td>conv</td><td>256</td><td>1×1 / 1</td><td>26×26×256</td></tr><tr><td><br />36</td><td>conv</td><td>512</td><td>3×3 / 1</td><td>26×26×512</td></tr><tr><td><br />37</td><td>conv</td><td>255</td><td>1×1 / 1</td><td>26×26×255</td></tr><tr><td><br />38</td><td>conv</td><td>128</td><td>1×1 / 1</td><td>26×26×128</td></tr><tr><td><br />39</td><td>upsample</td><td></td><td>2×</td><td>52×52×128</td></tr><tr><td><br />40</td><td>conv</td><td>128</td><td>1×1 / 1</td><td>52×52×128</td></tr><tr><td><br />41</td><td>conv</td><td>256</td><td>3×3 / 1</td><td>52×52×256</td></tr><tr><td><br />42</td><td>conv</td><td>128</td><td>1×1 / 1</td><td>52×52×128</td></tr><tr><td><br />43</td><td>conv</td><td>256</td><td>3×3 / 1</td><td>52×52×256</td></tr><tr><td><br />44</td><td>conv</td><td>128</td><td>1×1 / 1</td><td>52×52×128</td></tr><tr><td><br />45</td><td>conv</td><td>256</td><td>3×3 / 1</td><td>52×52×256</td></tr><tr><td><br />46</td><td>conv</td><td>255</td><td>1×1 / 1</td><td>52×52×255</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="70">表2中空格表示此层网络无此参数, 在实际网络结构中, 编号为11、12、13层网络组成一个模块连续循环4遍, 编号为15、16、17层网络也组成一个模块连续循环4遍, 编号19、20、21层网络组成一个模块连续循环2遍, 此目的为了更好地提取图像特征。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">2.3 <b>损失函数</b></h4>
                <div class="p1">
                    <p id="72">损失函数 (loss function) 是用来估量模型的预测值与真实值的不一致程度, 损失函数越小, 模型的鲁棒性就越强。在训练过程中, 由于大多数样本都是简单易区分的负样本即背景样本, 类别不均衡, 致使训练过程中不能充分学习到正样本的特征信息, 其次简单的背景样本太多, 易分背景样本会产生一定幅度的损失, 加之数量巨大, 最终会对损失函数起到主要的贡献作用, 从而导致梯度更新方向发生变化, 掩盖正样本的作用。</p>
                </div>
                <div class="p1">
                    <p id="73">针对上述问题, 本文结合focal loss function<citation id="204" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>作为解决类别不均衡的更高效的替代方法。它能动态地缩放交叉熵, 随着正确类别的置信度增加, 其中的尺度因子衰减到零。直观感受, 这个缩放因子可以自动减小训练过程中easy example的贡献的比例并快速聚焦hard examples。使用的交叉熵损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>E</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo>-</mo><mi>lg</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mi>y</mi><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mi>lg</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>p</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中<i>p</i>∈[0, 1]模型对于标签<i>y</i>=1的估计概率, 记作<i>p</i><sub><i>t</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mspace width="0.25em" /><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mi>y</mi><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mn>1</mn></mtd></mtr><mtr><mtd><mn>1</mn><mo>-</mo><mi>p</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">重写式 (8) 得:</p>
                </div>
                <div class="p1">
                    <p id="78"><i>CE</i> (<i>p</i><sub><i>t</i></sub>) =-lg (<i>p</i><sub><i>t</i></sub>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="79">针对类别不均衡的常用方法是用一个权重参数<i>α</i>∈[0, 1]针对于类1, 1-<i>α</i>针对于类 -1。实际应用上, <i>α</i>被设定为类频率的逆或者作为超参数, 通过交叉验证设定。为了标记方便, 定义<i>α</i><sub><i>t</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="80"><i>CE</i> (<i>P</i><sub><i>t</i></sub>) =-<i>α</i><sub><i>t</i></sub> lg (<i>p</i><sub><i>t</i></sub>)      (10) </p>
                </div>
                <div class="p1">
                    <p id="81">训练时, 为解决样本不均衡问题, 使得各个样本拥有自身的权重, 即网络预测的该样本属于true class的概率大, 则该样本对网络来说就属于easy example, 若该样本属于true class的概率很小, 则该样本属于hard example。故定义损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="82"><i>FL</i> (<i>p</i><sub><i>t</i></sub>) =- (1-<i>p</i><sub><i>t</i></sub>) <sup><i>γ</i></sup> lg (<i>p</i><sub><i>t</i></sub>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="83">其中<i>γ</i>是可以调节的参数, <i>γ</i>&gt;0;当参数<i>γ</i>=0时, 就是普通的交叉熵函数。实验发现<i>γ</i>=2时效果最好。</p>
                </div>
                <h3 id="84" name="84" class="anchor-tag">3 基于行人检测的实时测距方法</h3>
                <div class="p1">
                    <p id="85">实时测距方法基于单目视觉来检测机车与前方行人的距离, 从而为无人驾驶机车智能化控制输入参数。摄像机拍摄到的场景图像是三维空间场景在二维平面的投影, 而在利用机器视觉对巷道前方道路情况进行识别过程中, 则需要一个逆向求解的过程, 即从二维图像还原成路面真实图像。此过程就是巷道前方道路深度信息的获取。根据小孔成像原理, 将单目视觉系统简化为摄像机投影模型 (如图4所示) 。</p>
                </div>
                <div class="p1">
                    <p id="86">如图4中 (b) 所示, 距离计算, 在直观意义上, 当距离确定时, 较大行人在图片上相对较大, 对于一个物体, 更远的距离意味着物体的像素尺寸更小。对于行人来说, 距离和图像中行人的像素大小成反比。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 摄像机投影模型" src="Detail/GetImg?filename=images/JSJY201903013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 摄像机投影模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Camera projection model</p>

                </div>
                <div class="p1">
                    <p id="88">根据所去矿井统计井下行人的平均身高为173 cm, 在这里将井下巷道的行人身高记为173 cm。根据以上结论, 将巷道行人的身高看作固定参数, 用<i>H</i>表示, 用行人检测算法模块得到的行人边界框的高<i>b</i><sub><i>h</i></sub>表示巷道行人在图像平面上的像素高度。用<i>D</i>表示相机与对象行人之间的实际距离。所以, 井下巷道行人与相机间的实际距离遵循以下等式:</p>
                </div>
                <div class="p1">
                    <p id="89"><i>b</i><sub><i>h</i></sub>=<i>f</i> (<i>D</i>, <i>c</i>, <i>H</i>)      (12) </p>
                </div>
                <div class="p1">
                    <p id="90"><i>H</i>是固定的, <i>c</i>是一个常数, 对于不同的相机<i>c</i>是不一样的, 故实验使用多组<i>D</i>和<i>b</i><sub><i>h</i></sub>拟合函数曲线, 最后可以得到相机的<i>c</i>。</p>
                </div>
                <div class="p1">
                    <p id="91">为了保证检测的准确性, 当检测到的行人的宽度<i>b</i><sub><i>w</i></sub>与高度<i>b</i><sub><i>h</i></sub>的比值超过特定阈值时, 即行人可能处于蹲坐状态, 则采取将行人高度强行提高3倍, 以避免测距发生错误。</p>
                </div>
                <h3 id="92" name="92" class="anchor-tag">4 实验</h3>
                <div class="p1">
                    <p id="93">本文实验主要分为两部分:一部分是井下巷道的行人检测实验, 负责检测图片中井下巷道行人及行人边界框位置, 为后期距离计算提供数据源;另一部分是测距实验, 负责计算行人距机车的实际距离。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94">4.1 <b>数据集扩增</b></h4>
                <div class="p1">
                    <p id="95">本文中的矿井实验数据来自于桃园煤矿和新集煤矿的井下机车摄像头所拍摄的井下巷道视频, 视频按照每秒30帧将视频转换为13 400张分辨率为1 280×720的图片, 由于深度网络的训练数据量巨大, 这些图片的数量远远不足, 于是在训练之前, 对图片进行扩增, 主要使用的扩增方式有以下几种 (如图5) 。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">1) 旋转变换 (rotation) :</h4>
                <div class="p1">
                    <p id="97">随机将图像旋转一定角度。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">2) 翻转变换 (flip) :</h4>
                <div class="p1">
                    <p id="99">对图像作翻转变换。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">3) 缩放变换 (zoom) :</h4>
                <div class="p1">
                    <p id="101">缩小或放大图像。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">4) 对比度变换 (contrast) :</h4>
                <div class="p1">
                    <p id="103">通过改变图像像元的亮度值来改变图像像元的对比度。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">5) 裁剪变换 (cropping) :</h4>
                <div class="p1">
                    <p id="105">裁剪出图像中的目标物体。</p>
                </div>
                <div class="p1">
                    <p id="106">扩增后数据集共19 450张, 其中训练集13 410张, 测试集3 020张, 验证集3 020张。数据集包含了各种尺度的行人, 以及昏暗条件下的图片数据, 有利于增强网络的鲁棒性。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 数据集扩增" src="Detail/GetImg?filename=images/JSJY201903013_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 数据集扩增  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Dataset augmentation</p>

                </div>
                <h4 class="anchor-tag" id="108" name="108">4.2 <b>井下巷道行人检测实验</b></h4>
                <h4 class="anchor-tag" id="109" name="109">4.2.1 网络训练</h4>
                <div class="p1">
                    <p id="110">整个训练过程使用随机梯度下降及反向传播算法来学习网络参数。训练的批处理大小batch为16, subversion为4, 动量 (momentum) 为0.9, 权重衰减, (decay) 为0.000 5, 最大迭代次数为75 120次。实验基于Ubuntu 16.04, 64 位操作系统, 使用的深度学习框架是Pytorch, GPU为GeForce GTX 1080i, 初始化网络训练的学习率为0.001, 经过3 000次迭代后, 将学习率调整为0.01, 迭代10 000次后将学习率调整为0.001, 迭代35 000次后调整学习率为0.000 1, 迭代60 000次后调整学习率为0.000 01。</p>
                </div>
                <div class="p1">
                    <p id="111">训练过程中模型的损失函数收敛曲线如图6, 由图可知, 损失函数随着迭代次数的增加越来越接近于0, 网络是稳定收敛的。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 损失函数曲线" src="Detail/GetImg?filename=images/JSJY201903013_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 损失函数曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Loss curve</p>

                </div>
                <h4 class="anchor-tag" id="113" name="113">4.2.2 评价指标</h4>
                <div class="p1">
                    <p id="114">召回率 (Recall) : 是测试集中所有正样本样例中, 被正确识别为正样本的比例, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">其中:<i>TP</i> (True Positives, TP) 为正样本被正确识别为正样本的数量, <i>FN</i> (False Negatives, FN) 为正样本被错误识别为负样本的数量。</p>
                </div>
                <div class="p1">
                    <p id="117">精确度 (Precision) :在识别出来的图片中, <i>TP</i>所占的比率, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">其中<i>FP</i> (False Positives, FP) 即负样本被错误识别为正样本数量。</p>
                </div>
                <div class="p1">
                    <p id="120">平均精度 (Average-Precision, AP) :Precision-Recall 曲线下面的面积, 通常来说一个越好的分类器, <i>AP</i>值越高, 分类器性能越好。</p>
                </div>
                <div class="p1">
                    <p id="121">漏检率 (Miss Rate, MR) :与召回率相对应, 召回率与漏检率相加和为1。</p>
                </div>
                <div class="p1">
                    <p id="122">平均每张图片误检数 (False Positives Per Image, FPPI) :平均每张图片误检测数目, 公式如下:</p>
                </div>
                <div class="p1">
                    <p id="123"><i>FPPI</i>=<i>FP</i>/<i>N</i><sub>img</sub>      (15) </p>
                </div>
                <div class="p1">
                    <p id="124">其中<i>N</i><sub>img</sub>为图片总数目。当评估一个识别方法性能时, 通过设置不同的分数阈值, 可以得到不同组 (MR, FPPI) 值, 从而可以画出MR-FPPI曲线。</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125">4.2.3 行人检测结果对比及分析</h4>
                <div class="p1">
                    <p id="126">为进一步测试本文网络模型的检测性能, 分别使用Faster R-CNN、YOLOv1和本文算法, 在验证图片以评估模型检测性能, 改变识别阈值<i>IOU</i>, 阈值的变化同时会导致精确度与召回率值发生变化, 从而得到曲线。得到精确度-召回率曲线性能对比如图7, 算法检测时间和平均精度 (AP) 对比数据如表3。</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同算法召回率-准确率对比" src="Detail/GetImg?filename=images/JSJY201903013_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同算法召回率-准确率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Recall-Precision curve comparison of different algorithms</p>

                </div>
                <div class="p1">
                    <p id="128">由图7可得, 本文算法在召回率相同的情况下, 检测准确度都高于Faster R-CNN、YOLOv1算法。</p>
                </div>
                <div class="area_img" id="129">
                    <p class="img_tit"><b>表</b>3 <b>不同算法性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Performance comparison of different algorithms</p>
                    <p class="img_note"></p>
                    <table id="129" border="1"><tr><td><br />算法</td><td>每张检测时间/s</td><td>AP/%</td></tr><tr><td><br />Faster R-CNN</td><td>2.00</td><td>80</td></tr><tr><td><br />YOLOv1</td><td>0.50</td><td>90</td></tr><tr><td><br />本文算法</td><td>0.04</td><td>94</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="130">由表3所示, 本文提出的算法检测时间仅仅需要0.04 s, 且准确度达到94%, 达到了准确而又快速的实时检测效果。</p>
                </div>
                <div class="p1">
                    <p id="131">本实验还以平均每张图片误检数 (FPPI) 作为横坐标, 漏检率 (MR) 作为纵坐标, 对Faster R-CNN、YOLOv1和本文算法进行性能对比, 如图8。</p>
                </div>
                <div class="p1">
                    <p id="132">从图8中可以看出, 平均每张图片的误检数与漏检率呈现负相关, 在对应的FPPI相同情况下, 本文算法的漏检率较低, 即检测性能最好。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同算法每张图片误检测数-漏检率对比" src="Detail/GetImg?filename=images/JSJY201903013_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同算法每张图片误检测数-漏检率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 FPPI-MR Comparison of different algorithms</p>

                </div>
                <h4 class="anchor-tag" id="134" name="134">4.3 <b>测距实验结果</b></h4>
                <div class="p1">
                    <p id="135">部分拟合数据如表4所示。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表</b>4 <b>部分拟合数据</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Partially fitted data</p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td><br />实际距离<i>D</i>/m</td><td><i>b</i><sub><i>h</i></sub>/pix</td><td></td><td>实际距离<i>D</i>/m</td><td><i>b</i><sub><i>h</i></sub>/pix</td></tr><tr><td>4</td><td>357</td><td></td><td>15</td><td>98</td></tr><tr><td><br />6</td><td>245</td><td></td><td>18</td><td>82</td></tr><tr><td><br />8</td><td>184</td><td></td><td>20</td><td>74</td></tr><tr><td><br />10</td><td>146</td><td></td><td>25</td><td>59</td></tr><tr><td><br />13</td><td>114</td><td></td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="137">根据表4中的拟合数据, 拟合函数如下:</p>
                </div>
                <div class="p1">
                    <p id="138"><i>b</i><sub><i>h</i></sub>=-453/<i>D</i><sup>2</sup>+1 558/<i>D</i>-3      (16) </p>
                </div>
                <div class="p1">
                    <p id="139">测距误差是影响距离精度的重要因素之一, 通过使用同一相机的另一组数据测试该方程, 利用行人行走时对多个实际距离进行测距实验, 并人工计算测距误差。定义测距误差为:</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>e</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>d</mi><msub><mrow></mrow><mi>r</mi></msub><mo>-</mo><mi>D</mi></mrow><mo>|</mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141">其中:<i>D</i>为行人到机车实际距离, 测量距离为<i>d</i><sub><i>r</i></sub>。式 (17) 计算得到每一次的误差<i>d</i><sub><i>e</i></sub>, 同时计算出每组测量数据的误差百分比, 对实际距离值<i>D</i>、测量值<i>d</i><sub><i>r</i></sub>、误差<i>d</i><sub><i>e</i></sub>、误差百分比进行对比验证, 结果如表5。</p>
                </div>
                <div class="p1">
                    <p id="142">表5结果表明, 本文算法误差控制在4%以内, 证明它具有很好的适用性。</p>
                </div>
                <div class="area_img" id="143">
                    <p class="img_tit"><b>表</b>5 <b>测距结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Distance detection results</p>
                    <p class="img_note"></p>
                    <table id="143" border="1"><tr><td>实际距<br />离<i>D</i>/m</td><td>检测距<br />离<i>d</i><sub><i>r</i></sub>/m</td><td>误差<br /><i>d</i><sub><i>e</i></sub>/cm</td><td>误差百<br />分比/%</td><td></td><td>实际距<br />离<i>D</i>/m</td><td>检测距<br />离<i>d</i><sub><i>r</i></sub>/m</td><td>误差<br /><i>d</i><sub><i>e</i></sub>/cm</td><td>误差百<br />分比/%</td></tr><tr><td>4</td><td>4.026</td><td>2.6</td><td>0.650</td><td></td><td>18</td><td>17.82</td><td>18</td><td>1.000</td></tr><tr><td><br />5</td><td>5.056</td><td>5.6</td><td>1.120</td><td></td><td>19</td><td>18.70</td><td>30</td><td>1.580</td></tr><tr><td><br />6</td><td>6.090</td><td>9.0</td><td>1.500</td><td></td><td>20</td><td>19.68</td><td>32</td><td>1.600</td></tr><tr><td><br />7</td><td>7.100</td><td>10.0</td><td>1.420</td><td></td><td>21</td><td>20.25</td><td>75</td><td>3.570</td></tr><tr><td><br />8</td><td>8.110</td><td>11.0</td><td>1.375</td><td></td><td>22</td><td>21.35</td><td>65</td><td>2.950</td></tr><tr><td><br />9</td><td>9.130</td><td>13.0</td><td>1.440</td><td></td><td>23</td><td>23.55</td><td>55</td><td>2.300</td></tr><tr><td><br />10</td><td>10.090</td><td>9.0</td><td>0.900</td><td></td><td>24</td><td>23.30</td><td>70</td><td>2.917</td></tr><tr><td><br />11</td><td>11.120</td><td>12.0</td><td>1.090</td><td></td><td>25</td><td>24.45</td><td>55</td><td>2.200</td></tr><tr><td><br />12</td><td>12.150</td><td>15.0</td><td>1.250</td><td></td><td>26</td><td>25.68</td><td>32</td><td>1.230</td></tr><tr><td><br />13</td><td>13.130</td><td>13.0</td><td>1.000</td><td></td><td>27</td><td>26.10</td><td>90</td><td>3.330</td></tr><tr><td><br />14</td><td>14.280</td><td>28.0</td><td>2.000</td><td></td><td>28</td><td>27.10</td><td>90</td><td>3.210</td></tr><tr><td><br />15</td><td>15.260</td><td>26.0</td><td>1.730</td><td></td><td>29</td><td>28.05</td><td>95</td><td>3.276</td></tr><tr><td><br />16</td><td>16.300</td><td>30.0</td><td>1.875</td><td></td><td>30</td><td>30.59</td><td>59</td><td>1.970</td></tr><tr><td><br />17</td><td>17.250</td><td>25.0</td><td>1.470</td><td></td><td></td><td></td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="144" name="144">4.4 <b>实验结果展示</b></h4>
                <div class="p1">
                    <p id="145">图9展示了本文网络在输入不同井下场景图片时的检测结果。从图9可以看出, 井下巷道行人检测定位算法取得了很好的检测及测距效果。</p>
                </div>
                <div class="area_img" id="146">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201903013_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同场景下的检测结果" src="Detail/GetImg?filename=images/JSJY201903013_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同场景下的检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201903013_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Detection results under different scenes</p>

                </div>
                <h3 id="147" name="147" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="148">针对井下巷道行人检测及距离测量问题, 提出了基于深度学习的端到端的系统检测模型。在行人检测模块设计中, 采用残差网络提取细粒度特征, 并提取多尺度特征;同时, 改进损失函数提高行人所在位置区域的精度。针对机车前方行人的距离测量问题, 利用大量数据训练拟合函数, 最终得到测距结果。但是, 本文的研究对于整体网络复杂度相对较高, 测距方法对于被遮挡行人测距检测误差较大, 仍有提高空间, 下一步的研究重点将放在以下两点:1) 保证检测精度的前提下, 降低网络复杂度;2) 提升测距精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="162">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SHQC200707013&amp;v=MjM0NjZHNEh0Yk1xSTlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3cklOaVhhYmI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>乔维高, 徐学进.无人驾驶汽车的发展现状及方向[J].上海汽车, 2007 (7) :40-43. (QIAO W G, XU X J.The development situation and direction of the driverless vehicle[J].Shanghai Auto, 2007 (7) :40-43.) 
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MKJX201704059&amp;v=MDM2NTFwRmlEbFc3cklLQ2JCZHJHNEg5Yk1xNDlBYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>李晓明, 郎文辉, 马忠磊, 等.基于图像处理的井下机车行人检测技术[J].煤矿机械, 2017, 38 (4) :167-170. (LI X M, LANG WH, MA Z L, et al.Pedestrian detection technology for mine locomotive based on image processing[J].Coal Mine Machinery, 2017, 38 (4) :167-170.) 
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extended faster R-CNN for long distance human detection:finding pedestrians in UAV images">

                                <b>[3]</b>LIU T, FU H Y, WEN Q, et al.Extended faster R-CNN for long distance human detection:finding pedestrians in UAV images[C]//Proceedings of the 2018 IEEE International Conference on Consumer Electronics.Piscataway, NJ:IEEE, 2018:1-2.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[4]</b>GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition">

                                <b>[5]</b>HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[6]</b>GIRSHICK R.Fast R-CNN[C]//Proceedings of the 2015 IEEEInternational Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better Faster,Stronger">

                                <b>[7]</b>REDMON J, FARHADI A.YOLO9000:better, faster, stronger[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6517-6525.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017725396.nh&amp;v=MDc0OTc3cklWRjI2R2JTNkc5TEZxWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>郑嘉祺.基于DCNN的井下行人检测系统的研究与设计[D].西安:西安科技大学, 2017:84-87. (ZHENG J Q.Research and design on pedestrian detection system under the mine based on DCNN[D].Xi'an:Xi'an University of Science and Technology, 2017:84-87.) 
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified real-time object detection">

                                <b>[9]</b>REDMON J, DIWALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2016:779-788.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                王琳, 卫晨, 李伟山, 等.结合金字塔池化模块的YOLOv2的井下行人检测[EB/OL].[2018-05-21].https://www.doc88.com/p-0714870779937.html. (WANG L, WEI C, LI W S, et al.Pedestrian detection based on YOLOv2 with pyramid pooling module in underground coal mine[EB/OL].[2018-05-21].https://www.doc88.com/p-0714870779937.html.) 
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                李伟山, 卫晨, 王琳.改进的Faster R-CNN煤矿井下行人检测算法[EB/OL].[2018-07-15].http://kns.cnki.net/kcms/detail/11.2127.TP.20180522.0944.002.html. (LI W S, WEI C, WANG L.An improved faster R-CNN approach for pedestrian detection in underground coal mine[EB/OL].[2018-07-15].http://kns.cnki.net/kcms/detail/11.2127.TP.20180522.0944.002.html.) 
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">

                                <b>[12]</b>REN S, HE K, GIRSHICK R, et al.Faster R-CNN:towards realtime object detection with region proposal networks[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2015, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201504013&amp;v=MjkzNjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGaURsVzdySUlUZklZckc0SDlUTXE0OUVaNFFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>沈彤, 刘文波, 王京.基于双目立体视觉的目标测距系统[J].电子测量技术, 2015, 38 (4) :52-54. (SHEN T, LIU W B, WANGJ.Distance measurement system based on binocular stereo vision[J].Electronic Measurement Technology, 2015, 38 (4) :52-54.) 
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200601011&amp;v=MTc5NjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRmlEbFc3cklQeXJmYkxHNEh0Zk1ybzlFWllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>郭磊, 徐友春, 李克强, 等.基于单目视觉的实时测距方法研究[J].中国图象图形学报, 2006, 11 (1) :74-81. (GUO L, XU YC, LI K Q, et al.Study on real-time distance detection based on monocular vision technique[J].Journal of Image and Graphics, 2006, 11 (1) :74-81.) 
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vehicle distance detection based on monocular vision">

                                <b>[15]</b>BAO D, WANG P.Vehicle distance detection based on monocular vision[C]//Proceedings of the 2016 International Conference on Progress in Informatics and Computing.Piscataway, NJ:IEEE, 2016:187-191.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focal Loss for Dense Object Detection">

                                <b>[16]</b>LIN T Y, GOYAL P, GIRSHICK R, et al.Focal loss for dense object detection[C]//Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2017:2999-3007.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201903013" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903013&amp;v=MTc1ODU3RzRIOWpNckk5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZpRGxXN3JJTHo3QmQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONFRYTUhIM2RrMzdIS2psYmhhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="3" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
