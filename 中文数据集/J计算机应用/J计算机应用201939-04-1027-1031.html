<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136775658252500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904016%26RESULT%3d1%26SIGN%3dFt%252buD9B3NVrDoOeTqx6WYT0CQKE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904016&amp;v=MjYxODlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWTDdJTHo3QmQ3RzRIOWpNcTQ5RVlvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 算法原理 ">1 算法原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="1.1 &lt;b&gt;连通距离阈值&lt;/b&gt;&lt;i&gt;D&lt;/i&gt;&lt;b&gt;和连通强度阈值&lt;/b&gt;&lt;i&gt;I&lt;/i&gt;">1.1 <b>连通距离阈值</b><i>D</i><b>和连通强度阈值</b><i>I</i></a></li>
                                                <li><a href="#53" data-title="1.2 &lt;b&gt;聚类特征向量融入到数据对象中&lt;/b&gt;">1.2 <b>聚类特征向量融入到数据对象中</b></a></li>
                                                <li><a href="#66" data-title="1.3 &lt;b&gt;聚类特征树的簇先合并再分裂&lt;/b&gt;">1.3 <b>聚类特征树的簇先合并再分裂</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 算法流程及实现 ">2 算法流程及实现</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#130" data-title="3 时间空间复杂度 ">3 时间空间复杂度</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="4 聚类应用 ">4 聚类应用</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#144" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="图1 本文算法流程">图1 本文算法流程</a></li>
                                                <li><a href="#136" data-title="图2 数据集1聚类结果">图2 数据集1聚类结果</a></li>
                                                <li><a href="#137" data-title="图3 数据集2聚类结果">图3 数据集2聚类结果</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同算法在&lt;/b&gt;&lt;i&gt;UCI&lt;/i&gt;&lt;b&gt;数据集上的聚类准确率和运行耗时的对比&lt;/b&gt;"><b>表</b>1 <b>不同算法在</b><i>UCI</i><b>数据集上的聚类准确率和运行耗时的对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="161">


                                    <a id="bibliography_1" title="ZHANG T, RAMAKRISHNAN R, LINVY M.BIRCH:an efficient data clustering method for very large databases[C]//SIGMOD1996:Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1996:103-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BIRCH: An efficient data clustering method for very large databases">
                                        <b>[1]</b>
                                        ZHANG T, RAMAKRISHNAN R, LINVY M.BIRCH:an efficient data clustering method for very large databases[C]//SIGMOD1996:Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1996:103-114.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_2" title="邵峰晶, 张斌, 于忠清.多阈值BIRCH聚类算法及其应用[J].计算机工程与应用, 2004, 41 (12) :174-176. (SHAO F J, ZHANG B, YU Z Q.BIRCH clustering algorithm with mult_threshold[J].Computer Engineering and Applications, 2004, 41 (12) :174-176.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG200412054&amp;v=MzI0NDdmWnVac0Z5RGdWTDdJTHo3TWFiRzRIdFhOclk5QVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        邵峰晶, 张斌, 于忠清.多阈值BIRCH聚类算法及其应用[J].计算机工程与应用, 2004, 41 (12) :174-176. (SHAO F J, ZHANG B, YU Z Q.BIRCH clustering algorithm with mult_threshold[J].Computer Engineering and Applications, 2004, 41 (12) :174-176.) 
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_3" title="蒋盛益, 李霞.一种改进的BIRCH聚类算法[J].计算机应用, 2009, 29 (1) :293-296. (JIANG S Y, LI X.Improved BIRCHclustering algorithm[J].Journal of Computer Applications, 2009, 29 (1) :293-296.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200901089&amp;v=MDk1OTFzRnlEZ1ZMN0lMejdCZDdHNEh0ak1ybzlOYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        蒋盛益, 李霞.一种改进的BIRCH聚类算法[J].计算机应用, 2009, 29 (1) :293-296. (JIANG S Y, LI X.Improved BIRCHclustering algorithm[J].Journal of Computer Applications, 2009, 29 (1) :293-296.) 
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_4" title="仰孝富.基于BIRCH改进算法的文本聚类研究[D].北京:北京林业大学, 2013:22-29. (YANG X F.Research of text clustering based on improved BIRCH[D].Beijing:Beijing Forestry University, 2013:22-29.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1013213981.nh&amp;v=Mjc4NTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURnVkw3SVZGMjZIYkc1SGRqRXJwRWJQSVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        仰孝富.基于BIRCH改进算法的文本聚类研究[D].北京:北京林业大学, 2013:22-29. (YANG X F.Research of text clustering based on improved BIRCH[D].Beijing:Beijing Forestry University, 2013:22-29.) 
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                    曾晓迪.一种基于K-mediods改进BIRCH的大数据聚类方法[D].昆明:云南财经大学, 2014:27-31. (ZENG X D.A big data clustering method based on K-mediods improved BIRCH[D].Kunming:Yunnan University of Finance and Economics, 2014:27-31.) </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_6" title="邹杰涛, 赵方霞, 汪海燕.基于加权相似性的聚类算法[J].数学的实践与认知, 2011, 47 (16) :118-124. (ZOU J T, ZHAO FX, WANG H Y.Clustering algorithm based on weighted similarity[J].Mathematics in Practice and Theory, 2011, 47 (16) :118-124.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SSJS201116021&amp;v=MDAyOTdMN0lOajdCZmJHNEg5RE5xWTlIWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        邹杰涛, 赵方霞, 汪海燕.基于加权相似性的聚类算法[J].数学的实践与认知, 2011, 47 (16) :118-124. (ZOU J T, ZHAO FX, WANG H Y.Clustering algorithm based on weighted similarity[J].Mathematics in Practice and Theory, 2011, 47 (16) :118-124.) 
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_7" title="HUANG Z X.A fast clustering algorithm to cluster very large categorical data sets in data mining[EB/OL].[2018-05-10].http://www.vladestivill-castro.net/teaching/kdd.d/readings.d/huang97fast.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fast clustering algorithm to cluster very large categorical data sets in data mining">
                                        <b>[7]</b>
                                        HUANG Z X.A fast clustering algorithm to cluster very large categorical data sets in data mining[EB/OL].[2018-05-10].http://www.vladestivill-castro.net/teaching/kdd.d/readings.d/huang97fast.pdf.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                    MACQUEEN J.Some methods for classification and analysis of multivariate observations[C]//Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability.Berkeley:University of California Press, 1967, 1:281-297.</a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_9" title="韦相.基于密度的改进BIRCH聚类算法[J].计算机工程与应用, 2013, 49 (10) :201-205. (WEI X.Improved BIRCH clustering algorithm based on density[J].Computer Engineering and Applications, 2013, 49 (10) :201-205.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201310052&amp;v=MjgxODl1WnNGeURnVkw3SUx6N01hYkc0SDlMTnI0OUFab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        韦相.基于密度的改进BIRCH聚类算法[J].计算机工程与应用, 2013, 49 (10) :201-205. (WEI X.Improved BIRCH clustering algorithm based on density[J].Computer Engineering and Applications, 2013, 49 (10) :201-205.) 
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_10" title="MADAN S, DANA K J.Modified Balanced Iterative Reducing and Clustering using Hierarchies (m-BIRCH) for visual clustering[J].Pattern Analysis and Applications, 2016, 19 (4) :1023-1040." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modified balanced iterative reducing and clustering using hierarchies (m-BIRCH)for visual clustering">
                                        <b>[10]</b>
                                        MADAN S, DANA K J.Modified Balanced Iterative Reducing and Clustering using Hierarchies (m-BIRCH) for visual clustering[J].Pattern Analysis and Applications, 2016, 19 (4) :1023-1040.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_11" title="ESTER M, KRIEGEL H, SANDER J, et a1.A density-based algorithm for discovering clusters in large spatial databases with noise[C]//Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.Menlo Park, CA:AAAI Press, 1996:226-231." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise">
                                        <b>[11]</b>
                                        ESTER M, KRIEGEL H, SANDER J, et a1.A density-based algorithm for discovering clusters in large spatial databases with noise[C]//Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.Menlo Park, CA:AAAI Press, 1996:226-231.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_12" title="AWWAD B, HASAN S, GAN J Q.Sequential EM for unsupervised adaptive Gaussian mixture model based classifier[C]//MLDM 2009:Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition, LNAI5632.Berlin:Springer, 2009:96-106." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sequential EM for unsupervised adaptive Gaussian mixture model based classifier">
                                        <b>[12]</b>
                                        AWWAD B, HASAN S, GAN J Q.Sequential EM for unsupervised adaptive Gaussian mixture model based classifier[C]//MLDM 2009:Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition, LNAI5632.Berlin:Springer, 2009:96-106.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_13" title="LAUER F, SCHNORR C.Spectral clustering of linear subspaces for motion segmentation[C]//Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:678-685." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral clustering of linear subspaces for motion segmentation">
                                        <b>[13]</b>
                                        LAUER F, SCHNORR C.Spectral clustering of linear subspaces for motion segmentation[C]//Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:678-685.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_14" title="NG A Y, JORDAN M I, WEISS Y.On spectral clustering:analysis and an algorithm[EB/OL].[2018-05-10].http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On spectral clustering:analysis and an algorithm">
                                        <b>[14]</b>
                                        NG A Y, JORDAN M I, WEISS Y.On spectral clustering:analysis and an algorithm[EB/OL].[2018-05-10].http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_15" title="MERZC J, MERPHY P.UCI repository of machine learning databases[DB/OL].[2018-03-19].http://archive.ics.uci.edu/ml/index.php." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=UCI repository of machine learning databases">
                                        <b>[15]</b>
                                        MERZC J, MERPHY P.UCI repository of machine learning databases[DB/OL].[2018-03-19].http://archive.ics.uci.edu/ml/index.php.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-11-16 13:53</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),1027-1031 DOI:10.11772/j.issn.1001-9081.2018081790            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于连通距离和连通强度的BIRCH改进算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A8%8A%E4%BB%B2%E6%AC%A3&amp;code=11617321&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">樊仲欣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%85%B4&amp;code=17334339&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王兴</a>
                                <a href="javascript:;">苗春生</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%B0%94%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%AE%9E%E9%AA%8C%E6%95%99%E5%AD%A6%E4%B8%AD%E5%BF%83&amp;code=0151773&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京信息工程大学大气与环境实验教学中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E4%BF%A1%E5%A4%A7%E6%B0%94%E8%B1%A1%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京信大气象科技有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为解决利用层次方法的平衡迭代规约和聚类 (BIRCH) 算法聚类结果依赖于数据对象的添加顺序, 且对非球状的簇聚类效果不好以及受簇直径阈值的限制每个簇只能包含数量相近的数据对象的问题, 提出一种改进的BIRCH算法。该算法用描述数据对象个体间连通性的连通距离和连通强度阈值替代簇直径阈值, 还将簇合并的步骤加入到聚类特征树的生成过程中。在自定义及iris、wine、 pendigits数据集上的实验结果表明, 该算法比多阈值BIRCH、密度改进BIRCH等现有改进算法的聚类准确率更高, 尤其在大数据集上比密度改进BIRCH准确率提高6个百分点, 耗时降低61%。说明该算法能够适用于在线实时增量数据, 可以识别非球形簇和体积不均匀簇, 具有去噪功能, 且时间和空间复杂度明显降低。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">层次聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%A8%E7%BA%BF%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">在线算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=BIRCH&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">BIRCH;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB%E7%89%B9%E5%BE%81%E6%A0%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类特征树;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *樊仲欣 (1981—) , 男, 江苏南京人, 工程师, 硕士, 主要研究方向:数据挖掘、神经网络;电子邮箱fan_zhong_xin@qq.com;
                                </span>
                                <span>
                                    王兴 (1983—) , 男, 江苏泰州人, 高级工程师, 博士, 主要研究方向:数据挖掘、神经网络;;
                                </span>
                                <span>
                                    苗春生 (1954—) , 男, 江苏南京人, 教授, 博士生导师, 主要研究方向:天气气候预测、气象信息工程与技术。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-30</p>

            </div>
                    <h1><b>Improved BIRCH clustering algorithm based on connectivity distance and intensity</b></h1>
                    <h2>
                    <span>FAN Zhongxin</span>
                    <span>WANG Xing</span>
                    <span>MIAO Chunsheng</span>
            </h2>
                    <h2>
                    <span>Experimental Teaching Center of Atmosphere and Environment, Nanjing University of Information Science & Technology</span>
                    <span>Nanjing Xinda Meteorological Science and Technology Company Limited</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Focusing on the issues that clustering results of Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) depend on the adding order of data objects, BIRCH has poor clustering effect on non-convex clusters, and each cluster of BIRCH can only contain a similar number of data objects because of the cluster diameter threshold, an improved BIRCH algorithm was proposed. In this algorithm, the cluster diameter threshold was replaced by connectivity distance and intensity threshold which described the connectivity between the data objects, and cluster merging step was added into the generation of cluster feature tree. Experimental result on custom and iris, wine, pendigits datasets show that the proposed algorithm has higher clustering accuracy than the existing improved algorithms such as multi-threshold BIRCH and density-improved BIRCH; especially on large datasets, the proposed algorithm has accuracy increased by 6 percentage points and running time reduced by 61% compared to density-improved BIRCH. The proposed algorithm can be applied to online real-time incremental data processing and identify non-convex clusters and clusters with uneven volume, has denoising function and significantly reduces time-complexity and space-complexity.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hierarchical%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hierarchical clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=on-line%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">on-line algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Balanced%20Iterative%20Reducing%20and%20Clustering%20using%20Hierarchies%20(BIRCH)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Cluster%20Feature%20(CF)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Cluster Feature (CF) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Cluster%20Feature%20Tree%20(CF%20Tree)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Cluster Feature Tree (CF Tree) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    FAN Zhongxin, born in 1981, M. S. , engineer. His research interests include data mining, neural network.;
                                </span>
                                <span>
                                    WANG Xing, born in 1983, Ph. D. , senior engineer. His research interests include data mining, neural network.;
                                </span>
                                <span>
                                    MIAO Chunsheng, born in 1954, professor. His research interests include weather and climate prediction, meteorological information engineering and technology.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-30</p>
                            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="34">利用层次方法的平衡迭代规约和聚类 (Balanced Iterative Reducing and Clustering using Hierarchies, BRICH) 算法<citation id="191" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是具有代表性的分层聚类算法, 该算法的特点在于在线实时运行, 计算流程简单, 算法时间空间效率高, 且可识别噪声。该算法通过构建一个聚类特征树 (Cluster Feature Tree, CF Tree) , 树中每个节点的聚类特征向量如下:</p>
                </div>
                <div class="p1">
                    <p id="35"><b><i>CF</i></b>= (<i>N</i>, <i>LS</i>, <i>SS</i>) </p>
                </div>
                <div class="p1">
                    <p id="36">其中:<i>N</i>是簇中对象数目;<i>LS</i>是<i>N</i>个对象线性和<mathml id="37"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>SS</i>是<i>N</i>个对象平方和<mathml id="38"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>。合并两个簇只需要两个聚类特征向量算术相加即可, 而计算簇距离、簇直径只需要用到 (<i>N</i>, <i>LS</i>, <i>SS</i>) 这三个值就足够了。但该算法的缺点在于结果依赖于数据对象的加入顺序, 对非球状的簇和对高维数据聚类效果不好, 而且受簇直径阈值<i>T</i>的限制, 每个簇只能包含数量相近的数据对象。</p>
                </div>
                <div class="p1">
                    <p id="39">在最初的BIRCH算法的基础上, 后期有不少学者对其进行了改进。邵峰晶等<citation id="197" type="reference"><link href="163" rel="bibliography" /><link href="165" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>提出了动态阈值及多阈值<i>T</i>的方法, 解决了各簇的数据对象数量差距大时使用统一<i>T</i>值导致的问题, 并引入了分割因子的概念消除了簇分裂中的不确定性。仰孝富<citation id="192" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了文本聚类算法TCBIBK (Text Clustering algorithm Based on Improved BIRCH and <i>K</i>-nearest neighbor) , 在BIRCH聚类前先做<i>K</i>近邻划分的数据预处理, 以提高聚类精度。曾晓迪等<citation id="198" type="reference"><link href="169" rel="bibliography" /><link href="171" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>则采用了二次聚类的办法, 在BIRCH的聚类基础上再对各簇用<i>K</i>中心点 (<i>K</i>-mediods) <citation id="193" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、<i>K</i>均值 (<i>K</i>-means) <citation id="194" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等算法进行聚类, 实现了发现任意形状簇的功能目标。韦相等<citation id="199" type="reference"><link href="177" rel="bibliography" /><link href="179" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>为了解决非球形簇的聚类问题, 直接改进了BIRCH算法, 先建立起多阈值的多棵聚类特征树代表各个簇, 再合并树, 用具有噪声的基于密度的聚类 (Density-Based Spatial Clustering of Applications with Noise, DBSCAN) 方法<citation id="195" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、高斯混合模型 (Gaussian Mixture Model, GMM) <citation id="196" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、<i>K</i>-means或谱聚类 (Spectral Clustering) <citation id="200" type="reference"><link href="185" rel="bibliography" /><link href="187" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>等方法进行全局聚类。</p>
                </div>
                <div class="p1">
                    <p id="40">上述各种BIRCH聚类改进算法均存在一些问题:动态阈值和多阈值<i>T</i>的方法不能突破球形簇限制;聚类预处理和二次聚类方法则都是结合多种已有算法, 并未对BIRCH算法进行本质性的提升, 且不适用于在线实时数据;而建立多棵聚类特征树的方法其算法比较繁琐, 时间空间复杂度高, 且由于需要全局聚类, 部分算法还需要根据具体应用选择聚类方式。因此, 本文提出一种改进的BIRCH算法 (简称连通改进BIRCH) , 该算法使用描述数据对象个体间连通性的连通距离和连通强度阈值替代簇直径<i>T</i>, 同时还将簇合并的步骤加入到聚类特征树的生成过程中, 以解决上述问题。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">1 算法原理</h3>
                <div class="p1">
                    <p id="42">本文借鉴了<i>DBSCAN</i>中区域半径<i>Eps</i>和邻域对象数阈值<i>MinPts</i>的概念, 对BIRCH算法进行了如下几点改进:</p>
                </div>
                <div class="p1">
                    <p id="43">1) 将簇直径阈值<i>T</i>改为簇的连通距离阈值<i>D</i>和连通强度阈值<i>I</i>。</p>
                </div>
                <div class="p1">
                    <p id="44">2) 将聚类特征向量由<b><i>CF</i></b>= (<i>N</i>, <i>LS</i>, <i>SS</i>) 改为<b><i>CF</i></b>= (<i>N</i>) , 同时在每个数据对象中增加一个数据项<i>M</i>, <i>M</i>为数据对象的模的平方。</p>
                </div>
                <div class="p1">
                    <p id="45">3) 每个新增的数据对象不再是先加入最近的簇再依据簇直径阈值分裂聚类特征树节点, 而是直接依据连通距离<i>D</i>和连通强度<i>I</i>, 将新加入的数据对象和已有的若干簇合并, 或者将新数据对象直接作为新簇加入聚类特征树, 再像BIRCH算法那样分裂节点最终生成聚类特征树。简而言之, 就是将原来BIRCH算法的簇分裂变为簇先合并再分裂的算法。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46">1.1 <b>连通距离阈值</b><i>D</i><b>和连通强度阈值</b><i>I</i></h4>
                <div class="p1">
                    <p id="47"><b>定义</b>1 连通距离。连通距离即两个簇之间最接近的数据对象的距离, 该距离用欧氏距离:<mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mo>=</mo><msqrt><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo>⋯</mo><mo>+</mo><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>, </mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>⋯</mo><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></math></mathml>和<i>y</i><sub>1</sub><i>y</i><sub>2</sub>…<i>y</i><sub><i>n</i></sub>分别为两个数据对象的向量表示。</p>
                </div>
                <div class="p1">
                    <p id="49"><b>定义</b>2 连通距离阈值。连通距离阈值即判断两个簇是否连通的阈值, 如两个簇间的连通距离小于等于连通距离阈值则为连通的簇, 反之则为不连通的簇。</p>
                </div>
                <div class="p1">
                    <p id="50"><b>定义</b>3 连通强度。连通强度即连通距离阈值范围内的数据对象个数。</p>
                </div>
                <div class="p1">
                    <p id="51"><b>定义</b>4 连通强度阈值。连通强度阈值即判断两个簇是否为强连通的阈值, 如两个簇间的连通强度大于等于连通强度阈值则为强连通簇 (或称为簇核) , 反之则为弱连通簇 (或称为干扰点) 。</p>
                </div>
                <div class="p1">
                    <p id="52">使用连通距离阈值<i>D</i>和连通强度阈值<i>I</i>代替簇直径阈值<i>T</i>, 该方法具有可以发现任意形状簇和排除干扰点干扰这两大优点。首先, 使用连通距离只描述簇中数据对象个体间的连通性, 而不像簇直径描述的是簇中数据对象的整体范围体积, 因此可以通过该连通性发现任意形状簇, 且适用于簇与簇数据对象数量差距大的情况。其次, 现实情况中的数据集往往因为主观的和客观的各种原因而存在干扰点, 使得数据的聚类达不到理想的效果, 所以为了排除干扰, 需要定义连通强度将簇中数据对象最密集的部位 (即簇核) 给筛选出来作为强连通簇, 而被筛选掉的数据对象则是弱连通簇或干扰点, 干扰点并不一定就是必须去除掉的噪点, 因为在很多情况下干扰点确实是客观存在的, 因此应将其作为簇的非簇核部分重新合并入簇中, 但在合并入簇的过程中如果干扰点与簇核的距离大于连通距离阈值<i>D</i>, 则可以确定此干扰点为噪点。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53">1.2 <b>聚类特征向量融入到数据对象中</b></h4>
                <div class="p1">
                    <p id="54">由于连通距离和连通强度描述的是数据对象个体间连通性的参数, 因此BIRCH算法原有<b><i>CF</i></b>聚类特征向量中的<i>LS</i>和<i>SS</i>聚类特征 (用于计算簇心和簇直径) 便不再需要, 取而代之的是计算簇与簇之间任意两个数据对象距离的参数, 该参数在计算簇与簇最近距离时需要用到。</p>
                </div>
                <div class="p1">
                    <p id="55">根据三角形余弦定理, 已知两边边长<i>A</i><sub>L</sub>和<i>B</i><sub>L</sub>及两边夹角<i>θ</i>求第三边边长<i>C</i><sub>L</sub>:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>=</mo><msqrt><mrow><mi>A</mi><msub><mrow></mrow><mtext>L</mtext></msub><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>B</mi><msub><mrow></mrow><mtext>L</mtext></msub><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mn>2</mn><mi>A</mi><msub><mrow></mrow><mtext>L</mtext></msub><mi>B</mi><msub><mrow></mrow><mtext>L</mtext></msub><mspace width="0.25em" /><mi>cos</mi><mspace width="0.25em" /><mi>θ</mi></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">那么现以坐标原点<i>O</i>和向量<b><i>a</i></b>、<b><i>b</i></b>为三角形的三个顶点, 则用向量<b><i>c</i></b>的模表示<b><i>a</i></b>和<b><i>b</i></b>间的距离, 其值为:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">c</mi><mo>|</mo></mrow><mo>=</mo><msqrt><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">a</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">b</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mn>2</mn><mrow><mo>|</mo><mi mathvariant="bold-italic">a</mi><mo>|</mo></mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">b</mi><mo>|</mo></mrow><mi>cos</mi><mspace width="0.25em" /><mi>θ</mi></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中根据向量内积的定义:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">A</mi><mo>→</mo></mover><mo>⋅</mo><mover accent="true"><mi mathvariant="bold-italic">B</mi><mo>→</mo></mover><mo>=</mo><mrow><mo>|</mo><mover accent="true"><mi mathvariant="bold-italic">A</mi><mo>→</mo></mover><mo>|</mo></mrow><mrow><mo>|</mo><mover accent="true"><mi mathvariant="bold-italic">B</mi><mo>→</mo></mover><mo>|</mo></mrow><mrow><mi>cos</mi></mrow><mspace width="0.25em" /><mi>θ</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61"><i>n</i>是向量的维度, 因此式 (2) 变换为:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">c</mi><mo>|</mo></mrow><mo>=</mo><msqrt><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">a</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">b</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mn>2</mn><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中:<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">a</mi><mo>|</mo></mrow></mrow></math></mathml>、<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">b</mi><mo>|</mo></mrow></mrow></math></mathml>为向量的模, 由此公式可知只要有数据对象的模和内积就可以求出距离, 因此在每个数据对象中增加一个数据项:模平方<i>M</i>= (<i>a</i><sub>1</sub><sup>2</sup>+<i>a</i><sub>2</sub><sup>2</sup>+…+<i>a</i><sub><i>n</i></sub><sup>2</sup>) 。该数据项可以优化距离的计算, 每次加入新的数据对象时就算好<i>M</i>的值存储下来, 后面计算任意数据对象到该数据对象的距离就主要计算一下内积就可以求出结果。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.3 <b>聚类特征树的簇先合并再分裂</b></h4>
                <div class="p1">
                    <p id="67">簇分裂的算法和<i>BIRCH</i>相同, 在此不再敷述, 而增加的簇合并的算法结合连通距离作为簇聚类的限制参数, 可以避免数据加入顺序随机导致的聚类结果随机。因为簇直径随着数据加入顺序的不同, 其值的变化方式也是不同的, 当其变化曲线为上凸形状时就有可能会将同一个簇的数据对象分割到不同簇中, 而<i>BIRCH</i>算法的簇只能分裂不能合并, 因此无法在加入一个可以连通各簇的数据对象时进行簇的合并, 所以需要采取簇先合并然后再分裂节点生成聚类特征树的算法解决这个问题。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 算法流程及实现</h3>
                <div class="p1">
                    <p id="69">算法流程步骤分为三步 (如图1, 其中<i>D</i>为连通距离阈值, <i>I</i>为连通强度阈值) :</p>
                </div>
                <div class="p1">
                    <p id="70">步骤1 数据预处理及簇的合并生成。预处理除了要读取连通距离阈值<i>D</i>、连通强度阈值<i>I</i>、叶子节点分支因子<i>L</i>、非叶子节点分支因子<i>B</i>这四个预置聚类参数以外, 还要不断地读取在线聚类数据, 并在每次读取时对新加数据对象追加一个数据项<i>M</i>, <i>M</i>=数据对象的模的平方。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904016_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法流程" src="Detail/GetImg?filename=images/JSJY201904016_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904016_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="72">簇的合并生成部分的程序伪代码如下。</p>
                </div>
                <div class="area_img" id="160">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201904016_16000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="160">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201904016_16001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="128">步骤2 筛选强连通簇插入聚类特征树, 该步骤和BIRCH算法分裂节点生成聚类特征树的流程一样, 在此不再赘述。</p>
                </div>
                <div class="p1">
                    <p id="129">步骤3 弱连通簇并入强连通簇并输出聚类特征树, 该步骤需要将先前筛选出的非簇核部分重新合并入簇中, 合并的方式使用最近距离的概念, 依次从弱连通簇的列表中找出距离聚类特征树底层各个簇核最近的一个弱连通簇, 将其加入到最近距离的那个簇核之中, 直到所有弱连通簇全部加入完毕为止, 但是距离大于阈值<i>D</i>的弱连通簇为噪点不加入。最后, 输出聚类特征树底层簇的数据对象数量<i>N</i>及各个数据对象, 还有聚类特征树其余各层的<i>N</i>值及各层间的父子节点隶属关系, 作为当前数据的在线聚类结果。</p>
                </div>
                <h3 id="130" name="130" class="anchor-tag">3 时间空间复杂度</h3>
                <div class="p1">
                    <p id="131">对于<i>BIRCH</i>算法, 其在给定数据占用内存空间为<i>MEM</i>时的空间复杂度为<i>O</i> (<i>MEM</i>) , 本文算法的空间复杂度为<i>O</i> (<i>MEM</i>+<i>MEM</i>/<i>d</i>) , 其中<i>d</i>表示数据维度。</p>
                </div>
                <div class="p1">
                    <p id="132">BIRCH算法加入数据对象的时间复杂度为<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>d</mi><mi>Ν</mi><mi>B</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mi>B</mi></msub><mfrac><mi>Μ</mi><mi>Ρ</mi></mfrac><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow></math></mathml>。本文算法时间复杂度为<i>O</i> (BIRCH) +<i>O</i> (<i>HN</i>+<i>kHN</i>) +<i>O</i> (<i>sn</i>) , 其中:<i>HN</i>为已加入的数据对象个数, <i>k</i>为与新加数据对象距离在阈值<i>D</i>内的数据个数, <i>sn</i>为强连通簇所有数据对象个数。增加的<i>O</i> (<i>HN</i>+<i>kHN</i>) 和<i>O</i> (<i>sn</i>) 为图1步骤一和步骤三的时间复杂度, 考虑到数据密集时<i>k</i>值会比较大, 因此本文算法主要时间复杂度为<i>O</i> (BIRCH) +<i>O</i> (<i>kHN</i>) 。</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">4 聚类应用</h3>
                <div class="p1">
                    <p id="135">本文算法使用<i>Windows</i>7 64位专业版操作系统和<i>Eclipse Luna Service Release</i> 2 (4.4.2) 编程软件基于一台<i>CPU Inter core i</i>5-4570 3.2 <i>GHz</i>, 内存8 <i>GB</i>的<i>PC</i>上实现。</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904016_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 数据集1聚类结果" src="Detail/GetImg?filename=images/JSJY201904016_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 数据集1聚类结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904016_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Clustering results on dataset</i> 1</p>

                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904016_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 数据集2聚类结果" src="Detail/GetImg?filename=images/JSJY201904016_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 数据集2聚类结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904016_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Clustering results on dataset</i> 2</p>

                </div>
                <div class="area_img" id="138">
                                            <p class="img_tit">
                                                <b>表</b>1 <b>不同算法在</b><i>UCI</i><b>数据集上的聚类准确率和运行耗时的对比</b>
                                                    <br />
                                                <i>Tab</i>. 1 <i>Comparison of accuracy and running time of different algorithms on UCI datasets</i>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904016_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201904016_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904016_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 不同算法在UCI数据集上的聚类准确率和运行耗时的对比" src="Detail/GetImg?filename=images/JSJY201904016_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="139">为验证其有效性, 使用五个数据集比较本文算法 (连通改进<i>BIRCH</i>) 与<i>BIRCH</i>以及多阈值<i>BIRCH</i><citation id="201" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、密度改进<i>BIRCH</i><citation id="202" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>这四种在线实时运行算法的聚类结果。数据集1为自定义数据集:2维3 050个数据对象, 呈大小簇分布, 带有噪点;数据集2为自定义数据集:2维400个数据对象, 呈环形和圆形分布;数据集3为<i>iris</i>数据集:4维150个数据对象;数据集4为<i>wine</i>均一化后数据集:13维178个数据对象;数据集5为<i>pendigits</i>数据集:16维10 992个数据对象。最后三组数据集均来源于加州大学欧文分校机器学习库 (<i>University of California Irvine Machine Learning Repository</i>) <citation id="203" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。在<i>iris</i>、<i>wine</i>和<i>pendigits</i>数据集中, 每个数据点的类标签都是已知的, 因此可以充分利用准确率来评价算法的聚类质量。令<i>AD</i>表示数据集中的数据个数, <i>CD</i>表示<i>AD</i>中被正确划分的数据个数, 则准确率为<i>Correct</i>_<i>rate</i>=<i>CD</i>/<i>AD</i>*100%。</p>
                </div>
                <div class="p1">
                    <p id="140">从图2可看出, 对于具有大小簇以及包含噪点的非球形数据集1:BIRCH算法无法分割出大小簇及噪点;多阈值BIRCH算法虽然分割出了大小簇, 但由于其识别非球形簇的能力很有限, 所以图中的弧形簇被分割了开来且不能识别噪点;密度改进和连通改进BIRCH算法则解决了这些问题, 可以分割大小簇, 识别弧形簇, 以及去噪点。图3的数据集2含有更加不易识别的环形簇, 但密度改进和连通改进BIRCH算法依旧表现良好, 可以将环形簇分割出来, 而多阈值BIRCH算法则表现一般, BIRCH算法效果最差。值得一提的是, 上述BIRCH和多阈值BIRCH算法的聚类结果具有一定随机性, 因为其结果依赖于数据的加入先后顺序, 而密度改进和连通改进BIRCH算法则解决了这一问题。</p>
                </div>
                <div class="p1">
                    <p id="141">使用UCI数据集对上述四种BIRCH聚类算法进行测试, 结果如表1, 其中:参数<i>B</i> (非叶子节点分支因子) 与<i>L</i> (叶子节点分支因子) 对所有算法及数据集的取值均为2, 表中仅列出取值不一样的参数。</p>
                </div>
                <div class="p1">
                    <p id="142">从表1可看出:BIRCH算法耗时最短, 但在大数据集上的准确率也是最低的;多阈值BIRCH准确率有所提高;密度改进BIRCH和连通改进BIRCH则准确率提升比较明显但同时耗时也增加不少, 而其中连通改进BIRCH算法则在大数据集上明显更优一些, 其在pendigits数据集上的运行准确率比密度改进BIRCH提高6个百分点, 且耗时减少55.2 s, 降低了密度改进BIRCH算法61%的耗时量。连通改进BIRCH算法在功能上和密度改进BIRCH算法接近, 但是耗时明显较短。原因如下:</p>
                </div>
                <div class="p1">
                    <p id="143">密度改进BIRCH由于构建了多棵聚类特征树所以首先在空间复杂度上明显高于连通改进BIRCH。其次, 密度改进BIRCH中用<i>Eps</i>和<i>MinPts</i>代替了聚类特征树的簇直径<i>T</i>, 而加入数据对象时“代表点密度增加, 当它变成核心点后, 还要扫描所有核心点聚类特征树, 归入邻近核心点”<citation id="204" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 这就相当于连通改进算法流程 (见图1) 的步骤一中<i>n</i>&gt;<i>I</i>为否的情况 (其耗时等于本文算法的主要增加时间复杂度<i>O</i> (<i>kHN</i>) ) , 考虑到两种算法前后增加的计算过程, 尤其是密度改进算法还有多次合并多棵聚类特征树的分裂节点、更新聚类特征值等较为复杂的操作, 所以可见连通改进BIRCH算法耗时更短且实现更为简单。</p>
                </div>
                <h3 id="144" name="144" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="145">本文针对<i>BIRCH</i>及其现有改进算法在不能在线实时运行、只能聚类球形簇、聚类结果依赖于数据对象的添加顺序、每个簇只能包含数量相近的数据对象、算法时间空间复杂度高等方面存在的一个或若干个问题, 提出了可以解决上述问题的改进算法。本文连通改进<i>BIRCH</i>算法融合了<i>DBSCAN</i>算法区域半径和邻域对象数阈值的概念, 用其代替簇直径阈值<i>T</i>, 并且采用簇先合并再分裂的方法, 提高了BIRCH算法的适用范围和运行性能。时间空间复杂度及实验结果表明, 该算法在损失有限的运算时间和存储空间的前提下可以有效地提高聚类准确率, 并且在处理大数据集时依然具有不错的性能表现。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="161">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BIRCH: An efficient data clustering method for very large databases">

                                <b>[1]</b>ZHANG T, RAMAKRISHNAN R, LINVY M.BIRCH:an efficient data clustering method for very large databases[C]//SIGMOD1996:Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data.New York:ACM, 1996:103-114.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG200412054&amp;v=MTE3MjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEZ1ZMN0lMejdNYWJHNEh0WE5yWTlBWUlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>邵峰晶, 张斌, 于忠清.多阈值BIRCH聚类算法及其应用[J].计算机工程与应用, 2004, 41 (12) :174-176. (SHAO F J, ZHANG B, YU Z Q.BIRCH clustering algorithm with mult_threshold[J].Computer Engineering and Applications, 2004, 41 (12) :174-176.) 
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200901089&amp;v=MDM1MjhadVpzRnlEZ1ZMN0lMejdCZDdHNEh0ak1ybzlOYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>蒋盛益, 李霞.一种改进的BIRCH聚类算法[J].计算机应用, 2009, 29 (1) :293-296. (JIANG S Y, LI X.Improved BIRCHclustering algorithm[J].Journal of Computer Applications, 2009, 29 (1) :293-296.) 
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1013213981.nh&amp;v=MTQ1MDZxZlp1WnNGeURnVkw3SVZGMjZIYkc1SGRqRXJwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>仰孝富.基于BIRCH改进算法的文本聚类研究[D].北京:北京林业大学, 2013:22-29. (YANG X F.Research of text clustering based on improved BIRCH[D].Beijing:Beijing Forestry University, 2013:22-29.) 
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                曾晓迪.一种基于K-mediods改进BIRCH的大数据聚类方法[D].昆明:云南财经大学, 2014:27-31. (ZENG X D.A big data clustering method based on K-mediods improved BIRCH[D].Kunming:Yunnan University of Finance and Economics, 2014:27-31.) 
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SSJS201116021&amp;v=MDgxMTdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWTDdJTmo3QmZiRzRIOUROcVk5SFo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>邹杰涛, 赵方霞, 汪海燕.基于加权相似性的聚类算法[J].数学的实践与认知, 2011, 47 (16) :118-124. (ZOU J T, ZHAO FX, WANG H Y.Clustering algorithm based on weighted similarity[J].Mathematics in Practice and Theory, 2011, 47 (16) :118-124.) 
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fast clustering algorithm to cluster very large categorical data sets in data mining">

                                <b>[7]</b>HUANG Z X.A fast clustering algorithm to cluster very large categorical data sets in data mining[EB/OL].[2018-05-10].http://www.vladestivill-castro.net/teaching/kdd.d/readings.d/huang97fast.pdf.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                MACQUEEN J.Some methods for classification and analysis of multivariate observations[C]//Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability.Berkeley:University of California Press, 1967, 1:281-297.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201310052&amp;v=MDY4OTFzRnlEZ1ZMN0lMejdNYWJHNEg5TE5yNDlBWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>韦相.基于密度的改进BIRCH聚类算法[J].计算机工程与应用, 2013, 49 (10) :201-205. (WEI X.Improved BIRCH clustering algorithm based on density[J].Computer Engineering and Applications, 2013, 49 (10) :201-205.) 
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modified balanced iterative reducing and clustering using hierarchies (m-BIRCH)for visual clustering">

                                <b>[10]</b>MADAN S, DANA K J.Modified Balanced Iterative Reducing and Clustering using Hierarchies (m-BIRCH) for visual clustering[J].Pattern Analysis and Applications, 2016, 19 (4) :1023-1040.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise">

                                <b>[11]</b>ESTER M, KRIEGEL H, SANDER J, et a1.A density-based algorithm for discovering clusters in large spatial databases with noise[C]//Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.Menlo Park, CA:AAAI Press, 1996:226-231.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sequential EM for unsupervised adaptive Gaussian mixture model based classifier">

                                <b>[12]</b>AWWAD B, HASAN S, GAN J Q.Sequential EM for unsupervised adaptive Gaussian mixture model based classifier[C]//MLDM 2009:Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition, LNAI5632.Berlin:Springer, 2009:96-106.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral clustering of linear subspaces for motion segmentation">

                                <b>[13]</b>LAUER F, SCHNORR C.Spectral clustering of linear subspaces for motion segmentation[C]//Proceedings of the 2009 IEEE 12th International Conference on Computer Vision.Piscataway, NJ:IEEE, 2009:678-685.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On spectral clustering:analysis and an algorithm">

                                <b>[14]</b>NG A Y, JORDAN M I, WEISS Y.On spectral clustering:analysis and an algorithm[EB/OL].[2018-05-10].http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=UCI repository of machine learning databases">

                                <b>[15]</b>MERZC J, MERPHY P.UCI repository of machine learning databases[DB/OL].[2018-03-19].http://archive.ics.uci.edu/ml/index.php.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904016&amp;v=MjYxODlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGdWTDdJTHo3QmQ3RzRIOWpNcTQ5RVlvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
