<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136664928752500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907039%26RESULT%3d1%26SIGN%3dAUgv%252bAcodp3Fx0nNkZFlT9PKDN8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907039&amp;v=MDU2MTJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dVNzNPTHo3QmQ3RzRIOWpNcUk5R2JZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 算法流程 ">1 算法流程</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#45" data-title="2 数据集准备 ">2 数据集准备</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="3 人眼检测与状态识别 ">3 人眼检测与状态识别</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="3.1 &lt;b&gt;人眼检测&lt;/b&gt;">3.1 <b>人眼检测</b></a></li>
                                                <li><a href="#64" data-title="3.2 &lt;b&gt;人眼状态识别&lt;/b&gt;">3.2 <b>人眼状态识别</b></a></li>
                                                <li><a href="#76" data-title="3.3 &lt;b&gt;卷积神经网络训练&lt;/b&gt;">3.3 <b>卷积神经网络训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="4 实验结果及分析 ">4 实验结果及分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#110" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="图1 疲劳检测流程">图1 疲劳检测流程</a></li>
                                                <li><a href="#62" data-title="图2 人脸水平灰度积分投影">图2 人脸水平灰度积分投影</a></li>
                                                <li><a href="#63" data-title="图3 人眼定位网络结构">图3 人眼定位网络结构</a></li>
                                                <li><a href="#66" data-title="图4 人眼的6个特征点">图4 人眼的6个特征点</a></li>
                                                <li><a href="#70" data-title="图5 特征点检测网络结构">图5 特征点检测网络结构</a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同方法的人眼定位准确率和检测速率对比&lt;/b&gt;"><b>表</b>1 <b>不同方法的人眼定位准确率和检测速率对比</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同姿态下人眼定位结果&lt;/b&gt;"><b>表</b>2 <b>不同姿态下人眼定位结果</b></a></li>
                                                <li><a href="#105" data-title="图6 各特征点检测准确率">图6 各特征点检测准确率</a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同人眼开闭度下人眼状态检测&lt;/b&gt;"><b>表</b>3 <b>不同人眼开闭度下人眼状态检测</b></a></li>
                                                <li><a href="#109" data-title="图7 模拟清醒与疲劳状态下人眼开闭度">图7 模拟清醒与疲劳状态下人眼开闭度</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="128">


                                    <a id="bibliography_1" title=" YOU Z, GAO Y, ZHANG J, et al.A study on driver fatigue recognition based on SVM method [C]// Proceedings of the 2017 4th International Conference on Transportation Information and Safety.Piscataway, NJ:IEEE, 2017:693-697." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A study on driver fatigue recognition based on SVM method">
                                        <b>[1]</b>
                                         YOU Z, GAO Y, ZHANG J, et al.A study on driver fatigue recognition based on SVM method [C]// Proceedings of the 2017 4th International Conference on Transportation Information and Safety.Piscataway, NJ:IEEE, 2017:693-697.
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_2" title=" CHAI R, NAIK G, NGUYEN T N, et al.Driver fatigue classification with independent component by entropy rate bound minimization analysis in an EEG-based system [J].IEEE Journal of Biomedical and Health Informatics, 2017, 21 (3) :715-724." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Driver Fatigue Classification With Independent Component by Entropy Rate Bound Minimization Analysis in an EEG-Based System">
                                        <b>[2]</b>
                                         CHAI R, NAIK G, NGUYEN T N, et al.Driver fatigue classification with independent component by entropy rate bound minimization analysis in an EEG-based system [J].IEEE Journal of Biomedical and Health Informatics, 2017, 21 (3) :715-724.
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_3" title=" XU J, MIN J, HU J.Real-time eye tracking for the assessment of driver fatigue [J].Healthcare Technology Letters, 2018, 5 (2) :54-58." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time eye tracking for the assessment of driver fatigue">
                                        <b>[3]</b>
                                         XU J, MIN J, HU J.Real-time eye tracking for the assessment of driver fatigue [J].Healthcare Technology Letters, 2018, 5 (2) :54-58.
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_4" title=" 唐广发, 张会林.人眼疲劳预测技术的研究[J].计算机工程与应用, 2016, 52 (9) :213-218. (TANG G F, ZHANG H L.Research on human eye fatigue prediction technology[J].Computer Engineering and Applications, 2016, 52 (9) :213-218.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201609039&amp;v=MDMxMTVnVTczT0x6N01hYkc0SDlmTXBvOUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         唐广发, 张会林.人眼疲劳预测技术的研究[J].计算机工程与应用, 2016, 52 (9) :213-218. (TANG G F, ZHANG H L.Research on human eye fatigue prediction technology[J].Computer Engineering and Applications, 2016, 52 (9) :213-218.) 
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_5" title=" ZENG S, LI J, JIANG L, et al.A driving assistant safety method based on human eye fatigue detection [C]// Proceedings of the 2017 Control and Decision Conference.Piscataway, NJ:IEEE, 2017:6370-6377." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A driving assistant safety method based on human eye fatigue detection">
                                        <b>[5]</b>
                                         ZENG S, LI J, JIANG L, et al.A driving assistant safety method based on human eye fatigue detection [C]// Proceedings of the 2017 Control and Decision Conference.Piscataway, NJ:IEEE, 2017:6370-6377.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_6" title=" DENG Z, JING R, JIAO L, et al.Fatigue detection based on isophote curve [C]// Proceedings of the 2015 International Conference on Computer and Computational Sciences.Piscataway, NJ:IEEE, 2015:146-150." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fatigue detection based on isophote curve">
                                        <b>[6]</b>
                                         DENG Z, JING R, JIAO L, et al.Fatigue detection based on isophote curve [C]// Proceedings of the 2015 International Conference on Computer and Computational Sciences.Piscataway, NJ:IEEE, 2015:146-150.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_7" title=" 李响, 谭南林, 李国正, 等.基于Zernike矩的人眼定位与状态识别[J].电子测量与仪器学报, 2015 (3) :390-398. (LI X, TAN N L, LI G Z, et al.Human eye localization and state recognition based on Zernike moment[J].Journal of Electronic Measurement and Instrumentation, 2015 (3) :390-398) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZIY201503012&amp;v=MTQ1MDhadVpzRnkvZ1U3M09JVGZDZDdHNEg5VE1ySTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         李响, 谭南林, 李国正, 等.基于Zernike矩的人眼定位与状态识别[J].电子测量与仪器学报, 2015 (3) :390-398. (LI X, TAN N L, LI G Z, et al.Human eye localization and state recognition based on Zernike moment[J].Journal of Electronic Measurement and Instrumentation, 2015 (3) :390-398) 
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_8" title=" ARAUJO G M, FML R, JUNIOR W S, et al.Weak classifier for density estimation in eye localization and tracking[J].IEEE Transactions on Image Processing, 2017, 26 (7) :3410-3424." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weak classifier for density estimation in eye localization and tracking">
                                        <b>[8]</b>
                                         ARAUJO G M, FML R, JUNIOR W S, et al.Weak classifier for density estimation in eye localization and tracking[J].IEEE Transactions on Image Processing, 2017, 26 (7) :3410-3424.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_9" title=" SONG M, TAO D, SUN Z, et al.Visual-context boosting for eye detection[J].IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) , 2010, 40 (6) :1460-1467." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual-Context Boosting for Eye Detection">
                                        <b>[9]</b>
                                         SONG M, TAO D, SUN Z, et al.Visual-context boosting for eye detection[J].IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) , 2010, 40 (6) :1460-1467.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_10" title=" LI J, WONG H C, LO S L, et al.Multiple object detection by a deformable part-based model and an R-CNN[J].IEEE Signal Processing Letters, 2018, 25 (2) :288-292." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiple object detection by a deformable part-based model and an R-CNN">
                                        <b>[10]</b>
                                         LI J, WONG H C, LO S L, et al.Multiple object detection by a deformable part-based model and an R-CNN[J].IEEE Signal Processing Letters, 2018, 25 (2) :288-292.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_11" title=" LI J, LIANG X, SHEN S M, et al.Scale-aware fast R-CNN for pedestrian detection [J].IEEE Transactions on Multimedia, 2018, 20 (4) :985-996." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scale-aware fast R-CNN for pedestrian detection">
                                        <b>[11]</b>
                                         LI J, LIANG X, SHEN S M, et al.Scale-aware fast R-CNN for pedestrian detection [J].IEEE Transactions on Multimedia, 2018, 20 (4) :985-996.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_12" title=" ABDULNABI A H, WANG G, LU J, et al.Multi-task CNN model for attribute prediction [J].IEEE Transactions on Multimedia, 2015, 17 (11) :1949-1959." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-task CNN model for attribute prediction">
                                        <b>[12]</b>
                                         ABDULNABI A H, WANG G, LU J, et al.Multi-task CNN model for attribute prediction [J].IEEE Transactions on Multimedia, 2015, 17 (11) :1949-1959.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_13" title=" LUO Y, GUAN Y P.Adaptive skin detection using face location and facial structure estimation [J].IET Computer Vision, 2017, 11 (7) :550-559." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive skin detection using face location and facial structure estimation">
                                        <b>[13]</b>
                                         LUO Y, GUAN Y P.Adaptive skin detection using face location and facial structure estimation [J].IET Computer Vision, 2017, 11 (7) :550-559.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_14" title=" YANG W, ZHANG Z, ZHANG Y, et al.Real-time digital image stabilization based on regional field image gray projection[J].Journal of Systems Engineering and Electronics, 2016, 27 (1) :224-231." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTGJ201601023&amp;v=MDQwNTh0R0ZyQ1VSN3FmWnVac0Z5L2dVNzNPUFRuTVpMRzRIOWZNcm85SFo0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         YANG W, ZHANG Z, ZHANG Y, et al.Real-time digital image stabilization based on regional field image gray projection[J].Journal of Systems Engineering and Electronics, 2016, 27 (1) :224-231.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_15" title=" K&#214;NIG D, ADAM M, JARVERS C, et al.Fully convolutional region proposal networks for multispectral person detection[C]// Proceedings of the 2017 Computer Vision and Pattern Recognition Workshops.Washington, DC:IEEE Computer Society, 2017:243-250." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Region Proposal Networks for Multispectral Person Detection">
                                        <b>[15]</b>
                                         K&#214;NIG D, ADAM M, JARVERS C, et al.Fully convolutional region proposal networks for multispectral person detection[C]// Proceedings of the 2017 Computer Vision and Pattern Recognition Workshops.Washington, DC:IEEE Computer Society, 2017:243-250.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_16" title=" FU L, ZHANG J, HUANG K.Mirrored non-maximum suppression for accurate object part localization [C]// Proceedings of the 2015 3rd IAPR Asian Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2015:51-55." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mirrored non-maximum suppression for accurate object part localization">
                                        <b>[16]</b>
                                         FU L, ZHANG J, HUANG K.Mirrored non-maximum suppression for accurate object part localization [C]// Proceedings of the 2015 3rd IAPR Asian Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2015:51-55.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_17" title=" MANDAL B, LI L, WANG G S, et al.Towards detection of bus driver fatigue based on robust visual analysis of eye state [J].IEEE Transactions on Intelligent Transportation Systems, 2017, 18 (3) :545-557." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards detection of bus driver fatigue based on robust visual analysis of eye state">
                                        <b>[17]</b>
                                         MANDAL B, LI L, WANG G S, et al.Towards detection of bus driver fatigue based on robust visual analysis of eye state [J].IEEE Transactions on Intelligent Transportation Systems, 2017, 18 (3) :545-557.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-08 14:58</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),2098-2102 DOI:10.11772/j.issn.1001-9081.2018122441            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于人眼信息特征的人体疲劳检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BD%97%E5%85%83&amp;code=11420285&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">罗元</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%91%E6%98%8E%E9%9D%99&amp;code=41650523&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">云明静</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%89%BA&amp;code=42202203&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王艺</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E7%AB%8B%E6%98%8E&amp;code=37747834&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵立明</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E5%85%89%E7%94%B5%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学光电工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E5%85%88%E8%BF%9B%E5%88%B6%E9%80%A0%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学先进制造工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>人眼状态是反映疲劳程度的重要指标, 头部姿势变化、光线等因素对人眼定位造成很大影响, 从而影响人眼状态识别以及疲劳检测的准确性, 为此提出了一种利用级联卷积神经网络通过检测人眼6个特征点来识别人眼状态进而识别人体疲劳的方法。首先, 一级网络采用灰度积分投影结合区域—卷积神经网实现人眼的检测与定位;然后, 二级网络将人眼图片进行分割后采用并联子卷积系统进行人眼特征点回归;最后, 利用人眼特征点计算人眼开闭度识别当前人眼状态, 并根据单位时间闭眼百分比 (PERCLOS) 准则判断人体疲劳状态。实验结果表明, 利用所提方法实现了在归一化误差为0.05时, 人眼6特征点的平均检测准确率为95.8%, 并根据模拟视频帧的PERCLOS值识别疲劳状态验证了该方法的有效性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%81%B0%E5%BA%A6%E7%A7%AF%E5%88%86%E6%8A%95%E5%BD%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">灰度积分投影;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E7%9C%BC%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人眼定位;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E7%9C%BC%E7%8A%B6%E6%80%81%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人眼状态识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%96%B2%E5%8A%B3%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">疲劳检测;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    罗元 (1972—) , 女, 湖北宜昌人, 教授, 博士, 主要研究方向:数字图像处理;;
                                </span>
                                <span>
                                    *云明静 (1994—) , 男, 四川绵阳人, 硕士研究生, 主要研究方向:机器视觉;电子邮箱975727433@qq.com;
                                </span>
                                <span>
                                    王艺 (1999—) , 女, 重庆渝北人, 主要研究方向:光信息处理;;
                                </span>
                                <span>
                                    赵立明 (1981—) , 男, 河北秦皇岛人, 讲师, 博士, 主要研究方向:机器视觉。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>重庆市科委基础与前沿研究计划项目 (cstc2016jcyjA0537);</span>
                    </p>
            </div>
                    <h1><b>Human fatigue detection based on eye information characteristics</b></h1>
                    <h2>
                    <span>LUO Yuan</span>
                    <span>YUN Mingjing</span>
                    <span>WANG Yi</span>
                    <span>ZHAO Liming</span>
            </h2>
                    <h2>
                    <span>College of Optoelectronic Engineering, Chongqing University of Posts and Telecommunications</span>
                    <span>College of Advanced Manufacturing Engineering, Chongqing University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The eye state is an important indicator reflecting the degree of fatigue. Changes in head posture and light have a great influence on human eye positioning, which affects the accuracy of eye state recognition and fatigue detection. A cascade Convolutional Neural Network (CNN) was proposed, by which the human eye state could be identified by detecting six feature points of human eye to identify human body fatigue. Firstly, grayscale integral projection and regional-convolution neural network were used as the first-level network to realize the positioning and detection of human eyes. Then, the secondary network was adopted to divide the human eye image and parallel sub-convolution system was used to perform human eye feature point regression. Finally, human eye feature points were used to calculate the human eye opening and closing degree to identify the current eye state, and the human body fatigue state was judged according to the PERcentage of eyelid CLOSure over the pupil time (PERCLOS) criterion. The experimental results show that the average detection accuracy of six eye feature points reaches 95.8% when the normalization error is 0.05, thus the effectiveness of the proposed method is verified by identifying the fatigue state based on the PERCLOS value of analog video.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=grayscale%20integral%20projection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">grayscale integral projection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=eye%20positioning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">eye positioning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=eye%20state%20identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">eye state identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fatigue%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fatigue detection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LUO Yuan, born in 1972, Ph. D. , professor. Her research interests include digital image processing. ;
                                </span>
                                <span>
                                    YUN Mingjing, born in 1994, M. S. candidate. His research interests include machine vision. ;
                                </span>
                                <span>
                                    WANG Yi, born in 1999. Her research interests include optical information processing. ;
                                </span>
                                <span>
                                    ZHAO Liming, born in 1981, Ph. D. , lecturer. His research interests include machine vision.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Basic and Frontier Research Project of Chongqing Municipal Science and Technology Commission (cstc2016jcyjA0537);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="38">疲劳是指在一定环境条件下因机体长时间从事体力或脑力劳动而引起的劳动效率趋向下降的状态。疲劳不仅是多种慢性疾病的起源, 更重要的是疲劳在某些领域会对社会安全造成重大危害, 尤其在高空建筑作业、车辆驾驶、大型复杂工业等高风险作业中, 每年因人员疲劳操作造成的事故数量巨大。目前, 疲劳检测主要分为基于生理特征信号检测、基于视觉特征检测<citation id="162" type="reference"><link href="128" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 对于驾驶疲劳检测还包括驾驶行为的检测方式。基于生理特征信号的检测方式具有较高的准确率, 但是该方法信号采集设备复杂, 相关设备的小型化与实用化也仍有不足, 因此目前的主要研究还是在实验室进行<citation id="163" type="reference"><link href="130" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>;而基于视觉检测的方式在保持较高准确率的同时设备要求低, 同时具有非入侵性特点<citation id="164" type="reference"><link href="132" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 是疲劳检测的主要研究方向之一。</p>
                </div>
                <div class="p1">
                    <p id="39">基于眼睛状态的疲劳检测主要包括人眼定位以及人眼状态的识别<citation id="169" type="reference"><link href="134" rel="bibliography" /><link href="136" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。Deng等<citation id="165" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>利用肤色模型结合人脸三庭五眼的布局定位人眼, 并利用人眼的积分投影区域大小识别人眼状态;这种方法虽然算法简单, 但是定位的准确率受环境光照影响较大, 且由于人眼区域在图像中占的比例很小, 利用积分投影的人眼状态识别准确率较低。李响等<citation id="166" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>利用图像的矩特征通过计算人眼模板的Zernike矩特征向量与待识别的人脸区域作相似度计算, 选取相似度最大的区域作为人眼区域;该方法虽然能够减小环境光照对于检测结果的影响, 但计算量较大, 并且结果受选取的人眼模板影响较大。在人眼状态识别中, 传统人眼状态识别方法通过对检测的人眼区域进行人眼形状拟合<citation id="167" type="reference"><link href="142" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>或者对人眼区域采用积分投影根据投影区域的宽度<citation id="168" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>识别人眼的张开程度。形状拟合的方法不仅计算复杂而且在头部姿态变化时容易失真, 积分投影的方法要求检测的人眼区域完全匹配人眼的上下眼睑, 实际情况下很难做到, 容易造成投影区域宽度过大或过小影响检测结果。</p>
                </div>
                <div class="p1">
                    <p id="40">针对这些传统算法在实际环境中易受头部姿势变化、光线等因素的干扰, 以卷积神经网络 (Convolutional Neural Network, CNN) 为代表的深度学习在图像检测领域具有较高的准确性同时能具有较强的鲁棒性, 在目标检测方面就有区域—卷积神经网络 (Region-Convolutional Neural Network, R-CNN) <citation id="170" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、快速区域—卷积神经网 (fast Region-Convolutional Neural Network, fast R-CNN) <citation id="171" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、多任务卷积神经网 (Mutil-task-Convolutional Neural Network, Mutil-task-CNN) <citation id="172" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等, 但是这些网络结构较为复杂, 常用于图像中的多目标检测, 本文针对人脸图像设计了一种级联神经网络结构来检测人眼以及人眼特征点, 并提出两点改进:1) 采用灰度积分投影进行人眼粗定位后将结果输入神经网络进行精定位, 提高了检测速度;2) 将人眼图像进行2×2分割后利用4个子卷积网络构成的卷积神经网进行特征点回归预测, 同时考虑到眼睑与眼部其他特征的差异, 在最后一层卷积层采用不共享权值, 在一定程度上提高了人眼特征点的定位准确率。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">1 算法流程</h3>
                <div class="p1">
                    <p id="42">一般情况下, 对于在自然环境中得到的人物图像, 人眼在图像中所占比例较小, 直接检测人眼往往比较困难且检测准确率不高。本文首先进行人脸检测, 对得到的人脸图像利用改进的级联卷积神经网进行人眼的定位以及人眼特征点检测, 通过6特征点计算人眼开闭度从而识别人眼状态, 最后根据计算模拟视频帧的单位时间闭眼百分比 (PERcentage of eyelid CLOSure over the pupil time, PERCLOS) 值识别疲劳状态验证本文方法, 疲劳检测算法如图1所示。</p>
                </div>
                <div class="area_img" id="43">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907039_043.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 疲劳检测流程" src="Detail/GetImg?filename=images/JSJY201907039_043.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 疲劳检测流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907039_043.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of fatigue detection</p>

                </div>
                <div class="p1">
                    <p id="44">在人脸检测部分, 本文采用了肤色模型<citation id="173" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>与支持向量机 (Support Vector Machine, SVM) 分类器从全图中提取人脸图像。经过肤色检测后找到图像中可能存在肤色的连通区域的最小外接矩形, 经过实验表明, 小于40×40的人脸图像中人眼属于小目标, 针对小目标人眼的状态识别非常困难且检测效率较低, 因此舍弃小于40×40大小的矩形区域, 将剩下的矩形区域内的图像从原图中分割作为候选区域输入SVM分类器进行分类识别。</p>
                </div>
                <h3 id="45" name="45" class="anchor-tag">2 数据集准备</h3>
                <div class="p1">
                    <p id="46">训练数据 本文采用CEW人眼睁闭数据库来训练一级网络, CEW库是用于人眼睁闭检测的数据集, 该数据集共包含2 423张不同人眼开闭状态的带人脸图片, 其中包括来自于网络的1 192张闭眼图片以及来自于LFW人脸数据库的1 231张睁眼图片。利用BBox-Label-Tool工具制作人眼位置框标签, 每幅图片的标签信息包含实际矩形框的坐标信息、宽高信息以及框内目标的类别信息。训练二级网络从CEW库中裁剪人眼图片2 133张, 其中闭眼图片共1 015张, 睁眼图片共1 118张, 左眼图片1 060张, 右眼图片1 073张, 利用sloth工具为人眼图片添加特征点标签。</p>
                </div>
                <div class="p1">
                    <p id="47">实验数据 由于目前缺少关于人体疲劳的公开数据集, 本文根据自采集模拟人体清醒与疲劳两种状态的视频进行实验。首先定义当上下眼睑均未遮挡虹膜时的眼睛状态为完全张开状态, 通过实验统计眼睛完全张开时开闭度在 (0.35, 0.4]区间, 并从清醒状态视频与疲劳状态视频中各选取200帧共400帧带人脸视频帧进行实验, 选取的视频帧均包含不同人眼张开度, 其中在清醒状态帧中选取人眼开闭度为 (0.1, 0.2]共40帧, (0.2, 0.3]共100帧, (0.3, 0.4]共60帧, 同时在疲劳帧中人眼完全闭合共50帧, 开闭度为[0, 0.1]共150帧。</p>
                </div>
                <h3 id="48" name="48" class="anchor-tag">3 人眼检测与状态识别</h3>
                <h4 class="anchor-tag" id="49" name="49">3.1 <b>人眼检测</b></h4>
                <div class="p1">
                    <p id="50">由于人眼区域占人脸比例较小, 若直接进行人眼定位需要对整幅人脸图像进行计算, 造成大量计算资源的浪费, 因此本文设计了一种结合灰度积分投影与卷积神经网的方法, 先对得到的人脸区域图像进行人眼的粗定位再利用神经网络提取特征并进行人眼精定位。灰度积分投影法<citation id="174" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>是一种简易的人眼粗定位方法, 根据人脸先验知识, 人眼及眉毛区域的灰度较人脸其他区域低, 因此将人脸图像转换为灰度图像并将其像素灰度值投影到水平或者垂直方向累加得到灰度曲线, 对于待检测图像<i>f</i> (<i>x</i>, <i>y</i>) , 其水平灰度积分可表示为:</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munderover><mi>f</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">其中<i>x</i><sub><i>i</i></sub>表示水平方向第<i>i</i>个像素的位置。</p>
                </div>
                <div class="p1">
                    <p id="53">由图2水平灰度积分投影曲线可知, 人脸灰度第一极大值与第二极大值点之间对应于人的额头位置与鼻中部位置, 人眼位置就位于两个极大值点之间的极小值点附近, 为了使截取的图片完整包含人眼, 提取第一极大值与第二极大值点之间的区域即额头至鼻中部区域图片作为人眼的候选区域。</p>
                </div>
                <div class="p1">
                    <p id="54">人眼定位网络结构为一种7层结构的网络结合2层RPN (Region Proposal Network) , 主要实现人眼分类与人眼框回归预测两种任务, 网络结构如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="55">网络结构中第1层为输入层;第2层为灰度积分投影人眼粗定位;第3、4、5层为卷积层, 第6层为全连接层, 第7层为两个全连接层构成的输出层;RPN结构为两层全卷积层。各层的具体结构如下:</p>
                </div>
                <div class="p1">
                    <p id="56">1) 第1层为输入层, 由输入样本构成, 实验采用的人脸图片大小均为100×100大小, 即输入样本为100×100的矩阵, 每一个矩阵中的点表示图像像素的大小。</p>
                </div>
                <div class="p1">
                    <p id="57">2) 第2层为灰度积分投影人眼粗定位, 将得到的人眼候选区域图像调整为60×100大小。</p>
                </div>
                <div class="p1">
                    <p id="58">3) 第3、4、5层为卷积层;用来提取图像特征, 在第3、4、5层分别采用了8、2、1种卷积核, 卷积核大小均为3×3, 最终得到了16个特征图;在每个卷积层的后面采用了2×2的最大池化层, 主要作用是用来简化卷积层输出的特征。</p>
                </div>
                <div class="p1">
                    <p id="59">4) RPN为全卷积网络, 其主要包含两个卷积层, 采用5×5大小的卷积核, 主要作用是提取人脸图像中人眼可能存在的区域, 根据文献<citation id="175" type="reference">[<a class="sup">15</a>]</citation>建议采用了9种anchor, 经过非极大值抑制 (Non-Maximum Suppression, NMS) <citation id="176" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>后保留60个人眼候选框, 其中正负类比例为1∶3。</p>
                </div>
                <div class="p1">
                    <p id="60">5) 第6层为全连接层, 其主要作用是将各个特征图映射成一个特征向量, 便于后续的分类与人眼位置回归。</p>
                </div>
                <div class="p1">
                    <p id="61">6) 第7层为输出层, 对于人眼分类采用softmax分类最后输出分类概率, 对于人眼框回归输出为人眼定位矩形框的长、宽以及中心点的坐标。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907039_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 人脸水平灰度积分投影" src="Detail/GetImg?filename=images/JSJY201907039_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 人脸水平灰度积分投影  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907039_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Horizontal gray integral projection of human face</p>

                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907039_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 人眼定位网络结构" src="Detail/GetImg?filename=images/JSJY201907039_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 人眼定位网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907039_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Network structure of eye positioning</p>

                </div>
                <h4 class="anchor-tag" id="64" name="64">3.2 <b>人眼状态识别</b></h4>
                <div class="p1">
                    <p id="65">本文利用改进的卷积神经网络作为人眼检测后的2级网络进行人眼特征点回归, 本文共检测了人眼的6个特征点, 如图4所示。其中A、B点分别为上下眼睑的内交点与外交点, C、E点为AB连线三等分点靠近内交点处与上下眼睑的交点, D、F点为AB连线三等分点靠近外交点处与上下眼睑的交点。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907039_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 人眼的6个特征点" src="Detail/GetImg?filename=images/JSJY201907039_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 人眼的6个特征点  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907039_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Six feature points of human eye</p>

                </div>
                <div class="p1">
                    <p id="67">定义人眼的开闭度:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>d</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mn>2</mn><mi>d</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中:<i>d</i>为点A、B之间的直线距离, <i>d</i><sub>1</sub>为点C、E之间的距离, <i>d</i><sub>2</sub>为点D、F之间的距离。当比值&lt;0.1时, 认为当前人眼状态为闭眼状态, 0.1为根据P80<citation id="177" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>计算的经验值, 人眼特征点检测网络结构如图5所示。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907039_070.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 特征点检测网络结构" src="Detail/GetImg?filename=images/JSJY201907039_070.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 特征点检测网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907039_070.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Network structure of feature points detection</p>

                </div>
                <div class="p1">
                    <p id="71">网络具体结构如下:</p>
                </div>
                <div class="p1">
                    <p id="72">1) 第1层为输入层, 将得到的人眼图像调整为48×48大小, 输入大小为48×48的矩阵, 将输入进行2×2分割后分别连接4个并行子卷积结构。</p>
                </div>
                <div class="p1">
                    <p id="73">2) 每个子卷积结构为3层网络;各个卷积层分别采用8、2、4种卷积核, 在前两个卷积层后采用2×2的最大池化降维, 最终得到了16个特征图, 因为在最后一个卷积层采用了不共享卷积方式。共享卷积的主要优点是减少了参数个数, 但是忽视了图像不同部分特征的差异性, 对于人眼特征点的预测, 眼睑的高层特征与眼睛其他部位差别比较大, 且由于人眼图片较小, 因此在最后一层卷积层采用非共享卷积能够更好地提取图像特征。</p>
                </div>
                <div class="p1">
                    <p id="74">3) 第5层为全连接层, 将子卷积的各特征图映射成一个特征向量。</p>
                </div>
                <div class="p1">
                    <p id="75">4) 第6层输出6个人眼特征点的坐标共12个神经元。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">3.3 <b>卷积神经网络训练</b></h4>
                <div class="p1">
                    <p id="77">卷积神经网的训练主要是通过梯度下降法与反向传播来实现的, 主要原理输入数据, 计算每一层的激活值, 最后计算损失函数, 根据损失函数大小是否满足设定的阈值判断是否结束迭代过程, 若不满足条件则反向更新参数并继续进行迭代。</p>
                </div>
                <div class="p1">
                    <p id="78">对于神经网中第<i>k</i>层第<i>m</i>个特征图, 第<i>j</i>个神经元, 其激活值表达式为:</p>
                </div>
                <div class="p1">
                    <p id="79"><i>a</i><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>k</mi><mo>, </mo><mi>m</mi></mrow></msubsup></mrow></math></mathml>=<i>g</i> (<i>x</i><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>k</mi><mo>, </mo><mi>m</mi></mrow></msubsup></mrow></math></mathml>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<i>a</i><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>k</mi><mo>, </mo><mi>m</mi></mrow></msubsup></mrow></math></mathml>为该神经元激活值;<i>x</i><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>k</mi><mo>, </mo><mi>m</mi></mrow></msubsup></mrow></math></mathml>为该神经元输入值;<i>g</i> () 为激活函数。本文采用的激活函数为tanh函数, 其表达式为 (e<sup><i>z</i></sup>-e<sup>-<i>z</i></sup>) / (e<sup><i>z</i></sup>+e<sup>-<i>z</i></sup>) , 其中e为自然常数, 可以看出tanh函数的取值范围在[-1, 1], 均值为0, 在训练中较sigmod函数具有更快的收敛速度。</p>
                </div>
                <div class="p1">
                    <p id="85">在一级网络的输出中主要存在人眼分类与人眼位置回归两种任务, 对于分类任务, 采用交叉熵损失函数, 表达式为:</p>
                </div>
                <div class="p1">
                    <p id="86"><i>Loss</i>=-[<b><i>y</i></b> lb<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="88">其中:<b><i>y</i></b>为分类标签实际值;<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover></math></mathml>为神经网络输出值。</p>
                </div>
                <div class="p1">
                    <p id="90">在人眼位置回归任务中, 假设人眼框标签实际值为<b><i>X</i></b>= (<i>x</i>, <i>y</i>, <i>w</i>, <i>h</i>) , 预测值为<b><i>X</i></b>′= (<i>x</i>′, <i>y</i>′, <i>w</i>′, <i>h</i>′) , 损失函数采用二次损失函数, 表达式为:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>r</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><msqrt><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>y</mi><mo>-</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>w</mi><mo>-</mo><msup><mi>w</mi><mo>′</mo></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>h</mi><mo>-</mo><msup><mi>h</mi><mo>′</mo></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">其中: (<i>x</i>, <i>y</i>, <i>w</i>, <i>h</i>) 分别表示实际回归框左上顶点的坐标与实际回归框的宽度与高度, 而 (<i>x</i>′, <i>y</i>′, <i>w</i>′, <i>h</i>′) 则表示预测回归框的坐标与宽高。</p>
                </div>
                <div class="p1">
                    <p id="93">在二级网络主要任务属于人眼特征点回归任务, 因此也采用一种二次损失函数, 其表达式为:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mi>o</mi><mi>i</mi><mi>n</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>r</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><msqrt><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mstyle><mo>/</mo><mi>l</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">其中: (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 为第<i>i</i>个特征点实际标注点的坐标; (<i>x</i><sub><i>i</i></sub>′, <i>y</i><sub><i>i</i></sub>′) 为其预测点的坐标;<i>i</i>的顺序为1～<i>N</i>, <i>N</i>为特征点的个数;<i>l</i>为人眼图像的长度。</p>
                </div>
                <div class="p1">
                    <p id="96">采用梯度下降法进行神经网络参数的更新, 设标签实际值为<b><i>Y</i></b>, 神经网络输出值为<b><i>Y</i></b>′, 那么网络中权值以及偏置更新:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">w</mi><mspace width="0.25em" /><mo>←</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">w</mi><mo>+</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><msup><mi mathvariant="bold-italic">Y</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">w</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">b</mi><mspace width="0.25em" /><mo>←</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">b</mi><mo>+</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><msup><mi mathvariant="bold-italic">Y</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">b</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">其中:<b><i>w</i></b>为权值矢量;<b><i>b</i></b>为偏置矢量;<i>η</i>为学习率, 训练过程迭代1 000次, 学习率采用以0.1为基数的luong234衰减, 即在总步数的三分之二后开始衰减, 在后面的步数中平均衰减4次, 每次衰减为上次的二分之一。</p>
                </div>
                <h3 id="99" name="99" class="anchor-tag">4 实验结果及分析</h3>
                <div class="p1">
                    <p id="100">为了验证人眼定位算法的有效性, 本文在人眼定位部分与<i>Haar</i>+<i>AdaBoost</i>算法、<i>Gabor</i>+<i>SVM</i>等算法以及未作人眼粗定位的本文神经网络作了比较, 同时分析了不同光照以及不同头部偏转姿态下的人眼定位, 强光设定为平均灰度大于180, 弱光灰度设定为平均灰度小于60。实验在<i>Matlab</i> 2016<i>b</i>环境下进行, 计算机的<i>CPU</i>主频为3.30 <i>GHz</i>, 实验结果分别如表1～2所示。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表</b>1 <b>不同方法的人眼定位准确率和检测速率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Comparison of human eye positioning accuracy and</i><i>detection rate with different methods</i></p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td rowspan="2"><br />检测方法</td><td colspan="4"><br />准确率/%</td><td rowspan="2">检测速率/<br /> (帧·<i>s</i><sup>-1</sup>) </td></tr><tr><td><br />自然状态</td><td>强光</td><td colspan="2">暗光</td></tr><tr><td>本文方法 (无人眼粗定位) </td><td>93.4</td><td>86.20</td><td>73.2</td><td colspan="2">7</td></tr><tr><td><br />本文方法 (有人眼粗定位) </td><td>96.2</td><td>88.70</td><td>75.7</td><td colspan="2">11</td></tr><tr><td><br /><i>Haar</i>+<i>AdaBoost</i></td><td>87.2</td><td>69.20</td><td>53.6</td><td colspan="2">10</td></tr><tr><td><br /><i>YOLO</i></td><td>79.2</td><td>70.60</td><td>62.3</td><td colspan="2">14</td></tr><tr><td><br /><i>Gabor</i>+<i>SVM</i></td><td>85.8</td><td>77.85</td><td>65.9</td><td colspan="2">8</td></tr><tr><td><br /><i>Spp</i>-<i>net</i></td><td>90.7</td><td>83.60</td><td>72.5</td><td colspan="2">7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表</b>2 <b>不同姿态下人眼定位结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 2 <i>Human eye positioning results with different postures</i></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td><br />姿态</td><td>检测帧数</td><td>正确检测帧数</td><td>正确率/%</td></tr><tr><td><br />0°偏转</td><td>40</td><td>40</td><td>100.0</td></tr><tr><td><br />左偏15°</td><td>40</td><td>35</td><td>87.5</td></tr><tr><td><br />左偏35°</td><td>40</td><td>28</td><td>70.0</td></tr><tr><td><br />右偏15°</td><td>40</td><td>36</td><td>90.0</td></tr><tr><td><br />右偏35°</td><td>40</td><td>26</td><td>65.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="103">从表1～2可以看出, 本文采用的方法与目前人眼定位中常用的<i>Haar</i>特征结合<i>AdaBoost</i>算法、<i>Gabor</i>+<i>SVM</i>算法以及<i>Spp</i>-<i>net</i>目标检测算法相比具有最高的定位准确率和检测速率, 检测速率略低于<i>YOLO</i>目标检测算法但具有更高的定位准确率, 同时与未采用灰度积分投影进行人眼粗定位的神经网络以及相比, 在自然状态下检测准确率提升了2.8个百分点, 而检测速率由7帧/<i>s</i> (<i>Frames Per Second</i>, <i>FPS</i>) 提升到了11帧/<i>s</i>, 提升了57%左右, 说明采用灰度积分投影的粗定位在一定程度上提高了检测性能。同时在不同光照下的检测结果表明本文对于光照环境变化的人眼定位具有一定鲁棒性, 对于人体头部姿态偏转角度在小幅度偏转时具有较好的检测效果, 但是当偏转角度过大时定位准确率会大幅下降。</p>
                </div>
                <div class="p1">
                    <p id="104">从图6中可以看出, 本文方法能够实现较好的特征点检测定位, 在归一化误差允许范围为0.05时各个特征点的定位准确率均达到92%～98%, 平均定位准确率达95.8%, 其中<i>A</i>、<i>B</i>点位于角点, 特征较为复杂, 定位准确率较高, 下眼睑两点因为闭眼时睫毛的遮挡导致特征模糊因此检测准确率较上眼睑两点低。同时本文与主动形状模型 (<i>Active Shape Model</i>, <i>ASM</i>) 、主动外观模型 (<i>Active Appearance Model</i>, <i>AAM</i>) 传统方法以及受限深度卷积神经网络 (<i>Tasks</i>-<i>Constrained Deep Convolutional Network</i>, <i>TCDCN</i>) 方法进行了比较, 本文方法、<i>ASM</i>、<i>AAM</i>、<i>TCDCN</i>方法的特征点定位平均准确率分别为95.8%、78.5%、72.4%、92.4%, 结果表明本文方法具有最高的平均定位准确率。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907039_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 各特征点检测准确率" src="Detail/GetImg?filename=images/JSJY201907039_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 各特征点检测准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907039_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Accuracy of each feature point</i></p>

                </div>
                <div class="p1">
                    <p id="106">通过对选取的400帧带人脸视频帧检测人眼特征点后计算人眼开闭度识别人眼状态。表3中人眼状态平均检测准确率为97.5%, 由表3可知, 在人眼状态检测中, 误检主要存在于阈值0.1附近, 人眼开闭度越接近阈值0.1, 检测结果越容易出错, 而人在正常睁眼状态下开闭度在 (0.2, 0.3]区间, 当人眼开闭度位于0.1附近时一般是人处于眨眼状态。</p>
                </div>
                <div class="area_img" id="107">
                    <p class="img_tit"><b>表</b>3 <b>不同人眼开闭度下人眼状态检测</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Human eye state detection results with</i><i>different human eye opening and closing degrees</i></p>
                    <p class="img_note"></p>
                    <table id="107" border="1"><tr><td><br />人眼<br />开闭度</td><td>实际<br />帧数</td><td>检测<br />帧数</td><td>正确<br />检测</td><td>失败<br />检测</td><td>准确<br />率/%</td><td>状态</td></tr><tr><td><br />0</td><td>50</td><td>50</td><td rowspan="2"><br />194</td><td rowspan="2">6</td><td rowspan="2">97</td><td rowspan="2">闭眼状态</td></tr><tr><td><br /> (0, 0.1]</td><td>150</td><td>144</td></tr><tr><td><br /> (0.1, 0.2]</td><td>40</td><td>36</td><td rowspan="3"><br />196</td><td rowspan="3">4</td><td rowspan="3">98</td><td rowspan="3">睁眼状态</td></tr><tr><td><br /> (0.2, 0.3]</td><td>100</td><td>100</td></tr><tr><td><br /> (0.3, 0.4]</td><td>60</td><td>60</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="108"><i>PERCLOS</i>是指一定时间内眼睛闭合时间占总时间的百分比, 是目前视觉疲劳检测方面最有效的指标。研究表明人平均每分钟眨眼10至20次, 即平均每3 <i>s</i>到6 <i>s</i>一次, 人的正常眨眼时间为0.2～0.3 <i>s</i>, 此时<i>PERCLOS</i>值处于3.3%至10%之间, 而若眨眼时间达到0.5～3 <i>s</i>则可视为疲劳状态, 此时<i>PERCLOS</i>值处于16.7%至100%之间。为了提高容错率并且更为准确地区分疲劳与清醒状态, 规定<i>PERCLOS</i>≥20%时, 则判断当前人体处于疲劳状态。图7是分别模拟人体清醒与疲劳的两段视屏的人眼状态检测结果两段。其中图7 (<i>a</i>) 是模拟人体清醒状态下的人眼状态检测结果, 检测到模拟视频眨眼过程中的闭眼时间达到4帧, 可以计算<i>PERCLOS</i>为8%;在疲劳状态下, 结果如图7 (<i>b</i>) 所示, 检测到模拟视频眨眼过程中的闭眼时间为22帧, 可以计算<i>PERCLOS</i>为44%, 均能够正确反映身体状态。测试结果证明了本文方法的有效性。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907039_109.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 模拟清醒与疲劳状态下人眼开闭度" src="Detail/GetImg?filename=images/JSJY201907039_109.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 模拟清醒与疲劳状态下人眼开闭度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907039_109.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Human eye opening and closing degree under</i><i>simulated fatigue and awake state</i></p>

                </div>
                <h3 id="110" name="110" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="111">本文针对视觉疲劳检测, 提出了一种级联卷积神经网络。一级网络结合灰度积分投影与卷积神经网络定位人眼提高了检测速度与检测准确率, 利用改进的卷积神经网络进行人眼特征点检测, 通过人眼特征点计算人眼开闭度来识别人眼状态, 并最终根据<i>PERCLOS</i>准则检测人体疲劳。结果表明本文方法具有较高的检测速度与检测准确率, 能够满足疲劳检测的需要, 但是本文方法针对头部姿势偏转过大时人眼定位效果还有待进一步提高, 可以作为下一步的研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="128">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A study on driver fatigue recognition based on SVM method">

                                <b>[1]</b> YOU Z, GAO Y, ZHANG J, et al.A study on driver fatigue recognition based on SVM method [C]// Proceedings of the 2017 4th International Conference on Transportation Information and Safety.Piscataway, NJ:IEEE, 2017:693-697.
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Driver Fatigue Classification With Independent Component by Entropy Rate Bound Minimization Analysis in an EEG-Based System">

                                <b>[2]</b> CHAI R, NAIK G, NGUYEN T N, et al.Driver fatigue classification with independent component by entropy rate bound minimization analysis in an EEG-based system [J].IEEE Journal of Biomedical and Health Informatics, 2017, 21 (3) :715-724.
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time eye tracking for the assessment of driver fatigue">

                                <b>[3]</b> XU J, MIN J, HU J.Real-time eye tracking for the assessment of driver fatigue [J].Healthcare Technology Letters, 2018, 5 (2) :54-58.
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201609039&amp;v=MDUzNThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1U3M09MejdNYWJHNEg5Zk1wbzlHYlk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 唐广发, 张会林.人眼疲劳预测技术的研究[J].计算机工程与应用, 2016, 52 (9) :213-218. (TANG G F, ZHANG H L.Research on human eye fatigue prediction technology[J].Computer Engineering and Applications, 2016, 52 (9) :213-218.) 
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A driving assistant safety method based on human eye fatigue detection">

                                <b>[5]</b> ZENG S, LI J, JIANG L, et al.A driving assistant safety method based on human eye fatigue detection [C]// Proceedings of the 2017 Control and Decision Conference.Piscataway, NJ:IEEE, 2017:6370-6377.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fatigue detection based on isophote curve">

                                <b>[6]</b> DENG Z, JING R, JIAO L, et al.Fatigue detection based on isophote curve [C]// Proceedings of the 2015 International Conference on Computer and Computational Sciences.Piscataway, NJ:IEEE, 2015:146-150.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZIY201503012&amp;v=MTczNTBUTXJJOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTczT0lUZkNkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 李响, 谭南林, 李国正, 等.基于Zernike矩的人眼定位与状态识别[J].电子测量与仪器学报, 2015 (3) :390-398. (LI X, TAN N L, LI G Z, et al.Human eye localization and state recognition based on Zernike moment[J].Journal of Electronic Measurement and Instrumentation, 2015 (3) :390-398) 
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weak classifier for density estimation in eye localization and tracking">

                                <b>[8]</b> ARAUJO G M, FML R, JUNIOR W S, et al.Weak classifier for density estimation in eye localization and tracking[J].IEEE Transactions on Image Processing, 2017, 26 (7) :3410-3424.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual-Context Boosting for Eye Detection">

                                <b>[9]</b> SONG M, TAO D, SUN Z, et al.Visual-context boosting for eye detection[J].IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) , 2010, 40 (6) :1460-1467.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiple object detection by a deformable part-based model and an R-CNN">

                                <b>[10]</b> LI J, WONG H C, LO S L, et al.Multiple object detection by a deformable part-based model and an R-CNN[J].IEEE Signal Processing Letters, 2018, 25 (2) :288-292.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scale-aware fast R-CNN for pedestrian detection">

                                <b>[11]</b> LI J, LIANG X, SHEN S M, et al.Scale-aware fast R-CNN for pedestrian detection [J].IEEE Transactions on Multimedia, 2018, 20 (4) :985-996.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-task CNN model for attribute prediction">

                                <b>[12]</b> ABDULNABI A H, WANG G, LU J, et al.Multi-task CNN model for attribute prediction [J].IEEE Transactions on Multimedia, 2015, 17 (11) :1949-1959.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive skin detection using face location and facial structure estimation">

                                <b>[13]</b> LUO Y, GUAN Y P.Adaptive skin detection using face location and facial structure estimation [J].IET Computer Vision, 2017, 11 (7) :550-559.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTGJ201601023&amp;v=MDkwMTFNcm85SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dVNzNPUFRuTVpMRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> YANG W, ZHANG Z, ZHANG Y, et al.Real-time digital image stabilization based on regional field image gray projection[J].Journal of Systems Engineering and Electronics, 2016, 27 (1) :224-231.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Region Proposal Networks for Multispectral Person Detection">

                                <b>[15]</b> KÖNIG D, ADAM M, JARVERS C, et al.Fully convolutional region proposal networks for multispectral person detection[C]// Proceedings of the 2017 Computer Vision and Pattern Recognition Workshops.Washington, DC:IEEE Computer Society, 2017:243-250.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mirrored non-maximum suppression for accurate object part localization">

                                <b>[16]</b> FU L, ZHANG J, HUANG K.Mirrored non-maximum suppression for accurate object part localization [C]// Proceedings of the 2015 3rd IAPR Asian Conference on Pattern Recognition.Piscataway, NJ:IEEE, 2015:51-55.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards detection of bus driver fatigue based on robust visual analysis of eye state">

                                <b>[17]</b> MANDAL B, LI L, WANG G S, et al.Towards detection of bus driver fatigue based on robust visual analysis of eye state [J].IEEE Transactions on Intelligent Transportation Systems, 2017, 18 (3) :545-557.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907039" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907039&amp;v=MDU2MTJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dVNzNPTHo3QmQ3RzRIOWpNcUk5R2JZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
