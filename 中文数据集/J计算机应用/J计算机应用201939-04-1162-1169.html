<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136782331533750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201904037%26RESULT%3d1%26SIGN%3dJIfqqIPsJMrvGteon7uMPJqMKf4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201904037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904037&amp;v=MTI0NDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURoVmI3Qkx6N0JkN0c0SDlqTXE0OUdZNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#55" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="1 Retinex理论和相关工作 ">1 Retinex理论和相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="1.1 Retinex&lt;b&gt;理论&lt;/b&gt;">1.1 Retinex<b>理论</b></a></li>
                                                <li><a href="#65" data-title="1.2 &lt;b&gt;相关工作&lt;/b&gt;">1.2 <b>相关工作</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="2 本文算法 ">2 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="2.1 &lt;b&gt;派生图&lt;/b&gt;">2.1 <b>派生图</b></a></li>
                                                <li><a href="#89" data-title="2.2 &lt;b&gt;网络结构&lt;/b&gt;">2.2 <b>网络结构</b></a></li>
                                                <li><a href="#96" data-title="2.3 &lt;b&gt;损失函数&lt;/b&gt;">2.3 <b>损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#102" data-title="3.1 &lt;b&gt;实验环境与参数设置&lt;/b&gt;">3.1 <b>实验环境与参数设置</b></a></li>
                                                <li><a href="#104" data-title="3.2 &lt;b&gt;训练数据&lt;/b&gt;">3.2 <b>训练数据</b></a></li>
                                                <li><a href="#108" data-title="3.3 &lt;b&gt;实验结果分析&lt;/b&gt;">3.3 <b>实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="图1 文献算法增强失败例子">图1 文献算法增强失败例子</a></li>
                                                <li><a href="#88" data-title="图2 派生图样例">图2 派生图样例</a></li>
                                                <li><a href="#93" data-title="图3 MDIIN网络结构">图3 MDIIN网络结构</a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表&lt;/b&gt;1 MDIIN&lt;b&gt;网络参数&lt;/b&gt;"><b>表</b>1 MDIIN<b>网络参数</b></a></li>
                                                <li><a href="#107" data-title="图4 合成弱光照图像例子">图4 合成弱光照图像例子</a></li>
                                                <li><a href="#114" data-title="图5 各算法在合成弱光照图像上实验的部分结果">图5 各算法在合成弱光照图像上实验的部分结果</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;各算法在合成弱光照图像上实验的&lt;/b&gt;MSE、PSNR、SSIM&lt;b&gt;指标&lt;/b&gt;"><b>表</b>2 <b>各算法在合成弱光照图像上实验的</b>MSE、PSNR、SSIM<b>指标</b></a></li>
                                                <li><a href="#124" data-title="图6 各算法在真实弱光照图像上实验结果">图6 各算法在真实弱光照图像上实验结果</a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各算法在真实弱光照图像上实验的&lt;/b&gt;NIQE、&lt;b&gt;对比度增益、熵指标&lt;/b&gt;"><b>表</b>3 <b>各算法在真实弱光照图像上实验的</b>NIQE、<b>对比度增益、熵指标</b></a></li>
                                                <li><a href="#129" data-title="图7 原图替换各派生图后的增强结果">图7 原图替换各派生图后的增强结果</a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;各算法处理图片的时间&lt;/b&gt; s"><b>表</b>4 <b>各算法处理图片的时间</b> s</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="173">


                                    <a id="bibliography_1" title="康晓东, 王昊, 郭军, 等.无监督深度学习彩色图像识别方法[J].计算机应用, 2015, 35 (9) :2636-2639. (TANG X D, WANG H, GUO J, et al.Unsupervised deep learning method for color image recognition[J].Journal of Computer Applications, 2015, 35 (9) :2636-2639.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201509045&amp;v=MjEwNzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEaFZiN0FMejdCZDdHNEg5VE1wbzlCWVk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        康晓东, 王昊, 郭军, 等.无监督深度学习彩色图像识别方法[J].计算机应用, 2015, 35 (9) :2636-2639. (TANG X D, WANG H, GUO J, et al.Unsupervised deep learning method for color image recognition[J].Journal of Computer Applications, 2015, 35 (9) :2636-2639.) 
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_2" title="杨朔, 陈丽芳, 石瑀, 等.基于深度生成式对抗网络的蓝藻语义分割[J].计算机应用, 2018, 38 (6) :1554-1561. (YANG S, CHEN L F, SHI Y, et al.Semantic segmentation of blue-green algae based on deep generative adversarial net[J].Journal of Computer Applications, 2018, 38 (6) :1554-1561.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806006&amp;v=MjU5NTJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGhWYjdBTHo3QmQ3RzRIOW5NcVk5RllvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        杨朔, 陈丽芳, 石瑀, 等.基于深度生成式对抗网络的蓝藻语义分割[J].计算机应用, 2018, 38 (6) :1554-1561. (YANG S, CHEN L F, SHI Y, et al.Semantic segmentation of blue-green algae based on deep generative adversarial net[J].Journal of Computer Applications, 2018, 38 (6) :1554-1561.) 
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_3" title="LI H M.Deep learning for image denoising[J].International Journal of Signal Processing, Image Processing and Pattern Recognition, 2014, 7 (3) :171-180." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning for Image Denoising">
                                        <b>[3]</b>
                                        LI H M.Deep learning for image denoising[J].International Journal of Signal Processing, Image Processing and Pattern Recognition, 2014, 7 (3) :171-180.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_4" title="CAI B, XU X, JIA K, et al.Dehaze Net:an end-to-end system for single image haze removal[J].IEEE Transactions on Image Processing, 2016, 25 (11) :5187-5198." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dehaze Net:an end-to-end system for single image haze removal">
                                        <b>[4]</b>
                                        CAI B, XU X, JIA K, et al.Dehaze Net:an end-to-end system for single image haze removal[J].IEEE Transactions on Image Processing, 2016, 25 (11) :5187-5198.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_5" title="REN W, LIU S, ZHANG H, et al.Single image dehazing via multiscale convolutional neural networks[C]//ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:154-169." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single image dehazing via multi-scale convolutional neural networks">
                                        <b>[5]</b>
                                        REN W, LIU S, ZHANG H, et al.Single image dehazing via multiscale convolutional neural networks[C]//ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:154-169.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_6" title="LIM B, SON S, KIM H, et al.Enhanced deep residual networks for single image super-resolution[C]//Proceedings of the 2017 IEEEConference on Computer Vision and Pattern Recognition Workshops.Piscataway, NJ:IEEE, 2017:136-144." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">
                                        <b>[6]</b>
                                        LIM B, SON S, KIM H, et al.Enhanced deep residual networks for single image super-resolution[C]//Proceedings of the 2017 IEEEConference on Computer Vision and Pattern Recognition Workshops.Piscataway, NJ:IEEE, 2017:136-144.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_7" title="LAND E H.The Retinex[J].American Scientist, 1964, 52 (2) :247-264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The retinex">
                                        <b>[7]</b>
                                        LAND E H.The Retinex[J].American Scientist, 1964, 52 (2) :247-264.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_8" title="LEE C, LEE C, KIM C S.Contrast enhancement based on layered difference representation of 2D histograms[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5372-5384." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contrast enhancement based on layered difference representation">
                                        <b>[8]</b>
                                        LEE C, LEE C, KIM C S.Contrast enhancement based on layered difference representation of 2D histograms[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5372-5384.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_9" title="PISANO E D, ZONG S, HEMMINGER B M, et al.Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms[J].Journal of Digital Imaging, 1998, 11 (4) :193-200." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003620786&amp;v=MDE1MjErWnVGaXJsVzd2T0lWWT1OajdCYXJPNEh0SFBxWTFGWStNSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRa&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        PISANO E D, ZONG S, HEMMINGER B M, et al.Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms[J].Journal of Digital Imaging, 1998, 11 (4) :193-200.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_10" title="GUO X, LI Y, LING H.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LIME:Low-light image enhancement via illumination map estimation">
                                        <b>[10]</b>
                                        GUO X, LI Y, LING H.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_11" title="FU X, LIAO Y, ZENG D, et al.A probabilistic method for image enhancement with simultaneous illumination and reflectance estimation[J].IEEE Transactions on Image Processing, 2015, 24 (12) :4965-4977." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Probabilistic Method for Image Enhancement With Simultaneous Illumination and Reflectance Estimation.">
                                        <b>[11]</b>
                                        FU X, LIAO Y, ZENG D, et al.A probabilistic method for image enhancement with simultaneous illumination and reflectance estimation[J].IEEE Transactions on Image Processing, 2015, 24 (12) :4965-4977.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_12" title="FU X, ZENG D, HUANG Y, et al.A weighted variational model for simultaneous reflectance and illumination estimation[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2782-2790." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A weighted variational model for simultaneous reflectanceand illumination estimation">
                                        <b>[12]</b>
                                        FU X, ZENG D, HUANG Y, et al.A weighted variational model for simultaneous reflectance and illumination estimation[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2782-2790.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_13" title="WANG S, ZHENG J, HU H M, et al.Naturalness preserved enhancement algorithm for non-uniform illumination images[J].IEEETransactions on Image Processing, 2013, 22 (9) :3538-3548." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Naturalness preserved enhancement algorithm for non-uniform illumination images">
                                        <b>[13]</b>
                                        WANG S, ZHENG J, HU H M, et al.Naturalness preserved enhancement algorithm for non-uniform illumination images[J].IEEETransactions on Image Processing, 2013, 22 (9) :3538-3548.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_14" title="FU X, ZENG D, HUANG Y, et al.A fusion-based enhancing method for weakly illuminated images[J].Signal Processing, 2016, 129:82-96." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fusion-based enhancing method for weakly illuminated images">
                                        <b>[14]</b>
                                        FU X, ZENG D, HUANG Y, et al.A fusion-based enhancing method for weakly illuminated images[J].Signal Processing, 2016, 129:82-96.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_15" title="DONG X, WANG G, PANG Y, et al.Fast efficient algorithm for enhancement of low lighting video[C]//Proceedings of the 2011IEEE International Conference on Multimedia and Expo.Piscataway, NJ:IEEE, 2011:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast efficient algorithm for enhancement of low lighting video">
                                        <b>[15]</b>
                                        DONG X, WANG G, PANG Y, et al.Fast efficient algorithm for enhancement of low lighting video[C]//Proceedings of the 2011IEEE International Conference on Multimedia and Expo.Piscataway, NJ:IEEE, 2011:1-6.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_16" title="FOTIADOU K, TSAGKATAKIS G, TSAKALIDES P.Low light image enhancement via sparse representations[C]//ICIAR 2014:Proceedings of the 2014 International Conference on Image Analysis and Recognition.Berlin:Springer, 2014:84-93." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Low Light Image Enhancement via Sparse Representations">
                                        <b>[16]</b>
                                        FOTIADOU K, TSAGKATAKIS G, TSAKALIDES P.Low light image enhancement via sparse representations[C]//ICIAR 2014:Proceedings of the 2014 International Conference on Image Analysis and Recognition.Berlin:Springer, 2014:84-93.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_17" title="LI C, GUO J, PORIKLI F, et al.Lighten Net:a convolutional neural network for weakly illuminated image enhancement[J].Pattern Recognition Letters, 2018, 104:15-22." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAF0D84FE7B514C6C60215328AEA6E557&amp;v=MDQyNTRoS3lXVVY2ajE4VFh6Z3BHTkFDTFRoUUwrWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMeTV3YUE9TmlmT2ZjTE9IcVhFcS9rd1k1a0tEWA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        LI C, GUO J, PORIKLI F, et al.Lighten Net:a convolutional neural network for weakly illuminated image enhancement[J].Pattern Recognition Letters, 2018, 104:15-22.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_18" title="LORE K G, AKINTAYO A, SARKAR S.LLNet:a deep autoencoder approach to natural low-light image enhancement[J].Pattern Recognition, 2017, 61:650-662." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LLNet:a deep autoencoder approach to natural low-light image enhancement">
                                        <b>[18]</b>
                                        LORE K G, AKINTAYO A, SARKAR S.LLNet:a deep autoencoder approach to natural low-light image enhancement[J].Pattern Recognition, 2017, 61:650-662.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_19" title="MAO X J, SHEN C, YANG Y B.Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections[C]//Proceedings of the 30th International Conference on Neural Information Processing Systems.New York:Curran Associates, 2016:2810-2818." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections">
                                        <b>[19]</b>
                                        MAO X J, SHEN C, YANG Y B.Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections[C]//Proceedings of the 30th International Conference on Neural Information Processing Systems.New York:Curran Associates, 2016:2810-2818.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_20" title="TSAI Y H, SHEN X, LIN Z, et al.Deep image harmonization[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2799-2807." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep image harmonization">
                                        <b>[20]</b>
                                        TSAI Y H, SHEN X, LIN Z, et al.Deep image harmonization[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2799-2807.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_21" title="REN W, MA L, ZHANG J, et al.Gated fusion network for single image dehazing[C]//Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:3253-3261." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gated fusion network for single image dehazing">
                                        <b>[21]</b>
                                        REN W, MA L, ZHANG J, et al.Gated fusion network for single image dehazing[C]//Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:3253-3261.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_22" title="XIONG W, LUO W, MA L, et al.Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks[C]//Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:2364-2373." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks">
                                        <b>[22]</b>
                                        XIONG W, LUO W, MA L, et al.Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks[C]//Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:2364-2373.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_23" title="LIU Y, ZHAO G, GONG B, et al.Improved techniques for learning to dehaze and beyond:a collective study[EB/OL].[2018-07-26].https://arxiv.org/pdf/1807.00202." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved techniques for learning to dehaze and beyond:a collective study">
                                        <b>[23]</b>
                                        LIU Y, ZHAO G, GONG B, et al.Improved techniques for learning to dehaze and beyond:a collective study[EB/OL].[2018-07-26].https://arxiv.org/pdf/1807.00202.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_24" title="KINGMA D P, BA J L.ADAM:a method for stochastic optimization[EB/OL].[2018-05-10].https://simplecore.intel.com/nervana/wp-content/uploads/sites/53/2017/06/1412.6980.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ADAM:a method for stochastic optimization">
                                        <b>[24]</b>
                                        KINGMA D P, BA J L.ADAM:a method for stochastic optimization[EB/OL].[2018-05-10].https://simplecore.intel.com/nervana/wp-content/uploads/sites/53/2017/06/1412.6980.pdf.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_25" title="王一宁, 秦品乐, 李传朋, 等.基于残差神经网络的图像超分辨率改进算法[J].计算机应用, 2018, 38 (1) :246-254. (WANG Y N, QIN P L, LI C P, et al.Improved algorithm of image super resolution based on residual neural network[J].Journal of Computer Applications, 2018, 38 (1) :246-254.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201801044&amp;v=MzI1NDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURoVmI3QUx6N0JkN0c0SDluTXJvOUJZSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                        王一宁, 秦品乐, 李传朋, 等.基于残差神经网络的图像超分辨率改进算法[J].计算机应用, 2018, 38 (1) :246-254. (WANG Y N, QIN P L, LI C P, et al.Improved algorithm of image super resolution based on residual neural network[J].Journal of Computer Applications, 2018, 38 (1) :246-254.) 
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_26" title="梁中豪, 彭德巍, 金彦旭, 等.基于交通场景区域增强的单幅图像去雾方法[J].计算机应用, 2018, 38 (5) :1420-1426. (LIANG Z H, PENG D W, JIN Y X, et al.Single image dehazing algorithm based on traffic scene region enhancement[J].Journal of Computer Applications, 2018, 38 (5) :1420-1426.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201805035&amp;v=MTk2NzdiN0FMejdCZDdHNEg5bk1xbzlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEaFY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                        梁中豪, 彭德巍, 金彦旭, 等.基于交通场景区域增强的单幅图像去雾方法[J].计算机应用, 2018, 38 (5) :1420-1426. (LIANG Z H, PENG D W, JIN Y X, et al.Single image dehazing algorithm based on traffic scene region enhancement[J].Journal of Computer Applications, 2018, 38 (5) :1420-1426.) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-11-15 16:49</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(04),1162-1169 DOI:10.11772/j.issn.1001-9081.2018091979            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积神经网络的弱光照图像增强算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E5%AE%87&amp;code=37016462&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%93%E5%BE%B7%E7%A5%A5&amp;code=08982319&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邓德祥</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A2%9C%E4%BD%B3&amp;code=22094911&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">颜佳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8C%83%E8%B5%90%E6%81%A9&amp;code=14978229&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">范赐恩</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉大学电子信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有的弱光照图像增强算法强烈依赖于Retinex理论、需人工调整参数等问题, 提出一种基于卷积神经网络 (CNN) 的弱光照图像增强算法。首先, 利用四种图像增强手段处理弱光照图像得到四张派生图, 分别为:限制对比度自适应直方图均衡派生图、伽马变换派生图、对数变换派生图、亮通道增强派生图;然后, 将弱光照图像及其四张派生图输入到CNN中;最后经过CNN的激活, 输出增强图像。所提算法直接端到端地实现弱光照图像到正常光照图像的映射, 不需要按照Retinex模型先估计光照图像或反射率图像, 也无需调整任何参数。所提算法与NPEA (Naturalness Preserved Enhancement Algorithm for non-uniform illumination images) 、LIME (Low-light image enhancement via Illumination Map Estimation) 、LNET (LightenNet) 等算法进行了对比。在合成弱光照图像的实验中, 所提算法的均方误差 (MSE) 、峰值信噪比 (PSNR) 、结构相似度 (SSIM) 指标均优于对比算法。在真实弱光照图像实验中, 所提算法的平均自然图像质量评价度量 (NIQE) 、熵指标为所有对比方法中最优, 平均对比度增益指标在所有方法中排名第二。实验结果表明:相对于对比算法, 所提算法的鲁棒性较好;经所提算法增强后, 图像的细节更丰富, 对比度更高, 拥有更好的视觉效果和图像质量。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%B1%E5%85%89%E7%85%A7%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">弱光照图像增强;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Retinex%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Retinex模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B4%BE%E7%94%9F%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">派生图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自然图像质量评价;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    程宇 (1996—) , 男, 江西景德镇人, 硕士研究生, 主要研究方向:图像去雾、弱光照图像增强;;
                                </span>
                                <span>
                                    *邓德祥 (1961—) , 男, 湖北荆州人, 教授, 博士, 主要研究方向:图像识别、目标跟踪、智能物联网;电子邮箱ddx@whu.edu.cn;
                                </span>
                                <span>
                                    颜佳 (1983—) , 男, 湖北天门人, 副研究员, 博士, 主要研究方向:图像质量评价、图像美学评价、目标跟踪;;
                                </span>
                                <span>
                                    范赐恩 (1975—) , 女, 浙江慈溪人, 副教授, 博士, 主要研究方向:图像超分辨率重建、图像质量评价。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61701351);</span>
                    </p>
            </div>
                    <h1><b>Weakly illuminated image enhancement algorithm based on convolutional neural network</b></h1>
                    <h2>
                    <span>CHENG Yu</span>
                    <span>DENG Dexiang</span>
                    <span>YAN Jia</span>
                    <span>FAN Ci'en</span>
            </h2>
                    <h2>
                    <span>Electronic Information School, Wuhan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Existing weakly illuminated image enhancement algorithms are strongly dependent on Retinex model and require manual adjustment of parameters. To solve those problems, an algorithm based on Convolutional Neural Network (CNN) was proposed to enhance weakly illuminated image. Firstly, four image enhancement techniques were used to process weakly illuminated image to obtain four derivative images, including contrast limited adaptive histogram equalization derivative image, Gamma correction derivative image, logarithmic correction derivative image and bright channel enhancement derivative image. Then, the weakly illuminated image and its four derivative images were input into CNN. Finally, the enhanced image was output after activation by CNN. The proposed algorithm can directly map the weakly illuminated image to the normal illuminated image in end-to-end way without estimating the illumination map or reflection map according to Retinex model nor adjusting any parameters. The proposed algorithm was compared with Naturalness Preserved Enhancement Algorithm for non-uniform illumination images (NPEA) , Low-light image enhancement via Illumination Map Estimation (LIME) , LightenNet (LNET) , etc. In the experiment on synthetic weakly illuminated images, the average Mean Square Error (MSE) , Peak Signal-to-Noise Ratio (PSNR) and Structural SIMilarity index (SSIM) metrics of the proposed algorithm are superior to comparison algorithms. In the real weakly illuminated images experiment, the average Natural Image Quality Evaluator (NIQE) and entropy metric of the proposed algorithm are the best of all comparison algorithms, and the average contrast gain metric ranks the second among all algorithms. Experimental results show that compared with comparison algorithms, the proposed algorithm has better robustness, and the details of the images enhanced by the proposed algorithm are richer, the contrast is higher, and the visual effect and image quality are better.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weakly%20illuminated%20image%20enhancement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weakly illuminated image enhancement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Retinex%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Retinex model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=derivative%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">derivative image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=blind%20image%20quality%20assessment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">blind image quality assessment;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHENG Yu, born in 1996, M. S. candidate. His research interests include image dehazing, weakly illuminated image enhancement.;
                                </span>
                                <span>
                                    DENG Dexiang, born in 1961, Ph. D. , professor. His research interests include image recognition, target tracking, intelligent Internet of things.;
                                </span>
                                <span>
                                    YAN Jia, born in 1983, Ph. D. , associate research fellow. His research interests include image quality assessment, image aesthetics assessment, target tracking.;
                                </span>
                                <span>
                                    FAN Ci'en, born in 1975, Ph. D. , associate professor. Her research interests include image super-resolution reconstruction, image quality assessment.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-26</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61701351);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="55" name="55" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="56">在很多计算机视觉任务中, 如目标检测、图像检索、图像分割等, 都要求输入图像亮度合适、细节清晰。然而, 在弱光照或者曝光不足的情况下, 采集到的图像存在亮度低、色彩不饱和、细节模糊等缺点, 这些缺点将影响到后续的计算机视觉任务。因此, 研究弱光照图像的增强很有必要。为了改善这类图像的视觉效果, 需要对其进行增强处理, 基本增强手段主要包括:1) 通过调整对比度来增强图像边缘和细节;2) 通过调节动态范围抑制噪声等手段来改善图像清晰度;3) 通过提高较暗区域的亮度, 使图像亮度保持均匀;4) 通过调整图像的颜色饱和度使其获得良好的视觉效果等。</p>
                </div>
                <div class="p1">
                    <p id="57">近年来, 深度学习发展迅速, 在高层次视觉任务中应用非常广泛, 如图像识别<citation id="225" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、语义分割<citation id="226" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等。与此同时, 也有一些研究人员尝试用深度学习算法去解决低层次图像领域问题, 如图像去噪<citation id="227" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、图像去雾<citation id="229" type="reference"><link href="179" rel="bibliography" /><link href="181" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>、图像超分辨率<citation id="228" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等, 这些算法也取得了较好的成绩。相对于传统算法, 深度学习算法具有不需要人工设计特征提取方法, 可直接端到端地训练和输出结果等优势。</p>
                </div>
                <div class="p1">
                    <p id="58">因此, 在深度学习广泛应用的背景下, 本文尝试用卷积神经网络 (Convolutional Neural Network, CNN) 算法对传统的弱光照图像增强问题进行改进。</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag">1 Retinex理论和相关工作</h3>
                <h4 class="anchor-tag" id="60" name="60">1.1 Retinex<b>理论</b></h4>
                <div class="p1">
                    <p id="61">弱光照图像增强是图像处理领域的一个经典问题。该问题旨在从亮度偏暗、细节模糊、质量较低的图像中恢复出亮度适中、细节明显、有良好视觉效果的图像。</p>
                </div>
                <div class="p1">
                    <p id="62">Retinex理论模型<citation id="230" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>是弱光照图像增强领域的一个基础理论模型。模型的基本假设是原始图像是光照图像和反射率图像的乘积, 可表示为下式形式:</p>
                </div>
                <div class="p1">
                    <p id="63"><i>I</i> (<i>x</i>) =<i>R</i> (<i>x</i>) ·<i>L</i> (<i>x</i>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="64">其中:<i>x</i>表示像素点;<i>I</i> (<i>x</i>) 表示采集到的原始图像;<i>L</i> (<i>x</i>) 表示光照图像;<i>R</i> (<i>x</i>) 表示反射率图像。基于Retinex模型的图像增强算法的一般处理顺序是先从原始图像中估计出光照图像<i>L</i> (<i>x</i>) , 进而算出反射率图像<i>R</i> (<i>x</i>) , 消除<i>L</i> (<i>x</i>) 中弱光照的影响后得到亮度适中的增强光照图像<i>L</i><sub>en</sub> (<i>x</i>) , <i>L</i><sub>en</sub> (<i>x</i>) 与<i>R</i> (<i>x</i>) 相乘得到增强图像。目前大多数弱光照增强算法的研究都是基于Retinex理论模型, 这类算法的主要难点在于利用人工提取的图像特征和统计先验估计光照图像。</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">1.2 <b>相关工作</b></h4>
                <div class="p1">
                    <p id="66">弱光照图像增强算法的相关研究一直在进行, 从中发展出了许多不同类型的算法。例如, 文献<citation id="236" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>根据直方图调整图像的灰度动态范围来增强图像, 此类算法计算简单易于实现, 但在图像较暗区域的增强效果不足, 较亮区域又容易过度增强, 导致颜色失真。后来, 很多研究者提出基于Retinex模型的算法, 这类算法的一般步骤都是先从弱光照图像中估计出光照图像, 然后根据式 (1) 得到反射率图像, 通过一定的增强手段增强光照图像, 最后与反射率图像相乘得到增强图像。Guo等<citation id="231" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了LIME (Low-light image enhancement via Illumination Map Estimation) 算法, 该算法通过优化弱光照图像的亮通道图得到全局平滑且边缘清晰的光照图像, 光照图像经伽马变换后得到增强的光照图像。文献<citation id="237" type="reference">[<a class="sup">11</a>,<a class="sup">12</a>]</citation>中均用高斯分布和拉普拉斯分布来拟合光照图像和反射率图像的分布规律, 以弱光照图像的<i>V</i>通道图像为初始光照图像, 经ADMM (Alternating Direction Method of Multipliers) 算法迭代求得光照图像和反射率图像的最优解;然后与文献<citation id="232" type="reference">[<a class="sup">10</a>]</citation>一样, 使用伽马变换得到增强的光照图像。文献<citation id="233" type="reference">[<a class="sup">13</a>]</citation>中提出了NPEA (Naturalness Preserved Enhancement Algorithm for non-uniform illumination images) 算法, 该算法设计了一个光照敏感的滤波器, 使用该滤波器对弱光照图像滤波得到光照图像, 然后用改进的对数变换对光照图像进行调节得到增强的光照图像。然而, 根据式 (1) 可知, 在反射率图像<i>R</i> (<i>x</i>) 未知的条件下, 根据原始图像<i>I</i> (<i>x</i>) 估计光照图像<i>L</i> (<i>x</i>) 是一个病态问题, 估计的光照图像并不完全准确, 光照图像估计错误会导致增强图像中出现亮度不自然的问题, 这也是所有基于Retinex模型的算法的固有缺陷。图1是个典型例子, 文献<citation id="234" type="reference">[<a class="sup">11</a>]</citation>算法因为光照图像估计错误导致增强失败, 本文算法则能得到视觉效果良好的增强图像。Fu等<citation id="235" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了基于多派生光照图像融合的算法MF (Multi-scale derived images Fusion) 。首先根据亮通道方法得到光照图像和反射率图像, 使用引导滤波优化光照图像;然后用三种增强手段处理光照图像得到三个派生光照图像;最后根据人工设计的权重参数, 融合三个派生光照图像得到增强的光照图像, 反射率图像和增强的光照图像相乘得到增强图像。该算法能有效改善图像较暗区域的视觉效果, 同时保持明亮区域不出现失真现象;但是该算法的融合权重参数需要人工设计, 且融合权重参数不是基于学习的方法得到的, 不具有统计规律, 因而鲁棒性不佳。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904037_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 文献[11]算法增强失败例子" src="Detail/GetImg?filename=images/JSJY201904037_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 文献<citation id="238" type="reference">[<a class="sup">11</a>]</citation>算法增强失败例子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904037_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Example of enhancement failure of algorithm in reference <citation id="239" type="reference"><link href="193" rel="bibliography" />[11]</citation></p>

                </div>
                <div class="p1">
                    <p id="68">此外, 有研究者发现弱光照图像取反的结果类似于有雾图片。Dong等<citation id="240" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出的快速有效低光照视频增强算法, 将弱光照图像取反后, 用暗通道去雾算法对其进行处理, 然后将结果再次取反得到增强图像。这类算法的增强效果取决于去雾处理中透射率图估计的准确程度。而由雾形成的物理模型可知, 直接根据雾图像估计透射率图也是一个病态问题<citation id="241" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 所以基于Retinex模型的算法的缺陷同样也存在于这类方法中。</p>
                </div>
                <div class="p1">
                    <p id="69">有别于传统增强算法, 近年来发展出了一些基于学习的增强算法。Fotiadou等<citation id="242" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出一种基于稀疏表示的增强算法。首先, 分别在暗光条件下和正常光照条件对相同场景采集图像, 将暗光图像集和正常图像集用于联合字典学习, 得到两个图像集的联合字典及具有匹配关系的暗光条件编码与正常光照条件编码;然后, 用联合字典对输入的弱光照图像编码, 得到暗光条件编码, 根据匹配关系找到对应正常光照条件下的编码;最后, 再由正常光照条件下的编码和联合字典恢复出增强图像。在深度学习广泛运用于计算机视觉的各个领域的情况下, 也有研究人员尝试用深度学习来解决弱光照图像增强问题。Li等<citation id="243" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了卷积神经网络弱光照图像增强算法LNET (LightenNet) , 该算法使用卷积神经网络来估计光照图像, 然后使用引导滤波优化光照图像, 最后根据Retinex模型得到增强图像。Lore等<citation id="244" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation> 受到深度学习在图像去噪中应用的启发, 将一个经典的图像去噪自编码器SSDA (Stack Sparse Denoising Autoencoder) 运用在弱光照图像增强上。文献<citation id="245" type="reference">[<a class="sup">17</a>,<a class="sup">18</a>]</citation>的尝试表明深度学习算法在弱光照图像增强问题上同样适用, 因此本文结合传统图像增强手段和卷积神经网络, 提出了基于卷积神经网络的弱光照图像增强算法。本文算法摆脱了Retinex模型的限制, 不需要估计光照图像或反射率图像, 直接端到端地实现弱光照图像增强。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">2 本文算法</h3>
                <div class="p1">
                    <p id="71">本文提出的卷积神经网络简称为MDIIN (Multiple Derived Image Inputs Network) 。MDIIN的作用是激活原始弱光照图像和由其生成的四张派生图, 输出增强图像。通过在合成数据集上的训练, MDIIN成功学习到了映射规律, 有效地实现了弱光照图像的增强。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">2.1 <b>派生图</b></h4>
                <div class="p1">
                    <p id="73">弱光照条件下采集到的图像存在以下问题:图像的对比度低, 整体亮度偏低, 暗处区域的细节不清晰。针对以上的问题, 本文算法首先采用传统增强手段生成四种派生图, 这四种派生图在对比度、亮度、颜色饱和度上均优于原图。</p>
                </div>
                <div class="p1">
                    <p id="74">1) 限制对比度自适应直方图均衡派生图:直方图均衡可提升图像的对比度, 提高图像中弱光照区域的亮度。直方图均衡的方法有很多种, 简单的直方图均衡计算复杂度低、耗时短;但是由于这种方法是全局均衡, 对于整体亮度偏低的图像的增强效果有限, 而且可能导致颜色失真。为了克服简单直方图均衡的缺点, 文献<citation id="246" type="reference">[<a class="sup">9</a>]</citation>中提出了限制对比度自适应直方图均衡算法CLAHE (Contrast Limited Adaptive Histogram Equalization) , CLAHE的分块操作使图像的整体亮度得到提升, 亮度分布也更加均匀。CLAHE算法中带阈值限制的图像子块直方图均衡可适当地增强图像的对比度。因此, 本文选取CLAHE算法生成第一张派生图<i>I</i><sub>ch</sub>:</p>
                </div>
                <div class="p1">
                    <p id="75"><i>I</i><sub>ch</sub>=<i>CLAHE</i> (<i>I</i>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="76">2) 伽马变换派生图:伽马变换是一种非线性地改变图像亮度的方法。本文选用伽马变换生成第二张派生图<i>I</i><sub>gm</sub>:</p>
                </div>
                <div class="p1">
                    <p id="77"><i>I</i><sub>gm</sub>=<i>αI</i><sup><i>γ</i></sup>      (3) </p>
                </div>
                <div class="p1">
                    <p id="78">当<i>γ</i>&lt;1时, 伽马变换可提升图像的亮度, 增强图像暗处的细节。本文中<i>α</i>=1, <i>γ</i>=0.4。如图2 (c) 所示, 伽马变换可有效提高图像的整体亮度。</p>
                </div>
                <div class="p1">
                    <p id="79">3) 对数变换派生图:与伽马变换一样, 对数变换也是一种非线性地改变图像亮度的方法。两者的区别是伽马变换对低亮度区域的亮度提升作用更大, 对数变换对高亮度区域的亮度提升作用更大。本文算法中, 两种变换互为补充, 能够更合理地提升图像亮度。对数变换公式如下:</p>
                </div>
                <div class="p1">
                    <p id="80"><i>I</i><sub>log</sub>=<i>c</i>·log<sub> (1+<i>v</i>) </sub> (1+<i>I</i>·<i>v</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="81">本文中<i>c</i>=1, <i>v</i>=10。</p>
                </div>
                <div class="p1">
                    <p id="82">4) 亮通道增强派生图:首先将原始弱光照图像的亮通道图像当作光照图像<i>L</i><citation id="247" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 由式 (1) 可得到反射率图像<i>R</i>。对原始图像做<i>α</i>=1, <i>γ</i>=0.5的伽马变换得到增强的光照图像<i>L</i><sub>en</sub>, <i>L</i><sub>en</sub>和<i>R</i>相乘得到亮通道增强派生图<i>I</i><sub>le</sub>:</p>
                </div>
                <div class="p1">
                    <p id="83"><i>L</i>=max (<i>I</i> (<i>R</i>, <i>G</i>, <i>B</i>) )      (5) </p>
                </div>
                <div class="p1">
                    <p id="84"><i>R</i>=<i>I</i>/<i>L</i>      (6) </p>
                </div>
                <div class="p1">
                    <p id="85"><i>L</i><sub>en</sub>=<i>I</i><sup>0.5</sup>      (7) </p>
                </div>
                <div class="p1">
                    <p id="86"><i>I</i><sub>le</sub>=<i>L</i><sub>en</sub>·<i>R</i>      (8) </p>
                </div>
                <div class="p1">
                    <p id="87">如图2 (e) 所示, 相对于原图, 亮通道增强派生图的颜色饱和度、亮度都有提升。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 派生图样例" src="Detail/GetImg?filename=images/JSJY201904037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 派生图样例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Examples of derivative images</p>

                </div>
                <h4 class="anchor-tag" id="89" name="89">2.2 <b>网络结构</b></h4>
                <div class="p1">
                    <p id="90">MDIIN是一个基于Encoder-Decoder结构的网络。在图像去噪<citation id="248" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、颜色校正<citation id="249" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、图像去雾<citation id="250" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、延时摄影视频生成<citation id="251" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>等领域中, Encoder-Decoder结构得到了广泛的应用, 这说明Encoder-Decoder结构非常适用于图像生成网络。</p>
                </div>
                <div class="p1">
                    <p id="91">MDIIN在Encoder-Decoder结构中增加了跳跃连接, 将Encoder的特征图和浅层Decoder的特征图输入到Decoder的最后一层。跳跃连接在网络浅层和深层之间增加了通路, 可以大幅加快网络训练的收敛速度<citation id="252" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 而且跳跃连接让每一层的特征图得到了更充分的利用, 有助于生成细节更清晰的增强图像。</p>
                </div>
                <div class="p1">
                    <p id="92">图3为MDIIN的结构。MDIIN包含15个卷积层和3个反卷积层, 卷积操作的步长都为1, 每个卷积层和反卷积层后面都连接着一个Leaky ReLU (Leaky Rectified Linear Unit) 激活层, Leaky ReLU激活函数的负半区的斜率为0.1。MDIIN中的所有卷积操作都是空洞卷积, 空洞卷积未增加网络的计算量, 却能增大局部感受野的大小, 利用更多的图像信息。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904037_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 MDIIN网络结构" src="Detail/GetImg?filename=images/JSJY201904037_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 MDIIN网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904037_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structure of MDIIN</p>

                </div>
                <div class="p1">
                    <p id="94">MDIIN的网络参数设置如表1所示, 其中:Block表示卷积块类型, Layer表示层类型, conv表示卷积层, deconv表示反卷积层, Weight Dimension表示卷积核参数维度, Dilation表示空洞卷积的间隔像素点个数, Padding表示卷积时边缘补充像素的数量。网络输入为弱光照图像及其四张派生图, 因此Encoder1的输入通道数为15, 输出为增强图像, 因此Decoder3输出通道数为3。将Encoder1、Encoder2、Encoder3、Decoder1、Decoder2的输出和Decoder3中第二个卷积层的输出聚合, 然后全部输入到Decoder3的最后一个卷积层, 因此Decoder3的最后一个卷积层的输入通道数为192。</p>
                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表</b>1 MDIIN<b>网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Parameters of MDIIN</p>
                    <p class="img_note"></p>
                    <table id="95" border="1"><tr><td><br />Block</td><td>#</td><td>Layer</td><td>Weight Dimension</td><td>Dilation</td><td>Padding</td></tr><tr><td rowspan="3"><br />Encoder1</td><td><br />1</td><td>conv</td><td>32×15×7×7</td><td>3</td><td> (9, 9) </td></tr><tr><td><br />2</td><td>conv</td><td>32×32×3×3</td><td>3</td><td> (3, 3) </td></tr><tr><td><br />3</td><td>conv</td><td>32×32×3×3</td><td>3</td><td> (3, 3) </td></tr><tr><td rowspan="3"><br />Encoder2</td><td><br />1</td><td>conv</td><td>32×32×3×3</td><td>3</td><td> (3, 3) </td></tr><tr><td><br />2</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />3</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td rowspan="3"><br />Encoder3</td><td><br />1</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />2</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />3</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td rowspan="3"><br />Decoder1</td><td><br />1</td><td>deconv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />2</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />3</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td rowspan="3"><br />Decoder2</td><td><br />1</td><td>deconv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />2</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />3</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td rowspan="3"><br />Decoder3</td><td><br />1</td><td>deconv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />2</td><td>conv</td><td>32×32×3×3</td><td>1</td><td> (1, 1) </td></tr><tr><td><br />3</td><td>conv</td><td>3×192×3×3</td><td>1</td><td> (1, 1) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">2.3 <b>损失函数</b></h4>
                <div class="p1">
                    <p id="97">本文采用均方误差 (Mean Square Error, MSE) 和<i>L</i><sub>1</sub>范数损失作为MDIIN的损失函数。在图像生成类任务中, MSE是最常用的损失函数。近期有研究显示, 训练中使用复合形式的损失函数比使用单一的MSE损失函数能得到表现更好的网络<citation id="253" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。因此, 除MSE外, 本文还加入<i>L</i><sub>1</sub>范数损失以提高增强结果的图像质量。最终的损失函数公式如下:</p>
                </div>
                <div class="area_img" id="172">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201904037_17200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="100">其中:<i>N</i>表示训练样本对的个数;<i>I</i><sub><i>i</i></sub>表示训练样本中第<i>i</i>个弱光照图像;<i>J</i><sub><i>i</i></sub>表示与<i>I</i><sub><i>i</i></sub>对应的正常光照图像;<i>F</i>表示MDIIN的激活作用;<i>w</i>表示MDIIN的参数;本文算法中<i>α</i>=0.7, <i>β</i>=0.3。</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="102" name="102">3.1 <b>实验环境与参数设置</b></h4>
                <div class="p1">
                    <p id="103">MDIIN训练时, 输入图像块大小设置为128×128, 优化算法为ADAM (Adaptive Moment Estimation) <citation id="254" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 批处理图像块数量为10, 初始学习率为0.000 01, 每20 000次迭代学习率衰减75%, 总迭代次数为60 000。训练使用的GPU型号为Nvidia K80, 在该环境下, 训练一次耗时12 h。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">3.2 <b>训练数据</b></h4>
                <div class="p1">
                    <p id="105">训练数据直接决定了卷积神经网络能学习到什么映射规律。在弱光照图像增强领域, 目前还没有既包含弱光照图像又包含其对应正常光照图像的数据集, 因此, 本文利用正常清晰光照的图像合成弱光照图像。</p>
                </div>
                <div class="p1">
                    <p id="106">图4是一个合成弱光照图像的例子。首先, 在互联网和其他图像数据集上找到600张光照正常对比度高的图像;然后, 将图像转换到HSV (Hue Saturation Value) 空间, 对<i>V</i>通道图像<i>V</i>做伽马变换得到合成弱光照图像的<i>V</i>通道图像<i>V</i><sub>dark</sub>:<i>V</i><sub>dark</sub>=<i>αV</i><sup><i>γ</i></sup>, 其中<i>α</i>∈ (0.8, 1) , <i>γ</i>∈ (1.8, 3.4) ;然后用<i>V</i><sub>dark</sub>替换<i>V</i>, 其他两个通道不变, 转换回RGB (Red Green Blue) 空间得到合成弱光照图像。对每一张正常光照图像, 随机选取7组参数, 生成7张弱光照图像, 最终一共得到4 200张训练图像。从中选取由100张正常光照图像合成的700张合成图像作为测试集, 剩余的作为训练集。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 合成弱光照图像例子" src="Detail/GetImg?filename=images/JSJY201904037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 合成弱光照图像例子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Examples of synthetic weakly illuminated images</p>

                </div>
                <h4 class="anchor-tag" id="108" name="108">3.3 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="109">为了验证本文算法的有效性, 本文在真实弱光照图像和合成弱光照图像上均进行了对比实验。对比的算法有:Dong算法<citation id="255" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、LIME算法<citation id="256" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、MF算法<citation id="257" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、NPEA<citation id="258" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、LNET算法<citation id="259" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。各算法的参数值均为原文献中的推荐值。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">3.3.1 合成弱光照图像增强对比实验</h4>
                <div class="p1">
                    <p id="111">首先, 在合成弱光照图像上将本文算法与其他算法对比。合成弱光照图像的方法和3.2节中网络训练集的合成方法相同, 总共选取50张合成图片。为了保证实验的公正性, 这50张图片都未在训练集中出现过。为了说明本文算法在不同光照条件下的鲁棒性, 本文选取的合成图像中场景包括室内、野外、图像的光照条件有整体光照偏弱、光照不均匀等。</p>
                </div>
                <div class="p1">
                    <p id="112">图5是部分实验结果。观察图5可看出, Dong算法对图像的亮度有一定提升, 但是提升能力不足, 图像整体亮度仍然很低, 细节仍然不清晰。LIME算法的增强效果非常明显, 增强后图像的平均亮度也是所有算法中最高的, 但是这也带来了增强过度的问题, 图像中出现过曝和失真现象, 如图5的station中火车头左上角的光晕和ride中的石子路。LNET算法在station上增强效果较好, 在owl上不明显, owl增强后亮度仍然偏暗, 在room中出现了过曝现象, 如地板和窗帘的亮度。MF算法和NPEA的增强图像的亮度介于Dong算法和LIME算法之间。由station的增强结果可看出, 这两种算法对光照不均匀但平均亮度较高的图像的增强效果不错, 但是当图像整体亮度都偏暗时, 如owl和ride, 增强后图像亮度仍偏暗、视觉效果欠佳。这也说明MF, NPEA的鲁棒性不够好, 需要针对不同场景和光照情况调整参数才能得到较为理想的结果。而本文算法是基于学习的算法, 在训练中学习了不同的场景和光照条件, 对于光照各异的输入图片, 无需调整任何参数就能输出较理想的增强结果。图5中也能看出, 本文算法的增强结果最接近正常光照图像, 增强后的图像亮度适中、颜色自然、细节清晰、具有最好的视觉效果。所以, 从主观视觉感受上来说, 本文算法比其他算法更有优势。</p>
                </div>
                <div class="p1">
                    <p id="113">除了主观评判外, 本文还在MSE、峰值信噪比 (Peak Signal-to-Noise Ratio, PSNR) 、结构相似度 (Structural SIMilarity index, SSIM) 三项客观指标上对几种算法进行对比。表2是几种算法在50张合成样本上实验结果的平均指标。表2可看出, 本文算法的平均MSE、PSNR指标是所有算法中最优的。而且, 尽管本文算法在训练时使用的损失函数是MSE和<i>L</i><sub>1</sub>范数, 但在SSIM指标对比中, 本文算法仍然是所有算法中最优。在合成图像上实验结果表明, 无论是从主观视觉感受还是客观评价指标来说, 本文算法对弱光照图像的增强能力均优于其他几种算法。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904037_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 各算法在合成弱光照图像上实验的部分结果" src="Detail/GetImg?filename=images/JSJY201904037_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 各算法在合成弱光照图像上实验的部分结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904037_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Partial experimental results of each algorithm on synthetic weakly illuminated images</p>

                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表</b>2 <b>各算法在合成弱光照图像上实验的</b>MSE、PSNR、SSIM<b>指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 MSE, PSNR, SSIM metrics of each algorithm on synthetic weakly illuminated images</p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />算法</td><td>MSE</td><td>PSNR/dB</td><td>SSIM/%</td></tr><tr><td><br />Dong算法<sup>[15]</sup></td><td>2 630.48</td><td>14.39</td><td>69.02</td></tr><tr><td><br />LIME算法<sup>[10]</sup></td><td>491.22</td><td>21.53</td><td>87.33</td></tr><tr><td><br />MF算法<sup>[14]</sup></td><td>844.47</td><td>19.74</td><td>86.27</td></tr><tr><td><br />NPEA<sup>[13]</sup></td><td>1 133.45</td><td>18.17</td><td>83.18</td></tr><tr><td><br />LNET算法<sup>[17]</sup></td><td>971.60</td><td>19.04</td><td>83.38</td></tr><tr><td><br />本文MDIIN算法</td><td><b>482.59</b></td><td><b>22.13</b></td><td><b>88.20</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">3.3.2 真实弱光照图像增强对比实验</h4>
                <div class="p1">
                    <p id="117">为了进一步说明本文算法的优势, 本文也在真实弱光照图像上进行了对比实验, 图6是实验结果。从图6可看出, 各算法在真实图像上的增强表现跟合成图像上的表现基本一致:Dong算法增强后图像亮度整体偏暗, 细节处的视觉效果提升不明显, 如girl中的车窗和mountain中的山体部分;MF算法的鲁棒性一般, 部分图片增强后亮度较好, 如girl、house、duck, 其他剩余图片仍然偏暗;NPEA的增强结果整体偏暗, 且出现失真, 如tower中右侧树枝的亮度不自然;从meeting和girl的增强结果可看出, LIME算法依然存在增强过度的问题, 增强后图像的光照不自然;LNET中增强过度的现象更明显, 如tower、house中的天空与road中的马路, 都出现了过曝问题;本文算法的增强结果较为自然, 在提升原图暗处的同时, 很好地保持了原图亮度区域不过曝, 增强后的图像有良好的视觉观感。</p>
                </div>
                <div class="p1">
                    <p id="118">为了更客观地对比各算法的增强表现, 还需要测试实验结果的客观评价指标。不同于合成图像, 真实弱光照图像无对应的正常光照图像, 无法测试MSE、PSNR等指标。本文以图像的自然图像质量评价度量 (Natural Image Quality Evaluator, NIQE) 、熵 (Entropy) 、对比度增益 (Contrast Gain) 三项指标来对比各算法。NIQE是一个根据图像的自然统计特征得到的图像质量参考值, 数值越小表示图像质量越高;熵是度量图像中信息量多少的指标, 可以反映图像细节丰富程度, 熵越大, 说明图像包含信息越多, 细节越丰富;对比度增益反映了图像增强前后对比度提升的程度, 值越大表示对比度提升越明显。</p>
                </div>
                <div class="p1">
                    <p id="119">表3中记录了图6实验结果的三项指标。具体分析如下:</p>
                </div>
                <div class="p1">
                    <p id="120">1) 在NIQE的对比中, 本文算法在其中的四张图片上排名第一, 在其中三张图片上排名第二, 在其中一张图片上排名第四, 平均排名第一。排名第四的图片名为house, 观察图6可知, 本文算法增强后, house中整体亮度过于平均, 未形成较高的对比度, 这可能是影响该图片NIQE指标的因素。综合比较NIQE指标可以说明, 相对于其他方法, 本文算法增强后图像的有更高的图像质量。</p>
                </div>
                <div class="p1">
                    <p id="121">2) 在熵的对比中, 本文算法在其中的四张图片上排名第一, 在其中的两张图片上排名第二, 另外两张图片mountain和tower分别排名第三和第四, 平均排名第一。理论上熵最大的情况为图像中每个像素的灰度值都不一样, 本文算法增强结果在图片mountain中的山体部分和tower中的地面部分的灰度值分布变化很少, 因此这两张图片的熵指标相对较低。但是, 在八张图片上熵指标的综合比较中, 本文算法仍然是最优的。这说明相对于其他方法, 经本文算法增强后图像的细节更丰富。</p>
                </div>
                <div class="p1">
                    <p id="122">3) 在对比度增益的对比中, 本文算法的平均对比度增益排名第二, 排名第一的为LIME算法。前文的主观视觉感受对比中可以发现, LIME有过度增强的倾向, 因此LIME算法的高对比度增益是建立在过度增强的基础之上。</p>
                </div>
                <div class="p1">
                    <p id="123">通过对比NIQE、熵、对比度增益三项指标, 可以说明本文算法在真实弱光照图像上的增强表现更好, 本文算法增强后的图像有较高的图像质量与较丰富的细节;而且, 本文算法在保持增强效果自然性的前提下, 能较好地提升图像对比度。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904037_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 各算法在真实弱光照图像上实验结果" src="Detail/GetImg?filename=images/JSJY201904037_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 各算法在真实弱光照图像上实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904037_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Experimental results of each algorithm on real weakly illuminated images</p>

                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表</b>3 <b>各算法在真实弱光照图像上实验的</b>NIQE、<b>对比度增益、熵指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 NIQE, contrast gain, entropy metrics of each algorithm on real weakly illuminated images</p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td rowspan="2">图片</td><td colspan="3"><br />Dong算法</td><td rowspan="2"></td><td colspan="3"><br />LIME算法</td><td rowspan="2"></td><td colspan="3"><br />MF算法</td><td rowspan="2"></td><td colspan="3"><br />NPEA</td><td rowspan="2"></td><td colspan="3"><br />LNET算法</td><td rowspan="2"></td><td colspan="3"><br />本文MDIIN算法</td></tr><tr><td><br />N</td><td>E</td><td>C</td><td><br />N</td><td>E</td><td>C</td><td><br />N</td><td>E</td><td>C</td><td><br />N</td><td>E</td><td>C</td><td><br />N</td><td>E</td><td>C</td><td><br />N</td><td>E</td><td>C</td></tr><tr><td>mesh</td><td>3.92</td><td>7.36</td><td>2.17</td><td></td><td>4.28</td><td><b>7.78</b></td><td><b>6.01</b></td><td></td><td>4.23</td><td>7.60</td><td>3.49</td><td></td><td>4.13</td><td>7.51</td><td>2.89</td><td></td><td>4.23</td><td>7.59</td><td>3.13</td><td></td><td><b>3.57</b></td><td>7.62</td><td>3.39</td></tr><tr><td><br />meeting</td><td>3.77</td><td>7.72</td><td>1.49</td><td></td><td>3.83</td><td>7.71</td><td><b>2.69</b></td><td></td><td>3.45</td><td>7.73</td><td>1.61</td><td></td><td>3.22</td><td>7.62</td><td>1.25</td><td></td><td><b>3.08</b></td><td>7.80</td><td>1.42</td><td></td><td>3.09</td><td><b>7.80</b></td><td>1.53</td></tr><tr><td><br />girl</td><td>2.51</td><td>7.56</td><td>1.44</td><td></td><td>1.88</td><td>7.61</td><td><b>2.83</b></td><td></td><td>2.13</td><td>7.77</td><td>1.71</td><td></td><td>2.07</td><td>7.61</td><td>1.30</td><td></td><td>1.70</td><td>7.76</td><td>1.70</td><td></td><td><b>1.63</b></td><td><b>7.88</b></td><td>1.48</td></tr><tr><td><br />mountain</td><td>2.45</td><td>7.69</td><td>1.55</td><td></td><td><b>1.98</b></td><td>7.78</td><td><b>3.31</b></td><td></td><td>2.08</td><td>7.77</td><td>1.87</td><td></td><td>2.02</td><td>7.76</td><td>2.00</td><td></td><td>2.01</td><td><b>7.83</b></td><td>1.73</td><td></td><td>2.01</td><td>7.73</td><td>2.25</td></tr><tr><td><br />tower</td><td>3.58</td><td>7.46</td><td>2.24</td><td></td><td>3.09</td><td>7.62</td><td><b>5.51</b></td><td></td><td>3.37</td><td>7.47</td><td>2.90</td><td></td><td>3.00</td><td>7.08</td><td>2.41</td><td></td><td>3.09</td><td><b>7.73</b></td><td>3.37</td><td></td><td><b>2.98</b></td><td>7.46</td><td>3.14</td></tr><tr><td><br />road</td><td>2.59</td><td>7.65</td><td>1.39</td><td></td><td>2.32</td><td>7.75</td><td><b>3.28</b></td><td></td><td>2.40</td><td>7.73</td><td>1.68</td><td></td><td>2.52</td><td>7.81</td><td>1.36</td><td></td><td>2.28</td><td>7.77</td><td>1.70</td><td></td><td><b>2.10</b></td><td><b>7.86</b></td><td>1.64</td></tr><tr><td><br />house</td><td>2.47</td><td>7.59</td><td>1.65</td><td></td><td>2.18</td><td>7.63</td><td><b>3.95</b></td><td></td><td>2.18</td><td>7.82</td><td>2.17</td><td></td><td><b>1.94</b></td><td>7.59</td><td>1.72</td><td></td><td>2.08</td><td><b>7.91</b></td><td>2.64</td><td></td><td>2.36</td><td>7.82</td><td>2.39</td></tr><tr><td><br />duck</td><td>2.54</td><td>7.11</td><td>1.85</td><td></td><td>2.59</td><td>7.46</td><td><b>5.60</b></td><td></td><td>2.82</td><td>7.46</td><td>3.05</td><td></td><td>2.81</td><td>7.31</td><td>1.84</td><td></td><td><b>2.25</b></td><td>7.25</td><td>1.81</td><td></td><td>2.46</td><td><b>7.61</b></td><td>2.68</td></tr><tr><td><br />平均值</td><td>2.98</td><td>7.52</td><td>1.72</td><td></td><td>2.77</td><td>7.67</td><td><b>4.15</b></td><td></td><td>2.83</td><td>7.67</td><td>2.31</td><td></td><td>2.71</td><td>7.54</td><td>1.85</td><td></td><td>2.59</td><td>7.71</td><td>2.19</td><td></td><td><b>2.53</b></td><td><b>7.72</b></td><td>2.31</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:N表示NIQE, C表示对比度增益, E表示熵。</p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="126" name="126">3.3.3 派生图对增强结果的影响</h4>
                <div class="p1">
                    <p id="127">本文针对各派生图对增强结果的影响进行了分析。卷积神经网络的映射作用不可用解析表达式表示, 因此无法直接通过公式分析各派生图的作用。最终, 本文选择使用替换方法对此进行分析。具体实施过程为:保持MDIIN的其他输入不变, 将其中一张派生图用原始弱光照图像代替, 得到增强结果。</p>
                </div>
                <div class="p1">
                    <p id="128">图7中显示了将四张派生图依次用原始弱光照图像代替后的增强结果:由图7 (a) ～ (b) 中球面的纹路及衣服上的褶皱可看出, 限制对比度自适应直方图均衡派生图影响到增强结果的局部对比度, 将限制对比度自适应直方图均衡派生图用原图代替后, 图像的局部对比度降低;图7 (c) 与图7 (a) 相比, 图像的整体亮度降低, 这说明伽马校正派生图对增强结果的全局亮度有较大提升作用;图7 (d) 与图7 (a) 相比, 颜色饱和度严重下降, 这表明亮通道增强派生图起到了提升增强结果颜色饱和度的作用, 也间接影响到图像的主观视觉感受。图7 (e) 是将对数变换派生图用原图替换后的增强结果, 替换后, 图像出现过曝的现象, 这说明在MDIIN的映射中, 对数变换派生图与最终增强结果的亮度为负相关关系。</p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201904037_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 原图替换各派生图后的增强结果" src="Detail/GetImg?filename=images/JSJY201904037_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 原图替换各派生图后的增强结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201904037_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Enhanced results of replacing each derivative image with original weakly illuminated image</p>

                </div>
                <h4 class="anchor-tag" id="130" name="130">3.3.4 时间复杂度分析</h4>
                <div class="p1">
                    <p id="131">此外, 本文也对各算法的时间复杂度进行了比较。表4中记录了各算法分别处理mesh、meeting、girl、mountain四张图片的时间。Dong算法、LIME、MF、NPEA、LNET算法均在Matlab2017中运行, 本文算法在深度学习框架Pytorch中运行, 代码中使用GPU加速。电脑的CPU型号为Intel Core i7-7700HQ, GPU型号为Nvidia 1080TI。</p>
                </div>
                <div class="p1">
                    <p id="132">从表4可看出:NPEA的处理时间最长;LNET算法由于未使用GPU加速, 因此时间也较长;Dong算法、LIME、MF算法的处理时间较短, 均在1 s以内;本文算法的处理时间最短, 只需0.1 s左右。因此, 在算法处理速度上本文算法也具有优势。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表</b>4 <b>各算法处理图片的时间</b> s <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Images processing time of each algorithm s</p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td><br />算法</td><td>mesh</td><td>meeting</td><td>girl</td><td>mountain</td></tr><tr><td><br />Dong算法</td><td>0.97</td><td>0.35</td><td>0.30</td><td>0.58</td></tr><tr><td><br />LIME算法</td><td>0.99</td><td>0.46</td><td>0.43</td><td>0.70</td></tr><tr><td><br />MF算法</td><td>0.61</td><td>0.20</td><td>0.18</td><td>0.27</td></tr><tr><td><br />NPEA</td><td>11.61</td><td>5.40</td><td>5.36</td><td>9.96</td></tr><tr><td><br />LNET算法</td><td>6.61</td><td>2.93</td><td>2.81</td><td>5.60</td></tr><tr><td><br />本文MDIIN算法</td><td><b>0.17</b></td><td><b>0.08</b></td><td><b>0.08</b></td><td><b>0.14</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="135">本文提出一种将传统图像增强手段与深度学习算法结合的弱光照图像增强算法。与原始图像相比, 四种传统增强手段生成的派生图在对比度、亮度、颜色饱和度上均有提升。卷积神经网络可充分激活各派生图的优点从而输出光照明亮视觉效果良好的增强图像。本文算法不受Retinex模型约束, 无需估计光照图像和反射率图像, 直接端到端生成增强图像。由于训练数据集中包含丰富的场景和光照条件, 因此本文算法也无需调整任何参数, 在光照较弱和光照不均匀的情况下均表现突出。本文在合成弱光照图像和真实弱光照图像上均进行了对比实验, 实验结果从主观感受和客观评价指标两方面都验证了本文算法的有效性。与对比算法相比, 本文算法的增强图像在图像质量、图像细节丰富程度、图像对比度上均具有优势。</p>
                </div>
                <div class="p1">
                    <p id="136">在分析各派生图作用时, 本文算法中对数变换派生图与最终增强结果的亮度之间为负相关关系, 这有可能是伽马变换派生图与对数变换派生图作用重叠导致的。因此, 在下一步研究中, 尝试保留伽马变换派生图和对数变换派生图中的一个进行实验。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="173">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201509045&amp;v=MDIzMDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEaFZiN0FMejdCZDdHNEg5VE1wbzlCWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>康晓东, 王昊, 郭军, 等.无监督深度学习彩色图像识别方法[J].计算机应用, 2015, 35 (9) :2636-2639. (TANG X D, WANG H, GUO J, et al.Unsupervised deep learning method for color image recognition[J].Journal of Computer Applications, 2015, 35 (9) :2636-2639.) 
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806006&amp;v=MjA3NTN5RGhWYjdBTHo3QmQ3RzRIOW5NcVk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>杨朔, 陈丽芳, 石瑀, 等.基于深度生成式对抗网络的蓝藻语义分割[J].计算机应用, 2018, 38 (6) :1554-1561. (YANG S, CHEN L F, SHI Y, et al.Semantic segmentation of blue-green algae based on deep generative adversarial net[J].Journal of Computer Applications, 2018, 38 (6) :1554-1561.) 
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning for Image Denoising">

                                <b>[3]</b>LI H M.Deep learning for image denoising[J].International Journal of Signal Processing, Image Processing and Pattern Recognition, 2014, 7 (3) :171-180.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dehaze Net:an end-to-end system for single image haze removal">

                                <b>[4]</b>CAI B, XU X, JIA K, et al.Dehaze Net:an end-to-end system for single image haze removal[J].IEEE Transactions on Image Processing, 2016, 25 (11) :5187-5198.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single image dehazing via multi-scale convolutional neural networks">

                                <b>[5]</b>REN W, LIU S, ZHANG H, et al.Single image dehazing via multiscale convolutional neural networks[C]//ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:154-169.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">

                                <b>[6]</b>LIM B, SON S, KIM H, et al.Enhanced deep residual networks for single image super-resolution[C]//Proceedings of the 2017 IEEEConference on Computer Vision and Pattern Recognition Workshops.Piscataway, NJ:IEEE, 2017:136-144.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The retinex">

                                <b>[7]</b>LAND E H.The Retinex[J].American Scientist, 1964, 52 (2) :247-264.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contrast enhancement based on layered difference representation">

                                <b>[8]</b>LEE C, LEE C, KIM C S.Contrast enhancement based on layered difference representation of 2D histograms[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5372-5384.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003620786&amp;v=MzIyODB6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFc3dk9JVlk9Tmo3QmFyTzRIdEhQcVkxRlkrTUpZM2s1&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>PISANO E D, ZONG S, HEMMINGER B M, et al.Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms[J].Journal of Digital Imaging, 1998, 11 (4) :193-200.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LIME:Low-light image enhancement via illumination map estimation">

                                <b>[10]</b>GUO X, LI Y, LING H.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Probabilistic Method for Image Enhancement With Simultaneous Illumination and Reflectance Estimation.">

                                <b>[11]</b>FU X, LIAO Y, ZENG D, et al.A probabilistic method for image enhancement with simultaneous illumination and reflectance estimation[J].IEEE Transactions on Image Processing, 2015, 24 (12) :4965-4977.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A weighted variational model for simultaneous reflectanceand illumination estimation">

                                <b>[12]</b>FU X, ZENG D, HUANG Y, et al.A weighted variational model for simultaneous reflectance and illumination estimation[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2782-2790.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Naturalness preserved enhancement algorithm for non-uniform illumination images">

                                <b>[13]</b>WANG S, ZHENG J, HU H M, et al.Naturalness preserved enhancement algorithm for non-uniform illumination images[J].IEEETransactions on Image Processing, 2013, 22 (9) :3538-3548.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fusion-based enhancing method for weakly illuminated images">

                                <b>[14]</b>FU X, ZENG D, HUANG Y, et al.A fusion-based enhancing method for weakly illuminated images[J].Signal Processing, 2016, 129:82-96.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast efficient algorithm for enhancement of low lighting video">

                                <b>[15]</b>DONG X, WANG G, PANG Y, et al.Fast efficient algorithm for enhancement of low lighting video[C]//Proceedings of the 2011IEEE International Conference on Multimedia and Expo.Piscataway, NJ:IEEE, 2011:1-6.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Low Light Image Enhancement via Sparse Representations">

                                <b>[16]</b>FOTIADOU K, TSAGKATAKIS G, TSAKALIDES P.Low light image enhancement via sparse representations[C]//ICIAR 2014:Proceedings of the 2014 International Conference on Image Analysis and Recognition.Berlin:Springer, 2014:84-93.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAF0D84FE7B514C6C60215328AEA6E557&amp;v=MTQxNzBmT2ZjTE9IcVhFcS9rd1k1a0tEWGhLeVdVVjZqMThUWHpncEdOQUNMVGhRTCtZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekx5NXdhQT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>LI C, GUO J, PORIKLI F, et al.Lighten Net:a convolutional neural network for weakly illuminated image enhancement[J].Pattern Recognition Letters, 2018, 104:15-22.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LLNet:a deep autoencoder approach to natural low-light image enhancement">

                                <b>[18]</b>LORE K G, AKINTAYO A, SARKAR S.LLNet:a deep autoencoder approach to natural low-light image enhancement[J].Pattern Recognition, 2017, 61:650-662.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections">

                                <b>[19]</b>MAO X J, SHEN C, YANG Y B.Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections[C]//Proceedings of the 30th International Conference on Neural Information Processing Systems.New York:Curran Associates, 2016:2810-2818.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep image harmonization">

                                <b>[20]</b>TSAI Y H, SHEN X, LIN Z, et al.Deep image harmonization[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2799-2807.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gated fusion network for single image dehazing">

                                <b>[21]</b>REN W, MA L, ZHANG J, et al.Gated fusion network for single image dehazing[C]//Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:3253-3261.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks">

                                <b>[22]</b>XIONG W, LUO W, MA L, et al.Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks[C]//Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:2364-2373.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved techniques for learning to dehaze and beyond:a collective study">

                                <b>[23]</b>LIU Y, ZHAO G, GONG B, et al.Improved techniques for learning to dehaze and beyond:a collective study[EB/OL].[2018-07-26].https://arxiv.org/pdf/1807.00202.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ADAM:a method for stochastic optimization">

                                <b>[24]</b>KINGMA D P, BA J L.ADAM:a method for stochastic optimization[EB/OL].[2018-05-10].https://simplecore.intel.com/nervana/wp-content/uploads/sites/53/2017/06/1412.6980.pdf.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201801044&amp;v=MDY4NDk5bk1ybzlCWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEaFZiN0FMejdCZDdHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b>王一宁, 秦品乐, 李传朋, 等.基于残差神经网络的图像超分辨率改进算法[J].计算机应用, 2018, 38 (1) :246-254. (WANG Y N, QIN P L, LI C P, et al.Improved algorithm of image super resolution based on residual neural network[J].Journal of Computer Applications, 2018, 38 (1) :246-254.) 
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201805035&amp;v=MDQ3MjdmWnVac0Z5RGhWYjdBTHo3QmQ3RzRIOW5NcW85R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b>梁中豪, 彭德巍, 金彦旭, 等.基于交通场景区域增强的单幅图像去雾方法[J].计算机应用, 2018, 38 (5) :1420-1426. (LIANG Z H, PENG D W, JIN Y X, et al.Single image dehazing algorithm based on traffic scene region enhancement[J].Journal of Computer Applications, 2018, 38 (5) :1420-1426.) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201904037" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201904037&amp;v=MTI0NDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURoVmI3Qkx6N0JkN0c0SDlqTXE0OUdZNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
