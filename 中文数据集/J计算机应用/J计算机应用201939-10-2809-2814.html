<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136455204033750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910004%26RESULT%3d1%26SIGN%3dp8cXYd5wnaiyA7RKcYNJWx3oiTk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910004&amp;v=MjU5MzJyNDlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFU3dk1MejdCZDdHNEg5ak4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="1 基于改进的弹性网的神经网络优化模型 ">1 基于改进的弹性网的神经网络优化模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 改进的弹性网的相关性质 ">2 改进的弹性网的相关性质</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="2.1 &lt;i&gt;Oracle&lt;/i&gt;&lt;b&gt;性质&lt;/b&gt;">2.1 <i>Oracle</i><b>性质</b></a></li>
                                                <li><a href="#122" data-title="2.2 &lt;b&gt;群组选择能力&lt;/b&gt;">2.2 <b>群组选择能力</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#149" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#150" data-title="3.1 &lt;b&gt;实验数据集&lt;/b&gt;">3.1 <b>实验数据集</b></a></li>
                                                <li><a href="#154" data-title="3.2 &lt;b&gt;实验设置&lt;/b&gt;">3.2 <b>实验设置</b></a></li>
                                                <li><a href="#156" data-title="3.3 &lt;b&gt;实验分析&lt;/b&gt;">3.3 <b>实验分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="图1 深层神经网络示意图">图1 深层神经网络示意图</a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;实验中所使用的分类数据集&lt;/b&gt;"><b>表</b>1 <b>实验中所使用的分类数据集</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;实验中所使用的回归数据集&lt;/b&gt;"><b>表</b>2 <b>实验中所使用的回归数据集</b></a></li>
                                                <li><a href="#158" data-title="图2 四个数据集的对数损失变化曲线">图2 四个数据集的对数损失变化曲线</a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;四种方法在分类数据测试集的准确率&lt;/b&gt;"><b>表</b>3 <b>四种方法在分类数据测试集的准确率</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;四种方法在回归数据测试集的最终结果&lt;/b&gt;"><b>表</b>4 <b>四种方法在回归数据测试集的最终结果</b></a></li>
                                                <li><a href="#161" data-title="图3 分类数据集的训练集和测试集准确率之差">图3 分类数据集的训练集和测试集准确率之差</a></li>
                                                <li><a href="#162" data-title="图4 回归数据集的训练集和测试集损失之差">图4 回归数据集的训练集和测试集损失之差</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="240">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                    LECUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444.</a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                    SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1-9.</a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_3" title="陈文兵,管正雄,陈允杰.基于条件生成式对抗网络的数据增强方法[J].计算机应用,2018,38(11):3305-3311.(CHENW B,GUAN Z X,CHEN Y J.Data augmentation method based on conditional generative adversarial net model[J].Journal of Computer Applications,2018,38(11):3305-3311.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811046&amp;v=MDkzNTN5bmhVN3ZQTHo3QmQ3RzRIOW5Ocm85QllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        陈文兵,管正雄,陈允杰.基于条件生成式对抗网络的数据增强方法[J].计算机应用,2018,38(11):3305-3311.(CHENW B,GUAN Z X,CHEN Y J.Data augmentation method based on conditional generative adversarial net model[J].Journal of Computer Applications,2018,38(11):3305-3311.)
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_4" title="SRIVASTAVA N,HINTON G,KRIZHEVSKY A,et al.Dropout:a simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research,2014,15(1):1929-1958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">
                                        <b>[4]</b>
                                        SRIVASTAVA N,HINTON G,KRIZHEVSKY A,et al.Dropout:a simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research,2014,15(1):1929-1958.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_5" title="ZHANG Z,LUO P,LOY C C,et al.Facial landmark detection by deep multi-task learning[C]//Proceedings of the 2014 European Conference on Computer Vision,LNCS 8694.Cham:Springer,2014:94-108." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial landmark detection by deep multi-task learning">
                                        <b>[5]</b>
                                        ZHANG Z,LUO P,LOY C C,et al.Facial landmark detection by deep multi-task learning[C]//Proceedings of the 2014 European Conference on Computer Vision,LNCS 8694.Cham:Springer,2014:94-108.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_6" title="TIKHONOV A N.Solution of incorrectly formulated problems and the regularization method[J].Doklady Akademii Nauk SSSR,1963,151:501-504." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On solving incorrectly posed problems and method of regularization">
                                        <b>[6]</b>
                                        TIKHONOV A N.Solution of incorrectly formulated problems and the regularization method[J].Doklady Akademii Nauk SSSR,1963,151:501-504.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_7" title="ANTONIADIS A,FAN J.Regularization of wavelet approximations[J].Journal of the American Statistical Association,2001,96(455):939-967." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800007328&amp;v=MzIyMzE9TmpuQmFySzdIdGZPcDQ5RlpPc0lEMzR4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSVZvVWJoVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        ANTONIADIS A,FAN J.Regularization of wavelet approximations[J].Journal of the American Statistical Association,2001,96(455):939-967.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_8" title="LIAN L,LIU A,LAU V K N.Weighted LASSO for sparse recovery with statistical prior support information[J].IEEE Transactions on Signal Processing,2018,66(6):1607-1618." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weighted LASSO for Sparse Recovery with Statistical Prior Support Information">
                                        <b>[8]</b>
                                        LIAN L,LIU A,LAU V K N.Weighted LASSO for sparse recovery with statistical prior support information[J].IEEE Transactions on Signal Processing,2018,66(6):1607-1618.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_9" title="CUI C,WANG D.High dimensional data regression using Lasso model and neural networks with random weights[J].Information Sciences,2016,372:505-517." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES90950C8B796C60C44D6DB0C84D1D3062&amp;v=Mjg2MDF0cGh4YnkveEs4PU5pZk9mYnE0RjlUTTNJYzNZK0lKZjNvNXZCSVhuamtKT24rUnBCWkJlTWFYUmJ5ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        CUI C,WANG D.High dimensional data regression using Lasso model and neural networks with random weights[J].Information Sciences,2016,372:505-517.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_10" title="TANG J,DENG C,HUANG G.Extreme learning machine for multilayer perceptron[J].IEEE Transactions on Neural Networks and Learning Systems,2016,27(4):809-821." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ex-treme learning machine for multilayer perceptron">
                                        <b>[10]</b>
                                        TANG J,DENG C,HUANG G.Extreme learning machine for multilayer perceptron[J].IEEE Transactions on Neural Networks and Learning Systems,2016,27(4):809-821.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_11" title="PHAISANGITTISAGUL E.An analysis of the regularization between l2 and dropout in single hidden layer neural network[C]//Proceedings of the 7th International Conference on Intelligent System,Modelling and Simulation.Piscataway:IEEE,2016:174-179." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An analysis of the regularization between L2 and dropout in single hidden layer neural network">
                                        <b>[11]</b>
                                        PHAISANGITTISAGUL E.An analysis of the regularization between l2 and dropout in single hidden layer neural network[C]//Proceedings of the 7th International Conference on Intelligent System,Modelling and Simulation.Piscataway:IEEE,2016:174-179.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_12" title="JIN J,CHEN C L P.Regularized robust broad learning system for uncertain data modeling[J].Neurocomputing,2018,322:58-69." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAE7C402394B92BB4515C0FDF04A7797E&amp;v=MDU4NzNMVTA1dHBoeGJ5L3hLOD1OaWZPZmNMTkdhTElyNDFHYmU5OUJYNUx2UklXNnpvT1NBbVcyaEl4Q0xXVFRMM3FDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        JIN J,CHEN C L P.Regularized robust broad learning system for uncertain data modeling[J].Neurocomputing,2018,322:58-69.
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_13" title="李光早,王士同.基于稀疏表示和弹性网络的人脸识别[J].计算机应用,2017,37(3):901-905.(LI G Z,WANG S T.Face recognition based on sparse representation and elastic network[J].Journal of Computer Applications,2017,37(3):901-905.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201703052&amp;v=MDIwMjhadVpzRnluaFU3dlBMejdCZDdHNEg5Yk1ySTlBWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        李光早,王士同.基于稀疏表示和弹性网络的人脸识别[J].计算机应用,2017,37(3):901-905.(LI G Z,WANG S T.Face recognition based on sparse representation and elastic network[J].Journal of Computer Applications,2017,37(3):901-905.)
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_14" title="LI Q,SUN Y,WANG C,et al.Elastic net hypergraph learning for image clustering and semi-supervised classification[J].IEEETransactions on Image Processing,2017,26(1):452-463." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Elastic Net Hypergraph Learning for Image Clustering and Semi-Supervised Classification">
                                        <b>[14]</b>
                                        LI Q,SUN Y,WANG C,et al.Elastic net hypergraph learning for image clustering and semi-supervised classification[J].IEEETransactions on Image Processing,2017,26(1):452-463.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_15" title="KINGMA D P,BA J L.Adam:a method for stochastic optimization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6980.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A Method for Stochastic Optimization[C/OL]">
                                        <b>[15]</b>
                                        KINGMA D P,BA J L.Adam:a method for stochastic optimization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6980.pdf.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_16" title="MARQUARIDT D W.Generalized inverses,ridge regression,biased linear estimation,and nonlinear estimation[J].Technometrics,1970,12(3):591-612." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation">
                                        <b>[16]</b>
                                        MARQUARIDT D W.Generalized inverses,ridge regression,biased linear estimation,and nonlinear estimation[J].Technometrics,1970,12(3):591-612.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-19 08:55</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2809-2814 DOI:10.11772/j.issn.1001-9081.2019040624            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>改进的弹性网模型在深度神经网络中的应用</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E6%98%8E%E7%9A%93&amp;code=42897015&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯明皓</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%A4%A9%E4%BC%A6&amp;code=40891377&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张天伦</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%9E%97%E8%BE%89&amp;code=41849118&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王林辉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E8%8D%A3&amp;code=23451645&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈荣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%BF%9E%E5%B0%91%E9%9D%99&amp;code=36105482&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">连少静</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A7%E8%BF%9E%E6%B5%B7%E4%BA%8B%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0106810&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">大连海事大学信息科学技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E5%8C%97%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0106010&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河北大学数学与信息科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于具有较高的模型复杂度,深层神经网络容易产生过拟合问题,为了减少该问题对网络性能的不利影响,提出一种基于改进的弹性网模型的深度学习优化方法。首先,考虑到变量之间的相关性,对弹性网模型中的L1范数的不同变量进行自适应加权,从而得到L2范数与自适应加权的L1范数的线性组合。其次,将改进的弹性网络模型与深度学习的优化模型相结合,给出在这种新正则项约束下求解神经网络参数的过程。然后,推导出改进的弹性网模型在神经网络优化中具有群组选择能力和Oracle性质,进而从理论上保证该模型是一种更加鲁棒的正则化方法。最后,在多个回归问题和分类问题的实验中,相对于L1、L2和弹性网正则项,该方法的回归测试误差可分别平均降低87.09、88.54和47.02,分类测试准确度可分别平均提高3.98、2.92和3.58个百分点。由此,在理论和实验两方面验证了改进的弹性网模型可以有效地增强深层神经网络的泛化能力,提升优化算法的性能,解决深度学习的过拟合问题。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">正则化方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%B9%E6%80%A7%E7%BD%91%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">弹性网模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%87%E6%8B%9F%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">过拟合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    冯明皓(1995—),男,天津人,硕士研究生,主要研究方向:深度学习、最优化算法;;
                                </span>
                                <span>
                                    张天伦(1991—),男,河北保定人,博士研究生,主要研究方向:机器学习、计算视觉;;
                                </span>
                                <span>
                                    王林辉(1995—),男,山东烟台人,硕士研究生,主要研究方向:深度学习、文本分类;;
                                </span>
                                <span>
                                    *陈荣(1969—),男,辽宁大连人,教授,博士,CCF会员,主要研究方向:人工智能、软件工程;电子邮箱rchen.cs@gmail.com;
                                </span>
                                <span>
                                    连少静(1990—),女,河北邯郸人,硕士,主要研究方向:机器学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61672122,61402070,61602077);</span>
                                <span>辽宁省自然科学基金资助项目(20170540097,2015020023);</span>
                                <span>辽宁省科学事业公益研究基金资助项目(GY-2017-0005);</span>
                                <span>中央高校基本科研业务费资助项目(3132019207,3132019355);</span>
                    </p>
            </div>
                    <h1><b>Improved elastic network model for deep neural network</b></h1>
                    <h2>
                    <span>FENG Minghao</span>
                    <span>ZHANG Tianlun</span>
                    <span>WANG Linhui</span>
                    <span>CHEN Rong</span>
                    <span>LIAN Shaojing</span>
            </h2>
                    <h2>
                    <span>College of Information Science and Technology, Dalian Maritime University</span>
                    <span>College of Mathematics and Information Science, Hebei University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Deep neural networks tend to suffer from overfitting problem because of the high complexity of the model. To reduce the adverse eeffects of the problem on the network performance, an improved elastic network model based deep learning optimization method was proposed. Firstly, considering the strong correlation between the variables, the adaptive weights were assigned to different variables of L1-norm in elastic network model, so that the linear combination of the L2-norm and the adaptively weighted L1-norm was obtained. Then, the solving process of neural network parameters under this new regularization term was given by combining improved elastic network model with the deep learning optimization model. Moreover, the robustness of this proposed model was theoretically demonstrated by showing the grouping selection ability and Oracle property of the improved elastic network model in the optimization of neural network. At last, in regression and classification experiments, the proposed model was compared with L1-norm, L2-norm and elastic network regularization term, and had the regression error decreased by 87.09, 88.54 and 47.02 and the classification accuracy improved by 3.98, 2.92 and 3.58 percentage points respectively. Thus, theory and experimental results prove that the improved elastic network model can effectively improve the generalization ability of deep neural network model and the performance of optimization algorithm, and solve the overfitting problem of deep learning.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=regularization%20method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">regularization method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=elastic%20network%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">elastic network model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=overfitting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">overfitting;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    FENG Minghao,born in 1995,M.S.candidate.His research interests include deep learning,optimization algorithm.;
                                </span>
                                <span>
                                    ZHANG Tianlun,born in 1991,Ph.D.candidate.His research interests include machine learning,computer vision.;
                                </span>
                                <span>
                                    WANG Linhui,born in 1995,M.S.candidate.His research interests include deep learning,text categorization.;
                                </span>
                                <span>
                                    CHEN Rong,born in 1969,Ph.D.,professor.His research interests include artificial intelligence,software engineering.;
                                </span>
                                <span>
                                    LIAN Shaojing,born in 1990,M.S.Her research interests include machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61672122,61402070,61602077);</span>
                                <span>the Natural Science Foundation of Liaoning Province(20170540097,2015020023);</span>
                                <span>the Public Research Funds for Scientific Ventures of Liaoning Province(GY-2017-0005);</span>
                                <span>the Fundamental Research Funds for the Central Universities(3132019207,3132019355);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="36">近年来,深度学习<citation id="272" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>技术受到广泛关注,并在众多应用领域有着较好表现。理论上,深层神经网络能够拟合任意分布的数据,但是在实际中,有限的数据资源和较高的模型复杂度使得神经网络很难具有理想的泛化能力,过拟合现象由此产生。该现象是机器学习中一种常见的病态问题,解决该问题的方法通常分为三种:第一种是扩充训练数据规模,例如,Szegedy等<citation id="273" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>通过对原有图像样例进行翻转、裁剪等变化来增加训练数据,从而提升了神经网络在图像分类中的性能;与之不同,陈文兵等<citation id="274" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>通过生成小样本数据来增加训练样本的数量,这种合成数据的方法同样可以降低神经网络的过拟合程度。第二种是更改训练方式,最具代表性的工作是Dropout<citation id="275" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>,该方法通过在每次优化迭代中随机删除一些神经元来降低模型的复杂程度;除此之外,Zhang等<citation id="276" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>通过监控损失值的变化来决定网络训练的中止或重启,从而避免过拟合问题。第三种是在损失函数中引入正则项因子,对网络模型中的参数进行约束优化求解。与第一种方法相比,正则化方法没有增加额外的训练数据,从而不会加剧计算负担,同时,也不会受到冗余数据和噪声数据的影响;与第二种方法相比,正则化方法具有较好的收敛性,不会导致较长的训练周期。此外,正则化方法具有可靠的理论基础和严格的理论推导,因此,在解决过拟合问题的工作中,对正则化方法的研究与应用最为普遍。</p>
                </div>
                <div class="p1">
                    <p id="37">正则化方法最早由Tikhonov等<citation id="277" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在1963年提出,并于20世纪90年代以后,成为一种主流的解决机器学习中解决病态问题的有效方法。本质上,该方法是将正则项作为含有解的先验知识引进经验风险中,从而约束解空间的范围,进而获得理想中的稳定解。在理论上,Antoniadis等<citation id="278" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出一个理想的正则化方法应该具备以下四个性质:1)连续性。所求参数的估计值在范围内应当连续,以获得一个更稳定的解。2)无偏性。所求参数的估计值应当是近似无偏的,以获得一个偏差较小的模型。3)稀疏性。所求模型能够将较小参数直接压缩为0,降低模型的复杂程度。4)Oracle性质。正则项能够正确识别模型的能力,可用渐进正态性和变量选择一致性来解释。在该领域里,这四个性质被广泛用作评价正则化方法的标准,因此在设计正则化方法时,应尽量保证所提出的模型具备以上性质。</p>
                </div>
                <div class="p1">
                    <p id="38">在实际中,被广泛使用的正则化方法主要有:最小绝对值收敛和选择算子(Least Absolute Shrinkage and Selection Operator, LASSO)模型、岭回归模型和弹性网模型。其中:LASSO模型又被称为L1正则化方法,该方法可以保证求解结果具有稀疏性<citation id="279" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,降低原始数据维度,并可以过滤出重要的特征,因而该模型通常被用于高维数据的建模问题,例如,Cui等<citation id="280" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>利用LASSO模型构造特征池来对高维数据进行特征提取;Tang等<citation id="281" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出一种基于L1正则化方法的稀疏自动编码器模型。但是LASSO模型忽略了变量之间的相关性,不满足无偏性。岭回归模型又被称为L2正则化方法,这种方法在参数估计中限制较大的解,从而启发式地得到趋近于零的解,并且该方法可以保留变量之间的相关性,在样本维度高于样本规模时,可以获得光滑的稳定解。因此为了避免神经网络模型过于复杂,L2正则化方法常被用于惩罚神经网络中较大的参数<citation id="282" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>;同时,Jin等<citation id="283" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过在神经网络优化问题中引入L2正则项来提高神经网络对离群点数据的鲁棒性。但是L2正则化方法往往得到较稠密的解,因而不具备稀疏性。弹性网模型是前两种方法的线性组合,该模型既具备L1正则化的特征选择能力和稀疏性的特点,又具备L2正则化保留变量之间相关性的特点,因而该模型在防止过拟合的问题中有更加广泛的应用<citation id="284" type="reference"><link href="264" rel="bibliography" /><link href="266" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>;但是在理论上,弹性网很难满足无偏性和Oracle性质。</p>
                </div>
                <div class="p1">
                    <p id="39">为了更好地解决深度学习里的过拟合问题,本文在神经网络的损失函数中引入一种改进的弹性网模型,该模型可以被看作是L2正则化与加权的L1正则化的线性组合。在该模型里,L1正则化中的不同变量被自适应地赋予不同的权重因子,在优化神经网络参数时,这种加权的方式可以有选择性地保留重要的权重分量,使得到的网络参数具有稀疏性,从而降低过拟合的风险。同时,通过理论推导可以证明,在L2正则化的协同作用下,该模型具有合理的群组选择能力,因而相关性强的权重分量得以同时去除或者保留。除此之外,该模型还具备Oracle性质,可以保证对系数不为零的参数进行无偏估计。因而该模型是一种功能性更强的正则化模型。为了进一步验证该模型的实际效果,本文做了充分的对比实验,通过实验结果验证了该方法可以有效地防止深度学习中的过拟合问题,而且正则化的效果要明显优于现有的主流模型。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag">1 基于改进的弹性网的神经网络优化模型</h3>
                <div class="p1">
                    <p id="41">深层神经网络(如图1所示)的学习目标是在给定的样本集上最小化期望损失,可以定义为:</p>
                </div>
                <div class="p1">
                    <p id="42"><i>E</i>(<i>J</i>(<i><b>w</b></i>,<i><b>b</b></i>;<i><b>x</b></i>, <i><b>y</b></i><sup>*</sup>))=∫<sub><i><b>x</b></i></sub><i>J</i>(<i><b>w</b></i>,<i><b>b</b></i>;<i><b>x</b></i>, <i><b>y</b></i><sup>*</sup>)<i>p</i>(<i><b>x</b></i>)d(<i><b>x</b></i>)</p>
                </div>
                <div class="p1">
                    <p id="43">其中:<i><b>w</b></i>和<i><b>b</b></i>分别是网络的权重和偏置,也是深度学习的优化对象;<i><b>x</b></i>是神经网络的输入,即样本属性; <i><b>y</b></i><sup>*</sup>是样本的期望输出; <i>p</i>(<i><b>x</b></i>)为样本的概率密度函数;<i>J</i>(<i><b>w</b></i>,<i><b>b</b></i>;<i><b>x</b></i>, <i><b>y</b></i><sup>*</sup>)是关于权重<i><b>w</b></i>和偏置<i><b>b</b></i>的函数,该函数用来度量实际输出<i><b>y</b></i>和期望输出<i><b>y</b></i><sup>*</sup>之间的距离(可以是欧氏距离,也可以是Kullback-Leibler(K-L)散度)。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910004_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 深层神经网络示意图" src="Detail/GetImg?filename=images/JSJY201910004_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 深层神经网络示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910004_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Schematic diagram of deep neural network</p>

                </div>
                <div class="p1">
                    <p id="45">为了提升神经网络的性能,一种最有效的办法是通过加入正则化项对损失函数进行约束求解,有如下定义:</p>
                </div>
                <div class="p1">
                    <p id="46"><i>J</i>(<i><b>w</b></i>,<i><b>b</b></i>;<i><b>x</b></i>, <i><b>y</b></i><sup>*</sup>)+<i>R</i>(<i><b>w</b></i>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="47">其中:<i>R</i>(<i><b>w</b></i>)为正则化项。常用正则化项有:<mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></math></mathml>,即L1正则化;<mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>,即L2正则化;<mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>,即弹性网。<i>λ</i>为正则项参数,表示正则项在整体模型中占的比重。这些模型的性质已在引言中进行讨论,为了克服这些现有模型的不足,一种改进的弹性网正则化模型被引入到神经网络的优化过程中。</p>
                </div>
                <div class="p1">
                    <p id="48">首先,引入这种改进的弹性网模型:</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mrow><mo>|</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">其中:</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mo>=</mo><mo stretchy="false">(</mo><mrow><mo>|</mo><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow><mo>+</mo><mi>ε</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo>-</mo><mi>γ</mi></mrow></msup></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52"><i>ε</i>为一个非常小的数,用来防止分母为0;<i>w</i><sup>*</sup><sub><i>i</i></sub>为弹性网模型的参数优化结果;<i>λ</i><sub>2</sub>必须和弹性网的参数相同;<i>λ</i><sup>*</sup><sub>1</sub>可以和<i>λ</i><sub>1</sub>相同也可以不同;参数<i>γ</i>是一个正数。</p>
                </div>
                <div class="p1">
                    <p id="53">最后,将改进的弹性网模型代入式(1),则深度学习优化模型的定义可变形为:</p>
                </div>
                <div class="p1">
                    <p id="54"><i>J</i>(<i><b>w</b></i>,<i><b>b</b></i>;<mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mrow><mo>|</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="55">在分类实验里,损失函数为交叉熵(Cross Entropy, CE)函数,定义如下:</p>
                </div>
                <div class="p1">
                    <p id="56"><i>J</i><sub>CE</sub>(<i><b>w</b></i>,<i><b>b</b></i>;<mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo stretchy="false">(</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>y</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow><mo>*</mo></msubsup><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="57">其中:<i>c</i>为输出层神经元的个数;<i>n</i>为样本数。在回归实验里,损失函数为均方误差(Mean Squared Error, MSE),定义如下:</p>
                </div>
                <div class="p1">
                    <p id="58"><i>J</i><sub>MSE</sub>(<i><b>w</b></i>,<i><b>b</b></i>;<mathml id="171"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="59">具体地,这里选取目前最流行的深度学习优化算法为研究对象,即自适应矩估计(Adaptive moment estimation, Adam)<citation id="285" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。其中,在Adam模型中梯度的计算如下:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>J</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">w</mi></mrow></mfrac><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mspace width="0.25em" /><mrow><mi>sgn</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mn>2</mn></mstyle><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">其中:sgn(·)为符号函数,<mathml id="172"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>J</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi mathvariant="bold-italic">b</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">w</mi></mrow></mfrac></mrow></math></mathml>可以通过神经网络的BP算法得到。基于改进的弹性网的神经网络优化算法的流程如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="62">算法1 基于改进的弹性网的神经网络优化算法。</p>
                </div>
                <div class="p1">
                    <p id="63">输入 数据集(<i><b>X</b></i>,<i><b>Y</b></i>),Adam算法超参数,正则项系数<i>λ</i>。</p>
                </div>
                <div class="p1">
                    <p id="64">输出 训练后的模型参数。</p>
                </div>
                <div class="p1">
                    <p id="65">1)输入数据集,构建弹性网模型进行迭代:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>J</i>(<i><b>w</b></i>,<i><b>b</b></i>;<mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><mo>,</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="67">2)得到弹性网模型的参数后,按式(2)构建改进的弹性网。</p>
                </div>
                <div class="p1">
                    <p id="68">3)按式(3)构建新的优化模型,重新进行迭代。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 改进的弹性网的相关性质</h3>
                <h4 class="anchor-tag" id="70" name="70">2.1 <i>Oracle</i><b>性质</b></h4>
                <div class="p1">
                    <p id="71">为了防止反向传播中的梯度消失现象,通常采用的激活函数为修正线性单元(<i>Rectified Linear Unit</i>, <i>ReLU</i>),此函数的形式如下:</p>
                </div>
                <div class="p1">
                    <p id="72"><i>f</i>(<i>x</i>)=max(0,<i>x</i>)</p>
                </div>
                <div class="p1">
                    <p id="73">不失一般性,考虑神经元输出不为零的情况,此时一个神经元的前向计算可以被视为一个线性回归过程。为了表述方便,下面的证明中偏置暂时不被考虑。设第<i>l</i>层神经元的输入为<i>n</i>×<i>c</i><sup><i>l</i></sup><sup>-1</sup>维度的矩阵<i><b>X</b></i>,设期望输出为<i>n</i>×<i>c</i><sup><i>l</i></sup>维度的矩阵<i><b>Y</b></i>,其中<i>c</i><sup><i>l</i></sup><sup>-1</sup>和<i>c</i><sup><i>l</i></sup>分别表示第<i>l</i>层和第<i>l</i>-1层神经元的个数。在衡量实际输出与期望输出的距离时,使用欧氏距离作为标准,此时优化目标可以表示如下:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mo stretchy="false">(</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>c</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>c</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>c</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>c</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mover accent="true"><mi>w</mi><mo>^</mo></mover></mstyle><msub><mrow></mrow><mi>j</mi></msub><mrow><mo>|</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>c</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>c</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">Marquardt<citation id="286" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出:对输入进行一些改造后,岭回归可以被视为最小二乘估计。基于此观点,对输入<i><b>X</b></i>进行改造后,优化目标可以转换为如下问题:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mo stretchy="false">(</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>c</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>c</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>c</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>c</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mover accent="true"><mi>w</mi><mo>^</mo></mover></mstyle><msub><mrow></mrow><mi>j</mi></msub><mrow><mo>|</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中:<image id="237" type="formula" href="images/JSJY201910004_23700.jpg" display="inline" placement="inline"><alt></alt></image>,维度为(<i>n</i>+<i>c</i><sup><i>l</i></sup>)×<i>c</i><sup><i>l</i></sup><sup>-1</sup>;<image id="238" type="formula" href="images/JSJY201910004_23800.jpg" display="inline" placement="inline"><alt></alt></image>,维度为(<i>n</i>+<i>c</i><sup><i>l</i></sup>)×<i>c</i><sup><i>l</i></sup>,根据改造后的数据集(<i><b>X</b></i><sup>*</sup>,<i><b>Y</b></i><sup>*</sup>),有:</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo>+</mo><mi mathvariant="bold-italic">ε</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></math></mathml>,对于固定的<i>λ</i><sub>2</sub>,<i>ε</i><sup>*</sup><sub><i>i</i></sub>独立同分布服从N(0,<i>σ</i><sup>2</sup>),<mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover></math></mathml>为模型的真实参数;</p>
                </div>
                <div class="p1">
                    <p id="83"><mathml id="176"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>2</mn><mo stretchy="false">)</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>n</mi><mo>*</mo></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>n</mi></msub><mo>+</mo><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mi>n</mi></mfrac><mi mathvariant="bold-italic">Ι</mi><mo>→</mo><mi mathvariant="bold-italic">D</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="84">不妨设<mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mo>=</mo><mo stretchy="false">{</mo><mi>j</mi><mo stretchy="false">|</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>≠</mo><mn>0</mn><mo stretchy="false">}</mo></mrow></math></mathml>,总共有<i>p</i><sub>0</sub>个不为0的真实参数,<i>A</i><sup>*</sup>={<i>i</i>:<mathml id="178"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>≠</mo><mn>0</mn><mo stretchy="false">}</mo></mrow></math></mathml>,矩阵<i><b>D</b></i>可以分块为<image id="239" type="formula" href="images/JSJY201910004_23900.jpg" display="inline" placement="inline"><alt></alt></image>,其中<i><b>D</b></i><sub>1</sub>的维度为<i>p</i><sub>0</sub>×<i>p</i><sub>0</sub>,定义为不等于0的参数对应的样本属性。</p>
                </div>
                <div class="p1">
                    <p id="87"><b>定理</b>1 Oracle性质。假设(<i>λ</i><sup>*</sup><sub>1</sub>/<i>n</i>)→0,<i>λ</i><sup>*</sup><sub>1</sub><i>n</i><sup>(</sup><sup><i>γ</i></sup><sup>-1)/2</sup>→∞并且(<i>λ</i><sub>2</sub>/<i>n</i>)→0,则Oracle性质满足:</p>
                </div>
                <div class="p1">
                    <p id="88">1)渐进正态性,<mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mi>n</mi></msqrt><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>A</mi><mo>*</mo></msubsup><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>A</mi></msub><mo stretchy="false">)</mo><mover><mo stretchy="true" symmetric="false">→</mo><mi>d</mi></mover><mtext>Ν</mtext><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="89">2)变量选择一致性,<mathml id="180"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>lim</mi></mrow></mstyle><mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow></munder><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false">(</mo><mi>A</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="90">证明</p>
                </div>
                <div class="p1">
                    <p id="91">对性质1的证明:</p>
                </div>
                <div class="p1">
                    <p id="92">令<mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><mo>=</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo>+</mo><mfrac><mi mathvariant="bold-italic">u</mi><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow></math></mathml>,其中<i><b>u</b></i>为任意<i>c</i><sup><i>l</i></sup><sup>-1</sup>×<i>c</i><sup><i>l</i></sup>维矩阵,定义为:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ψ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mrow><mo>(</mo><mrow><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo>+</mo><mfrac><mi mathvariant="bold-italic">u</mi><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>c</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>c</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mover accent="true"><mi>w</mi><mo>^</mo></mover></mstyle><msub><mrow></mrow><mi>j</mi></msub><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mfrac><mrow><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow><mo>|</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">令<mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">u</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow><mspace width="0.25em" /><mi>ψ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">)</mo></mrow></math></mathml>,则<mathml id="183"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">u</mi><mo>^</mo></mover><mo>=</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo stretchy="false">)</mo></mrow></math></mathml>。其中:<mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup></mrow></math></mathml>为模型参数估计值,<mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover></math></mathml>为模型参数真实值,并且</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ψ</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>c</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>c</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mover accent="true"><mi>w</mi><mo>^</mo></mover></mstyle><msub><mrow></mrow><mi>j</mi></msub><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">那么经过化简,可以得到:</p>
                </div>
                <div class="p1">
                    <p id="97"><i>ψ</i>(<i><b>u</b></i>)-<i>ψ</i>(<b>0</b>)=<i>V</i><sub>1</sub>+<i>V</i><sub>2</sub>+<i>V</i><sub>3</sub>+<i>V</i><sub>4</sub></p>
                </div>
                <div class="p1">
                    <p id="98">其中:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>V</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi mathvariant="bold-italic">u</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mfrac><mrow><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msup><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mi>n</mi></mfrac><mi mathvariant="bold-italic">u</mi></mtd></mtr><mtr><mtd><mi>V</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mo>-</mo><mi mathvariant="bold-italic">ε</mi><msup><mrow></mrow><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msup><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mfrac><mi mathvariant="bold-italic">u</mi><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mtd></mtr><mtr><mtd><mi>V</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mo>-</mo><mfrac><mrow><mi mathvariant="bold-italic">u</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msup><mi mathvariant="bold-italic">ε</mi><msup><mrow></mrow><mo>*</mo></msup></mtd></mtr><mtr><mtd><mi>V</mi><msub><mrow></mrow><mn>4</mn></msub><mo>=</mo><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>c</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>c</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mover accent="true"><mi>w</mi><mo>^</mo></mover></mstyle><msub><mrow></mrow><mi>j</mi></msub><msqrt><mi>n</mi></msqrt><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mfrac><mrow><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow><mo>|</mo></mrow><mo>-</mo><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">容易得知:</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi mathvariant="bold-italic">ε</mi><msup><mrow></mrow><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msup><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mo>,</mo><mfrac><mrow><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msup><mi mathvariant="bold-italic">ε</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mover><mo stretchy="true" symmetric="false">→</mo><mi>d</mi></mover><mi>Q</mi><mo>=</mo><mtext>Ν</mtext><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>n</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">作出如下假设:</p>
                </div>
                <div class="p1">
                    <p id="103">(A)存在序列{<i>a</i><sub><i>n</i></sub>},使得<i>a</i><sub><i>n</i></sub>→∞,并且有</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>lim</mi></mrow></mstyle><mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow></munder><mspace width="0.25em" /><mspace width="0.25em" /><mi>a</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>lim</mi></mrow></mstyle><mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow></munder><mtext> </mtext><mspace width="0.25em" /><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>lim</mi></mrow></mstyle><mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow></munder><mspace width="0.25em" /><mspace width="0.25em" /><mi>a</mi><msubsup><mrow></mrow><mi>n</mi><mi>γ</mi></msubsup><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mo>→</mo><mi>∞</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">对于<i>V</i><sub>4</sub>部分,可分情况来讨论:</p>
                </div>
                <div class="p1">
                    <p id="106">1)当<i>u</i><sub><i>j</i></sub>=0时,易得<mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mn>4</mn></msub><mover><mo stretchy="true" symmetric="false">→</mo><mi>p</mi></mover><mn>0</mn></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="107">2)当<mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mo>≠</mo><mn>0</mn><mo>,</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>≠</mo><mn>0</mn></mrow></math></mathml>时,有<mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mover><mo stretchy="true" symmetric="false">→</mo><mi>p</mi></mover><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mi>γ</mi></mrow></msup><mo>,</mo><msqrt><mi>n</mi></msqrt><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mfrac><mrow><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow><mo>|</mo></mrow><mo>-</mo><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mo>→</mo><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mspace width="0.25em" /><mrow><mi>sgn</mi></mrow><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>,根据Slutsky定理的得:<mathml id="189"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mn>4</mn></msub><mover><mo stretchy="true" symmetric="false">→</mo><mi>p</mi></mover><mn>0</mn></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="108">3)当<mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mo>≠</mo><mn>0</mn><mo>,</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mn>0</mn></mrow></math></mathml>时,<mathml id="191"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mi>n</mi></msqrt><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mfrac><mrow><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow><mo>|</mo></mrow><mo>-</mo><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mo>=</mo><mrow><mo>|</mo><mrow><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>,</mo><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mo stretchy="false">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo><msup><mrow></mrow><mi>γ</mi></msup><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><msqrt><mi>n</mi></msqrt><mi>w</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup></mrow></mfrac></mrow></math></mathml>,根据假设(A),令序列<i>a</i><sub><i>n</i></sub>为<mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mi>n</mi></msqrt></mrow></math></mathml>,可以得到<mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mo stretchy="false">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo><msup><mrow></mrow><mi>γ</mi></msup><mo>→</mo><mi>∞</mi></mrow></math></mathml>;由于<i>γ</i>&gt;0,则<mathml id="194"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><msqrt><mi>n</mi></msqrt><mi>w</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><msqrt><mi>n</mi></msqrt><mo stretchy="false">(</mo><mi>w</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo>-</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup></mrow></mfrac><mo>→</mo><mi>∞</mi></mrow></math></mathml>,所以<mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mn>4</mn></msub><mover><mo stretchy="true" symmetric="false">→</mo><mi>p</mi></mover><mi>∞</mi></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="109">综上,令<i>V</i><sub>5</sub>=<i>ψ</i>(<i><b>u</b></i>)-<i>ψ</i>(<b>0</b>),则有:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mn>5</mn></msub><mover><mo stretchy="true" symmetric="false">→</mo><mi>d</mi></mover><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mi>A</mi><mtext>Τ</mtext></msubsup><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mi>n</mi></mfrac><mi mathvariant="bold-italic">Ι</mi></mrow><mo>)</mo></mrow><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>A</mi></msub><mo>-</mo><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mi>A</mi><mtext>Τ</mtext></msubsup><mi>Q</mi><msub><mrow></mrow><mi>A</mi></msub><mo>-</mo><mi>Q</mi><msub><mrow></mrow><mi>A</mi></msub><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>A</mi></msub><mo>,</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mn>0</mn><mspace width="0.25em" /><mtext>o</mtext><mtext>r</mtext><mspace width="0.25em" /><mi>j</mi><mo>∈</mo><mi>A</mi></mtd></mtr><mtr><mtd><mi>∞</mi><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">此时<i>V</i><sub>4</sub>为凸函数并且唯一极小值点为:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mi>n</mi></mfrac><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi>Q</mi><msub><mrow></mrow><mi>A</mi></msub><mo>,</mo><mn>0</mn></mrow><mo>)</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">其中<i>Q</i><sub><i>A</i></sub>=N(0,<i>σ</i><sup>2</sup><i><b>D</b></i><sub>1</sub>),则有:</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">u</mi><mo>^</mo></mover><msub><mrow></mrow><mi>A</mi></msub><mo>=</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>A</mi></msub><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>A</mi></msub><mo stretchy="false">)</mo><mover><mo stretchy="true" symmetric="false">→</mo><mi>d</mi></mover><mtext>Ν</mtext><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">对性质2)的证明:</p>
                </div>
                <div class="p1">
                    <p id="116">命题可以等价于∀<i>j</i>′∉<i>A</i>,<i>P</i>(<i>j</i>′∈<i>A</i><sup>*</sup>)→0,由KKT(Karush-Kuhn-Tucker)条件可得:</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>2</mn><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><msub><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118">其中:<i><b>X</b></i><mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msubsup></mrow></math></mathml>代表所有样本的某个和权重<mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mo>*</mo></msubsup></mrow></math></mathml>对应的属性向量。由对性质1)的证明可得<mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mspace width="0.25em" /><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mo stretchy="false">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo><msup><mrow></mrow><mi>γ</mi></msup><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><msqrt><mi>n</mi></msqrt><mi>w</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup></mrow></mfrac><mo>→</mo><mi>∞</mi></mrow></math></mathml>,则式(4)等号左侧可以写成:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mn>2</mn><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac><mspace width="0.25em" /><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">ε</mi><msup><mrow></mrow><mo>*</mo></msup><mo>+</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo>-</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mn>2</mn><mrow><mo>(</mo><mrow><mfrac><mrow><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msubsup><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><msqrt><mi>n</mi></msqrt><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></mfrac><mo>+</mo><mfrac><mrow><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msubsup><mi mathvariant="bold-italic">ε</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow><mo>)</mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">由对性质1)的证明和Slutsky定理可知:<mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msubsup><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><msqrt><mi>n</mi></msqrt><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></mfrac></mrow></math></mathml>和<mathml id="200"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msubsup><mi mathvariant="bold-italic">ε</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mrow><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow></math></mathml>分别收敛于某两个正态分布,则<mathml id="201"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><msup><mi>j</mi><mo>′</mo></msup><mo>∉</mo><mi>A</mi><mo stretchy="false">)</mo><mo>≤</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mn>2</mn><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mrow><mo>*</mo><mtext>Τ</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mo>*</mo></msup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><msub><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup></msub><mo stretchy="false">)</mo><mo>→</mo><mn>0</mn></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="121">综上,通过证明改进的弹性网模型满足性质1)与性质2),说明了该模型具有Oracle性质。该性质表明:该模型的估计值可以以1的概率正确估计非零的参数,并且估计值的非零部分服从渐近正态分布。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122">2.2 <b>群组选择能力</b></h4>
                <div class="p1">
                    <p id="123"><b>定理</b>2 群组选择能力。给定神经元的输入和期望输出,假设估计值<mathml id="202"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow></math></mathml>、<mathml id="203"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow></math></mathml>均大于0,并且<mathml id="204"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow></math></mathml>、<mathml id="205"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow></math></mathml>对应的为不同的样本属性,定义<mathml id="206"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow></mrow></mfrac><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow></mrow></math></mathml>,所以</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>+</mo><mfrac><mrow><mi>γ</mi><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><mn>2</mn><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow></mrow></mfrac><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">证明 由假设可以得到<mathml id="207"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>sgn</mi></mrow><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>sgn</mi></mrow><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>,定义:</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">w</mi></munder><mspace width="0.25em" /><mi>L</mi><mo stretchy="false">(</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mo>,</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">让<i>L</i>分别对<mathml id="208"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow></math></mathml>和<mathml id="209"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow></math></mathml>求偏导为0,可得</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>-</mo><mn>2</mn><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">X</mi><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mrow><mi>sgn</mi></mrow><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>+</mo><mn>2</mn><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>=</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mo>-</mo><mn>2</mn><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">X</mi><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mspace width="0.25em" /><mrow><mi>sgn</mi></mrow><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>+</mo><mn>2</mn><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo>=</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">将式(5)减去式(6),在等号两边加上绝对值,化简得</p>
                </div>
                <div class="p1">
                    <p id="130" class="code-formula">
                        <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mo>|</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">)</mo><mi mathvariant="bold-italic">r</mi><mo>-</mo><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mn>2</mn></mfrac><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mi>sgn</mi><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="131">其中:<mathml id="210"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">r</mi><mo>=</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">X</mi><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="132">由<mathml id="211"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mrow><mo>|</mo><mi>A</mi><mo>|</mo></mrow><mo>-</mo><mrow><mo>|</mo><mi>B</mi><mo>|</mo></mrow></mrow><mo>|</mo></mrow><mo>≤</mo><mrow><mo>|</mo><mrow><mi>A</mi><mo>±</mo><mi>B</mi></mrow><mo>|</mo></mrow><mo>≤</mo><mrow><mo>|</mo><mi>A</mi><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mi>B</mi><mo>|</mo></mrow></mrow></math></mathml>,可得:</p>
                </div>
                <div class="p1">
                    <p id="133" class="code-formula">
                        <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow><mo>≤</mo><mrow><mo>|</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">)</mo><mi mathvariant="bold-italic">r</mi></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mn>2</mn></mfrac><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="134">易知:<mathml id="212"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mo>,</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>≤</mo><mi>L</mi><mo stretchy="false">(</mo><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mo>,</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>˜</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>,即<mathml id="213"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">r</mi><mo>|</mo></mrow><mo>≤</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow></mrow></math></mathml>。则</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow><mo>≤</mo><mrow><mo>|</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup><mo stretchy="false">)</mo><mi mathvariant="bold-italic">Y</mi></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mn>2</mn></mfrac><mo stretchy="false">(</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136">不等式两端同时除以<mathml id="214"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow></mrow></math></mathml>和<i>λ</i><sub>2</sub>,可得:</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>+</mo><mfrac><mrow><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><mn>2</mn><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow></mrow></mfrac><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">假设<mathml id="215"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>w</mi><mo>^</mo></mover></math></mathml><sub><i>j</i></sub>的相容估计为<mathml id="216"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mi>γ</mi></mrow></msup></mrow></math></mathml>,定义一个函数<i>f</i>(<i>x</i>)=<i>x</i><sup>-</sup><sup><i>γ</i></sup>,由中值定理得<mathml id="217"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>-</mo><mi>f</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mo>≤</mo><mi>γ</mi><mrow><mo>|</mo><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="139">同理:</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>=</mo><mrow><mo>|</mo><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mi>γ</mi></mrow></msup><mo>-</mo><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mi>γ</mi></mrow></msup></mrow><mo>|</mo></mrow><mo>≤</mo><mi>γ</mi><mrow><mo>|</mo><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mo>-</mo><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>|</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141">由绝对值不等式||<i>a</i>|-|<i>b</i>||≤|<i>a</i>±<i>b</i>|≤|<i>a</i>|+|<i>b</i>|,可得</p>
                </div>
                <div class="p1">
                    <p id="142" class="code-formula">
                        <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><mrow><mo>|</mo><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mo>-</mo><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>|</mo></mrow><mo>≤</mo><mi>γ</mi><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="143">则</p>
                </div>
                <div class="p1">
                    <p id="144" class="code-formula">
                        <mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>+</mo><mfrac><mrow><mi>γ</mi><mi>λ</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow><mrow><mn>2</mn><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow></mrow></mfrac><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="145">不失一般性,可假设:输入<i><b>X</b></i>服从标准正态分布。</p>
                </div>
                <div class="p1">
                    <p id="146">令<i>ρ</i>=<i><b>X</b></i><mathml id="218"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><i><b>X</b></i><sub><i>j</i></sub>,则</p>
                </div>
                <div class="p1">
                    <p id="147" class="code-formula">
                        <mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>=</mo><msqrt><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></msqrt><mo>=</mo><msqrt><mrow><mn>2</mn><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>ρ</mi><mo stretchy="false">)</mo></mrow></msqrt></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="148">根据标准正态分布的性质,可得<mathml id="219"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><mo>≈</mo><mn>1</mn><mo>,</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>≈</mo><mn>0</mn></mrow></math></mathml>,当两个变量强相关时,则这两个变量对于决策变量的影响是近似相同的,模型分配给它们的真实参数也近似相同,即<mathml id="220"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>趋近于0,那么<i>B</i>(<i>i</i>, <i>j</i>)也会趋近于0,即<mathml id="221"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>-</mo><mover accent="true"><mi>w</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup></mrow><mo>|</mo></mrow></mrow></math></mathml>趋近于0。通过以上证明可以看出,群组选择能力具体表现为:若两个参数同时为0,则这两个参数对应的变量会被模型同时去掉;若两个参数近似相同且不为0,则这两个参数对应的变量会被模型同时保留。</p>
                </div>
                <h3 id="149" name="149" class="anchor-tag">3 实验与分析</h3>
                <h4 class="anchor-tag" id="150" name="150">3.1 <b>实验数据集</b></h4>
                <div class="p1">
                    <p id="151">本文实验采用的数据集包括分类实验数据集和回归实验数据集两个部分,这些数据集的信息分别汇总在表1～2中。这些数据集来自<i>UCI</i>机器学习数据库网站和<i>KEEL</i>数据集网站,并且在回归问题中都是对单一属性进行回归预测。为了更好地进行实验,数据集在标准化处理后被划分为训练集、验证集和测试集。</p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表</b>1 <b>实验中所使用的分类数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.1 <i>Datasets used in the classification experiment</i></p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td rowspan="2">数据集</td><td rowspan="2">属性数</td><td rowspan="2">分类数</td><td colspan="3"><br />样本数</td></tr><tr><td><br />训练集</td><td>验证集</td><td>测试集</td></tr><tr><td><i>Human</i></td><td>561</td><td>6</td><td>500</td><td>200</td><td>200</td></tr><tr><td><br /><i>Image</i></td><td>19</td><td>7</td><td>500</td><td>200</td><td>200</td></tr><tr><td><br /><i>Car</i></td><td>6</td><td>4</td><td>500</td><td>200</td><td>200</td></tr><tr><td><br /><i>Balance</i></td><td>4</td><td>3</td><td>400</td><td>100</td><td>100</td></tr><tr><td><br /><i>Pima</i></td><td>8</td><td>2</td><td>400</td><td>150</td><td>150</td></tr><tr><td><br /><i>Sports</i></td><td>59</td><td>2</td><td>500</td><td>200</td><td>200</td></tr><tr><td><br /><i>Contraceptive</i></td><td>9</td><td>3</td><td>500</td><td>200</td><td>200</td></tr><tr><td><br /><i>Vehicle</i></td><td>18</td><td>4</td><td>500</td><td>150</td><td>150</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit"><b>表</b>2 <b>实验中所使用的回归数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.2 <i>Datasets used in the regression experiment</i></p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td rowspan="2"><br />数据集</td><td rowspan="2">属性数</td><td colspan="3"><br />样本数</td></tr><tr><td><br />训练集</td><td>验证集</td><td>测试集</td></tr><tr><td><br /><i>Airfoil</i></td><td>5</td><td>300</td><td>100</td><td>100</td></tr><tr><td><br /><i>Concrete</i></td><td>8</td><td>300</td><td>100</td><td>100</td></tr><tr><td><br /><i>Estate</i></td><td>6</td><td>250</td><td>80</td><td>80</td></tr><tr><td><br /><i>Power</i></td><td>4</td><td>300</td><td>100</td><td>100</td></tr><tr><td><br /><i>Wine</i></td><td>11</td><td>500</td><td>200</td><td>200</td></tr><tr><td><br /><i>Mortgage</i></td><td>15</td><td>500</td><td>200</td><td>200</td></tr><tr><td><br /><i>Stock</i></td><td>9</td><td>500</td><td>200</td><td>200</td></tr><tr><td><br /><i>Weather</i></td><td>9</td><td>500</td><td>200</td><td>200</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="154" name="154">3.2 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="155">所有实验的实验环境为<i>Windows</i> 10(64位)操作系统, <i>Python</i> 3.6以及<i>Tensorflow</i> 1.5 <i>GPU</i>版本,显卡为<i>GTX</i> 1060 6 <i>GB</i>显存。实验中用到的神经网络为全连接的多隐层前向传播网络模型,并且对于同一个数据集采用的网络结构相同。超参数<i>λ</i><sub>1</sub>和<i>λ</i><sub>2</sub>的取值范围是{10<sup>7</sup>,10<sup>6</sup>,…,10<sup>-7</sup>},在每个数据集上固定网络结构,经过多次实验,使得L1、L2和弹性网(Elastic Net,EN)模型取得最好结果的参数作为该数据集上实验对比所用的参数,改进的弹性网(Advanced EN, AEN)模型的<i>λ</i><sup>*</sup><sub>1</sub>和<i>λ</i><sub>2</sub>设置与EN相同,后续实验可以验证,在EN达到最好效果时,改进的弹性网仍能继续改善网络的性能。训练集用来进行模型参数的学习,验证集用来评估不同阶段的模型的表现;测试集用来评估训练结束后的模型的性能。分类实验采用交叉熵(CE)作为验证集上的评价指标,同时采用准确度(Accuracy, ACC)作为测试集上的评价指标;回归实验采用均方误差(MSE)作为验证集和测试集上的评价指标。</p>
                </div>
                <h4 class="anchor-tag" id="156" name="156">3.3 <b>实验分析</b></h4>
                <div class="p1">
                    <p id="157">本文主要对比的方法有:在<i>L</i>1正则项约束下的优化模型(<i>L</i>1),在<i>L</i>2正则项约束下的优化模型(<i>L</i>2),在弹性网约束下的优化模型(<i>EN</i>),以及在本文方法约束下的优化模型(<i>AEN</i>)。图2展示了在其中四个数据集的验证集上的对数损失值随迭代次数的变化趋势。图3～4为在不同数据集上训练结果和测试结果之间的差值,这些差值反映了模型的过拟合程度,差值越大,过拟合情况越严重。具体表现为:在分类问题上,测试准确率越低于训练准确率;在回归问题上,测试误差越高于训练误差。表3～4则给出了以上方法在不同测试集上的最终测试结果,以及<i>AEN</i>相对于<i>L</i>1、<i>L</i>2和<i>EN</i>的平均准确率提升数值和均方误差下降数值。</p>
                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910004_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 四个数据集的对数损失变化曲线" src="Detail/GetImg?filename=images/JSJY201910004_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 四个数据集的对数损失变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910004_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.2 <i>Logarithm loss curve of four datasets</i></p>

                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表</b>3 <b>四种方法在分类数据测试集的准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.3 <i>Accuracy of four methods on classification test sets</i></p>
                    <p class="img_note"></p>
                    <table id="159" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="4"><br />准确率(<i>ACC</i>)/%</td></tr><tr><td><br /><i>L</i>1</td><td><i>L</i>2</td><td><i>EN</i></td><td><i>AEN</i></td></tr><tr><td><br /><i>Human</i></td><td>80.00</td><td>83.99</td><td>81.49</td><td><b>87.50</b></td></tr><tr><td><br /> Image</td><td>82.99</td><td>83.49</td><td>82.99</td><td><b>87.99</b></td></tr><tr><td><br /> Car</td><td>92.50</td><td>93.99</td><td>92.50</td><td><b>94.99</b></td></tr><tr><td><br /> Balance</td><td>92.00</td><td>92.00</td><td>94.99</td><td><b>97.00</b></td></tr><tr><td><br /> Pima</td><td>75.99</td><td>75.99</td><td>72.67</td><td><b>77.33</b></td></tr><tr><td><br /> Sports</td><td>82.99</td><td>84.50</td><td>81.00</td><td><b>86.00</b></td></tr><tr><td><br /> Contraceptive</td><td>46.50</td><td>45.50</td><td>48.50</td><td><b>50.00</b></td></tr><tr><td><br /> Vehicle</td><td>78.66</td><td>80.66</td><td>80.66</td><td><b>82.67</b></td></tr><tr><td><br />提升平均值</td><td>3.98</td><td>2.92</td><td>3.58</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="160">
                    <p class="img_tit"><b>表</b>4 <b>四种方法在回归数据测试集的最终结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.4 Final results of four methods on regression test sets</p>
                    <p class="img_note"></p>
                    <table id="160" border="1"><tr><td rowspan="2"><br />数据集</td><td colspan="4"><br />均方误差(MSE)</td></tr><tr><td><br />L1</td><td>L2</td><td>EN</td><td>AEN</td></tr><tr><td><br /> Airfoil</td><td>23.870 2</td><td>28.070 0</td><td>22.626 9</td><td><b>14.817</b><b>4</b></td></tr><tr><td><br /> Concrete</td><td>72.320 2</td><td>58.347 3</td><td>47.700 1</td><td><b>45.241</b><b>7</b></td></tr><tr><td><br /> Estate</td><td>220.744 5</td><td>270.233 6</td><td>157.648 5</td><td><b>152.933</b><b>3</b></td></tr><tr><td><br /> Power</td><td>124.359 8</td><td>128.160 8</td><td>126.873 9</td><td><b>74.175</b><b>8</b></td></tr><tr><td><br /> Wine</td><td>43.498 6</td><td>49.443 3</td><td>37.956 9</td><td><b>10.987</b><b>3</b></td></tr><tr><td><br /> Mortgage</td><td>327.359 7</td><td>357.457 7</td><td>190.870 7</td><td><b>94.363</b><b>0</b></td></tr><tr><td><br /> Stock</td><td>80.589 8</td><td>70.607 7</td><td>48.610 4</td><td><b>39.814</b><b>7</b></td></tr><tr><td><br /> Weather</td><td>448.603 6</td><td>390.613 0</td><td>388.538 5</td><td><b>212.292</b><b>0</b></td></tr><tr><td><br />下降平均值</td><td>87.09</td><td>88.54</td><td>47.02</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="161">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910004_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 分类数据集的训练集和测试集准确率之差" src="Detail/GetImg?filename=images/JSJY201910004_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 分类数据集的训练集和测试集准确率之差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910004_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Accuracy difference between training set and test set of classification datasets</p>

                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910004_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 回归数据集的训练集和测试集损失之差" src="Detail/GetImg?filename=images/JSJY201910004_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 回归数据集的训练集和测试集损失之差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910004_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Loss difference between training set and test set of regression datasets</p>

                </div>
                <div class="p1">
                    <p id="163">从图3～4可看出,AEN在防止过拟合的问题中取得了较为突出的表现。除此之外,从表3～4可看出:AEN方法在大多数情况下可以得到更低的回归损失值和较高的分类准确率,且通过图2中在验证集上的损失变化曲线可以看出:L1方法由于其稀疏性更容易得到一个较低的损失;L2方法由于其平滑的特性,曲线一般更加光滑稳定,但一般不会得到一个较低的损失;EN方法由于将两者结合,经常会得到一个比前两者更好的结果;AEN方法在不同的数据集上可以得到比其他方法更低的预测损失。结合图3～4和表3～4可以得出:AEN方法不仅可以在训练阶段使得深层神经网络模型更鲁棒地拟合数据分布,而且所得到的模型在未知数据上也有较好的泛化能力。产生这些结果的原因可以被归结为以下两点:1)在弹性网的框架下,能够兼顾稀疏性和平滑性,在能够减轻模型复杂度的同时又容易找到最优解;2)改进的弹性网模型是在弹性网基础上对L1部分再进行加权,对不同的参数,配给不同的权重变量,对于较大的参数将会配给一个较小的权重,对于较小的参数将会配给一个较大的权重,这样在迭代更新时,较大的、重要的参数将会更容易被保留下来,较小的、不重要的参数也会更容易接近0。</p>
                </div>
                <h3 id="164" name="164" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="165">本文对深度学习以及正则化方法进行了研究与分析。首先讨论了L1正则化、L2正则化和弹性网正则化,并基于这些正则化模型的优势与缺点,提出一种基于弹性网的改进模型。然后,将这个改进的弹性网正则化方法与深度学习算法相结合,提出一种对神经网络参数进行约束求解的新方法。在理论上,证明了这个改进的弹性网模型具有群组选择能力和Oracle性质,这些性质的证明可以保证改进的方法在一定程度上避免深度学习中的过拟合问题,从而提高深层神经网络的泛化能力。在实验上,通过对比不同方法在多个验证集和测试集的表现,可以看到改进的弹性网模型不仅可以在训练阶段较鲁棒地收敛至较低的损失值,在对未知样本的预测阶段也体现出了较好的泛化能力,通过对实验结果进行分析,得出改进的弹性网模型取得好的效果的原因,日后主要的研究目标是把本文方法应用在更多的实际问题中。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="240">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                LECUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2015:1-9.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811046&amp;v=MTEwMTMzenFxQnRHRnJDVVI3cWZadVpzRnluaFU3dlBMejdCZDdHNEg5bk5ybzlCWW9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>陈文兵,管正雄,陈允杰.基于条件生成式对抗网络的数据增强方法[J].计算机应用,2018,38(11):3305-3311.(CHENW B,GUAN Z X,CHEN Y J.Data augmentation method based on conditional generative adversarial net model[J].Journal of Computer Applications,2018,38(11):3305-3311.)
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">

                                <b>[4]</b>SRIVASTAVA N,HINTON G,KRIZHEVSKY A,et al.Dropout:a simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research,2014,15(1):1929-1958.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial landmark detection by deep multi-task learning">

                                <b>[5]</b>ZHANG Z,LUO P,LOY C C,et al.Facial landmark detection by deep multi-task learning[C]//Proceedings of the 2014 European Conference on Computer Vision,LNCS 8694.Cham:Springer,2014:94-108.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On solving incorrectly posed problems and method of regularization">

                                <b>[6]</b>TIKHONOV A N.Solution of incorrectly formulated problems and the regularization method[J].Doklady Akademii Nauk SSSR,1963,151:501-504.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800007328&amp;v=MDg2NTJaT3NJRDM0eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUlWb1ViaFU9TmpuQmFySzdIdGZPcDQ5Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>ANTONIADIS A,FAN J.Regularization of wavelet approximations[J].Journal of the American Statistical Association,2001,96(455):939-967.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weighted LASSO for Sparse Recovery with Statistical Prior Support Information">

                                <b>[8]</b>LIAN L,LIU A,LAU V K N.Weighted LASSO for sparse recovery with statistical prior support information[J].IEEE Transactions on Signal Processing,2018,66(6):1607-1618.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES90950C8B796C60C44D6DB0C84D1D3062&amp;v=MDU4MTVKZjNvNXZCSVhuamtKT24rUnBCWkJlTWFYUmJ5ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieS94Szg9TmlmT2ZicTRGOVRNM0ljM1krSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>CUI C,WANG D.High dimensional data regression using Lasso model and neural networks with random weights[J].Information Sciences,2016,372:505-517.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ex-treme learning machine for multilayer perceptron">

                                <b>[10]</b>TANG J,DENG C,HUANG G.Extreme learning machine for multilayer perceptron[J].IEEE Transactions on Neural Networks and Learning Systems,2016,27(4):809-821.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An analysis of the regularization between L2 and dropout in single hidden layer neural network">

                                <b>[11]</b>PHAISANGITTISAGUL E.An analysis of the regularization between l2 and dropout in single hidden layer neural network[C]//Proceedings of the 7th International Conference on Intelligent System,Modelling and Simulation.Piscataway:IEEE,2016:174-179.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESAE7C402394B92BB4515C0FDF04A7797E&amp;v=MzI3MzdMVTA1dHBoeGJ5L3hLOD1OaWZPZmNMTkdhTElyNDFHYmU5OUJYNUx2UklXNnpvT1NBbVcyaEl4Q0xXVFRMM3FDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>JIN J,CHEN C L P.Regularized robust broad learning system for uncertain data modeling[J].Neurocomputing,2018,322:58-69.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201703052&amp;v=MTI2ODdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFU3dlBMejdCZDdHNEg5Yk1ySTlBWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>李光早,王士同.基于稀疏表示和弹性网络的人脸识别[J].计算机应用,2017,37(3):901-905.(LI G Z,WANG S T.Face recognition based on sparse representation and elastic network[J].Journal of Computer Applications,2017,37(3):901-905.)
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Elastic Net Hypergraph Learning for Image Clustering and Semi-Supervised Classification">

                                <b>[14]</b>LI Q,SUN Y,WANG C,et al.Elastic net hypergraph learning for image clustering and semi-supervised classification[J].IEEETransactions on Image Processing,2017,26(1):452-463.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A Method for Stochastic Optimization[C/OL]">

                                <b>[15]</b>KINGMA D P,BA J L.Adam:a method for stochastic optimization[EB/OL].[2019-01-10].https://arxiv.org/pdf/1412.6980.pdf.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation">

                                <b>[16]</b>MARQUARIDT D W.Generalized inverses,ridge regression,biased linear estimation,and nonlinear estimation[J].Technometrics,1970,12(3):591-612.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910004&amp;v=MjU5MzJyNDlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFU3dk1MejdCZDdHNEg5ak4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
