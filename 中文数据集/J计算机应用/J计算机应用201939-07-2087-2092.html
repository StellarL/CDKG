<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136664847033750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201907037%26RESULT%3d1%26SIGN%3dslYDt%252b9mckECaxXINJmz8TF%252bOIk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201907037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907037&amp;v=MTAyMzZxQnRHRnJDVVI3cWZadVpzRnkvZ1U3ekFMejdCZDdHNEg5ak1xSTlHWTRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 多曝光融合 ">1 多曝光融合</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="1.1 &lt;b&gt;简介&lt;/b&gt;">1.1 <b>简介</b></a></li>
                                                <li><a href="#66" data-title="1.2 &lt;b&gt;入射光分量&lt;/b&gt;">1.2 <b>入射光分量</b></a></li>
                                                <li><a href="#89" data-title="1.3 &lt;b&gt;反射光分量&lt;/b&gt;">1.3 <b>反射光分量</b></a></li>
                                                <li><a href="#103" data-title="1.4 &lt;b&gt;最终结果&lt;/b&gt;">1.4 <b>最终结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="2 实验结果及分析 ">2 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#110" data-title="2.1 &lt;b&gt;主观比较&lt;/b&gt;">2.1 <b>主观比较</b></a></li>
                                                <li><a href="#115" data-title="2.2 &lt;b&gt;客观评价&lt;/b&gt;">2.2 <b>客观评价</b></a></li>
                                                <li><a href="#119" data-title="2.3 &lt;b&gt;计算效率&lt;/b&gt;">2.3 <b>计算效率</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="图1 S曲线">图1 S曲线</a></li>
                                                <li><a href="#102" data-title="图2 室内晚间场景的曝光序列图像融合结果">图2 室内晚间场景的曝光序列图像融合结果</a></li>
                                                <li><a href="#107" data-title="图3 本文算法流程">图3 本文算法流程</a></li>
                                                <li><a href="#112" data-title="图4 &lt;i&gt;Tower&lt;/i&gt;图像序列曝光融合结果">图4 <i>Tower</i>图像序列曝光融合结果</a></li>
                                                <li><a href="#121" data-title="图5 &lt;i&gt;Madison Capitol&lt;/i&gt;图像序列曝光融合结果">图5 <i>Madison Capitol</i>图像序列曝光融合结果</a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;图像质量评估分数&lt;/b&gt; (&lt;i&gt;MEF&lt;/i&gt;-&lt;i&gt;SSIM&lt;/i&gt;&lt;b&gt;方法&lt;/b&gt;) "><b>表</b>1 <b>图像质量评估分数</b> (<i>MEF</i>-<i>SSIM</i><b>方法</b>) </a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同方法的处理时间对比&lt;/b&gt;"><b>表</b>2 <b>不同方法的处理时间对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="179">


                                    <a id="bibliography_1" title=" 白本督, 刘军, 范九伦.高动态范围成像研究进展[J].西安邮电大学学报, 2016, 21 (3) :1-14. (BAI B D, LIU J, FAN J L.Recent research in high dynamic range imaging[J].Journal of Xi&#39;an University of Posts and Telecommunications, 2016, 21 (3) :1-14) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XAYD201603001&amp;v=MDg5NzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTd6QVBTelNhckc0SDlmTXJJOUZaWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         白本督, 刘军, 范九伦.高动态范围成像研究进展[J].西安邮电大学学报, 2016, 21 (3) :1-14. (BAI B D, LIU J, FAN J L.Recent research in high dynamic range imaging[J].Journal of Xi&#39;an University of Posts and Telecommunications, 2016, 21 (3) :1-14) 
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_2" title=" WU S, LI Z, ZHENG J, et al.Exposure-robust alignment of differently exposed images[J].IEEE Signal Processing Letters, 2014, 21 (7) :885-889." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exposure-robust alignment of differently exposed images">
                                        <b>[2]</b>
                                         WU S, LI Z, ZHENG J, et al.Exposure-robust alignment of differently exposed images[J].IEEE Signal Processing Letters, 2014, 21 (7) :885-889.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_3" title=" 胡胜男, 张伟, 刘侃, 等.高动态范围成像技术中的鬼影检测与去除方法综述[J].计算机科学, 2016, 43 (8) :13-18. (HU S N, ZHANG W, LIU K, et al.Survey of ghost detection and removal methods in HDR imaging technology [J].Computer Science, 2016, 43 (8) :13-18.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201608003&amp;v=MDI5OTlBTHo3QmI3RzRIOWZNcDQ5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dVN3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         胡胜男, 张伟, 刘侃, 等.高动态范围成像技术中的鬼影检测与去除方法综述[J].计算机科学, 2016, 43 (8) :13-18. (HU S N, ZHANG W, LIU K, et al.Survey of ghost detection and removal methods in HDR imaging technology [J].Computer Science, 2016, 43 (8) :13-18.) 
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_4" title=" LI Z, ZHENG J.Visual-salience-based tone mapping for high dynamic range images[J].IEEE Transactions on Industrial Electronics, 2014, 61 (12) :7076-7082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual-salience-based tone mapping for high dynamic range images">
                                        <b>[4]</b>
                                         LI Z, ZHENG J.Visual-salience-based tone mapping for high dynamic range images[J].IEEE Transactions on Industrial Electronics, 2014, 61 (12) :7076-7082.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_5" title=" MERTENS T, KAUTZ J, van REETH F.Exposure fusion:a simple and practical alternative to high dynamic range photography[J].Computer Graphics Forum, 2010, 28 (1) :161-171." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exposure Fusion: A Simple and Practical Alternative to High Dynamic Range Photography">
                                        <b>[5]</b>
                                         MERTENS T, KAUTZ J, van REETH F.Exposure fusion:a simple and practical alternative to high dynamic range photography[J].Computer Graphics Forum, 2010, 28 (1) :161-171.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_6" title=" ZHANG W, CHAN W K.Gradient-directed multi-exposure composition[J].IEEE Transactions on Image Processing, 2012, 21 (4) :2318-2323." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradient-Directed Multiexposure Composition">
                                        <b>[6]</b>
                                         ZHANG W, CHAN W K.Gradient-directed multi-exposure composition[J].IEEE Transactions on Image Processing, 2012, 21 (4) :2318-2323.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_7" title=" SHEN R, CHENG I, SHI J, et al.Generalized random walks for fusion of multi-exposure images[J].IEEE Transactions on Image Processing, 2011, 20 (12) :3634-3646." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized random walks for fusion of multi-exposure images">
                                        <b>[7]</b>
                                         SHEN R, CHENG I, SHI J, et al.Generalized random walks for fusion of multi-exposure images[J].IEEE Transactions on Image Processing, 2011, 20 (12) :3634-3646.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_8" title=" MA K, WANG Z.Multi-exposure image fusion:a patch-wise approach[C]// Proceedings of the 2015 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2015:1717-1721." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-exposure image fusion:A patch-wise approach">
                                        <b>[8]</b>
                                         MA K, WANG Z.Multi-exposure image fusion:a patch-wise approach[C]// Proceedings of the 2015 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2015:1717-1721.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_9" title=" VONIKAKIS V, BOUZOS O, ANDREADIS L.Multi-exposure image fusion based on illumination estimation[C]// Proceedings of the 2011 IASTED Signal and Image Processing and Applications.Crete, Greece:IASTED, 2011:738-051." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-exposure image fusion based on illumination estimation">
                                        <b>[9]</b>
                                         VONIKAKIS V, BOUZOS O, ANDREADIS L.Multi-exposure image fusion based on illumination estimation[C]// Proceedings of the 2011 IASTED Signal and Image Processing and Applications.Crete, Greece:IASTED, 2011:738-051.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_10" title=" LI S, KANG X, HU J.Image fusion with guided filtering[J].IEEE Transactions on Image Processing, 2013, 22 (7) :2864-2875." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image fusion with guided filtering">
                                        <b>[10]</b>
                                         LI S, KANG X, HU J.Image fusion with guided filtering[J].IEEE Transactions on Image Processing, 2013, 22 (7) :2864-2875.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_11" title=" LI Z G, ZHENG J H, RAHARDJA S.Detail-enhanced exposure fusion[J].IEEE Transactions on Image Processing, 2012, 21 (11) :4672-4676." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detail-Enhanced Exposure Fusion">
                                        <b>[11]</b>
                                         LI Z G, ZHENG J H, RAHARDJA S.Detail-enhanced exposure fusion[J].IEEE Transactions on Image Processing, 2012, 21 (11) :4672-4676.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_12" title=" LI Z G, WEI Z, WEN C, et al.Detail-enhanced multi-scale exposure fusion[J].IEEE Transactions on Image Processing, 2017, 26 (3) :1243-1252." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detailenhanced multi-Scale exposure fusion">
                                        <b>[12]</b>
                                         LI Z G, WEI Z, WEN C, et al.Detail-enhanced multi-scale exposure fusion[J].IEEE Transactions on Image Processing, 2017, 26 (3) :1243-1252.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_13" title=" LAND E H.The Retinex theory of color vision[J].Scientific American, 1977, 237 (6) :108-128." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The retinex theory of color vision">
                                        <b>[13]</b>
                                         LAND E H.The Retinex theory of color vision[J].Scientific American, 1977, 237 (6) :108-128.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_14" title=" LI M, LIU J, YANG W, et al.Structure-revealing low-light image enhancement via robust Retinex model[J].IEEE Transactions on Image Processing, 2018, 27 (6) :2828-2841." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structure-Revealing Low-Light Image Enhancement Via Robust Retinex Model.">
                                        <b>[14]</b>
                                         LI M, LIU J, YANG W, et al.Structure-revealing low-light image enhancement via robust Retinex model[J].IEEE Transactions on Image Processing, 2018, 27 (6) :2828-2841.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_15" title=" GUO X, LI Y, LING H.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LIME:Low-light image enhancement via illumination map estimation">
                                        <b>[15]</b>
                                         GUO X, LI Y, LING H.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_16" title=" XU L, YAN Q, XIA Y, et al.Structure extraction from texture via relative total variation[J].ACM Transactions on Graphics, 2012, 31 (6) :Article No.139." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007775&amp;v=MjEyNzcxc1VhUm89TmlmSVk3SzdIdGpOcjQ5RlpPc0lDM3M4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         XU L, YAN Q, XIA Y, et al.Structure extraction from texture via relative total variation[J].ACM Transactions on Graphics, 2012, 31 (6) :Article No.139.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_17" title=" YUAN L, SUN J.Automatic exposure correction of consumer photographs[C]// Proceedings of the 2012 European Conference on Computer Vision.Berlin:Springer, 2012:771-785." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic exposure correction of consumer photographs">
                                        <b>[17]</b>
                                         YUAN L, SUN J.Automatic exposure correction of consumer photographs[C]// Proceedings of the 2012 European Conference on Computer Vision.Berlin:Springer, 2012:771-785.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_18" title=" 江燊煜, 陈阔, 徐之海, 等.基于曝光适度评价的多曝光图像融合方法[J].浙江大学学报 (工学版) , 2015, 49 (3) :470-475. (JIANG S Y, CHEN K, XU Z H, et al.Multi-exposure image fusion based on well-exposedness assessment [J].Journal of Zhejiang University (Engineering Edition) , 2015, 49 (3) :470-475.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC201503011&amp;v=MDMwMzR6cXFCdEdGckNVUjdxZlp1WnNGeS9nVTd6QVB5blJiYkc0SDlUTXJJOUVaWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         江燊煜, 陈阔, 徐之海, 等.基于曝光适度评价的多曝光图像融合方法[J].浙江大学学报 (工学版) , 2015, 49 (3) :470-475. (JIANG S Y, CHEN K, XU Z H, et al.Multi-exposure image fusion based on well-exposedness assessment [J].Journal of Zhejiang University (Engineering Edition) , 2015, 49 (3) :470-475.) 
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_19" title=" GU B, LI W, WONG J, et al.Gradient field multi-exposure images fusion for high dynamic range image visualization[J].Journal of Visual Communication and Image Representation, 2012, 23 (4) :604-610." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501263279&amp;v=MTUxNzRETnFvOUVadTBNRG5zd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUoxc1VhUm89TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         GU B, LI W, WONG J, et al.Gradient field multi-exposure images fusion for high dynamic range image visualization[J].Journal of Visual Communication and Image Representation, 2012, 23 (4) :604-610.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_20" title=" LEE S, PARK J S, CHO N I.A multi-exposure image fusion based on the adaptive weights reflecting the relative pixel intensity and global gradient[C]// Proceedings of the 2018 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2018:1737-1741." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A multi-exposure image fusion based on the adaptive weights reflecting the relative pixel intensity and global gradient">
                                        <b>[20]</b>
                                         LEE S, PARK J S, CHO N I.A multi-exposure image fusion based on the adaptive weights reflecting the relative pixel intensity and global gradient[C]// Proceedings of the 2018 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2018:1737-1741.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_21" title=" MA K, ZENG K, WANG Z.Perceptual quality assessment for multi-exposure image fusion[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3345-3356." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual Quality Assessment for Multi-Exposure Image Fusion">
                                        <b>[21]</b>
                                         MA K, ZENG K, WANG Z.Perceptual quality assessment for multi-exposure image fusion[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3345-3356.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-31 09:52</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(07),2087-2092 DOI:10.11772/j.issn.1001-9081.2018112382            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于Retinex理论的多曝光图像融合算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%85%8B%E5%BC%BA&amp;code=35157898&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王克强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E9%9B%A8%E5%B8%85&amp;code=40897381&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张雨帅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%BF%9D%E7%BE%A4&amp;code=40897380&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王保群</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学通信与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>多曝光图像融合技术是将一组场景相同但曝光程度不同的图像序列直接融合成为一幅含有更多场景细节信息的高质量图像。针对现有算法局部对比度差和色彩失真的问题, 结合Retinex理论模型提出了一种新的多曝光图像融合算法。首先, 基于Retinex理论模型, 利用光照估计算法将曝光序列图像分为入射光分量序列和反射光分量序列, 然后分别采用不同的融合方法对这两组序列进行处理。对于入射光分量, 要保证场景的全局亮度的变化特性并且削弱过曝光和欠曝光区域的影响;而对于反射光分量, 要采用适度曝光的评价参数来更好地保留场景的色彩及细节信息。分别从主观和客观两方面对所提算法进行了分析。实验结果表明, 同传统基于图像域合成的算法相比, 该算法在结构相似度 (SSIM) 上平均提升了1.7%, 另外在图像色彩和局部细节上的处理效果更好。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E5%8A%A8%E6%80%81%E8%8C%83%E5%9B%B4%E6%88%90%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高动态范围成像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%9B%9D%E5%85%89%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多曝光图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Retinex&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Retinex;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9B%9D%E5%85%89%E9%80%82%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曝光适度;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王克强 (1976—) , 男, 山东青岛人, 正高级工程师, 硕士, 主要研究方向:移动终端产品研发;;
                                </span>
                                <span>
                                    *张雨帅 (1994—) , 男, 山东济宁人, 硕士研究生, 主要研究方向:低照度图像增强、多曝光图像融合;电子邮箱zys_0928@163.com;
                                </span>
                                <span>
                                    王保群 (1994—) , 男, 山东聊城人, 硕士研究生, 主要研究方向:机器学习、数据挖掘。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>长江学者和创新团队发展计划项目 (IRT_16R72);</span>
                    </p>
            </div>
                    <h1><b>Multi-exposure image fusion algorithm based on Retinex theory</b></h1>
                    <h2>
                    <span>WAGN Keqiang</span>
                    <span>ZHANG Yushuai</span>
                    <span>WANG Baoqun</span>
            </h2>
                    <h2>
                    <span>School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Multi-exposure image fusion technology directly combines a sequence of images with the same scene but different exposure levels into a high-quality image with more details of scene. Aiming at the problems of poor local contrast difference and color distortion of existing algorithms, a new multi-exposure image fusion algorithm was proposed based on Retinex theoretical model. Firstly, based on Retinex theoretical model, the exposure sequence images were divided into an illumination component sequence and a reflection component sequence by using the illumination estimation algorithm, and then two sets of sequences were processed by different fusion methods. For the illumination component, the variation characteristics of global brightness of scene were guaranteed and the effects of overexposed and underexposed regions were weakened, while for the reflection component, the evaluation parameters of moderate exposure were used to better preserve the color and detail information of scene. The proposed algorithm was analyzed from both subjective and objective aspects. The experimental results show that compared with traditional algorithm based on image domain synthesis, the proposed algorithm has an average increase of 1.7% in Structural SIMilarity (SSIM) and has better effect in the processing of image color and local details.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=high%20dynamic%20range%20imaging&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">high dynamic range imaging;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-exposure%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-exposure image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Retinex&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Retinex;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=well-exposedness&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">well-exposedness;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Keqiang, born in 1976, M. S. , senior engineer. His research interest includes mobile phone product research and development. ;
                                </span>
                                <span>
                                    ZHANG Yushuai, born in 1994, M. S. candidate. His research interests include low illumination image enhancement, multi-exposure image fusion. ;
                                </span>
                                <span>
                                    WAGN Baoqun, born in 1994, M. S. candidate. His research interests include machine learning, data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-03</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Program for Changjiang Scholars and Innovative Research Team in University (IRT_16R72);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="46">普通数码相机成像的动态范围远低于现实场景的动态范围, 其捕捉的画面很难完整地呈现现实场景的所有细节信息。将场景多个不同曝光程度的低动态范围 (Low Dynamic Range, LDR) 图像融合成高动态范围 (High Dynamic Range, HDR) 图像是克服相机有限的动态范围并降低照片中噪声的有效方法, 这种成像技术称为HDR成像<citation id="221" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。由于拍摄时的相机抖动以及场景内可能存在运动对象, 需要对所有LDR图像先进行对齐<citation id="222" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 然后根据预定义的参考图像同步所有运动对象<citation id="223" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 再将校正后的图像合成HDR图像, 以包括所有LDR图像的细节, 最后使用色调映射算法<citation id="224" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>将HDR图像最终转换为LDR图像, 以便通过常规显示设备来展示。</p>
                </div>
                <div class="p1">
                    <p id="47">除了HDR成像技术之外, 目前更为流行的一种技术是多曝光图像融合。不同于HDR成像那样需要生成中间HDR图像, 多曝光图像融合技术直接从所有LDR图像中合成信息量更大且视觉效果更好的LDR图像。Mertens等<citation id="225" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>在多尺度图像分解下, 利用曝光程度、对比度和饱和度的三个质量评价参数来确定给定像素对最终合成图像的贡献程度, 利用多分辨率融合有效地保留了全局对比度, 但是局部对比度较低。Zhang等<citation id="226" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了一种基于梯度信息的曝光融合方案, 认为当像素获得更好的曝光状态时, 梯度幅度变得更大, 并且随着像素接近曝光过度/曝光不足而逐渐减小。Shen等<citation id="227" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了一种基于概率模型的多曝光图像融合算法, 建立了一种广义随机游动框架, 通过将融合问题表示为概率估计, 计算出两种质量度量下的全局最优解, 以此得到最终的合成图像。Ma等<citation id="228" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的方法将每个彩色图像分解为三个概念上独立的分量:信号强度、信号结构和平均强度, 对这三个分量分别进行处理后得到融合图像。</p>
                </div>
                <div class="p1">
                    <p id="48">上述的多曝光图像融合算法都存在一个普遍的问题, 即融合图像中场景灰暗或明亮处丢失了较多细节信息。Vonikakis等<citation id="229" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的算法在一定程度上克服了这个缺陷, 但生成的图像全局对比度较低。Li等<citation id="230" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出一种基于引导滤波器的多曝光图像融合算法, 采用基于引导滤波的加权平均方法, 对底层和细节层进行融合, 虽然解决了该问题, 但是融合图像会出现一定程度的色彩失真, 且该算法稳定性较差。文献<citation id="231" type="reference">[<a class="sup">11</a>]</citation>中提出了一种新的基于二次优化的矢量场细节提取方法, 该方法同时从一组不同曝光的LDR图像中提取细节, 然后将所提取的精细细节添加到融合图像中, 但仍未能有效地解决该问题。文献<citation id="232" type="reference">[<a class="sup">12</a>]</citation>又在其基础上提出了一种多尺度曝光融合算法, 融合图像的效果较好, 但是该算法的计算成本很高, 特别是对于计算资源有限的移动设备。</p>
                </div>
                <div class="p1">
                    <p id="49">本文结合Retinex理论模型<citation id="233" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一种新的多曝光图像融合算法, 对曝光序列图像的入射光分量序列和反射光分量序列分别进行不同的处理, 能够较好地保留场景的明亮及灰暗区域处的细节和颜色。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">1 多曝光融合</h3>
                <h4 class="anchor-tag" id="51" name="51">1.1 <b>简介</b></h4>
                <div class="p1">
                    <p id="52">根据Land<citation id="234" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的Retinex理论模型, 图像可以分为反射光分量和入射光分量两部分:</p>
                </div>
                <div class="area_img" id="274">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201907037_27400.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="54">其中:<b><i>I</i></b>表示输入图像, <b><i>R</i></b>表示输入图像的反射光分量, <b><i>L</i></b>表示输入图像的入射光分量, 运算符。表示对应元素相乘。</p>
                </div>
                <div class="p1">
                    <p id="55">为了将上述经典的Retinex理论模型拓展到多曝光图像融合算法中, 本文将考虑以下两点:第一, 本文提出的多曝光图像融合算法是针对静态场景的, 对于一个静态场景的多曝光图像序列而言, 它的反射光分量应该是不变的, 但它的入射光分量是随曝光程度而变化的;第二, 本文借鉴Li等<citation id="235" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的Retinex模型, 将考虑到输入图像所包含的噪声, 尤其是曝光序列中的欠曝光图像, 因此, 本文将经典的Retinex模型修改如下:</p>
                </div>
                <div class="p1">
                    <p id="56"><b><i>I</i></b><sub><i>k</i></sub>=<b><i>R</i></b>。<b><i>L</i></b><sub><i>k</i></sub>+<b><i>N</i></b><sub><i>k</i></sub>; <i>k</i>=1, 2, …, <i>m</i>      (2) </p>
                </div>
                <div class="p1">
                    <p id="57">其中:<b><i>I</i></b><sub><i>k</i></sub>表示曝光序列中的第<i>k</i>张图像, <b><i>R</i></b>表示曝光序列所对应的反射光分量, <b><i>L</i></b><sub><i>k</i></sub>表示曝光序列中第<i>k</i>张图像的入射光分量, <b><i>N</i></b><sub><i>k</i></sub>表示曝光序列中第<i>k</i>张图像的噪声, <i>m</i>为曝光序列的图像数目。</p>
                </div>
                <div class="p1">
                    <p id="58">基于这种模型, 本文提出的多曝光图像融合方法将输入的曝光序列分为入射光分量序列和反射光分量序列两部分, 分别对这两部分进行融合得到图像的入射光分量<mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">L</mi><mo>˜</mo></mover></math></mathml>及反射光分量<mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">R</mi><mo>˜</mo></mover></math></mathml>, 然后相乘得到最终的融合图像<b><i>F</i></b>, 即:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><mo>=</mo><mover accent="true"><mi mathvariant="bold-italic">R</mi><mo>˜</mo></mover><mo>˚</mo><mover accent="true"><mi mathvariant="bold-italic">L</mi><mo>˜</mo></mover><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">如何根据原始图像的信息来计算入射光分量是Retinex算法的关键步骤, 本文通过求解一个优化问题来估计图像的入射光分量, 然后从得到的入射光分量序列融合成最终的入射光分量<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">L</mi><mo>˜</mo></mover></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="64">现有的许多Retinex算法主要关注于入射光分量<b><i>L</i></b>的估计, 并简单地将<b><i>R</i></b>=<b><i>I</i></b>/<b><i>L</i></b>作为得到的反射光分量, 这些方法忽略了噪声的影响, 导致最终的增强效果并不理想。Li等<citation id="236" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>为了解决这个问题, 通过迭代地更新优化方程中的变量, 以便同时计算出入射光及反射光分量, 但是这种方法增加了算法的复杂度, 并且现有的光照估计方法很难保证估计的准确性。多曝光融合算法的输入是一组曝光图像, 本文依旧利用<b><i>R</i></b><sub><i>k</i></sub>=<b><i>I</i></b><sub><i>k</i></sub>/<b><i>L</i></b><sub><i>k</i></sub>作为第<i>k</i>张图像的反射光分量, 然后通过图像融合的方法得到最终的反射光分量<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">R</mi><mo>˜</mo></mover></math></mathml>。这种方法不但可以抑制噪声的影响, 而且可以有效地保留图像的细节及色彩, 该部分在1.3节会给出详细的说明以及方法。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.2 <b>入射光分量</b></h4>
                <div class="p1">
                    <p id="67">对于彩色图像, 三个颜色通道应该享有相同的入射光分量。<i>Land</i>等<citation id="237" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>试图通过寻找三个颜色通道的最大值来估计照度, 但是这种估计会增强全局亮度。本文首先将图像转换到<i>YUV</i>颜色空间, 然后将其中的<i>Y</i>通道作为初始的入射光分量<b><i>L</i></b>^ <sub><i>k</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="68">对于具有相似结构的区域, 理想的图像亮度应该具有局部一致性。换而言之, 入射光分量应该去除细微的纹理边缘并保留图像中有意义的结构。本文的光照估计方法是通过求解下面的优化问题<citation id="238" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>来重新定义入射光分量<b><i>L</i></b><sub><i>k</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mrow><mi mathvariant="bold-italic">L</mi><mo>^</mo><mspace width="0.25em" /></mrow><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Μ</mi><mo>˚</mo><mo>∇</mo><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中:‖*‖<sup>2</sup><sub>F</sub>和‖*‖<sub>1</sub>分别代表F-范数和<i>l</i><sub>1</sub>范数;ᐁ是一阶微分算子, 包含ᐁ<sub>h</sub> (水平方向) 和ᐁ<sub>v</sub> (垂直方向) ;<b><i>M</i></b>是一个权重矩阵;<i>λ</i>是平衡式中前后两项的系数, 默认值为0.3;<i>k</i>代表曝光序列中图像的序号。式 (4) 中的第一项的作用是最小化初始映射<b><i>L</i></b>^ <sub><i>k</i></sub>和重新定义的<b><i>L</i></b><sub><i>k</i></sub>之间的差异, 而第二项则是用于保持<b><i>L</i></b><sub><i>k</i></sub>的平滑度。</p>
                </div>
                <div class="p1">
                    <p id="71">权重矩阵<b><i>M</i></b>的设计对入射光分量的估计很重要。局部窗口中的主要边缘要比具有复杂图案的纹理贡献更多的相似方向梯度<citation id="239" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 因此, 包含重要边缘的窗口的权重应小于仅包含纹理的窗口的权重。进而, 本文将权重矩阵设计为:</p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>∈</mo><mi>ω</mi><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></munder><mo>∇</mo></mstyle><msub><mrow></mrow><mi>d</mi></msub><mrow><mi mathvariant="bold-italic">L</mi><mo>^</mo><mspace width="0.25em" /></mrow><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><mo>+</mo><mi>ε</mi></mrow></mfrac></mrow></math></mathml>; <i>d</i>∈{h, v}      (5) </p>
                </div>
                <div class="p1">
                    <p id="74">其中:<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mo>*</mo><mo>|</mo></mrow></mrow></math></mathml>表示绝对值运算符;<i>ω</i> (<i>n</i>) 表示以像素<i>n</i>为中心的局部窗口, 窗口大小默认为5×5;<i>ε</i>是一个非常小的常数, 以避免分母为零的情况。</p>
                </div>
                <div class="p1">
                    <p id="76">为了减小计算复杂度, 本文借鉴Guo等<citation id="240" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的做法, 简化‖ᐁ<b><i>L</i></b><sub><i>k</i></sub>‖<sub>1</sub>的计算, 对其进行近似求解, 则上述优化问题可以近似为式 (6) :</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mrow><mi mathvariant="bold-italic">L</mi><mo>^</mo><mspace width="0.25em" /></mrow><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>d</mi><mo>∈</mo><mo stretchy="false">{</mo><mtext>h</mtext><mo>, </mo><mtext>v</mtext><mo stretchy="false">}</mo></mrow></munder><mrow><mfrac><mrow><mi mathvariant="bold-italic">Μ</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mo>⋅</mo><mo stretchy="false"> (</mo><mo>∇</mo><msub><mrow></mrow><mi>d</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mrow><mo>|</mo><mrow><mo>∇</mo><msub><mrow></mrow><mi>d</mi></msub><mrow><mi mathvariant="bold-italic">L</mi><mo>^</mo><mspace width="0.25em" /></mrow><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><mo>+</mo><mi>ε</mi></mrow></mfrac></mrow></mstyle></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">可以看出, 式 (6) 可以直接计算解决方案而无需任何迭代<citation id="241" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="79">至此, 通过上述方法可以求出曝光序列中每张图像对应的入射光分量, 进而得到一组入射光分量序列<b><i>L</i></b><sub><i>k</i></sub>={<b><i>L</i></b><sub>1</sub>, <b><i>L</i></b><sub>2</sub>, …, <b><i>L</i></b><sub><i>m</i></sub>}, 下一步则是需要利用这组序列得到能更好反映出场景亮度的入射光分量<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">L</mi><mo>˜</mo></mover></math></mathml>。为了保证入射光分量更注重结构而不是纹理的特性, 同时也为了保证计算效率, 本文使初步的<mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">L</mi><mo>˜</mo></mover></math></mathml>等于入射光分量序列的均值, 即:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">L</mi><mo>˜</mo></mover><mo>=</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>k</mi><mi>m</mi></munderover><mi mathvariant="bold-italic">L</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mi>m</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">然而, 这种方法入射光分量存在局部区域过度曝光或欠曝光的风险。为了解决这个问题, 本文使用非线性映射将每个像素的亮度全局映射到其所需的曝光。正如许多摄影师使用S形非线性曲线手动调整欠曝光和过曝光区域的曝光值一样, 本文采用图1这种典型的S曲线<citation id="242" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>对入射光分量进行调整。该曲线可以通过两个简单的参数 (<i>φ</i><sub><i>l</i></sub>和<i>φ</i><sub><i>h</i></sub>) 进行调节, 表示为:</p>
                </div>
                <div class="p1">
                    <p id="84"><i>t</i> (<i>i</i>) =<i>i</i>+<i>φ</i><sub><i>l</i></sub>·<i>t</i><sub><i>Δ</i></sub> (<i>i</i>) -<i>φ</i><sub><i>h</i></sub>·<i>t</i><sub><i>Δ</i></sub> (1-<i>i</i>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="85">其中:<i>i</i>和<i>t</i> (<i>i</i>) 分别是输入和输出的像素强度;调节参数<i>φ</i><sub><i>l</i></sub>和<i>φ</i><sub><i>h</i></sub>的默认值都为0.15;<i>t</i><sub><i>Δ</i></sub> (<i>i</i>) 是增量函数, 定义为<i>t</i><sub><i>Δ</i></sub> (<i>i</i>) =<i>k</i><sub>1</sub>·exp (-<i>k</i><sub>2</sub>·<i>i</i><sup><i>k</i><sub>3</sub></sup>) , 其中<i>k</i><sub>2</sub>和<i>k</i><sub>3</sub>用来控制欠曝光及过曝光区域的映射范围, 本文采用默认的参数 (<i>k</i><sub>1</sub>=5, <i>k</i><sub>2</sub>=14, <i>k</i><sub>3</sub>=1.6) 来保证<i>t</i><sub><i>Δ</i></sub> (<i>i</i>) 在[0, 0.5]有效。</p>
                </div>
                <div class="p1">
                    <p id="86">通过上述的非线性映射便可以得到最终的入射光分量<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">L</mi><mo>˜</mo></mover></math></mathml>。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 S曲线" src="Detail/GetImg?filename=images/JSJY201907037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 S曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 S curve</p>

                </div>
                <h4 class="anchor-tag" id="89" name="89">1.3 <b>反射光分量</b></h4>
                <div class="p1">
                    <p id="90">基于1.2节得到的入射光分量序列, 令<b><i>R</i></b><sub><i>k</i></sub>=<b><i>I</i></b><sub><i>k</i></sub>/<b><i>L</i></b><sub><i>k</i></sub>, 可以得到对应的一组反射光分量序列<b><i>R</i></b><sub><i>k</i></sub>={<b><i>R</i></b><sub>1</sub>, <b><i>R</i></b><sub>2</sub>, …, <b><i>R</i></b><sub><i>m</i></sub>}, 理想条件下, 这组反射光分量应该是一致的, 但受制于光照估计算法的准确性以及噪声的影响, 现有算法很难得到相同的反射光分量。</p>
                </div>
                <div class="p1">
                    <p id="91">本文通过图像融合的方法从反射光分量序列中还原出具有更多色彩及细节信息的反射光分量。对于场景区域而言, 曝光时间不足或曝光时间过长会导致图像对应位置产生灰暗或饱和区域, 颜色失真且细节信息丢失严重;相反地, 曝光合适的图像区域颜色信息及细节纹理表现丰富<citation id="243" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。此外, 欠曝光区域会包含大量的噪声。</p>
                </div>
                <div class="p1">
                    <p id="92">因此, 在反射光分量的融合过程中, 应该对曝光合适的像素分配较大的权值, 而对欠曝光或者过曝光区域的像素分配很小的权值。Mertens等<citation id="244" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>以像素的归一化像素值与0.5的接近程度评价曝光适度, 并以此来分配像素的融合权重, 利用高斯模型进行计算:</p>
                </div>
                <div class="p1">
                    <p id="93"><i>g</i> (<i>i</i>) =exp (- (<i>i</i>-0.5) <sup>2</sup>/ (2<i>σ</i><sup>2</sup>) )      (9) </p>
                </div>
                <div class="p1">
                    <p id="94">其中:<i>i</i>和<i>g</i> (<i>i</i>) 分别是输入和输出的像素值;<i>σ</i>是高斯方程的标准差, 默认值为0.2。</p>
                </div>
                <div class="p1">
                    <p id="95">Mertens等<citation id="245" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>是尽可能保留每幅输入图像中像素值在0.5附近的像素信息, 但这样容易丢失场景灰暗和明亮处的细节。而本文算法的融合权重是基于入射光分量得到的:<b><i>w</i></b><sub><i>k</i></sub>=<i>g</i> (<b><i>L</i></b><sub><i>k</i></sub>) , 这样更加合理, 得到的反射光分量也能更好地保留图像细节及色彩。</p>
                </div>
                <div class="p1">
                    <p id="96">然后, 对权重进行归一化处理, 以确保每个像素点的权值和为1, 即:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo>/</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi mathvariant="bold-italic">w</mi></mstyle><msub><mrow></mrow><msup><mi>k</mi><mo>′</mo></msup></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">最后, 通过加权融合得到融合后的反射光分量<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">R</mi><mo>˜</mo></mover></math></mathml>, 即:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">R</mi><mo>˜</mo></mover><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>^</mo></mover></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>˚</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>k</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">此外, 这样做还可以有效地抑制噪声的影响, 如图2所示, 两幅图像均是对智能手机所拍摄的一组曝光序列图像进行融合的结果。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907037_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 室内晚间场景的曝光序列图像融合结果" src="Detail/GetImg?filename=images/JSJY201907037_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 室内晚间场景的曝光序列图像融合结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907037_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Exposure sequence image fusion results of indoor evening scene</p>

                </div>
                <h4 class="anchor-tag" id="103" name="103">1.4 <b>最终结果</b></h4>
                <div class="p1">
                    <p id="104">至此, 本文分别采用不同的处理方法得到了入射光分量<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">L</mi><mo>˜</mo></mover></math></mathml>和反射光分量<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">R</mi><mo>˜</mo></mover></math></mathml>, 将两者相乘即可得到最终的图像<b><i>F</i></b>。本文算法的流程如图3所示。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法流程" src="Detail/GetImg?filename=images/JSJY201907037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Flow chart of the proposed algorithm</p>

                </div>
                <h3 id="108" name="108" class="anchor-tag">2 实验结果及分析</h3>
                <div class="p1">
                    <p id="109">对包括图4和图5在内的多组图像序列进行实验, 并与<i>Mertens</i>等<citation id="246" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、<i>Ma</i>等<citation id="247" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、<i>Li</i>13等<citation id="248" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、<i>Li</i>12等<citation id="249" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、<i>Li</i>17等<citation id="250" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、<i>Gu</i>等<citation id="251" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>以及<i>Lee</i>等<citation id="252" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出的方法进行比较, 以便综合分析本文方法的性能。实验所用计算机的处理器为<i>Intel Core i</i>5, 主频为3 <i>GHz</i>, 内存为8 <i>GB</i>, 相关算法程序在<i>Matlab R</i>2017<i>a</i>环境下编写。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">2.1 <b>主观比较</b></h4>
                <div class="p1">
                    <p id="111">主观分析主要是同<i>Mertens</i>等<citation id="253" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、<i>Ma</i>等<citation id="254" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、<i>Li</i>17<citation id="255" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>以及<i>Lee</i>等<citation id="256" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的算法进行对比分析。对图4 (<i>a</i>) 所示的“<i>Tower</i>”场景进行实验, 相关方法处理结果如图4 (<i>b</i>) 所示。<i>Mertens</i>等<citation id="257" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的算法呈现出了较好的全局对比度和色彩饱和度, 但局部对比度低, 主要是场景的亮处和暗处的区域。相比<i>Mertens</i>等<citation id="258" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的算法, <i>Lee</i>等<citation id="259" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的算法增强了整体效果以及暗处的细节, 但是高亮处的细节信息仍然丢失严重。<i>Ma</i>等<citation id="260" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的算法能够更好地保留场景亮处和暗处的细节信息, 但出现了一定的颜色退化现象, 主要表现为云朵区域的色彩效果较差。相比之下, <i>Li</i>17等<citation id="261" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的算法呈现了更好的效果, 但与本文算法相比, 塔身处的细节表现较差, 并且花草区域的色彩更暗淡些。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907037_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Tower图像序列曝光融合结果" src="Detail/GetImg?filename=images/JSJY201907037_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 <i>Tower</i>图像序列曝光融合结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907037_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Exposure fusion results of Tower sequence</i></p>

                </div>
                <div class="p1">
                    <p id="113">为了进一步分析本文算法的优势, 对图5 (<i>a</i>) 所示的室内场景“<i>Madison Capitol</i>”进行实验, 结果如图5 (<i>b</i>) 所示。<i>Mertens</i>等<citation id="262" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、<i>Ma</i>等<citation id="263" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和<i>Lee</i>等<citation id="264" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出的方法处理结果都未能保持良好的局部对比度, 图中窗户区域过亮, 天空颜色退化明显, 此外, 壁画区域的细节也不够清晰;<i>Li</i>17等<citation id="265" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出的算法较好地保留场景的细节信息, 但在窗户、壁画等区域都存在一定的颜色退化现象;而本文算法的处理结果兼顾了颜色和对比度, 更好地保持了柱子等处的颜色信息。</p>
                </div>
                <div class="p1">
                    <p id="114">综合而言, 本文算法得到的融合图像能够有效地保留场景不同亮度区域的细节信息, 并且融合图像还能有效地保持场景的色彩特性, 具有良好的视觉效果。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">2.2 <b>客观评价</b></h4>
                <div class="p1">
                    <p id="116">为保证实验的公平性, 本文选取了多组不同场景的多曝光序列对包括本文算法在内的8种算法进行客观分析, 所有测试序列都是静态的, 每个序列都有不同的特征, 如室内或室外、有无照明灯具以及曝光序列数目的多少, 并且本文算法的参数均采用默认值。</p>
                </div>
                <div class="p1">
                    <p id="117">本文采用<i>MEF</i>-<i>SSIM</i> (<i>Multi</i>-<i>Exposure image Fusion</i>-<i>Structural SIMilarity index</i>) 方法<citation id="266" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>作为客观评价指标, 该图像质量评估模型是专门为多曝光图像融合算法而设计的, 它基于多尺度结构相似性, 不仅考虑局部结构保持而且考虑全局亮度一致性。</p>
                </div>
                <div class="p1">
                    <p id="118">本文对12组曝光序列进行了测试, 结果如表1所示, 从中可以看出本文算法的优越性。本文所提算法的<i>MEF</i>-<i>SSIM</i>评分与<i>Li</i>17等<citation id="267" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和<i>Lee</i>等<citation id="268" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的算法相当, 但比其他算法有着明显的优势。总体而言, 本文算法在场景细节保留上有一定的优势, 并且算法的效果更加稳定。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119">2.3 <b>计算效率</b></h4>
                <div class="p1">
                    <p id="120">表2给出了本文算法同其他4种多曝光图像融合算法的计算效率对比结果, 所有算法均采用<i>Matlab</i>编程实现, 运行时间也都是在同一<i>PC</i>平台上运行得到。<i>Mertens</i>等<citation id="269" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出的方法有较快的处理速度, <i>Lee</i>等<citation id="270" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>和<i>Ma</i>等<citation id="271" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的方法次之, 但是他们的融合效果并不理想。<i>Li</i>17等<citation id="272" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出的方法基于多尺度分解, 融合效果较好, 但处理时间最长;而本文方法的处理效率虽然略高于<i>Ma</i>等<citation id="273" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的算法, 但是融合效果更好。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201907037_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Madison Capitol图像序列曝光融合结果" src="Detail/GetImg?filename=images/JSJY201907037_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>Madison Capitol</i>图像序列曝光融合结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201907037_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Exposure fusion results of Madison Capitol sequence</i></p>

                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表</b>1 <b>图像质量评估分数</b> (<i>MEF</i>-<i>SSIM</i><b>方法</b>)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Image quality assessment scores</i> (<i>MEF</i>-<i>SSIM method</i>) </p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td><br />曝光<br />序列</td><td><i>Mertens</i><br />等<sup>[5]</sup><br />算法</td><td><i>Gu</i><br />等<sup>[19]</sup><br />算法</td><td><i>Li</i>12<br />等<sup>[11]</sup><br />算法</td><td><i>Li</i>13<br />等<sup>[10]</sup><br />算法</td><td><i>Ma</i><br />等<sup>[8]</sup><br />算法</td><td><i>Li</i>17<br />等<sup>[12]</sup><br />算法</td><td><i>Lee</i><br />等<sup>[20]</sup><br />算法</td><td>本文<br />算法</td></tr><tr><td><br /><i>Belgium</i></td><td>0.965</td><td>0.896</td><td>0.954</td><td>0.964</td><td>0.965</td><td><b>0.968</b></td><td>0.966</td><td><b>0.970</b></td></tr><tr><td><br />Cave</td><td>0.974</td><td>0.934</td><td>0.923</td><td><b>0.978</b></td><td><b>0.980</b></td><td>0.973</td><td>0.974</td><td>0.976</td></tr><tr><td><br />Kluki</td><td><b>0.978</b></td><td>0.922</td><td>0.948</td><td>0.968</td><td>0.961</td><td><b>0.977</b></td><td>0.975</td><td>0.966</td></tr><tr><td><br />Landscape</td><td>0.976</td><td>0.941</td><td>0.948</td><td>0.942</td><td><b>0.991</b></td><td>0.976</td><td>0.981</td><td><b>0.982</b></td></tr><tr><td><br />Lighthouse</td><td><b>0.975</b></td><td>0.934</td><td>0.968</td><td>0.950</td><td>0.969</td><td><b>0.970</b></td><td>0.969</td><td>0.968</td></tr><tr><td><br />Madison <br />Capitol</td><td>0.977</td><td>0.864</td><td>0.949</td><td>0.968</td><td>0.974</td><td>0.967</td><td><b>0.981</b></td><td><b>0.979</b></td></tr><tr><td><br />Memorial</td><td>0.967</td><td>0.871</td><td>0.947</td><td><b>0.968</b></td><td>0.940</td><td><b>0.979</b></td><td>0.966</td><td>0.964</td></tr><tr><td><br />Office</td><td>0.984</td><td>0.900</td><td>0.954</td><td>0.967</td><td>0.986</td><td>0.981</td><td><b>0.989</b></td><td><b>0.987</b></td></tr><tr><td><br />River</td><td>0.919</td><td>0.875</td><td>0.945</td><td>0.928</td><td><b>0.946</b></td><td>0.944</td><td>0.933</td><td><b>0.945</b></td></tr><tr><td><br />Ship</td><td>0.950</td><td>0.899</td><td>0.965</td><td>0.869</td><td>0.962</td><td><b>0.967</b></td><td>0.961</td><td><b>0.963</b></td></tr><tr><td><br />Tower</td><td>0.986</td><td>0.931</td><td>0.950</td><td>0.986</td><td>0.981</td><td>0.984</td><td><b>0.987</b></td><td><b>0.987</b></td></tr><tr><td><br />Venice</td><td>0.966</td><td>0.889</td><td>0.937</td><td>0.954</td><td><b>0.978</b></td><td>0.951</td><td><b>0.972</b></td><td>0.971</td></tr><tr><td><br />平均</td><td>0.968</td><td>0.905</td><td>0.949</td><td>0.954</td><td>0.969</td><td>0.970</td><td><b>0.971</b></td><td><b>0.971</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="123" name="123" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="124">本文提出了一种能够更好保留场景细节及色彩的多曝光图像融合算法。该算法基于Retinex理论模型, 利用高效的光照估计方法将曝光序列分成入射光分量序列及反射光分量序列, 然后分别采用具有针对性的融合方法对这两组序列进行处理。选取了12组不同的曝光序列, 分别从主观和客观两方面对算法进行比较分析, 实验结果表明本文算法能够有效地提取场景不同亮度范围的细节信息并保持场景的颜色信息, 融合图像信息量高, 视觉效果表现最佳。后续研究将考虑针对动态场景的应用, 进一步提升算法的性能。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表</b>2 <b>不同方法的处理时间对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Processing time comparison of different methods </p>
                    <p class="img_note">s</p>
                    <table id="125" border="1"><tr><td><br />图像</td><td>Mertens<br />等<sup>[5]</sup>算法</td><td>Lee<br />等<sup>[20]</sup>算法</td><td>Ma<br />等<sup>[8]</sup>算法</td><td>Li17<br />等<sup>[12]</sup>算法</td><td>本文<br />算法</td></tr><tr><td><br />Venice</td><td>0.9</td><td>1.3</td><td>1.5</td><td>3.8</td><td>1.6</td></tr><tr><td><br />Cave</td><td>2.2</td><td>5.1</td><td>5.5</td><td>10.6</td><td>6.3</td></tr><tr><td><br />House</td><td>1.2</td><td>3.7</td><td>3.1</td><td>6.5</td><td>3.5</td></tr><tr><td><br />Lamp</td><td>1.7</td><td>3.8</td><td>4.3</td><td>8.7</td><td>5.0</td></tr><tr><td><br />Belgium</td><td>2.3</td><td>5.4</td><td>7.1</td><td>15.5</td><td>7.2</td></tr><tr><td><br />Madison</td><td>3.3</td><td>8.4</td><td>9.2</td><td>18.8</td><td>10.8</td></tr><tr><td><br />Office</td><td>1.9</td><td>4.7</td><td>6.3</td><td>12.7</td><td>5.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="179">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XAYD201603001&amp;v=MTk2NTdCdEdGckNVUjdxZlp1WnNGeS9nVTd6QVBTelNhckc0SDlmTXJJOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 白本督, 刘军, 范九伦.高动态范围成像研究进展[J].西安邮电大学学报, 2016, 21 (3) :1-14. (BAI B D, LIU J, FAN J L.Recent research in high dynamic range imaging[J].Journal of Xi'an University of Posts and Telecommunications, 2016, 21 (3) :1-14) 
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exposure-robust alignment of differently exposed images">

                                <b>[2]</b> WU S, LI Z, ZHENG J, et al.Exposure-robust alignment of differently exposed images[J].IEEE Signal Processing Letters, 2014, 21 (7) :885-889.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201608003&amp;v=Mjk0MDU3RzRIOWZNcDQ5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2dVN3pBTHo3QmI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 胡胜男, 张伟, 刘侃, 等.高动态范围成像技术中的鬼影检测与去除方法综述[J].计算机科学, 2016, 43 (8) :13-18. (HU S N, ZHANG W, LIU K, et al.Survey of ghost detection and removal methods in HDR imaging technology [J].Computer Science, 2016, 43 (8) :13-18.) 
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual-salience-based tone mapping for high dynamic range images">

                                <b>[4]</b> LI Z, ZHENG J.Visual-salience-based tone mapping for high dynamic range images[J].IEEE Transactions on Industrial Electronics, 2014, 61 (12) :7076-7082.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exposure Fusion: A Simple and Practical Alternative to High Dynamic Range Photography">

                                <b>[5]</b> MERTENS T, KAUTZ J, van REETH F.Exposure fusion:a simple and practical alternative to high dynamic range photography[J].Computer Graphics Forum, 2010, 28 (1) :161-171.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradient-Directed Multiexposure Composition">

                                <b>[6]</b> ZHANG W, CHAN W K.Gradient-directed multi-exposure composition[J].IEEE Transactions on Image Processing, 2012, 21 (4) :2318-2323.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized random walks for fusion of multi-exposure images">

                                <b>[7]</b> SHEN R, CHENG I, SHI J, et al.Generalized random walks for fusion of multi-exposure images[J].IEEE Transactions on Image Processing, 2011, 20 (12) :3634-3646.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-exposure image fusion:A patch-wise approach">

                                <b>[8]</b> MA K, WANG Z.Multi-exposure image fusion:a patch-wise approach[C]// Proceedings of the 2015 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2015:1717-1721.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-exposure image fusion based on illumination estimation">

                                <b>[9]</b> VONIKAKIS V, BOUZOS O, ANDREADIS L.Multi-exposure image fusion based on illumination estimation[C]// Proceedings of the 2011 IASTED Signal and Image Processing and Applications.Crete, Greece:IASTED, 2011:738-051.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image fusion with guided filtering">

                                <b>[10]</b> LI S, KANG X, HU J.Image fusion with guided filtering[J].IEEE Transactions on Image Processing, 2013, 22 (7) :2864-2875.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detail-Enhanced Exposure Fusion">

                                <b>[11]</b> LI Z G, ZHENG J H, RAHARDJA S.Detail-enhanced exposure fusion[J].IEEE Transactions on Image Processing, 2012, 21 (11) :4672-4676.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detailenhanced multi-Scale exposure fusion">

                                <b>[12]</b> LI Z G, WEI Z, WEN C, et al.Detail-enhanced multi-scale exposure fusion[J].IEEE Transactions on Image Processing, 2017, 26 (3) :1243-1252.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The retinex theory of color vision">

                                <b>[13]</b> LAND E H.The Retinex theory of color vision[J].Scientific American, 1977, 237 (6) :108-128.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structure-Revealing Low-Light Image Enhancement Via Robust Retinex Model.">

                                <b>[14]</b> LI M, LIU J, YANG W, et al.Structure-revealing low-light image enhancement via robust Retinex model[J].IEEE Transactions on Image Processing, 2018, 27 (6) :2828-2841.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LIME:Low-light image enhancement via illumination map estimation">

                                <b>[15]</b> GUO X, LI Y, LING H.LIME:low-light image enhancement via illumination map estimation[J].IEEE Transactions on Image Processing, 2017, 26 (2) :982-993.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007775&amp;v=MDA3NTd0ak5yNDlGWk9zSUMzczhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lKMXNVYVJvPU5pZklZN0s3SA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> XU L, YAN Q, XIA Y, et al.Structure extraction from texture via relative total variation[J].ACM Transactions on Graphics, 2012, 31 (6) :Article No.139.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic exposure correction of consumer photographs">

                                <b>[17]</b> YUAN L, SUN J.Automatic exposure correction of consumer photographs[C]// Proceedings of the 2012 European Conference on Computer Vision.Berlin:Springer, 2012:771-785.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC201503011&amp;v=MjE3OTVFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvZ1U3ekFQeW5SYmJHNEg5VE1ySTk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 江燊煜, 陈阔, 徐之海, 等.基于曝光适度评价的多曝光图像融合方法[J].浙江大学学报 (工学版) , 2015, 49 (3) :470-475. (JIANG S Y, CHEN K, XU Z H, et al.Multi-exposure image fusion based on well-exposedness assessment [J].Journal of Zhejiang University (Engineering Edition) , 2015, 49 (3) :470-475.) 
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501263279&amp;v=MzE0Njd0RmlubFVyM0lKMXNVYVJvPU5pZk9mYks3SHRETnFvOUVadTBNRG5zd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> GU B, LI W, WONG J, et al.Gradient field multi-exposure images fusion for high dynamic range image visualization[J].Journal of Visual Communication and Image Representation, 2012, 23 (4) :604-610.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A multi-exposure image fusion based on the adaptive weights reflecting the relative pixel intensity and global gradient">

                                <b>[20]</b> LEE S, PARK J S, CHO N I.A multi-exposure image fusion based on the adaptive weights reflecting the relative pixel intensity and global gradient[C]// Proceedings of the 2018 IEEE International Conference on Image Processing.Piscataway, NJ:IEEE, 2018:1737-1741.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual Quality Assessment for Multi-Exposure Image Fusion">

                                <b>[21]</b> MA K, ZENG K, WANG Z.Perceptual quality assessment for multi-exposure image fusion[J].IEEE Transactions on Image Processing, 2015, 24 (11) :3345-3356.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201907037" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201907037&amp;v=MTAyMzZxQnRHRnJDVVI3cWZadVpzRnkvZ1U3ekFMejdCZDdHNEg5ak1xSTlHWTRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
