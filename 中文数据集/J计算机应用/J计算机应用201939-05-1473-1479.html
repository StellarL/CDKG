<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136766301377500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905040%26RESULT%3d1%26SIGN%3dmzZGpaAVNbofhg%252bpHbUON%252bjZaHE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905040&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905040&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905040&amp;v=MzA4OTh6TUx6N0JkN0c0SDlqTXFvOUJaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="1 网络设计方案 ">1 网络设计方案</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#76" data-title="1.1 &lt;b&gt;运动补偿模块&lt;/b&gt;">1.1 <b>运动补偿模块</b></a></li>
                                                <li><a href="#94" data-title="1.2 &lt;b&gt;去压缩伪影模块结构&lt;/b&gt;">1.2 <b>去压缩伪影模块结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="2 实验与分析 ">2 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#106" data-title="2.1 &lt;b&gt;数据准备和实验设置&lt;/b&gt;">2.1 <b>数据准备和实验设置</b></a></li>
                                                <li><a href="#114" data-title="2.2 &lt;b&gt;网络结构分析&lt;/b&gt;">2.2 <b>网络结构分析</b></a></li>
                                                <li><a href="#129" data-title="2.3 &lt;b&gt;实验结果&lt;/b&gt;">2.3 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#135" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="图1 总体网络框架">图1 总体网络框架</a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;运动补偿模块中各卷积块的参数设置&lt;/b&gt;"><b>表</b>1 <b>运动补偿模块中各卷积块的参数设置</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;去压缩伪影网络各卷积层参数设置&lt;/b&gt;"><b>表</b>2 <b>去压缩伪影网络各卷积层参数设置</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;本文方法与其他光流估计方法的网络复杂度对比&lt;/b&gt;"><b>表</b>3 <b>本文方法与其他光流估计方法的网络复杂度对比</b></a></li>
                                                <li><a href="#118" data-title="图2 当前帧和后一帧以及补偿帧和后一帧的帧间差">图2 当前帧和后一帧以及补偿帧和后一帧的帧间差</a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;在测试序列上补偿帧对其压缩帧的像素补偿情况&lt;/b&gt;"><b>表</b>4 <b>在测试序列上补偿帧对其压缩帧的像素补偿情况</b></a></li>
                                                <li><a href="#126" data-title="图3 单一去伪影网络和运动补偿去伪影网络结果">图3 单一去伪影网络和运动补偿去伪影网络结果</a></li>
                                                <li><a href="#128" data-title="图4 运动补偿网络和去伪影网络收敛情况">图4 运动补偿网络和去伪影网络收敛情况</a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;i&gt;QP&lt;/i&gt;=37&lt;b&gt;时在不同测试序列上的五种算法&lt;/b&gt;Δ&lt;i&gt;PSNR&lt;/i&gt;&lt;b&gt;比较&lt;/b&gt; dB"><b>表</b>5 <i>QP</i>=37<b>时在不同测试序列上的五种算法</b>Δ<i>PSNR</i><b>比较</b> dB</a></li>
                                                <li><a href="#133" data-title="图5 去视频压缩伪影结果比较">图5 去视频压缩伪影结果比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="170">


                                    <a id="bibliography_1" title=" DONG C, DENG Y, CHEN C L, et al.Compression artifacts reduction by a deep convolutional network[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:576-584." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Compression artifacts reduction by a deep convolutional network &amp;quot;">
                                        <b>[1]</b>
                                         DONG C, DENG Y, CHEN C L, et al.Compression artifacts reduction by a deep convolutional network[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:576-584.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_2" title=" GUO J, CHAO H.Building dual-domain representations for compression artifacts reduction [C]// ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:628-644." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Building dual-domain representations for compression artifacts reduction">
                                        <b>[2]</b>
                                         GUO J, CHAO H.Building dual-domain representations for compression artifacts reduction [C]// ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:628-644.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_3" title=" GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial networks[J/OL].arXiv Preprint, 2014, 2014:arXiv:1406.2661 [2014- 06- 10].https://arxiv.org/abs/1406.2661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial networks">
                                        <b>[3]</b>
                                         GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial networks[J/OL].arXiv Preprint, 2014, 2014:arXiv:1406.2661 [2014- 06- 10].https://arxiv.org/abs/1406.2661.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_4" title=" GUO J, CHAO H.One-to-many network for visually pleasing compression artifacts reduction [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:4867-4876." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=One-to-many network for visually pleasing compression artifacts reduction">
                                        <b>[4]</b>
                                         GUO J, CHAO H.One-to-many network for visually pleasing compression artifacts reduction [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:4867-4876.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_5" title=" GALTERI L, SEIDENARI L, BERTINI M, et al.Deep generative adversarial compression artifact removal [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:4836-4845." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep generative adversarial compression artifact removal">
                                        <b>[5]</b>
                                         GALTERI L, SEIDENARI L, BERTINI M, et al.Deep generative adversarial compression artifact removal [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:4836-4845.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_6" title=" 杨丽丽, 盛国.一种基于卷积神经网络的矿井视频图像降噪方法[J].矿业研究与开发, 2018, 38 (2) :106-109. (YANG L L, SHENG G.A mine video image denoising method based on convolutional neural network[J].Mining Research and Development, 2018, 38 (2) :106-109.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KYYK201802022&amp;v=MTQ0MDc0SDluTXJZOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXJ6UExqVFNaYkc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         杨丽丽, 盛国.一种基于卷积神经网络的矿井视频图像降噪方法[J].矿业研究与开发, 2018, 38 (2) :106-109. (YANG L L, SHENG G.A mine video image denoising method based on convolutional neural network[J].Mining Research and Development, 2018, 38 (2) :106-109.) 
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_7" title=" REN W, PAN J, CAO X, et al.Video deblurring via semantic segmentation and pixel-wise non-linear kernel[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:1086-1094." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Video deblurring via semantic segmentation and pixel-wise non-linear kernel">
                                        <b>[7]</b>
                                         REN W, PAN J, CAO X, et al.Video deblurring via semantic segmentation and pixel-wise non-linear kernel[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:1086-1094.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_8" title=" SAJJADI M S M, VEMULAPALLI R, BROWN M.Frame-recurrent video super-resolution[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6626-6634." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Frame-recurrent video super-resolution">
                                        <b>[8]</b>
                                         SAJJADI M S M, VEMULAPALLI R, BROWN M.Frame-recurrent video super-resolution[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6626-6634.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_9" title=" TAO X, GAO H, LIAO R, et al.Detail-revealing deep video super-resolution [C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6626-6634." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detail-revealing deep video super-resolution">
                                        <b>[9]</b>
                                         TAO X, GAO H, LIAO R, et al.Detail-revealing deep video super-resolution [C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6626-6634.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_10" title=" 李玲慧, 杜军平, 梁美玉, 等.基于时空特征和神经网络的视频超分辨率算法[J].北京邮电大学学报, 2016, 39 (4) :1-6. (LI L H, DU J P, LIANG M Y, et al.Video super resolution algorithm based on spatiotemporal features and neural networks[J].Journal of Beijing University of Posts and Telecommunications, 2016, 39 (4) :1-6.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJYD201604001&amp;v=MDAzNjNTYXJHNEg5Zk1xNDlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVyelBKeWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         李玲慧, 杜军平, 梁美玉, 等.基于时空特征和神经网络的视频超分辨率算法[J].北京邮电大学学报, 2016, 39 (4) :1-6. (LI L H, DU J P, LIANG M Y, et al.Video super resolution algorithm based on spatiotemporal features and neural networks[J].Journal of Beijing University of Posts and Telecommunications, 2016, 39 (4) :1-6.) 
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_11" title=" WANG T, CHEN M, CHAO H.A novel deep learning-based method of improving coding efficiency from the decoder-end for HEVC[C]// Proceedings of the 2017 Data Compression Conference.Piscataway, NJ:IEEE, 2017:410-419." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A Novel Deep Learning-Based Method of Improving Coding Efficiency from the Decoder-End for HEVC,&amp;quot;">
                                        <b>[11]</b>
                                         WANG T, CHEN M, CHAO H.A novel deep learning-based method of improving coding efficiency from the decoder-end for HEVC[C]// Proceedings of the 2017 Data Compression Conference.Piscataway, NJ:IEEE, 2017:410-419.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_12" title=" YANG R, XU M, WANG Z.Decoder-side HEVC quality enhancement with scalable convolutional neural network[C]// Proceedings of the 2017 IEEE International Conference on Multimedia and Expo.Piscataway, NJ:IEEE, 2017:817-822." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Decoder-side HEVC quality enhancement with scalable convolutional neural network">
                                        <b>[12]</b>
                                         YANG R, XU M, WANG Z.Decoder-side HEVC quality enhancement with scalable convolutional neural network[C]// Proceedings of the 2017 IEEE International Conference on Multimedia and Expo.Piscataway, NJ:IEEE, 2017:817-822.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_13" title=" YANG R, XU M, WANG Z, et al.Enhancing quality for HEVC compressed videos [J/OL].arXiv Preprint, 2018, 2018:arXiv:1709.06734 (2017- 09- 20) [2018- 07- 06].https://arxiv.org/abs/1709.06734." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhancing quality for HEVC compressed videos">
                                        <b>[13]</b>
                                         YANG R, XU M, WANG Z, et al.Enhancing quality for HEVC compressed videos [J/OL].arXiv Preprint, 2018, 2018:arXiv:1709.06734 (2017- 09- 20) [2018- 07- 06].https://arxiv.org/abs/1709.06734.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_14" title=" YANG R, XU M, LIU T, et al.Multi-frame quality enhancement for compressed video [C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6664-6673." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-frame quality enhancement for compressed video">
                                        <b>[14]</b>
                                         YANG R, XU M, LIU T, et al.Multi-frame quality enhancement for compressed video [C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6664-6673.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_15" title=" DOSOVITSKIY A, FISCHERY P, ILG E, et al.FlowNet:learning optical flow with convolutional networks [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:2758-2766." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Flownet:Learning optical flow with convolutional networks">
                                        <b>[15]</b>
                                         DOSOVITSKIY A, FISCHERY P, ILG E, et al.FlowNet:learning optical flow with convolutional networks [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:2758-2766.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_16" title=" BAILER C, TAETZ B, STRICKER D.Flow fields:dense correspondence fields for highly accurate large displacement optical flow estimation[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:4015-4023." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Flow fields: Densecorrespondence fields for highly accurate large displacementoptical flow estimation">
                                        <b>[16]</b>
                                         BAILER C, TAETZ B, STRICKER D.Flow fields:dense correspondence fields for highly accurate large displacement optical flow estimation[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:4015-4023.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_17" title=" REVAUD J, WEINZAEPFEL P, HARCHAOUI Z, et al.EpicFlow:edge-preserving interpolation of correspondences for optical flow [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1164-1172." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EpicFlow:Edge-preserving interpolation of correspondences for optical flow">
                                        <b>[17]</b>
                                         REVAUD J, WEINZAEPFEL P, HARCHAOUI Z, et al.EpicFlow:edge-preserving interpolation of correspondences for optical flow [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1164-1172.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_18" title=" ILG E, MAYER N, SAIKIA T, et al.FlowNet2.0:evolution of optical flow estimation with deep networks[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017, 2:6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FlowNet 2.0 Evolution of Optical Flow Estimation with Deep Networks">
                                        <b>[18]</b>
                                         ILG E, MAYER N, SAIKIA T, et al.FlowNet2.0:evolution of optical flow estimation with deep networks[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017, 2:6.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_19" title=" MAHAJAN D, HUANG F C, MATUSIK W, et al.Moving gradients:a path-based method for plausible image interpolation [J].ACM Transactions on Graphics, 2009, 28 (3) :Article No.42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007592&amp;v=Mjg0ODZ3VmFSVT1OaWZJWTdLN0h0ak5yNDlGWk9zSUNYVTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         MAHAJAN D, HUANG F C, MATUSIK W, et al.Moving gradients:a path-based method for plausible image interpolation [J].ACM Transactions on Graphics, 2009, 28 (3) :Article No.42.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_20" title=" JADERBERG M, SIMONYAN K, ZISSERMAN A, et al.Spatial transformer networks[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:2017-2025." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial transformer networks">
                                        <b>[20]</b>
                                         JADERBERG M, SIMONYAN K, ZISSERMAN A, et al.Spatial transformer networks[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:2017-2025.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     NIKLAUS S, MAI L, LIU F.Video frame interpolation via adaptive separable convolution [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2017:261-270.</a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_22" title=" HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[22]</b>
                                         HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_23" title=" HE K, ZHANG X, REN S, et al.Identity mappings in deep residual networks [C]// ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">
                                        <b>[23]</b>
                                         HE K, ZHANG X, REN S, et al.Identity mappings in deep residual networks [C]// ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:630-645.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_24" title=" DROZDZAL M, VORONTSOV E, CHARTRAND G, et al.The importance of skip connections in biomedical image segmentation [M]// Deep Learning and Data Labeling for Medical Applications.Berlin:Springer, 2016:179-187." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Registry of the International Society for Heart and Lung Transplantation:Thirty-fourth Adult Lung And Heart-Lung Transplantation Report—2017;Focus Theme:Allograft ischemic time">
                                        <b>[24]</b>
                                         DROZDZAL M, VORONTSOV E, CHARTRAND G, et al.The importance of skip connections in biomedical image segmentation [M]// Deep Learning and Data Labeling for Medical Applications.Berlin:Springer, 2016:179-187.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_25" title=" BOSSEN F.Common test conditions and software reference configurations [S/OL].[2013- 06- 20].http://wftp3.itu.int/av-arch/jctvc-site/2010_07_B_Geneva/JCTVC-B300.doc." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Common HM test conditions and software reference configurations">
                                        <b>[25]</b>
                                         BOSSEN F.Common test conditions and software reference configurations [S/OL].[2013- 06- 20].http://wftp3.itu.int/av-arch/jctvc-site/2010_07_B_Geneva/JCTVC-B300.doc.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_26" title=" GLOROT X, BENGIO Y.Understanding the difficulty of training deep feedforward neural networks [C]// Proceedings of the 13th International Conference on Artificial Intelligence and Statistics.Sardinia, Italy:JMLR, 2010:249-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">
                                        <b>[26]</b>
                                         GLOROT X, BENGIO Y.Understanding the difficulty of training deep feedforward neural networks [C]// Proceedings of the 13th International Conference on Artificial Intelligence and Statistics.Sardinia, Italy:JMLR, 2010:249-256.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_27" title=" KINGMA D, BA J.Adam:a method for stochastic optimization[EB/OL].[2018- 03- 20].http://yeolab.weebly.com/uploads/2/5/5/0/25509700/a_method_for_stochastic_optimization_.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">
                                        <b>[27]</b>
                                         KINGMA D, BA J.Adam:a method for stochastic optimization[EB/OL].[2018- 03- 20].http://yeolab.weebly.com/uploads/2/5/5/0/25509700/a_method_for_stochastic_optimization_.pdf.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_28" title=" BARRON J T.A more general robust loss function[J/OL].arXiv Preprint, 2017, 2017:arXiv:1701.03077 (2017- 01- 11) [2017- 01- 11].https://arxiv.org/abs/1701.03077." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A more general robust loss function">
                                        <b>[28]</b>
                                         BARRON J T.A more general robust loss function[J/OL].arXiv Preprint, 2017, 2017:arXiv:1701.03077 (2017- 01- 11) [2017- 01- 11].https://arxiv.org/abs/1701.03077.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_29" title=" LAI W S, HUANG J B, AHUJA N, et al.Deep Laplacian pyramid networks for fast and accurate super-resolution[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5835-5843." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution">
                                        <b>[29]</b>
                                         LAI W S, HUANG J B, AHUJA N, et al.Deep Laplacian pyramid networks for fast and accurate super-resolution[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5835-5843.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1473-1479 DOI:10.11772/j.issn.1001-9081.2018081801            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于自适应可分离卷积核的视频压缩伪影去除算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%81%82%E5%8F%AF%E5%8D%89&amp;code=41747264&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聂可卉</a>
                                <a href="javascript:;">刘文哲</a>
                                <a href="javascript:;">童同</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%9C%E6%B0%91&amp;code=06680517&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杜民</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E9%92%A6%E6%B3%89&amp;code=36215191&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高钦泉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E7%89%A9%E7%90%86%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0094575&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福州大学物理与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%BB%BA%E7%9C%81%E5%8C%BB%E7%96%97%E5%99%A8%E6%A2%B0%E4%B8%8E%E5%8C%BB%E8%8D%AF%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E7%A6%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福建省医疗器械与医药技术重点实验室(福州大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A6%8F%E5%BB%BA%E5%B8%9D%E8%A7%86%E4%BF%A1%E6%81%AF%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">福建帝视信息科技有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对目前视频质量增强和超分辨率重建等任务中常采用的光流估计相关算法只能估计像素点间线性运动的问题, 提出了一种新型多帧去压缩伪影网络结构。该网络由运动补偿模块和去压缩伪影模块组成。运动补偿模块采用自适应可分离卷积代替传统的光流估计算法, 能够很好地处理光流法不能解决的像素点间的曲线运动问题。对于不同视频帧, 运动补偿模块预测出符合该图像结构和像素局部位移的卷积核, 通过局部卷积的方式实现对后一帧像素的运动偏移估计和像素补偿。将得到的运动补偿帧和原始后一帧联结起来作为去压缩伪影模块的输入, 通过融合包含不同像素信息的两视频帧, 得到对该帧去除压缩伪影后的结果。与目前最先进的多帧质量增强 (MFQE) 算法在相同的训练集和测试集上训练并测试, 实验结果表明, 峰值信噪比提升 (Δ<i>PSNR</i>) 较MFQE最大增加0.44 dB, 平均增加0.32 dB, 验证了所提出网络具有良好的去除视频压缩伪影的效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E8%B4%A8%E9%87%8F%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频质量增强;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%89%E6%B5%81%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光流估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%90%E5%8A%A8%E8%A1%A5%E5%81%BF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">运动补偿;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应可分离卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8E%BB%E8%A7%86%E9%A2%91%E5%8E%8B%E7%BC%A9%E4%BC%AA%E5%BD%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">去视频压缩伪影;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    聂可卉 (1994—) , 女, 江西吉安人, 硕士研究生, 主要研究方向:计算机视觉、图像处理;;
                                </span>
                                <span>
                                    刘文哲 (1993—) , 男, 福建三明人, 硕士, 主要研究方向:计算机视觉、图像处理;;
                                </span>
                                <span>
                                    童同 (1986—) , 男, 安徽安庆人, 博士, 主要研究方向:计算机视觉、医学影像处理、脑疾病辅助诊断;;
                                </span>
                                <span>
                                    杜民 (1955—) , 女, 福建泉州人, 教授, 博士, 主要研究方向:传感技术、生物医学仪器;;
                                </span>
                                <span>
                                    *高钦泉 (1986—) , 男, 福建福州人, 副教授, 博士, 主要研究方向:计算机视觉、增强现实、医学影像处理。电子邮箱gqinquan@fzu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-04</p>

            </div>
                    <h1><b>Video compression artifact removal algorithm based on adaptive separable convolution network</b></h1>
                    <h2>
                    <span>NIE Kehui</span>
                    <span>LIU Wenzhe</span>
                    <span>TONG Tong</span>
                    <span>DU Min</span>
                    <span>GAO Qinquan</span>
            </h2>
                    <h2>
                    <span>School of Physics and Information Engineering, Fuzhou University</span>
                    <span>Key Laboratory of Medical Instrumentation & Pharmaceutical Technology of Fujian Province (Fuzhou University)</span>
                    <span>Imperial Vision Technology Company</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The existing optical flow estimation methods, which are frequently used in video quality enhancement and super-resolution reconstruction tasks, can only estimate the linear motion between pixels. In order to solve this problem, a new multi-frame compression artifact removal network architecture was proposed. The network consisted of motion compensation module and compression artifact removal module. With the traditional optical flow estimation algorithms replaced with the adaptive separable convolution, the motion compensation module was able to handle with the curvilinear motion between pixels, which was not able to be well solved by optical flow methods. For each video frame, a corresponding convolutional kernel was generated by the motion compensation module based on the image structure and the local displacement of pixels. After that, motion offsets were estimated and pixels were compensated in the next frame by means of local convolution. The obtained compensated frame and the original next frame were combined together as input for the compression artifact removal module. By fusing different pixel information of the two frames, the compression artifacts of the original frame were removed. Compared with the state-of-the-art Multi-Frame Quality Enhancement (MFQE) algorithm on the same training and testing datasets, the proposed network has the improvement of Peak Signal-to-Noise Ratio (Δ<i>PSNR</i>) increased by 0.44 dB at most and 0.32 dB on average. The experimental results demonstrate that the proposed network performs well in removing video compression artifacts.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20quality%20enhancement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video quality enhancement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optical%20flow%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optical flow estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=motion%20compensation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">motion compensation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20separable%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive separable convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20compression%20artifact%20removal&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video compression artifact removal;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    NIE Kehui, born in 1994, M. S. candidate. Her research interests include computer vision, image processing. ;
                                </span>
                                <span>
                                    LIU Wenzhe, born in 1993, M. S. His research interests include computer vision, image processing. ;
                                </span>
                                <span>
                                    TONG Tong, born in 1986, Ph. D. His research interests include computer vision, medical image processing, brain disease assisted diagnosis. ;
                                </span>
                                <span>
                                    DU Min, born in 1955, Ph. D. , professor. Her research interests include sensing technology, biomedical instrument. ;
                                </span>
                                <span>
                                    GAO Qinquan, born in 1986, Ph. D. , associate professor. His research interests include computer vision, augmented reality, medical image processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-04</p>
                            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="64">去压缩伪影是计算机视觉中的经典问题。图像和视频压缩算法通常通过减小媒体文件大小以降低传输带宽, 达到节省传输成本和时间的效果;然而这种压缩算法不可避免地导致图像和视频中信息的丢失和引入不必要的伪影, 严重影响用户的视觉体验, 因此, 如何去除压缩伪影并复原这些图像和视频是现在热门的研究问题。</p>
                </div>
                <div class="p1">
                    <p id="65">在过去几年中, 随着深度学习的发展, 许多方法已成功应用于去除图像压缩伪影:首先, 伪影减少卷积神经网络 (Artifacts Reduction Convolutional Neural Network, AR-CNN) <citation id="228" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>已经证明了深度卷积神经网络 (Convolutional Neural Network, CNN) 在去除图像中JPEG (Joint Photographic Experts Group) 压缩伪影的有效性; 随后, 深度双域卷积网络 (Deep Dual-domain Convolutional Network, DDCN) <citation id="229" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>采用在频域和像素域上同时对图像进行处理来去除压缩伪影; 近年来, 随着生成对抗网络<citation id="230" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>被提出并被广泛使用后, Guo等<citation id="231" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和Galteri等<citation id="232" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>采用生成对抗网络来去除图像的压缩伪影。上述提及的方法都验证了深度神经网络对于去除单一图像压缩伪影的有效性。</p>
                </div>
                <div class="p1">
                    <p id="66">目前, 通过以单帧图像作为输入得到的去伪影后的视频帧仍存在较严重的物体轮廓模糊甚至信息丢失的情况, 可见该方法在处理连续视频帧上具有较大的局限性。通过融合视频中连续的多帧图像, 利用相邻帧之间像素的相关性和帧间的信息互补性, 从而补偿各帧丢失的信息, 可以获得更好的去视频压缩伪影效果。</p>
                </div>
                <div class="p1">
                    <p id="67">现有的对视频的质量进行增强的研究主要分布在视频去噪去模糊、视频超分辨率重建等工作<citation id="237" type="reference"><link href="180" rel="bibliography" /><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><link href="186" rel="bibliography" /><link href="188" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>上。近来, Wang等<citation id="233" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出深层卷积自动解码器 (Deep CNN-based Auto Decoder, DCAD) 网络用于压缩视频质量恢复, 该网络由10层卷积层组成, 由于网络体积较小, 重建效果因此受限。Yang等<citation id="234" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了解码侧卷积神经网络 (Decoder-Side Convolutional Neural Network, DS-CNN) 用于视频质量增强, 该网络由两个子网络组成, 其中帧内解码侧卷积神经网络 (Intra-Decoder-side Convolutional Neural Network, DS-CNN-I) 用来减少帧内编码的压缩伪影而帧间解码侧卷积神经网络 (Inter-Decoder-side Convolutional Neural Network, DS-CNN-B) 用来减少帧间编码的压缩伪影。由于以上两种方法均未使用到相邻视频帧间的信息, 故而均可看作是单帧图像去伪影算法。Yang等<citation id="235" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了分别通过两个不同网络处理HEVC (High Efficiency Video Coding) 帧内和帧间编码帧的质量增强卷积神经网络 (Quality Enhancement Convolutional Neural Network, QE-CNN) 方法。由于该方法仅考虑到去除HEVC编码的视频, 不适用于全部场景, 故而Yang等<citation id="236" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出多帧质量增强 (Multi-Frame Quality Enhancement, MFQE) 网络结构。MFQE包含四部分: 一个支持向量机 (Support Vector Machine, SVM) 用于对高质量帧 (Peak Quality Frame, PQF) 和非高质量帧 (non-Peak Quality Frame, non-PQF) 进行分类, 运动补偿网络用来实现帧间运动补偿, 两个不同的质量增强网络分别用来减少PQF和non-PQF帧的压缩伪影。若压缩视频不存在PQF和non-PQF时 (例如压缩质量系数设置为CRF (Constant Rate Factor) ) , 该网络将不能很好地发挥作用。</p>
                </div>
                <div class="p1">
                    <p id="68">光流估计算法是利用图像序列中图像在时间域上的变化以及相邻帧之间的相关性来找到上一帧和当前帧之间存在的对应关系从而计算相邻帧之间物体运动的一种方法。对于传统的光流估计法<citation id="239" type="reference"><link href="198" rel="bibliography" /><link href="200" rel="bibliography" /><link href="202" rel="bibliography" /><link href="204" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>来说, 需要通过光流图估计和像素形变这两个阶段得到预测帧, 由于缺乏光流图的真实值, 故而以上方法存在较大误差。文献<citation id="238" type="reference">[<a class="sup">19</a>]</citation>指出光流图估计法被看作是点到点的固定的变换图 (transmission map) , 也即假定像素点A到像素点B的移动是一条直线 (反之亦然) , 而并未考虑像素点的曲线运动, 并且在视频运动过程中出现遮挡和模糊的情况时, 光流法可能会由于找不到相邻帧中对应的像素点而无法得到较为准确的运动路径。</p>
                </div>
                <div class="p1">
                    <p id="69">空间转换网络 (Spatial Transformer Network) <citation id="240" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的提出使得网络可以学习到两张图片像素的空间映射关系, 并将这种点对点的映射关系以网格转换 (grid transform) 的形式表现, 该形式可以类似表示光流场中矢量运动, 很快该空间转换网络被用于编码运动视频中的光流图特征<citation id="241" type="reference"><link href="196" rel="bibliography" /><link href="210" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">21</a>]</sup></citation>进行运动补偿操作。</p>
                </div>
                <div class="p1">
                    <p id="70">本文通过使用两个级联网络解决去除视频压缩伪影的问题。本文网络包括两个模块:运动补偿模块和去压缩伪影模块。与通常使用基于网格映射进行运动补偿的方法不同, 本文中的运动补偿网络通过一维的局部可分离卷积方式实现, 不仅可以有效地估计像素偏移, 同时可以对相邻帧间信息进行补偿, 为缺损视频帧带来更多像素信息。随后, 运动补偿模块得到的对后一帧的补偿帧联结原始的后一帧作为去压缩伪影模块的输入, 通过融合包含不同像素信息两帧, 重建后一帧视频帧, 实现去除压缩伪影的效果。该网络可以实现端到端的训练。</p>
                </div>
                <div class="p1">
                    <p id="71">本文的主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="72">1) 采用可分离局部卷积实现相邻帧间像素估计与补偿。较光流估计法点到点的直线运动估计, 该方法通过非线性特征映射的方式可以对像素间可能存在的曲线运动进行估计, 因而更具灵活性。</p>
                </div>
                <div class="p1">
                    <p id="73">2) 提出了一种新颖的基于卷积神经网络去除视频压缩伪影的网络模型方案, 网络模型由运动补偿模块和去压缩伪影模块相连接实现, 通过联结多帧图像作为网络输入从而融合相邻帧间缺损信息, 可以达到更好的去除视频伪影效果。</p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">1 网络设计方案</h3>
                <div class="p1">
                    <p id="75">本文提出的网络由运动补偿模块和去压缩伪影模块两部分组成。前者用于实现视频帧间运动估计和像素信息补偿, 后者通过像素信息融合去除视频帧的压缩伪影, 总体网络结构如图1所示。运动补偿模块输入压缩视频的连续两帧, 经过运动补偿网络之后可得到后一帧<b><i>I</i></b><sub><i>c</i>2</sub>的运动补偿帧<b><i>I</i></b><sub><i>c</i>2</sub>′。这一过程得到的运动补偿帧<b><i>I</i></b><sub><i>c</i>2</sub>′实现了对当前帧<i>I</i><sub><i>c</i>1</sub>到后一帧<i>I</i><sub><i>c</i>2</sub>的像素偏移信息的学习并且补偿了压缩视频中当前帧<b><i>I</i></b><sub><i>c</i>1</sub>和后一帧<b><i>I</i></b><sub><i>c</i>2</sub>的像素缺损信息。随后, 将运动补偿网络中得到的预测帧<b><i>I</i></b><sub><i>c</i>2</sub>′与压缩视频中的原始后一帧<b><i>I</i></b><sub><i>c</i>2</sub>联结起来作为去压缩伪影模块的输入, 经过网络的不断学习过程, 去压缩伪影模块的输出即为原始后一帧<b><i>I</i></b><sub><i>c</i>2</sub>去除压缩伪影后的结果。接下来的部分将详细阐述本文采用的网络结构及原理。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">1.1 <b>运动补偿模块</b></h4>
                <div class="p1">
                    <p id="77"><i>Niklaus</i>等<citation id="242" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出了自适应可分离卷积网络用于视频插帧工作, 并取得了不错的效果。该方法通过网络学习视频的第<i>i</i>帧到第<i>i</i>+1帧以及第<i>i</i>+1帧到第<i>i</i>+2帧的偏移量, 通过融合偏移量以及原始输入 (第<i>i</i>帧和第<i>i</i>+2帧) , 得到第<i>i</i>+1帧的预测值。该过程可用式 (1) 近似表示:</p>
                </div>
                <div class="p1">
                    <p id="78"><b><i>I</i></b><sub><i>c</i>2</sub>′=<b><i>W</i></b><sub>1</sub>* (<b><i>I</i></b><sub><i>c</i>1</sub>+<i>Δ</i><sub><i>i</i>, <i>i</i>+1</sub>) +<b><i>W</i></b><sub>2</sub>* (<b><i>I</i></b><sub><i>c</i>3</sub>+<i>Δ</i><sub><i>i</i>+2, <i>i</i>+1</sub>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="79">其中:<b><i>I</i></b><sub><i>c</i>1</sub>、<b><i>I</i></b><sub><i>c</i>3</sub>分别表示第<i>i</i>帧、第<i>i</i>+2帧;<b><i>I</i></b><sub><i>c</i>2</sub>′表示第<i>i</i>+1帧的预测帧;<b><i>W</i></b><sub>1</sub>和<b><i>W</i></b><sub>2</sub>表示分配的权重;*定义为卷积操作;<i>Δ</i><sub><i>i</i>, <i>i</i>+1</sub>和<i>Δ</i><sub><i>i</i>+2, <i>i</i>+1</sub>表示第<i>i</i>帧到第<i>i</i>+1帧以及第<i>i</i>+2帧到第<i>i</i>+1帧的偏移量。</p>
                </div>
                <div class="p1">
                    <p id="80">从Niklaus等的思想中, 得到本文运动补偿网络的灵感。本文运动补偿网络学习连续视频的第<i>i</i>帧和第<i>i</i>+1帧的运动偏移量, 从而得到对第<i>i</i>+1帧的预测值, 实现视频帧间运动估计。该过程可用如下表示:</p>
                </div>
                <div class="p1">
                    <p id="81"><b><i>I</i></b><sub><i>c</i>2</sub>′=<i>f</i> (<b><i>I</i></b><sub><i>c</i>1</sub>, <b><i>I</i></b><sub><i>c</i>2</sub>) =<b><i>I</i></b><sub><i>c</i>1</sub>+<i>Δ</i><sub><i>i</i>, <i>i</i>+1</sub>      (2) </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<b><i>I</i></b><sub><i>c</i>1</sub>, <b><i>I</i></b><sub><i>c</i>2</sub>分别表示第<i>i</i>帧和第<i>i</i>+1帧, <i>Δ</i><sub><i>i</i>, <i>i</i>+1</sub>表示第<i>i</i>帧到第<i>i</i>+1帧的偏移量, <i>f</i>表示网络学习到的映射函数。</p>
                </div>
                <div class="p1">
                    <p id="83">在本文方案中, 通过输入运动补偿网络连续的两帧<b><i>I</i></b><sub><i>c</i>1</sub>和<b><i>I</i></b><sub><i>c</i>2</sub>, 运动补偿网络根据输入的两帧间像素位置偏移量和各像素信息得到对后一帧<b><i>I</i></b><sub><i>c</i>2</sub>的运动补偿帧<b><i>I</i></b><sub><i>c</i>2</sub>′。运动补偿网络结构如图1 (a) 所示。运动补偿网络由多个编码模块和解码模块以及可分离卷积模块三部分组成。每个编码模块均由多个卷积层和平均池化层组成, 每个解码模块依次由卷积层和上采样层组成, 并通过跳跃连接操作将不同尺度上的特征图相结合。最后一个解码模块得到的输出作为可分离卷积模块的输入。可分离卷积模块包括两个子网络, 两个子网络分别得到的一维卷积核与原始视频帧进行卷积操作得到补偿后的结果。</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905040_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 总体网络框架" src="Detail/GetImg?filename=images/JSJY201905040_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 总体网络框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905040_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Overall network structure</p>

                </div>
                <div class="p1">
                    <p id="85">在编解码模块中, 每一个编码模块由三个3×3卷积层和一个步长为2的平均池化层组成。该模块通过堆叠三个卷积层作为一个卷积块以获取更大的感受野从而提取更多的特征信息。同时, 利用平均池化层将特征图缩小至原来的1/2, 保证网络可以整合更多的空间信息。每一个解码模块由三个3×3大小的卷积层和一个双线性上采样层依次排列组成。鉴于图像处理过程中上采样方式使用不当而可能造成的棋盘效应, 本文选择双线性插值法进行上采样操作<citation id="243" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。通过双线性插值将特征图放大至原来的2倍, 有效避免重建图像中的棋盘伪影。最后跳跃连接操作将每个编码模块中卷积块获得的特征图和对应解码块中相同尺寸的特征图结合起来以增强特征信息。通过平衡训练数据量和网络深度和宽度以最大程度提高网络性能, 故本文设置运动补偿模块中各卷积块的输出通道数如表1所示, 同时除分离卷积操作之外, 每一个卷积操作后接非线性激活函数ReLU (Rectified Linear Units) 以增强网络的非线性表达能力。</p>
                </div>
                <div class="p1">
                    <p id="86">最后一个解码器模块得到的输出特征图作为可分离卷积模块的输入。可分离卷积模块中使用两个可分离卷积子网络分别实现对一维卷积核<b><i>K</i></b><sub>h</sub> (<i>x</i>, <i>y</i>) , <b><i>K</i></b><sub>v</sub> (<i>x</i>, <i>y</i>) 的预测。其中<b><i>K</i></b><sub>h</sub> (<i>x</i>, <i>y</i>) , <b><i>K</i></b><sub>v</sub> (<i>x</i>, <i>y</i>) 可近似看作二维卷积核<b><i>K</i></b>的水平和垂直向量表示。其中分别代表水平和垂直分量的两个一维卷积核和其对应的二维卷积核的关系可由如下近似表示:</p>
                </div>
                <div class="p1">
                    <p id="87"><b><i>K</i></b> (<i>x</i>, <i>y</i>) ≈<b><i>K</i></b><sub>h</sub> (<i>x</i>, <i>y</i>) *<b><i>K</i></b><sub>v</sub> (<i>x</i>, <i>y</i>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="88">最后, 在可分离卷积模块中得到的两个一维卷积核分别与原始后一帧输入<b><i>I</i></b><sub><i>c</i>2</sub>进行卷积操作可得到对<b><i>I</i></b><sub><i>c</i>2</sub>运动补偿后的结果<b><i>I</i></b><sub><i>c</i>2</sub>′。该过程可用如下表示:</p>
                </div>
                <div class="p1">
                    <p id="89"><b><i>I</i></b><sub><i>c</i>2</sub>′ (<i>x</i>, <i>y</i>) =<b><i>K</i></b><sub>h</sub> (<i>x</i>, <i>y</i>) *<b><i>K</i></b><sub>v</sub> (<i>x</i>, <i>y</i>) *<b><i>I</i></b><sub><i>c</i>2</sub> (<i>x</i>, <i>y</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="90">其中*表示卷积操作。</p>
                </div>
                <div class="p1">
                    <p id="91">通过网络自学习, 得到的两个一维卷积核<b><i>K</i></b><sub>h</sub> (<i>x</i>, <i>y</i>) , <b><i>K</i></b><sub>v</sub> (<i>x</i>, <i>y</i>) 以非线性特征映射的方式捕捉相邻两帧的运动位移并进行补偿。同时, 对于大小为<i>n</i>×<i>n</i>的卷积核<b><i>K</i></b> (<i>x</i>, <i>y</i>) 而言, 参与网络运算的卷积核参数量由<i>n</i><sup>2</sup>减少为2<i>n</i>。因而, 用一维卷积核<b><i>K</i></b><sub>h</sub> (<i>x</i>, <i>y</i>) , <b><i>K</i></b><sub>v</sub> (<i>x</i>, <i>y</i>) 取代传统的二维卷积核<b><i>K</i></b> (<i>x</i>, <i>y</i>) 参与网络计算的方式可大幅减少网络参数量, 实现网络加速。</p>
                </div>
                <div class="p1">
                    <p id="92">两个子网络结构中卷积操作部分与解码模块结构一致, 由3个大小为3×3的卷积层和1个线性上采样层构成。分离卷积操作中一维卷积核的大小分别设置为39×1和1×39, 这是因为在本文所采用的训练数据中, 由于相邻视频帧间最大运动偏移量约为30个像素值, 本文发现卷积核大小为39可以很好地适应本文采用数据集中大多帧间运动偏移。尽管卷积核增大可以处理更大偏移量, 但是同时也会导致更大的运算量, 增加网络计算代价。</p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表</b>1 <b>运动补偿模块中各卷积块的参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Parameter setting of each convolution block in motion compensation module</p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td><br />卷积块</td><td>卷积核大小</td><td>输出通道数</td><td>步长</td><td>激活函数</td></tr><tr><td><br />编码块1</td><td>3×3</td><td>32</td><td>2</td><td>ReLU</td></tr><tr><td><br />编码块2</td><td>3×3</td><td>64</td><td>2</td><td>ReLU</td></tr><tr><td><br />编码块3</td><td>3×3</td><td>128</td><td>2</td><td>ReLU</td></tr><tr><td><br />编码块4</td><td>3×3</td><td>256</td><td>2</td><td>ReLU</td></tr><tr><td><br />解码块1</td><td>3×3</td><td>256</td><td>1</td><td>ReLU</td></tr><tr><td><br />解码块2</td><td>3×3</td><td>128</td><td>1</td><td>ReLU</td></tr><tr><td><br />解码块3</td><td>3×3</td><td>64</td><td>1</td><td>ReLU</td></tr><tr><td><br />一维分离卷积<b><i>K</i></b><sub>h</sub></td><td>39×1</td><td>3</td><td>1</td><td>—</td></tr><tr><td><br />一维分离卷积<b><i>K</i></b><sub>v</sub></td><td>1×39</td><td>3</td><td>1</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="94" name="94">1.2 <b>去压缩伪影模块结构</b></h4>
                <div class="p1">
                    <p id="95">用<i>F</i>表示常用的压缩算法 (例如:JPEG、 H.264、 H.265) , 去除压缩伪影这一工作的最终目标是通过网络学习低质量的压缩视频帧到高质量视频帧的映射关系<i>F</i><sup>-1</sup>, 也即对于低质量的压缩视频中的每一帧<b><i>I</i></b><sub>LQ</sub>重建出其对应的高清视频帧<b><i>I</i></b><sub>HQ</sub>。此过程可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="96"><b><i>I</i></b><sub>HQ</sub>=<i>F</i><sup>-1</sup> (<b><i>I</i></b><sub>LQ</sub>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="97">由于人眼视觉系统对色度变化并不敏感, 故而通常在去压缩算法中, 都会沿用色彩通道转换的方法, 将RGB通道转换到YCbCr色彩空间, 将亮度和色度分开以减少计算量。因而在本文工作中, 去压缩伪影网络的输入仅考虑亮度通道Y。</p>
                </div>
                <div class="p1">
                    <p id="98">近年来, 跳跃连接的方法, 尤其是恒等快捷连接<citation id="245" type="reference"><link href="212" rel="bibliography" /><link href="214" rel="bibliography" /><sup>[<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>的方法在卷积神经网络中的使用越来越多。He等<citation id="246" type="reference"><link href="212" rel="bibliography" /><link href="214" rel="bibliography" /><sup>[<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>通过堆叠多个残差单元来解决识别任务。沿用他们的工作思路, 本实验使用跳跃连接的方式构建去压缩伪影网络, 见图1 (b) 。从图中可看出, 网络的输入是原始压缩视频帧<b><i>I</i></b><sub><i>c</i>2</sub>和运动补偿模块对压缩视频帧<b><i>I</i></b><sub><i>c</i>2</sub>的补偿结果<b><i>I</i></b><sub><i>c</i>2</sub>′。由于增强了原始压缩视频帧<b><i>I</i></b><sub><i>c</i>2</sub>的像素信息, 故而去伪影效果比使用单帧去伪影效果更好。同时, 经过网络中一系列卷积层和跳跃连接操作融合低层与高层的特征信息, 避免深层网络可能带来的梯度消失问题<citation id="244" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>的同时增强去压缩伪影效果。</p>
                </div>
                <div class="p1">
                    <p id="99">去压缩伪影模块由初始卷积模块、残差卷积模块和图像重建模块三部分构成。初始卷积模块中 (图1 (b) 中卷积1) 卷积核大小设置为7×7, 以便通过更大的感受野获得更丰富的全局特征。本文中去压缩伪影网络共设置三个残差卷积模块, 每个残差卷积模块依次包含一个卷积层, 一个非线性激活层和一个卷积层, 并通过跳跃式连接将每个残差模块的输入与输出相结合, 作为下一次卷积操作的输入。残差卷积模块中卷积核大小均为3×3, 同时步长设置为1, 以保证输出特征图大小与原图一致。最后残差卷积模块中得到的特征图作为图像重建模块 (即图1 (b) 中卷积4和5) 的输入, 同时将初始卷积模块的输出通过跳跃式连接输入到图像重建模块中, 以避免梯度消失问题。最后图像重建模块执行卷积操作后得到去除压缩伪影后的结果。去压缩伪影模块中各卷积层的参数设置如表2所示。去压缩伪影模块中初始卷积部分, 残差卷积部分和图像重建部分的公式表示分别如下:</p>
                </div>
                <div class="p1">
                    <p id="100"><b><i>F</i></b><sub>0</sub>=ReLU (<b><i>W</i></b><sub>0</sub> (<b><i>I</i></b><sub><i>c</i>2</sub>′, <b><i>I</i></b><sub><i>c</i>2</sub>) + <b><i>B</i></b><sub>0</sub>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="101"><b><i>F</i></b><sub><i>k</i></sub>=<b><i>W</i></b><sub><i>k</i>_2</sub>ReLU (<b><i>W</i></b><sub><i>k</i>_1</sub><b><i>X</i></b>+<b><i>B</i></b><sub><i>k</i>_1</sub>) +<b><i>B</i></b><sub><i>k</i>_2</sub>+<b><i>X</i></b>      (7) </p>
                </div>
                <div class="p1">
                    <p id="102"><b><i>I</i></b><sup>*</sup><sub><i>c</i>2</sub>=<b><i>W</i></b><sub>5</sub> (<b><i>W</i></b><sub>4</sub><b><i>F</i></b><sub>3</sub>+<b><i>B</i></b><sub>4</sub>+<b><i>F</i></b><sub>0</sub>) +<b><i>B</i></b><sub>5</sub>      (8) </p>
                </div>
                <div class="p1">
                    <p id="103">式 (6) 表示初始卷积部分, 其中 (<b><i>I</i></b><sub><i>c</i>2</sub>′, <b><i>I</i></b><sub><i>c</i>2</sub>) 表示将补偿网络的结果<b><i>I</i></b><sub><i>c</i>2</sub>′与原始后一帧<b><i>I</i></b><sub><i>c</i>2</sub>联结起来作为去压缩伪影模块的输入, <b><i>F</i></b><sub>0</sub>表示卷积1的输出, <b><i>W</i></b><sub>0</sub>和<b><i>B</i></b><sub>0</sub>分别表示该卷积层的权重和偏置。式 (7) 表示残差卷积部分, <b><i>F</i></b><sub><i>k</i></sub>表示每个残差块的输出, <b><i>X</i></b>表示每个残差块的输入, <b><i>W</i></b><sub><i>k</i>_<i>m</i></sub>和<b><i>B</i></b><sub><i>k</i>_<i>m</i></sub>分别表示残差卷积模块中对应的各卷积层的权重和偏置, <i>k</i>∈{1, 2, 3}, <i>m</i>∈{1, 2}。式 (8) 表示图像重建部分, <b><i>W</i></b><sub>4</sub>和<b><i>W</i></b><sub>5</sub>, <b><i>B</i></b><sub>4</sub>和<b><i>B</i></b><sub>5</sub>分别表示卷积4, 卷积5的权重和偏置, <b><i>F</i></b><sub>3</sub>表示残差卷积模块的输出, ReLU表示非线性修正单元, <b><i>I</i></b><sup>*</sup><sub><i>c</i>2</sub>表示去除伪影后的视频帧。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表</b>2 <b>去压缩伪影网络各卷积层参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Parameter setting of each convolution layer in compression artifact removal network</p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td><br />卷积层</td><td>卷积核<br />大小</td><td>卷积核<br />数量</td><td>步长</td><td>像素<br />填充</td><td>激活<br />函数</td></tr><tr><td>卷积1</td><td>7×7</td><td>64</td><td>1</td><td>1</td><td>ReLU</td></tr><tr><td><br />卷积1/2/3_1</td><td>3×3</td><td>64</td><td>1</td><td>1</td><td>ReLU</td></tr><tr><td><br />卷积1/2/3_2</td><td>3×3</td><td>64</td><td>1</td><td>1</td><td>—</td></tr><tr><td><br />卷积4</td><td>3×3</td><td>64</td><td>1</td><td>1</td><td>—</td></tr><tr><td><br />卷积5</td><td>3×3</td><td>1</td><td>1</td><td>1</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="105" name="105" class="anchor-tag">2 实验与分析</h3>
                <h4 class="anchor-tag" id="106" name="106">2.1 <b>数据准备和实验设置</b></h4>
                <div class="p1">
                    <p id="107">本文算法实现基于<i>MFQE</i><citation id="247" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的数据库, 该数据库中原始未压缩的训练和测试视频均来源于视频编码联合工作组 (<i>Joint Collaborative Team on Video Coding</i>, <i>JCT</i>-<i>VC</i>) <citation id="248" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>和<i>Xiph</i>.<i>org</i> (<i>https</i>://<i>media</i>.<i>xiph</i>.<i>org</i>/<i>video</i>/<i>derf</i>) 。本实验使用60个视频用于网络的训练, 6个测试视频用于网络测试。训练视频和测试视频均使用<i>FFmpeg</i> (<i>Fast Forward mpeg</i>) 命令并按照<i>HEVC</i>标准进行压缩, 压缩量化系数 (<i>Quantization Parameter</i>, <i>QP</i>) 设置为37。对原始训练视频和对应的压缩视频进行抽帧处理, 得到训练数据集由大约15 000帧连续压缩视频帧组成, 共包含约100 000个训练样本。</p>
                </div>
                <div class="p1">
                    <p id="108">由于<i>HEVC</i>编码宏块大小为4×4、8×8、16×16、32×32, 故而本实验将训练样本分割成64×64的图像块, 保证训练集数据可以囊括不同尺度的伪影信息。本实验硬件环境:<i>Intel Core i</i>7-7700 <i>CPU</i> 3.60 <i>GHz</i>, 主机内存为16 <i>GB</i>, 显卡型号为<i>NVIDIA Titan Xp</i>, 软件环境为<i>Ubuntu</i> 16.04, <i>CUDA</i> 8.0和<i>cuDNN</i> 7.1.4。算法使用<i>python</i>2.7进行编程, 采用<i>Tensorflow</i>深度学习框架实现。在训练网络模型时, 采用<i>Xavier</i><citation id="249" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>作为参数初始化方式, 网络的批处理个数设置为16, 网络训练采用自适应动量估计 (<i>Adaptive Moment Estimation</i>, <i>Adam</i>) <citation id="250" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>优化方式, <i>Adam</i>优化器中各参数采用默认值为<i>β</i><sub>1</sub>=0.9, <i>β</i><sub>2</sub>=0.999, <i>ε</i>=1E-8。设置学习率参数为1E-4优化网络参数。</p>
                </div>
                <div class="p1">
                    <p id="109">本实验所提出的整体去视频压缩伪影网络采用端到端的训练方式。为获取更好的补偿结果和最大程度降低网络负担, 本文首先使用RGB三通道的信息获得运动补偿帧, 随后将得到的运动补偿帧与对应的原始压缩视频帧同时转到Y通道上训练去压缩伪影网络。</p>
                </div>
                <div class="p1">
                    <p id="110">本实验采用Charbonnier函数<citation id="251" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>取代均方误差 (Mean Squared Error, MSE) 作为网络的损失函数, Charbonnier函数通过在MSE计算公式的基础上引入额外的变量<i>ε</i>, 可以更好地保留图像边缘信息以避免图像边缘模糊, 同时可以避免输入带来的不连续性, 有效地帮助网络稳定收敛, 并且已被证明在图像超分辨率重建工作中使用Charbonnier函数可以获得更好的视觉效果<citation id="252" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>。Charbonnier函数计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">θ</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mfrac><mn>1</mn><mi>Ν</mi></mfrac><msqrt><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mrow><mi>c</mi><mn>2</mn></mrow><mo>*</mo></msubsup><mo>;</mo><mi mathvariant="bold-italic">θ</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mrow><mi>c</mi><mn>2</mn></mrow><mi>g</mi></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mspace width="0.25em" /><mo>+</mo><mspace width="0.25em" /><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">其中:<i>F</i> (<b><i>I</i></b><sup>*</sup><sub><i>c</i>2</sub>;<i>θ</i>) 表示网络的输出;<i>θ</i>表示网络中各滤波器的权重和偏置;<b><i>I</i></b><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>c</mi><mn>2</mn></mrow><mi>g</mi></msubsup></mrow></math></mathml>表示压缩视频中当前帧<b><i>I</i></b><sub><i>c</i>2</sub>对应的原始未压缩视频帧;<i>N</i>表示批处理数量;<i>ε</i>是为了保留图像边缘而使用的正则化项, 基于经验设置<i>ε</i>为1E-3。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">2.2 <b>网络结构分析</b></h4>
                <div class="p1">
                    <p id="115">首先, 本文网络中运动补偿模块将光流估计法中像素估计 (<i>estimation</i>) 和形变 (<i>warping</i>) 两个阶段整合成一步完成, 不需要任何关于光流图的信息, 比以上两阶段 (<i>two</i>-<i>stage</i>) 的方法更具鲁棒性, 同时网络的参数量大幅减少, 如表3所示。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表</b>3 <b>本文方法与其他光流估计方法的网络复杂度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Network complexity comparison between</i><i>the proposed method and other optical estimation methods</i></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />网络模型</td><td>网络层数</td><td>参数量/10<sup>6</sup></td></tr><tr><td><br /><i>FlowNet</i>2<sup>[18]</sup></td><td>115</td><td>162.49</td></tr><tr><td><br /><i>EpicFlow</i><sup>[17]</sup></td><td>—</td><td>121.87</td></tr><tr><td><br /><i>FlowFields</i><sup>[16]</sup></td><td>—</td><td>103.12</td></tr><tr><td><br /><i>FlowNet C</i><sup>[15]</sup></td><td>26</td><td>39.16</td></tr><tr><td><br />本文补偿网络</td><td>45</td><td>30.24</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">其次, 对于运动估计中确定像素形变路径 (<i>transmission path</i>) 这一过程, 本文运动补偿网络采用局部卷积核预测的方式实现, 将预测出的卷积核与后一帧进行卷积以得到对于该帧的补偿帧, 不同于光流图估计思想的网格映射<citation id="253" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>算法中严格寻找相邻帧间点对点的像素形变 (<i>pixel</i>-<i>warping</i>) 的做法, 这一操作不局限于固定的变换路径 (<i>transmission path</i>) <citation id="254" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>而是针对视频帧中不同像素点的局部偏移及其结构从而产生不同的卷积核以获取对于每个原始像素点来说最匹配的形变路径, 这可看作是以一种非线性特征映射 (<i>feature</i>-<i>warping</i>) 的形式完成像素形变任务。图2展示了本文运动补偿网络的像素形变结果。图2 (<i>a</i>) 表示压缩后的相邻视频帧的帧间差Δ<i>f</i><sub>1</sub>, 图2 (b) 表示经本文运动补偿网络进行像素形变后的结果<b><i>I</i></b><sub><i>c</i>2</sub>′与<b><i>I</i></b><sub><i>c</i>2</sub>的帧间差Δ<i>f</i><sub>2</sub>, Δ<i>f</i><sub>1</sub>=2.52, Δ<i>f</i><sub>2</sub>=0.30。帧间差越小代表相比较的两帧越相似, 得出本文运动补偿网络结果<b><i>I</i></b><sub><i>c</i>2</sub>′与原始帧<b><i>I</i></b><sub><i>c</i>2</sub>具有较大相似性。同时从图亦可看出, 图 (a) 有明显的边缘轮廓, 这说明视频相邻帧间存在很大的像素偏移量, 而图 (b) 近乎全黑, 这说明本文提出的运动补偿网络较好地对后一帧<b><i>I</i></b><sub><i>c</i>2</sub>进行了像素形变。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905040_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 当前帧和后一帧以及补偿帧和后一帧的帧间差" src="Detail/GetImg?filename=images/JSJY201905040_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 当前帧和后一帧以及补偿帧和后一帧的帧间差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905040_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Inter-frame difference between current frame and next frame, compensated frame and next frame</p>

                </div>
                <div class="p1">
                    <p id="119">再者, 本文采用的网络具有补偿和融合视频帧间缺损像素值这一优势。当相邻两帧存在压缩伪影或者运动模糊时, 第一帧缺失的信息可能在第二帧中出现, 反之第一帧也可能包含第二帧缺失的信息。基于此, 本文运动补偿网络在特征提取相邻视频帧的像素信息的过程中有效地对相邻两帧的各像素进行形变和缺损信息补偿, 去压缩伪影网络融合视频帧像素信息从而恢复出包含更多像素信息的第二帧, 最终提升去视频压缩伪影的效果。</p>
                </div>
                <div class="p1">
                    <p id="120">用Δ<i>f</i><sub><i>c</i></sub>和<i>p</i><sub><i>c</i></sub>分别表示压缩后的后一帧<b><i>I</i></b><sub><i>c</i>2</sub>与其真实值<b><i>I</i></b><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>c</mi><mn>2</mn></mrow><mi>g</mi></msubsup></mrow></math></mathml>的帧间差和对应的峰值信噪比 (Peak Signal-to-Noise Ratio, PSNR) 值, Δ<i>f</i><sub><i>r</i></sub>和<i>p</i><sub><i>r</i></sub>分别表示后一帧的补偿帧<b><i>I</i></b><sub><i>c</i>2</sub>′与真实值<b><i>I</i></b><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>c</mi><mn>2</mn></mrow><mi>g</mi></msubsup></mrow></math></mathml>的帧间差 (frame difference) 和对应的PSNR值。随机选取四个测试集验证运动补偿网络对于帧间像素补偿的作用, 表4中第二列<i>p</i><sub><i>r</i></sub>&gt;<i>p</i><sub><i>c</i></sub>和第三列中Δ<i>f</i><sub><i>r</i></sub>&lt;Δ<i>f</i><sub><i>c</i></sub> 均说明了补偿结果<b><i>I</i></b><sub><i>c</i>2</sub>′与对应的真实值<b><i>I</i></b><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>c</mi><mn>2</mn></mrow><mi>g</mi></msubsup></mrow></math></mathml>较原始压缩帧<b><i>I</i></b><sub><i>c</i>2</sub>具有更高的相似性, 可以看出运动补偿网络给后一帧<b><i>I</i></b><sub><i>c</i>2</sub>带来了更多像素信息, 从而验证了运动补偿网络对缺损像素的补偿作用。</p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表</b>4 <b>在测试序列上补偿帧对其压缩帧的像素补偿情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Pixel compensation between compensated frame and corresponding compressed frame on test sequences</p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td><br />测试序列</td><td>Δ<i>PSNR</i> (<i>p</i><sub><i>r</i></sub>-<i>p</i><sub><i>c</i></sub>) /dB</td><td>Δ<i>f</i> (Δ<i>f</i><sub><i>r</i></sub>-Δ<i>f</i><sub><i>c</i></sub>) </td></tr><tr><td><br />Vidyo1</td><td>0.03</td><td>-0.04</td></tr><tr><td><br />Mad900</td><td>0.04</td><td>-0.02</td></tr><tr><td><br />BasketballPass</td><td>0.02</td><td>-0.03</td></tr><tr><td><br />Kimono</td><td>0.02</td><td>-0.07</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="125">同时本实验对比了仅去压缩伪影网络和运动补偿网络结合去压缩伪影网络的结果。随机选取视频序列vidyo1和vidyo4进行测试, 对比仅去压缩伪影网络和运动补偿网络结合去压缩伪影网络的结果分别如图3 (a) 和 (b) 所示。样例1中, 图 (a) 中仅通过去压缩伪影网络得到的结果像素缺损情况明显, 衣袖部分仍较为模糊;而图 (b) 中经过补偿网络的作用后, 衣袖部分缺损像素得到较好补偿, 得到的去伪影结果较前者更为平滑, 视觉效果有较大程度的提升。同时, 样例2中, 如框中所示, 在人脸鼻翼部分, 仅通过去压缩伪影网络得到的结果仍较为模糊, 伪影去除不够彻底, 存在较为严重的像素缺损情况。而右图中通过补偿网络的作用, 去伪影后的结果包含更丰富的像素信息, 边缘更清晰。不仅如此, 图中人物的帽子边缘、镜框边缘和唇部边缘细节信息更完整, 视觉效果均可见明显的提升。图 (a) 和 (b) 的对比结果说明了运动补偿网络对提升去除视频压缩伪影结果的作用。</p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905040_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 单一去伪影网络和运动补偿去伪影网络结果" src="Detail/GetImg?filename=images/JSJY201905040_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 单一去伪影网络和运动补偿去伪影网络结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905040_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Results of compression artifact removal network without and with motion compensation</p>

                </div>
                <div class="p1">
                    <p id="127">最后, 本文网络中运动补偿模块采用跳跃式连接结构将模型中心两侧的编码结构和解码结构对应连接, 为网络中梯度的流动提供捷径, 去压缩伪影模块采用跳跃式连接的残差网络结构, 这使得网络中所有的卷积层参数能被有效更新, 可以减轻深层网络易出现的梯度消失问题。由图4可以看出本文网络结构均得到稳定收敛。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905040_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 运动补偿网络和去伪影网络收敛情况" src="Detail/GetImg?filename=images/JSJY201905040_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 运动补偿网络和去伪影网络收敛情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905040_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Convergence of motion compensation network and compression artifact removal network</p>

                </div>
                <h4 class="anchor-tag" id="129" name="129">2.3 <b>实验结果</b></h4>
                <div class="p1">
                    <p id="130">表5中展示了本文算法与<i>AR</i>-<i>CNN</i><citation id="255" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、<i>DCAD</i><citation id="256" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、<i>DS</i>-<i>CNN</i><citation id="257" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和 <i>MFQE</i><citation id="258" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法在相同的6个测试序列上得到的<i>PSNR</i>在<i>Y</i>通道上的提升 (<i>Δ</i>PSNR) 结果。其中, <i>AR</i>-<i>CNN</i>是经典的压缩图像增强算法, <i>DCAD</i>、<i>DS</i>-<i>CNN</i>和 <i>MFQE</i>是目前先进的视频质量增强算法。所有测试序列均以QP=37压缩因子压缩, 并且均未参与到网络的训练当中。其中, Δ<i>PSNR</i>可表示视频帧质量增强的程度, 该值越高说明重建效果越好。由表5可见, 在所有的测试序列上, 对于AR-CNN、DCAD、DS-CNN以及MFQE算法, 平均Δ<i>PSNR</i>分别提高了0.15 dB、0.18 dB、0.31 dB和1.41 dB, 而本文算法平均Δ<i>PSNR</i>提高1.73 dB, 故而可看出, 本文所提出的算法在测试序列上表现均优于其他图像/视频质量增强算法。如表中可见, 在所有测试序列上, 经本文算法测试得到的Δ<i>PSNR</i>最大可达到2.56 dB, 同时, 在测试序列Kimono上经本算法测试得到Δ<i>PSNR</i>可超出MFQE<citation id="259" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>算法结果0.44 dB。</p>
                </div>
                <div class="area_img" id="131">
                    <p class="img_tit"><b>表</b>5 <i>QP</i>=37<b>时在不同测试序列上的五种算法</b>Δ<i>PSNR</i><b>比较</b> dB <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Δ<i>PSNR</i> comparison of five algorithms on different test sequences when <i>QP</i>=37 dB</p>
                    <p class="img_note"></p>
                    <table id="131" border="1"><tr><td>测试序列</td><td>AR-CNN</td><td>DCAD</td><td>DS-CNN</td><td>MFQE</td><td>本文算法</td></tr><tr><td><br />Kimono</td><td>0.11</td><td>0.11</td><td>0.24</td><td>1.81</td><td>2.25</td></tr><tr><td><br />MaD</td><td>0.15</td><td>0.20</td><td>0.25</td><td>1.65</td><td>1.79</td></tr><tr><td><br />RaceHorese</td><td>0.16</td><td>0.22</td><td>0.27</td><td>1.29</td><td>1.36</td></tr><tr><td><br />Vidyo1</td><td>0.19</td><td>0.23</td><td>0.33</td><td>1.06</td><td>1.14</td></tr><tr><td><br />Vidyo4</td><td>0.14</td><td>0.18</td><td>0.28</td><td>1.19</td><td>1.25</td></tr><tr><td><br />PeopleOnStreet</td><td>0.13</td><td>0.14</td><td>0.48</td><td>2.45</td><td>2.56</td></tr><tr><td><br />平均Δ<i>PSNR</i>/dB</td><td>0.15</td><td>0.18</td><td>0.31</td><td>1.41</td><td>1.73</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="132">图5展示了本文所提出的网络模型去除视频的压缩伪影之后的结果与上述其他算法对比结果。第2、4、6、8行的图片分别是第1、3、5、7行图片框中的内容放大显示, 通过细节放大以此更好地说明去伪影效果。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905040_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 去视频压缩伪影结果比较" src="Detail/GetImg?filename=images/JSJY201905040_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 去视频压缩伪影结果比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905040_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Comparison of video compression artifact removal results</p>

                </div>
                <div class="p1">
                    <p id="134">由图5可见, 第2列的压缩视频帧对于第1列的原始视频帧存在较大程度的压缩伪影, 而对比第2列的压缩视频帧, 第3～5列的视频帧中伪影现象均得到不同程度的减轻。对于图5中第2行的人脸部分:第3列的AR-CNN算法性能较为局限, 仅可以去除压缩视频帧中严重伪影部分; 对比于前者, 第4列的MFQE算法可以去除压缩帧中大部分的伪影, 但从图中可看到人物眼睛和额头部分仍存在伪影; 而第5列的本文算法可将该部分存在的伪影去除, 获得的结果较MFQE算法更平滑, 整体去伪影效果得到提升。同时, 对于第4行的人手部分: 第4列的MFQE算法伪影去除不彻底, 在手指部分仍存在较多伪影; 第5列的本文算法较其可以去除在人物手指部分存在的伪影, 同时在人物袖口和衣服褶皱处较前者具有更清晰边缘。对于第6行的手指部分: 虽然第4列的MFQE算法结果较AR-CNN算法结果去除了更多的压缩伪影, 但手指边缘仍存在模糊现象; 第5列的本文算法结果较MFQE算法结果在手指边缘和黑色背景边缘均可看到明显增强, 具有更丰富的细节信息, 纹理细节的清晰度有较大提升。对于第8行的字体部分: 第3列的AR-CNN算法虽去除了原始压缩帧的大部分伪影, 但字母边缘仍有较严重伪影存在; 第4列的MFQE算法结果较AR-CNN算法结果稍好, 字母边缘的伪影程度稍有减轻; 而第5列的本文算法解决了字母边缘存在的伪影的问题, 获得了较MFQE算法更好的视觉质量, 更接近第1列的原始未压缩帧。以上结果验证了本文算法可以更好地去除大多数的压缩伪影并保留图像细节, 从而提高视觉质量, 获得更好的重建效果。</p>
                </div>
                <h3 id="135" name="135" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="136">本文提出一种新型多帧去压缩伪影网络结构, 其中:运动补偿模块以自适应可分离卷积方式实现对后一帧像素的运动偏移估计和缺损像素补偿; 去压缩伪影模块通过融合含有不同像素信息量的补偿帧和对应的原始视频帧, 最终得到去视频压缩伪影结果。在本文实验中, 运动补偿网络得到的补偿帧较对应压缩帧的<i>PSNR</i>平均提升了0.03 <i>dB</i>, 与对应未压缩视频帧的帧间差较压缩帧平均减少了0.04 <i>dB</i>, 由此证明了运动补偿网络对缺损像素的补偿作用, 并且, 结合了运动补偿网络后去伪影结果比仅去压缩伪影网络结果在视觉效果上有显著提升。本文中结合了运动补偿网络的去压缩伪影结果较目前先进的<i>AR</i>-<i>CNN</i>、<i>DCAD</i>、<i>DS</i>-<i>CNN</i>和<i>MFQE</i>增强算法结果在相同测试序列上平均Δ<i>PSNR</i>分别提高了1.58 dB, 1.55 dB, 1.42 dB以及0.32 dB, 较MFQE算法在测试序列上最大Δ<i>PSNR</i>提升了0.44 dB, 并且本文网络去伪影后视觉效果较上述算法均有显著提升, 这表明本文所提出的网络具有良好的去除视频压缩伪影的作用。</p>
                </div>
                <div class="p1">
                    <p id="137">在未来工作中, 将展开对网络加速方法的研究, 例如尝试使用深度可分离卷积代替原始二维卷积的策略, 通过调整网络结构, 在保证网络性能的前提下对网络进行加速。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="170">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Compression artifacts reduction by a deep convolutional network &amp;quot;">

                                <b>[1]</b> DONG C, DENG Y, CHEN C L, et al.Compression artifacts reduction by a deep convolutional network[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:576-584.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Building dual-domain representations for compression artifacts reduction">

                                <b>[2]</b> GUO J, CHAO H.Building dual-domain representations for compression artifacts reduction [C]// ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:628-644.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial networks">

                                <b>[3]</b> GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial networks[J/OL].arXiv Preprint, 2014, 2014:arXiv:1406.2661 [2014- 06- 10].https://arxiv.org/abs/1406.2661.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=One-to-many network for visually pleasing compression artifacts reduction">

                                <b>[4]</b> GUO J, CHAO H.One-to-many network for visually pleasing compression artifacts reduction [C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:4867-4876.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep generative adversarial compression artifact removal">

                                <b>[5]</b> GALTERI L, SEIDENARI L, BERTINI M, et al.Deep generative adversarial compression artifact removal [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:4836-4845.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KYYK201802022&amp;v=MjYzODl1WnNGeURuVXJ6UExqVFNaYkc0SDluTXJZOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 杨丽丽, 盛国.一种基于卷积神经网络的矿井视频图像降噪方法[J].矿业研究与开发, 2018, 38 (2) :106-109. (YANG L L, SHENG G.A mine video image denoising method based on convolutional neural network[J].Mining Research and Development, 2018, 38 (2) :106-109.) 
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Video deblurring via semantic segmentation and pixel-wise non-linear kernel">

                                <b>[7]</b> REN W, PAN J, CAO X, et al.Video deblurring via semantic segmentation and pixel-wise non-linear kernel[C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2017:1086-1094.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Frame-recurrent video super-resolution">

                                <b>[8]</b> SAJJADI M S M, VEMULAPALLI R, BROWN M.Frame-recurrent video super-resolution[C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6626-6634.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detail-revealing deep video super-resolution">

                                <b>[9]</b> TAO X, GAO H, LIAO R, et al.Detail-revealing deep video super-resolution [C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6626-6634.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJYD201604001&amp;v=MDkyMDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VcnpQSnlmU2FyRzRIOWZNcTQ5RlpZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 李玲慧, 杜军平, 梁美玉, 等.基于时空特征和神经网络的视频超分辨率算法[J].北京邮电大学学报, 2016, 39 (4) :1-6. (LI L H, DU J P, LIANG M Y, et al.Video super resolution algorithm based on spatiotemporal features and neural networks[J].Journal of Beijing University of Posts and Telecommunications, 2016, 39 (4) :1-6.) 
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A Novel Deep Learning-Based Method of Improving Coding Efficiency from the Decoder-End for HEVC,&amp;quot;">

                                <b>[11]</b> WANG T, CHEN M, CHAO H.A novel deep learning-based method of improving coding efficiency from the decoder-end for HEVC[C]// Proceedings of the 2017 Data Compression Conference.Piscataway, NJ:IEEE, 2017:410-419.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Decoder-side HEVC quality enhancement with scalable convolutional neural network">

                                <b>[12]</b> YANG R, XU M, WANG Z.Decoder-side HEVC quality enhancement with scalable convolutional neural network[C]// Proceedings of the 2017 IEEE International Conference on Multimedia and Expo.Piscataway, NJ:IEEE, 2017:817-822.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhancing quality for HEVC compressed videos">

                                <b>[13]</b> YANG R, XU M, WANG Z, et al.Enhancing quality for HEVC compressed videos [J/OL].arXiv Preprint, 2018, 2018:arXiv:1709.06734 (2017- 09- 20) [2018- 07- 06].https://arxiv.org/abs/1709.06734.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-frame quality enhancement for compressed video">

                                <b>[14]</b> YANG R, XU M, LIU T, et al.Multi-frame quality enhancement for compressed video [C]// Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:6664-6673.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Flownet:Learning optical flow with convolutional networks">

                                <b>[15]</b> DOSOVITSKIY A, FISCHERY P, ILG E, et al.FlowNet:learning optical flow with convolutional networks [C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:2758-2766.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Flow fields: Densecorrespondence fields for highly accurate large displacementoptical flow estimation">

                                <b>[16]</b> BAILER C, TAETZ B, STRICKER D.Flow fields:dense correspondence fields for highly accurate large displacement optical flow estimation[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway, NJ:IEEE, 2015:4015-4023.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EpicFlow:Edge-preserving interpolation of correspondences for optical flow">

                                <b>[17]</b> REVAUD J, WEINZAEPFEL P, HARCHAOUI Z, et al.EpicFlow:edge-preserving interpolation of correspondences for optical flow [C]// Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1164-1172.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FlowNet 2.0 Evolution of Optical Flow Estimation with Deep Networks">

                                <b>[18]</b> ILG E, MAYER N, SAIKIA T, et al.FlowNet2.0:evolution of optical flow estimation with deep networks[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017, 2:6.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007592&amp;v=MDE3OTg0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRndWYVJVPU5pZklZN0s3SHRqTnI0OUZaT3NJQ1hVN29CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> MAHAJAN D, HUANG F C, MATUSIK W, et al.Moving gradients:a path-based method for plausible image interpolation [J].ACM Transactions on Graphics, 2009, 28 (3) :Article No.42.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial transformer networks">

                                <b>[20]</b> JADERBERG M, SIMONYAN K, ZISSERMAN A, et al.Spatial transformer networks[C]// Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2015:2017-2025.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 NIKLAUS S, MAI L, LIU F.Video frame interpolation via adaptive separable convolution [C]// Proceedings of the 2017 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2017:261-270.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[22]</b> HE K, ZHANG X, REN S, et al.Deep residual learning for image recognition [C]// Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">

                                <b>[23]</b> HE K, ZHANG X, REN S, et al.Identity mappings in deep residual networks [C]// ECCV 2016:Proceedings of the 2016 European Conference on Computer Vision.Berlin:Springer, 2016:630-645.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Registry of the International Society for Heart and Lung Transplantation:Thirty-fourth Adult Lung And Heart-Lung Transplantation Report—2017;Focus Theme:Allograft ischemic time">

                                <b>[24]</b> DROZDZAL M, VORONTSOV E, CHARTRAND G, et al.The importance of skip connections in biomedical image segmentation [M]// Deep Learning and Data Labeling for Medical Applications.Berlin:Springer, 2016:179-187.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Common HM test conditions and software reference configurations">

                                <b>[25]</b> BOSSEN F.Common test conditions and software reference configurations [S/OL].[2013- 06- 20].http://wftp3.itu.int/av-arch/jctvc-site/2010_07_B_Geneva/JCTVC-B300.doc.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">

                                <b>[26]</b> GLOROT X, BENGIO Y.Understanding the difficulty of training deep feedforward neural networks [C]// Proceedings of the 13th International Conference on Artificial Intelligence and Statistics.Sardinia, Italy:JMLR, 2010:249-256.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:a method for stochastic optimization">

                                <b>[27]</b> KINGMA D, BA J.Adam:a method for stochastic optimization[EB/OL].[2018- 03- 20].http://yeolab.weebly.com/uploads/2/5/5/0/25509700/a_method_for_stochastic_optimization_.pdf.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A more general robust loss function">

                                <b>[28]</b> BARRON J T.A more general robust loss function[J/OL].arXiv Preprint, 2017, 2017:arXiv:1701.03077 (2017- 01- 11) [2017- 01- 11].https://arxiv.org/abs/1701.03077.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution">

                                <b>[29]</b> LAI W S, HUANG J B, AHUJA N, et al.Deep Laplacian pyramid networks for fast and accurate super-resolution[C]// Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5835-5843.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905040" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905040&amp;v=MzA4OTh6TUx6N0JkN0c0SDlqTXFvOUJaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVXI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
