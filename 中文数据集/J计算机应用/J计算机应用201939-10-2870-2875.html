<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136455972315000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910014%26RESULT%3d1%26SIGN%3d27UmrkRWPKwgbIT95NMFtgSOMHA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910014&amp;v=MTMwODZHNEg5ak5yNDlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFViL0xMejdCZDc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="1 基于深度级联神经网络的运动规划模型 ">1 基于深度级联神经网络的运动规划模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="1.1 CNN&lt;b&gt;层网络设计&lt;/b&gt;">1.1 CNN<b>层网络设计</b></a></li>
                                                <li><a href="#53" data-title="1.2 &lt;i&gt;LSTM&lt;/i&gt;&lt;b&gt;层网络设计&lt;/b&gt;">1.2 <i>LSTM</i><b>层网络设计</b></a></li>
                                                <li><a href="#64" data-title="1.3 &lt;b&gt;网络输出与目标函数设计&lt;/b&gt;">1.3 <b>网络输出与目标函数设计</b></a></li>
                                                <li><a href="#70" data-title="1.4 &lt;b&gt;网络的训练与测试&lt;/b&gt;">1.4 <b>网络的训练与测试</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="2 实验与结果分析 ">2 实验与结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="图1 基于深度级联神经网络的运动规划模型">图1 基于深度级联神经网络的运动规划模型</a></li>
                                                <li><a href="#51" data-title="图2 &lt;i&gt;CNN&lt;/i&gt;层网络结构">图2 <i>CNN</i>层网络结构</a></li>
                                                <li><a href="#63" data-title="图3 LSTM层网络结构">图3 LSTM层网络结构</a></li>
                                                <li><a href="#72" data-title="图4 三个摄像机的图像示例">图4 三个摄像机的图像示例</a></li>
                                                <li><a href="#86" data-title="图5 离线测试实验结果">图5 离线测试实验结果</a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;均方根误差对比结果&lt;/b&gt;"><b>表</b>1 <b>均方根误差对比结果</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;稳定度对比结果&lt;/b&gt;"><b>表</b>2 <b>稳定度对比结果</b></a></li>
                                                <li><a href="#91" data-title="图6 在线测试连续帧截图">图6 在线测试连续帧截图</a></li>
                                                <li><a href="#92" data-title="图7 不同方法在同一场景的对比">图7 不同方法在同一场景的对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="115">


                                    <a id="bibliography_1" title="余翔,王新民,李俨.无人直升机路径规划算法研究[J].计算机应用,2006,26(2):494-495.(YU X,WANG X M,LI Y.Study of a path planning algorithm for unmanned helicopter[J].Journal of Computer Applications,2006,26(2):494-495.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200602077&amp;v=MDI3MjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWIvTEx6N0JkN0c0SHRmTXJZOUNZNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        余翔,王新民,李俨.无人直升机路径规划算法研究[J].计算机应用,2006,26(2):494-495.(YU X,WANG X M,LI Y.Study of a path planning algorithm for unmanned helicopter[J].Journal of Computer Applications,2006,26(2):494-495.)
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_2" title="张超超,房建东.基于定向加权A&lt;sup&gt;*&lt;/sup&gt;算法的自主移动机器人路径规划[J].计算机应用,2017,37(S2):77-81.(ZAHNG CC,FANG J D.Path planning of autonomous mobile robot based on directional weighted A&lt;sup&gt;*&lt;/sup&gt;algorithm[J].Journal of Computer Applications,2017,37(S2):77-81.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2017S2019&amp;v=MDY3MjF6N0JkN0c0SDlhdnJZOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWIvTEw=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        张超超,房建东.基于定向加权A&lt;sup&gt;*&lt;/sup&gt;算法的自主移动机器人路径规划[J].计算机应用,2017,37(S2):77-81.(ZAHNG CC,FANG J D.Path planning of autonomous mobile robot based on directional weighted A&lt;sup&gt;*&lt;/sup&gt;algorithm[J].Journal of Computer Applications,2017,37(S2):77-81.)
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_3" title="BRANDES U.A faster algorithm for betweenness centrality[J].Journal of Mathematical Sociology,2001,25(2):163-177." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD6A10B40C4316EAF6AC6EF4737D5920A3&amp;v=MTQwMzROdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoeGJ5OXdLcz1Oam5CYXJYSkg5RytxNDgyWU9nT0NnbEl1UkJpbVRrSVBudmxyeFZCZkx1V1JjdWNDTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        BRANDES U.A faster algorithm for betweenness centrality[J].Journal of Mathematical Sociology,2001,25(2):163-177.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_4" title="HART P E,NILSSON N J,RAPHAEL B.A formal basis for the heuristic determination of minimum cost paths[J].IEEE Transactions on Systems Science and Cybernetics,1968,4(2):100-107." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A formal basis for the heuristic determination of minimum cost paths">
                                        <b>[4]</b>
                                        HART P E,NILSSON N J,RAPHAEL B.A formal basis for the heuristic determination of minimum cost paths[J].IEEE Transactions on Systems Science and Cybernetics,1968,4(2):100-107.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_5" title="STENTZ A.Optimal and efficient path planning for partially-known environments[C]//Proceedings of the 1994 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,1994:3310-3317." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimal and efficient path planning for partially-known environments">
                                        <b>[5]</b>
                                        STENTZ A.Optimal and efficient path planning for partially-known environments[C]//Proceedings of the 1994 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,1994:3310-3317.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_6" title="KARAMAN S,WALTER M R,PEREZ A,et al.Anytime motion planning using the RRT&lt;sup&gt;*&lt;/sup&gt;[C]//Proceedings of the 2011 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2011:1478-1483." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anytime motion planning using the RRT*">
                                        <b>[6]</b>
                                        KARAMAN S,WALTER M R,PEREZ A,et al.Anytime motion planning using the RRT&lt;sup&gt;*&lt;/sup&gt;[C]//Proceedings of the 2011 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2011:1478-1483.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_7" title="HU X,CHEN L,TANG B,et al.Dynamic path planning for autonomous driving on various roads with avoidance of static and moving obstacles[J].Mechanical Systems and Signal Processing,2018,100:482-500." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7B1CECB560F13E90C4A082649FC6A917&amp;v=MDUzNzZDclRsVEx1WUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhieTl3S3M9TmlmT2ZiVEtINks1M1AxQVl1dDVEWDlNeGhaZzdrNTlRSDNrcUJ0RA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        HU X,CHEN L,TANG B,et al.Dynamic path planning for autonomous driving on various roads with avoidance of static and moving obstacles[J].Mechanical Systems and Signal Processing,2018,100:482-500.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                    LECUN Y L,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_9" title="HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjM2MjBvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJVm9XYWhFPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_10" title="胡学敏,易重辉,陈钦,等.基于运动显著图的人群异常行为检测[J].计算机应用,2018,38(4):1164-1169.(HU XM,YI C H,CHEN Q,et al.Abnormal crowd behavior detection based on motion saliency map[J].Journal of Computer Applications,2018,38(4):1164-1169.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201804042&amp;v=MTA0Mjl1WnNGeW5oVWIvTEx6N0JkN0c0SDluTXE0OUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        胡学敏,易重辉,陈钦,等.基于运动显著图的人群异常行为检测[J].计算机应用,2018,38(4):1164-1169.(HU XM,YI C H,CHEN Q,et al.Abnormal crowd behavior detection based on motion saliency map[J].Journal of Computer Applications,2018,38(4):1164-1169.)
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_11" title="WOJNA Z,GORBAN A N,LEE D,et al.Attention-based extraction of structured information from street view imagery[C]//Proceedings of the 14th IAPR International Conference on Document Analysis and Recognition.Piscataway:IEEE,2017:844-850." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention-Based Extraction of Structured Information from Street View Imagery">
                                        <b>[11]</b>
                                        WOJNA Z,GORBAN A N,LEE D,et al.Attention-based extraction of structured information from street view imagery[C]//Proceedings of the 14th IAPR International Conference on Document Analysis and Recognition.Piscataway:IEEE,2017:844-850.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_12" title="BOJARSKI M,del TESTA D,DWORAKOWSKI D,et al.End to end learning for self-driving cars[EB/OL].(2016-04-25)[2019-02-23].https://arxiv.org/pdf/1604.07316.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End to end learning for self-driving cars">
                                        <b>[12]</b>
                                        BOJARSKI M,del TESTA D,DWORAKOWSKI D,et al.End to end learning for self-driving cars[EB/OL].(2016-04-25)[2019-02-23].https://arxiv.org/pdf/1604.07316.pdf.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_13" title="CHEN C Y,SEFF A,KORNHASUER A,et al.Deep driving:learning affordance for direct perception in autonomous driving[C]//Proceedings of the IEEE 2015 International Conference on Computer Vision.Piscataway:IEEE,2015:2722-2730." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepDriving:Learning Affordance for Direct Perception in Autonomous Driving">
                                        <b>[13]</b>
                                        CHEN C Y,SEFF A,KORNHASUER A,et al.Deep driving:learning affordance for direct perception in autonomous driving[C]//Proceedings of the IEEE 2015 International Conference on Computer Vision.Piscataway:IEEE,2015:2722-2730.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_14" title="SALLAB A E L,ABDOU M,PEROT E,et al.Deep reinforcement learning framework for autonomous driving[EB/OL].[2019-01-10].https://arxiv.org/abs/1704.02532." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning framework for autonomous driving">
                                        <b>[14]</b>
                                        SALLAB A E L,ABDOU M,PEROT E,et al.Deep reinforcement learning framework for autonomous driving[EB/OL].[2019-01-10].https://arxiv.org/abs/1704.02532.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_15" title="SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-01-20].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[15]</b>
                                        SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-01-20].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_16" title="JIA Y,SHELHAMER E,DONAHUE J,et al.Caffe:convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM Conference on Multimedia.New York:ACM,2014:675-678." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional architecture for fast feature embedding">
                                        <b>[16]</b>
                                        JIA Y,SHELHAMER E,DONAHUE J,et al.Caffe:convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM Conference on Multimedia.New York:ACM,2014:675-678.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_17" title="THORPE S,FIZE D,MARLOT C.Speed of processing in the human visual system[J].Nature,1996,381(6582):520-522." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speed of processing in the human visual system">
                                        <b>[17]</b>
                                        THORPE S,FIZE D,MARLOT C.Speed of processing in the human visual system[J].Nature,1996,381(6582):520-522.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-19 09:11</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2870-2875 DOI:10.11772/j.issn.1001-9081.2019040629            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度级联神经网络的自动驾驶运动规划模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%99%BD%E4%B8%BD%E8%B4%87&amp;code=38783008&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">白丽贇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E5%AD%A6%E6%95%8F&amp;code=31682914&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡学敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%8B%E6%98%87&amp;code=40000732&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宋昇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%AB%A5%E7%A7%80%E8%BF%9F&amp;code=40769300&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">童秀迟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E8%8B%A5%E6%99%97&amp;code=42897025&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张若晗</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B9%96%E5%8C%97%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0234810&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">湖北大学计算机与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对基于规则的运动规划算法需要预先定义规则和基于深度学习的方法没有利用时间特征的问题,提出一种基于深度级联神经网络的运动规划模型。该模型将卷积神经网络(CNN)和长短期记忆网络(LSTM)这两种经典的深度学习模型进行融合并构成一种新的级联神经网络,分别提取输入图像的空间和时间特征,并用以拟合输入序列图像与输出运动参数之间的非线性关系,从而完成从输入序列图像到运动参数的端到端的规划。实验利用模拟驾驶环境的数据进行训练和测试,结果显示所提模型在乡村路、高速路、隧道和山路四种道路中均方根误差(RMSE)不超过0.017,且预测结果的稳定度优于未使用级联网络的算法一个数量级。结果表明,所提模型能有效地学习人类的驾驶行为,并且能够克服累积误差的影响,适应多种不同场景下的路况,具有较好的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自动驾驶;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">运动规划;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E7%BA%A7%E8%81%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度级联神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短期记忆模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    白丽贇(1995—),女,陕西汉中人,硕士研究生,主要研究方向:深度神经网络;;
                                </span>
                                <span>
                                    *胡学敏(1985—),男,湖南岳阳人,副教授,博士,主要研究方向:机器学习、运动规划;电子邮箱huxuemin2012@hubu.edu.cn;
                                </span>
                                <span>
                                    宋昇(1997—),男,河南郑州人,主要研究方向:运动规划;;
                                </span>
                                <span>
                                    童秀迟(1996—),女,湖北随州人,硕士研究生,主要研究方向:机器学习;;
                                </span>
                                <span>
                                    张若晗(1997—),女,湖北襄阳人,硕士研究生,主要研究方向:深度学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61806076);</span>
                                <span>湖北省自然科学基金资助项目(2018CFB158);</span>
                                <span>湖北省大学生创新创业训练计划项目(201810512055);</span>
                    </p>
            </div>
                    <h1><b>Motion planning model based on deep cascaded neural network for autonomous driving</b></h1>
                    <h2>
                    <span>BAI Liyun</span>
                    <span>HU Xuemin</span>
                    <span>SONG Sheng</span>
                    <span>TONG Xiuchi</span>
                    <span>ZHANG Ruohan</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Information Engineering, Hubei University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the problems that rule-based motion planning algorithms under constraints need pre-definition of rules and temporal features are not considered in deep learning-based methods, a motion planning model based on deep cascading neural networks was proposed. In this model, the two classical deep learning models, Convolutional Neural Network(CNN) and Long Short-Term Memory(LSTM) network, were combined to build a novel cascaded neural network, the spatial and temporal features of the input images were extracted respectively, and the nonlinear relationship between the input sequential images and the output motion parameters were fit to achieve the end-to-end planning from the input sequential images to the output motion parameters. In experiments, the data of simulated environment were used for training and testing. Results show that the Root Mean Squared Error(RMSE) of the proposed model in four scenes including country road, freeway, tunnel and mountain road is less than 0.017, and the stability of the prediction results of the proposed model is better than that of the algorithm without using cascading neural network by an order of magnitude. Experimental results show that the proposed model can effectively learn human driving behaviors, eliminate the effect of cumulative errors and adapt to different scenes of a variety of road conditions with good robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=autonomous%20driving&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">autonomous driving;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=motion%20planning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">motion planning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20cascaded%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep cascaded neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Long%20Short-Term%20Memory(LSTM)%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Long Short-Term Memory(LSTM) model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    BAI Liyun,born in 1995,M.S.candidate.Her research interests include deep neural network.;
                                </span>
                                <span>
                                    HU Xuemin,born in 1985,Ph.D.,associate professor.His research interests include machine learning,motion planning.;
                                </span>
                                <span>
                                    SONG Sheng,born in 1997.His research interests include motion planning.;
                                </span>
                                <span>
                                    TONG Xiuchi,born in 1996,M.S.candidate.Her research interests include machine learning.;
                                </span>
                                <span>
                                    ZHANG Ruohan,born in 1997,M.S.candidate.Her research interests include deep learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61806076);</span>
                                <span>the Hubei Provincial Natural Science Foundation of China(2018CFB158);</span>
                                <span>the Undergraduate Innovation Training Project of Hubei Province(201810512055);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="38">随着社会经济的飞速发展,机动车辆的大量增长给交通环境带来了巨大的压力,造成安全事故频发。自动驾驶技术能够突破驾驶员的限制,为解决驾驶的安全、交通拥挤等问题提供契机。运动规划作为自动驾驶的核心环节,是连接车辆的环境感知与操纵控制的基础和桥梁。其目的是在考虑当前状态、感知数据以及交通规则等多种约束条件下,为自动驾驶车辆提供安全到达目的地的运动参数或可行路径,其技术除了可用于无人车以外,还可用于无人机<citation id="149" type="reference"><link href="115" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、移动机器人<citation id="150" type="reference"><link href="117" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等自主无人系统,因此具有重要的研究意义和商业价值。</p>
                </div>
                <div class="p1">
                    <p id="39">目前,自动驾驶领域应用较为广泛的传统运动规划算法包括启发式搜索算法<citation id="153" type="reference"><link href="119" rel="bibliography" /><link href="121" rel="bibliography" /><link href="123" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>、快速搜索随机树算法<citation id="151" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和基于离散优化算法<citation id="152" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。基于规则的传统运动规划算法在无人车和智能机器人等领域取得了成功的应用。然而,这类方法需要根据预先定义的规则来建立相关数学模型,在规划之内的场景能够取得较好的规划效果,而对于规则之外的场景却难以适应。此外,这类算法不能直接对感知的数据进行处理,而需要对数据进行预处理,并抽象出模型可以接受的环境表达,而这些预处理的过程相当耗费时间,导致系统规划反应时间过长,尤其是在紧急情况下存在较高的安全隐患。</p>
                </div>
                <div class="p1">
                    <p id="40">近年来,深度学习的发展使得机器学习有了革命性的突破,其中应用较为广泛的两个模型是卷积神经网络(Convolutional Neural Network, CNN)<citation id="154" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和长短期记忆(Long Short-Term Memory, LSTM)网络<citation id="155" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。CNN能有效提取输入图像的空间特征<citation id="156" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,LSTM可以提取连续输入图像的时间特征<citation id="157" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。而运动规划的本质,是从感知的序列数据到运动参数的映射。因此,将深度神经网络引入运动规划领域,能够实现从感知数据到运动参数的规划。目前已有一些基于深度神经网络的方法取得了较好的成果。NVIDIA公司<citation id="158" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了一种基于CNN的端对端的运动规划算法,将道路线检测、路径规划和控制等子步骤通过CNN同时完成。该方法能有效地将驾驶图像特征映射为方向盘转角,但是没有考虑前后帧的时间特征。Chen等<citation id="159" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>在深度神经网络的基础上提出了13个可用于自动驾驶的场景描述指标,通过精确地学习这些指标的值后,可完成转向角的控制。该方法在没有车辆的道路的数据集中取得良好表现,但同样没有利用连续帧的信息,在路况复杂的情况下驾驶行为不稳定。Sallab等<citation id="160" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>使用深度强化学习提出了一种自动驾驶框架,它包含了用于信息集成的递归神经网络,使汽车能够处理部分可观察的场景。然而该方法需要通过在环境中试错来训练模型,训练时间长,且难以将模型迁移到实际环境中。此外,这些方法只利用单个前向摄像机获取驾驶信息,没有考虑周围的信息,容易造成累积误差的问题。</p>
                </div>
                <div class="p1">
                    <p id="41">针对基于传统的运动规划算法存在的需要预先定义规则、预处理复杂,以及基于深度学习方法中没有利用连续帧之间的时间特征、模型训练时间长、没考虑累积误差等问题,本文提出一种基于深度级联神经网络(Deep CNN, DCNN)的自动驾驶运动规划模型。该模型利用CNN和LSTM构成深度级联神经网络,分别提取驾驶场景序列图像中的空间和时间特征,并与车辆的方向盘转角建立映射关系,从而实现直接从感知图像到运动参数的端到端的运动规划;并且,为了解决累计误差的问题,训练时使用左、中、右三个摄像机采集的前向视频构成数据集,解决在线测试时累计误差的自动修正问题。该模型采用深度学习的方法解决自动驾驶运动规划的问题,让规划算法具备学习能力,能够应对多种复杂的道路场景,并且不需要预处理过程,实现端到端的运动规划。本文方法既解决了自动驾驶中复杂道路场景的时空特征表达问题,也为端到端的运动规划提供新的方法。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag">1 基于深度级联神经网络的运动规划模型</h3>
                <div class="p1">
                    <p id="43">本文提出的基于DCNN的运动规划模型如图1所示,模型输入为前向车载相机的序列图像,经过网络后输出为当前预测的方向盘转角。深度级联神经网络由CNN层和LSTM层组成。其中,CNN层对每一帧图像提取空间特征,然后输入到LSTM层提取连续帧图像的时间特征,最后模型输出得到方向盘转向角的预测结果。该过程可用式(1)来描述:</p>
                </div>
                <div class="p1">
                    <p id="44"><i>p</i>=<i>M</i>(<i><b>W</b></i><sup>*</sup>,<i><b>s</b></i>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="45">其中: <i>M</i>为深度级联神经网络模型;<i><b>W</b></i><sup>*</sup>表示网络权值;<i><b>s</b></i>表示输入的连续图像; <i>p</i>表示预测的方向盘转角。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910014_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于深度级联神经网络的运动规划模型" src="Detail/GetImg?filename=images/JSJY201910014_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于深度级联神经网络的运动规划模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910014_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Motion planning model based on deep cascading neural network</p>

                </div>
                <div class="p1">
                    <p id="47">在训练阶段,将左、中、右三个摄像机采集的序列图像数据集作为输入,模型预测输出方向盘转角。此外,利用输入图像对应的真实方向盘转向角,即数据标签,与预测的转向角之间的误差计算损失函数,经过反向传播算法对CNN和LSTM网络中的权值进行更新。为了消除数据采集中的误差影响,本文对人工采集的真实方向盘转角进行低通滤波处理。在测试阶段,仅将中心摄像机采集的视频作为输入,用训练好的模型预测当前输入下的方向盘转角,实现从输入图像到运动参数端到端的运动规划。</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48">1.1 CNN<b>层网络设计</b></h4>
                <div class="p1">
                    <p id="49">近年来,<i>CNN</i>被广泛应用于大规模的图像识别任务中。由于其使用了局部连接和权值共享的方式,在处理二维图像时,特别是在识别位移、缩放以及其他形式的扭曲不变性应用上有着良好的鲁棒性。自动驾驶场景复杂,车载相机获取的图像中目标种类较多,有效提取这些图像特征是一个较为困难的任务。</p>
                </div>
                <div class="p1">
                    <p id="50"><i>VGG</i>-<i>Net</i>(<i>Visual Geometry Group Net</i>)<citation id="161" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>是牛津大学计算机视觉组和<i>Google DeepMind</i>公司的研究员在2014年提出的一种<i>CNN</i>,采用<i>ImageNet</i>数据集进行训练,并广泛用于目标检测等图像识别领域。而自动驾驶场景中建筑物、车辆、行人等目标已经包含在该数据集中,因此本文选择<i>VGG</i>-<i>Net</i>作为<i>CNN</i>的网络结构基础,依据级联网络的设计进行改进,并利用驾驶场景数据集对网络权值进行微调,以此减少训练时间。此外,考虑到运动规划对实时性的要求,本文实验中采用<i>VGG</i>-16作为本文<i>CNN</i>层的基础模型,并进行改进。改进的<i>VGG</i>网络结构如图2所示,将224×224的三通道图像作为输入,在经过5个卷积层与5个池化层之后,通过三个卷积层输出得到1×1×4 096的特征矢量。其中,所有的卷积层使用3×3的卷积核,同时使用修正线性单元(<i>Rectified Linear Unit</i>, <i>ReLU</i>)作为激活函数。前5个卷积层的滑动步长为1个像素,卷积层的空间填充固定为1个像素,用来保持卷积后图像的长宽尺寸与卷积前一致。池化层采用尺寸为2×2最大池化方法,步长为2。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910014_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CNN层网络结构" src="Detail/GetImg?filename=images/JSJY201910014_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>CNN</i>层网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910014_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.2 <i>Architecture of CNN layer</i></p>

                </div>
                <div class="p1">
                    <p id="52"><i>CNN</i>中,卷积层和池化层一般用于提取图像特征,而全连接层一般作为分类器用于对目标进行分类。由于本文提出的深度级联网络中,<i>CNN</i>的目的是提取驾驶场景图像的空间特征,不需要进行分类,因此本文去掉了原始<i>VGG</i>-16中的最后3个全连接层,用3个3×3、滑动步长为2的卷积层代替原始<i>VGG</i>-16网络中的全连接层。相对于较大的卷积核,较小卷积核需要训练总的参数数量更少,更有利于训练时的收敛,减少训练时程序占用的计算资源。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53">1.2 <i>LSTM</i><b>层网络设计</b></h4>
                <div class="p1">
                    <p id="54"><i>LSTM</i>是一种经典的循环神经网络,可以学习长期依赖信息。由于其加入了门控和<i>LSTM</i>细胞状态等机制,其网络的权重可随时间尺度动态地改变,因此可以提取长期时间的序列特征。图3中虚线矩形框中<i>LSTM</i>单元描述了<i>LSTM</i>单元的内部结构,其中,<i>r</i>表示连续<i>N</i>帧图像中的任意一帧。LSTM单元中各个门的工作原理如式(2)～(7)所示:</p>
                </div>
                <div class="p1">
                    <p id="55"><i><b>f</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>=Sigmoid(<i><b>W</b></i><sub><i><b>xf</b></i></sub><i><b>x</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>+<i><b>W</b></i><sub><i><b>hf</b></i></sub><i><b>h</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub><sub>-1</sub>+<i><b>b</b></i><sub><i><b>f</b></i></sub>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="56"><i><b>i</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>=Sigmoid(<i><b>W</b></i><sub><i><b>xi</b></i></sub><i><b>x</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>+<i><b>W</b></i><sub><i><b>hi</b></i></sub><i><b>h</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub><sub>-1</sub>+<i><b>b</b></i><sub><i><b>i</b></i></sub>)      (3)</p>
                </div>
                <div class="p1">
                    <p id="57"><i><b>o</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>=Sigmoid(<i><b>W</b></i><sub><i><b>xo</b></i></sub><i><b>x</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>+<i><b>W</b></i><sub><i><b>ho</b></i></sub><i><b>h</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub><sub>-1</sub>+<i><b>b</b></i><sub><i><b>o</b></i></sub>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="58"><i><b>c</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>= <i><b>f</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>⊙<i><b>c</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub><sub>-1</sub>+<i><b>i</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>⊙<i><b>m</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>      (5)</p>
                </div>
                <div class="p1">
                    <p id="59"><i><b>m</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>=tanh(<i><b>W</b></i><sub><i><b>xm</b></i></sub><i><b>x</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>+<i><b>W</b></i><sub><i><b>hm</b></i></sub><i><b>h</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub><sub>-1</sub>+<i><b>b</b></i><sub><i><b>m</b></i></sub>)      (6)</p>
                </div>
                <div class="p1">
                    <p id="60"><i><b>h</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>=<i><b>o</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>⊙tanh(<i><b>c</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>)      (7)</p>
                </div>
                <div class="p1">
                    <p id="61">其中:<i><b>W</b></i><sub><i><b>xi</b></i></sub>与<i><b>b</b></i>分别表示对应门控单元的权值与偏差;<i><b>x</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>表示LSTM单元的输入;<i><b>h</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>与<i><b>h</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub><sub>-1</sub>分别表示当前LSTM单元的输出与上一个单元的输出;<i><b>c</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>与 <i><b>c</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub><sub>-1</sub>分别表示当前细胞状态与上一个单元的细胞状态; <i><b>f</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>表示遗忘门;<i><b>i</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>表示输入门;<i><b>o</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>表示输出门;<i><b>m</b></i><sub><i>t</i></sub><sub>-</sub><sub><i>r</i></sub>表示输入调制门;“⊙”为点乘。在LSTM单元中设置了四个控制门,每一个控制门都是由多层感知机与激活函数构成的。在LSTM中,首先由遗忘门读取<i><b>x</b></i><sub><i>r</i></sub>和<i><b>h</b></i><sub><i>r</i></sub><sub>-1</sub>,在决定丢弃的信息后输出<i><b>f</b></i><sub><i>r</i></sub>。下一步决定让多少信息加入到细胞状态,这个过程分为sigmoid层决定需要更新的信息<i><b>i</b></i><sub><i>r</i></sub>和一个tanh层生成备选的用来更新的内容<i><b>m</b></i><sub><i>r</i></sub>两部分组成,这两部分联合起来对细胞状态进行更新。在更新细胞状态时,把<i><b>f</b></i><sub><i>r</i></sub>与旧状态点乘后加上<i><b>i</b></i><sub><i>r</i></sub>⊙<i><b>m</b></i><sub><i>r</i></sub>完成细胞状态更新。最后确定输出信息,由一个sigmoid层来确定细胞状态的输出部分,并把细胞状态通过tanh进行处理并将它和sigmoid门的输出相乘,得到最终输出的信息。由于运动规划的输入视频图像在时间上是相关的,因此LSTM适合于提取视频中时间前后帧的关联信息。</p>
                </div>
                <div class="p1">
                    <p id="62">本文中LSTM网络设计如图3所示。对于每一帧图像,CNN网络输出一个特征向量<i><b>x</b></i>,对于连续<i>n</i>帧图像输出<i>n</i>个特征矢量。本文设计的CNN层的输出为1×1×4 096的向量,在这里作为空间特征向量输入到LSTM单元中,经过LSTM层和全连接层后输出转向角的预测。对于当前时刻<i>t</i>,其输出的值由特征向量<i><b>x</b></i><sub><i>t</i></sub>与上一个LSTM单元的输出 <i><b>h</b></i><sub><i>t</i></sub><sub>-1</sub>和状态<i><b>c</b></i><sub><i>t</i></sub><sub>-1</sub>决定,在经过全连接层与输出节点,即得到当前方向盘转角的预测值。在LSTM网络中,LSTM单元中的权值是共享的,即对应于不同时刻的驾驶图像,其对应的LSTM单元中的四个控制门的权值是一样的。在如图3所示的网络结构中,LSTM单元是同一个LSTM单元在复用,在上一时刻LSTM单元的输出值与细胞状态会传入下一时刻的单元中。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910014_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 LSTM层网络结构" src="Detail/GetImg?filename=images/JSJY201910014_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 LSTM层网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910014_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Architecture of LSTM layer</p>

                </div>
                <h4 class="anchor-tag" id="64" name="64">1.3 <b>网络输出与目标函数设计</b></h4>
                <div class="p1">
                    <p id="65">车辆的方向盘转向角度是一种连续的变量,因此转向角的预测问题可以看作是神经网络的回归问题。由于本文的预测的运动参数只有转向角,所以设计的深度级联神经网络的输出节点数量为1。此外,为设计目标函数训练深度级联神经网络,本文采用转向角的预测值与真实值之间的欧氏距离作为损失函数,如式(8)所示:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>L</i>(<i>p</i><sub><i>g</i></sub>,<i>M</i>(<i><b>s</b></i>,<i><b>W</b></i>))=‖<i>p</i><sub><i>g</i></sub>,<i>M</i>(<i><b>s</b></i>,<i><b>W</b></i>)‖<sub>2</sub>      (8)</p>
                </div>
                <div class="p1">
                    <p id="67">其中:<i>L</i>表示损失函数; <i>p</i><sub><i>g</i></sub>表示对应的方向盘转角真实值,由人工采集获取;<i><b>s</b></i>为模型的输入图像;<i>M</i>为神经网络模型。为防止网络训练的过拟合问题,本文采用L2正则化的方法。因此,本文设计的目标函数更新方法如式(9)所示:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mo>*</mo></msup><mo>←</mo><mrow><mi>min</mi></mrow><mspace width="0.25em" /><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><mo>∑</mo><mi>L</mi></mstyle><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub><mo>,</mo><mi>Μ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">s</mi><mo>,</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><mi>J</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中:<i><b>W</b></i><sup>*</sup>为待优化的目标网络权值矢量;<i>n</i>为历史数据长度;<i>J</i>(<i>f</i>)表示L2正则化;<i>λ</i>为正则化系数。本文中<i>n</i>=10,<i>λ</i>=0.005,为经验值。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">1.4 <b>网络的训练与测试</b></h4>
                <div class="p1">
                    <p id="71">本文提出的基于<i>DCNN</i>的运动规划方法,利用事先采集的训练样本进行模型训练,然后利用训练好的模型对测试样本进行离线测试。但是由于离线测试中存在累积误差的问题,如果不加以修正,难以直接用于在线测试。为了解决该问题,在采集训练数据时设置了3个摄像机,分别是左摄像机、中心摄像机和右摄像机。中心摄像机的主光轴与车身竖直方向平行,左、右两个摄像机的主光轴设置与中心摄像机有一定的夹角(本文中夹角设置均为25°)。三个摄像机采集的图像如图4所示。左、右两个摄像机能够显示车辆从当前车道中心的不同位移,以及与道路方向的偏航角度。从左、右摄像机分别获得两种不同位移的图像,通过对最近的摄像机图像进行视点变换,模拟摄像机与各偏航角之间的附加位移,并将转换后的图像样本的转向角标签作相应的角度调整,作为补充训练数据集。左、右摄像机与中心摄像机采集的样本数据及标签一起构成训练数据集。通过补充数据集中样本的训练,自动驾驶车辆能够在航向角偏离正常航线时及时自动修正方向,不让误差形成累积效应。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910014_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 三个摄像机的图像示例" src="Detail/GetImg?filename=images/JSJY201910014_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 三个摄像机的图像示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910014_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.4 <i>Examples of images from three cameras</i></p>

                </div>
                <div class="p1">
                    <p id="73">在训练阶段,使用包含多种场景的驾驶数据的训练集对网络进行训练,使用反向传播算法在每一次迭代中更新网络的权值。本文使用随机梯度下降算法来计算每一次更新的权值。由于训练集样本较多,随着迭代次数的增加,网络参数逐渐向最优参数逼近。本文中,迭代总次数设置为200 000,初始学习率为0.001 5,<i>batchsize</i>的大小为4。</p>
                </div>
                <div class="p1">
                    <p id="74">本文测试分为离线测试和在线测试。离线测试中,测试数据采用未经训练的路段驾驶视频流输入到网络中,记录每帧对应的输出,并与真实数据作对比,从对比的结果中判断网络输出的结果是否能作为准确的规划结果;在线测试主要是验证累积误差的修正问题,让算法在模拟器中运行,验证自动驾驶车辆是否能安全行驶。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">2 实验与结果分析</h3>
                <div class="p1">
                    <p id="76">为保证测试的安全性,本文实验在模拟器的环境下开展。欧洲卡车模拟器2是目前经典的一种驾驶模拟器,其逼真的模拟场景、大范围的地图和多路况和天气的模拟,很适合作为自动驾驶的模拟器。本文使用<i>Europilot</i>框架进行数据采集,通过模拟方向盘、油门和刹车踏板采集人工驾驶的方向盘转角数据作为训练样本的标注信息。</p>
                </div>
                <div class="p1">
                    <p id="77">在实验中,人类操作该模拟器以30 帧/秒的帧率进行三个摄像机的图像采集,每帧图像的像素尺寸为1 853×1 012,输入网络时将图像缩小为224×224。为保证驾驶场景的多样性,实验采集了约8 <i>h</i>的驾驶数据,包括4种不同的道路,分别是乡村路、高速路、隧道和山路。由于本文方法没有考虑交通标志信息,因此没有将城市道路作为实验路段。实验中每种道路取一段路作为测试集,其他数据均作为训练集,使用训练集中的数据对本文提出的<i>DCNN</i>模型进行训练,再使用测试集的数据对网络的输出进行测试。由于模仿学习是学习人类的驾驶行为,因此评判模型预测的准确性就以人类专家的驾驶数据作为标准。将测试结果与人类驾驶的数据进行对比,采用均方根误差(<i>Root Mean Squared Error</i>, <i>RMSE</i>)来衡量模型的性能,公式如下所示:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>-</mo><mi>p</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">其中: <i>p</i><sub><i>g</i></sub>(<i>t</i>)和<i>p</i>(<i>t</i>)分别为时刻<i>t</i>方向盘转角的真实值和预测值;<i>N</i>为每个场景的总帧数。所有的转向角均为归一化后的结果。</p>
                </div>
                <div class="p1">
                    <p id="80">稳定性也是衡量自动驾驶模型好坏的一个重要的指标,驾驶的稳定性影响着舒适度和安全性。由于均方误差的大小可以是从零到无穷大,只是针对每帧预测准确性作评估,无法衡量驾驶的平稳性,因此本文使用式(11)作为驾驶稳定性的衡量标准:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>Τ</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>(</mo><mrow><mrow><mrow><mfrac><mrow><mo>∂</mo><mi>p</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>t</mi></mrow></mfrac></mrow><mo>|</mo></mrow><msub><mrow></mrow><mrow><mi>t</mi><mo>=</mo><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中:<i>ST</i>为稳定度指标。对于弧度不同的弯道,稳定度指标有着较大的变换范围,因此该指标只能衡量同一场景下不同模型的稳定度,而不能对不同场景下的情况作出评判。</p>
                </div>
                <div class="p1">
                    <p id="83">本文实验的软件环境为Ubuntu 16.04,深度神经网络框架采用Caffe<citation id="162" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>,硬件环境CPU为Core i7-7700K (Quad-core 4.2 GHz)、GPU为 NVIDA GTX 1080Ti、内存为32 GB。实验同时测试了未使用LSTM的原始VGG-16和NVIDIA训练的神经网络<citation id="163" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>作为对比。实验的结果如图5～7,以及表1～2所示。根据实验结果,可得到如下结论:</p>
                </div>
                <div class="p1">
                    <p id="84">1)深度级联网络模型中的空间特征提取层基于改进的VGG设计,能够有效提取不同复杂场景的图像特征,因此对于不同的场景能够作出准确的预测。从图5中可以观察DCNN的预测曲线与真实曲线比较相近,4个场景的RMSE均不超过0.017,大约为转向角输出值范围的1%。图7为同一时刻三种方法与真实数据的对比,可以看出相对于原始的VGG网络与文献<citation id="164" type="reference">[<a class="sup">12</a>]</citation>方法所采用的网络,由于使用了LSTM,改进的网络预测更为准确。从表1可知在四个场景中原始的VGG和文献<citation id="165" type="reference">[<a class="sup">12</a>]</citation>方法的均方根误差均高于DCNN方法,而且4种场景的均方根误差都在同一水平。</p>
                </div>
                <div class="p1">
                    <p id="85">2)本文方法预测的转向角具有较好的稳定性。在4个场景中,预测转向的稳定度与真实转向的稳定度相差不大,而且远好于原始的VGG方法与文献<citation id="166" type="reference">[<a class="sup">12</a>]</citation>方法。与人类专家的驾驶稳定度相比,DCNN的稳定度与人类专家相近,而原始的VGG和文献<citation id="167" type="reference">[<a class="sup">12</a>]</citation>方法的稳定度指标高于人类专家一个数量级,相差比较大。这是因为人工采集的数据由于手的抖动和输入设备等问题,存在一些噪声。而本文方法中利用了滤波器对输入数据进行平滑,且增加了LSTM层,使得网络能够将时间上前后相邻的几帧图像联系在一起,输出更加平滑的预测值。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910014_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 离线测试实验结果" src="Detail/GetImg?filename=images/JSJY201910014_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 离线测试实验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910014_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Experiment results of offline test</p>

                </div>
                <div class="p1">
                    <p id="87">3)本文方法能有效地修正行驶中的累积误差。本文在训练网络时除了前向中心摄像机的图像,还增加了左、右两个摄像机的采集的图像作为训练样本来训练模型,使模型能够在偏离正确航向时修正方向,不让误差累积起来。图6为一段连续在线测试的图像。从图6的右后视镜中可以看出,第1帧中车辆有稍微向右偏离当前车道,但是模型能够自动修正误差,从第8帧开始回到当前车道中央。</p>
                </div>
                <div class="p1">
                    <p id="88">4)本文设计的深度级联神经网络算法采用了GPU加速,因此在处理速度上相对于传统的单线程算法具有一定速度上的优势。视频流从输入神经网络到输出预测值大约需要0.05 s,即每秒钟约20次规划。而人类的反应时间大约为0.2 s<citation id="168" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,要远大于本文方法预测的时间,因此本文方法能够满足在自动驾驶中运动规划的实时性要求。</p>
                </div>
                <div class="area_img" id="89">
                    <p class="img_tit"><b>表</b>1 <b>均方根误差对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.1 Comparative results of RMSE</p>
                    <p class="img_note"></p>
                    <table id="89" border="1"><tr><td><br />场景</td><td>文献[12]方法/10<sup>-3</sup></td><td>VGG/10<sup>-3</sup></td><td>DCNN/10<sup>-3</sup></td></tr><tr><td><br /> 乡村路</td><td>12.66</td><td>12.32</td><td>9.55</td></tr><tr><td><br /> 高速路</td><td>16.05</td><td>14.76</td><td>14.68</td></tr><tr><td><br /> 隧道</td><td>6.61</td><td>6.11</td><td>5.36</td></tr><tr><td><br /> 山路</td><td>20.41</td><td>18.27</td><td>16.48</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="90">
                    <p class="img_tit"><b>表</b>2 <b>稳定度对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.2 Comparative results of stability</p>
                    <p class="img_note"></p>
                    <table id="90" border="1"><tr><td><br />场景</td><td>真实值/10<sup>-7</sup></td><td>文献[12]方法/10<sup>-7</sup></td><td>VGG/10<sup>-7</sup></td><td>DCNN/10<sup>-7</sup></td></tr><tr><td>乡村路</td><td>15.49</td><td>193.44</td><td>188.52</td><td>9.36</td></tr><tr><td><br />高速路</td><td>10.31</td><td>287.38</td><td>357.05</td><td>19.36</td></tr><tr><td><br />隧道</td><td>3.20</td><td>504.55</td><td>47.29</td><td>2.29</td></tr><tr><td><br />山路</td><td>15.31</td><td>882.41</td><td>942.01</td><td>38.96</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910014_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 在线测试连续帧截图" src="Detail/GetImg?filename=images/JSJY201910014_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 在线测试连续帧截图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910014_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Successive snapshots of online test</p>

                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910014_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同方法在同一场景的对比" src="Detail/GetImg?filename=images/JSJY201910014_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同方法在同一场景的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910014_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Comparative images of different methods in same scene</p>

                </div>
                <h3 id="93" name="93" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="94">本文提出了一种深度级联神经网络,并利用该网络实现从输入图像到运动参数的端到端的自动驾驶运动规划。该方法中,利用前向车载相机的序列图像作为输入,使用提出的深度级联神经网络对自动驾驶的运动参数做回归,实现对方向盘转向角的预测。深度级联网络融合了<i>CNN</i>和<i>LSTM</i>两种经典的深度模型,使模型不仅能够提取驾驶场景的空间特征,还提取了连续图像的时间特征,实现对输出结果的准确预测;并且,训练时额外利用了左、右两个摄像机采集的数据作为补充数据集,解决了在线测试时的累计误差修正问题。实验结果表明,通过大量数据的训练,该网络能够有效、实时地针对不同场景的驾驶转向角作出预测,能够适应复杂的动态场景。由于该方法没有考虑交通信息以及全局路径信息,只能从网络的输入得到规划的结果,因此无法应用于城市道路。未来的工作将集中在如何将全局路径信息和交通信息融合于模型,让模型能够适应更复杂的城市道路。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="115">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200602077&amp;v=MTc3ODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhVYi9MTHo3QmQ3RzRIdGZNclk5Q1k0UUs=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>余翔,王新民,李俨.无人直升机路径规划算法研究[J].计算机应用,2006,26(2):494-495.(YU X,WANG X M,LI Y.Study of a path planning algorithm for unmanned helicopter[J].Journal of Computer Applications,2006,26(2):494-495.)
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2017S2019&amp;v=MjgxOTFzRnluaFViL0xMejdCZDdHNEg5YXZyWTlFYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>张超超,房建东.基于定向加权A<sup>*</sup>算法的自主移动机器人路径规划[J].计算机应用,2017,37(S2):77-81.(ZAHNG CC,FANG J D.Path planning of autonomous mobile robot based on directional weighted A<sup>*</sup>algorithm[J].Journal of Computer Applications,2017,37(S2):77-81.)
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD6A10B40C4316EAF6AC6EF4737D5920A3&amp;v=MTAyNjZpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4Ynk5d0tzPU5qbkJhclhKSDlHK3E0ODJZT2dPQ2dsSXVSQmltVGtJUG52bHJ4VkJmTHVXUmN1Y0NPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>BRANDES U.A faster algorithm for betweenness centrality[J].Journal of Mathematical Sociology,2001,25(2):163-177.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A formal basis for the heuristic determination of minimum cost paths">

                                <b>[4]</b>HART P E,NILSSON N J,RAPHAEL B.A formal basis for the heuristic determination of minimum cost paths[J].IEEE Transactions on Systems Science and Cybernetics,1968,4(2):100-107.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimal and efficient path planning for partially-known environments">

                                <b>[5]</b>STENTZ A.Optimal and efficient path planning for partially-known environments[C]//Proceedings of the 1994 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,1994:3310-3317.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anytime motion planning using the RRT*">

                                <b>[6]</b>KARAMAN S,WALTER M R,PEREZ A,et al.Anytime motion planning using the RRT<sup>*</sup>[C]//Proceedings of the 2011 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2011:1478-1483.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7B1CECB560F13E90C4A082649FC6A917&amp;v=MDMzMDJwaHhieTl3S3M9TmlmT2ZiVEtINks1M1AxQVl1dDVEWDlNeGhaZzdrNTlRSDNrcUJ0RENyVGxUTHVZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>HU X,CHEN L,TANG B,et al.Dynamic path planning for autonomous driving on various roads with avoidance of static and moving obstacles[J].Mechanical Systems and Signal Processing,2018,100:482-500.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                LECUN Y L,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjMxMDBaZVp0RmlubFVyM0lJVm9XYWhFPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201804042&amp;v=MDkwMTVCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFViL0xMejdCZDdHNEg5bk1xNDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>胡学敏,易重辉,陈钦,等.基于运动显著图的人群异常行为检测[J].计算机应用,2018,38(4):1164-1169.(HU XM,YI C H,CHEN Q,et al.Abnormal crowd behavior detection based on motion saliency map[J].Journal of Computer Applications,2018,38(4):1164-1169.)
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention-Based Extraction of Structured Information from Street View Imagery">

                                <b>[11]</b>WOJNA Z,GORBAN A N,LEE D,et al.Attention-based extraction of structured information from street view imagery[C]//Proceedings of the 14th IAPR International Conference on Document Analysis and Recognition.Piscataway:IEEE,2017:844-850.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End to end learning for self-driving cars">

                                <b>[12]</b>BOJARSKI M,del TESTA D,DWORAKOWSKI D,et al.End to end learning for self-driving cars[EB/OL].(2016-04-25)[2019-02-23].https://arxiv.org/pdf/1604.07316.pdf.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepDriving:Learning Affordance for Direct Perception in Autonomous Driving">

                                <b>[13]</b>CHEN C Y,SEFF A,KORNHASUER A,et al.Deep driving:learning affordance for direct perception in autonomous driving[C]//Proceedings of the IEEE 2015 International Conference on Computer Vision.Piscataway:IEEE,2015:2722-2730.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning framework for autonomous driving">

                                <b>[14]</b>SALLAB A E L,ABDOU M,PEROT E,et al.Deep reinforcement learning framework for autonomous driving[EB/OL].[2019-01-10].https://arxiv.org/abs/1704.02532.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[15]</b>SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019-01-20].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional architecture for fast feature embedding">

                                <b>[16]</b>JIA Y,SHELHAMER E,DONAHUE J,et al.Caffe:convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM Conference on Multimedia.New York:ACM,2014:675-678.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speed of processing in the human visual system">

                                <b>[17]</b>THORPE S,FIZE D,MARLOT C.Speed of processing in the human visual system[J].Nature,1996,381(6582):520-522.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910014" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910014&amp;v=MTMwODZHNEg5ak5yNDlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFViL0xMejdCZDc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
