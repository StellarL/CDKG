<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136674944065000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906019%26RESULT%3d1%26SIGN%3dnsz6wWY5YUpWhFktI0Bl1PDgVB8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906019&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906019&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906019&amp;v=MTEyMTgvQUx6N0JkN0c0SDlqTXFZOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3I=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 船舶跟踪识别原理 ">1 船舶跟踪识别原理</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="2 船舶特征提取网络 ">2 船舶特征提取网络</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="3 船舶跟踪与识别 ">3 船舶跟踪与识别</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="3.1 &lt;b&gt;船舶跟踪&lt;/b&gt;">3.1 <b>船舶跟踪</b></a></li>
                                                <li><a href="#82" data-title="3.2 &lt;b&gt;船舶类型识别&lt;/b&gt;">3.2 <b>船舶类型识别</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#88" data-title="4 实验与结果分析 ">4 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="4.1 &lt;b&gt;船舶监控视频数据源&lt;/b&gt;">4.1 <b>船舶监控视频数据源</b></a></li>
                                                <li><a href="#91" data-title="4.2 &lt;b&gt;实验结果分析&lt;/b&gt;">4.2 <b>实验结果分析</b></a></li>
                                                <li><a href="#95" data-title="4.3 &lt;b&gt;效果对比&lt;/b&gt;">4.3 <b>效果对比</b></a></li>
                                                <li><a href="#100" data-title="4.4 &lt;b&gt;不同方法对比实验&lt;/b&gt;">4.4 <b>不同方法对比实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#107" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 所提算法的船舶跟踪识别流程">图1 所提算法的船舶跟踪识别流程</a></li>
                                                <li><a href="#63" data-title="图2 船舶特征提取网络框架">图2 船舶特征提取网络框架</a></li>
                                                <li><a href="#64" data-title="图3 集装箱船低级特征图提取到的高级特征图">图3 集装箱船低级特征图提取到的高级特征图</a></li>
                                                <li><a href="#94" data-title="图4 平均损失曲线和P-R曲线">图4 平均损失曲线和P-R曲线</a></li>
                                                <li><a href="#97" data-title="图5 通航环境良好视频场景下传统算法的跟踪结果">图5 通航环境良好视频场景下传统算法的跟踪结果</a></li>
                                                <li><a href="#98" data-title="图6 通航环境良好视频场景下本文算法的跟踪识别结果">图6 通航环境良好视频场景下本文算法的跟踪识别结果</a></li>
                                                <li><a href="#102" data-title="图7 雾天航行环境视频场景下传统算法的跟踪结果">图7 雾天航行环境视频场景下传统算法的跟踪结果</a></li>
                                                <li><a href="#103" data-title="图8 雾天航行环境视频场景下本文算法的跟踪识别结果">图8 雾天航行环境视频场景下本文算法的跟踪识别结果</a></li>
                                                <li><a href="#104" data-title="图9 部分船舶类型及重要部位识别结果">图9 部分船舶类型及重要部位识别结果</a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同船舶类型识别算法的识别精度对比&lt;/b&gt;"><b>表</b>1 <b>不同船舶类型识别算法的识别精度对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="129">


                                    <a id="bibliography_1" title="黄于欣.基于open CV的视频路径船舶检测与跟踪[J].舰船科学技术, 2017, 39 (8A) :28-30. (HUANG Y X.Ship detection and tracking based on open CV video path[J].Ship Science and Technology, 2017, 39 (8A) :28-30." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCKX201716011&amp;v=MDA2OTE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3J6Skx5N0Fkckc0SDliTnFZOUVaWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        黄于欣.基于open CV的视频路径船舶检测与跟踪[J].舰船科学技术, 2017, 39 (8A) :28-30. (HUANG Y X.Ship detection and tracking based on open CV video path[J].Ship Science and Technology, 2017, 39 (8A) :28-30.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_2" title="滕飞, 刘清, 郭建明, 等.TLD框架下的内河船舶跟踪[J].应用科学学报, 2014, 32 (1) :105-110. (TENG F, LIU Q, GUO J M, et al.Inland waterway ship tracking using a TLD framework[J].Journal of Applied Sciences, 2014, 32 (1) :105-110.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYKX201401017&amp;v=MTE5MjRSN3FmWnVac0Z5L2hXcnpKUERUQWRyRzRIOVhNcm85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        滕飞, 刘清, 郭建明, 等.TLD框架下的内河船舶跟踪[J].应用科学学报, 2014, 32 (1) :105-110. (TENG F, LIU Q, GUO J M, et al.Inland waterway ship tracking using a TLD framework[J].Journal of Applied Sciences, 2014, 32 (1) :105-110.) 
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_3" title="朱广华.BP神经网络和卡尔曼滤波相结合的船舶运动跟踪[J].舰船科学技术, 2016, 38 (10A) :82-84. (ZHU G H.Ship motion tracking based on combination of BP neural network and Kalman filter[J].Ship Science and Technology, 2016, 38 (10A) :82-84.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCKX201620028&amp;v=MjMzMTR6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3J6Skx5N0Fkckc0SDlmT3I0OUhiSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        朱广华.BP神经网络和卡尔曼滤波相结合的船舶运动跟踪[J].舰船科学技术, 2016, 38 (10A) :82-84. (ZHU G H.Ship motion tracking based on combination of BP neural network and Kalman filter[J].Ship Science and Technology, 2016, 38 (10A) :82-84.) 
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_4" title="蒋少峰, 王超, 吴樊, 等.基于结构特征分析的COSMO-Sky Med图像商用船舶分类算法[J].遥感技术与应用, 2014, 29 (4) :607-615. (JIANG S F, WANG C, WU F, et al.Algorithm for merchant ship classification in COSMO-Sky Med images based on structural feature analysis[J].Remote Sensing Technology and Application, 2014, 29 (4) :607-615.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGJS201404011&amp;v=MTY0MTh0R0ZyQ1VSN3FmWnVac0Z5L2hXcnpKUENyQmZiRzRIOVhNcTQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        蒋少峰, 王超, 吴樊, 等.基于结构特征分析的COSMO-Sky Med图像商用船舶分类算法[J].遥感技术与应用, 2014, 29 (4) :607-615. (JIANG S F, WANG C, WU F, et al.Algorithm for merchant ship classification in COSMO-Sky Med images based on structural feature analysis[J].Remote Sensing Technology and Application, 2014, 29 (4) :607-615.) 
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_5" title="胡侯立, 魏维, 胡蒙娜.深度学习算法的原理及应用[J].信息技术, 2015 (2) :175-177. (HU H L, WEI W, HU M N.Principles and practices of deep learning[J].Information Technology, 2015 (2) :175-177.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HDZJ201502047&amp;v=MDgzMjU3cWZadVpzRnkvaFdyekpMU25SWkxHNEg5VE1yWTlCWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        胡侯立, 魏维, 胡蒙娜.深度学习算法的原理及应用[J].信息技术, 2015 (2) :175-177. (HU H L, WEI W, HU M N.Principles and practices of deep learning[J].Information Technology, 2015 (2) :175-177.) 
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_6" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet Classification w ith Deep Convolutional Neural Netw orks">
                                        <b>[6]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_7" title="SZEGEDY C, TOSHEV A, ERHAN D.Deep neural networks for object detection[C]//Proceedings of the 2013 International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2013:2553-2561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for object detection">
                                        <b>[7]</b>
                                        SZEGEDY C, TOSHEV A, ERHAN D.Deep neural networks for object detection[C]//Proceedings of the 2013 International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2013:2553-2561.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_8" title="ERHAN D, SZEGEDY C, TOSHEV A, et al.Scalable object detection using deep neural networks[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:2155-2162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">
                                        <b>[8]</b>
                                        ERHAN D, SZEGEDY C, TOSHEV A, et al.Scalable object detection using deep neural networks[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:2155-2162.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_9" title="SERMANET P, EIGEN D, ZHANG X, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[EB/OL].[2018-08-25].https://www.nvidia.co.kr/content/tesla/pdf/machine-learning/overfeat-recognition-localication-detection.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overfeat:Integrated recognition,localization and detection using convolutional networks">
                                        <b>[9]</b>
                                        SERMANET P, EIGEN D, ZHANG X, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[EB/OL].[2018-08-25].https://www.nvidia.co.kr/content/tesla/pdf/machine-learning/overfeat-recognition-localication-detection.pdf.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_10" title="GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[10]</b>
                                        GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:580-587.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_11" title="SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-08-25].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">
                                        <b>[11]</b>
                                        SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-08-25].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_12" title="SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[12]</b>
                                        SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2015:1-9.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_13" title="HE K M, ZHANG X Y, REN S Q, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">
                                        <b>[13]</b>
                                        HE K M, ZHANG X Y, REN S Q, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_14" title="GIRSHICK R.Fast R-CNN[EB/OL].[2018-08-24].http://cn.arxiv.org/pdf/1504.08083.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[14]</b>
                                        GIRSHICK R.Fast R-CNN[EB/OL].[2018-08-24].http://cn.arxiv.org/pdf/1504.08083.pdf.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_15" title="LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//ECCV 2016:Proceedings of the 2016European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">
                                        <b>[15]</b>
                                        LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//ECCV 2016:Proceedings of the 2016European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_16" title="REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">
                                        <b>[16]</b>
                                        REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:779-788.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_17" title="CHAUHAN A K, KRISHAN P.Moving object tracking using gaussian mixture model and optical flow[J].International Journal of Advanced Research in Computer Science and Software Engineering, 2013, 3 (4) :243-246." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Moving Object Tracking using Gaussian Mixture Model and Optical Flow">
                                        <b>[17]</b>
                                        CHAUHAN A K, KRISHAN P.Moving object tracking using gaussian mixture model and optical flow[J].International Journal of Advanced Research in Computer Science and Software Engineering, 2013, 3 (4) :243-246.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_18" title="TRIPATHI R P, GHOSH S, CHANDLE J O.Tracking of object using optimal adaptive Kalman filter[C]//Proceedings of the2016 IEEE International Conference on Engineering and Technology.Piscataway, NJ:IEEE, 2016:17-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tracking of Object using Optimal Adaptive Kalman Filter">
                                        <b>[18]</b>
                                        TRIPATHI R P, GHOSH S, CHANDLE J O.Tracking of object using optimal adaptive Kalman filter[C]//Proceedings of the2016 IEEE International Conference on Engineering and Technology.Piscataway, NJ:IEEE, 2016:17-18.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_19" title="翟卫欣, 程承旗.基于Kalman滤波的Camshift运动跟踪算法[J].北京大学学报 (自然科学版) , 2015, 51 (5) :799-804. (ZHAI W X, CHENG C Q.A Camshift motion tracking algorithm based on Kalman filter[J].Acta Scientiarum Naturalium Universitatis Pekinensis, 2015, 51 (5) :799-804.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJDZ201505003&amp;v=MDg5ODNVUjdxZlp1WnNGeS9oV3J6Skp5ZlBkTEc0SDlUTXFvOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        翟卫欣, 程承旗.基于Kalman滤波的Camshift运动跟踪算法[J].北京大学学报 (自然科学版) , 2015, 51 (5) :799-804. (ZHAI W X, CHENG C Q.A Camshift motion tracking algorithm based on Kalman filter[J].Acta Scientiarum Naturalium Universitatis Pekinensis, 2015, 51 (5) :799-804.) 
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_20" title="FANG J, ZHOU Y, YU Y, et al.Fine-grained vehicle model recognition using a coarse-to-fine convolutional neural network architecture[J].IEEE Transactions on Intelligent Transportation Systems, 2017, 18 (7) :1782-1792." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fine-grained vehicle model recognition using a coarse-to-fine convolutional neural network architecture">
                                        <b>[20]</b>
                                        FANG J, ZHOU Y, YU Y, et al.Fine-grained vehicle model recognition using a coarse-to-fine convolutional neural network architecture[J].IEEE Transactions on Intelligent Transportation Systems, 2017, 18 (7) :1782-1792.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_21" title="张舞杰, 李迪, 叶峰.基于Sigmoid函数拟合的亚像素边缘检测方法[J].华南理工大学学报 (自然科学版) , 2009, 37 (10) :39-43. (ZHANG W J, LI D, YE F.Sub-pixel edge detection method based on Sigmoid function fitting[J].Journal of South China University of Technology (Natural Science Edition) , 2009, 37 (10) :39-43.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HNLG200910011&amp;v=MzA5NDdmWnVac0Z5L2hXcnpKTFNQSGFiRzRIdGpOcjQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        张舞杰, 李迪, 叶峰.基于Sigmoid函数拟合的亚像素边缘检测方法[J].华南理工大学学报 (自然科学版) , 2009, 37 (10) :39-43. (ZHANG W J, LI D, YE F.Sub-pixel edge detection method based on Sigmoid function fitting[J].Journal of South China University of Technology (Natural Science Edition) , 2009, 37 (10) :39-43.) 
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_22" title="CHEN X Q, WANG S Z, SHI C J, et al.Robust ship tracking via multi-view learning and sparse representation[J].Journal of Navigation, 2019, 72 (1) :176-192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust ship tracking via multi-view learning and sparse representation">
                                        <b>[22]</b>
                                        CHEN X Q, WANG S Z, SHI C J, et al.Robust ship tracking via multi-view learning and sparse representation[J].Journal of Navigation, 2019, 72 (1) :176-192.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-29 10:11</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1663-1668 DOI:10.11772/j.issn.1001-9081.2018102190            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于Darknet网络和YOLOv</b>3<b>算法的船舶跟踪识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%8D%9A&amp;code=29442270&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘博</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%83%9C%E6%AD%A3&amp;code=08508042&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王胜正</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%BB%BA%E6%A3%AE&amp;code=31404764&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵建森</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%98%8E%E5%B3%B0&amp;code=41987897&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李明峰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E6%B5%B7%E4%BA%8B%E5%A4%A7%E5%AD%A6%E5%95%86%E8%88%B9%E5%AD%A6%E9%99%A2&amp;code=0121742&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海海事大学商船学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对我国沿海与内陆水域区域视频监控处理存在实际利用率低、误差率大、无识别能力、需人工参与等问题, 提出基于Darknet网络模型结合YOLOv3算法的船舶跟踪识别方法实现船舶的跟踪并实时检测识别船舶类型, 解决了重要监测水域船舶跟踪与识别问题。该方法提出的Darknet网络引入了残差网络的思想, 采用跨层跳跃连接方式以增加网络深度, 构建船舶深度特征矩阵提取高级船舶特征进行组合学习, 得到船舶特征图。在此基础上, 引入YOLOv3算法实现基于图像的全局信息进行目标预测, 将目标区域预测和目标类别预测整合于单个神经网络模型中。加入惩罚机制来提高帧序列间的船舶特征差异。通过逻辑回归层作二分类预测, 实现在准确率较高的情况下快速进行目标跟踪与识别。实验结果表明, 提出的算法在30 frame/s的情况下, 平均识别精度达到89.5%, 与传统以及深度学习算法相比, 不仅具有更好的实时性、准确性, 对各种环境变化具有较好的鲁棒性, 而且可以识别多种船舶的类型及其重要部位。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%B7%E4%B8%8A%E4%BA%A4%E9%80%9A&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海上交通;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%88%B9%E8%88%B6%E7%9B%91%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">船舶监测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%88%B9%E8%88%B6%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">船舶跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%88%B9%E8%88%B6%E7%B1%BB%E5%9E%8B%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">船舶类型识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Darknet%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Darknet网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=YOLOv3%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">YOLOv3算法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘博 (1992—) , 男, 安徽阜阳人, 硕士研究生, 主要研究方向:船舶智能航行、计算机视觉;;
                                </span>
                                <span>
                                    *王胜正 (1976—) , 男, 湖南双峰人, 教授, 博士生导师, 博士, 主要研究方向:航海仿真、智能船舶航行、大数据、机器学习;szwang@ shmtu. edu. cn;
                                </span>
                                <span>
                                    赵建森 (1983—) , 男, 黑龙江海林人, 副教授, 博士, 主要研究方向:智能通信、微波与天线;;
                                </span>
                                <span>
                                    李明峰 (1994—) , 男, 四川遂宁人, 硕士研究生, 主要研究方向:船舶智能航行、强化学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-31</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (51379121, 61304230);</span>
                                <span>上海市曙光人才计划项目 (15SG44);</span>
                    </p>
            </div>
                    <h1><b>Ship tracking and recognition based on Darknet network and YOLOv</b>3 <b>algorithm</b></h1>
                    <h2>
                    <span>LIU Bo</span>
                    <span>WANG Shengzheng</span>
                    <span>ZHAO Jiansen</span>
                    <span>LI Mingfeng</span>
            </h2>
                    <h2>
                    <span>Merchant Marine College, Shanghai Maritime University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problems of low utilization rate, high error rate, no recognition ability and manual participation in video surveillance processing in coastal and inland waters of China, a new ship tracking and recognition method based on Darknet network model and YOLOv3 algorithm was proposed to realize ship tracking and real-time detection and recognition of ship types, solving the problem of ship tracking and recognition in important monitored waters. In the Darknet network of the proposed method, the idea of residual network was introduced, the cross-layer jump connection was used to increase the depth of the network, and the ship depth feature matrix was constructed to extract advanced ship features for combination learning and obtaining the ship feature map. On the above basis, YOLOv3 algorithm was introduced to realize target prediction based on image global information, and target region prediction and target class prediction were integrated into a single neural network model. Punishment mechanism was added to improve the ship feature difference between frames. By using logistic regression layer for binary classification prediction, target tracking and recognition was able to be realized quickly with high accuracy. The experimental results show that, the proposed algorithm achieves an average recognition accuracy of 89.5% with the speed of 30 frame/s; compared with traditional and deep learning algorithms, it not only has better real-time performance and accuracy, but also has better robustness to various environmental changes, and can recognize the types and important parts of various ships.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sea%20traffic&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sea traffic;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ship%20surveillance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ship surveillance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ship%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ship tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ship%20type%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ship type recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Darknet%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Darknet network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=YOLOv3%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">YOLOv3 algorithm;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LIU Bo, born in 1992, M. S. candidate. His research interests include intelligent navigation of ship, computer vision. ;
                                </span>
                                <span>
                                    WANG Shengzheng, born in 1976, Ph. D. , professor. His research interests include navigation simulation, intelligent ship navigation, big data, machine learning. ;
                                </span>
                                <span>
                                    ZHAO Jiansen, born in 1983, Ph. D. , associate professor. His research interests include intelligent communication, microwave and antenna. ;
                                </span>
                                <span>
                                    LI Mingfeng, born in 1994, M. S. candidate. His research interests include intelligent navigation of ship, reinforcement learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-31</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (51379121, 61304230);</span>
                                <span>the“Dawn”Program of Shanghai Education Commission (15SG44);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">近年来各类涉海、用海活动数量越来越多, 规模越来越大, 海上的各种安全事故、违章、违法行为相应也越来越多, 海上船舶监测工作日益繁重。目前主要的监测手段是船舶自动识别系统 (Automatic Identification System, AIS) 与岸基雷达, 但是由于AIS信息中存在主观错误信息, 而雷达目标存在信息缺失等原因, 因此在港口、沿海, 以及江河沿岸都布置了大量视频摄像头, 对现有AIS与雷达信息进行补充, 辅助船舶监控, 但是现有视频监控利用率普遍比较低, 主要原因是需要依靠人工进行观察。为了解决该问题, 黄于欣<citation id="173" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>利用OpenCV结合智能算法实现船舶轨迹的监控和跟踪, 该方法通过提取船舶航行轨迹, 挖掘潜在特征进行分析。滕飞等<citation id="174" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>利用先跟踪再学习的检测 (Tracking-Learning-Detection, TLD) 框架实现船舶识别和跟踪, 通过构建特征约束方程来校验特征像素的运动状态, 求解像素与特征方法相关的系数, 获得了短时间的船舶跟踪结果。为了提高船舶跟踪的精度, 朱广华<citation id="175" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>结合BP (Back Propagation) 神经网络和卡尔曼滤波算法, 构建了一种船舶动态跟踪模型, 该跟踪模型通过估计船舶运动参数来自适应获得船舶位置。蒋少峰等<citation id="176" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>还提出了一种基于结构特征分析的船舶类型识别方法, 该方法能够有效提取高分辨率的图像特征, 并利用支持向量机进行分类。但是由于海上视频摄像头安装位置、距离、方位的限制等, 导致视频质量低下, 严重影响了船舶跟踪识别率。因此快速、有效、稳定的船舶目标跟踪与识别是江海沿岸船舶智能监测亟需解决的问题, 对于保障海事交通安全, 提高海上交通通行效率, 实施海事执法等海上任务具有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="49">当前, 基于计算机视觉的深度学习算法<citation id="177" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>在目标跟踪识别领域取得了较好的效果, Krizhevsky等<citation id="178" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出基于深度学习理论的深度卷积神经网络 (Deep Convolutional Neural Network, DCNN) 的图像分类算法, 使图像分类的准确率大幅提升, 同时也带动了目标检测准确率的提升。Szegedy等<citation id="179" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将目标检测问题看作目标的回归问题, 使用深度卷积神经网络作为回归器预测输入图像中目标。Erhan等<citation id="180" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>使用深度卷积神经网络对目标的包围盒进行回归预测, 并给出每个包围盒包含类别无关对象的置信度。Sermanet等<citation id="181" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出一种基于深度卷积神经网络 (DCNN) 框架OverFeat, 集成了识别、定位和检测任务。与OverFeat不同, 区域卷积神经网络 (Region-CNN, R-CNN) <citation id="182" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>采用选择性搜索策略来提高检测效率, 对每个包围盒利用卷积神经网络 (CNN) 提取特征, 输入到为每个类训练好的支持向量机 (Support Vector Machine, SVM) 分类器, 得到包围盒属于每个类的分数。Simonyan等<citation id="183" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和Szegedy等<citation id="184" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>设计了层数22层的深度卷积神经网络, 采用的检测框架都类似区域卷积神经网络 (R-CNN) 。He等<citation id="185" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一个可以看作单层的网络层, 叫作感兴趣区域 (Region of Interest, ROI) Pooling, 这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量, 提高了检测速率。为了避免候选区域提取耗时比实际对象检测还多的场景, 于是Girshick<citation id="186" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了快速区域卷积神经网络, 出现了一个端到端的对象检测模型, 检测阶段非常方便快捷。Liu等<citation id="187" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出的单点多盒探测器 (Single Shot multibox Detector, SSD) 的检测方法, 使用小型卷积滤波器来预测边界框位置中的对象类别和偏移量, 对不同的宽高比检测使用不同的预测器 (滤波器) , 并将这些滤波器应用于网络后期的多个特征映射中, 以执行多个尺度的检测。而面向海上交通视觉感知任务中, 考虑到水上交通中船舶成像尺寸的变化, 光照变化、船舶成像视角变化和交叉会遇局面中的船舶成像重叠以及人工参与等问题, 严重影响了船舶跟踪识别率。</p>
                </div>
                <div class="p1">
                    <p id="50">针对以上问题, 本文提出了基于深度学习框架的Darknet网络模型结合YOLOv3算法<citation id="188" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的船舶跟踪识别方法。该方法改进了传统深度学习中的基础分类网络结构和目标的二分类预测方法, 实现了船舶的跟踪并实时检测识别船舶类型。首先, 对输入的船舶图像样本数据, 使用残差连接方式构建的深层网络模型通过构建的船舶特征矩阵进行卷积操作, 提取相应特征, 进行组合学习, 训练得到物体的特征图模型;并在此基础上添加特征交互层, 分为三个尺度, 每个尺度内通过卷积核的方式实现特征图局部的特征交互, 根据数据标准化处理以及维度聚类、细粒度特征操作, 直接预测出船舶目标物体的中心坐标;同时, 引入惩罚机制来提高模型的泛化能力, 更好地匹配定位跟踪船舶, 在此基础上添加多标签多分类的逻辑回归层, 对每个类别作二分类从而实现对目标船舶进行分类识别。所提出的跨层连接方式, 解决了传统的深度学习中因卷积神经网络的层数增加而出现的梯度消失和梯度爆炸情况, 避开了过拟合操作, 有很好的收敛性和鲁棒性。最后, 通过大量实验对比经典算法和深度学习算法在复杂的海况下的跟踪识别率和鲁棒性差异, 验证了该算法的有效性。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 船舶跟踪识别原理</h3>
                <div class="p1">
                    <p id="52">在海上智能交通监测中, 基于视频的船舶跟踪识别是非常重要的任务之一。高效的船舶跟踪算法和识别算法不仅能够在不同应用场景下有效地跟踪感兴趣区域 (ROI) 的船舶, 还要识别出多种船舶的类型。传统处理方法都是基于AIS系统和雷达结合, 通过对基于 Multi-view 学习和稀疏学习机制的船舶跟踪算子 (Ship Tracker based on Multi-view learning and Sparse representation, STMS) 、卡尔曼滤波、粒子滤波、mean-shift等算法<citation id="189" type="reference"><link href="161" rel="bibliography" /><link href="163" rel="bibliography" /><link href="165" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>的改进, 取得了一定的效果;但是对于跟踪和识别没有很好的结合, 尚不能很好地应用于海上智能交通管理中。本文以Darknet网络为基础和YOLOv3算法构建了一种实时的船舶跟踪识别算法, 达到速度和精度的均衡化。所提出的船舶跟踪识别模型中, 船舶特征提取网络 (Darknet网络) 及YOLOv3算法的分类识别是实现船舶跟踪识别的关键环节, 因此这里着重介绍特征提取网络和YOLOv3算法学习过程。本文算法的船舶跟踪识别流程如图1所示。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 所提算法的船舶跟踪识别流程" src="Detail/GetImg?filename=images/JSJY201906019_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 所提算法的船舶跟踪识别流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Flow chart of ship tracking and recognition of the proposed algorithm</p>

                </div>
                <h3 id="54" name="54" class="anchor-tag">2 船舶特征提取网络</h3>
                <div class="p1">
                    <p id="55">特征提取网络对船舶特征提取、学习的能力是船舶跟踪以及类型识别的关键。深度学习框架的Darknet网络结构采用残差网络的连接思想, 引入了Residual结构, 在保证网络结构很深的情况下, 仍能收敛。但针对于船舶特征提取过程中, 由于船舶所属空间属于动态变化的高海况复杂区域, 其原有的基础网络结构提取到的船舶类型特征有一部分是低级船舶特征, 而这部分低级船舶特征之间不仅没有显著差别, 还会直接影响计算速度、不同类型的船舶识别精度。为了可以提取到不同的船舶深度特征表达, 产生更多特征组合学习得到物体的特征图, 在原有的基础网络结构中构建船舶深度特征矩阵, 以提取船舶的高级特征, 同时加入最大池化层, 以及引入局部神经元的活动创建竞争机制。基于Darknet网络结构为面向船舶特征提取的网络框架如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="56">通过网络结构的卷积层获取上一个深度网络层输出的底层船舶特征进行特征学习, 提取出高度抽象的船舶特征<citation id="190" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。以集装箱船为例, 通过构建的网络结构对低层油轮特征图进行提取得到的高层油轮特征图如图3所示。卷积层中的船舶深度特征提取对应的矩阵如下:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><msubsup><mrow></mrow><mi>n</mi><mi>r</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mi mathvariant="bold-italic">S</mi></mstyle><msubsup><mrow></mrow><mi>m</mi><mrow><mi>r</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>*</mo><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow><mi>r</mi></msubsup><mo>+</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>n</mi><mi>r</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中:<b><i>S</i></b><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>r</mi></msubsup></mrow></math></mathml>是第<i>r</i>层卷积网络的第<i>n</i>个输出的特征映射;函数<i>f</i>表示第<i>r</i>层卷积神经网络神经元的激活函数;<b><i>S</i></b><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mrow><mi>r</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>是第<i>r</i>-1个网络层的第<i>m</i>个输入的船舶特征映射;<b><i>P</i></b><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow><mi>r</mi></msubsup></mrow></math></mathml>是第<i>n</i>个网络输出层的船舶特征映射和第<i>m</i>个输入特征映射的连接权重;参数<b><i>W</i></b><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>r</mi></msubsup></mrow></math></mathml>是第<i>r</i>层卷积神经网络的第<i>n</i>个特征映射的偏置量。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 船舶特征提取网络框架" src="Detail/GetImg?filename=images/JSJY201906019_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 船舶特征提取网络框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Ship feature extraction network framework</p>

                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 集装箱船低级特征图提取到的高级特征图" src="Detail/GetImg?filename=images/JSJY201906019_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 集装箱船低级特征图提取到的高级特征图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 High-level feature maps extracted from low-level feature maps of container ship</p>

                </div>
                <div class="p1">
                    <p id="65">从卷积层输出的船舶特征图, 学习并提取船舶特征图中深层次、具有语义信息的特征, 忽略次要特征图的学习结果。故引用最大池化算法来池化卷积层输出的船舶征图。池化特征图的表达式:</p>
                </div>
                <div class="p1">
                    <p id="66"><b><i>pool</i></b><sub><i>u</i>, <i>v</i></sub>=<b><i>Max</i></b><sub><i>i</i>∈[1, <i>k</i>], <i>j</i>∈[1, <i>k</i>]</sub><b><i>S</i></b><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo stretchy="false">{</mo><mi>i</mi><mo>+</mo><mo stretchy="false"> (</mo><mi>u</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mi>d</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>+</mo><mo stretchy="false"> (</mo><mi>v</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mi>d</mi><mo stretchy="false">}</mo></mrow><mi>r</mi></msubsup></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="68">其中:<i>k</i>是池化核的维度;<b><i>S</i></b><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>r</mi></msubsup></mrow></math></mathml>是卷积层生成的第<i>n</i>个船舶特征图, <i>d</i>是代表池化步长;<b><i>pool</i></b><sub><i>u</i>, <i>v</i></sub>是池化层对特征图<b><i>S</i></b><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>r</mi></msubsup></mrow></math></mathml>池化得到的特征, 参数<i>u</i>, <i>v</i>是<b><i>pool</i></b><sub><i>u</i>, <i>v</i></sub>的维度。</p>
                </div>
                <div class="p1">
                    <p id="71">为了提高特征学习的速度, 引入局部神经元的活动创建竞争机制, 增加了局部响应归一化 (Local Response Normalization, LRN) 层。使其中响应比较大的值变得相对更大, 并抑制其他反馈较小的神经元, 增强了模型的泛化能力。LRN层给定特征图的响应函数如下:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϕ</mtext><msubsup><mrow></mrow><mi>i</mi><mi>r</mi></msubsup><mo>=</mo><mi mathvariant="bold-italic">Γ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mspace width="0.25em" /><mi>r</mi></mrow></msubsup><mo>/</mo><mrow><mo stretchy="false">[</mo><mo stretchy="false"> (</mo></mrow><mi>α</mi><mo>+</mo><mi>γ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>max</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo>-</mo><mi>v</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo></mrow><mrow><mi>min</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>i</mi><mo>+</mo><mi>v</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></munderover><mi mathvariant="bold-italic">Γ</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mtext> </mtext><mn>2</mn></mrow></msubsup><mo stretchy="false">) </mo><mi>φ</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">其中:ϕ<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>r</mi></msubsup></mrow></math></mathml>是第<i>r</i>层LRN单元的第<i>i</i>个船舶特征图的响应;<i>Γ</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mspace width="0.25em" /><mi>r</mi></mrow></msubsup></mrow></math></mathml>是第<i>r</i>层 LRN 单元中的第<i>i</i>个船舶特征图的特征值;<i>α</i>、<i>γ</i>、<i>u</i>、<i>v</i>是卷积神经网络的超参数。通过上述各个网络层的特征学习和提取过程, 网络结构可以获得船舶数据样本泛化的、独特的特征图。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">3 船舶跟踪与识别</h3>
                <div class="p1">
                    <p id="77"><i>YOLOv</i>3算法通过特征提取网络对输入图像采用回归思想直接提取特征, 得到一定尺寸的特征图, 然后将输入图像分成相应大小的网格, 通过网格预测出的包围盒与真实边框中目标物体的中心坐标直接进行匹配定位, 在此基础上对目标物体进行分类识别。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">3.1 <b>船舶跟踪</b></h4>
                <div class="p1">
                    <p id="79">对于船舶所在的空间环境具有背景相似性以及船舶移动出现的重叠性, 直接采用边框回归进行船舶目标的坐标预测从而匹配定位进行跟踪船舶, 会出现船舶边界框丢失的结果。为了有效抑制背景相似物体防止跟踪框漂移, 同时实现对目标本身的移动更加鲁棒的效果, 构建惩罚机制对模型特征进行处理, 并对这些特征进行表示和学习, 以实现视频序列中船舶跟踪的目的。为了获得较好的跟踪效果, 引入坐标预测框作为代价函数, 该惩罚机制如下:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Μ</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mrow><mi mathvariant="bold-italic">D</mi><mo>, </mo><mi mathvariant="bold-italic">U</mi><mo>, </mo><mi mathvariant="bold-italic">V</mi></mrow></munder><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>Κ</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>f</mi></mstyle><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mi>Κ</mi></msup><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mi>Κ</mi></msup><mo>-</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mi>Κ</mi></msup><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ρ</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mn>2</mn></mrow></msub><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Q</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mn>2</mn></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mi>Κ</mi></msup><mo>=</mo><mi mathvariant="bold-italic">Ρ</mi><msup><mrow></mrow><mi>Κ</mi></msup><mo>+</mo><mi mathvariant="bold-italic">Q</mi><msup><mrow></mrow><mi>Κ</mi></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中:对于该模型中每一个特征<i>k</i>, 用<b><i>D</i></b><sup><i>K</i></sup>来表示船舶图像跟踪序列, <b><i>V</i></b><sup><i>K</i></sup>为船舶特征图的特征矩阵, <b><i>U</i></b><sup><i>K</i></sup>为序列<b><i>D</i></b><sup><i>K</i></sup>的矩阵表现形式; <i>f</i><sub><i>L</i></sub>是代价函数, 用来评估船舶目标和船舶特征图矩阵在第<i>i</i>个特征的差异程度。较小的代价值说明该模型获得了较好的跟踪效果。将矩阵<b><i>U</i></b><sup><i>K</i></sup>分解为系数矩阵<b><i>P</i></b><sup><i>K</i></sup>和<b><i>Q</i></b><sup><i>K</i></sup> (如式 (5) 所示) 。<b><i>P</i></b><sup><i>K</i></sup>为第<i>k</i>个特征的矩阵表现形式, 通过水平填充<b><i>P</i></b><sup><i>K</i></sup>获取全局表征矩阵<b><i>P</i></b>, 类似可求全局系数矩阵<b><i>Q</i></b>。参数‖<b><i>P</i></b>‖<sub>1, 2</sub>表示该模型各个特征的独立性, 即全局系数矩阵<b><i>P</i></b>行稀疏程度越高, 越能体现不同特征之间的耦合程度。‖<b><i>Q</i></b><sup>T</sup>‖<sub>1, 2</sub>表示模型跟踪的异常结果。式 (4) 中的参数<i>λ</i><sub>1</sub>表征了全局系数矩阵<b><i>P</i></b>的惩罚度, 参数<i>λ</i><sub>2</sub>是全局系数矩阵<b><i>Q</i></b>对应的惩罚系数。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">3.2 <b>船舶类型识别</b></h4>
                <div class="p1">
                    <p id="83">对于海上船舶类型的识别, 船舶的空间分布具有交叠现象, 会出现同一个框检测对应两个不同的船舶, 这样只能识别出一种船舶类型, 造成了识别率下降。本文采用多标签分类进行目标类别预测, 在网络结构上添加了多标签多分类的逻辑回归层。以<i>sigmoid</i>函数<citation id="191" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>为逻辑回归单元来对每个类别作二分类。表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="84"><i>y</i>=1/ (e<sup>-<i>x</i></sup>+1)      (6) </p>
                </div>
                <div class="p1">
                    <p id="85">因此当一张图像经过特征提取后的某一类输出经过sigmoid函数约束为[0, 1]的分布范围内。为了更好地区分船舶类型特征的相似性, 提高识别精度, 采用交叉熵代价函数来衡量神经网络的预测值与实际值之间的差异。对于<i>m</i>个船舶样本, 总的损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">[</mo></mstyle><mi>Τ</mi><msup><mrow></mrow><mi>i</mi></msup><mrow><mi>lg</mi></mrow><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>Τ</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mrow><mi>lg</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">其中:<i>m</i>是样本数;<i>T</i>是标签, 取值0或1;<i>i</i>表示第<i>i</i>个样本;<i>f</i> (<i>x</i>) 表示预测的输出。</p>
                </div>
                <h3 id="88" name="88" class="anchor-tag">4 实验与结果分析</h3>
                <h4 class="anchor-tag" id="89" name="89">4.1 <b>船舶监控视频数据源</b></h4>
                <div class="p1">
                    <p id="90">上海港已成为世界上最重要的港口之一, 其集装箱吞吐量在2016年已经超过3 600万<i>TEU</i> (<i>Twenty</i>-<i>foot Equivalent Unit</i>) 。上海港巨大的集装箱吞吐量导致上海港腹地内的航道是国内内河运输最为繁忙的航道之一。因此, 利用提出的船舶跟踪识别模型检测上海港监控视频中的船舶具有一定的实际意义。将采集到的海事监控视频被分为两组, 以评价船舶检测算法的性能。第一组船舶监控视频基于通航环境良好的情况下, 用于评估船舶检测模型不同交通状态下的检测性能。第二组船舶监控视频是基于雾天航行的环境下, 用于测试在能见度很低情况下, 船舶检测模型的鲁棒性和准确性。本研究的实验平台是 <i>Windows</i> 10操作系统, 16 <i>GB RAM</i>, <i>CPU</i>处理器的主频是 3.2 <i>GHz</i>, <i>GPU</i>为<i>GTX</i> 1050<i>Ti</i>, 实验平台是<i>PyCharm</i> (2018版) 。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">4.2 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="92">为了验证检测的有效性和可靠性, 根据图3 (<i>a</i>) 显示的训练过程迭代次数的平均损失曲线发现, 训练迭代400次后, 随着迭代次数增加, 平均损失函数值基本保持不变, 并趋于稳定, 表明该算法在训练过程中具有很快的收敛性。通过准确率-召回率 (<i>Precision</i>-<i>Recall</i>) 曲线来衡量分类器的性能指标, 以集装箱船为例, 测试图像数据有<i>A</i>个集装箱船, 通过训练好的模型检测到集装箱船有<i>B</i>个, 而其中被正确检测到的集装箱船有<i>C</i>个, 则准确率-召回率定义如下:召回率 (Recall) 为<i>C</i>/<i>A</i>;准确率 (Precision) 为<i>C</i>/<i>B</i>。</p>
                </div>
                <div class="p1">
                    <p id="93">根据图4 (b) 召回率-准确率 (P-R) 曲线可知, 在不损失精度的条件下能达到85%召回率;而当召回率达到80%时, 准确率依然可以达到80%。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_094.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 平均损失曲线和P-R曲线" src="Detail/GetImg?filename=images/JSJY201906019_094.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 平均损失曲线和P-R曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_094.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Average loss curve and P-R curve</p>

                </div>
                <h4 class="anchor-tag" id="95" name="95">4.3 <b>效果对比</b></h4>
                <div class="p1">
                    <p id="96">分别提取不同环境和不同交通流量下的船舶监控视频中关键帧来评价算法的检测性能。用本文提出的<i>YOLOv</i>3算法对视频序列的船舶进行检测;此外, 使用基于传统算法的卡尔曼 (<i>Kalman</i>) 算法和<i>Meanshift</i> 跟踪算法, 以及基于 <i>Multi</i>-<i>view</i> 学习和稀疏学习机制的船舶跟踪算子 (<i>STMS</i>) 模型也分别检测该视频序列的船舶, 以比较不同场景下不同算法的检测性能<citation id="192" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。从图5中可以看出, 基于传统算法在早期的船舶跟踪过程中均显示了较好的跟踪效果;然而, 在第168帧和372帧出现跟踪边框距离真实的目标船舶较远, 甚至在第450帧时, 没有正确跟踪到目标船舶。而且, 以上算法只能标记一类船舶目标, 无法进行多目标的跟踪且没有识别功能。根据图6可以看出, 本文提出的<i>YOLOv</i>3算法几乎能跟踪识别该帧中所有的船舶, 做到了实时检测船舶。在海上交通流量较大的情况下, 该算法的检测性能基本保持不变, 而传统算法则存在一定的误检率。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 通航环境良好视频场景下传统算法的跟踪结果" src="Detail/GetImg?filename=images/JSJY201906019_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 通航环境良好视频场景下传统算法的跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Tracking results of traditional algorithms in video scenes with</i><i>good navigation environment</i></p>

                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 通航环境良好视频场景下本文算法的跟踪识别结果" src="Detail/GetImg?filename=images/JSJY201906019_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 通航环境良好视频场景下本文算法的跟踪识别结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Tracking and recognition results of the proposed algorithm in</i><i>video scenes with good navigation environment</i></p>

                </div>
                <div class="p1">
                    <p id="99">为了验证本文<i>YOLOv</i>3算法的鲁棒性和准确性, 对船舶雾天航行的海事监控视频进行船舶检测实验, 结果如图7～8所示。由图7～8可以看出, 基于传统的算法在船舶航行过程中, 已经跟丢了船舶目标, 极大地挑战了船舶跟踪算法的鲁棒性和准确性;而本文的<i>YOLOv</i>3算法检测效果显示, 其能够准确地跟踪船舶目标并识别出船舶。同时, 从图8中可以看出, 本文算法有效地克服了雾霾天气以及光照变化带来的影响, 在能见度很低的情况下依然能有效地跟踪识别船舶。在识别船舶类型基础上, 进一步处理船舶视觉感知任务, 结果如图9所示, 本文的<i>YOLOv</i>3算法准确地识别出多种船舶类型以及船舶的重要部位。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">4.4 <b>不同方法对比实验</b></h4>
                <div class="p1">
                    <p id="101">为了验证本文方法的有效性, 用当前常用的深度学习目标识别方法来进一步对比验证该模型的识别性能。采用K-近邻算法 (K-<i>Nearest Neighbors</i>, K<i>NN</i>) 、人工神经网络 (<i>Artificial Neural Network</i>, <i>ANN</i>) 、传统神经网络 (<i>CNN</i>) 和深度卷积神经网络 (<i>DCNN</i>) 方法对不同船舶类型作实验对比。在本实验的条件下, 统一约定<i>IoU</i> (<i>Intersection</i>-<i>over</i>-<i>Union</i>) 值大于0.75为目标检测正确, 结果如表1所示。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 雾天航行环境视频场景下传统算法的跟踪结果" src="Detail/GetImg?filename=images/JSJY201906019_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 雾天航行环境视频场景下传统算法的跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_102.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Tracking results of traditional algorithms in video scenes with</i><i>foggy navigation environment</i></p>

                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 雾天航行环境视频场景下本文算法的跟踪识别结果" src="Detail/GetImg?filename=images/JSJY201906019_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 雾天航行环境视频场景下本文算法的跟踪识别结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 8 <i>Tracking and recognition results of the proposed algorithm in</i><i>video scenes with foggy navigation environment</i></p>

                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906019_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 部分船舶类型及重要部位识别结果" src="Detail/GetImg?filename=images/JSJY201906019_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 部分船舶类型及重要部位识别结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906019_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 9 <i>Recognition results of some ship types and important parts</i></p>

                </div>
                <div class="p1">
                    <p id="105">在以上五种船舶类型的识别任务中, K<i>NN</i>算法和<i>ANN</i>算法对杂货船和散货船的识别精度最低, 其识别精度分别为31.2%和30.1%, 而<i>CNN</i>方法对 <i>LNG</i> (<i>Liquefied Natural Gas</i>) 船的识别精度最低, 其识别精度仅为63.2%;深度学习<i>DCNN</i> 算法对散货船和<i>LNG</i>船的识别精度分别为72.5%和66.7%, 而本文方法对上述两种船型的识别精度分别为88.2%和82.7%, 对于集装箱船的识别精度是96.8%。船舶类型识别精度结果表明, 上述传统方法 (K<i>NN</i>和<i>ANN</i>) 和基于深度学习方法 (<i>CNN</i>和<i>DCNN</i>) 并不能很好提取不同船舶类型的特征, 而基于本文模型的船舶类型识别方法可以较好地识别不同船舶类型的深度特征, 能够获得较好的船舶识别效果。</p>
                </div>
                <div class="area_img" id="106">
                    <p class="img_tit"><b>表</b>1 <b>不同船舶类型识别算法的识别精度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Comparison of recognition accuracy of different ship type</i><i>recognition algorithms</i></p>
                    <p class="img_note">%</p>
                    <table id="106" border="1"><tr><td><br />类型</td><td>K<i>NN</i></td><td><i>ANN</i></td><td><i>CNN</i></td><td><i>DCNN</i></td><td>本文方法</td></tr><tr><td><br />杂货船</td><td>34.20</td><td>30.10</td><td>80.00</td><td>86.00</td><td>89.50</td></tr><tr><td><br />散货船</td><td>31.20</td><td>33.10</td><td>71.20</td><td>72.50</td><td>88.20</td></tr><tr><td><br />集装箱</td><td>53.20</td><td>61.20</td><td>85.20</td><td>90.70</td><td>96.80</td></tr><tr><td><br /><i>LNG</i>船</td><td>42.10</td><td>37.00</td><td>63.20</td><td>66.70</td><td>82.70</td></tr><tr><td><br />油船</td><td>46.10</td><td>45.30</td><td>79.50</td><td>84.60</td><td>90.50</td></tr><tr><td><br />平均值</td><td>40.90</td><td>42.60</td><td>76.70</td><td>81.40</td><td>89.50</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="107" name="107" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="108">本文提出的基于<i>Darknet</i>网络和<i>YOLOv</i>3算法的船舶跟踪识别方法在面向智能航行视觉感知任务的船舶跟踪识别问题中, 有效克服了不同的光照、不同天气和风浪等各种海况条件下以及人工参与的缺陷。与传统算法的对比实验表明, 该算法能够在快速实时检测船舶的情况下, 又有很好的准确性和鲁棒性。本研究是基于可视条件较好的船舶跟踪和船舶类型识别研究, 并未开展可视条件不佳的船舶检测、跟踪与船舶类型识别相关研究, 例如基于夜视条件下的船舶检测、船舶跟踪和船舶类型识别研究。后续工作将融合雷达、红外和 <i>AIS</i> 等海上交通数据, 对可视条件不佳的船舶跟踪识别等问题展开深入研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="129">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCKX201716011&amp;v=MTc2NDRkckc0SDliTnFZOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3J6Skx5N0E=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>黄于欣.基于open CV的视频路径船舶检测与跟踪[J].舰船科学技术, 2017, 39 (8A) :28-30. (HUANG Y X.Ship detection and tracking based on open CV video path[J].Ship Science and Technology, 2017, 39 (8A) :28-30.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYKX201401017&amp;v=MDE3MTh0R0ZyQ1VSN3FmWnVac0Z5L2hXcnpKUERUQWRyRzRIOVhNcm85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>滕飞, 刘清, 郭建明, 等.TLD框架下的内河船舶跟踪[J].应用科学学报, 2014, 32 (1) :105-110. (TENG F, LIU Q, GUO J M, et al.Inland waterway ship tracking using a TLD framework[J].Journal of Applied Sciences, 2014, 32 (1) :105-110.) 
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCKX201620028&amp;v=MjU4NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnpKTHk3QWRyRzRIOWZPcjQ5SGI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>朱广华.BP神经网络和卡尔曼滤波相结合的船舶运动跟踪[J].舰船科学技术, 2016, 38 (10A) :82-84. (ZHU G H.Ship motion tracking based on combination of BP neural network and Kalman filter[J].Ship Science and Technology, 2016, 38 (10A) :82-84.) 
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGJS201404011&amp;v=MjQxNzh6SlBDckJmYkc0SDlYTXE0OUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3I=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>蒋少峰, 王超, 吴樊, 等.基于结构特征分析的COSMO-Sky Med图像商用船舶分类算法[J].遥感技术与应用, 2014, 29 (4) :607-615. (JIANG S F, WANG C, WU F, et al.Algorithm for merchant ship classification in COSMO-Sky Med images based on structural feature analysis[J].Remote Sensing Technology and Application, 2014, 29 (4) :607-615.) 
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HDZJ201502047&amp;v=MzAxNjl1WnNGeS9oV3J6SkxTblJaTEc0SDlUTXJZOUJZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>胡侯立, 魏维, 胡蒙娜.深度学习算法的原理及应用[J].信息技术, 2015 (2) :175-177. (HU H L, WEI W, HU M N.Principles and practices of deep learning[J].Information Technology, 2015 (2) :175-177.) 
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet Classification w ith Deep Convolutional Neural Netw orks">

                                <b>[6]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//NIPS2012:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for object detection">

                                <b>[7]</b>SZEGEDY C, TOSHEV A, ERHAN D.Deep neural networks for object detection[C]//Proceedings of the 2013 International Conference on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2013:2553-2561.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">

                                <b>[8]</b>ERHAN D, SZEGEDY C, TOSHEV A, et al.Scalable object detection using deep neural networks[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:2155-2162.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overfeat:Integrated recognition,localization and detection using convolutional networks">

                                <b>[9]</b>SERMANET P, EIGEN D, ZHANG X, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[EB/OL].[2018-08-25].https://www.nvidia.co.kr/content/tesla/pdf/machine-learning/overfeat-recognition-localication-detection.pdf.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[10]</b>GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:580-587.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">

                                <b>[11]</b>SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-08-25].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[12]</b>SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2015:1-9.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">

                                <b>[13]</b>HE K M, ZHANG X Y, REN S Q, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[14]</b>GIRSHICK R.Fast R-CNN[EB/OL].[2018-08-24].http://cn.arxiv.org/pdf/1504.08083.pdf.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:single shot multibox detector">

                                <b>[15]</b>LIU W, ANGUELOV D, ERHAN D, et al.SSD:single shot multibox detector[C]//ECCV 2016:Proceedings of the 2016European Conference on Computer Vision, LNCS 9905.Cham:Springer, 2016:21-37.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">

                                <b>[16]</b>REDMON J, DIVVALA S, GIRSHICK R, et al.You only look once:unified, real-time object detection[C]//Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2015:779-788.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Moving Object Tracking using Gaussian Mixture Model and Optical Flow">

                                <b>[17]</b>CHAUHAN A K, KRISHAN P.Moving object tracking using gaussian mixture model and optical flow[J].International Journal of Advanced Research in Computer Science and Software Engineering, 2013, 3 (4) :243-246.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tracking of Object using Optimal Adaptive Kalman Filter">

                                <b>[18]</b>TRIPATHI R P, GHOSH S, CHANDLE J O.Tracking of object using optimal adaptive Kalman filter[C]//Proceedings of the2016 IEEE International Conference on Engineering and Technology.Piscataway, NJ:IEEE, 2016:17-18.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJDZ201505003&amp;v=MDg3OTVxcUJ0R0ZyQ1VSN3FmWnVac0Z5L2hXcnpKSnlmUGRMRzRIOVRNcW85Rlo0UUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>翟卫欣, 程承旗.基于Kalman滤波的Camshift运动跟踪算法[J].北京大学学报 (自然科学版) , 2015, 51 (5) :799-804. (ZHAI W X, CHENG C Q.A Camshift motion tracking algorithm based on Kalman filter[J].Acta Scientiarum Naturalium Universitatis Pekinensis, 2015, 51 (5) :799-804.) 
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fine-grained vehicle model recognition using a coarse-to-fine convolutional neural network architecture">

                                <b>[20]</b>FANG J, ZHOU Y, YU Y, et al.Fine-grained vehicle model recognition using a coarse-to-fine convolutional neural network architecture[J].IEEE Transactions on Intelligent Transportation Systems, 2017, 18 (7) :1782-1792.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HNLG200910011&amp;v=MDMzNjBMU1BIYWJHNEh0ak5yNDlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFdyeko=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>张舞杰, 李迪, 叶峰.基于Sigmoid函数拟合的亚像素边缘检测方法[J].华南理工大学学报 (自然科学版) , 2009, 37 (10) :39-43. (ZHANG W J, LI D, YE F.Sub-pixel edge detection method based on Sigmoid function fitting[J].Journal of South China University of Technology (Natural Science Edition) , 2009, 37 (10) :39-43.) 
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust ship tracking via multi-view learning and sparse representation">

                                <b>[22]</b>CHEN X Q, WANG S Z, SHI C J, et al.Robust ship tracking via multi-view learning and sparse representation[J].Journal of Navigation, 2019, 72 (1) :176-192.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906019" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906019&amp;v=MTEyMTgvQUx6N0JkN0c0SDlqTXFZOUViWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeS9oV3I=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
