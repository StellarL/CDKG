<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136767834346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905038%26RESULT%3d1%26SIGN%3dDMf2DJ%252fKfFS%252beruj7WrgPy0444Q%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905038&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905038&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905038&amp;v=MjAwMjNVUjdxZlp1WnNGeURuVUx2QUx6N0JkN0c0SDlqTXFvOUdiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="1 定位技术理论和模型 ">1 定位技术理论和模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="1.1 &lt;b&gt;基于相似变换的人脸初始化&lt;/b&gt;">1.1 <b>基于相似变换的人脸初始化</b></a></li>
                                                <li><a href="#107" data-title="1.2 &lt;b&gt;自适应由粗到细的窗口调节方法&lt;/b&gt;">1.2 <b>自适应由粗到细的窗口调节方法</b></a></li>
                                                <li><a href="#124" data-title="1.3 &lt;b&gt;基于互信息的特征选择方法&lt;/b&gt;">1.3 <b>基于互信息的特征选择方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#139" data-title="2 算法伪代码 ">2 算法伪代码</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#201" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#202" data-title="3.1 &lt;b&gt;实验环境&lt;/b&gt;">3.1 <b>实验环境</b></a></li>
                                                <li><a href="#204" data-title="3.2 &lt;b&gt;实验细节&lt;/b&gt;">3.2 <b>实验细节</b></a></li>
                                                <li><a href="#209" data-title="3.3 &lt;b&gt;实验结果&lt;/b&gt;">3.3 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#227" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#106" data-title="图1 自适应窗回归算法技术路线">图1 自适应窗回归算法技术路线</a></li>
                                                <li><a href="#212" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同算法定位误差比较&lt;/b&gt;"><b>表</b>1 <b>不同算法定位误差比较</b></a></li>
                                                <li><a href="#214" data-title="图2 &lt;i&gt;LFPW&lt;/i&gt;库各算法运行结果">图2 <i>LFPW</i>库各算法运行结果</a></li>
                                                <li><a href="#217" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同窗口策略定位误差比较&lt;/b&gt;"><b>表</b>2 <b>不同窗口策略定位误差比较</b></a></li>
                                                <li><a href="#220" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;i&gt;LFPW&lt;/i&gt;&lt;b&gt;上不同算法定位正确率比较&lt;/b&gt; %"><b>表</b>3 <i>LFPW</i><b>上不同算法定位正确率比较</b> %</a></li>
                                                <li><a href="#225" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;i&gt;LFPW&lt;/i&gt;&lt;b&gt;库各算法运行速度比较&lt;/b&gt;"><b>表</b>4 <i>LFPW</i><b>库各算法运行速度比较</b></a></li>
                                                <li><a href="#226" data-title="图3 本文算法运行结果">图3 本文算法运行结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="249">


                                    <a id="bibliography_1" title=" VIOLA P, JONES M.Rapid object detection using a boosted cascade of simple features[C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2001:511." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Improved Computaul Viola, Michael Jones">
                                        <b>[1]</b>
                                         VIOLA P, JONES M.Rapid object detection using a boosted cascade of simple features[C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2001:511.
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_2" title=" 陈凯星, 刘赟, 王金海, 等.基于遗传机制和高斯变差的自动前景提取方法[J].计算机应用, 2017, 37 (11) :3231-3237. (CHEN K X, LIU Y, WANG J H, et al.Foreground extraction with genetic and difference of Guassian[J].Journal of Computer Applications, 2017, 37 (11) :3231-3237.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201711034&amp;v=MjE3ODZHNEg5Yk5ybzlHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVMdkFMejdCZDc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         陈凯星, 刘赟, 王金海, 等.基于遗传机制和高斯变差的自动前景提取方法[J].计算机应用, 2017, 37 (11) :3231-3237. (CHEN K X, LIU Y, WANG J H, et al.Foreground extraction with genetic and difference of Guassian[J].Journal of Computer Applications, 2017, 37 (11) :3231-3237.) 
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_3" title=" 张欢欢, 洪敏, 袁玉波.基于极端学习机的人脸特征深度稀疏自编码方法[J].计算机应用, 2018, 38 (11) :3193-3198. (ZHANG H H, HONG M, YUAN Y B.Deep sparse auto-encoder using extreme learning machine for facial feature[J].Journal of Computer Applications, 2018, 38 (11) :3193-3198.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811026&amp;v=MDcxODJDVVI3cWZadVpzRnlEblVMdkFMejdCZDdHNEg5bk5ybzlIWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         张欢欢, 洪敏, 袁玉波.基于极端学习机的人脸特征深度稀疏自编码方法[J].计算机应用, 2018, 38 (11) :3193-3198. (ZHANG H H, HONG M, YUAN Y B.Deep sparse auto-encoder using extreme learning machine for facial feature[J].Journal of Computer Applications, 2018, 38 (11) :3193-3198.) 
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_4" title=" COOTES T F, TAYLOR C J, COOPER D H, et al.Active shape models-their training and application[J].Computer Vision &amp;amp; Image Understanding, 1995, 61 (1) :38-59." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084429&amp;v=Mjg3NDRaZVp0RmlubFVyM0lLRndYYmhvPU5pZk9mYks3SHRETnFvOUVaT01MQ0g0d29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         COOTES T F, TAYLOR C J, COOPER D H, et al.Active shape models-their training and application[J].Computer Vision &amp;amp; Image Understanding, 1995, 61 (1) :38-59.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_5" title=" COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active appearance models">
                                        <b>[5]</b>
                                         COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_6" title=" KAZEMI V, SULLIVAN J.One millisecond face alignment with an ensemble of regression trees[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1867-1874." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=One millisecond face alignment with an ensemble of regression trees">
                                        <b>[6]</b>
                                         KAZEMI V, SULLIVAN J.One millisecond face alignment with an ensemble of regression trees[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1867-1874.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_7" title=" DOLLAR P, WELINDER P, PERONA P.Cascaded pose regression[C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:1078-1085." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascaded Pose Regression">
                                        <b>[7]</b>
                                         DOLLAR P, WELINDER P, PERONA P.Cascaded pose regression[C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:1078-1085.
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_8" title=" OZUYSAL M, CALONDER M, LEPETIT V, et al.Fast keypoint recognition using random ferns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (3) :448-461." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Keypoint Recognition Using Random Ferns">
                                        <b>[8]</b>
                                         OZUYSAL M, CALONDER M, LEPETIT V, et al.Fast keypoint recognition using random ferns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (3) :448-461.
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_9" title=" CAO X, WEI Y, WEN F, et al.Face alignment by explicit shape regression[C]// Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2012:2887-2894." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face alignment by Explicit Shape Regression">
                                        <b>[9]</b>
                                         CAO X, WEI Y, WEN F, et al.Face alignment by explicit shape regression[C]// Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2012:2887-2894.
                                    </a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_10" title=" REN S, CAO X, WEI Y, et al.Face alignment at 3000 fps via regressing local binary features[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1685-1692." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face alignment at 3000 fps via regressing local binary features">
                                        <b>[10]</b>
                                         REN S, CAO X, WEI Y, et al.Face alignment at 3000 fps via regressing local binary features[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1685-1692.
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_11" title=" BURGOS-ARTIZZU X P, PERONA P, DOLLAR P.Robust face landmark estimation under occlusion[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2014:1513-1520." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust face landmark estimation under occlusion">
                                        <b>[11]</b>
                                         BURGOS-ARTIZZU X P, PERONA P, DOLLAR P.Robust face landmark estimation under occlusion[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2014:1513-1520.
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_12" title=" XIONG X, TORRE F D L.Supervised descent method and its applications to face alignment[C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2013:532-539." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised descent method and its applications to face alignment">
                                        <b>[12]</b>
                                         XIONG X, TORRE F D L.Supervised descent method and its applications to face alignment[C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2013:532-539.
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_13" title=" SUN Y, WANG X, TANG X.Deep convolutional network cascade for facial point detection[C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2013:3476-3483." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep convolutional network cascade for facial point detection">
                                        <b>[13]</b>
                                         SUN Y, WANG X, TANG X.Deep convolutional network cascade for facial point detection[C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2013:3476-3483.
                                    </a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_14" title=" ZHOU E, FAN H, CAO Z, et al.Extensive facial landmark localization with coarse-to-fine convolutional network cascade[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops.Washington, DC:IEEE Computer Society, 2013:386-391." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extensive Facial Landmark Localization with Coarse-to-Fine Convolu-tional Network Cascade">
                                        <b>[14]</b>
                                         ZHOU E, FAN H, CAO Z, et al.Extensive facial landmark localization with coarse-to-fine convolutional network cascade[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops.Washington, DC:IEEE Computer Society, 2013:386-391.
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_15" title=" 贾项南, 于凤芹, 陈莹.改进的显式形状回归人脸特征点定位算法[J].计算机应用, 2018, 38 (5) :1289-1293. (JIA X N, YU F Q, CHEN Y.Improved explicit shape regression for face alignment algorithm[J].Journal of Computer Applications, 2018, 38 (5) :1289-1293.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201805013&amp;v=MDExNjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVUx2QUx6N0JkN0c0SDluTXFvOUVaNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         贾项南, 于凤芹, 陈莹.改进的显式形状回归人脸特征点定位算法[J].计算机应用, 2018, 38 (5) :1289-1293. (JIA X N, YU F Q, CHEN Y.Improved explicit shape regression for face alignment algorithm[J].Journal of Computer Applications, 2018, 38 (5) :1289-1293.) 
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_16" title=" YU L, LIU H.Efficient feature selection via analysis of relevance and redundancy[J].Journal of Machine Learning Research, 2004, 5 (12) :1205-1224." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient feature selection via analysis of relevance and redundancy">
                                        <b>[16]</b>
                                         YU L, LIU H.Efficient feature selection via analysis of relevance and redundancy[J].Journal of Machine Learning Research, 2004, 5 (12) :1205-1224.
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_17" title=" 黄玉琴, 潘华伟.基于分类外形搜索的人脸特征点定位[J/OL].计算机应用研究, 2019, 36 (5) [2018- 09- 20].http://www.arocmag.com/article/02-2019-05-056.html. (HUANG Y Q, PAN H W.Face alignment based on classified shape searching[J/OL].Application Research of Computers, 2019, 36 (5) [2018- 09- 20].http://www.arocmag.com/article/02-2019-05-056.html.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201904063&amp;v=MTIzODdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEblVMdkFMejdTWkxHNEg5ak1xNDlEWjRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         黄玉琴, 潘华伟.基于分类外形搜索的人脸特征点定位[J/OL].计算机应用研究, 2019, 36 (5) [2018- 09- 20].http://www.arocmag.com/article/02-2019-05-056.html. (HUANG Y Q, PAN H W.Face alignment based on classified shape searching[J/OL].Application Research of Computers, 2019, 36 (5) [2018- 09- 20].http://www.arocmag.com/article/02-2019-05-056.html.) 
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_18" title=" BELHUMEUR P N, JACOBS D W, KRIEGMAN D J, et al.Localizing parts of faces using a consensus of exemplars[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (12) :2930-2940." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Localizing Parts of Faces Using a Consensus of Exemplars">
                                        <b>[18]</b>
                                         BELHUMEUR P N, JACOBS D W, KRIEGMAN D J, et al.Localizing parts of faces using a consensus of exemplars[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (12) :2930-2940.
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_19" title=" LE V, BRANDT J, LIN Z, et al.Interactive facial feature localization[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer-Verlag, 2012:679-692." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interactive facial feature localization">
                                        <b>[19]</b>
                                         LE V, BRANDT J, LIN Z, et al.Interactive facial feature localization[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer-Verlag, 2012:679-692.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-21 09:33</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1459-1465 DOI:10.11772/j.issn.1001-9081.2018102057            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>人脸特征点定位的自适应窗回归方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AD%8F%E5%98%89%E6%97%BA&amp;code=40431140&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">魏嘉旺</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%82%96&amp;code=40431141&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王肖</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E7%8E%89%E6%B3%A2&amp;code=30719092&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁玉波</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%9C%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0024290&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华东理工大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对显式形状回归 (ESR) 对于一些面部遮挡、面部表情过大样本定位精度低的问题, 提出一种自适应窗回归方法。首先, 应用先验信息为每张图片生成精确的人脸框, 用人脸框的中心点对人脸进行特征映射, 并进行相似变换得到多个初始形状;其次, 提出一种自适应窗口调整策略, 基于先前回归的均方误差自适应地调整特征窗口大小;最后, 基于互信息 (MI) 的特征选择策略, 提出新的相关性计算方法, 在候选像素集中选出最相关的特征。在三个公开数据集LFPW、HELEN、COFW上, 相较于ESR算法, 所提方法的定位精度分别提升7.52%、5.72%和5.89%。实验结果表明, 自适应窗回归方法可以有效提高人脸特征点定位精度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%98%BE%E5%BC%8F%E5%BD%A2%E7%8A%B6%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">显式形状回归;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E7%89%B9%E5%BE%81%E7%82%B9%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸特征点定位;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E4%BC%BC%E4%BA%BA%E8%84%B8%E5%8F%98%E6%8D%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相似人脸变换;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E7%AA%97%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应窗回归;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%92%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">互信息;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    魏嘉旺 (1995—) , 男, 辽宁锦州人, 硕士研究生, 主要研究方向:数据挖掘、计算机视觉;;
                                </span>
                                <span>
                                    王肖 (1995—) , 女, 甘肃天水人, 硕士研究生, 主要研究方向:数据挖掘、机器学习;;
                                </span>
                                <span>
                                    *袁玉波 (1976—) , 男, 云南宣威人, 副教授, 博士, 主要研究方向:机器学习、数据科学、数据质量评估、数据挖掘。电子邮箱ybyuan@ecust.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61001200);</span>
                                <span>上海市科研计划项目 (17DZ1101003);</span>
                    </p>
            </div>
                    <h1><b>Adaptive window regression method for face feature point positioning</b></h1>
                    <h2>
                    <span>WEI Jiawang</span>
                    <span>WANG Xiao</span>
                    <span>YUAN Yubo</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Engineering, East China University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Focused on the low positioning accuracy of Explicit Shape Regression (ESR) for some facical occlusion and excessive facial expression samples, an adaptive window regression method was proposed. Firstly, the priori information was used to generate an accurate face area box for each image, feature mapping of faces was performed by using the center point of the face area box, and similar transformation was performed to obtain multiple initial shapes. Secondly, an adaptive window adjustment strategy was given, in which the feature window size was adaptively adjusted based on the mean square error of the previous regression. Finally, based on the feature selection strategy of Mutual Information (MI) , a new correlation calculation method was proposed, and the most relevant features were selected in the candidate pixel set. On the three public datasets LFPW, HELEN and COFW, the positioning accuracy of the proposed method is increased by 7.52%, 5.72% and 5.89% respectively compared to ESR algorithm. The experimental results show that the adaptive window regression method can effectively improve the positioning accuracy of face feature points.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Explicit%20Shape%20Regression%20(ESR)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Explicit Shape Regression (ESR) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20feature%20point%20position&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face feature point position;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=similar%20face%20transformation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">similar face transformation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20window%20regression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive window regression;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Mutual%20Information%20(MI)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Mutual Information (MI) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WEI Jiawang, born in 1995, M. S. candidate. His research interests include data mining, computer vision. ;
                                </span>
                                <span>
                                    WANG Xiao, born in 1995, M. S. candidate. Her research interests include data mining, machine learning. ;
                                </span>
                                <span>
                                    YUAN Yubo, born in 1976, Ph. D. , associate professor. His research interests include machine learning, data science, data quality assessment, data mining.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61001200);</span>
                                <span>the Scientific Research Project of Shanghai (17DZ1101003);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">人脸特征点定位是目前学术界研究的焦点, 这项技术基于人脸框的精准检测<citation id="288" type="reference"><link href="249" rel="bibliography" /><link href="251" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>, 在检测到人脸框后精准定位出眼睛、鼻子、嘴巴和下巴等对于人脸识别<citation id="287" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、人脸跟踪、人脸动画和3D人脸建模等工作是必不可少的。近些年, 随着个人和网络照片的爆炸式增长, 全自动、高效和强大的人脸特征点定位方法是十分需要的。</p>
                </div>
                <div class="p1">
                    <p id="43">人脸特征点定位的算法有很多。在早期大多基于模板匹配, 不断搜索寻优。比较有代表性的是Cootes等<citation id="289" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出的主动形状模型 (Active Shape Model, ASM) , 通过手工对面部特征点进行标注, 然后将坐标连接形成几何形状, 再用传统机器学习方法, 将形状与面部特征进行匹配。由于是较早期算法, 在一些通用数据集上面的结果不是很理想。之后, Cootes等<citation id="290" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>又提出了主动外观模型 (Active Appearance Model, AAM) , 它是对ASM算法的一种改进。该方法在形状特征的基础上增加纹理特征, 融合形成外观模型, 并通过最小化模型残差进行优化。由于外观模型对捕捉复杂表情的能力有限, 因此在面部表情较大的数据集上AAM算法的效果不尽如人意。</p>
                </div>
                <div class="p1">
                    <p id="44">目前主流的人脸特征点定位算法大多基于级联回归<citation id="291" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。级联回归采用集成学习的思想, 首先训练一系列弱分类器, 前一级弱分类器的输出作为后一级弱分类器的输入, 然后串联形成一个强分类器, 逐步拟合残差。Dollar等<citation id="292" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出一种级联的姿态回归算法 (Cascaded Pose Regression, CPR) , 使用一系列随机蕨<citation id="293" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>来逐步预测对象姿态参数。Cao等<citation id="294" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>在CPR算法的基础上提出一种显式形状回归算法 (Explicit Shape Regression, ESR) , 采用无参数表示方法, 通过最小化面对齐误差, 并且使用基于稀疏编码的模型压缩, 使算法可较快速且准确地进行人脸关键点定位。Ren等<citation id="295" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>对ESR算法进行改进, 提出基于局部二值特征的算法 (Local Binary Feature, LBF) , 该方法没有直接采用随机蕨的预测结果, 而是将结果编码成0、1矩阵作为局部特征, 再将局部特征连接形成全局特征, 最后训练转换矩阵得到最终结果。Burgos-Artizzu等<citation id="296" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>针对ESR与CPR算法在大姿态和高遮挡下效果不佳的情况, 提出一种鲁棒的级联姿态回归算法 (Robust Cascaded Pose Regression, RCPR) , 在训练集上人工增加一项是否被遮挡的信息, 在训练过程中通过对被遮挡的部分赋予较小的权重, 使其对最终的结果影响较弱, 使整体定位精度有较大提高。Xiong等<citation id="297" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>对牛顿法进行改进, 提出监督下降法 (Supervised Descent Method, SDM) , 采用图像角点特征, 并对模型的下降方向和步长进行有监督学习, 使目标函数可以快速地收敛到全局最优解。</p>
                </div>
                <div class="p1">
                    <p id="45">值得一提的是, 近些年随着高性能计算设备的发展, 深度学习迎来了新一轮的浪潮。2013年Sun等<citation id="298" type="reference"><link href="273" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>首次将卷积神经网络引入到人脸特征点定位中, 搭建一组深度卷积神经网络模型 (Deep Convolutional Neural Network, DCNN) , 最终输出5个人脸特征点坐标。Zhou等<citation id="299" type="reference"><link href="275" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在DCNN算法的基础上将5个特征点扩充为68个, 并且将面部特征点分为内点和外点分别训练, 最后进行结果融合。</p>
                </div>
                <div class="p1">
                    <p id="46">从定位结果上面来看, 深度学习相较于传统模板匹配方法的提升较大, 但对比级联回归的方法提升不明显, 且会带来巨大的运算开销。由于人脸特征点定位算法通常会在移动端应用, 这就对运行速度有一定要求, 因此, 基于深度学习的算法并不是最优的选择。</p>
                </div>
                <div class="p1">
                    <p id="47">本文研究的出发点是针对级联回归方法对于一些面部遮挡、面部表情过大样本的定位精度较低的问题, 提出了一种新的自适应窗回归方法。基于相似变换的人脸初始化, 可以有效地减小由于面部尺度大小、面部偏转角度以及面部表情带来的误差;通过自适应由粗到细的窗口调节方法与基于互信息 (Mutual Information, MI) 的特征选择方法, 可以有效提取出面部最有代表性的特征, 也可以在一定程度上对面部遮挡区域进行过滤, 减少遮挡区域特征的提取, 一定程度上提高遮挡样本的定位精度。</p>
                </div>
                <h3 id="48" name="48" class="anchor-tag">1 定位技术理论和模型</h3>
                <div class="p1">
                    <p id="49">输入一张人脸图像<i>I</i>, 研究目标是输出图像中人脸重要位置的特征点, 记为:</p>
                </div>
                <div class="p1">
                    <p id="50"><b><i>S</i></b>=[<b><i>P</i></b><sub>1</sub>, <b><i>P</i></b><sub>2</sub>, …, <b><i>P</i></b><sub><i>N</i><sub>0</sub></sub>]<sup>T</sup>; <b><i>P</i></b><sub><i>j</i></sub>= (<i>x</i><sub><i>j</i></sub>, <i>y</i><sub><i>j</i></sub>) , <i>j</i>=1, 2, …, <i>N</i><sub>0</sub>      (1) </p>
                </div>
                <div class="p1">
                    <p id="51">其中: (<i>x</i><sub><i>j</i></sub>, <i>y</i><sub><i>j</i></sub>) 表示人脸特征点的横、纵坐标值, <i>N</i><sub>0</sub>表示人脸特征点的个数, 在通用数据集中一般标<citation id="300">注68</citation>或29个特征点。</p>
                </div>
                <div class="p1">
                    <p id="52">为了得到人脸特征点定位结果, 使用的手工标注的训练集记为<b><i>S</i></b>^ , 表示如下:</p>
                </div>
                <div class="area_img" id="312">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201905038_31200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="55">其中<image id="313" type="formula" href="images/JSJY201905038_31300.jpg" display="inline" placement="inline"><alt></alt></image>是标注过的人脸的特征点集合。</p>
                </div>
                <div class="p1">
                    <p id="56">假设可选择算法的空间为F, 研究的目标是在F中选择到最优的算法<i>F</i>, 使得在训练集合上的特征点差异极小, 其模型记为:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi>F</mi><mo>∈</mo><mtext>F</mtext></mrow></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></munderover><mo stretchy="false">∥</mo></mstyle></mrow></mstyle><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>-</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>^</mo><mspace width="0.25em" /></mrow><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mo>*</mo></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中:<i>F</i><sup>*</sup>表示优化目标, <b><i>P</i></b><sub><i>ij</i></sub>表示第<i>i</i>幅人脸图像的第<i>j</i>个特征点, 是算法<i>F</i>的输出;<i>M</i><sub>0</sub>表示人脸图像数量;‖·‖<sub>*</sub>表示差异的范数选择, 在算法中, 常选择2范数。</p>
                </div>
                <div class="p1">
                    <p id="59">本文选择的算法空间F是级联回归函数空间, ESR算法的技术框架分为两级, 第一级回归共有<i>K</i>个强回归器 (<i>R</i><sup>1</sup>, <i>R</i><sup>2</sup>, …, <i>R</i><sup><i>K</i></sup>) , 强回归器的功能是在人脸图片上面提取候选像素点, 对于每一个强回归器<i>R</i><sup><i>k</i></sup>, 由<i>L</i>个第二级弱回归器 (<i>r</i><sup>1</sup>, <i>r</i><sup>2</sup>, …, <i>r</i><sup><i>L</i></sup>) 组成。弱回归器的功能是在候选像素点集中选出最优特征值, 然后更新形状。整个算法迭代模型为:</p>
                </div>
                <div class="p1">
                    <p id="60"><i>S</i><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>=<i>S</i><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>+<i>R</i><sup><i>k</i></sup> (<i>I</i><sub><i>i</i></sub>, <i>S</i><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>) ;<i>i</i>=1, 2, …, <i>M</i><sub>0</sub>, <i>k</i>=1, 2, …, <i>K</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="64">其中:<i>S</i><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>表示第<i>i</i>个人脸图像样本在第<i>k</i>次回归后的预测形状; <i>R</i><sup><i>k</i></sup>表示第<i>k</i>个强回归器, 它的输入为第<i>i</i>个样本图片<i>I</i><sub><i>i</i></sub>和前一次回归的预测形状<i>S</i><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>, 它的输出为形状增量。</p>
                </div>
                <div class="p1">
                    <p id="67">第<i>i</i>个样本的候选像素集是在图片<i>I</i><sub><i>i</i></sub>的人脸框范围内随机选择<i>N</i><sub>1</sub>个像素点, 分别找到距离各像素点最近的特征点, 记下特征点编号, 算出像素点坐标对应于该特征点坐标在<i>x</i>、<i>y</i>方向上的偏移。得到<i>N</i><sub>1</sub>个像素点后, 再将它们之间的灰度值两两作差, 形成<i>N</i><sub>1</sub>×<i>N</i><sub>1</sub>个像素差。然后将上一级回归器的定位误差矩阵右乘一个随机向量, 得到要预测的误差值。这里的误差矩阵表示预测的坐标点与手工标注的坐标点之间的欧氏距离。使用相关性公式, 求出与误差值相关性最大的一个像素差, 循环<i>N</i><sub>2</sub>次, 得到<i>N</i><sub>2</sub>个像素差。</p>
                </div>
                <div class="p1">
                    <p id="68">通过<i>N</i><sub>2</sub>个像素差, 构建深度为<i>N</i><sub>2</sub>, 叶节点个数为2<sup><i>N</i><sub>2</sub></sup>的随机蕨。每一层随机蕨都会随机设置一个阈值, 节点分裂的规则为, 若第<i>f</i>个像素差的值小于第<i>f</i>层阈值, 则落入左子节点, 反之落入右子节点。最后样本<i>i</i>会落入到一个叶节点中, 设叶节点为<i>b</i>, 叶节点的值<i>E</i><sub><i>b</i></sub>由所有落入该叶节点的样本共同决定:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mi>b</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>β</mi><mo>/</mo><mo stretchy="false">|</mo><mi>φ</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">|</mo></mrow></mfrac><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></munderover><mo stretchy="false">∥</mo></mstyle></mrow></mstyle><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>-</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><mo>^</mo><mspace width="0.25em" /></mrow><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mo stretchy="false">|</mo><mi>φ</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">|</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中:|<i>φ</i><sub><i>b</i></sub>|表示所有落入叶节点<i>b</i>的样本个数, <i>β</i>是一个缩放因子, 用来约束整个公式, 防止|<i>φ</i><sub><i>b</i></sub>|过小而造成过拟合。 <i>β</i>的取值与样本规模有关, 本文将<i>β</i>设为1 000。</p>
                </div>
                <div class="p1">
                    <p id="71">本文算法的技术路线如图1所示。</p>
                </div>
                <div class="p1">
                    <p id="72">下面分别阐述三个技术关键内容:1) 基于相似变换的人脸初始化;2) 自适应由粗到细的窗口调节方法;3) 基于互信息的特征选择方法。通过三个方面对ESR算法进行改进。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">1.1 <b>基于相似变换的人脸初始化</b></h4>
                <div class="p1">
                    <p id="74">基于级联回归的特征点定位方法, 无论是在训练过程中还是测试过程中, 在第一级回归时都要事先生成一个初始人脸形状<citation id="301" type="reference"><link href="277" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 作为第一级回归的输入。后面每一级的输入为前一级回归的输出。对于初始人脸形状的选择方法有很多, 有的学者对训练集样本直接取平均作为初始形状。这种方法简单易行, 但由于人脸大小不一, 直接取平均的效果不是很好。本文采用一种基于相似变换的人脸初始化方法。</p>
                </div>
                <div class="p1">
                    <p id="75">首先, 根据样本<i>i</i>的特征点标记信息生成样本<i>i</i>的人脸框。方法为:先找到特征点中<i>x</i>坐标值最小的关键点坐标 (<i>x</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>, <i>y</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>) , 然后依次找到<i>x</i>坐标值最大的、<i>y</i>坐标值最小的、<i>y</i>坐标值最大的关键点坐标 (<i>x</i><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>, <i>y</i><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>) 、 (<i>x</i><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mi>i</mi></msubsup></mrow></math></mathml>, <i>y</i><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mi>i</mi></msubsup></mrow></math></mathml>) 、 (<i>x</i><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>4</mn><mi>i</mi></msubsup></mrow></math></mathml>, <i>y</i><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>4</mn><mi>i</mi></msubsup></mrow></math></mathml>) 。</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>W</mi><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mi>x</mi><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup><mo>-</mo><mi>x</mi><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mtd></mtr><mtr><mtd><mi>Η</mi><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mi>y</mi><msubsup><mrow></mrow><mn>4</mn><mi>i</mi></msubsup><mo>-</mo><mi>y</mi><msubsup><mrow></mrow><mn>3</mn><mi>i</mi></msubsup></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">其中:<i>W</i><sup><i>i</i></sup>表示人脸形状<i>i</i>的宽度, <i>H</i><sup><i>i</i></sup>表示人脸形状<i>i</i>的高度。根据经验, 通常取人脸框的宽度为1.5倍人脸形状的宽度, 人脸框的高度为1.5倍人脸形状的高度。获得人脸框的方法为:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>x</mi><msubsup><mrow></mrow><mn>0</mn><mi>i</mi></msubsup><mo>=</mo><mi>x</mi><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup><mo>-</mo><mi>W</mi><msup><mrow></mrow><mi>i</mi></msup><mo>/</mo><mn>4</mn></mtd></mtr><mtr><mtd><mi>y</mi><msubsup><mrow></mrow><mn>0</mn><mi>i</mi></msubsup><mo>=</mo><mi>y</mi><msubsup><mrow></mrow><mn>3</mn><mi>i</mi></msubsup><mo>-</mo><mi>Η</mi><msup><mrow></mrow><mi>i</mi></msup><mo>/</mo><mn>4</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">其中:<i>x</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>0</mn><mi>i</mi></msubsup></mrow></math></mathml>、<i>y</i><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>0</mn><mi>i</mi></msubsup></mrow></math></mathml>为人脸框<i>i</i>左上角顶点的横坐标值和纵坐标值。根据人脸框左上角顶点坐标和人脸框的宽度和高度, 可以唯一确定样本<i>i</i>的人脸框。</p>
                </div>
                <div class="p1">
                    <p id="90">然后, 在训练集中随机选择<i>M</i>个样本, 对于每个随机选择的样本<i>m</i>, 根据其人脸框对每个特征点<i>j</i>进行归一化, 方法为:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup><mo>=</mo><mfrac><mrow><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup><mo>-</mo><mover accent="true"><mi>X</mi><mo>˙</mo></mover><msup><mrow></mrow><mi>m</mi></msup></mrow><mrow><mi>W</mi><msup><mrow></mrow><mi>m</mi></msup><mo>/</mo><mn>2</mn></mrow></mfrac></mtd></mtr><mtr><mtd><mover accent="true"><mi>Y</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup><mo>=</mo><mfrac><mrow><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup><mo>-</mo><mover accent="true"><mi>Y</mi><mo>˙</mo></mover><msup><mrow></mrow><mi>m</mi></msup></mrow><mrow><mi>Η</mi><msup><mrow></mrow><mi>m</mi></msup><mo>/</mo><mn>2</mn></mrow></mfrac></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">其中:<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>¯</mo></mover></math></mathml><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup></mrow></math></mathml>、<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Y</mi><mo>¯</mo></mover></math></mathml><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup></mrow></math></mathml>为归一化后样本<i>m</i>特征点<i>j</i>的横、纵坐标值;<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>˙</mo></mover></math></mathml><sup><i>m</i></sup>、<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Y</mi><mo>˙</mo></mover></math></mathml><sup><i>m</i></sup>为样本<i>m</i>的人脸框的中心点横、纵坐标值。</p>
                </div>
                <div class="p1">
                    <p id="99">最后, 对归一化后的样本<i>m</i>特征点, 根据样本<i>i</i>的人脸框进行相似变换, 方法为:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mi>m</mi><mi>i</mi></mrow></msubsup><mo>=</mo><mfrac><mrow><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup><mo>*</mo><mi>W</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mn>2</mn></mfrac><mo>+</mo><mover accent="true"><mi>X</mi><mo>˙</mo></mover><msup><mrow></mrow><mi>i</mi></msup></mtd></mtr><mtr><mtd><mover accent="true"><mi>Y</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mi>m</mi><mi>i</mi></mrow></msubsup><mo>=</mo><mfrac><mrow><mover accent="true"><mi>Y</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup><mo>*</mo><mi>Η</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mn>2</mn></mfrac><mo>+</mo><mover accent="true"><mi>Y</mi><mo>˙</mo></mover><msup><mrow></mrow><mi>i</mi></msup></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">其中: (<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>˜</mo></mover></math></mathml><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>m</mi><mi>i</mi></mrow></msubsup></mrow></math></mathml>, <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Y</mi><mo>˜</mo></mover></math></mathml><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>m</mi><mi>i</mi></mrow></msubsup></mrow></math></mathml>) 就是通过相似变换获得的样本<i>m</i>相对于样本<i>i</i>人脸框的初始化人脸形状中特征点的坐标。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905038_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 自适应窗回归算法技术路线" src="Detail/GetImg?filename=images/JSJY201905038_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 自适应窗回归算法技术路线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905038_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Technology roadmap of adaptive window regression algorithm</p>

                </div>
                <h4 class="anchor-tag" id="107" name="107">1.2 <b>自适应由粗到细的窗口调节方法</b></h4>
                <div class="p1">
                    <p id="108"><i>ESR</i>算法的候选像素点选自整个人脸框。对于多级级联回归, 在前几级回归中, 预测形状与真实形状的偏差较大, 这时, 用距离特征点较远的像素点作为输入特征可以更好地预测定位残差。而随着回归次数的增加, 预测形状与真实形状逐渐接近, 这时, 只有特征点附近的像素点才可以更好地预测细微的定位残差。</p>
                </div>
                <div class="p1">
                    <p id="109">本文提出一种自适应由粗到细的特征选择方法, 基于先前回归的均方误差自适应地调整特征搜索窗口大小。在误差较大时, 窗口也较大。窗口缩小的速率与误差减小的速率相关, 当误差减小的速率很低时, 就认为预测形状已经很接近真实形状, 这时只需要对预测形状进行微调, 同时搜索窗口的变化也应该很小。</p>
                </div>
                <div class="p1">
                    <p id="110">对于每一个特征点 (<i>x</i><sub><i>j</i></sub>, <i>y</i><sub><i>j</i></sub>) , 特征窗口的范围是 (<i>x</i><sub><i>j</i></sub>+<i>Δ</i>, <i>y</i><sub><i>j</i></sub>+<i>Δ</i>) , 其中<i>Δ</i>∈[-<i>γ</i>, <i>γ</i>] (<i>γ</i>&gt;0) 。设初始形状<i>S</i><sup>0</sup>与真实形状<b><i>S</i></b>^ 之间的均方误差为<i>err</i><sup>0</sup>, 对于级联回归 (<i>R</i><sup>1</sup>, <i>R</i><sup>2</sup>, …, <i>R</i><sup><i>K</i></sup>) 。每一次回归后都会得到一个预测误差 (<i>err</i><sup>1</sup>, <i>err</i><sup>2</sup>, …, <i>err</i><sup><i>K</i></sup>) 。根据预测误差动态地去调整特征窗口的大小。</p>
                </div>
                <div class="p1">
                    <p id="111">首先给出双眼距离<i>δ</i>的计算公式:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>δ</mi><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><msqrt><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>3</mn><mn>7</mn></mrow><mrow><mn>4</mn><mn>2</mn></mrow></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mi>x</mi><mi>j</mi></msubsup></mrow><mn>6</mn></mfrac><mo>-</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>4</mn><mn>3</mn></mrow><mrow><mn>4</mn><mn>8</mn></mrow></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mi>x</mi><mi>j</mi></msubsup></mrow><mn>6</mn></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>3</mn><mn>7</mn></mrow><mrow><mn>4</mn><mn>2</mn></mrow></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mi>y</mi><mi>j</mi></msubsup></mrow><mn>6</mn></mfrac><mo>-</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>4</mn><mn>3</mn></mrow><mrow><mn>4</mn><mn>8</mn></mrow></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mi>y</mi><mi>j</mi></msubsup></mrow><mn>6</mn></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub><mo>=</mo><mn>6</mn><mn>8</mn></mtd></mtr><mtr><mtd><msqrt><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>9</mn></mrow><mrow><mi>α</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mi>x</mi><mi>j</mi></msubsup></mrow><mn>5</mn></mfrac><mo>-</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mn>0</mn></mrow><mrow><mi>α</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mi>x</mi><mi>j</mi></msubsup></mrow><mn>5</mn></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>9</mn></mrow><mrow><mi>α</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mi>y</mi><mi>j</mi></msubsup></mrow><mn>5</mn></mfrac><mo>-</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mn>0</mn></mrow><mrow><mi>α</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mi>y</mi><mi>j</mi></msubsup></mrow><mn>5</mn></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub><mo>=</mo><mn>2</mn><mn>9</mn></mtd></mtr></mtable></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">其中:<i>N</i><sub>0</sub>为人脸框中特征点个数, <i>l</i><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>j</mi></msubsup></mrow></math></mathml>为第<i>j</i>个坐标点的<i>x</i>坐标值, <i>l</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>y</mi><mi>j</mi></msubsup></mrow></math></mathml>为第<i>j</i>个坐标点的<i>y</i>坐标值。<i>α</i><sub>1</sub>={9, 11, 13, 14, 17}, <i>α</i><sub>2</sub>={10, 12, 15, 16, 18}分别表示左眼和右眼周围多个特征点编号。</p>
                </div>
                <div class="p1">
                    <p id="116">根据多次实验结果本文将第一级回归的特征窗口长度<i>γ</i><sup>1</sup>设置为0.3倍双眼距离, 特征窗口长度<i>γ</i>的更新公式为:</p>
                </div>
                <div class="p1">
                    <p id="117"><i>γ</i><sup>1</sup>=0.3*<i>δ</i>      (11) </p>
                </div>
                <div class="p1">
                    <p id="118"><i>γ</i><sup><i>k</i></sup>=<i>γ</i><sup><i>k</i>-1</sup>-<i>ξθ</i>;<i>k</i>=2, 3, …, <i>K</i>       (12) </p>
                </div>
                <div class="p1">
                    <p id="119">其中:<i>γ</i><sup><i>k</i></sup>为当前回归的窗口长度; <i>γ</i><sup><i>k</i>-1</sup>为前一级回归的窗口长度; <i>ξ</i>为缩减参数; <i>θ</i>为常量因子, 根据多次实验结果取 0.025。</p>
                </div>
                <div class="p1">
                    <p id="120">缩减参数的更新基于级联回归的预测误差, 给出<i>ξ</i>的更新公式:</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ξ</mi><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>e</mi><mi>r</mi><mi>r</mi><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>2</mn></mrow></msup><mo>-</mo><mi>e</mi><mi>r</mi><mi>r</mi><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">) </mo><mo>/</mo><mi>e</mi><mi>r</mi><mi>r</mi><msup><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mrow><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>k</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>e</mi><mi>r</mi><mi>r</mi><msup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>2</mn></mrow></msup><mo>-</mo><mi>e</mi><mi>r</mi><mi>r</mi><msup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">) </mo><mo>/</mo><mi>e</mi><mi>r</mi><mi>r</mi><msup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="false">]</mo><mo>/</mo></mrow><mo stretchy="false">|</mo><mi>k</mi><mo>-</mo><mn>1</mn><mo stretchy="false">|</mo></mrow></mfrac><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122"><i>k</i>=2, 3, …, <i>K</i>      (13) </p>
                </div>
                <div class="p1">
                    <p id="123">其中:分子为<i>k</i>-2次回归与<i>k</i>-1次回归的均方误差之差与<i>k</i>-1次回归的均方误差的比值; 分母为先前各级回归的误差比值的平均值。该公式度量的是当前回归误差下降的速率与先前回归误差下降速率的均值的比值。配合上常量因子<i>θ</i>, 当比值大时, 窗口缩小的速率就较大; 当比值小时, 窗口缩小的速率就较小。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">1.3 <b>基于互信息的特征选择方法</b></h4>
                <div class="p1">
                    <p id="125">对于每一次级联回归<i>R</i><sup><i>k</i></sup>, 根据上面自适应由粗到细的窗口调节方法会得到一个窗口长度<i>γ</i>, 首先在所有特征点中随机选择一个特征点 (<i>x</i><sub><i>j</i></sub>, <i>y</i><sub><i>j</i></sub>) , 然后在 (<i>x</i><sub><i>j</i></sub>+<i>Δ</i>, <i>y</i><sub><i>j</i></sub>+<i>Δ</i>) 范围内随机选择一个像素点, 其中<i>Δ</i>∈[-<i>γ</i>, <i>γ</i>] (<i>γ</i>&gt;0) 。更换特征点, 不断迭代<i>N</i><sub>1</sub>次, 可以得到<i>N</i><sub>1</sub>个像素点。再将它们之间的灰度值两两作差, 可以形成<i>N</i><sub>1</sub>×<i>N</i><sub>1</sub>个像素差。本文使用基于互信息<citation id="302" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的特征选择方法, 从<i>N</i><sub>1</sub>×<i>N</i><sub>1</sub>个像素差中选出<i>N</i><sub>2</sub>个最有代表性的像素差。</p>
                </div>
                <div class="p1">
                    <p id="126">ESR算法的特征选择是基于线性相关系数, 通过求出<i>N</i><sub>1</sub>×<i>N</i><sub>1</sub>个像素差与特征点定位残差之间的线性相关系数, 并选择相关系数最大的<i>N</i><sub>2</sub>个像素差作为候选特征;然而, 在实际情况中, 并不能保证像素差与特征点定位残差之间一定是线性相关的, 因此, 本文采用互信息并引入熵相关系数<citation id="303" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>进行特征选择, 这样即使像素差与特征点定位残差之间是非线性相关的也可以进行特征选择。</p>
                </div>
                <div class="p1">
                    <p id="127">首先, 给出任意一个像素差<i>μ</i>的信息熵<i>H</i> (<i>μ</i>) 的概念。</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false"> (</mo><mi>μ</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mo stretchy="false">[</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">其中:<i>P</i> (<i>μ</i><sub><i>m</i></sub>) 表示像素差<i>μ</i>取第<i>m</i>个值的概率。</p>
                </div>
                <div class="p1">
                    <p id="130">令<i>Y</i>表示特征点定位残差值。由于, 当前定位残差<i>E</i>的维度为 (1×2<i>N</i><sub>0</sub>) , 所以要先对其进行处理, 方法为:将其右乘一个 (2<i>N</i><sub>0</sub>×1) 的随机向量<b><i>v</i></b>, 得到定位残差值<i>Y</i>。向量<b><i>v</i></b>的值为随机生成。给出已知像素差<i>μ</i>, 特征点定位残差值<i>Y</i>的条件信息熵<i>H</i> (<i>Y</i>|<i>μ</i>) 的计算方法:</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">|</mo><mi>μ</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>n</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>m</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>μ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mo stretchy="false">[</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>μ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">其中:<i>P</i> (<i>Y</i><sub><i>n</i></sub>) 表示特征点定位残差<i>Y</i>取第<i>n</i>个值的概率, <i>P</i> (<i>Y</i><sub><i>n</i></sub>|<i>μ</i><sub><i>m</i></sub>) 表示像素差<i>μ</i>取第<i>m</i>个值时特征点定位残差<i>Y</i>取第<i>n</i>个值的条件概率。</p>
                </div>
                <div class="p1">
                    <p id="133">特征点定位残差<i>Y</i>与像素差<i>μ</i>之间的互信息<i>I</i> (<i>Y</i>, <i>μ</i>) 可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ι</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mspace width="0.25em" /><mi>μ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">|</mo><mi>μ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>μ</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>μ</mi><mo stretchy="false">|</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>Y</mi><mo>, </mo><mspace width="0.25em" /><mi>μ</mi></mrow></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mspace width="0.25em" /><mi>μ</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mrow><mo>[</mo><mrow><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mspace width="0.25em" /><mi>μ</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>μ</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135">其中:<i>P</i> (<i>Y</i>, <i>μ</i>) 表示特征点定位残差<i>Y</i>与像素差<i>μ</i>的联合概率密度函数。</p>
                </div>
                <div class="p1">
                    <p id="136">直接使用互信息进行相关性判断会受到某类信息熵过大的影响, 因此引入熵相关系数。特征点定位残差<i>Y</i>与像素差<i>μ</i>之间的熵相关系数<i>ECC</i> (<i>Y</i>, <i>μ</i>) 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>C</mi><mi>C</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mspace width="0.25em" /><mi>μ</mi><mo stretchy="false">) </mo><mo>=</mo><mn>2</mn><mrow><mo>[</mo><mrow><mfrac><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mspace width="0.25em" /><mi>μ</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Η</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>μ</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">当特征点定位残差<i>Y</i>与像素差<i>μ</i>之间不相关时<i>ECC</i> (<i>Y</i>, <i>μ</i>) =0, 当特征点定位残差<i>Y</i>与像素差<i>μ</i>之间完全相关时<i>ECC</i> (<i>Y</i>, <i>μ</i>) =1, 当特征点定位残差<i>Y</i>与像素差<i>μ</i>之间部分相关时<i>ECC</i> (<i>Y</i>, <i>μ</i>) ∈[0, 1]。</p>
                </div>
                <h3 id="139" name="139" class="anchor-tag">2 算法伪代码</h3>
                <div class="p1">
                    <p id="140">为了清晰表达技术内容, 本文以算法伪代码的形式列出技术内容如下:</p>
                </div>
                <div class="p1">
                    <p id="141">由于训练样本数量较少, 而自适应窗回归方法需要学习的参数较多, 如果不进行样本扩充, 容易造成模型过拟合。故提出一种基于相似变换的人脸样本扩充算法, 如算法1所示, 通过与训练集中<i>M</i>个不同人脸进行相似变换, 可将样本数量扩大<i>M</i>倍, 增加算法的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="142">算法1 基于相似变换的人脸初始化。</p>
                </div>
                <div class="area_img" id="308">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201905038_30800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="308">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201905038_30801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="159">自适应窗回归方法在每次级联回归 (<i>R</i><sup>1</sup>, <i>R</i><sup>2</sup>, …, <i>R</i><sup><i>K</i></sup>) 前需要确定特征搜索窗口的大小, 而初始窗口设置过大或过小都会对结果产生影响。本文提出一种动态调节窗口的策略, 如算法2所示, 根据先前回归的误差来调整窗口的大小。</p>
                </div>
                <div class="p1">
                    <p id="160">算法2 自适应由粗到细的窗口调节。</p>
                </div>
                <div class="area_img" id="309">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201905038_30900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="175">得到特征搜索窗口后, 需要在窗口内选择最相关的像素差特征, 本文以熵相关系数作为评价指标, 通过将目标输出与不同的随机向量相乘可以得到多个不同的像素差特征。如算法3所示, 随机生成<i>N</i><sub>2</sub>个随机向量, 将其与当前定位残差相乘, 分别计算熵相关系数并选出相关系数最大的像素差, 这些像素差作为后面弱回归器的输入。</p>
                </div>
                <div class="p1">
                    <p id="176">算法3 基于互信息的特征选择。</p>
                </div>
                <div class="area_img" id="310">
                                <img alt="" src="Detail/GetImg?filename=images/JSJY201905038_31000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="201" name="201" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="202" name="202">3.1 <b>实验环境</b></h4>
                <div class="p1">
                    <p id="203">本实验的运行环境为:<i>Windows</i>10 64位操作系统, 8 <i>GB</i>内存, <i>Intel Core i</i>7-4700<i>MQ</i> 2.4 <i>GHz</i>中央处理器, <i>Matlab R</i>2016<i>a</i>工作站。实验数据选用人脸特征点定位领域三大通用数据集:<i>LFPW</i><citation id="304" type="reference"><link href="283" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、<i>HELEN</i><citation id="305" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、<i>COFW</i><citation id="306" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。<i>LFPW</i>是一个轻量级的图片库, 图片全部来源于网络, 包含各种姿态以及不同室外环境下的人脸图片, 共有1 035张图片。本文选择其中811张图片作为训练集, 224张图片作为测试集, 每张图片上面标记了68个特征点坐标。<i>HELEN</i>图片库共有2 330张图片, 同样来源于网络。本文选取2 000张作为训练图片, 其余330张作为测试图片, 每张图片标<citation id="307" type="reference">注68</citation>个特征点。<i>COFW</i>数据集是一个高遮挡且大姿态的数据集, 由现实世界中有挑战性的几类图片组成, 例如:戴墨镜的图片、戴帽子的图片、夸张表情的图片以及大角度的侧脸图片等。该数据集可用来测试算法的鲁棒性, 共有1 852张图片, 本文选择1 345张图片作为训练集, 剩余507张图片作为测试集, 每张图片标记29个特征点坐标。</p>
                </div>
                <h4 class="anchor-tag" id="204" name="204">3.2 <b>实验细节</b></h4>
                <div class="p1">
                    <p id="205">参数设置 人脸初始化形状个数<i>M</i>设为10, 初始特征选择窗口的长度为0.3倍双眼距离, 第一级级联回归器的个数<i>K</i>设为10, 第二级回归器的个数<i>L</i>设为500, 每次级联回归在人脸框范围内选择的候选像素点个数<i>N</i><sub>1</sub>设为500, 在候选像素点中选出最相关的像素差个数<i>N</i><sub>2</sub>设为5, 式 (5) 中的<i>β</i>设为1 000, 式 (12) 中的<i>θ</i>设为0.025。</p>
                </div>
                <div class="p1">
                    <p id="206">衡量指标 为了衡量不同人脸定位的误差, 采用了一种对于不同尺寸人脸均适用的误差评价指标:</p>
                </div>
                <div class="p1">
                    <p id="207" class="code-formula">
                        <mathml id="207"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>e</mi><mi>r</mi><mi>r</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><mo>*</mo><mi>Ν</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>^</mo><mspace width="0.25em" /></mrow><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow><mi>δ</mi></mfrac></mrow></mstyle><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="208">其中:<i>N</i>为测试图片样本个数, <i>N</i><sub>0</sub>为测试图片特征点个数, <b><i>S</i></b><sub><i>i</i></sub>表示第<i>i</i>个测试图片的预测形状, <b><i>S</i></b>^ <sub><i>i</i></sub>表示第<i>i</i>个测试图片的真实形状, <i>δ</i>为人脸图片双眼间距。为了使结果更有表现力, 将结果扩大100倍。</p>
                </div>
                <h4 class="anchor-tag" id="209" name="209">3.3 <b>实验结果</b></h4>
                <h4 class="anchor-tag" id="210" name="210">3.3.1 与现存方法对比</h4>
                <div class="p1">
                    <p id="211">为了验证本文提出的改进算法是有效的, 本文分别在<i>LFPW</i>、<i>HELEN</i>、<i>COFW</i>数据集上与<i>ESR</i>算法进行对比。为了综合衡量本算法的效果, 本文还选择了当前特征点定位领域最先进的三个算法:<i>RCPR</i>算法、<i>SDM</i>算法、<i>LBF</i>算法进行对比。由表1可知:本文算法在<i>LFPW</i>和<i>HELEN</i>数据集上面的定位误差都要小于上述四个算法, 在<i>COFW</i>数据库上面的定位误差略大于<i>RCPR</i>算法。这是由于<i>RCPR</i>算法针对大面积遮挡与丰富面部表情的数据样本进行算法优化, 引入了样本特征点是否遮挡这一先验信息, 而<i>COFW</i>数据集上面大多数都是带有遮挡的样本, 故<i>RCPR</i>算法在<i>COFW</i>数据集上面效果很好。本文算法通过相似变换的人脸初始化, 可以有效减小面部尺度大小、面部偏转角度以及面部表情带来的误差;通过自适应由粗到细的窗口调节方法与基于互信息的特征选择方法, 可以有效地提取出面部最有代表性的特征, 也可以在一定程度上对面部遮挡区域进行过滤, 减少遮挡区域特征的提取, 提高遮挡样本的定位精度。</p>
                </div>
                <div class="area_img" id="212">
                    <p class="img_tit"><b>表</b>1 <b>不同算法定位误差比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Positioning error comparison of different algorithms</i></p>
                    <p class="img_note"></p>
                    <table id="212" border="1"><tr><td><br />算法</td><td><i>LFPW</i></td><td><i>HELEN</i></td><td><i>COFW</i></td></tr><tr><td><br /><i>ESR</i>算法</td><td>6.11</td><td>8.03</td><td>10.17</td></tr><tr><td><br /><i>RCPR</i>算法</td><td>6.05</td><td>7.79</td><td>9.26</td></tr><tr><td><br /><i>SDM</i>算法</td><td>5.87</td><td>7.71</td><td>10.03</td></tr><tr><td><br /><i>LBF</i>算法</td><td>5.91</td><td>7.62</td><td>9.96</td></tr><tr><td><br />本文算法</td><td>5.65</td><td>7.57</td><td>9.57</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="213">图2给出了各个算法在<i>LFPW</i>数据集上运行结果对比。从图中样例1可以看出, 本文改进算法已经十分接近样本的手工标注结果, 而其他算法都存在较大的误差, 只有脸部轮廓定位正确, 五官严重移位; 样例2, 本文改进算法的定位结果依旧优于其余算法, 脸部轮廓定位准确, 五官定位精准;样例3和样例4, 各个算法的定位结果都大体相同, 然而在大多数轮廓点的定位上, 本文的改进算法更加接近手工标注点的位置。</p>
                </div>
                <div class="area_img" id="214">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905038_214.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 LFPW库各算法运行结果" src="Detail/GetImg?filename=images/JSJY201905038_214.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>LFPW</i>库各算法运行结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905038_214.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Experimental results of each algorithm on LFPW dataset</i></p>

                </div>
                <h4 class="anchor-tag" id="215" name="215">3.3.2 自适应窗口对比</h4>
                <div class="p1">
                    <p id="216">本文提出一种自适应由粗到细的窗口调节方法, 基于先前回归的均方误差自适应地调整特征搜索窗口大小。为了验证其有效性, 本文分别在三个数据集上将自适应窗口与固定窗口进行对比。首先固定缩减参数<i>ξ</i>的值, <i>ξ</i>可取任意值, 本文分别取<i>ξ</i>为1和2, 然后使用式 (13) 中自适应的<i>ξ</i>取值方法与之对比。实验结果如表2所示。在三个数据集上的实验结果表明:本文提出的自适应窗口调节方法, 要优于直接固定缩减参数。直接固定缩减参数的问题在于:首先, 无法找到一个最佳的缩减参数值, 而随机选择一个值很容易得到较差的结果;其次, 固定一个值将会导致在前几级和后几级回归时窗口缩减速率相同, 不能灵活地应对变化, 无法收敛到全局最优。</p>
                </div>
                <div class="area_img" id="217">
                    <p class="img_tit"><b>表</b>2 <b>不同窗口策略定位误差比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Positioning error comparison of different window strategies</p>
                    <p class="img_note"></p>
                    <table id="217" border="1"><tr><td><br />数据库</td><td>固定缩减参数 (<i>ξ</i>=1) </td><td>固定缩减参数 (<i>ξ</i>=2) </td><td>自适应窗口</td></tr><tr><td><br />LFPW</td><td>5.87</td><td>5.98</td><td>5.65</td></tr><tr><td><br />HELEN</td><td>7.82</td><td>7.88</td><td>7.57</td></tr><tr><td><br />COFW</td><td>9.86</td><td>10.04</td><td>9.57</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="218" name="218">3.3.3 各算法正确率对比</h4>
                <div class="p1">
                    <p id="219">目前有很多学者提出特征点定位正确率的概念:有些学者认为当特征点定位误差小于0.25时认为该点定位成功;也有些学者认为定位误差小于0.15时才视为正确。本文为了综合衡量不同误差指标下的定位效果, 分别设置了9组递增的阈值, 并计算出在9组阈值下各个算法的定位正确率。表3给出在<i>LFPW</i>数据集上各算法的定位正确率。可以看到:本文算法在阈值为0.19时首先达到了100%的定位正确率, 领先其余四个算法;并且在各个阈值上, 本文算法均表现出较高的正确率。</p>
                </div>
                <div class="area_img" id="220">
                    <p class="img_tit"><b>表</b>3 <i>LFPW</i><b>上不同算法定位正确率比较</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 3 <i>Positioning accuracy comparison of</i><i>different algorithms on LFPW dataset</i> %</p>
                    <p class="img_note"></p>
                    <table id="220" border="1"><tr><td rowspan="2"><br />误差<br />指标</td><td colspan="5"><br />算法</td></tr><tr><td><br /><i>ESR</i></td><td><i>RCPR</i></td><td><i>SDM</i></td><td><i>LBF</i></td><td>本文算法</td></tr><tr><td><br />&lt;0.01</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td><br />&lt;0.04</td><td>31.9</td><td>34.2</td><td>35.6</td><td>36.6</td><td>39.3</td></tr><tr><td><br />&lt;0.07</td><td>82.4</td><td>84.7</td><td>84.2</td><td>84.7</td><td>85.6</td></tr><tr><td><br />&lt;0.10</td><td>95.7</td><td>96.8</td><td>95.9</td><td>95.9</td><td>96.1</td></tr><tr><td><br />&lt;0.13</td><td>98.1</td><td>98.3</td><td>98.1</td><td>97.4</td><td>98.3</td></tr><tr><td><br />&lt;0.16</td><td>99.2</td><td>98.4</td><td>99.7</td><td>99.3</td><td>99.6</td></tr><tr><td><br />&lt;0.19</td><td>99.6</td><td>98.9</td><td>99.7</td><td>99.6</td><td>100.0</td></tr><tr><td><br />&lt;0.22</td><td>99.6</td><td>99.7</td><td>100.0</td><td>100.0</td><td>100.0</td></tr><tr><td><br />&lt;0.25</td><td>100.0</td><td>99.7</td><td>100.0</td><td>100.0</td><td>100.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="221" name="221">3.3.4 各算法运行速度对比</h4>
                <div class="p1">
                    <p id="222">表4给出了在<i>LFPW</i>库各个算法的运行速度。这里的运行速度针对测试图片, 用帧率进行表示, 即模型每秒可以处理图片的个数。本文算法运行速度达到每秒140帧, 优于<i>RCPR</i>算法、<i>SDM</i>算法;相较于<i>ESR</i>算法, 优化了人脸归一化方式, 改变了候选特征选择方法, 使得运行速度得到提升;由于<i>LBF</i>算法对像素特征进行稀疏编码, 并训练转换矩阵, 其在运行速度方面优于本算法。</p>
                </div>
                <h4 class="anchor-tag" id="223" name="223">3.3.5 程序运行效果</h4>
                <div class="p1">
                    <p id="224">本文算法在<i>LFPW</i>、<i>HELEN</i>、<i>COFW</i>数据集上的运行结果如图3, 其中:图片上面标有方框的代表定位失败。对于<i>LFPW</i>与<i>HELEN</i>数据集上的大部分图片, 本文算法均可以实现面部轮廓与五官的精准定位;对于一些具有挑战的数据集, 例如<i>COFW</i>数据集, 存在大量大面积遮挡与丰富面部表情的样本, 本文算法也可以实现对五官较为精准的定位。综上所述, 本文算法具备对人脸图片进行人脸特征点精准定位的能力。</p>
                </div>
                <div class="area_img" id="225">
                    <p class="img_tit"><b>表</b>4 <i>LFPW</i><b>库各算法运行速度比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 4 <i>Running speed comparison of</i><i>different algorithms on LFPW dataset</i></p>
                    <p class="img_note"></p>
                    <table id="225" border="1"><tr><td><br />算法</td><td>误差</td><td>帧率/ (<i>frame</i>·<i>s</i><sup>-1</sup>) </td></tr><tr><td><br /><i>ESR</i>算法</td><td>6.11</td><td>110</td></tr><tr><td><br /><i>RCPR</i>算法</td><td>6.05</td><td>90</td></tr><tr><td><br /><i>SDM</i>算法</td><td>5.87</td><td>55</td></tr><tr><td><br /><i>LBF</i>算法</td><td>5.91</td><td>190</td></tr><tr><td><br />本文算法</td><td>5.65</td><td>140</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="226">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905038_226.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法运行结果" src="Detail/GetImg?filename=images/JSJY201905038_226.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法运行结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905038_226.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Experimental results of the proposed algorithm</i></p>

                </div>
                <h3 id="227" name="227" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="228">本文对<i>ESR</i>算法进行了以下改进:首先, 应用先验信息为每张图片生成人脸框, 然后进行特征映射, 再进行相似变换得到多个初始形状;其次, 提出一种自适应由粗到细的特征选择方法, 基于先前回归的均方误差自适应地调整特征窗口大小;最后, 改进了相关性公式, 使用基于互信息的特征选择方法, 在候选像素集中选出最相关的特征。在三个公开数据集<i>LFPW</i>、<i>HELEN</i>、<i>COFW</i>上的实验结果表明, 本文的改进算法相较于<i>ESR</i>算法定位精度有较大提升。</p>
                </div>
                <div class="p1">
                    <p id="229">然而, 本文算法对于大面积遮挡样本的定位并不是十分精准, 存在细小的误差, 并且, 根据表1中的结果, 本文算法的定位误差在<i>LFPW</i>、<i>HELEN</i>、<i>COFW</i>数据集上是递增的。这是因为三个数据集中遮挡样本所占比例依次递增。由此可见, 本文算法对于遮挡样本的定位能力还有提升空间。未来工作, 将集中在提升遮挡样本的定位精度上, 设计一种方法, 可以在不使用遮挡信息的情况下, 提高定位精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="249">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Improved Computaul Viola, Michael Jones">

                                <b>[1]</b> VIOLA P, JONES M.Rapid object detection using a boosted cascade of simple features[C]// Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2001:511.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201711034&amp;v=MjQ3NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RG5VTHZBTHo3QmQ3RzRIOWJOcm85R1lJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 陈凯星, 刘赟, 王金海, 等.基于遗传机制和高斯变差的自动前景提取方法[J].计算机应用, 2017, 37 (11) :3231-3237. (CHEN K X, LIU Y, WANG J H, et al.Foreground extraction with genetic and difference of Guassian[J].Journal of Computer Applications, 2017, 37 (11) :3231-3237.) 
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201811026&amp;v=MTEwODhadVpzRnlEblVMdkFMejdCZDdHNEg5bk5ybzlIWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 张欢欢, 洪敏, 袁玉波.基于极端学习机的人脸特征深度稀疏自编码方法[J].计算机应用, 2018, 38 (11) :3193-3198. (ZHANG H H, HONG M, YUAN Y B.Deep sparse auto-encoder using extreme learning machine for facial feature[J].Journal of Computer Applications, 2018, 38 (11) :3193-3198.) 
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084429&amp;v=MDUxNDRUTW53WmVadEZpbmxVcjNJS0Z3WGJobz1OaWZPZmJLN0h0RE5xbzlFWk9NTENINHdvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> COOTES T F, TAYLOR C J, COOPER D H, et al.Active shape models-their training and application[J].Computer Vision &amp; Image Understanding, 1995, 61 (1) :38-59.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active appearance models">

                                <b>[5]</b> COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=One millisecond face alignment with an ensemble of regression trees">

                                <b>[6]</b> KAZEMI V, SULLIVAN J.One millisecond face alignment with an ensemble of regression trees[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1867-1874.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascaded Pose Regression">

                                <b>[7]</b> DOLLAR P, WELINDER P, PERONA P.Cascaded pose regression[C]// Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2010:1078-1085.
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Keypoint Recognition Using Random Ferns">

                                <b>[8]</b> OZUYSAL M, CALONDER M, LEPETIT V, et al.Fast keypoint recognition using random ferns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (3) :448-461.
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face alignment by Explicit Shape Regression">

                                <b>[9]</b> CAO X, WEI Y, WEN F, et al.Face alignment by explicit shape regression[C]// Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2012:2887-2894.
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face alignment at 3000 fps via regressing local binary features">

                                <b>[10]</b> REN S, CAO X, WEI Y, et al.Face alignment at 3000 fps via regressing local binary features[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1685-1692.
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust face landmark estimation under occlusion">

                                <b>[11]</b> BURGOS-ARTIZZU X P, PERONA P, DOLLAR P.Robust face landmark estimation under occlusion[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision.Washington, DC:IEEE Computer Society, 2014:1513-1520.
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised descent method and its applications to face alignment">

                                <b>[12]</b> XIONG X, TORRE F D L.Supervised descent method and its applications to face alignment[C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2013:532-539.
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep convolutional network cascade for facial point detection">

                                <b>[13]</b> SUN Y, WANG X, TANG X.Deep convolutional network cascade for facial point detection[C]// Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2013:3476-3483.
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extensive Facial Landmark Localization with Coarse-to-Fine Convolu-tional Network Cascade">

                                <b>[14]</b> ZHOU E, FAN H, CAO Z, et al.Extensive facial landmark localization with coarse-to-fine convolutional network cascade[C]// Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops.Washington, DC:IEEE Computer Society, 2013:386-391.
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201805013&amp;v=MTE3ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURuVUx2QUx6N0JkN0c0SDluTXFvOUVaNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 贾项南, 于凤芹, 陈莹.改进的显式形状回归人脸特征点定位算法[J].计算机应用, 2018, 38 (5) :1289-1293. (JIA X N, YU F Q, CHEN Y.Improved explicit shape regression for face alignment algorithm[J].Journal of Computer Applications, 2018, 38 (5) :1289-1293.) 
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient feature selection via analysis of relevance and redundancy">

                                <b>[16]</b> YU L, LIU H.Efficient feature selection via analysis of relevance and redundancy[J].Journal of Machine Learning Research, 2004, 5 (12) :1205-1224.
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201904063&amp;v=MDYxNzBac0Z5RG5VTHZBTHo3U1pMRzRIOWpNcTQ5RFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 黄玉琴, 潘华伟.基于分类外形搜索的人脸特征点定位[J/OL].计算机应用研究, 2019, 36 (5) [2018- 09- 20].http://www.arocmag.com/article/02-2019-05-056.html. (HUANG Y Q, PAN H W.Face alignment based on classified shape searching[J/OL].Application Research of Computers, 2019, 36 (5) [2018- 09- 20].http://www.arocmag.com/article/02-2019-05-056.html.) 
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Localizing Parts of Faces Using a Consensus of Exemplars">

                                <b>[18]</b> BELHUMEUR P N, JACOBS D W, KRIEGMAN D J, et al.Localizing parts of faces using a consensus of exemplars[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (12) :2930-2940.
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interactive facial feature localization">

                                <b>[19]</b> LE V, BRANDT J, LIN Z, et al.Interactive facial feature localization[C]// Proceedings of the 12th European Conference on Computer Vision.Berlin:Springer-Verlag, 2012:679-692.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905038" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905038&amp;v=MjAwMjNVUjdxZlp1WnNGeURuVUx2QUx6N0JkN0c0SDlqTXFvOUdiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
