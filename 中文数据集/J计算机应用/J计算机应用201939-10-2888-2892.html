<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136456419971250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910017%26RESULT%3d1%26SIGN%3d6oWXKMv4XqUxdHqbsdwXIsLJys0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910017&amp;v=MTkzNDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWJ2T0x6N0JkN0c0SDlqTnI0OUVZNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="1.1 &lt;b&gt;机器学习中的代价敏感学习&lt;/b&gt;">1.1 <b>机器学习中的代价敏感学习</b></a></li>
                                                <li><a href="#50" data-title="1.2 &lt;b&gt;代价敏感决策树&lt;/b&gt;">1.2 <b>代价敏感决策树</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="2 改进的代价敏感决策树 ">2 改进的代价敏感决策树</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="2.1 &lt;b&gt;加入类分布的代价敏感决策树&lt;/b&gt;">2.1 <b>加入类分布的代价敏感决策树</b></a></li>
                                                <li><a href="#90" data-title="2.2 &lt;b&gt;代价敏感决策树的集成&lt;/b&gt;">2.2 <b>代价敏感决策树的集成</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="3 实验与分析 ">3 实验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#115" data-title="3.1 &lt;b&gt;实验数据的预处理&lt;/b&gt;">3.1 <b>实验数据的预处理</b></a></li>
                                                <li><a href="#123" data-title="3.2 &lt;b&gt;实验设置&lt;/b&gt;">3.2 <b>实验设置</b></a></li>
                                                <li><a href="#125" data-title="3.3 &lt;b&gt;实验结果&lt;/b&gt;">3.3 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#147" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;二分类代价矩阵&lt;/b&gt;"><b>表</b>1 <b>二分类代价矩阵</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同分类器的总体代价&lt;/b&gt;"><b>表</b>2 <b>不同分类器的总体代价</b></a></li>
                                                <li><a href="#121" data-title="图1 amount频率分布直方图">图1 amount频率分布直方图</a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;实验结果对比分析&lt;/b&gt;"><b>表</b>3 <b>实验结果对比分析</b></a></li>
                                                <li><a href="#133" data-title="图2 不同算法的ROC曲线对比">图2 不同算法的ROC曲线对比</a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同转换公式下的结果&lt;/b&gt;"><b>表</b>4 <b>不同转换公式下的结果</b></a></li>
                                                <li><a href="#143" data-title="图3 不同转换公式下的对比">图3 不同转换公式下的对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="172">


                                    <a id="bibliography_1" title="GUO Y,ZHOU W,LUO C,et al.Instance-based credit risk assessment for investment decisions in P2P lending[J].European Journal of Operational Research,2016,249(2):417-426." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB19D0CF3CD321E41C920147AED5FBC88&amp;v=MTEwMTR1SFlmT0dRbGZCckxVMDV0cGh4Ynk5eEs0PU5pZk9mY0c1RjZYTTNQbEdGNThNRG4xTXl4ZGc0ejE5U1h2bDNXZEJmTVRtTnJLWENPTnZGU2lXV3I3SklGcG1hQg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        GUO Y,ZHOU W,LUO C,et al.Instance-based credit risk assessment for investment decisions in P2P lending[J].European Journal of Operational Research,2016,249(2):417-426.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_2" title="ANGELINI E,CAMBA-MENDEZ G,GIANNONE D,et al.Short term forecasts of euro area GDP growth[J].The Econometrics Journal,2011,14(1):25-44." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Short-term forecasts of euro area GDP growth">
                                        <b>[2]</b>
                                        ANGELINI E,CAMBA-MENDEZ G,GIANNONE D,et al.Short term forecasts of euro area GDP growth[J].The Econometrics Journal,2011,14(1):25-44.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_3" title="ZHOU Z,LIU X.Training cost-sensitive neural networks with methods addressing the class imbalance problem[J].IEEE Transactions on Knowledge and Data Engineering,2006,18(1):63-77." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem">
                                        <b>[3]</b>
                                        ZHOU Z,LIU X.Training cost-sensitive neural networks with methods addressing the class imbalance problem[J].IEEE Transactions on Knowledge and Data Engineering,2006,18(1):63-77.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_4" title="LIU A,GHOSH J,MARTIN C E.Generative oversampling for mining imbalanced datasets[EB/OL].[2018-12-10].http://wwwmath1.uni-muenster.de/u/lammers/EDU/ws07/Softcomputing/Literatur/4-DMI5467.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative oversampling for mining imbalanced datasets">
                                        <b>[4]</b>
                                        LIU A,GHOSH J,MARTIN C E.Generative oversampling for mining imbalanced datasets[EB/OL].[2018-12-10].http://wwwmath1.uni-muenster.de/u/lammers/EDU/ws07/Softcomputing/Literatur/4-DMI5467.pdf.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_5" title="JAPKOWICZ N.Concept-learning in the presence of between-class and within-class imbalances[C]//Proceedings of the 14th Biennial Conference of the Canadian Society for Computational Studies of Intelligence,LNCS 2056.Berlin:Springer,2001:67-77." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Concept-learning in the presence of between-class and within-class imbalances">
                                        <b>[5]</b>
                                        JAPKOWICZ N.Concept-learning in the presence of between-class and within-class imbalances[C]//Proceedings of the 14th Biennial Conference of the Canadian Society for Computational Studies of Intelligence,LNCS 2056.Berlin:Springer,2001:67-77.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_6" title="CHAWLA N V,BOWYER K W,HALL L O,et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research,2002,16:321-357." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">
                                        <b>[6]</b>
                                        CHAWLA N V,BOWYER K W,HALL L O,et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research,2002,16:321-357.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_7" title="DRUMMOND C,HOLTE R C.Exploiting the cost(in)sensitivity of decision tree splitting criteria[C]//Proceedings of the 17th International Conference on Machine Learning.San Francisco:Morgan Kaufmann,2000:239-246." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the Cost in Sensitivity of Decision Tree Splitting Criteria">
                                        <b>[7]</b>
                                        DRUMMOND C,HOLTE R C.Exploiting the cost(in)sensitivity of decision tree splitting criteria[C]//Proceedings of the 17th International Conference on Machine Learning.San Francisco:Morgan Kaufmann,2000:239-246.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_8" title="FAN W,STOLFO S J,ZHANG J,et al.Ada Cost:misclassification cost-sensitive boosting[C]//Proceedings of the 16th International Conference on Machine Learning.San Francisco,CA:Morgan Kaufmann,1999:97-105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adacost:Misclassification Cost-Sensitive Boosting">
                                        <b>[8]</b>
                                        FAN W,STOLFO S J,ZHANG J,et al.Ada Cost:misclassification cost-sensitive boosting[C]//Proceedings of the 16th International Conference on Machine Learning.San Francisco,CA:Morgan Kaufmann,1999:97-105.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_9" title="DOMINGOS P.Meta Cost:a general method for making classifiers cost-sensitive[C]//Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,1999:155-164." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Metacost:a general method for making classifiers cost-sensitive">
                                        <b>[9]</b>
                                        DOMINGOS P.Meta Cost:a general method for making classifiers cost-sensitive[C]//Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,1999:155-164.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_10" title="周宇航,周志华.价敏感大间隔分布学习机[J].计算机研究与发展,2016,53(9):1964-1970.(ZHOU Y H,ZHOU Z H.Cost-sensitive large margin distribution machine[J].Journal of Computer Research and Development,2016,53(9):1964-1970.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201609007&amp;v=MTk4MTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhVYnZPTHl2U2RMRzRIOWZNcG85Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        周宇航,周志华.价敏感大间隔分布学习机[J].计算机研究与发展,2016,53(9):1964-1970.(ZHOU Y H,ZHOU Z H.Cost-sensitive large margin distribution machine[J].Journal of Computer Research and Development,2016,53(9):1964-1970.)
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_11" title="李航.统计学习方法[M].北京:清华大学出版社,2012:22-24.(LI H.Statistical Learning Methods[M].Beijing:Tsinghua University Press,2012:22-24.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302275954001&amp;v=MTEwODFOUExxb1pBWU9zUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3N01JbG9TWEZxekdiQzRI&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        李航.统计学习方法[M].北京:清华大学出版社,2012:22-24.(LI H.Statistical Learning Methods[M].Beijing:Tsinghua University Press,2012:22-24.)
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_12" title="BANBURA M,MODUGNO M.Maximum likelihood estimation of factor models on datasets with arbitrary pattern of missing data[J].Journal of Applied Econometrics,2014,29(1):133-160." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD14012500001053&amp;v=Mjc5MDVCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUlWb1diaFE9TmlmY2FySzhIdERPcW85RlpPc09ESGs2bw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        BANBURA M,MODUGNO M.Maximum likelihood estimation of factor models on datasets with arbitrary pattern of missing data[J].Journal of Applied Econometrics,2014,29(1):133-160.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_13" title="邹鹏,莫佳卉,江亦华,等.基于代价敏感决策树的客户价值细分[J].管理科学,2011,24(2):20-29.(ZOU P,MO JH,KIANG M,et al.A cost-sensitive decision tree learning model-an application to customer-value based segmentation[J].Journal of Management Science,2011,24(2):20-29.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCJJ201102003&amp;v=MjIxNDVMRzRIOURNclk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmhVYnZPTHk3Qlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        邹鹏,莫佳卉,江亦华,等.基于代价敏感决策树的客户价值细分[J].管理科学,2011,24(2):20-29.(ZOU P,MO JH,KIANG M,et al.A cost-sensitive decision tree learning model-an application to customer-value based segmentation[J].Journal of Management Science,2011,24(2):20-29.)
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_14" title="李秋洁,赵雅琴,顾洲.代价敏感学习中的损失函数设计[J].控制理论与应用,2015,32(5):689-694.(LI Q J,ZHAO Y Q,GU Z.Design of loss function for cost-sensitive learning[J].Control Theory&amp;amp;Applications,2015,32(5):689-694.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZLY201505015&amp;v=MDMwNjk5VE1xbzlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFVidk9MamZIZDdHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        李秋洁,赵雅琴,顾洲.代价敏感学习中的损失函数设计[J].控制理论与应用,2015,32(5):689-694.(LI Q J,ZHAO Y Q,GU Z.Design of loss function for cost-sensitive learning[J].Control Theory&amp;amp;Applications,2015,32(5):689-694.)
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_15" title="周志华.机器学习[M].北京:清华大学出版社,2016:86-128.(ZHOU Z H.Machine Learning[M].Beijing:Tsinghua U-niversity Press,2016:86-128.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MjM0OTNiQzRITlhPckkxTlkrc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bm5VNzdNSWxvU1hGcXpH&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        周志华.机器学习[M].北京:清华大学出版社,2016:86-128.(ZHOU Z H.Machine Learning[M].Beijing:Tsinghua U-niversity Press,2016:86-128.)
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_16" title="付忠良.不平衡多分类问题的连续Ada Boost算法研究[J].计算机研究与发展,2011,48(12):2326-2333.(FU Z L.Real Ada Boost algorithm for multi-class and imbalanced classification problems[J].Journal of Computer Research and Development,2011,48(12):2326-2333.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201112021&amp;v=MDIzNDRkTEc0SDlETnJZOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWJ2T0x5dlM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        付忠良.不平衡多分类问题的连续Ada Boost算法研究[J].计算机研究与发展,2011,48(12):2326-2333.(FU Z L.Real Ada Boost algorithm for multi-class and imbalanced classification problems[J].Journal of Computer Research and Development,2011,48(12):2326-2333.)
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-07-03 14:46</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2888-2892 DOI:10.11772/j.issn.1001-9081.2019020827            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进的代价敏感决策树的网络贷款分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E5%86%B0%E6%A5%A0&amp;code=41505733&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭冰楠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E5%B9%BF%E6%BD%AE&amp;code=07561879&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴广潮</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E5%8D%97%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0122765&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华南理工大学数学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在网络贷款用户数据集中,贷款成功和贷款失败的用户数量存在着严重的不平衡,传统的机器学习算法在解决该类问题时注重整体分类正确率,导致贷款成功用户的预测精度较低。针对此问题,在代价敏感决策树敏感函数的计算中加入类分布,以减弱正负样本数量对误分类代价的影响,构建改进的代价敏感决策树;以该决策树作为基分类器并以分类准确度作为衡量标准选择表现较好的基分类器,将它们与最后阶段生成的分类器集成得到最终的分类器。实验结果表明,与已有的常用于解决此类问题的算法(如MetaCost算法、代价敏感决策树、AdaCost算法等)相比,改进的代价敏感决策树对网络贷款用户分类可以降低总体的误分类错误率,具有更强的泛化能力。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8D%E5%B9%B3%E8%A1%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">不平衡;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">代价敏感;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E8%B4%B7%E6%AC%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络贷款;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%86%B3%E7%AD%96%E6%A0%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">决策树;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *郭冰楠(1992—),女,河南三门峡人,硕士研究生,主要研究方向:数据挖掘、机器学习;电子邮箱bingnan_nn@163.com;
                                </span>
                                <span>
                                    吴广潮(1972—),男,广东汕头人,副教授,博士,主要研究方向:数据挖掘、机器学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-22</p>

            </div>
                    <h1><b>Classification of online loan based on improved cost-sensitive decision tree</b></h1>
                    <h2>
                    <span>GUO Bingnan</span>
                    <span>WU Guangchao</span>
            </h2>
                    <h2>
                    <span>College of Mathematics, South China University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the online loan user data set, there is a serious imbalance between the number of successful and failed loan users. The traditional machine learning algorithm pays attention to the overall classification accuracy when solving such problems, which leads to lower prediction accuracy of successful loan users. In order to solve this problem, the class distribution was added to the calculation of cost-sensitive decision tree sensitivity function, in order to weaken the impact of positive and negative samples on the misclassification cost, and an improved cost-sensitive decision tree based on ID3(ID3 cs)was constructed. With the improved cost-sensitive decision tree as the base classifier and the classification accuracy as the criterion, the base classifiers with better performance were selected and integrated with the classifier generated in the last stage to obtain the final classifier. Experimental results show that compared with the existing algorithms to solve such problems(such as MetaCost algorithm, cost-sensitive decision tree, AdaCost algorithm), the improved cost-sensitive decision tree can reduce the overall misclassification rate of online loan users and has stronger generalization ability.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=imbalance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">imbalance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cost-sensitive&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cost-sensitive;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=online%20loan&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">online loan;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=integrated%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">integrated learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=decision%20tree&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">decision tree;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    GUO Bingnan,born in 1992,M.S.candidate.Her research interests include data mining,machine learning.;
                                </span>
                                <span>
                                    WU Guangchao,born in 1972,Ph.D.,associate professor.His research interests include data mining,machine learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-22</p>
                            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="36">随着互联网金融市场的不断增长,网络贷款也得到了如火如荼的发展<citation id="204" type="reference"><link href="172" rel="bibliography" /><link href="174" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。本文以某网络贷款征信服务公司提供的真实数据为背景,利用机器学习中的分类方法建立分类模型来识别可以给予贷款的用户,从而减少后续人工审核筛选用户的工作量,降低高违约风险用户带来的损失。</p>
                </div>
                <div class="p1">
                    <p id="37">对于本文研究的网络贷款用户分类而言,贷款获批的人数较少,属于小概率事件。对于这样的小概率事件,主要存在以下问题:一是贷款成功的用户不容易检测到;二是将贷款成功的用户预测为贷款失败,比将贷款不合格的用户预测为贷款成功,所付出的代价更大。而传统的分类方法对于这样的分类问题不具有良好的性能。</p>
                </div>
                <div class="p1">
                    <p id="38">在现实的分类场景中,这样的问题广泛存在,例如医疗诊断、信用卡欺诈检测、广告点击率预测等<citation id="205" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。因此,基于代价敏感的学习成了机器学习中的一个热门研究方向。</p>
                </div>
                <div class="p1">
                    <p id="39">目前对于代价敏感学习的研究大致可以分为以下几个方面:1)对数据集采样的代价敏感学习,通过改变原始数据集的分布,训练分类器得到具有代价敏感性的模型。在采样的过程中主要依据代价因子,插入或删除部分样本来调整数据集的分布,例如随机欠采样<citation id="206" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、随机过采样<citation id="207" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、合成少数类过采样技术(Synthetic Minority Oversampling Technique, SMOTE)<citation id="208" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等方法;2)在学习过程中引入分类代价,使得生成的分类器给予误分类代价更高的类别更多的关注,使其更容易被正确分类,针对不同的分类方法提出了不同的代价敏感模型,如Drummond等<citation id="209" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出的代价敏感决策树;3)对分类结果的处理,主要包括决策阈值的修改以及对分类器的集成,如AdaCost算法<citation id="210" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Meta-Cost算法<citation id="211" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等。</p>
                </div>
                <div class="p1">
                    <p id="40">文献<citation id="214" type="reference">[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</citation>中提出的随机欠采样、随机过采样、合成少数类过采样(SMOTE)方法,会改变数据集的样本分布,影响分类器的性能。文献<citation id="212" type="reference">[<a class="sup">7</a>]</citation>提出的代价敏感决策树(cost-sensitive decision tree based on ID3, ID3cs)算法在分类过程中加入了误分类代价,使决策树在选择分裂属性时,选择能使误分类代价下降最快的。本文在文献<citation id="213" type="reference">[<a class="sup">7</a>]</citation>算法的基础上,引入了样本的类分布,考虑在分类节点上样本分布对分类代价的影响,以此作为选择分裂属性的依据,构造决策树;进而在形成的决策树的基础上进行分类器集成,以此得到最终的代价敏感决策树。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="42" name="42">1.1 <b>机器学习中的代价敏感学习</b></h4>
                <div class="p1">
                    <p id="43">传统的分类算法一般基于分类代价相等的假设,目标是最小化分类错误率,而代价敏感学习的目标是最小化总期望的误分类代价,也就是说代价敏感学习是在给定不同类别的误分类代价后,使用训练集样本学习得到一个分类器,使其在测试集上预测类别时,具有尽量小的总期望误分类代价<citation id="215" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="44">代价敏感学习中一般使用代价矩阵<citation id="216" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>(见表1)表示分类器的误分类代价,其中<i>c</i><sub><i>ij</i></sub>表示把第<i>i</i>类误分为第<i>j</i>类所造成的损失。</p>
                </div>
                <div class="area_img" id="45">
                    <p class="img_tit"><b>表</b>1 <b>二分类代价矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.1 Two-class cost matrix</p>
                    <p class="img_note"></p>
                    <table id="45" border="1"><tr><td rowspan="2"><br />真实类别</td><td colspan="2"><br />预测类别</td></tr><tr><td><br />0</td><td>1</td></tr><tr><td><br />0</td><td><i>c</i><sub>00</sub></td><td><i>c</i><sub>01</sub></td></tr><tr><td><br />1</td><td><i>c</i><sub>10</sub></td><td><i>c</i><sub>11</sub></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="46">一般情况下,<i>c</i><sub>00</sub>=<i>c</i><sub>11</sub>&gt;<i>c</i><sub>01</sub>&gt;<i>c</i><sub>10</sub>。</p>
                </div>
                <div class="p1">
                    <p id="47">代价矩阵确定后,利用贝叶斯定理构建风险函数<citation id="217" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,如式(1)所示,进而通过最小化后验概率来确定相应的类别,如式(2)所示。</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mo stretchy="false">(</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>p</mi></mstyle><mo stretchy="false">(</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo><mi>c</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>c</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi>i</mi><mo>=</mo><mn>0</mn><mo>,</mo><mn>1</mn></mrow></munder><mo stretchy="false">{</mo><mi>R</mi><mo stretchy="false">(</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中: <i>p</i>(<i>c</i><sub><i>j</i></sub>|<i>x</i>)表示把样本<i>x</i>分为<i>c</i><sub><i>j</i></sub>类的概率。</p>
                </div>
                <h4 class="anchor-tag" id="50" name="50">1.2 <b>代价敏感决策树</b></h4>
                <div class="p1">
                    <p id="51">在文献<citation id="218" type="reference">[<a class="sup">13</a>]</citation>中,提出基于ID3决策树的代价敏感决策树ID3cs算法,主要思想是依据不同类别具有不同的误分类代价,在损失函数中加入了误分类代价,分裂属性的选择准则是使误分类代价下降最快的属性。具体描述如下:</p>
                </div>
                <div class="p1">
                    <p id="52">对于一个二分类问题,数据集<i>S</i>包含特征<i>A</i><sub><i>j</i></sub>(<i>j</i>=1,2,…,<i>n</i>),对于每一个类别特征<i>A</i><sub><i>j</i></sub>,都含有<i>m</i>个不同的值<i>A</i><sub><i>ji</i></sub>(<i>i</i>=1,2,…,<i>m</i>)。在<i>A</i><sub><i>j</i></sub>处的误分类代价为:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>p</mi></mstyle><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>C</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">其中: <i>p</i>(<i>i</i>)表示分支<i>i</i>的比例;<i>C</i>(<i>i</i>)是分支<i>i</i>的代价。对非叶子节点<i>i</i>,它的代价是所有分支代价的加权和。对于叶子节点<i>i</i>,它的代价<i>C</i>(<i>i</i>)可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mspace width="0.25em" /><mi>p</mi><mo stretchy="false">(</mo><mfrac><mi>Ρ</mi><mrow><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false">)</mo><mo>⋅</mo><mi>C</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo>+</mo><mi>p</mi><mo stretchy="false">(</mo><mfrac><mi>Ν</mi><mrow><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false">)</mo><mo>⋅</mo><mi>C</mi><msub><mrow></mrow><mi>Ν</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">其中:<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mfrac><mi>Ρ</mi><mrow><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>Ρ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></math></mathml>表示正样本在<i>node</i><sub><i>i</i></sub>处比例,<i>S</i><sub><i>i</i></sub>表示在<i>node</i><sub><i>i</i></sub>处样本的总数目;<mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mfrac><mi>Ν</mi><mrow><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></math></mathml>表示负样本在<i>node</i><sub><i>i</i></sub>处比例;<i>C</i><sub><i>P</i></sub>是在叶子节点<i>node</i><sub><i>i</i></sub>处正样本(贷款成功的用户)的代价;<i>C</i><sub><i>N</i></sub>是在叶子节点<i>node</i><sub><i>i</i></sub>处负样本(贷款失败的用户)的代价。</p>
                </div>
                <div class="p1">
                    <p id="57">决策树的目标是最小化叶子节点{<i>C</i><sub><i>P</i></sub>,<i>C</i><sub><i>N</i></sub>}处的总代价。换句话说,如果<i>C</i><sub><i>P</i></sub>≥<i>C</i><sub><i>N</i></sub>,则叶子节点被标记为正;否则为负。</p>
                </div>
                <div class="p1">
                    <p id="58"><i>C</i><sub><i>P</i></sub>=<i>C</i><sub><i>FP</i></sub>·<i>FP</i>      (5)</p>
                </div>
                <div class="p1">
                    <p id="59"><i>C</i><sub><i>N</i></sub>=<i>C</i><sub><i>FN</i></sub>·<i>FN</i>      (6)</p>
                </div>
                <div class="p1">
                    <p id="60">代价敏感决策树的误分类代价定义如式(7)、(8)所示:</p>
                </div>
                <div class="p1">
                    <p id="61"><i>C</i>=<i>C</i><sub><i>FP</i></sub>·<i>FP</i>+<i>C</i><sub><i>FN</i></sub>·<i>FN</i>      (7)</p>
                </div>
                <div class="p1">
                    <p id="62"><i>AVG</i>(<i>C</i>)=<i>C</i>/(<i>P</i>+<i>N</i>)      (8)</p>
                </div>
                <div class="p1">
                    <p id="63">其中:<i>AVG</i>(<i>C</i>)表示每个样本的平均损失代价,用来评估模型的性能。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">2 改进的代价敏感决策树</h3>
                <h4 class="anchor-tag" id="65" name="65">2.1 <b>加入类分布的代价敏感决策树</b></h4>
                <div class="p1">
                    <p id="66">受到<i>C</i>4.5算法的影响,以及文献<citation id="219" type="reference">[<a class="sup">14</a>]</citation>中提到类分布会影响到分裂节点的选择,因此为减弱正负类样本数量对误分类代价的影响,本文在代价敏感函数的计算中加入类分布,代价敏感函数C<sub>P</sub>、C<sub>N</sub>的计算方法修改为:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo>=</mo><mi>C</mi><msub><mrow></mrow><mrow><mi>F</mi><mi>Ρ</mi></mrow></msub><mo>⋅</mo><mi>F</mi><mi>Ρ</mi><mo>⋅</mo><mfrac><mi>p</mi><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>=</mo><mi>C</mi><msub><mrow></mrow><mrow><mi>F</mi><mi>Ν</mi></mrow></msub><mo>⋅</mo><mi>F</mi><mi>Ν</mi><mo>⋅</mo><mfrac><mi>n</mi><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中:n代表节点处负样本的个数; p代表正样本的个数。</p>
                </div>
                <div class="p1">
                    <p id="69">本文所提出的改进的代价敏感决策树(Improved ID3cs, IID3cs)算法描述如下:</p>
                </div>
                <div class="p1">
                    <p id="70">输入 训练集<i>S</i>={(<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>),(<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>),…,(<i>x</i><sub><i>m</i></sub>, <i>y</i><sub><i>m</i></sub>)},属性集<i>A</i>={<i>a</i><sub>1</sub>,<i>a</i><sub>2</sub>,…,<i>a</i><sub><i>n</i></sub>}。</p>
                </div>
                <div class="p1">
                    <p id="71">输出 以node为根节点的决策树。</p>
                </div>
                <div class="p1">
                    <p id="72">函数TreeGenerate(<i>S</i>,<i>A</i>)</p>
                </div>
                <div class="p1">
                    <p id="73">生成节点node</p>
                </div>
                <div class="p1">
                    <p id="74">if <i>S</i>中样本全属于同一类别<i>C</i> then</p>
                </div>
                <div class="p1">
                    <p id="75">将node标记为<i>C</i>类叶节点;return</p>
                </div>
                <div class="p1">
                    <p id="76">end if</p>
                </div>
                <div class="p1">
                    <p id="77">if <i>A</i>=∅ or <i>S</i>中样本在<i>A</i>上取值相同 then</p>
                </div>
                <div class="p1">
                    <p id="78">将node标记为叶节点,其类标记为<i>S</i>中样本数最多的类;return</p>
                </div>
                <div class="p1">
                    <p id="79">end if</p>
                </div>
                <div class="p1">
                    <p id="80">根据(9)、(10)计算代价以及减少的代价,从<i>A</i>中选择最佳划分属性<i>a</i><sub>*</sub></p>
                </div>
                <div class="p1">
                    <p id="81">for <i>a</i><sub>*</sub>的每一个值<i>a</i><sup><i>v</i></sup><sub>*</sub> do</p>
                </div>
                <div class="p1">
                    <p id="82">为node生成一个分支;令<i>S</i><sub><i>v</i></sub>表示<i>S</i>中在<i>a</i><sub>*</sub>上取值为<i>a</i><sup><i>v</i></sup><sub>*</sub>的样本集;</p>
                </div>
                <div class="p1">
                    <p id="83">if <i>S</i><sub><i>v</i></sub>=∅ then</p>
                </div>
                <div class="p1">
                    <p id="84">将分支节点标记为叶节点,其类别标记为<i>S</i>中样本数最多的类;return</p>
                </div>
                <div class="p1">
                    <p id="85">else</p>
                </div>
                <div class="p1">
                    <p id="86">以TreeGenerate(<i>S</i><sub><i>v</i></sub>,<i>Aa</i><sub>*</sub>})为分支节点</p>
                </div>
                <div class="p1">
                    <p id="87">endif</p>
                </div>
                <div class="p1">
                    <p id="88">end for</p>
                </div>
                <div class="p1">
                    <p id="89">ID3决策树算法使用准确率作为剪枝的评判准则。在算法IID3cs中,树的生成和剪枝都是基于新定义的代价函数式(9)、(10),即若删除该某分支后,误分类代价减小,则进行剪枝操作;反之亦然。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">2.2 <b>代价敏感决策树的集成</b></h4>
                <div class="p1">
                    <p id="91">文献<citation id="220" type="reference">[<a class="sup">13</a>]</citation>中针对单一模型的不稳定性,提出对基分类器集成的思想,本文在对模型集成的过程中,采用的集成策略大致过程如下:</p>
                </div>
                <div class="p1">
                    <p id="92">1)从原数据集中有放回地重采样获得<i>L</i>个不同的子训练集,以IID3cs作为基分类器进行训练,得到不同的、具有差异性的<i>L</i>个分类器<citation id="221" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>,并计算这<i>L</i>个分类器的AUC值。</p>
                </div>
                <div class="p1">
                    <p id="93">2)计算训练集中每个样本属于每个类别的概率和期望代价,以期望代价最小的准则将样本重标记。</p>
                </div>
                <div class="p1">
                    <p id="94">3)运用IID3cs算法训练重标记的数据集,得到新的分类器<i>M</i>。</p>
                </div>
                <div class="p1">
                    <p id="95">4)选择AUC值较高的几个基分类器(在本文选择AUC高于0.6的基分类器)与模型<i>M</i>进行加权,权重计算方式如式(11)所示,得到最终的分类器。将该算法称为IID3cs_Bagging。其中第<i>k</i>个分类器的AUC值记为<i>AUC</i><sub><i>k</i></sub>,权重为<i>w</i><sub><i>k</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mfrac><mrow><mi>A</mi><mi>U</mi><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>A</mi><mi>U</mi><mi>C</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>A</mi></mstyle><mi>U</mi><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">其中:<i>l</i>代表基分类器中AUC值超过0.6的基分类器个数;<i>AUC</i><sub><i>M</i></sub>表示分类器<i>M</i>的AUC值;<i>k</i>=1,2,…,<i>l</i>,<i>M</i>。</p>
                </div>
                <div class="p1">
                    <p id="98">IID3cs_Bagging算法可描述为:</p>
                </div>
                <div class="p1">
                    <p id="99">输入 训练集<i>S</i>={(<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>),(<i>x</i><sub>2</sub>, <i>y</i><sub>2</sub>),…,(<i>x</i><sub><i>m</i></sub>, <i>y</i><sub><i>m</i></sub>)},基学习算法IID3cs,训练轮数<i>T</i>。</p>
                </div>
                <div class="p1">
                    <p id="100">输出 最终的分类器<i>H</i>(<i>x</i>)=∑<i>w</i><sub><i>k</i></sub><i>h</i><sub><i>k</i></sub>(<i>x</i>)。</p>
                </div>
                <div class="p1">
                    <p id="101">for <i>t</i>=1,2,…,<i>T</i> do</p>
                </div>
                <div class="p1">
                    <p id="102"><i>h</i><sub><i>t</i></sub>=IID3cs(<i>S</i>,<i>S</i><sub><i>bs</i></sub>)  //以IID3cs算法构建分类器</p>
                </div>
                <div class="p1">
                    <p id="103">计算每个分类器的AUC值及权重</p>
                </div>
                <div class="p1">
                    <p id="104">end for</p>
                </div>
                <div class="p1">
                    <p id="105">for <i>i</i>=1,2,…,<i>m</i> do</p>
                </div>
                <div class="p1">
                    <p id="106">for <i>j</i>=1,2 do</p>
                </div>
                <div class="p1">
                    <p id="107"><i>p</i><sub><i>ij</i></sub>= <i>p</i>(<i>j</i>|<i>x</i><sub><i>i</i></sub>)  //计算每个样本属于每个类别的概率</p>
                </div>
                <div class="p1">
                    <p id="108"><i>R</i><sub><i>ij</i></sub>=<i>R</i>(<i>j</i>|<i>x</i><sub><i>i</i></sub>)  //计算每个样本属于每个类别的期望代价</p>
                </div>
                <div class="p1">
                    <p id="109">重标记样本标签,得到新数据集<i>S</i>′</p>
                </div>
                <div class="p1">
                    <p id="110"><i>M</i>=IID3cs(<i>S</i>′)  //训练得到新模型</p>
                </div>
                <div class="p1">
                    <p id="111">计算模型<i>M</i>的AUC值及权重</p>
                </div>
                <div class="area_img" id="112">
                    <p class="img_tit"><b>表</b>2 <b>不同分类器的总体代价</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.2 Overall costs of different classifiers</p>
                    <p class="img_note"></p>
                    <table id="112" border="1"><tr><td rowspan="2"><br />cost</td><td colspan="5"><br />分类器</td></tr><tr><td><br />ID3cs</td><td>IID3cs</td><td>ID3cs_Bagging</td><td>IID3cs_Bagging</td><td>MetaCost</td></tr><tr><td><br />5</td><td>115.90±11.02</td><td>110.50±9.75</td><td>108.60±6.35</td><td><b>102.30±1.25</b></td><td>103.50±4.25</td></tr><tr><td><br />10</td><td>195.90±16.75</td><td>188.20±10.21</td><td>148.70±8.29</td><td><b>115.80±7.18</b></td><td>126.30±7.23</td></tr><tr><td><br />15</td><td>239.80±20.37</td><td>226.70±16.93</td><td>208.20±10.49</td><td><b>187.20±8.14</b></td><td>190.70±11.93</td></tr><tr><td><br />20</td><td>325.10±45.64</td><td>314.50±35.26</td><td>273.40±33.49</td><td><b>248.90±24.14</b></td><td>262.40±18.19</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:总体代价(mean±std)表示总体代价的均值和标准差。</p>
                    <p class="img_note"></p>
                </div>
                <h3 id="113" name="113" class="anchor-tag">3 实验与分析</h3>
                <div class="p1">
                    <p id="114">为了验证本文算法的有效性,将IID3cs_Bagging算法与其他几个常用于网络贷款用户分类的经典算法进行对比实验。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">3.1 <b>实验数据的预处理</b></h4>
                <div class="p1">
                    <p id="116">对原始数据集进行预处理,具体如下:</p>
                </div>
                <div class="p1">
                    <p id="117">1)由于本文研究的是网络贷款审批状态与其他变量间的关系,而原始数据集中并没有此特征,根据<i>loanamount</i>特征分析出贷款审批状态<i>status</i>特征:若<i>loanamount</i>不为空,即贷款审批通过,<i>status</i>=1;否则<i>status</i>=0。</p>
                </div>
                <div class="p1">
                    <p id="118">2)由于数据集中部分属性含有异常值和缺失值,结合属性取值以及缺失情况对缺失值进行处理。</p>
                </div>
                <div class="p1">
                    <p id="119">3)由于amount属性有26个取值,且变化幅度较大(最小值为500,最大值为3 000 000),对其进行离散化,模型会更加稳定,同时也降低了模型过拟合的风险。</p>
                </div>
                <div class="p1">
                    <p id="120">绘制其频率分布如图1所示。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 amount频率分布直方图" src="Detail/GetImg?filename=images/JSJY201910017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 amount频率分布直方图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Frequency distribution histogram of amount</p>

                </div>
                <div class="p1">
                    <p id="122">观察图1,对amount属性采用自定义宽度划分法,具体设置如下:0～10 000设为1,10 000～30 000设为2,30 000～50 000设为3,50 000～70 000设为4,70 000以上设为5。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123">3.2 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="124">在本实验中,为了验证不同代价对实验结果的影响,设置负样本的误分类代价为1,正样本的误分类代价取值范围为{5,10,15,20}。针对每个误分类代价取值组合进行10次实验,在每次实验中,随机将数据集按照7∶3的比例划分为训练集、测试集,先在训练集上进行5折交叉验证构建模型,调整参数,参数为决策树的深度,取值范围为{7,8,9,10,11},用训练好的模型对测试集进行分类,最后根据测试集的分类结果计算总代价以及计算分类模型中少数类样本的AUC值和F值。</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125">3.3 <b>实验结果</b></h4>
                <h4 class="anchor-tag" id="126" name="126">3.3.1 配对<i>t</i>检验</h4>
                <div class="p1">
                    <p id="127">表2列出了网络贷款用户分析在不同误分类代价时的实验结果,并用粗体标出了在置信度为95%时进行配对<i>t</i>检验表现最佳分类器的总代价。</p>
                </div>
                <div class="p1">
                    <p id="128">从表2中可看出,在正类样本误分类代价在设定的范围内时,本文集成决策树IID3cs_Bagging算法的平均总体代价均小于其他算法,标准差表现也较好,说明IID3cs_Bagging算法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="129">为了比较在相同的分类代价下不同学习算法的效果,设置<i>cost</i>=5,对不同算法下的AUC值、F值及ROC曲线进行对比及分析。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130">3.3.2 不同算法性能分析</h4>
                <div class="p1">
                    <p id="131">在网络贷款数据集上,对本文提出的IID3cs_Bagging算法进行性能测试,并与ID3cs、IID3cs、ID3cs_Bagging算法以及MetaCost算法<citation id="222" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>进行对比,实验结果如表3所示,同时,本文给出了网络贷款用户数据集在不同算法上的ROC曲线图,如图2所示。</p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表</b>3 <b>实验结果对比分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.3 Comparative analysis of experimental results</p>
                    <p class="img_note"></p>
                    <table id="132" border="1"><tr><td><br />算法</td><td>AUC值</td><td>F值</td></tr><tr><td><br />  ID3cs</td><td>0.629</td><td>0.576</td></tr><tr><td><br />  IID3cs</td><td>0.684</td><td>0.629</td></tr><tr><td><br />  ID3cs_Bagging</td><td>0.752</td><td>0.642</td></tr><tr><td><br />  IID3cs_Bagging</td><td>0.851</td><td>0.689</td></tr><tr><td><br />  MetaCost</td><td>0.778</td><td>0.653</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910017_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同算法的ROC曲线对比" src="Detail/GetImg?filename=images/JSJY201910017_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同算法的ROC曲线对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910017_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 ROC curve comparison diagram of different algorithms</p>

                </div>
                <div class="p1">
                    <p id="134">从图2中可以看到,虽然不同算法的ROC曲线出现了交叉,但是IID3cs_Bagging算法的ROC曲线位于最左上角,完全“包住”了其他算法的ROC曲线,说明在网络贷款用户分类数据集中该算法优于其他算法;ID3cs_Bagging算法和MetaCost算法的ROC曲线出现了交叉,交叉点在<i>fpr</i>≈0.28的位置,该点之前ID3cs_Bagging算法优于MetaCost算法,但之后MetaCost优于ID3cs_Bagging算法,进一步说明单一的代价敏感决策树在一定情况下已具有一定优势;IID3cs和ID3cs出现交叉,根据文献<citation id="223" type="reference">[<a class="sup">15</a>]</citation>提到,此时应比较两曲线下的面积,即AUC值的大小,根据表3,IID3cs、ID3cs算法的AUC值分别为0.684、0.629,IID3cs算法优于ID3cs算法。因此,在网络贷款用户分类数据集上,代价敏感决策树,尤其是集成后的呈现出明显的优势,说明了改进的IID3cs_Bagging算法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="135">从时间效率上来说,虽然在代价敏感算法中加入了代价矩阵的计算,但是它的运行效率并没有比ID3cs、ID3cs_Bagging以及MetaCost算法明显降低,因此从时间效率上分析,IID3cs以及IID3cs_Bagging算法也有一定的优越性。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136">3.3.3 ID3cs_Bagging中权重公式的选择</h4>
                <div class="p1">
                    <p id="137">在2.1节中提出在最后由基分类器组成最后的分类器时,是先计算出各分类器的AUC值之后按照式(11)计算出各个基分类器的权重,进而合成最后的分类器。在转化中常用的转化方式主要有以下几种:</p>
                </div>
                <div class="p1">
                    <p id="138" class="code-formula">
                        <mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>w</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mfrac><mrow><mi>A</mi><mi>U</mi><mi>C</mi><msubsup><mrow></mrow><mi>k</mi><mn>2</mn></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>A</mi></mstyle><mi>U</mi><mi>C</mi><msubsup><mrow></mrow><mi>j</mi><mn>2</mn></msubsup><mo>+</mo><mi>A</mi><mi>U</mi><mi>C</mi><msubsup><mrow></mrow><mi>Μ</mi><mn>2</mn></msubsup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="139">其中:<i>k</i>=1,2,…,<i>L</i>,<i>M</i>。</p>
                </div>
                <div class="p1">
                    <p id="140">为了简化描述,分别将式(11)～(13)称为sum-conver、quad-sum-conver、aver-conver。</p>
                </div>
                <div class="p1">
                    <p id="141">为了说明转化方法的合理性以及有效性,对几种常用的转化方法进行了对比,结果如表4和图3所示。</p>
                </div>
                <div class="area_img" id="142">
                    <p class="img_tit"><b>表</b>4 <b>不同转换公式下的结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.4 Results under Different Conversion Formulas</p>
                    <p class="img_note"></p>
                    <table id="142" border="1"><tr><td><br />公式</td><td>AUC值</td><td>F值</td></tr><tr><td><br />  sum-conver</td><td>0.806</td><td>0.672</td></tr><tr><td><br />  quad-sum-conver</td><td>0.772</td><td>0.638</td></tr><tr><td><br />  aver-conver</td><td>0.674</td><td>0.589</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910017_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同转换公式下的对比" src="Detail/GetImg?filename=images/JSJY201910017_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同转换公式下的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910017_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison under different conversion formulas</p>

                </div>
                <div class="p1">
                    <p id="144">从表4和图3可以看出:在网络贷款用户分类数据集中,本文所采用的权重转换方式相对其他方式而言,具有较大优势,表现效果较好。</p>
                </div>
                <div class="p1">
                    <p id="145">对于该数据集,集成分类器的大小为6,各分类器的权重为9.8%、10.3%、13.2%、14.9%、20.1%、31.7%。</p>
                </div>
                <div class="p1">
                    <p id="146">以上实验结即使表明,改进的代价敏感决策树IID3cs_Bagging算法相对其他已有的代价敏感学习而言,在网络贷款用户分类数据集上具有更好的效果,而且IID3cs_Bagging算法的时间效率并不差于已有的算法。因此,在网络贷款用户分析中,本文所提代价敏感决策树IID3cs_Bagging算法具有较强的鲁棒性和泛化能力。</p>
                </div>
                <h3 id="147" name="147" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="148">本文通过对已有代价敏感决策树ID3cs算法的研究,考虑到正负类样本数量对期望误分类总代价的影响,在代价的计算过程中引入了样本的类别分布,即IID3cs算法。为进一步提高分类器的性能,以IID3cs算法作为基分类器进行集成。在网络贷款分类数据集的应用中,该算法与已有的常用于处理类别不平衡、代价敏感的算法相比,具有较好的表现,在最小化误分类总代价的同时,提高了分类准确率,尤其是少数类样本的分类准确率。</p>
                </div>
                <div class="p1">
                    <p id="149">代价敏感学习仍然未能解决过拟合的问题,而且在考虑代价的过程中只是分析了误分代价,在未来的研究中,将会在这些方面进行更多的尝试。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="172">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB19D0CF3CD321E41C920147AED5FBC88&amp;v=MDg4MzJOaWZPZmNHNUY2WE0zUGxHRjU4TURuMU15eGRnNHoxOVNYdmwzV2RCZk1UbU5yS1hDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4Ynk5eEs0PQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>GUO Y,ZHOU W,LUO C,et al.Instance-based credit risk assessment for investment decisions in P2P lending[J].European Journal of Operational Research,2016,249(2):417-426.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Short-term forecasts of euro area GDP growth">

                                <b>[2]</b>ANGELINI E,CAMBA-MENDEZ G,GIANNONE D,et al.Short term forecasts of euro area GDP growth[J].The Econometrics Journal,2011,14(1):25-44.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem">

                                <b>[3]</b>ZHOU Z,LIU X.Training cost-sensitive neural networks with methods addressing the class imbalance problem[J].IEEE Transactions on Knowledge and Data Engineering,2006,18(1):63-77.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative oversampling for mining imbalanced datasets">

                                <b>[4]</b>LIU A,GHOSH J,MARTIN C E.Generative oversampling for mining imbalanced datasets[EB/OL].[2018-12-10].http://wwwmath1.uni-muenster.de/u/lammers/EDU/ws07/Softcomputing/Literatur/4-DMI5467.pdf.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Concept-learning in the presence of between-class and within-class imbalances">

                                <b>[5]</b>JAPKOWICZ N.Concept-learning in the presence of between-class and within-class imbalances[C]//Proceedings of the 14th Biennial Conference of the Canadian Society for Computational Studies of Intelligence,LNCS 2056.Berlin:Springer,2001:67-77.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">

                                <b>[6]</b>CHAWLA N V,BOWYER K W,HALL L O,et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research,2002,16:321-357.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the Cost in Sensitivity of Decision Tree Splitting Criteria">

                                <b>[7]</b>DRUMMOND C,HOLTE R C.Exploiting the cost(in)sensitivity of decision tree splitting criteria[C]//Proceedings of the 17th International Conference on Machine Learning.San Francisco:Morgan Kaufmann,2000:239-246.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adacost:Misclassification Cost-Sensitive Boosting">

                                <b>[8]</b>FAN W,STOLFO S J,ZHANG J,et al.Ada Cost:misclassification cost-sensitive boosting[C]//Proceedings of the 16th International Conference on Machine Learning.San Francisco,CA:Morgan Kaufmann,1999:97-105.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Metacost:a general method for making classifiers cost-sensitive">

                                <b>[9]</b>DOMINGOS P.Meta Cost:a general method for making classifiers cost-sensitive[C]//Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,1999:155-164.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201609007&amp;v=MTkxNzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWJ2T0x5dlNkTEc0SDlmTXBvOUZZNFFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>周宇航,周志华.价敏感大间隔分布学习机[J].计算机研究与发展,2016,53(9):1964-1970.(ZHOU Y H,ZHOU Z H.Cost-sensitive large margin distribution machine[J].Journal of Computer Research and Development,2016,53(9):1964-1970.)
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302275954001&amp;v=MTA2NjM5ZmJ2bktyaWZaZVp2RnlublU3N01JbG9TWEZxekdiQzRITlBMcW9aQVlPc1BEUk04enhVU21EZDlTSDduM3hF&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>李航.统计学习方法[M].北京:清华大学出版社,2012:22-24.(LI H.Statistical Learning Methods[M].Beijing:Tsinghua University Press,2012:22-24.)
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD14012500001053&amp;v=MDk3MzBLOEh0RE9xbzlGWk9zT0RIazZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJVm9XYmhRPU5pZmNhcg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>BANBURA M,MODUGNO M.Maximum likelihood estimation of factor models on datasets with arbitrary pattern of missing data[J].Journal of Applied Econometrics,2014,29(1):133-160.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JCJJ201102003&amp;v=MTc4MzNZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWJ2T0x5N0JaTEc0SDlETXI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>邹鹏,莫佳卉,江亦华,等.基于代价敏感决策树的客户价值细分[J].管理科学,2011,24(2):20-29.(ZOU P,MO JH,KIANG M,et al.A cost-sensitive decision tree learning model-an application to customer-value based segmentation[J].Journal of Management Science,2011,24(2):20-29.)
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZLY201505015&amp;v=MTk1NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWJ2T0xqZkhkN0c0SDlUTXFvOUVZWVE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>李秋洁,赵雅琴,顾洲.代价敏感学习中的损失函数设计[J].控制理论与应用,2015,32(5):689-694.(LI Q J,ZHAO Y Q,GU Z.Design of loss function for cost-sensitive learning[J].Control Theory&amp;Applications,2015,32(5):689-694.)
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MjMyNjVIN24zeEU5ZmJ2bktyaWZaZVp2RnlublU3N01JbG9TWEZxekdiQzRITlhPckkxTlkrc1BEQk04enhVU21EZDlT&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>周志华.机器学习[M].北京:清华大学出版社,2016:86-128.(ZHOU Z H.Machine Learning[M].Beijing:Tsinghua U-niversity Press,2016:86-128.)
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201112021&amp;v=MDI3MDRkTEc0SDlETnJZOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWJ2T0x5dlM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>付忠良.不平衡多分类问题的连续Ada Boost算法研究[J].计算机研究与发展,2011,48(12):2326-2333.(FU Z L.Real Ada Boost algorithm for multi-class and imbalanced classification problems[J].Journal of Computer Research and Development,2011,48(12):2326-2333.)
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910017&amp;v=MTkzNDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5oVWJ2T0x6N0JkN0c0SDlqTnI0OUVZNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
