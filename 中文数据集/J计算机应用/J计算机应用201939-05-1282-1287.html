<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136763053408750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201905007%26RESULT%3d1%26SIGN%3dvT1lO%252bvvpaX6lJI%252fUJ%252bXjkUIAqw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201905007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905007&amp;v=MDY2MTJxbzlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVY3N0lMejdCZDdHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#53" data-title="1 微表情识别的全局方法 ">1 微表情识别的全局方法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="2 动作单元与局部关键区域的划分 ">2 动作单元与局部关键区域的划分</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="2.1 AU&lt;b&gt;与特定微表情的关系&lt;/b&gt;">2.1 AU<b>与特定微表情的关系</b></a></li>
                                                <li><a href="#66" data-title="2.2 &lt;b&gt;划分局部关键区域&lt;/b&gt;">2.2 <b>划分局部关键区域</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="3 局部区域时空模式特征提取 ">3 局部区域时空模式特征提取</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#97" data-title="4 实验仿真 ">4 实验仿真</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#98" data-title="4.1 &lt;b&gt;微表情数据集&lt;/b&gt;CASME2&lt;b&gt;与识别性能指标&lt;/b&gt;">4.1 <b>微表情数据集</b>CASME2<b>与识别性能指标</b></a></li>
                                                <li><a href="#101" data-title="4.2 &lt;b&gt;实验设计&lt;/b&gt;">4.2 <b>实验设计</b></a></li>
                                                <li><a href="#110" data-title="4.3 &lt;b&gt;实验结果及分析&lt;/b&gt;">4.3 <b>实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="图1 人脸分割区域块">图1 人脸分割区域块</a></li>
                                                <li><a href="#57" data-title="图2 特征向量的提取">图2 特征向量的提取</a></li>
                                                <li><a href="#62" data-title="图3 常规表情高兴的AU分解">图3 常规表情高兴的AU分解</a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;常规表情与相应&lt;/b&gt;FACS AU&lt;b&gt;的对应关系&lt;/b&gt;"><b>表</b>1 <b>常规表情与相应</b>FACS AU<b>的对应关系</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;微表情与相应&lt;/b&gt;FACS AU&lt;b&gt;的对应关系&lt;/b&gt;"><b>表</b>2 <b>微表情与相应</b>FACS AU<b>的对应关系</b></a></li>
                                                <li><a href="#68" data-title="图4 局部区域块的划分">图4 局部区域块的划分</a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;面部区域划分及其对应的&lt;/b&gt;FACS AU"><b>表</b>3 <b>面部区域划分及其对应的</b>FACS AU</a></li>
                                                <li><a href="#71" data-title="图5 49个关键点位置">图5 49个关键点位置</a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;各局部区域边界坐标点&lt;/b&gt;"><b>表</b>4 <b>各局部区域边界坐标点</b></a></li>
                                                <li><a href="#96" data-title="图6 本文所提方法流程">图6 本文所提方法流程</a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同区域在不同参数设置情况下的性能比较&lt;/b&gt;"><b>表</b>5 <b>不同区域在不同参数设置情况下的性能比较</b></a></li>
                                                <li><a href="#119" data-title="图7 局部区域方法进行微表情识别的混淆矩阵">图7 局部区域方法进行微表情识别的混淆矩阵</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" CLANCY M.The Philosophy of Deception[M].Oxford:Oxford University Press, 2009:118-133." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Philosophy of Deception">
                                        <b>[1]</b>
                                         CLANCY M.The Philosophy of Deception[M].Oxford:Oxford University Press, 2009:118-133.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" SALTER F, GRAMMER K, RIKOWSKI A.Sex differences in negotiating with powerful males[J].Human Nature, 2005, 16 (3) :306-321." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001370735&amp;v=MDM5NjE3ek1JVjQ9Tmo3QmFyTzRIdEhOckloRlkrZ0tZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpcmxX&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         SALTER F, GRAMMER K, RIKOWSKI A.Sex differences in negotiating with powerful males[J].Human Nature, 2005, 16 (3) :306-321.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" WHITEHILL J, SERPELL Z, LIN Y C, et al.The faces of engagement:automatic recognition of student engagementfrom facial expressions[J].IEEE Transactions on Affective Computing, 2014, 5 (1) :86-98." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The faces of engagement:automatic recognition of student engagementfrom facial expressions">
                                        <b>[3]</b>
                                         WHITEHILL J, SERPELL Z, LIN Y C, et al.The faces of engagement:automatic recognition of student engagementfrom facial expressions[J].IEEE Transactions on Affective Computing, 2014, 5 (1) :86-98.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" POOL L D, QUALTER P.Improving emotional intelligence and emotional self-efficacy through a teaching intervention for university students[J].Learning &amp;amp; Individual Differences, 2012, 22 (3) :306-312." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501311274&amp;v=MjA2MTE3SHRETnFvOUVaK29PRG5zOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUtGMFFheEk9TmlmT2ZiSw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         POOL L D, QUALTER P.Improving emotional intelligence and emotional self-efficacy through a teaching intervention for university students[J].Learning &amp;amp; Individual Differences, 2012, 22 (3) :306-312.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" STEWART P A, WALLER B M, SCHUBERT J N.Presidential speechmaking style:emotional response to micro-expressions of facial affect[J].Motivation &amp;amp; Emotion, 2009, 33 (2) :125-135." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110100483739&amp;v=MTM5OTBadEZpbmxVcjNJS0YwUWF4ST1OajdCYXJLOUg5RE1ybzlGWU9NTUMzOHdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         STEWART P A, WALLER B M, SCHUBERT J N.Presidential speechmaking style:emotional response to micro-expressions of facial affect[J].Motivation &amp;amp; Emotion, 2009, 33 (2) :125-135.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" EKMAN P.Micro Expression Training Tool (METT) [M].San Francisco:University of California, 2002:1877-1903." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Micro Expression Training Tool (METT)">
                                        <b>[6]</b>
                                         EKMAN P.Micro Expression Training Tool (METT) [M].San Francisco:University of California, 2002:1877-1903.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" FRANK M G, HERBASZ M, SINUK K, et al.I see how you feel:training laypeople and professionals to recognize fleeting emotions[C]// Proceedings of the 2009 Annual Meeting of the International Communication Association.New York:[s.n.], 2009:3515-3522." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=I see how you feel:Training laypeople and professionals to recognize fleeting emotions">
                                        <b>[7]</b>
                                         FRANK M G, HERBASZ M, SINUK K, et al.I see how you feel:training laypeople and professionals to recognize fleeting emotions[C]// Proceedings of the 2009 Annual Meeting of the International Communication Association.New York:[s.n.], 2009:3515-3522.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" OJALA T, PIETIKAINEN M, HARWOOD D.A comparative study of texture measures with classification based on featured distributions[J].Pattern Recognition, 1996, 29 (1) :51-59." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600742093&amp;v=MDY4ODhESFU2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJS0YwUWF4ST1OaWZPZmJLN0h0RE5xWTlGWSs4Tg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         OJALA T, PIETIKAINEN M, HARWOOD D.A comparative study of texture measures with classification based on featured distributions[J].Pattern Recognition, 1996, 29 (1) :51-59.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" ZHAO G, PIETIKAINEN M.Dynamic texture recognition using local binary patterns with an application to facial expressions[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (6) :915-928." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic Texture Recognition Using Local Binary Patterns with an Application to Facial Expressions">
                                        <b>[9]</b>
                                         ZHAO G, PIETIKAINEN M.Dynamic texture recognition using local binary patterns with an application to facial expressions[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (6) :915-928.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" HUANG X, ZHAO G, HONG X, et al.Spontaneous facial micro-expression analysis using spatiotemporal completed local quantized patterns[J].Neurocomputing, 2016, 175:564-578." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5A5677F640C7E77943CC88F65E8EA7D0&amp;v=MTMwMTJjY2ZsUXM2ZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHpMdTd3YWc9TmlmT2ZiYkpHOWZMcVBsRFlPdDhDd2sreUI4WDZVd09RSGVVcWhkQQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         HUANG X, ZHAO G, HONG X, et al.Spontaneous facial micro-expression analysis using spatiotemporal completed local quantized patterns[J].Neurocomputing, 2016, 175:564-578.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" WANG Y, SEE J, PHAN R C, et al.LBP with six intersection points:reducing redundant information in LBP-TOP for micro-expression recognition[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:525-537." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lbp with six intersection points:Reducing redundant information in lbp-top for micro-expression recognition">
                                        <b>[11]</b>
                                         WANG Y, SEE J, PHAN R C, et al.LBP with six intersection points:reducing redundant information in LBP-TOP for micro-expression recognition[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:525-537.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" FU X, WEI W.Centralized binary patterns embedded with image Euclidean distance for facial expression recognition[C]// Proceedings of the 2008 4th International Conference on Natural Computation.Washington, DC:IEEE Computer Society, 2008:115-119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Centralized binary patterns embedded with image Euclidean distance for facial expression recognition">
                                        <b>[12]</b>
                                         FU X, WEI W.Centralized binary patterns embedded with image Euclidean distance for facial expression recognition[C]// Proceedings of the 2008 4th International Conference on Natural Computation.Washington, DC:IEEE Computer Society, 2008:115-119.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" SUN B, LI L, WU X, et al.Combining feature-level and decision-level fusion in a hierarchical classifier for emotion recognition in the wild[J].Journal on Multimodal User Interfaces, 2016, 10 (2) :125-137." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining feature-level and decision-level fusion in a hierarchical classifier for emotion recognition in the wild">
                                        <b>[13]</b>
                                         SUN B, LI L, WU X, et al.Combining feature-level and decision-level fusion in a hierarchical classifier for emotion recognition in the wild[J].Journal on Multimodal User Interfaces, 2016, 10 (2) :125-137.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" LI X, HONG X, MOILANEN A, et al.Towards reading hidden emotions:a comparative study of spontaneous micro-expression spotting and recognition methods[J].IEEE Transactions on Affective Computing, 2018, 9 (4) :563-577." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards reading hidden emotions:a comparative study of spontaneous micro-expression spotting and recognition methods">
                                        <b>[14]</b>
                                         LI X, HONG X, MOILANEN A, et al.Towards reading hidden emotions:a comparative study of spontaneous micro-expression spotting and recognition methods[J].IEEE Transactions on Affective Computing, 2018, 9 (4) :563-577.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" COHN J F, AMBADAR Z, EKMAN P.Observer-based measurement of facial expression with the facial action coding system[J].Neuroscience Letters, 2007, 394 (3) :203-221." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Observer-based measurement of facial expression with the facial action coding system">
                                        <b>[15]</b>
                                         COHN J F, AMBADAR Z, EKMAN P.Observer-based measurement of facial expression with the facial action coding system[J].Neuroscience Letters, 2007, 394 (3) :203-221.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" MARTINEZ B, VALSTAR M F, JIANG B, et al.Automatic analysis of facial actions:a survey[J/OL].IEEE Transactions on Affective Computing, 2017 [2018- 06- 20].https://ieeexplore.ieee.org/document/7990582." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic analysis of facial actions:a survey">
                                        <b>[16]</b>
                                         MARTINEZ B, VALSTAR M F, JIANG B, et al.Automatic analysis of facial actions:a survey[J/OL].IEEE Transactions on Affective Computing, 2017 [2018- 06- 20].https://ieeexplore.ieee.org/document/7990582.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" YAN W J, LI X, WANG S J, et al.CASME II:an improved spontaneous micro-expression database and the baseline evaluation[J].PLoS One, 2014, 9 (1) :1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CASME II:an improved spontaneous micro-expression database and the baseline evaluation">
                                        <b>[17]</b>
                                         YAN W J, LI X, WANG S J, et al.CASME II:an improved spontaneous micro-expression database and the baseline evaluation[J].PLoS One, 2014, 9 (1) :1-8.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" ASTHANA A, ZAFEIRIOU S, CHENG S, et al.Incremental face alignment in the wild[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1859-1866." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incremental face alignment in the wild">
                                        <b>[18]</b>
                                         ASTHANA A, ZAFEIRIOU S, CHENG S, et al.Incremental face alignment in the wild[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1859-1866.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active appearance models">
                                        <b>[19]</b>
                                         COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" 刘丽, 谢毓湘, 魏迎梅, 等.局部二进制模式方法综述[J].中国图象图形学报, 2014, 19 (12) :1696-1720. (LIU L, XIE Y X, WEI Y M, et al.Survey of local binary pattern method[J].Journal of Image and Graphics, 2014, 19 (12) :1696-1720.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201412002&amp;v=MTcxMzR6cXFCdEdGckNVUjdxZlp1WnNGeURtVjc3SVB5cmZiTEc0SDlYTnJZOUZab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         刘丽, 谢毓湘, 魏迎梅, 等.局部二进制模式方法综述[J].中国图象图形学报, 2014, 19 (12) :1696-1720. (LIU L, XIE Y X, WEI Y M, et al.Survey of local binary pattern method[J].Journal of Image and Graphics, 2014, 19 (12) :1696-1720.) 
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" OH Y, NGO A C, SEE J, et al.Monogenic Riesz wavelet representation for micro-expression recognition[C]// Proceedings of the 2015 IEEE International Conference on Digital Signal Processing.Piscataway, NJ:IEEE, 2015:1237-1241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Monogenic riesz wavelet representation for micro expression recognition">
                                        <b>[21]</b>
                                         OH Y, NGO A C, SEE J, et al.Monogenic Riesz wavelet representation for micro-expression recognition[C]// Proceedings of the 2015 IEEE International Conference on Digital Signal Processing.Piscataway, NJ:IEEE, 2015:1237-1241.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" LIONG S, SEE J, PHAN R C, et al.Subtle expression recognition using optical strain weighted features[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:644-657." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Subtle expression recognition using optical strain weighted features">
                                        <b>[22]</b>
                                         LIONG S, SEE J, PHAN R C, et al.Subtle expression recognition using optical strain weighted features[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:644-657.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" NGO A C, PHAN R C, SEE J, et al.Spontaneous subtle expression recognition:imbalanced databases and solutions[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:33-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spontaneous subtle expression recognition:imbalanced databases and solutions">
                                        <b>[23]</b>
                                         NGO A C, PHAN R C, SEE J, et al.Spontaneous subtle expression recognition:imbalanced databases and solutions[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:33-48.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(05),1282-1287 DOI:10.11772/j.issn.1001-9081.2018102090            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于局部区域方法的微表情识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%BB%B6%E8%89%AF&amp;code=07202205&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张延良</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%A2%E5%86%B0&amp;code=38732548&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卢冰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B4%AA%E6%99%93%E9%B9%8F&amp;code=41747331&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">洪晓鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%9B%BD%E8%8B%B1&amp;code=30562484&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵国英</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%BC%9F%E6%B6%9B&amp;code=21759404&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张伟涛</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E5%8D%97%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E7%89%A9%E7%90%86%E4%B8%8E%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0026206&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河南理工大学物理与电子信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A5%A5%E5%8D%A2%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%92%8C%E4%BF%A1%E5%8F%B7%E5%88%86%E6%9E%90%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0154295&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">奥卢大学机器视觉和信号分析研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0008505&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安电子科技大学电子工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>微表情 (ME) 的发生只牵涉到面部局部区域, 具有动作幅度小、持续时间短的特点, 但面部在产生微表情的同时也存在一些无关的肌肉动作。现有微表情识别的全局区域方法会提取这些无关变化的时空模式, 从而降低特征向量对于微表情的表达能力, 进而影响识别效果。针对这个问题, 提出使用局部区域方法进行微表情识别。首先, 根据微表情发生时所牵涉到的动作单元 (AU) 所在区域, 通过面部关键点坐标将与微表情相关的七个局部区域划分出来;然后, 提取这些局部区域组合的时空模式并串联构成特征向量, 进行微表情识别。留一交叉验证的实验结果表明局部区域方法较全局区域方法进行微表情识别的识别率平均提高9.878%。而通过对各区域识别结果的混淆矩阵进行分析表明所提方法充分利用了面部各局部区域的结构信息, 并有效摒除与微表情无关区域对识别性能的影响, 较全局区域方法可以显著提高微表情识别的性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BE%AE%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">微表情识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征向量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A8%E4%BD%9C%E5%8D%95%E5%85%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">动作单元;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%B1%80%E5%8C%BA%E5%9F%9F%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全局区域方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%80%E9%83%A8%E5%8C%BA%E5%9F%9F%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">局部区域方法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *张延良 (1979—) , 男, 河南平顶山人, 副教授, 博士, 主要研究方向:人工智能、机器学习;电子邮箱ylzhang@hpu.edu.cn;
                                </span>
                                <span>
                                    卢冰 (1992—) , 女, 河南邓州人, 硕士研究生, 主要研究方向:智能信号处理、机器学习;;
                                </span>
                                <span>
                                    洪晓鹏 (1983—) , 男, 广东揭阳人, 讲师, 博士生导师, 博士, 主要研究方向:计算机视觉、模式识别;;
                                </span>
                                <span>
                                    赵国英 (1977—) , 女, 山东聊城人, 教授, 博士, 主要研究方向:计算机视觉、情感计算;;
                                </span>
                                <span>
                                    张伟涛 (1983—) , 男, 陕西西安人, 副教授, 博士, 主要研究方向:盲信号处理、机械故障诊断。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61571339);</span>
                                <span>网络与交换技术国家重点实验室开放课题 (SKLNST-2016-1-02);</span>
                                <span>河南理工大学博士基金资助项目 (B2012-100);</span>
                    </p>
            </div>
                    <h1><b>Micro-expression recognition based on local region method</b></h1>
                    <h2>
                    <span>ZHANG Yanliang</span>
                    <span>LU Bing</span>
                    <span>HONG Xiaopeng</span>
                    <span>ZHAO Guoying</span>
                    <span>ZHANG Weitao</span>
            </h2>
                    <h2>
                    <span>School of Physics and Electronic Information, Henan Polytechnic University</span>
                    <span>Center for Machine Vision and Signal Analysis, University of Oulu</span>
                    <span>School of Electronic Engineering, Xidian University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Micro-Expression (ME) occurrence is only related to local region of face, with very short time and subtle movement intensity. There are also some unrelated muscle movements in the face during the occurrence of micro-expressions. By using existing global method of micro-expression recognition, the spatio-temporal patterns of these unrelated changes were extracted, thereby reducing the representation capability of feature vectors, and thus affecting the recognition performance. To solve this problem, the local region method was proposed to recognize micro-expression. Firstly, according to the region with the Action Units (AU) related to the micro-expression, seven local regions related to the micro-expression were partitioned by facial key coordinates. Then, the spatio-temporal patterns of these local regions were extracted and connected in series to form feature vectors for micro-expression recognition. The experimental results of leave-one-subject-out cross validation show that the micro-expression recognition accuracy of local region method is 9.878% higher than that of global region method. The analysis of the confusion matrix of each region's recognition result shows that the proposed method makes full use of the structural information of each local region of face, effectively eliminating the influence of unrelated regions of the micro-expression on the recognition performance, and its performance of micro-expression recognition can be significantly improved compared with the global region method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=micro-expression%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">micro-expression recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20vector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature vector;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Action%20Unit%20(AU)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Action Unit (AU) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=global%20region%20method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">global region method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=local%20region%20method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">local region method;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Yanliang, born in 1979, Ph. D. , associate professor. His research interests include artificial intelligence, machine learning. ;
                                </span>
                                <span>
                                    LU Bing, born in 1992, M. S. candidate. Her research interests include intelligent signal processing, machine learning. ;
                                </span>
                                <span>
                                    HONG Xiaopeng, born in 1983, Ph. D. , lecturer. His research interests include computer vision, pattern recognition. ;
                                </span>
                                <span>
                                    ZHAO Guoying, born in 1977, Ph. D. , professor. Her research interests include machine vision, affective computing. ;
                                </span>
                                <span>
                                    ZHANG Weitao, born in 1983, Ph. D. , associate professor. His research interests include blind source processing, mechanical fault diagnosis.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-16</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61571339);</span>
                                <span>the Open Project of State Key Laboratory of Network and Switching Technology (SKLNST-2016-1-02);</span>
                                <span>the Doctoral Fund of Henan Polytechnic University (B2012-100);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="50">面部表情是人类传递个人内心感受的一个重要途径。在过去几十年中, 表情识别一直是机器视觉领域的重要研究课题之一。除了日常生活中经常见到的常规表情, 在特定情形下, 人们会试图掩盖内在情绪的外露, 而产生不易被人察觉的微表情 (Micro-Expression, ME) 。它持续时间通常少于 0.5 s, 发生时面部肌肉的动作幅度轻微、区域小。这种特殊的面部微小动作, 可以作为识别人内在情感的重要依据, 在司法审讯<citation id="124" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、交流谈判<citation id="125" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、教学效果评估<citation id="129" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>及心理咨询<citation id="126" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等场合有广泛的应用价值。因为用裸眼准确捕捉和识别微表情成功率很低, Ekman<citation id="127" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>开发了微表情识别训练工具METT来提高人们对微表情的识别率, 但是, 经过专业训练的人士, 识别率也仅能达到47%<citation id="128" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。因此, 运用计算机视觉方法实现微表情的识别成为情感计算领域的一个重要研究课题。</p>
                </div>
                <div class="p1">
                    <p id="51">自动识别微表情的一般步骤是:首先设计特征表达方法, 提取微表情视频序列的特征向量; 然后再通过模式分类的方法实现识别。在微表情识别的开创性工作中, Pfiste等运用局部二值模式 (Local Binary Pattern, LBP) <citation id="130" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的一种拓展描述算子三正交平面局部二值模式 (Local Binary Pattern on Three Orthogonal Plane, LBP-TOP) <citation id="131" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>来编码局部像素的时空共生模式。该方法采用时空描述子, 分别抽取视频在<i>XY</i>、<i>YT</i>及<i>XT</i>三个平面的LBP特征, 既考虑了图像的局部纹理信息, 又对视频随时间变化的情况进行了描述。这种采用时空局部特征建立特征向量的思路被微表情识别领域的研究者广泛采用。后续的时空完备局部量化模式<citation id="132" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、六交点局部二值模式<citation id="133" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、中心化二值模式<citation id="134" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等均是对LBP-TOP方法的改进。采用在三个平面抽取不同于LBP的局部特征作为时空描述子的思路, 后续又出现了三正交平面局部相位量化算子 (Local Phase Quantization on Three Orthogonal Planes, LPQ-TOP) <citation id="135" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、三正交平面方向梯度直方图算子 (Histograms of Oriented Gradients on Three Orthogonal Planes, HOG-TOP) <citation id="136" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>及改进的三正交平面方向梯度直方图算子 (Histogram of Image Gradient Orientation on Three Orthogonal Planes, HIGO-TOP) <citation id="137" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等局部时空模式用于微表情识别。</p>
                </div>
                <div class="p1">
                    <p id="52">这些方法往往将视频序列面部全局区域等分成若干个立方块, 然后提取每个块的特征, 再将这些特征串联起来构成该视频序列的特征向量。这种作法虽然考虑了局部模式的位置信息, 但它将各个块同等对待, 没有利用面部各组成部分 (如眼睛、鼻子、嘴巴、下巴等) 的内部结构信息, 是一种全局的方法。实际上, 微表情在发生时只牵涉到部分面部区域。面部在产生微表情时也存在一些无关的变化, 全局方法提取这些无关变化的局部模式会降低特征向量对于微表情特征的表达能力, 进而影响识别成功率。本文根据微表情发生时所牵涉到的动作单元所在区域作为分块的标准, 依据面部关键点坐标, 将与微表情相关的7个区域划分出来。提取这些区域的局部时空模式串联构成特征向量, 再用模式分类的方法进行微表情识别。实验表明局部区域的方法可以有效摒除与微表情无关区域对识别性能的影响, 较全局方法可以显著提高微表情识别的性能。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag">1 微表情识别的全局方法</h3>
                <div class="p1">
                    <p id="54">全局区域的微表情识别是把整个面部图像作为研究对象, 将人脸分割为若干个重叠或者不重叠的块, 如图1<citation id="138" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>所示。其中, 图 (a) 中人脸划分为不重叠的9×8个区域块, 图 (b) 中人脸划分为相互重叠的4×3个区域块。在每个区域块上分别提取LBP-TOP特征向量, 然后串联起来作为整幅图像的特征向量, 过程示意图如图2<citation id="139" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>所示。最后将特征向量输入训练好的分类器实现微表情的分类。</p>
                </div>
                <div class="p1">
                    <p id="55">这种在面部全局区域提取特征的方法, 虽然可以提取到面部各个部分的动作特征, 但在应用于微表情识别时存在如下缺点:1) 不同于常规表情, 微表情发生仅牵涉到眼睛、鼻子、嘴巴等局部区域的细微动作。而全局方法会提取到与微表情无关的其他区域的动作特征, 这会对微表情的识别造成不良影响;2) 在对面部区域进行均匀分块时, 由于面部各主要部位分布的不规则性, 会导致同一个器官被划分到不同的块中, 从而导致对同一部位提取到的动作特征不完整。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905007_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 人脸分割区域块" src="Detail/GetImg?filename=images/JSJY201905007_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 人脸分割区域块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905007_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Face segmentation region blocks</p>

                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905007_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 特征向量的提取" src="Detail/GetImg?filename=images/JSJY201905007_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 特征向量的提取  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905007_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Extraction of feature vectors</p>

                </div>
                <h3 id="58" name="58" class="anchor-tag">2 动作单元与局部关键区域的划分</h3>
                <div class="p1">
                    <p id="59">虽然人类面部表情千姿百态, 但同类表情发生时牵涉到的面部部位及其肌肉动作的方式也具有一定的共性。面部动作编码系统 (Facial Action Coding System, FACS) <citation id="140" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>将人类常规表情分解为若干个特定动作单元 (Action Unit, AU) <citation id="141" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>协同作用的结果。参照这一思路, 微表情也可以分解为若干动作单元协同作用的结果。这样, 在进行微表情识别时, 只提取这些动作单元所在局部区域的特征, 可以有效排除无关区域对微表情识别的干扰, 大幅提高特征向量对微表情的表征能力。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">2.1 AU<b>与特定微表情的关系</b></h4>
                <div class="p1">
                    <p id="61">FACS<citation id="142" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>是美国心理学家Ekman和Friesen为客观进行人类面部表情识别而制定的。该系统定义了46个动作单元AU, 其中的32个AU与面部肌肉动作有关, 每个AU描述面部一个局部肌肉群的特定动作模式。例如AU1表示眉毛內端上扬, AU10表示上嘴唇上扬等。任何一个面部表情可以认为是由一个或者若干个AU的联合动作引起的。图3通过1个例子标示出了常规表情高兴所包含的AU及其含义。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905007_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 常规表情高兴的AU分解" src="Detail/GetImg?filename=images/JSJY201905007_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 常规表情高兴的AU分解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905007_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 AU decomposition of happiness</p>

                </div>
                <div class="p1">
                    <p id="63">7种常规表情与相应AU的对应关系如表1所示, 但是, 因微表情是人在高度紧张等特殊情形下面部肌肉的短暂、轻微动作, 其与AU的对应关系不同于常规表情。例如, “厌恶”在常规表情时对应AU9+AU15+AU16, 但在微表情时对应AU9或者AU10或者AU4+ AU7。微表情数据库CASME2的建立者中国科学院心理所傅小兰课题组从该数据库中总结的微表情与相应AU的对应关系<citation id="143" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>如表2所示。</p>
                </div>
                <div class="area_img" id="64">
                    <p class="img_tit"><b>表</b>1 <b>常规表情与相应</b>FACS AU<b>的对应关系</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Corresponding relationship between conventional expressions and corresponding FACS AU</p>
                    <p class="img_note"></p>
                    <table id="64" border="1"><tr><td><br />常规表情类别</td><td>FACS AU</td></tr><tr><td><br />高兴</td><td>AU6+AU12</td></tr><tr><td><br />厌恶</td><td>AU 9+AU15+AU16</td></tr><tr><td><br />惊奇</td><td>AU1+AU2+AU5+AU26</td></tr><tr><td><br />悲伤</td><td>AU1+AU4+AU15</td></tr><tr><td><br />害怕</td><td>AU1+AU2+AU4+AU5+AU7+AU20+AU26</td></tr><tr><td><br />愤怒</td><td>AU4+AU5+ AU7+AU23</td></tr><tr><td><br />轻蔑</td><td>AU12+AU14</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="65">
                    <p class="img_tit"><b>表</b>2 <b>微表情与相应</b>FACS AU<b>的对应关系</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Corresponding relationship between micro-expressions and corresponding FACS AU</p>
                    <p class="img_note"></p>
                    <table id="65" border="1"><tr><td><br />微表情类别</td><td>FACS AU</td></tr><tr><td><br />高兴</td><td>AU6或 AU12</td></tr><tr><td><br />厌恶</td><td>AU9或 AU10或AU4+AU7</td></tr><tr><td><br />惊奇</td><td>AU1+AU2, AU25或 AU2</td></tr><tr><td><br />压抑</td><td>AU15, AU17单独或者联合出现</td></tr><tr><td><br />其他</td><td>AU4或 AU14或AU17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">2.2 <b>划分局部关键区域</b></h4>
                <div class="p1">
                    <p id="67">根据表2中给出的微表情发生时相应AU所在的面部区域位置, 把面部划分为如图4所示的7个局部区域块, 其中, 1、2局部区域块为眼睛, 3为鼻子, 4为嘴巴, 5、6为面颊, 7为下巴。 每块区域所对应的FACS AU如表3所示。 通过对比表2和表3可知, 本文所划分的7个局部区域块涵盖了各微表情类别所对应的全部动作单元, 从而有效避免了与面部其他区域对微表情识别的影响。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 局部区域块的划分" src="Detail/GetImg?filename=images/JSJY201905007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 局部区域块的划分  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Partition of local blocks</p>

                </div>
                <div class="area_img" id="69">
                    <p class="img_tit"><b>表</b>3 <b>面部区域划分及其对应的</b>FACS AU <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Partition of facial regions and corresponding FACS AU</p>
                    <p class="img_note"></p>
                    <table id="69" border="1"><tr><td><br />区域序号</td><td>区域部位</td><td>FACS AU</td></tr><tr><td><br />1, 2</td><td>眼睛, 眉毛</td><td>AU1, AU2, AU4, AU7</td></tr><tr><td><br />3</td><td>鼻子</td><td>AU9</td></tr><tr><td><br />4</td><td>嘴巴</td><td>AU10, AU12, AU14, AU15, AU25</td></tr><tr><td><br />5, 6</td><td>面颊</td><td>AU6</td></tr><tr><td><br />7</td><td>下巴</td><td>AU17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="70">为了实现上述局部区域块的自动划分, 首先要进行面部关键点的定位。本文采用面部形变鉴别模型方法<citation id="144" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>自动标识人脸的49个关键点坐标。不同于经典的主动外观模型 (Active Appearance Model, AAM) <citation id="145" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>方法, 该方法属于判别式方法, 可以实现面部模型的增量学习, 因此对关键点的定位精度高, 而且速度可以达到每秒80 帧。49点位置如图5所示。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905007_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 49个关键点位置" src="Detail/GetImg?filename=images/JSJY201905007_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 49个关键点位置  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905007_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 49 key points</p>

                </div>
                <div class="p1">
                    <p id="72">依据相应关键点坐标, 分割出人脸的7个矩形局部区域块。 矩形区域框的位置和大小取决于4个边界 (即上、下、左、右) 对应的坐标点, 表4列出了这7个区域块边界对应的坐标点。 依据这一方法可以实现上述区域块的自动提取。</p>
                </div>
                <div class="area_img" id="73">
                    <p class="img_tit"><b>表</b>4 <b>各局部区域边界坐标点</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Boundary coordinates of each local region</p>
                    <p class="img_note"></p>
                    <table id="73" border="1"><tr><td><br />矩形区域框</td><td>左边</td><td>右边</td><td>上边</td><td>下边</td></tr><tr><td>左眼</td><td>1</td><td>5</td><td>3</td><td>24</td></tr><tr><td><br />右眼</td><td>6</td><td>10</td><td>8</td><td>31</td></tr><tr><td><br />鼻子</td><td>22</td><td>27</td><td>11</td><td>17与45的连线中点</td></tr><tr><td><br />嘴巴</td><td>32</td><td>38</td><td>34</td><td>41</td></tr><tr><td><br />左脸颊</td><td>1</td><td>4</td><td>24</td><td>15</td></tr><tr><td><br />右脸颊</td><td>7</td><td>10</td><td>31</td><td>19</td></tr><tr><td><br />下巴</td><td>32</td><td>38</td><td>41</td><td>图像下界</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">3 局部区域时空模式特征提取</h3>
                <div class="p1">
                    <p id="75">划分好局部区域块后, 接下来需要提取各区域块的时空模式特征。考虑到LBP描述子在人脸及表情识别领域的优异性能, 本文采用LBP的一种时空拓展LBP-TOP<citation id="146" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>描述子来提取人脸视频序列的特征。</p>
                </div>
                <div class="p1">
                    <p id="76">LBP-TOP是芬兰奥卢大学的赵国英等将描述静态纹理的局部二值模式<citation id="147" type="reference"><link href="17" rel="bibliography" /><link href="41" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">20</a>]</sup></citation>推广到动态纹理分析领域而得到的。它可以认为是从视频帧序列的相互垂直的<i>XY</i>、<i>XT</i>、<i>YT</i>三个平面分别提取各自的LBP直方图特征, 然后串联起来而得到的。<i>XT</i>、<i>YT</i>平面包含像素值随时间变化的信息, 因此LBP-TOP可以用于分类视频序列中的微表情。</p>
                </div>
                <div class="p1">
                    <p id="77">提取三个平面的LBP特征时, 在坐标轴<i>X</i>、<i>Y</i>及<i>T</i>上的半径分别记为<i>R</i><sub><i>X</i></sub>、<i>R</i><sub><i>Y</i></sub>及<i>R</i><sub><i>T</i></sub>;三个平面的邻域点数分别记为<i>P</i><sub><i>XY</i></sub>、<i>P</i><sub><i>XT</i></sub>及<i>P</i><sub><i>YT</i></sub>, 相应的LBP-TOP特征记为<i>LBP</i>-<i>TOP</i><sub><i>R</i><sub><i>X</i></sub>, <i>R</i><sub><i>Y</i></sub>, <i>R</i><sub><i>T</i></sub>, <i>P</i><sub><i>XY</i></sub>, <i>P</i><sub><i>XT</i></sub>, <i>P</i><sub><i>YT</i></sub></sub>。</p>
                </div>
                <div class="p1">
                    <p id="78">中心像素<i>g</i><sub><i>t</i><sub><i>c</i></sub>, <i>c</i></sub>的坐标记为 (<i>x</i><sub><i>c</i></sub>, <i>y</i><sub><i>c</i></sub>, <i>z</i><sub><i>c</i></sub>) , 则该像素在<i>XY</i>平面的邻域像素<i>g</i><sub><i>XY</i>, <i>p</i></sub>的坐标为:</p>
                </div>
                <div class="p1">
                    <p id="79"> (<i>x</i><sub><i>c</i></sub>-<i>R</i><sub><i>X</i></sub> sin (2π<i>p</i>/<i>P</i><sub><i>XY</i></sub>) , <i>y</i><sub><i>c</i></sub>+<i>R</i><sub><i>Y</i></sub> cos (2π<i>p</i>/<i>P</i><sub><i>XY</i></sub>) , <i>t</i><sub><i>c</i></sub>) ;<i>p</i>=0, 1, …, <i>P</i><sub><i>XY</i></sub>-1       (1) </p>
                </div>
                <div class="p1">
                    <p id="81">该像素在<i>XT</i>平面的邻域像素<i>g</i><sub><i>XT</i>, <i>p</i></sub>的坐标为:</p>
                </div>
                <div class="p1">
                    <p id="82"> (<i>x</i><sub><i>c</i></sub>-<i>R</i><sub><i>X</i></sub> sin (2π<i>p</i>/<i>P</i><sub><i>XT</i></sub>) , <i>y</i><sub><i>c</i></sub>, <i>t</i><sub><i>c</i></sub>-<i>R</i><sub><i>T</i></sub> cos (2π<i>p</i>/<i>P</i><sub><i>XT</i></sub>) ) ;<i>p</i>=0, 1, …, <i>P</i><sub><i>XT</i></sub>-1      (2) </p>
                </div>
                <div class="p1">
                    <p id="84">该像素在<i>YT</i>平面的邻域像素<i>g</i><sub><i>YT</i>, <i>p</i></sub>的坐标为:</p>
                </div>
                <div class="p1">
                    <p id="85"> (<i>x</i><sub><i>c</i></sub>, <i>y</i><sub><i>c</i></sub>+<i>R</i><sub><i>Y</i></sub> sin (2π<i>p</i>/<i>P</i><sub><i>YT</i></sub>) , <i>t</i><sub><i>c</i></sub>-<i>R</i><sub><i>T</i></sub> cos (2π<i>p</i>/<i>P</i><sub><i>YT</i></sub>) ) ;<i>p</i>=0, 1, …, <i>P</i><sub><i>YT</i></sub>-1      (3) </p>
                </div>
                <div class="p1">
                    <p id="87">以面部区域为一个整体, 分别计算<i>XY</i>、<i>XT</i>、<i>YT</i>三个平面的直方图并串联起来作为特征这一方法忽略了LBP模式的位置信息。因此, 可以采用把面部区域划分为若干块, 同时把视频时间轴也等分。接着计算每个立方块的三个平面的LBP直方图模式, 再把所有立方块的串联起来构成特征。这种特征称为全局特征。</p>
                </div>
                <div class="p1">
                    <p id="88">在某一立方块的某一平面上的直方图定义为:</p>
                </div>
                <div class="area_img" id="89">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201905007_08900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="91">其中:<i>n</i><sub><i>j</i></sub>是在<i>j</i>平面 (<i>j</i>=0:<i>XY</i>, 1:<i>XT</i>, 2:<i>YT</i>) 上LBP模式数目; <i>f</i><sub><i>j</i></sub> (<i>x</i><sub><i>c</i></sub>, <i>y</i><sub><i>c</i></sub>, <i>t</i><sub><i>c</i></sub>) 计算位置在 (<i>x</i><sub><i>c</i></sub>, <i>y</i><sub><i>c</i></sub>, <i>t</i><sub><i>c</i></sub>) 的像素在<i>j</i>平面上的LBP编码; <i>b</i>是像素 (<i>x</i><sub><i>c</i></sub>, <i>y</i><sub><i>c</i></sub>, <i>t</i><sub><i>c</i></sub>) 所在行分块序号, <i>l</i>是所在列分块序号。</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo></mtd><mtd columnalign="left"><mspace width="0.25em" /><mi>A</mi><mtext>为</mtext><mtext>真</mtext></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo></mtd><mtd columnalign="left"><mspace width="0.25em" /><mi>A</mi><mtext>为</mtext><mtext>假</mtext></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">通常将直方图采用如下方法归一化以得到一个更清晰的表达:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>b</mi><mo>, </mo><mi>l</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>, </mo><mi>i</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>Η</mi><msub><mrow></mrow><mrow><mi>b</mi><mo>, </mo><mi>l</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>, </mo><mi>i</mi></mrow></msub></mrow><mrow><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mn>1</mn></mrow></munderover><mi>Η</mi></mstyle><msub><mrow></mrow><mrow><mi>b</mi><mo>, </mo><mi>l</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>, </mo><mi>k</mi></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">采用上述方法分别提取2.2节划分的微表情视频序列7个局部区域的LBP-TOP特征; 接着将选定的若干区域的特征向量串联起来构成微表情特征向量; 然后输入训练好的支持向量机分类器进行微表情分类。本文方法的总体流程如图6所示。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905007_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 本文所提方法流程" src="Detail/GetImg?filename=images/JSJY201905007_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 本文所提方法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905007_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Flow chart of the proposed method</p>

                </div>
                <h3 id="97" name="97" class="anchor-tag">4 实验仿真</h3>
                <h4 class="anchor-tag" id="98" name="98">4.1 <b>微表情数据集</b>CASME2<b>与识别性能指标</b></h4>
                <div class="p1">
                    <p id="99">实验中使用的微表情数据集是中国科学院心理所提供的CASME2<citation id="148" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 该数据集包含了26位参与者录制的247个包含微表情的视频。微表情分类为高兴 (32个样本) 、厌恶 (64个样本) 、惊奇 (25个样本) 、压抑 (27个样本) 及其他 (99个样本) 共五种。</p>
                </div>
                <div class="p1">
                    <p id="100">采用留一交叉验证的方法检验本文所提算法的性能。具体来说, 将数据集按照参与者不同划分为26个子集, 从中选取25个作为训练集, 对机器学习分类器进行训练, 另外1个作为测试集, 提取其中每一视频序列的特征向量输入分类器进行微表情分类。这样就可获得26组训练/测试集, 依此进行的26组训练和测试称为一次实验。一次实验中分类正确的样本数目占总样本数 (247) 的比率即为总体分类准确率 (Accuracy, Acc) 。在本实验中采用支持向量机<citation id="149" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>作为分类器。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">4.2 <b>实验设计</b></h4>
                <div class="p1">
                    <p id="102">为研究面部不同关键区域的特征对微表情识别性能的影响, 通过2.1节定义的7个区域的不同组合作为一次实验的特征提取区域。考虑到与微表情有关的动作单元主要集中在眼睛周围及嘴巴附近, 以“1+2+4”为基本区域, 通过基本区域与其他区域的组合构成一次实验的特征提取的局部关键区域。设计的特征提取区域如下:</p>
                </div>
                <div class="p1">
                    <p id="103">A: 全局区域特征;</p>
                </div>
                <div class="p1">
                    <p id="104">B: “1+2+4”;</p>
                </div>
                <div class="p1">
                    <p id="105">C: “1+2+4+3”;</p>
                </div>
                <div class="p1">
                    <p id="106">D: “1+2+4+3+7”;</p>
                </div>
                <div class="p1">
                    <p id="107">E: “1+2+4+3+5+6”;</p>
                </div>
                <div class="p1">
                    <p id="108">F: “1+2+4+3+5+6+7”。</p>
                </div>
                <div class="p1">
                    <p id="109">在每一个子区域提取特征时, 子区域进一步按照分块 (<i>n</i>×<i>m</i>, <i>τ</i>) , 其中<i>n</i>×<i>m</i>代表空间分块个数, <i>τ</i>代表时间轴的分割份数。为便于比较, 根据文献<citation id="150" type="reference">[<a class="sup">14</a>]</citation>的设置方法, 令<i>n</i>=<i>m</i>;同时, 三个平面提取LBP特征时的半径设置为相等, 即<i>R</i><sub><i>X</i></sub>=<i>R</i><sub><i>Y</i></sub>=<i>R</i><sub><i>T</i></sub>=<i>r</i>;三个平面的邻域点数目亦设置为相等, 即<i>P</i><sub><i>XY</i></sub>=<i>P</i><sub><i>XT</i></sub>=<i>P</i><sub><i>YT</i></sub>=<i>p</i>。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">4.3 <b>实验结果及分析</b></h4>
                <h4 class="anchor-tag" id="111" name="111">4.3.1 各区域识别性能分析</h4>
                <div class="p1">
                    <p id="112">首先实验在全局区域及不同关键区域组合分块提取LBP-TOP特征然后进行微表情识别的性能。根据文献<citation id="151" type="reference">[<a class="sup">14</a>]</citation>, 分别实验 (<i>p</i>, <i>r</i>) 设置为 (4, 1) 、 (8, 2) 及 (8, 3) , (<i>n</i>×<i>m</i>, <i>τ</i>) 为不同情况时的识别准确率 (Acc) , 如表5所示。各区域性能最好的结果已用粗体标示。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit"><b>表</b>5 <b>不同区域在不同参数设置情况下的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Performance comparison of different regions under different parameter settings</p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td rowspan="2"> (<i>p</i>, <i>r</i>) </td><td colspan="2"><br />全局A</td><td rowspan="2"></td><td colspan="2"><br />区域B</td><td rowspan="2"></td><td colspan="2"><br />区域C</td><td rowspan="2"></td><td colspan="2"><br />区域D</td><td rowspan="2"></td><td colspan="2"><br />区域E</td><td rowspan="2"></td><td colspan="2"><br />区域F</td></tr><tr><td><br /> (<i>n</i>×<i>m</i>, <i>τ</i>) </td><td>Acc/%</td><td><br /> (<i>n</i>×<i>m</i>, <i>τ</i>) </td><td>Acc/%</td><td><br /> (<i>n</i>×<i>m</i>, <i>τ</i>) </td><td>Acc/%</td><td><br /> (<i>n</i>×<i>m</i>, <i>τ</i>) </td><td>Acc/%</td><td><br /> (<i>n</i>×<i>m</i>, <i>τ</i>) </td><td>Acc/%</td><td><br /> (<i>n</i>×<i>m</i>, <i>τ</i>) </td><td>Acc/%</td></tr><tr><td rowspan="4"><br /> (4, 1) </td><td> (5×5, 1) </td><td>40.89</td><td></td><td> (3×3, 1) </td><td>41.30</td><td></td><td> (3×3, 1) </td><td>40.08</td><td></td><td> (3×3, 1) </td><td>37.65</td><td></td><td> (3×3, 2) </td><td>49.39</td><td></td><td> (3×3, 2) </td><td>46.96</td></tr><tr><td><br /> (5×5, 2) </td><td>43.32</td><td></td><td> (3×3, 2) </td><td>46.15</td><td></td><td> (3×3, 2) </td><td>49.80</td><td></td><td> (3×3, 2) </td><td>43.32</td><td></td><td> (3×3, 3) </td><td>48.58</td><td></td><td> (3×3, 3) </td><td>44.53</td></tr><tr><td><br /> (5×5, 3) </td><td>41.30</td><td></td><td> (3×3, 4) </td><td>46.15</td><td></td><td> (3×3, 3) </td><td>47.77</td><td></td><td> (3×3, 3) </td><td>43.73</td><td></td><td> (5×5, 2) </td><td>48.99</td><td></td><td> (5×5, 2) </td><td>46.56</td></tr><tr><td><br /> (8×8, 1) </td><td>37.65</td><td></td><td> (5×5, 1) </td><td>42.91</td><td></td><td> (5×5, 2) </td><td>42.92</td><td></td><td> (5×5, 2) </td><td>42.51</td><td></td><td> (5×5, 3) </td><td>48.18</td><td></td><td> (5×5, 3) </td><td>46.15</td></tr><tr><td rowspan="4"><br /> (8, 2) </td><td> (8×8, 2) </td><td>46.15</td><td></td><td> (5×5, 3) </td><td>46.96</td><td></td><td> (5×5, 3) </td><td>45.34</td><td></td><td> (5×5, 3) </td><td>43.32</td><td></td><td> (5×5, 4) </td><td>46.96</td><td></td><td> (5×5, 4) </td><td>47.37</td></tr><tr><td><br /> (8×8, 3) </td><td>43.32</td><td></td><td> (5×5, 4) </td><td>47.37</td><td></td><td> (5×5, 4) </td><td>44.13</td><td></td><td> (5×5, 4) </td><td>44.53</td><td></td><td> (5×5, 5) </td><td>47.37</td><td></td><td> (5×5, 5) </td><td>46.15</td></tr><tr><td><br /> (8×8, 1) </td><td>40.89</td><td></td><td> (5×5, 5) </td><td>48.99</td><td></td><td> (3×3, 2) </td><td>52.23</td><td></td><td> (5×5, 5) </td><td>45.34</td><td></td><td> (3×3, 2) </td><td>52.23</td><td></td><td> (3×3, 2) </td><td>48.58</td></tr><tr><td><br /></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td> (3×3, 2) </td><td>46.96</td><td></td><td> (3×3, 2) </td><td>53.44</td><td></td><td> (3×3, 2) </td><td>49.39</td></tr><tr><td rowspan="2"> (8, 3) </td><td> (8×8, 2) </td><td>46.96</td><td></td><td> (3×3, 2) </td><td>51.42</td><td></td><td> (3×3, 3) </td><td>49.39</td><td rowspan="2"></td><td rowspan="2"> (3×3, 2) </td><td rowspan="2">46.96</td><td rowspan="2"></td><td rowspan="2"> (4×4, 2) </td><td rowspan="2">48.18</td><td rowspan="2"></td><td rowspan="2"> (4×4, 2) </td><td rowspan="2">46.15</td></tr><tr><td><br /> (8×8, 3) </td><td>44.94</td><td></td><td> (3×3, 2) </td><td>51.42</td><td></td><td> (3×3, 2) </td><td>48.58</td></tr><tr><td>平均值</td><td></td><td>42.82</td><td></td><td></td><td>46.96</td><td></td><td></td><td>46.69</td><td></td><td></td><td>43.81</td><td></td><td></td><td>49.26</td><td></td><td></td><td>46.87</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="114">由性能平均值可以看出在关键区域提取特征的B、C、D、E及F五种情况下的性能均优于在全局区域A提取特征的识别效果。对比各区域性能最好情况, B、C、D、E及F较A指标Acc提高的比例分别为11.42%、13.39%、1.76%、15.80%及7.02%, 平均为9.878%。</p>
                </div>
                <div class="p1">
                    <p id="115">分析各区域最好性能, C较B增加了区域3, 微表情识别性能得到了提升;E较C增加了区域5+6, 性能亦得到了提升。但D较C增加了区域7, F较E亦增加了区域7, 但是增加后的微表情识别率反而下降了。这充分说明, 微表情仅发生在某些局部区域。在提取特征时, 如果把与微表情动作关联度不大的区域考虑在内, 这些区域内的无关动作会降低微表情的识别性能。</p>
                </div>
                <div class="p1">
                    <p id="116">另外, 比较了本文算法与文献<citation id="152" type="reference">[<a class="sup">11</a>,<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>]</citation>的微表情识别算法在CASME2数据集的最好性能, 本文算法较上述算法识别率分别提高了7.44%、7.29%、11.15%及9.73%。这也充分说明了本文算法的优越性。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">4.3.2 各区域混淆矩阵分析</h4>
                <div class="p1">
                    <p id="118">特征提取区域为A、B、C、D、E及F六种情况下性能最好情况下的混淆矩阵如图7所示。通过混淆矩阵可以看出在不同特征提取区域情况下各种微表情的识别成功率。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201905007_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 局部区域方法进行微表情识别的混淆矩阵" src="Detail/GetImg?filename=images/JSJY201905007_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 局部区域方法进行微表情识别的混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201905007_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Confusion matrixes of local region method for micro-expression recognition</p>

                </div>
                <div class="p1">
                    <p id="120">在全局区域A的情况下, 厌恶微表情的识别成功率只有16%, 而高兴微表情的识别成功率达到50%;在局部区域B 的情况下, 厌恶和惊奇微表情的识别成功率较高, 分别达到47%、64%, 压抑微表情的识别成功率较低为15%;局部区域C对各个微表情的识别比较均衡, 高兴、厌恶、惊奇、压抑四种类别的识别成功率分别为41%、36%、56%及26%;区域D对压抑微表情的识别成功率较高为33%, 对高兴微表情的识别成功率相对较低为34%。区域E对高兴微表情的识别成功率较高为50%, 对压抑微表情的识别成功率比较低为7%;区域F对各类微表情的识别成功率比较均衡分别为44%、36%、52%及22%。</p>
                </div>
                <div class="p1">
                    <p id="121">通过上述分析可以看出, 不同区域划分方法对不同微表情类别的识别能力有很大的差异, 这从一个侧面说明了微表情肌肉动作幅度小, 不同的微表情类别出现在面部的不同局部区域。而采用局部关键区域的方法可以有效摒除与微表情无关区域对识别性能的影响, 提高微表情识别的成功率。</p>
                </div>
                <h3 id="122" name="122" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="123">微表情是个体在特定情形下, 无意识、不能自主控制的面部表情, 具有动作幅度小、持续时间短的特点。现有的微表情识别方法的步骤是: 首先对面部全局区域进行无差别分块, 然后分别提取各块的时空模式特征并串联构成特征向量, 再通过模式分类的方法实现识别。实际上, 微表情只牵涉到面部局部区域, 面部在产生微表情时也存在一些无关的肌肉动作, 全局方法提取这些无关变化的局部模式会降低特征向量对于微表情特征的表达能力, 进而影响识别效果。本文根据微表情发生时所牵涉到的动作单元所在区域, 通过面部关键点坐标, 将与微表情相关的7个局部区域划分出来。提取这些局部区域组合的时空模式并串联构成特征向量, 进行微表情识别。实验表明局部区域的方法充分利用了面部各局部区域的结构信息, 有效摒除与微表情无关区域对识别性能的影响, 较全局方法可以显著提高微表情识别的性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Philosophy of Deception">

                                <b>[1]</b> CLANCY M.The Philosophy of Deception[M].Oxford:Oxford University Press, 2009:118-133.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001370735&amp;v=MjI4MzhybFc3ek1JVjQ9Tmo3QmFyTzRIdEhOckloRlkrZ0tZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZp&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> SALTER F, GRAMMER K, RIKOWSKI A.Sex differences in negotiating with powerful males[J].Human Nature, 2005, 16 (3) :306-321.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The faces of engagement:automatic recognition of student engagementfrom facial expressions">

                                <b>[3]</b> WHITEHILL J, SERPELL Z, LIN Y C, et al.The faces of engagement:automatic recognition of student engagementfrom facial expressions[J].IEEE Transactions on Affective Computing, 2014, 5 (1) :86-98.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501311274&amp;v=MDYxNTNIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lLRjBRYXhJPU5pZk9mYks3SHRETnFvOUVaK29PRG5zOW9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> POOL L D, QUALTER P.Improving emotional intelligence and emotional self-efficacy through a teaching intervention for university students[J].Learning &amp; Individual Differences, 2012, 22 (3) :306-312.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110100483739&amp;v=MDU2OTF3WmVadEZpbmxVcjNJS0YwUWF4ST1OajdCYXJLOUg5RE1ybzlGWU9NTUMzOHdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> STEWART P A, WALLER B M, SCHUBERT J N.Presidential speechmaking style:emotional response to micro-expressions of facial affect[J].Motivation &amp; Emotion, 2009, 33 (2) :125-135.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Micro Expression Training Tool (METT)">

                                <b>[6]</b> EKMAN P.Micro Expression Training Tool (METT) [M].San Francisco:University of California, 2002:1877-1903.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=I see how you feel:Training laypeople and professionals to recognize fleeting emotions">

                                <b>[7]</b> FRANK M G, HERBASZ M, SINUK K, et al.I see how you feel:training laypeople and professionals to recognize fleeting emotions[C]// Proceedings of the 2009 Annual Meeting of the International Communication Association.New York:[s.n.], 2009:3515-3522.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600742093&amp;v=Mjc3OTZGaW5sVXIzSUtGMFFheEk9TmlmT2ZiSzdIdEROcVk5RlkrOE5ESFU2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> OJALA T, PIETIKAINEN M, HARWOOD D.A comparative study of texture measures with classification based on featured distributions[J].Pattern Recognition, 1996, 29 (1) :51-59.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic Texture Recognition Using Local Binary Patterns with an Application to Facial Expressions">

                                <b>[9]</b> ZHAO G, PIETIKAINEN M.Dynamic texture recognition using local binary patterns with an application to facial expressions[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (6) :915-928.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES5A5677F640C7E77943CC88F65E8EA7D0&amp;v=MTMyOTVmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekx1N3dhZz1OaWZPZmJiSkc5ZkxxUGxEWU90OEN3ayt5QjhYNlV3T1FIZVVxaGRBY2NmbFFzNg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> HUANG X, ZHAO G, HONG X, et al.Spontaneous facial micro-expression analysis using spatiotemporal completed local quantized patterns[J].Neurocomputing, 2016, 175:564-578.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lbp with six intersection points:Reducing redundant information in lbp-top for micro-expression recognition">

                                <b>[11]</b> WANG Y, SEE J, PHAN R C, et al.LBP with six intersection points:reducing redundant information in LBP-TOP for micro-expression recognition[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:525-537.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Centralized binary patterns embedded with image Euclidean distance for facial expression recognition">

                                <b>[12]</b> FU X, WEI W.Centralized binary patterns embedded with image Euclidean distance for facial expression recognition[C]// Proceedings of the 2008 4th International Conference on Natural Computation.Washington, DC:IEEE Computer Society, 2008:115-119.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining feature-level and decision-level fusion in a hierarchical classifier for emotion recognition in the wild">

                                <b>[13]</b> SUN B, LI L, WU X, et al.Combining feature-level and decision-level fusion in a hierarchical classifier for emotion recognition in the wild[J].Journal on Multimodal User Interfaces, 2016, 10 (2) :125-137.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards reading hidden emotions:a comparative study of spontaneous micro-expression spotting and recognition methods">

                                <b>[14]</b> LI X, HONG X, MOILANEN A, et al.Towards reading hidden emotions:a comparative study of spontaneous micro-expression spotting and recognition methods[J].IEEE Transactions on Affective Computing, 2018, 9 (4) :563-577.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Observer-based measurement of facial expression with the facial action coding system">

                                <b>[15]</b> COHN J F, AMBADAR Z, EKMAN P.Observer-based measurement of facial expression with the facial action coding system[J].Neuroscience Letters, 2007, 394 (3) :203-221.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic analysis of facial actions:a survey">

                                <b>[16]</b> MARTINEZ B, VALSTAR M F, JIANG B, et al.Automatic analysis of facial actions:a survey[J/OL].IEEE Transactions on Affective Computing, 2017 [2018- 06- 20].https://ieeexplore.ieee.org/document/7990582.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CASME II:an improved spontaneous micro-expression database and the baseline evaluation">

                                <b>[17]</b> YAN W J, LI X, WANG S J, et al.CASME II:an improved spontaneous micro-expression database and the baseline evaluation[J].PLoS One, 2014, 9 (1) :1-8.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incremental face alignment in the wild">

                                <b>[18]</b> ASTHANA A, ZAFEIRIOU S, CHENG S, et al.Incremental face alignment in the wild[C]// Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEE Computer Society, 2014:1859-1866.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active appearance models">

                                <b>[19]</b> COOTES T F, EDWARDS G J, TAYLOR C J.Active appearance models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2001, 23 (6) :681-685.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201412002&amp;v=MDE4MTVGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVY3N0lQeXJmYkxHNEg5WE5yWTk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 刘丽, 谢毓湘, 魏迎梅, 等.局部二进制模式方法综述[J].中国图象图形学报, 2014, 19 (12) :1696-1720. (LIU L, XIE Y X, WEI Y M, et al.Survey of local binary pattern method[J].Journal of Image and Graphics, 2014, 19 (12) :1696-1720.) 
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Monogenic riesz wavelet representation for micro expression recognition">

                                <b>[21]</b> OH Y, NGO A C, SEE J, et al.Monogenic Riesz wavelet representation for micro-expression recognition[C]// Proceedings of the 2015 IEEE International Conference on Digital Signal Processing.Piscataway, NJ:IEEE, 2015:1237-1241.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Subtle expression recognition using optical strain weighted features">

                                <b>[22]</b> LIONG S, SEE J, PHAN R C, et al.Subtle expression recognition using optical strain weighted features[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:644-657.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spontaneous subtle expression recognition:imbalanced databases and solutions">

                                <b>[23]</b> NGO A C, PHAN R C, SEE J, et al.Spontaneous subtle expression recognition:imbalanced databases and solutions[C]// Proceedings of the 12th Asian Conference on Computer Vision.Berlin:Springer, 2014:33-48.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201905007" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201905007&amp;v=MDY2MTJxbzlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEbVY3N0lMejdCZDdHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
