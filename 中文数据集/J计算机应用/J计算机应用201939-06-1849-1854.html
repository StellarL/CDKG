<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136753959346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906049%26RESULT%3d1%26SIGN%3doucZ0G1LOPyHEf00oYI8DV96El4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906049&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906049&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906049&amp;v=MDQ2NjdmWnVac0Z5RGtXN3ZJTHo3QmQ3RzRIOWpNcVk5QmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="1 算法设计 ">1 算法设计</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="1.1 &lt;b&gt;算法整体架构&lt;/b&gt;">1.1 <b>算法整体架构</b></a></li>
                                                <li><a href="#50" data-title="1.2 &lt;b&gt;基于&lt;/b&gt;ORB-SLAM2&lt;b&gt;的定位与建图算法&lt;/b&gt;">1.2 <b>基于</b>ORB-SLAM2<b>的定位与建图算法</b></a></li>
                                                <li><a href="#60" data-title="1.3 &lt;b&gt;基于立体视觉的障碍物检测算法&lt;/b&gt;">1.3 <b>基于立体视觉的障碍物检测算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="2 基于ROS的软件架构设计 ">2 基于ROS的软件架构设计</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="3.1 &lt;b&gt;数据集测试&lt;/b&gt;">3.1 <b>数据集测试</b></a></li>
                                                <li><a href="#84" data-title="3.2 &lt;b&gt;室内实物实验&lt;/b&gt;">3.2 <b>室内实物实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="图1 算法整体架构">图1 算法整体架构</a></li>
                                                <li><a href="#52" data-title="图2 ORB-SLAM2系统结构框图">图2 ORB-SLAM2系统结构框图</a></li>
                                                <li><a href="#75" data-title="图3 系统整体流程">图3 系统整体流程</a></li>
                                                <li><a href="#78" data-title="图4 系统节点总体设计">图4 系统节点总体设计</a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同方法的定位均方根误差对比&lt;/b&gt;"><b>表</b>1 <b>不同方法的定位均方根误差对比</b></a></li>
                                                <li><a href="#87" data-title="图5 实验硬件系统">图5 实验硬件系统</a></li>
                                                <li><a href="#90" data-title="图6 运动场景平面图">图6 运动场景平面图</a></li>
                                                <li><a href="#94" data-title="图7 关键帧轨迹与环境特征点地图">图7 关键帧轨迹与环境特征点地图</a></li>
                                                <li><a href="#97" data-title="图8 简单障碍物环境检测结果">图8 简单障碍物环境检测结果</a></li>
                                                <li><a href="#98" data-title="图9 复杂障碍物环境检测结果">图9 复杂障碍物环境检测结果</a></li>
                                                <li><a href="#99" data-title="图10 动态障碍物环境测试结果">图10 动态障碍物环境测试结果</a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;障碍物检测的距离误差&lt;/b&gt;"><b>表</b>2 <b>障碍物检测的距离误差</b></a></li>
                                                <li><a href="#111" data-title="图11 三种环境下每帧处理时间">图11 三种环境下每帧处理时间</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="135">


                                    <a id="bibliography_1" title="杨洋, 童东兵, 陈巧玉.面向未知地图的六足机器人路径规划算法[J].计算机应用, 2018, 38 (6) :1809-1813. (YANG Y, TONG D B, CHEN Q Y.Six-legged robot path planning algorithm for unknown map[J].Journal of Computer Applications, 2018, 38 (6) :1809-1813.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806048&amp;v=MDcyODVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURrVzd2SUx6N0JkN0c0SDluTXFZOUJiSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        杨洋, 童东兵, 陈巧玉.面向未知地图的六足机器人路径规划算法[J].计算机应用, 2018, 38 (6) :1809-1813. (YANG Y, TONG D B, CHEN Q Y.Six-legged robot path planning algorithm for unknown map[J].Journal of Computer Applications, 2018, 38 (6) :1809-1813.) 
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_2" title="NAYA B, SABRINA H, STEFAN W, et al.Navigation and vision system of a mobile robot[C]//Proceedings of the 2018 19th International Conference on Research and Education in Mechatronics.Piscataway, NJ:IEEE, 2018:99-104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Navigation and vision system of a mobile robot">
                                        <b>[2]</b>
                                        NAYA B, SABRINA H, STEFAN W, et al.Navigation and vision system of a mobile robot[C]//Proceedings of the 2018 19th International Conference on Research and Education in Mechatronics.Piscataway, NJ:IEEE, 2018:99-104.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_3" title="胡章芳, 鲍合章, 陈旭, 等.基于改进闭环检测算法的视觉同时定位与地图构建[J].计算机应用, 2018, 38 (3) :873-878. (HUZ F, BAO H Z, CHEN X, et al.Visual simultaneous location and mapping based on improved closed-loop detection algorithm[J].Journal of Computer Applications, 2018, 38 (3) :873-878.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201803046&amp;v=MTI1NTBuTXJJOUJZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURrVzd2SUx6N0JkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        胡章芳, 鲍合章, 陈旭, 等.基于改进闭环检测算法的视觉同时定位与地图构建[J].计算机应用, 2018, 38 (3) :873-878. (HUZ F, BAO H Z, CHEN X, et al.Visual simultaneous location and mapping based on improved closed-loop detection algorithm[J].Journal of Computer Applications, 2018, 38 (3) :873-878.) 
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_4" title="张永, 杨浩.基于优化视觉词袋模型的图像分类方法[J].计算机应用, 2017, 37 (8) :2244-2247, 2252. (ZHANG Y, YANG H.Image classification method based on optimized bag-of-visual words model[J].Journal of Computer Applications, 2017, 37 (8) :2244-2247, 2252.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201708022&amp;v=MDAxNDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGtXN3ZJTHo3QmQ3RzRIOWJNcDQ5SFpvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        张永, 杨浩.基于优化视觉词袋模型的图像分类方法[J].计算机应用, 2017, 37 (8) :2244-2247, 2252. (ZHANG Y, YANG H.Image classification method based on optimized bag-of-visual words model[J].Journal of Computer Applications, 2017, 37 (8) :2244-2247, 2252.) 
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_5" title="NISTER D, NARODITSKY O, BERGEN J R.Visual odometry[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2004:652-659." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual odometry">
                                        <b>[5]</b>
                                        NISTER D, NARODITSKY O, BERGEN J R.Visual odometry[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2004:652-659.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_6" title="ENGEL J, SCHOPS T, CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[C]//Proceedings of the 2014 13th European Conference on Computer Vision, LNCS 8690.Berlin:Springer, 2014:834-849." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSD-SLAM:Large-scale direc monocular SLAM">
                                        <b>[6]</b>
                                        ENGEL J, SCHOPS T, CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[C]//Proceedings of the 2014 13th European Conference on Computer Vision, LNCS 8690.Berlin:Springer, 2014:834-849.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_7" title="SIM R, ELINAS P, GRIFFIN M, et al.Design and analysis of a framework for real-time vision-based SLAM using Rao-Blackwellised particle filters[C]//CRV 2006:Proceedings of the 3rd Canadian Conference on Computer and Robot Vision.Washington, DC:IEEEComputer Society, 2006:21-29." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design and analysis of a framework for real-time visionbased SLAM using Rao-Blackwellised particle filters">
                                        <b>[7]</b>
                                        SIM R, ELINAS P, GRIFFIN M, et al.Design and analysis of a framework for real-time vision-based SLAM using Rao-Blackwellised particle filters[C]//CRV 2006:Proceedings of the 3rd Canadian Conference on Computer and Robot Vision.Washington, DC:IEEEComputer Society, 2006:21-29.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_8" title="YANG M L, LIU Y G, YOU Z S.The Euclidean embedding learning based on convolutional neural network for stereo matching[J].Neurocomputing, 2017, 267:195-200." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES261786373F4801D1FFE401460D535819&amp;v=MTM2OTRVMDV0cGh6TG0zeEtnPU5pZk9mYkcrSDliRXFZeENaNTBMQkh3NHV4ZGxuRXA1U0g3bXFoSkJmTEdSVGJ1V0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        YANG M L, LIU Y G, YOU Z S.The Euclidean embedding learning based on convolutional neural network for stereo matching[J].Neurocomputing, 2017, 267:195-200.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_9" title="KLEIN G, MURRAY D.Parallel tracking and mapping for small AR workspaces[C]//ISMAR 2007:Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality.Washington, DC:IEEE Computer Society, 2007:225-234." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel tracking and mapping for small AR workspaces">
                                        <b>[9]</b>
                                        KLEIN G, MURRAY D.Parallel tracking and mapping for small AR workspaces[C]//ISMAR 2007:Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality.Washington, DC:IEEE Computer Society, 2007:225-234.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_10" title="MUR-ARTAL R, MONTIEL J M M, TARDOS J D.ORB-SLAM:a versatile and accurate monocular SLAM system[J].IEEETransactions on Robotics, 2015, 31 (5) :1147-1163." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM: A Versatile and Accurate Monocular SLAM System">
                                        <b>[10]</b>
                                        MUR-ARTAL R, MONTIEL J M M, TARDOS J D.ORB-SLAM:a versatile and accurate monocular SLAM system[J].IEEETransactions on Robotics, 2015, 31 (5) :1147-1163.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_11" title="周武, 赵春霞, 沈亚强, 等.基于全局观测地图模型的SLAM研究[J].机器人, 2010, 32 (5) :647-654. (ZHOU W, ZHAO CX, SHEN Y Q, et al.SLAM research based on global observation map model[J].Robot, 2010, 32 (5) :647-654.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201005012&amp;v=MDI2OTVrVzd2SUx6elpmTEc0SDlITXFvOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeUQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        周武, 赵春霞, 沈亚强, 等.基于全局观测地图模型的SLAM研究[J].机器人, 2010, 32 (5) :647-654. (ZHOU W, ZHAO CX, SHEN Y Q, et al.SLAM research based on global observation map model[J].Robot, 2010, 32 (5) :647-654.) 
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_12" title="王轩, 叶平, 贾庆轩.双目立体视觉栅格地图构建方法[J].软件, 2012, 33 (11) :233-236. (WANG X, YE P, JIA Q X.Stereo vision location and grid map building method[J].Computer Engineering&amp;amp;Software, 2012, 33 (11) :233-236.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJZZ201211063&amp;v=MjE2MjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlEa1c3dklOeWZSZExHNEg5UE5ybzlEWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        王轩, 叶平, 贾庆轩.双目立体视觉栅格地图构建方法[J].软件, 2012, 33 (11) :233-236. (WANG X, YE P, JIA Q X.Stereo vision location and grid map building method[J].Computer Engineering&amp;amp;Software, 2012, 33 (11) :233-236.) 
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_13" title="陈炜楠, 朱蕾, 张宏, 等.稀疏视觉SLAM对平面激光雷达传感的稠密化模拟[J].机器人, 2018, 40 (3) :273-281. (CHEN W N, ZHUL, ZHANG H, et al.Planar Li DAR densified simulation from sparse visual SLAM[J].Robot, 2018, 40 (3) :273-281.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201803003&amp;v=MjgwMTlJTHp6WmZMRzRIOW5Nckk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGtXN3Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        陈炜楠, 朱蕾, 张宏, 等.稀疏视觉SLAM对平面激光雷达传感的稠密化模拟[J].机器人, 2018, 40 (3) :273-281. (CHEN W N, ZHUL, ZHANG H, et al.Planar Li DAR densified simulation from sparse visual SLAM[J].Robot, 2018, 40 (3) :273-281.) 
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_14" title="MUR-ARTAL R, TARDOS J D.ORB-SLAM2:an open-source SLAM system for monocular, stereo and RGB-D cameras[J].IEEETransactions on Robotics, 2016, 33 (5) :1255-1262." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM2:an open-source SLAM system for monocular,stereo and RGB-D cameras">
                                        <b>[14]</b>
                                        MUR-ARTAL R, TARDOS J D.ORB-SLAM2:an open-source SLAM system for monocular, stereo and RGB-D cameras[J].IEEETransactions on Robotics, 2016, 33 (5) :1255-1262.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_15" title="HIRSCHMULLER H.Accurate and efficient stereo processing by semi-global matching and mutual information[C]//CVPR 2005:Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2005:807-814." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate and efficient stereo processing by semi-global matching and mutual information">
                                        <b>[15]</b>
                                        HIRSCHMULLER H.Accurate and efficient stereo processing by semi-global matching and mutual information[C]//CVPR 2005:Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2005:807-814.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_16" title="GALVEZ-LOPEZ D, TARDOS J D.Bags of binary words for fast place recognition in image sequences[J].IEEE Transactions on Robotics, 2012, 28 (5) :1188-1197." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bags of Binary Words for Fast Place Recognition in Image Sequences">
                                        <b>[16]</b>
                                        GALVEZ-LOPEZ D, TARDOS J D.Bags of binary words for fast place recognition in image sequences[J].IEEE Transactions on Robotics, 2012, 28 (5) :1188-1197.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_17" title="SUZUKI S, ABE K.Topological structural analysis of digitized binary images by border following[J].Computer Vision, Graphics, and Image Processing, 1985, 30 (1) :32-46." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Topological structural analysis of digitized binary images by border following">
                                        <b>[17]</b>
                                        SUZUKI S, ABE K.Topological structural analysis of digitized binary images by border following[J].Computer Vision, Graphics, and Image Processing, 1985, 30 (1) :32-46.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_18" title="GEIGER A, LENZ P, STILLER C, et al.Vision meets robotics:the KITTI dataset[J].International Journal of Robotics Research, 2013, 32 (11) :1231-1237." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vision meets robotics:the KITTI dataset">
                                        <b>[18]</b>
                                        GEIGER A, LENZ P, STILLER C, et al.Vision meets robotics:the KITTI dataset[J].International Journal of Robotics Research, 2013, 32 (11) :1231-1237.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-29 10:11</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1849-1854 DOI:10.11772/j.issn.1001-9081.2018102187            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于视觉的机器人自主定位与障碍物检测方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%81%E6%96%97%E5%BB%BA&amp;code=41987920&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">丁斗建</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E6%99%93%E6%9E%97&amp;code=30884660&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵晓林</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%95%BF%E6%A0%B9&amp;code=42679757&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王长根</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E5%85%B3%E6%A0%B9&amp;code=23334458&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高关根</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AF%87%E7%A3%8A&amp;code=29321769&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">寇磊</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E8%A3%85%E5%A4%87%E7%AE%A1%E7%90%86%E4%B8%8E%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0274788&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军工程大学装备管理与无人机工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B94639%E9%83%A8%E9%98%9F&amp;code=1745212&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军94639部队</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E9%A3%9E%E8%A1%8C%E8%87%AA%E5%8A%A8%E6%8E%A7%E5%88%B6%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0107214&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安飞行自动控制研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对稀疏型同时定位与地图构建 (SLAM) 算法环境信息丢失导致无法检测障碍物问题, 提出一种基于视觉的机器人自主定位与障碍物检测方法。首先, 利用双目相机得到观测场景的视差图。然后, 在机器人操作系统 (ROS) 架构下, 同时运行定位与建图和障碍物检测两个节点。定位与建图节点基于ORB-SLAM2完成位姿估计与环境建图。障碍物检测节点引入深度阈值, 将视差图二值化;运用轮廓提取算法得到障碍物轮廓信息并计算障碍物凸包面积;再引入面积阈值, 剔除误检测区域, 从而实时准确地解算出障碍物坐标。最后, 将检测到的障碍物信息插入到环境的稀疏特征地图当中。实验结果表明, 该方法能够在实现机器人自主定位的同时, 快速检测出环境中的障碍物, 检测精度能够保证机器人顺利避障。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉定位;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%9C%E7%A2%8D%E7%89%A9%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">障碍物检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E5%90%8C%E6%97%B6%E5%AE%9A%E4%BD%8D%E4%B8%8E%E5%9C%B0%E5%9B%BE%E6%9E%84%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉同时定位与地图构建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器人操作系统;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">立体视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E4%BA%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器人;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    丁斗建 (1995—) , 男, 江西南昌人, 硕士研究生, 主要研究方向:视觉导航、视觉同时定位与地图构建;;
                                </span>
                                <span>
                                    *赵晓林 (1982—) , 男, 山东东明人, 副教授, 博士, 主要研究方向:无人机协同控制、无人机视觉导航;85327505@qq.com;
                                </span>
                                <span>
                                    王长根 (1993—) , 男, 江西吉安人, 工程师, 主要研究方向:视觉导航、视觉同时定位与地图构建;;
                                </span>
                                <span>
                                    高关根 (1983—) , 男, 安徽太和人, 高级工程师, 硕士, 主要研究方向:卫星导航、多导航源信息融合;;
                                </span>
                                <span>
                                    寇磊 (1981—) , 女, 陕西西安人, 高级工程师, 主要研究方向:卫星导航。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-31</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61503405);</span>
                                <span>航空科学基金资助项目 (20160896007);</span>
                    </p>
            </div>
                    <h1><b>Autonomous localization and obstacle detection method of robot based on vision</b></h1>
                    <h2>
                    <span>DING Doujian</span>
                    <span>ZHAO Xiaolin</span>
                    <span>WANG Changgen</span>
                    <span>GAO Guangen</span>
                    <span>KOU Lei</span>
            </h2>
                    <h2>
                    <span>Equipment Management and UAV Engineering College, Air Force Engineering University</span>
                    <span>Chinese People's Liberation Army 94639 Troop</span>
                    <span>Xi'an Flight Automatic Control Research Institute</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the obstacle detection problem caused by the loss of environmental information in sparse Simultaneous Localization And Mapping (SLAM) algorithm, an autonomous location and obstacle detection method of robot based on vision was proposed. Firstly, the parallax map of the observed scene was obtained by binocular camera. Secondly, under the framework of Robot Operating System (ROS) , localization and mapping node and obstacle detection node were operated simultaneously. The localization and mapping node completed pose estimation and map building based on ORB-SLAM2. In the obstacle detection node, a depth threshold was introduced to binarize the parallax graph and the contour extraction algorithm was used to obtain the contour information of the obstacle and calculate the convex hull area of the obstacle, then an area threshold was introduced to eliminate the false detection areas, so as to accurately obtain the coordinates of obstacles in real time. Finally, the detected obstacle information was inserted into the sparse feature map of the environment. Experiment results show that this method can quickly detect obstacles in the environment while realizing autonomous localization of the robot, and the detection accuracy can ensure the robot to avoid obstacles smoothly.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20localization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual localization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=obstacle%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">obstacle detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Visual%20Simultaneous%20Localization%20And%20Mapping%20(VSLAM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Visual Simultaneous Localization And Mapping (VSLAM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=robot%20operating%20system&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">robot operating system;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stereo%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stereo vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=robot&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">robot;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    DING Doujian, born in 1995, M. S. candidate. His research interests include visual navigation, visual simultaneous localization and mapping.;
                                </span>
                                <span>
                                    ZHAO Xiaolin, born in 1982, Ph. D. , associate professor. His research interests include unmanned aerial vehicle collaborative control, unmanned aerial vehicle visual navigation. ;
                                </span>
                                <span>
                                    WANG Changgen, born in 1993, engineer. His research interests include visual navigation, visual simultaneous localization and mapping. ;
                                </span>
                                <span>
                                    GAO Guangen, born in 1983, M. S. , senior engineer. His research interests include satellite navigation, multiple navigation source information fusion. ;
                                </span>
                                <span>
                                    KOU Lei, born in 1981, senior engineer. Her research interests include satellite navigation.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-31</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61503405);</span>
                                <span>the Aeronautical Science Foundation (20160896007);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="40">随着机器人应用领域的不断拓展, 对其自主性的要求也在不断提升<citation id="171" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。要实现机器人自主移动, 自主定位技术和自主避障技术是最为关键的两个方面。在机器人自主定位与自主避障研究中, 视觉传感器由于具有功耗低、价格便宜、可以获得更加丰富的环境信息等优点<citation id="172" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 得到了广泛应用<citation id="173" type="reference"><link href="139" rel="bibliography" /><link href="141" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="41">基于视觉的机器人自主定位研究已有近二十年的历史。文献<citation id="174" type="reference">[<a class="sup">5</a>]</citation>中提出视觉里程计以后, 基于视觉里程计的视觉同时定位与地图构建 (Visual Simultaneous Localization And Mapping, VSLAM) 算法受到广泛的关注。VSLAM根据所利用的图像信息可分为基于特征的稀疏方法和直接的稠密方法。稠密方法可以建立密集的环境地图, 便于后续的自主导航, 具有代表性的工作是LSD-SLAM<citation id="175" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。但是, 稠密方法计算量很大, 难以做到较好的实时性, 因此, 出于减少计算量、提高系统实时性的目的, 稀疏型SLAM受到了许多研究者的青睐。文献<citation id="176" type="reference">[<a class="sup">7</a>]</citation>使用尺度不变特征变换 (Scale Invariant Feature Transform, SIFT) 特征检测方法, 解决了特征匹配中图像旋转、缩放以及光照变化的影响, 但SIFT特征的提取本身就比较耗时。文献<citation id="177" type="reference">[<a class="sup">8</a>]</citation>在特征匹配过程中应用了极线约束, 提高了匹配的速度与精度。文献<citation id="178" type="reference">[<a class="sup">9</a>]</citation>提供了一个大场景VSLAM框架, 采用关键帧优化方法, 成为了VSLAM的一个特色, 但由于缺少回环检测, 定位的精度并不高。针对这一问题, 文献<citation id="179" type="reference">[<a class="sup">10</a>]</citation>提出一种基于ORB (Oriented FAST and Rotated BRIEF) 特征的SLAM解决方案, 通过加入回环检测线程和自动初始化功能, 全面提高了系统的性能。但是, 以上提到的方法都是通过特征点来建图, 得到的只是稀疏的点图, 只反映了环境的局部特征信息。因此, 只依赖稀疏型SLAM算法, 能够实现较好的定位, 却无法辨别出环境中的障碍物, 难以直接用于后续的避障与路径规划等工作中。</p>
                </div>
                <div class="p1">
                    <p id="42">为解决上述问题, 文献<citation id="180" type="reference">[<a class="sup">11</a>]</citation>提出了一种基于全局观测地图模型的SLAM方案, 将全局密集信息嵌入到稀疏特征地图中, 并将EKF-SLAM算法与全局地图观测模型相结合, 得到了较为可信的密集地图。文献<citation id="181" type="reference">[<a class="sup">12</a>]</citation>基于双目立体视觉, 提出了一种栅格地图构建方法, 得到了包含环境几何信息的地图。文献<citation id="182" type="reference">[<a class="sup">13</a>]</citation>利用迭代滤波以及高斯分布对数据进行稠密化处理, 实现了对稀疏性VSLAM地图的数据补插。以上方法基本都是致力于对稀疏特征地图进行数据补充, 在得到密集地图的同时, 也会给系统引入较大的计算量。然而, 机器人实现一般的自主移动, 并不都需要所有的环境信息, 很多情况下只需检测出环境中的障碍物即可满足自主移动要求。</p>
                </div>
                <div class="p1">
                    <p id="43">针对上述情况, 本文利用立体视觉法检测障碍物来弥补稀疏型SLAM环境交互能力弱的缺陷, 提出了一种基于视觉的机器人自主定位与障碍物检测方法。首先, 利用双目相机得到场景视差图, 在视差图的基础上, 同时运行两个节点。定位与建图节点依赖成熟的视觉SLAM算法ORB-SLAM2<citation id="183" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>实现相应功能。障碍物检测节点首先对视差图进行三维重建与伪彩色处理, 得到特征点的三维信息并将疑似障碍物区域红色显示;然后依据机器人运动特性将视差图二值化, 在二值图像基础上提取障碍物轮廓与位置信息并将障碍物信息插入环境地图。通过实验验证可知, 该方法能够在机器人SLAM的同时, 快速检测出环境中的障碍物, 且检测效果较好。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag">1 算法设计</h3>
                <h4 class="anchor-tag" id="45" name="45">1.1 <b>算法整体架构</b></h4>
                <div class="p1">
                    <p id="46">算法主要由三个模块组成, 如图1所示, 包括视差图获取模块、障碍物检测模块和定位与建图模块。</p>
                </div>
                <div class="p1">
                    <p id="47">视差图获取模块首先由双目相机采集场景图像, 并转换为灰度图像;基于相机标定模型, 消除图片畸变并对图片进行立体校正;然后, 利用ORB特征提取算法提取左右图像的特征点信息;最后, 采用半全局立体匹配 (Semi-Global Block Matching, SGBM) 算法<citation id="184" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>计算视差, 得到较为精确的致密视差图。得到视差图之后, 定位与建图模块依据ORB-SLAM2算法, 实现相机位姿的跟踪并建立环境的稀疏特征地图。与此同时, 障碍物检测模块依据立体视觉法检测出环境中障碍物, 并将障碍物信息插入到定位与建图模块所建立的环境地图当中。</p>
                </div>
                <div class="p1">
                    <p id="48">算法最终得到一幅具有障碍物信息的环境地图, 这一地图包含了障碍物坐标信息、特征点三维信息和相机位姿, 在ORB-SLAM2基础上实现了环境信息的补充, 为机器人与环境的交互提供依据, 同时为后续的路径规划打下基础。</p>
                </div>
                <div class="area_img" id="49">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法整体架构" src="Detail/GetImg?filename=images/JSJY201906049_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法整体架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Overall framework of the proposed algorithm</p>

                </div>
                <h4 class="anchor-tag" id="50" name="50">1.2 <b>基于</b>ORB-SLAM2<b>的定位与建图算法</b></h4>
                <div class="p1">
                    <p id="51">定位与建图算法基于ORB-SLAM2实现, ORB-SLAM2功能非常全面, 并且具有良好的精度和实时性, 是当前稀疏型SLAM的代表。因此, 本文选择ORB-SLAM2的双目部分完成算法的定位与建图模块, 其整体的结构框图如图2所示, 主要包含了位姿跟踪、局部构图和回环检测三个线程。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 ORB-SLAM2系统结构框图" src="Detail/GetImg?filename=images/JSJY201906049_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 ORB-SLAM2系统结构框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 ORB-SLAM2 system structure diagram</p>

                </div>
                <div class="p1">
                    <p id="53">位姿跟踪线程通过寻找当前帧与地图点的匹配, 利用光束平差法 (Bundle Adjustment, BA) 来最小化重投影误差, 从而实现相机位姿的实时估计与优化, 其基本原理如式 (1) 所示:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">R</mi><mo>, </mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">}</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">R</mi><mo>, </mo><mi mathvariant="bold-italic">t</mi></mrow></munder><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>χ</mi></mrow></munder><mi>ρ</mi></mstyle><mo stretchy="false">∥</mo><mi>u</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mfrac><mi mathvariant="bold-italic">Κ</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">R</mi><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">其中:{<b><i>R</i></b>, <b><i>t</i></b>}为相机的位姿;<i>ρ</i>为鲁棒核函数;<b><i>K</i></b>为相机内参;<i>s</i><sub><i>i</i></sub>为尺度因子;<i>u</i><sub><i>i</i></sub>为当前帧关键点;<b><i>P</i></b><sub><i>i</i></sub>为与之匹配的地图点。</p>
                </div>
                <div class="p1">
                    <p id="56">局部构图线程依据相应准则管理地图点和关键帧并建立局部地图。利用局部BA实现优化, 如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo stretchy="false">{</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>i</mi><mo>∈</mo><mi>Ρ</mi><msub><mrow></mrow><mi>L</mi></msub><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>∈</mo><mi>Κ</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">}</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>χ</mi></mrow></munder><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><mi>Κ</mi><msub><mrow></mrow><mi>L</mi></msub><mstyle displaystyle="true"><mo>∪</mo><mi>Κ</mi></mstyle><msub><mrow></mrow><mi>F</mi></msub></mrow></munder><mi>ρ</mi></mstyle><mo stretchy="false">∥</mo><mi>u</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mfrac><mi mathvariant="bold-italic">Κ</mi><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>k</mi></msub><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中:<i>P</i><sub><i>L</i></sub>为关键帧中能观测到的地图点;<i>K</i><sub><i>L</i></sub>为共视关键帧;<i>K</i><sub><i>F</i></sub>为其他关键帧。与位姿跟踪不同, 此时关键帧位姿与地图点坐标都为优化变量。</p>
                </div>
                <div class="p1">
                    <p id="59">回环检测线程通过基于二进制词袋 (Bags of Binary Words, DBoW) <citation id="185" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>方法实现的位置识别模块来更正累积漂移误差并在新建的线程中执行全局BA优化。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.3 <b>基于立体视觉的障碍物检测算法</b></h4>
                <div class="p1">
                    <p id="61">双目立体视觉是通过两个不同位置的摄像机采集场景图像, 根据两幅不同角度的场景图中的对应点来计算视差。本文在视差图的基础上, 设计了障碍物检测算法。</p>
                </div>
                <div class="p1">
                    <p id="62">首先, 在视差图基础上, 利用三角测量原理将图像转换成三维点云。给定视差<i>d</i>、重投影矩阵<b><i>Q</i></b>和点坐标 (<i>x</i>, <i>y</i>) , 根据最简单的相似三角形关系, 就可以得到对应的三维坐标 (<i>X</i>/<i>W</i>, <i>Y</i>/<i>W</i>, <i>Z</i>/<i>W</i>) , 其转换公式为:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">[</mo><mtable><mtr><mtd><mi>X</mi></mtd><mtd><mi>Y</mi></mtd><mtd><mi>Ζ</mi></mtd><mtd><mi>W</mi></mtd></mtr></mtable><mo stretchy="false">]</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo><mi mathvariant="bold-italic">Q</mi><mo stretchy="false">[</mo><mtable><mtr><mtd><mi>x</mi></mtd><mtd><mi>y</mi></mtd><mtd><mi>d</mi></mtd><mtd><mn>1</mn></mtd></mtr></mtable><mo stretchy="false">]</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">然后按照视差值的不同将视界离散化为一个个平面, 从而对图像进行伪彩色处理, 如式 (4) ～ (6) 所示:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>2</mn><mn>5</mn><mn>5</mn><mo>-</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>&gt;</mo><mn>0</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>G</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>2</mn><mo>×</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mn>0</mn><mo>&lt;</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>≤</mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd><mn>2</mn><mo>×</mo><mo stretchy="false">[</mo><mn>2</mn><mn>5</mn><mn>5</mn><mo>-</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>&gt;</mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>2</mn><mn>5</mn><mn>5</mn><mo>-</mo><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>h</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>&gt;</mo><mn>0</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中:<i>h</i> (<i>x</i>, <i>y</i>) 代表经过整形缩放后各个像素点灰度值, 范围为 (0, 255) ;<i>R</i> (<i>x</i>, <i>y</i>) 、<i>G</i> (<i>x</i>, <i>y</i>) 、<i>B</i> (<i>x</i>, <i>y</i>) 表示各个像素点的RGB值。根据机器人运动特性, 引入深度阈值<i>T</i>, 将图像二值化, 如式 (7) 所示:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>Τ</mi></mtd></mtr><mtr><mtd><mn>1</mn><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>≥</mo><mi>Τ</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中: <i>f</i> (<i>x</i>, <i>y</i>) 为图像中各个像素点的深度值;<i>g</i> (<i>x</i>, <i>y</i>) 为二值化后的图像。</p>
                </div>
                <div class="p1">
                    <p id="70">最后, 利用轮廓提取算法<citation id="186" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>得到障碍物轮廓信息, 并计算障碍物凸包面积<i>S</i><sub><i>i</i></sub>, 引入面积阈值<i>H</i>, 如果面积小于阈值<i>H</i>, 则认为是噪声不予以考虑, 如式 (8) 所示:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mtext> </mtext><mtext> </mtext><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mi>Η</mi></mtd></mtr><mtr><mtd><mn>1</mn><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≥</mo><mi>Η</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中:<i>P</i><sub><i>i</i></sub>表示是否输出凸包<i>i</i>。算法最终输出的是障碍物中心坐标与轮廓。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">2 基于ROS的软件架构设计</h3>
                <div class="p1">
                    <p id="74">本文算法基于机器人操作系统 (<i>Robot Operating System</i>, <i>ROS</i>) 架构实现, 系统的整体流程如图3所示。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 系统整体流程" src="Detail/GetImg?filename=images/JSJY201906049_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 系统整体流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Overall flow chart of the proposed system</i></p>

                </div>
                <div class="p1">
                    <p id="76">系统在开始阶段, 首先从外部文件中读取摄像头参数并将参数初始化;然后, 由图像采集节点不断地发布相机采集的场景图像, 得到图像信息后, <i>ORB</i>-<i>SLAM</i>2节点与障碍物检测节点同时运行并订阅图像信息。<i>ORB</i>-<i>SLAM</i>2节点逐渐完成系统的初始化并针对输入的图像不断执行位姿跟踪、局部构图和回环检测三个线程, 从而实现相机位姿的跟踪和环境特征地图构建。障碍物检测节点首先对摄像头参数进行检查, 在获得正确的参数情况下完成图像的立体匹配得到视差图, 然后在视差图的基础上依次执行算法的各个步骤, 从而实现环境中障碍物的检测并将障碍物信息发布出去, 这一信息将会被<i>ORB</i>-<i>SLAM</i>2节点订阅并将其插入到环境地图中。当两个节点同时判断系统关闭时关闭整个系统。</p>
                </div>
                <div class="p1">
                    <p id="77">系统的节点总体设计如图4所示。虚线框外的方框代表的是外部节点, 分别是机器人控制节点和传感器之间的坐标转换节点。虚线框内的是本文运行的节点, 图像采集节点主要完成图像的采集和分发;障碍物检测节点订阅来自图像采集节点的图像信息并实现障碍物检测和发布障碍物的信息;<i>ORB</i>_<i>SLAM</i>2节点订阅来自图像采集节点的图像信息处理后, 实时发布相机的位姿信息, 并订阅来自障碍物检测节点的障碍物信息插入环境地图。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 系统节点总体设计" src="Detail/GetImg?filename=images/JSJY201906049_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 系统节点总体设计  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Overall design of system nodes</i></p>

                </div>
                <h3 id="79" name="79" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="80" name="80">3.1 <b>数据集测试</b></h4>
                <div class="p1">
                    <p id="81">针对系统的定位性能, 本文利用著名的双目数据集<i>KITTI</i><citation id="187" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>对系统进行测试, 实验设备为一台配备<i>Intel core i</i>7处理器的笔记本电脑, 并装有64位的<i>Ubuntu</i>16.04操作系统。各数据子集上不同方法的定位均方根误差结果对比如表1所示。</p>
                </div>
                <div class="area_img" id="82">
                    <p class="img_tit"><b>表</b>1 <b>不同方法的定位均方根误差对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>. 1 <i>Comparison of root</i>-<i>mean</i>-<i>square error of different methods</i></p>
                    <p class="img_note">m</p>
                    <table id="82" border="1"><tr><td><br />数据子集</td><td><i>LSD</i>-<i>SLAM</i></td><td><i>ORB</i>-<i>SLAM</i></td><td>本文算法</td></tr><tr><td>03</td><td>1.21</td><td>0.62</td><td>0.62</td></tr><tr><td><br />04</td><td>0.23</td><td>0.23</td><td>0.23</td></tr><tr><td><br />05</td><td>1.51</td><td>0.81</td><td>0.82</td></tr><tr><td><br />06</td><td>1.32</td><td>0.81</td><td>0.82</td></tr><tr><td><br />07</td><td>0.50</td><td>0.51</td><td>0.51</td></tr><tr><td><br />08</td><td>3.92</td><td>3.61</td><td>3.62</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="83">从表1中可以看出, 在大多数数据子集的测试中, 本文算法的定位精度优于稠密方法<i>LSD</i>-<i>SLAM</i>, 且和<i>ORB</i>-<i>SLAM</i>2的定位精度基本一致, 这是由于本文的定位与建图算法是基于<i>ORB</i>-<i>SLAM</i>2的双目模块实现的, 因此能够保证较高的定位精度。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">3.2 <b>室内实物实验</b></h4>
                <h4 class="anchor-tag" id="85" name="85">3.2.1 实验硬件选型</h4>
                <div class="p1">
                    <p id="86">实验硬件系统由两个同一型号的<i>USB</i>摄像机和图像处理终端共同组成, 并将其安装在轮式机器人<i>Arduino</i> 4<i>WD</i>上进行实验。为了实现左右图像序列的获取, 本文选择两个内参数相近的<i>JD</i>-202<i>USB</i>摄像机, 如图5 (<i>a</i>) 所示, 其感光元件为<i>CMOS</i>, 最大分辨率为1 280×720, 帧数率为30 <i>frame</i>/<i>s</i>。为了提高图像处理能力, 本文选择了香蕉派<i>BPI</i>-<i>M</i>3作为图像处理终端, 如图5 (<i>b</i>) 所示。香蕉派<i>BPI</i>-<i>M</i>3是一个拥有8核1.8 <i>GHz</i>处理器和2 <i>GB LPDDR</i>3内存的超级单板电脑, 它可以运行<i>Android</i>、<i>Ubuntu</i>等操作系统, 具有计算处理速度快、外设接口丰富、体积小巧轻便易挂载和开源的社区等特点, 满足本文算法的设计需求。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 实验硬件系统" src="Detail/GetImg?filename=images/JSJY201906049_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 实验硬件系统  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Experimental hardware system</i></p>

                </div>
                <h4 class="anchor-tag" id="88" name="88">3.2.2 实验方案设计</h4>
                <div class="p1">
                    <p id="89">实验数据来源于实验室运动场景, 运动场景的平面图以及设计的运动轨迹如图6所示。机器人在室内按预定轨迹运动一周, 运动的途中, 为在场景图标注的三个障碍物点设定三种不同的障碍物环境, 以验证算法在不同环境下的障碍物检测效果。障碍物1为简单环境, 只设置一个纹理清晰的静态障碍物;障碍物2为复杂环境, 是在简单环境的基础上添加了多个不同的静态障碍物;障碍物3为动态环境, 是指将静态障碍物换成运动的履带小车。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 运动场景平面图" src="Detail/GetImg?filename=images/JSJY201906049_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 运动场景平面图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Plan of moving scene</i></p>

                </div>
                <div class="p1">
                    <p id="91">简单环境和复杂环境实验过程中, 分别在机器人距离障碍物0.3 <i>m</i>、0.8 <i>m</i>、1.5 <i>m</i>、2.0 <i>m</i>、2.5 <i>m</i>、3.0 <i>m</i>的地方, 对相应场景进行障碍物检测, 分别记录系统的检测距离、单帧处理时间和误检率。动态障碍物环境下, 履带车在机器人前以不同的速度进行运动, 以此来检验系统对于运动目标的检测能力。深度阈值取对应的场景距离分别为1.8 <i>m</i>、2.5 <i>m</i>、3.2 <i>m</i>的三个值, 在不同阈值下进行多次实验;面积阈值取为100个像素点, 将凸包面积小于100个像素点的区域视为误检测区域, 予以剔除。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">3.2.3 结果分析</h4>
                <div class="p1">
                    <p id="93">自主定位方面, 最终生成的关键帧轨迹与环境特征点地图如图7所示。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 关键帧轨迹与环境特征点地图" src="Detail/GetImg?filename=images/JSJY201906049_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 关键帧轨迹与环境特征点地图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Key frame trajectory and environment feature point map</i></p>

                </div>
                <div class="p1">
                    <p id="95">对于实物实验的自主定位性能, 本文只针对实验结果作简要分析。精度方面, 形成闭环路径前, 系统生成的路径会出现局部的偏移, 闭环路径形成后, 系统开始进行全局优化, 对轨迹图和位姿进行调整。将生成的路径与设计的路径比较, 可以发现整体生成图与设计的路线拟合得比较好, 没有出现较大的偏差。实时性方面, 系统启动以后, 每秒处理帧数在14～18, 可以满足机器人一般运动状态下的定位需要。稳定性方面, 实验过程中, 有时会出现跟踪丢失的情况, 但是由于采用了关键帧存取, 实现了系统快速的重定位能力, 可以保证稳定的定位输出。</p>
                </div>
                <div class="p1">
                    <p id="96">障碍物检测方面, 三种环境下不同时刻的障碍物检测结果如图8～10所示, 图中子图 (<i>a</i>) 、 (<i>b</i>) 、 (<i>c</i>) 分别为机器人在距离障碍物2.0 <i>m</i>、1.5 <i>m</i>和0.8 <i>m</i>处的实验结果。简单环境和复杂环境的测试距离分别由远到近进行, 动态障碍物的运动分为沿相机坐标系的<i>X</i>轴和<i>Z</i>轴方向运动。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 简单障碍物环境检测结果" src="Detail/GetImg?filename=images/JSJY201906049_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 简单障碍物环境检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Test results of simple obstacle environment</p>

                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 复杂障碍物环境检测结果" src="Detail/GetImg?filename=images/JSJY201906049_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 复杂障碍物环境检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Test results of complex obstacle environment</p>

                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 动态障碍物环境测试结果" src="Detail/GetImg?filename=images/JSJY201906049_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 动态障碍物环境测试结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Test results of dynamic obstacle environment</p>

                </div>
                <div class="p1">
                    <p id="100">分别用A、B、C表示简单障碍物环境、复杂障碍物环境和动态障碍物环境下的检测结果, 实验结果主要从以下三个方面分析。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">1) 障碍物检测精度。</h4>
                <div class="p1">
                    <p id="102">障碍物检测精度主要从误检率与检测距离误差两个方面进行评价, 通过记录不同环境下的误检测和对比算法输出的障碍器检测距离与实际距离的数值来评价整个系统的精度。误检率定义为:</p>
                </div>
                <div class="p1">
                    <p id="103">误检率<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo><mfrac><mrow><mtext>误</mtext><mtext>检</mtext><mtext>测</mtext><mtext>数</mtext></mrow><mrow><mtext>总</mtext><mtext>检</mtext><mtext>测</mtext><mtext>数</mtext></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="105">所谓误检测, 是指物体位于深度阈值所对应的场景距离之外, 而被检测为障碍物, 或者指物体位于深度阈值所对应的场景距离之内, 而不被检测为障碍物。本文在不同设定阈值下进行多次实验, 得到了相应的统计结果。</p>
                </div>
                <div class="p1">
                    <p id="106">当障碍物数量较少时, 系统的误检率比较低, 仅为3.5%, 检测效果较好;当障碍物数量较多时, 误检率升高至7.3%;动态障碍物环境下, 误检率最高, 为12.9%。由此可以得出, 障碍物之间的相互干扰会影响障碍物的检测效果, 且易出现被检测成为一个大的障碍物的情况;同时运动过程也会造成匹配过程的准确度降低, 更容易出现障碍物丢失。在各个检测位置上, 算法输出的障碍物中心与机器人的距离和实际距离的误差如表2所示。</p>
                </div>
                <div class="area_img" id="107">
                    <p class="img_tit"><b>表</b>2 <b>障碍物检测的距离误差</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Distance error of obstacle detection </p>
                    <p class="img_note"> m</p>
                    <table id="107" border="1"><tr><td><br />实际距离</td><td>简单</td><td>复杂</td><td>动态</td></tr><tr><td><br />0.3</td><td>0.02</td><td>0.05</td><td>0.08</td></tr><tr><td><br />0.8</td><td>0.03</td><td>0.06</td><td>0.13</td></tr><tr><td><br />1.5</td><td>0.04</td><td>0.06</td><td>0.14</td></tr><tr><td><br />2.0</td><td>0.05</td><td>0.08</td><td>0.16</td></tr><tr><td><br />2.5</td><td>0.07</td><td>0.09</td><td>0.17</td></tr><tr><td><br />3.0</td><td>0.09</td><td>0.13</td><td>0.17</td></tr><tr><td><br />均值</td><td>0.05</td><td>0.08</td><td>0.14</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="108">从表2数据可知, 简单环境下, 障碍物检测的距离误差在5 cm左右;复杂环境下, 误差上升到8 cm左右, 两种环境下, 检测精度都较高, 在控制机器人规避障碍物时, 通过设定一定的余量, 即可实现机器人的顺利避障。而动态环境下误差最大, 达到近14 cm, 检测效果不太理想。同时, 本文还发现, 当障碍物沿<i>Y</i>轴运动时, 系统的持续跟踪检测能力较强;当障碍物沿<i>Z</i>轴运动时, 障碍物容易丢失, 提取的障碍物与实际大小相差较大。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">2) 障碍物检测实时性。</h4>
                <div class="p1">
                    <p id="110">障碍物检测的实时性通过系统处理每帧图像的时间来体现, 具体时间如图11所示。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906049_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 三种环境下每帧处理时间" src="Detail/GetImg?filename=images/JSJY201906049_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 三种环境下每帧处理时间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906049_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Processing time per frame under three environments</p>

                </div>
                <div class="p1">
                    <p id="112">从图11中可以看出, 简单环境下, 系统处理每帧图像的时间稳定在35 ms附近;复杂情况下上升至37 ms左右;动态环境下系统处理每帧图像耗费的时间最长, 为40 ms左右。总的来说, 三种环境下, 系统处理图像的速度都相对较快, 实时性较好, 基本能够满足机器人一般运动下顺利避障的要求。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113">3) 障碍物检测范围。</h4>
                <div class="p1">
                    <p id="114">障碍物检测范围是通过改变机器人与障碍物的距离来测试。在三种环境中, 简单环境的检测范围最大, 最小检测距离为0.3 m, 最大检测距离为2.9 m;复杂环境其次, 最小检测距离为0.6 m, 最大检测距离为2.8 m;动态环境下的最小检测距离为0.7 m, 最大检测距离为2.3 m。由此可以得出, 三种环境下, 障碍物检测范围都较宽, 且物体运动对测量范围的影响较大。</p>
                </div>
                <div class="p1">
                    <p id="115">综上所述, 所提算法能够在实现自主定位的同时, 检测出环境中的障碍物, 且在三种设定环境下, 障碍物检测效果都较好, 基本能够满足机器人一般运动下自主避障要求。其中, 简单环境下算法检测效果最好, 复杂环境次之, 动态障碍物对算法检测效果影响较大。</p>
                </div>
                <h3 id="116" name="116" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="117">本文主要研究了基于视觉的机器人自主定位和障碍物检测问题, 利用典型稀疏性<i>SLAM</i>算法<i>ORB</i>-<i>SLAM</i>2实现了算法的自主定位功能。针对<i>ORB</i>-<i>SLAM</i>2存在的环境信息丢失的问题, 在<i>ROS</i>架构下, 引入了基于深度的障碍物检测算法, 解决了其不能检测障碍物的问题。在基于立体视觉的障碍物检测算法中, 进一步引入深度与面积阈值, 提高算法的检测精度。将所提算法在搭建的实验平台上进行验证, 实验结果表明本文系统是有效可行的。如何在具有障碍物信息的环境地图中进行路径规划是未来需要进一步研究的方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="135">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806048&amp;v=MDIxMjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURrVzd2SUx6N0JkN0c0SDluTXFZOUJiSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>杨洋, 童东兵, 陈巧玉.面向未知地图的六足机器人路径规划算法[J].计算机应用, 2018, 38 (6) :1809-1813. (YANG Y, TONG D B, CHEN Q Y.Six-legged robot path planning algorithm for unknown map[J].Journal of Computer Applications, 2018, 38 (6) :1809-1813.) 
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Navigation and vision system of a mobile robot">

                                <b>[2]</b>NAYA B, SABRINA H, STEFAN W, et al.Navigation and vision system of a mobile robot[C]//Proceedings of the 2018 19th International Conference on Research and Education in Mechatronics.Piscataway, NJ:IEEE, 2018:99-104.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201803046&amp;v=MjY1NzZXN3ZJTHo3QmQ3RzRIOW5Nckk5QllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>胡章芳, 鲍合章, 陈旭, 等.基于改进闭环检测算法的视觉同时定位与地图构建[J].计算机应用, 2018, 38 (3) :873-878. (HUZ F, BAO H Z, CHEN X, et al.Visual simultaneous location and mapping based on improved closed-loop detection algorithm[J].Journal of Computer Applications, 2018, 38 (3) :873-878.) 
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201708022&amp;v=MjE2OTR6cXFCdEdGckNVUjdxZlp1WnNGeURrVzd2SUx6N0JkN0c0SDliTXA0OUhab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>张永, 杨浩.基于优化视觉词袋模型的图像分类方法[J].计算机应用, 2017, 37 (8) :2244-2247, 2252. (ZHANG Y, YANG H.Image classification method based on optimized bag-of-visual words model[J].Journal of Computer Applications, 2017, 37 (8) :2244-2247, 2252.) 
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual odometry">

                                <b>[5]</b>NISTER D, NARODITSKY O, BERGEN J R.Visual odometry[C]//Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2004:652-659.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSD-SLAM:Large-scale direc monocular SLAM">

                                <b>[6]</b>ENGEL J, SCHOPS T, CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[C]//Proceedings of the 2014 13th European Conference on Computer Vision, LNCS 8690.Berlin:Springer, 2014:834-849.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design and analysis of a framework for real-time visionbased SLAM using Rao-Blackwellised particle filters">

                                <b>[7]</b>SIM R, ELINAS P, GRIFFIN M, et al.Design and analysis of a framework for real-time vision-based SLAM using Rao-Blackwellised particle filters[C]//CRV 2006:Proceedings of the 3rd Canadian Conference on Computer and Robot Vision.Washington, DC:IEEEComputer Society, 2006:21-29.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES261786373F4801D1FFE401460D535819&amp;v=MjI2NThkbG5FcDVTSDdtcWhKQmZMR1JUYnVXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBoekxtM3hLZz1OaWZPZmJHK0g5YkVxWXhDWjUwTEJIdzR1eA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>YANG M L, LIU Y G, YOU Z S.The Euclidean embedding learning based on convolutional neural network for stereo matching[J].Neurocomputing, 2017, 267:195-200.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel tracking and mapping for small AR workspaces">

                                <b>[9]</b>KLEIN G, MURRAY D.Parallel tracking and mapping for small AR workspaces[C]//ISMAR 2007:Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality.Washington, DC:IEEE Computer Society, 2007:225-234.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM: A Versatile and Accurate Monocular SLAM System">

                                <b>[10]</b>MUR-ARTAL R, MONTIEL J M M, TARDOS J D.ORB-SLAM:a versatile and accurate monocular SLAM system[J].IEEETransactions on Robotics, 2015, 31 (5) :1147-1163.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201005012&amp;v=MjYzMDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURrVzd2SUx6elpmTEc0SDlITXFvOUVab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>周武, 赵春霞, 沈亚强, 等.基于全局观测地图模型的SLAM研究[J].机器人, 2010, 32 (5) :647-654. (ZHOU W, ZHAO CX, SHEN Y Q, et al.SLAM research based on global observation map model[J].Robot, 2010, 32 (5) :647-654.) 
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJZZ201211063&amp;v=MTczNzE0TzN6cXFCdEdGckNVUjdxZlp1WnNGeURrVzd2SU55ZlJkTEc0SDlQTnJvOURaNFFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>王轩, 叶平, 贾庆轩.双目立体视觉栅格地图构建方法[J].软件, 2012, 33 (11) :233-236. (WANG X, YE P, JIA Q X.Stereo vision location and grid map building method[J].Computer Engineering&amp;Software, 2012, 33 (11) :233-236.) 
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201803003&amp;v=MjI0NjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5RGtXN3ZJTHp6WmZMRzRIOW5Nckk5Rlo0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>陈炜楠, 朱蕾, 张宏, 等.稀疏视觉SLAM对平面激光雷达传感的稠密化模拟[J].机器人, 2018, 40 (3) :273-281. (CHEN W N, ZHUL, ZHANG H, et al.Planar Li DAR densified simulation from sparse visual SLAM[J].Robot, 2018, 40 (3) :273-281.) 
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM2:an open-source SLAM system for monocular,stereo and RGB-D cameras">

                                <b>[14]</b>MUR-ARTAL R, TARDOS J D.ORB-SLAM2:an open-source SLAM system for monocular, stereo and RGB-D cameras[J].IEEETransactions on Robotics, 2016, 33 (5) :1255-1262.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate and efficient stereo processing by semi-global matching and mutual information">

                                <b>[15]</b>HIRSCHMULLER H.Accurate and efficient stereo processing by semi-global matching and mutual information[C]//CVPR 2005:Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington, DC:IEEEComputer Society, 2005:807-814.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bags of Binary Words for Fast Place Recognition in Image Sequences">

                                <b>[16]</b>GALVEZ-LOPEZ D, TARDOS J D.Bags of binary words for fast place recognition in image sequences[J].IEEE Transactions on Robotics, 2012, 28 (5) :1188-1197.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Topological structural analysis of digitized binary images by border following">

                                <b>[17]</b>SUZUKI S, ABE K.Topological structural analysis of digitized binary images by border following[J].Computer Vision, Graphics, and Image Processing, 1985, 30 (1) :32-46.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vision meets robotics:the KITTI dataset">

                                <b>[18]</b>GEIGER A, LENZ P, STILLER C, et al.Vision meets robotics:the KITTI dataset[J].International Journal of Robotics Research, 2013, 32 (11) :1231-1237.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906049" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906049&amp;v=MDQ2NjdmWnVac0Z5RGtXN3ZJTHo3QmQ3RzRIOWpNcVk5QmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZJTWYxaVVtWjVZcnN4dE5mQUIrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
