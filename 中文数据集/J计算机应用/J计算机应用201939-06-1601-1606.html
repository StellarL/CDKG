<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136674625002500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201906009%26RESULT%3d1%26SIGN%3dyTr7A2HvoANyPdmQIl2kJlxEdLA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201906009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906009&amp;v=MTMzNjNVUjdxZlp1WnNGeS9oVmJyT0x6N0JkN0c0SDlqTXFZOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#45" data-title="1 传统卷积神经网络结构 ">1 传统卷积神经网络结构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="1.1 &lt;b&gt;卷积层&lt;/b&gt;">1.1 <b>卷积层</b></a></li>
                                                <li><a href="#54" data-title="1.2 &lt;b&gt;池化层&lt;/b&gt;">1.2 <b>池化层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="2 改进CNN结构 ">2 改进CNN结构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="2.1 &lt;b&gt;训练阶段最大值池化&lt;/b&gt;&lt;i&gt;Dropout&lt;/i&gt;">2.1 <b>训练阶段最大值池化</b><i>Dropout</i></a></li>
                                                <li><a href="#114" data-title="2.2 &lt;b&gt;测试阶段双重概率加权模型平均&lt;/b&gt;">2.2 <b>测试阶段双重概率加权模型平均</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="3 实验参数与数据集 ">3 实验参数与数据集</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#146" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#147" data-title="4.1 &lt;i&gt;MNIST&lt;/i&gt;&lt;b&gt;数据集&lt;/b&gt;">4.1 <i>MNIST</i><b>数据集</b></a></li>
                                                <li><a href="#159" data-title="4.2  CIFAR- 10&lt;b&gt;数据集&lt;/b&gt;">4.2  CIFAR- 10<b>数据集</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 二维卷积操作示意图">图1 二维卷积操作示意图</a></li>
                                                <li><a href="#66" data-title="图2 最大值池化操作图解">图2 最大值池化操作图解</a></li>
                                                <li><a href="#82" data-title="图3 最大值池化Dropout操作图解">图3 最大值池化Dropout操作图解</a></li>
                                                <li><a href="#136" data-title="图4 MNIST、CIFAR- 10数据集的部分示例图">图4 MNIST、CIFAR- 10数据集的部分示例图</a></li>
                                                <li><a href="#150" data-title="图5 &lt;i&gt;MNIST&lt;/i&gt;相同结构下不同方法测试错误率">图5 <i>MNIST</i>相同结构下不同方法测试错误率</a></li>
                                                <li><a href="#154" data-title="图6 &lt;i&gt;MNIST&lt;/i&gt;不同保留概率&lt;i&gt;p&lt;/i&gt;下测试错误率">图6 <i>MNIST</i>不同保留概率<i>p</i>下测试错误率</a></li>
                                                <li><a href="#158" data-title="图7 MNIST错误率与保留概率&lt;i&gt;p&lt;/i&gt;的关系">图7 MNIST错误率与保留概率<i>p</i>的关系</a></li>
                                                <li><a href="#162" data-title="图8 CIFAR- 10错误率与保留概率&lt;i&gt;p&lt;/i&gt;的关系">图8 CIFAR- 10错误率与保留概率<i>p</i>的关系</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="188">


                                    <a id="bibliography_1" title="BISHOP C.Pattern Recognition and Machine Learning[M].Berlin:Springer, 2006:560-571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pattern Recognition and Machine Learning">
                                        <b>[1]</b>
                                        BISHOP C.Pattern Recognition and Machine Learning[M].Berlin:Springer, 2006:560-571.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_2" title="SCHMIDHUBER J.Deep learning in neural networks:an overview[J].Neural Networks, 2015, 61:85-117." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700296012&amp;v=MjAyMzVHZXJxUVRNbndaZVp0RmlubFVyM0lKMW9TYnhRPU5pZk9mYks4SDlETXFJOUZadUlKREgwN29CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        SCHMIDHUBER J.Deep learning in neural networks:an overview[J].Neural Networks, 2015, 61:85-117.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_3" title="LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521:436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning">
                                        <b>[3]</b>
                                        LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521:436-444.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_4" title="周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251. (ZHOU F Y, JIN L P, DONG J.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MjgzODBMejdCZHJHNEg5Yk1xWTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFZick8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251. (ZHOU F Y, JIN L P, DONG J.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251.) 
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_5" title="HINTON G E, SRIVASTAVA N, KRIZHEVSKY A, et al.Improving neural networks by preventing co-adaptation of feature detector[J].ar Xiv Preprint, 2012, 2012:ar Xiv.1207.0580." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving neural networks by preventing co-adaptation of feature detector">
                                        <b>[5]</b>
                                        HINTON G E, SRIVASTAVA N, KRIZHEVSKY A, et al.Improving neural networks by preventing co-adaptation of feature detector[J].ar Xiv Preprint, 2012, 2012:ar Xiv.1207.0580.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_6" title="SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research, 2014, 15 (1) :1929-1958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">
                                        <b>[6]</b>
                                        SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research, 2014, 15 (1) :1929-1958.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_7" title="GOOGFELLOW I, WARDE-FARLEY D, MIRZA M, et al.Maxout networks[C]//Proceedings of the 30th International Conference on Machine Learning.Atlanta, Georgia:JMLR W&amp;amp;CP, 2013:1319-1327." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maxout networks">
                                        <b>[7]</b>
                                        GOOGFELLOW I, WARDE-FARLEY D, MIRZA M, et al.Maxout networks[C]//Proceedings of the 30th International Conference on Machine Learning.Atlanta, Georgia:JMLR W&amp;amp;CP, 2013:1319-1327.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_8" title="WAN L, ZEILER M D, ZHANG S, et al.Regularization of neural networks using dropconnect[C]//Proceeding of the 30th International Conference on Machine Learning.Atlanta, Georgia:JMLRW&amp;amp;CP, 2013:1058-1066." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Regularization of Neural Networks Using Ropconnect">
                                        <b>[8]</b>
                                        WAN L, ZEILER M D, ZHANG S, et al.Regularization of neural networks using dropconnect[C]//Proceeding of the 30th International Conference on Machine Learning.Atlanta, Georgia:JMLRW&amp;amp;CP, 2013:1058-1066.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_9" title="LAMBERT J, SENER O, SAVARESE S.Deep learning under privileged information using heteroscedastic dropout[EB/OL].[2018-08-29].https://arxiv.org/pdf/1805.11614.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning under privileged information using heteroscedastic dropout">
                                        <b>[9]</b>
                                        LAMBERT J, SENER O, SAVARESE S.Deep learning under privileged information using heteroscedastic dropout[EB/OL].[2018-08-29].https://arxiv.org/pdf/1805.11614.pdf.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_10" title="ZEILER M D, FERGUS R.Stochastic pooling for regularization of deep convolutional neural networks[EB/OL].[2018-09-17].https://arxiv.org/pdf/1301.3557.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic Pooling for Regularization of Deep Convolutional Neural Networks">
                                        <b>[10]</b>
                                        ZEILER M D, FERGUS R.Stochastic pooling for regularization of deep convolutional neural networks[EB/OL].[2018-09-17].https://arxiv.org/pdf/1301.3557.pdf.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_11" title="WU H, GU X.Towards dropout training for convolutional neural networks[J].Neural Networks, 2015, 71:1-10." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6AC2EFB7CEA0C6A9FE9AF8F9C65A8CA3&amp;v=MTA1MDRGNTUrREE4L3ZoOWxuellNUG5lVXBXRXpmTU9jTnN1Y0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHc3eTV4YTQ9TmlmT2ZiWEpiZE81MmYxQw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        WU H, GU X.Towards dropout training for convolutional neural networks[J].Neural Networks, 2015, 71:1-10.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_12" title="LIN M, CHEN Q, YAN S.Network in network[J/OL].ar Xiv Preprint, [2018-09-17].https://arxiv.org/pdf/1312.4400.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network in Network[C/OL]">
                                        <b>[12]</b>
                                        LIN M, CHEN Q, YAN S.Network in network[J/OL].ar Xiv Preprint, [2018-09-17].https://arxiv.org/pdf/1312.4400.pdf.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_13" title="赵慧珍, 刘付显, 李龙跃, 等.基于混合maxout单元的卷积神经网络性能优化[J].通信学报, 2017, 38 (7) :105-114. (ZHAOH Z, LIU F X, LI L Y, et al.Improving deep convolutional neural networks with mixed maxout units[J].Journal on Communications, 2017, 38 (7) :105-114.) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201707012&amp;v=MTE2NzA1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFZick9NVFhUYkxHNEg5Yk1xSTlFWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        赵慧珍, 刘付显, 李龙跃, 等.基于混合maxout单元的卷积神经网络性能优化[J].通信学报, 2017, 38 (7) :105-114. (ZHAOH Z, LIU F X, LI L Y, et al.Improving deep convolutional neural networks with mixed maxout units[J].Journal on Communications, 2017, 38 (7) :105-114.) 
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                    KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York:Curran Associates Inc., 2012:1097-1105.</a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_15" title="KRIZHEVSKY A.Learning multiple layers of features from tiny images[D].Toronto:University of Toronto, 2009:16-22." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multiple layers of features from tiny images">
                                        <b>[15]</b>
                                        KRIZHEVSKY A.Learning multiple layers of features from tiny images[D].Toronto:University of Toronto, 2009:16-22.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_16" title="AL-SAFFAR A A M, TAO H, TALAB M A.Review of deep convolution neural network in image classification[C]//Proceedings of the 2017 International Conference on Radar, Antenna, Microwave, Electronics, and Telecommunications.Piscataway, NJ:IEEE, 2017:26-31." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Review of deep convolution neural network in image classification,&amp;quot;">
                                        <b>[16]</b>
                                        AL-SAFFAR A A M, TAO H, TALAB M A.Review of deep convolution neural network in image classification[C]//Proceedings of the 2017 International Conference on Radar, Antenna, Microwave, Electronics, and Telecommunications.Piscataway, NJ:IEEE, 2017:26-31.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_17" title="XIE L, WANG J, LIN W, et al.Towards reversal-invariant image representation[J].International Journal of Computer Vision, 2017, 123 (2) :226-250." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD133CEB751E7FAE945E2F067BF5D3440E&amp;v=MTgxNTBJV256MExTSG5sM21Rd0RiR1FRYnJxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzd5NXhhND1OajdCYXJLN0hhSzUzWWhBWlo0SWVnMU14aA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        XIE L, WANG J, LIN W, et al.Towards reversal-invariant image representation[J].International Journal of Computer Vision, 2017, 123 (2) :226-250.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_18" title="GOODFELLOW I, BENGIO Y, COURVILLE A, et al.Deep Learning[M].Cambridge, MA:MIT Press, 2016:11-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning">
                                        <b>[18]</b>
                                        GOODFELLOW I, BENGIO Y, COURVILLE A, et al.Deep Learning[M].Cambridge, MA:MIT Press, 2016:11-12.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_19" title="FENG Y, ZENG S, YANG Y, et al.Study on the optimization of CNN based on image identification[C]//Proceedings of the 17th International Symposium on Distributed Computing and Applications for Business Engineering and Science.Piscataway, NJ:IEEE, 2018:123-126." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Study on the optimization of CNN based on image identification">
                                        <b>[19]</b>
                                        FENG Y, ZENG S, YANG Y, et al.Study on the optimization of CNN based on image identification[C]//Proceedings of the 17th International Symposium on Distributed Computing and Applications for Business Engineering and Science.Piscataway, NJ:IEEE, 2018:123-126.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-04-10 07:00</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(06),1601-1606 DOI:10.11772/j.issn.1001-9081.2018122501            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于Dropout的改进卷积神经网络模型平均方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E4%BF%8A%E5%8D%8E&amp;code=39181105&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程俊华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%BE%E5%9B%BD%E8%BE%89&amp;code=25335425&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曾国辉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%B2%81%E6%95%A6%E7%A7%91&amp;code=36357418&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鲁敦科</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E5%8B%83&amp;code=36577169&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄勃</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0202052&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海工程技术大学电子电气工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对深度卷积神经网络 (CNN) 中的过拟合问题, 提出一种基于Dropout改进CNN的模型预测平均方法。首先, 训练阶段在池化层引入Dropout, 使得池化层单元值具有稀疏性;然后, 在测试阶段将训练时池化层Dropout选择单元值的概率与池化区域各单元值所占概率相乘作为双重概率;最后, 将提出的双重概率加权的模型平均方法应用于测试阶段, 使得训练阶段池化层Dropout的稀疏效果能够更好地反映到测试阶段池化层上, 从而使测试错误率达到与训练的较低错误率相近的结果。在给定大小的网络中所提方法在MNIST和CIFAR-10数据集上的测试错误率分别为0.31%和11.23%。实验结果表明:仅考虑池化层对结果的影响, 所提方法与Prob.weighted pooling和Stochastic Pooling方法相比具有更低的错误率, 表明池化层Dropout使得模型更具泛化性, 并且池化单元值对于模型泛化具有一定帮助, 能够更有效避免过拟合。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dropout%E6%AD%A3%E5%88%99%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dropout正则化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%87%E6%8B%9F%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">过拟合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E5%B9%B3%E5%9D%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型平均;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *程俊华 (1994—) , 男, 河南洛阳人, 硕士研究生, 主要研究方向:模式识别、计算机视觉;cheng_jun_hua@ 163. com;
                                </span>
                                <span>
                                    曾国辉 (1975—) , 男, 江西乐安人, 副教授, 博士, 主要研究方向:智能控制、电力电子系统及其控制;;
                                </span>
                                <span>
                                    鲁敦科 (1983—) , 男, 湖北咸宁人, 讲师, 博士, 主要研究方向:光纤传感、特种光纤设计;;
                                </span>
                                <span>
                                    黄勃 (1985—) , 男, 湖北武汉人, 讲师, 博士, 主要研究方向:人工智能、大数据。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目 (61603242);</span>
                                <span>江西省经济犯罪侦查与防控技术协同创新中心开放课题 (JXJZXTCX-030);</span>
                    </p>
            </div>
                    <h1><b>Improved convolution neural network model averaging method based on Dropout</b></h1>
                    <h2>
                    <span>CHENG Junhua</span>
                    <span>ZENG Guohui</span>
                    <span>LU Dunke</span>
                    <span>HUANG Bo</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Electrical Engineering, Shanghai University of Engineering Science</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to effectively solve the overfitting problem in deep Convolutional Neural Network (CNN) , a model prediction averaging method based on Dropout improved CNN was proposed. Firstly, Dropout was employed in the pooling layers to sparse the unit values of pooling layers in the training phase. Then, in the testing phase, the probability of selecting unit value according to pooling layer Dropout was multiplied by the probability of each unit value in the pooling area as a double probability. Finally, the proposed double-probability weighted model averaging method was applied to the testing phase, so that the sparse effect of the pooling layer Dropout in the training phase was able to be better reflected on the pooling layer in the testing phase, thus achieving the low testing error as training result. The testing error rates of the proposed method in the given size network on MNIST and CIFAR-10 data sets were 0.31% and 11.23% respectively. The experimental results show that the improved method has lower error rate than Prob. weighted pooling and Stochastic Pooling method with only the impact of pooling layer on the results considered. It can be seen that the pooling layer Dropout makes the model more generalized and the pooling unit value is helpful for model generalization and can effectively avoid overfitting.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolution%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolution Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dropout%20regularization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dropout regularization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=overfitting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">overfitting;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20averaging&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model averaging;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHENG Junhua, born in 1994, M. S. candidate. His research interests include pattern recognition, computer vision. ;
                                </span>
                                <span>
                                    ZENG Guohui, born in 1975, Ph. D. , associate professor. His research interests include intelligent control, power electronic system and its control. ;
                                </span>
                                <span>
                                    LU Dunke, born in 1983, Ph. D. , lecturer. His research interests include optical fiber sensing, special optical fiber design. ;
                                </span>
                                <span>
                                    HUANG Bo, born in 1985, Ph. D. , lecturer. His research interests include artificial intelligence, big data.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-19</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China (61603242);</span>
                                <span>the Project of Jiangxi Provincial Economic Crime Collaborative Innovation Center of Prevention and Control Technology (JXJZXTCX-030);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">近年来, 随着人工智能的快速发展, 深度学习在各领域的应用越来越广泛<citation id="226" type="reference"><link href="188" rel="bibliography" /><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><link href="194" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。其中, 卷积神经网络 (Convolutional Neural Network, CNN) 在机器学习和计算机视觉等领域拥有的出色表现, 使得它被应用到更多问题的解决方案中。卷积神经网络的优势在于其能够自动提取特征, 从而能够避免人工提取特征的困难和麻烦, 但不可避免地会出现由于数据集较小或者训练次数过多等原因出现的过度学习现象, 又称“过拟合”。对于分类任务, 过拟合问题体现在在训练集上错误率较低, 而在测试集上错误率却偏高。过度学习将训练集某类数据的特殊特征当作该类数据的一般特征进行学习, 使得学习的模型在测试集数据上进行测试时, 将该类数据的特殊特征当作一般特征进行判断, 不能够对数据进行准确分类, 错误率较高。这是监督学习中一个普遍存在且相当棘手的问题, 本文致力于对卷积神经网络的过拟合问题提供解决方法。</p>
                </div>
                <div class="p1">
                    <p id="43">目前, 很多科研机构和高校对于有效解决过拟合问题均进行了深入研究。Dropout方法最早由Hinton等<citation id="227" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出用于解决过拟合问题。在训练时加入Dropout, 一部分激活单元随机受到抑制, 每一次训练的网络结构均不同, 训练结果可看作是不同神经元子集的模型的组合, 而在测试时保留所有神经元, 采用模型平均的方法来对训练过的所有可能模型求取平均近似值。Dropout刚开始被用于卷积神经网络的全连接层, 全连接层占据卷积神经网络的大部分参数, 对于小型网络表现较好, 但在稍复杂的中大型网络上, 其他层的参数增多, 表现不明显。之后, Srivastava等<citation id="228" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>又尝试在卷积层使用Dropout, 但卷积层本身具有的参数共享和稀疏连接特性减少了参数的数量, 从而降低了卷积层过拟合的可能性, 因此Dropout在卷积层的优势并不明显。随后, Goodfellow等<citation id="229" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>采用一组线性函数中取最大值作为神经单元激活值的方法, 该方法基于Dropout建立了Maxout network。然而文献<citation id="230" type="reference">[<a class="sup">7</a>]</citation>并没有通过Dropout对修正对应模型进行训练, 而是直接将其与Maxout network进行比较, 因此, 对于是什么因素对取得的显著成果产生的影响并不清楚。Wan等<citation id="231" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation><sup></sup>将Dropout用于抑制平坦层神经元之间连接权重, 提出了DropConnect方法, 提升了特征学习的细腻度, 从而更有效避免过拟合;但该方法只适用于全连接层, 对于其他层具有局限性。Lambert等<citation id="232" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了异方差Dropout, 使Dropout方差成为特权信息函数, 显著提高了学习过程的样本效率, 但对于过拟合并没有显著影响。Zeiler 等<citation id="233" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation> 提出了随机池化, 从池化方法入手来进一步解决过拟合问题。与最大值池化选取池化区域最具代表性特征作为池化激活值不同, 随机池化在池化区域内随机选取池化激活值, 在测试时以池化区域内各单元值概率加权到激活值上进行模型平均, 该方法类似于最大值池化Dropout方法, 但最大值池化Dropout方法可能训练的模型数量更多, 抑制过拟合效果更好。Wu等<citation id="234" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>在最大值池化中引入Dropout, 随机抑制池化值再利用最大值池化选取池化激活值, 在测试阶段采用Prob.weighted pooling模型预测平均方法作用于池化层得到模型平均, 但Prob.weighted pooling方法仅考虑了池化激活值选择概率。</p>
                </div>
                <div class="p1">
                    <p id="44">受文献<citation id="235" type="reference">[<a class="sup">10</a>,<a class="sup">11</a>]</citation>研究启发, 本文针对最大值池化Dropout解决过拟合问题给出了一种更优的模型平均方法, 训练时在池化层采用最大值池化Dropout, 在测试阶段模型平均时, 与Prob.weighted pooling方法相比将单元值在池化区域中的概率考虑在内, 结合池化激活值选择概率共同加权到被选择单元值的激活值上, 得到遍历所有可能模型近似平均的池化结果。相比于随机池化, 训练模型更具泛化性。实验结果表明, 在同样网络结构的卷积神经网络下, 本文提出的方法具有更好的抑制过拟合效果。</p>
                </div>
                <h3 id="45" name="45" class="anchor-tag">1 传统卷积神经网络结构</h3>
                <div class="p1">
                    <p id="46">一般的CNN由卷积层、池化层和全连接层组成。卷积层和池化层交替连接用于特征提取, 全连接层最后用于对提取特征进行分类。CNN的局部连接和权值共享等特点, 使其比一般神经网络具有更少的参数, 具有平移不变性、特征降维以及强大的表征能力等优点, 并且由于神经元之间的稀疏连接, 使得其具有良好的泛化性。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47">1.1 <b>卷积层</b></h4>
                <div class="p1">
                    <p id="48">在卷积层中输入图像与相应的卷积核进行卷积操作, 用于对图像进行特征提取, 一个二维步长为1、无填充卷积操作过程可如图1所示。</p>
                </div>
                <div class="p1">
                    <p id="49">假设<b><i>y</i></b><mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>为输入数据在第<i>l</i>层中的第<i>i</i> (<i>i</i>=1, 2, …, <i>n</i><sup><i>l</i></sup>) 个feature map, <b><i>y</i></b><mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>与相应的卷积核<b><i>W</i></b><sup><i>l</i>+1, <i>k</i></sup>卷积得到的卷积矩阵求和加上相对应的偏置<b><i>b</i></b><sup><i>l</i>+1, <i>k</i></sup>, 即可得到第<i>l</i>+1层的输入<b><i>z</i></b><sup><i>l</i>+1, <i>k</i></sup>, 经过非线性激活函数处理即可得到数据在第<i>l</i>+1层的第<i>k</i>个输出矩阵, 即<b><i>y</i></b><sup><i>l</i>+1, <i>k</i></sup>。其过程可表述为:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>k</mi></mrow></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mi>c</mi></mstyle><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>k</mi></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">y</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>k</mi></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>k</mi></mrow></msup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>k</mi></mrow></msup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906009_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 二维卷积操作示意图" src="Detail/GetImg?filename=images/JSJY201906009_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 二维卷积操作示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906009_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Schematic diagram of two-dimensional convolution operation</p>

                </div>
                <h4 class="anchor-tag" id="54" name="54">1.2 <b>池化层</b></h4>
                <div class="p1">
                    <p id="55">在卷积神经网络中, 卷积层之后一般为池化层, 池化层对卷积层得到的特征进行下采样, 从而减小计算量, 降低网络复杂度, 提高计算效率。通常的池化操作可表示为:</p>
                </div>
                <div class="p1">
                    <p id="56"><i>a</i><mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>=<i>pool</i> (<i>a</i><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>1</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml>, <i>a</i><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>2</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml>, …, <i>a</i><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>, …, <i>a</i><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="62">其中:<i>a</i><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>表示<b><i>R</i></b><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>区域内第<i>j</i> (<i>j</i>=1, 2, …, <i>n</i>) 个池化单元值, <i>l</i>为第<i>l</i>层;<i>n</i>表示<b><i>R</i></b><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>区域池化单元的数目;<i>pool</i> (·) 表示池化操作。通常采用的池化方法为平均池化和最大值池化, 平均池化方法在卷积神经网络刚提出时应用较多;最大值池化因其能够将最具代表性的特征保留的特点, 受到广泛应用, 以2×2池化大小为例, 最大值池化过程如图2所示。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906009_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 最大值池化操作图解" src="Detail/GetImg?filename=images/JSJY201906009_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 最大值池化操作图解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906009_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Operation diagram of max pooling</p>

                </div>
                <div class="p1">
                    <p id="67">最大值池化Dropout既可以改善平均池化减弱高激活值神经元的弊端, 又能避免最大值池化无视其他池化单元值的缺点, 能够有效地表达池化层的特征。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">2 改进CNN结构</h3>
                <div class="p1">
                    <p id="69">本文在<i>CNN</i>池化层引入<i>Dropout</i>, 在训练阶段池化层引入<i>Dropout</i>, 在测试时使用双重概率加权模型平均来代替采样, 将训练时池化层<i>Dropout</i>的可能训练的所有模型进行有效的平均, 实现模型预测。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">2.1 <b>训练阶段最大值池化</b><i>Dropout</i></h4>
                <div class="p1">
                    <p id="71">在池化层引入<i>Dropout</i>后, 在进行池化操作之前, 首先进行掩模处理, 使得池化区域中的部分神经元激活值以一定概率被置为0, 在保留下来的神经元中再进行最大值池化操作, 其过程可表示为:</p>
                </div>
                <div class="p1">
                    <p id="72"><b><i>r</i></b><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>～Bernoulli (<i>p</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">a</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup><mo>=</mo><mi mathvariant="bold-italic">r</mi><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup><mo>*</mo><mi mathvariant="bold-italic">a</mi><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>a</mi><msubsup><mrow></mrow><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mi>Μ</mi><mi>a</mi><mi>x</mi><mi>p</mi><mi>o</mi><mi>o</mi><mi>l</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>1</mn></mrow><mi>l</mi></msubsup><mo>, </mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>2</mn></mrow><mi>l</mi></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>l</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中:<b><i>r</i></b><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>表示一个二进制掩模, 服从Bernoulli分布;<mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">a</mi><mo>^</mo></mover></math></mathml><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>表示对输入<b><i>a</i></b><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>掩模处理后的feature map, <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">a</mi><mo>^</mo></mover></math></mathml><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>输入到最大值池化中得到池化结果, 即为最大值池化Dropout方法。该池化方法具有随机性, 以2×2池化为例, 如图3所示。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906009_082.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 最大值池化Dropout操作图解" src="Detail/GetImg?filename=images/JSJY201906009_082.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 最大值池化Dropout操作图解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906009_082.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Operation diagram of max pooling Dropout</p>

                </div>
                <div class="p1">
                    <p id="83">未引入Dropout时, 将特征图中每个2×2池化区域的最大值输出作为该区域的池化激活值, 如最后一个池化区域输出最大值为7, 引入Dropout后, 区域中每个神经元均可能被抑制, 如最后一个池化区域7被抑制, 1、4、5被保留, 则最大值池化的结果为5。</p>
                </div>
                <div class="p1">
                    <p id="84">训练时使用最大值池化Dropout, 池化区域中的每个单元均有概率<i>q</i> (<i>q</i>=1-<i>p</i>, 表示被抑制概率, <i>p</i>为保留概率) 的可能被置为0。假设在每一个池化区域<b><i>R</i></b><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>中的激活值 (<i>a</i><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>1</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml>, <i>a</i><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>2</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml>, …, <i>a</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>) 被重新排列为一个非减序列0≤<i>a</i>′<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>1</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml>≤<i>a</i>′<sup><i>l</i></sup><sub><i>m</i>, 2</sub>≤…≤<i>a</i>′<mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>, 采用线性修正单元Relu作为激活函数使得激活值非负, 那么<i>a</i>′<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>被选择为整个池化区域池化输出值的情况为:</p>
                </div>
                <div class="p1">
                    <p id="92">1) 所有大于<i>a</i>′<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>的单元值 (<i>a</i>′<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>+</mo><mn>1</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml>, <i>a</i>′<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>+</mo><mn>2</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml>, …, <i>a</i>′<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>) 被抑制;</p>
                </div>
                <div class="p1">
                    <p id="97">2) <i>a</i>′<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>被保留下来。</p>
                </div>
                <div class="p1">
                    <p id="99">则上述两种情况同时发生的概率<i>p</i><sub><i>j</i></sub>, 即为<i>a</i>′<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>被选为池化输出值的概率:</p>
                </div>
                <div class="p1">
                    <p id="101">Pr (<i>a</i><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>=<i>a</i>′<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>) =<i>p</i><sub><i>j</i></sub>=<i>pq</i><sup><i>n</i>-<i>j</i></sup>; <i>j</i>=1, 2, …, <i>n</i>      (7) </p>
                </div>
                <div class="p1">
                    <p id="104">一种特殊情况为池化区域<b><i>R</i></b><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>中的所有单元值全部被抑制而置为0, 这种情况发生的概率记为<i>p</i><sub>0</sub> (<i>p</i><sub>0</sub>=<i>q</i><sup><i>n</i></sup>) , 这种情况下池化输出激活值为0, 记为<i>a</i>′<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>0</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml> (<i>a</i>′<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mn>0</mn></mrow><mi>l</mi></msubsup></mrow></math></mathml>=0) , 结合所有情况, 在池化区域执行最大值池化Dropout时, 通过多项式分布来选择池化区域<b><i>R</i></b><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>l</mi></msubsup></mrow></math></mathml>中经过非减排列的第<i>j</i>个激活值<i>a</i>′<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>作为池化输出, 即下标索引值<i>j</i>服从多项式分布, 即:</p>
                </div>
                <div class="p1">
                    <p id="110"><i>a</i><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>=<i>a</i>′<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>; <i>j</i>～Multinomial (<i>p</i><sub>0</sub>, <i>p</i><sub>1</sub>, <i>p</i><sub>2</sub>, …, <i>p</i><sub><i>n</i></sub>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="113">假设第<i>l</i>层有<i>r</i>个feature map, 每个feature map的尺寸为<i>s</i>, 池化区域大小为<i>t</i>, 不考虑重叠池化, 则有<i>rs</i>/<i>t</i>个池化区域, 则<i>l</i>层可能训练的模型数量为 (<i>t</i>+1) <sup><i>rs</i>/<i>t</i></sup>。因此最大值池化Dropout可能训练的模型数量与输入到池化层的单元数量呈指数型关系。在池化层引入Dropout, 池化层单元值被随机抑制, 使得网络在池化层上也能够生成更多子模型, 使得可训练的特征增多, 进而能够更有效地抑制过拟合。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">2.2 <b>测试阶段双重概率加权模型平均</b></h4>
                <div class="p1">
                    <p id="115">训练时在池化层引入<i>Dropout</i>, 则池化单元值被随机抑制, 但在测试时, 整个网络包括所有池化层的单元值都应该参与网络运作, 因此需要通过模型平均的方式将训练时池化层引入<i>Dropout</i>的效果体现到测试阶段。文献<citation id="236" type="reference">[<a class="sup">11</a>]</citation>在测试阶段采用的模型平均方法利用式 (7) 得到池化区域输出结果:</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><msubsup><mrow></mrow><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><msup><mi>a</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><msup><mi>a</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117"><i>j</i>=0为池化单元值全部抑制的情况, 该方式称为Prob.weighted pooling。但Prob.weighted pooling方法仅考虑了池化输出值的选择概率作为加权概率。本文同时顾及到<i>p</i>与<i>q</i>共同作用选择池化输出值的概率和选中池化激活值对池化区域的影响, 提出了一种新的模型平均方法, 能够更有效地得到一个准确的平均池化区域遍历经过训练的所有可能模型的近似值。</p>
                </div>
                <div class="p1">
                    <p id="118">随机池化 (Stochastic Pooling) <citation id="237" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>方法训练时在池化层随机选择池化激活值, 然后在测试时应用池化区域内各单元的概率作为加权概率的模型平均。本文受随机池化方法启发, 考虑到测试阶段被选中池化激活值对所在池化区域的影响, 得到池化区域内每个单元值的比重<i>p</i><sub><i>m</i>, <i>j</i></sub>, 即:</p>
                </div>
                <div class="p1">
                    <p id="119"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><mo>=</mo><mi>a</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup><mo>/</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>a</mi></mstyle><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>k</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>; <i>j</i>=1, 2, …, <i>n</i>      (10) </p>
                </div>
                <div class="p1">
                    <p id="121">本文采用双重概率加权的池化方法, 能够更有效地将训练阶段池化层Dropout的效果反映到测试阶段池化层上, 该方法同时考虑概率<i>p</i><sub><i>m</i>, <i>j</i></sub>, 以及保留概率<i>p</i>和抑制概率<i>q</i>=1-<i>p</i>共同作用选择池化输出值概率<i>p</i><sub><i>j</i></sub>的影响, 即将训练阶段使用最大池化Dropout选择池化输出值的概率<i>p</i><sub><i>j</i></sub>与测试阶段池化区域各单元值所占概率<i>p</i><sub><i>m</i>, <i>j</i></sub>相乘后的概率值, 线性加权到测试阶段池化区域单元值上, 再对池化区域所有概率加权的单元值求和, 得到一个准确的池化区域遍历经过训练的所有可能模型的平均近似值, 作为测试阶段池化方法, 即:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><msubsup><mrow></mrow><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>p</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><msup><mi>a</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>p</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow></msub><msup><mi>a</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">式中:<i>p</i><sub><i>j</i></sub>、<i>p</i><sub><i>m</i>, <i>j</i></sub>分别为式 (7) 、式 (10) 计算出来的概率。假如在训练时以概率<i>p</i><sub><i>j</i></sub>将一个池化区域中某单元值<i>a</i>′<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mspace width="0.25em" /><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>选择为该池化区域的输出值, 则在测试时该单元值被选中的概率<i>p</i><sub><i>j</i></sub>和该单元值在池化区域中所占的概率<i>p</i><sub><i>m</i>, <i>j</i></sub>应当加权到该单元值的激活值上。这样能够确保在测试时的池化输出值与训练时池化层最大值池化Dropout选择单元值的期望值保持一致。</p>
                </div>
                <div class="p1">
                    <p id="125">本文方法与Prob.weighted pooling相比, 除了考虑池化值选择概率外, 还将池化区域单元值概率考虑在内, 进一步保证了测试阶段池化输出值与训练阶段池化单元值的期望值的一致性。如2.1节所述, 本文方法在<i>l</i>层可能训练的模型数量为 (<i>t</i>+1) <sup><i>rs</i>/<i>t</i></sup>。就随机池化而言, 由于其未使用Dropout, 每个池化区域只有<i>t</i>种选择, 即其在<i>l</i>层可能训练的模型数量为<i>t</i><sup><i>rs</i>/<i>t</i></sup>。例如, 在第<i>l</i>层, <i>s</i>=32×32, <i>r</i>=96, <i>t</i>=2×2, 则随机池化方法可能训练的模型数量为4<sup> (1/4) ×32×32×96</sup>, 而本文方法最大值池化Dropout可能训练的模型数量为5<sup> (1/4) ×32×32×96</sup>。虽然基数相差较小, 但是本文方法可能训练的模型数量是随机池化方法的1.25<sup>24 576</sup>倍, 因此本文最大值池化Dropout方法能够比随机池化方法提供更多可能训练的模型, 从而能够学习更广泛的特征, 使模型更具泛化性, 避免过拟合。</p>
                </div>
                <div class="p1">
                    <p id="126">这种双重概率加权池化可以理解为模型平均的一种形式, 每一个选择的索引<i>j</i>对应一个不同的模型。在训练阶段, 服从多项式分布进行采样以选择一个新的索引, 为每一个训练样本的每个表示产生一个新的模型, 可能的模型数量随传入到最大值池化层的单元值数量呈指数型增长。在测试阶段, 使用双重概率加权池化来代替采样, 可以有效地得到所有这些可能模型的平均值的近似值, 而无需实例化它们。</p>
                </div>
                <h3 id="127" name="127" class="anchor-tag">3 实验参数与数据集</h3>
                <div class="p1">
                    <p id="128">在经过卷积层和全连接层的输出后需要输入到激活函数中进行非线性处理使得神经元更具表达性, 常用的非线性激活函数有<i>Tanh</i>函数、<i>Sigmoid</i>函数以及<i>Relu</i>函数<citation id="238" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 前两者为饱和非线性函数, 而<i>Relu</i>函数为非饱和非线性函数, 在达到相同训练误差的情况下, 非饱和激活函数要比饱和激活函数快许多倍, 同时由于<i>Relu</i>函数的单侧抑制作用, 使得网络具有稀疏激活性, 减少网络参数的相互协作, 能够更好地学习数据特征, 且能够避免梯度消失问题, 使模型持续收敛, 而不会像饱和激活函数那样导致收敛停滞。本文在卷积层和全连接层均采用<i>Relu</i>函数作为激活函数, 网络最后一个全连接层输出连接<i>Softmax</i>层, 使得网络输出分类结果, 根据<i>Relu</i>函数的表达式, 卷积层和全连接层的激活值可表达为:</p>
                </div>
                <div class="p1">
                    <p id="129"><b><i>a</i></b><sup><i>l</i>+1</sup>=max (0, <b><i>W</i></b><sup><i>l</i>+1</sup><b><i>a</i></b><sup><i>l</i></sup>+<b><i>b</i></b><sup><i>l</i>+1</sup>)      (12) </p>
                </div>
                <div class="p1">
                    <p id="130">其中:<b><i>a</i></b><sup><i>l</i></sup>为第<i>l</i>+1层的输入;<b><i>W</i></b><sup><i>l</i>+1</sup>为第<i>l</i>+1层的权重参数;<b><i>b</i></b><sup><i>l</i>+1</sup>为第<i>l</i>+1层的与输入相对应的偏置;<b><i>a</i></b><sup><i>l</i>+1</sup>为经过激活函数处理后的输出特征图。</p>
                </div>
                <div class="p1">
                    <p id="131">Softmax函数作为常用的分类器, 其依据结果的概率大小来输出概率最大的类别来进行分类, 其表达式为:</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>o</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">a</mi><msubsup><mrow></mrow><mi>i</mi><mi>L</mi></msubsup><mo stretchy="false">) </mo><mo>/</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mrow><mi>exp</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">a</mi><msubsup><mrow></mrow><mi>k</mi><mi>L</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">其中:<b><i>a</i></b><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>L</mi></msubsup></mrow></math></mathml>表示最后一个全连接层的输出, 通常为一个特征向量; <i>i</i>, <i>k</i>表示特征向量中的索引。</p>
                </div>
                <div class="p1">
                    <p id="135">网络训练使用小批量梯度下降 (mini-batch Stochastic Gradient Descent, mini-batch SGD) 法, 批次大小为100, 动量为0.9, 学习率设置为0.1。使用均值为0、方差为0.01的正态分布来随机初始化权重, 偏置初始化为0, 每当误差趋于平缓时就将学习率减小, 默认保留概率<i>p</i>为0.5。实验环境为:操作系统Win 10, 处理器Intel core i7 CPU @ 3.00 GHz, 6个内核12线程, RAM 16 GB, GPU NVIDIA GTX 1080ti。实验采用MNIST、CIFAR- 10数据集进行验证。MNIST、CIFAR- 10数据集的部分样本分别如图4 (a) 、 (b) 所示。</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906009_136.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 MNIST、CIFAR- 10数据集的部分示例图" src="Detail/GetImg?filename=images/JSJY201906009_136.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 MNIST、CIFAR- 10数据集的部分示例图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906009_136.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Partial example images of MNIST and CIFAR- 10 datasets</p>

                </div>
                <div class="p1">
                    <p id="137">引入目标函数, 以使得最小化目标函数来得到最优解, 综合考量目前深度学习常用的几种代价函数, 选择交叉熵代价函数<citation id="239" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>作为目标函数, 其表达式为:</p>
                </div>
                <div class="p1">
                    <p id="138" class="code-formula">
                        <mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mi>o</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>o</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="139">其中:<i>N</i>为每个mini-batch的训练样本数;<b><i>y</i></b>为真实标签值;<b><i>o</i></b>为网络的实际输出预测值。</p>
                </div>
                <div class="p1">
                    <p id="140">要使目标函数最优化, 需要用到优化方法, 实验采用随机梯度下降 (Stochastic Gradient Descent, SGD) 算法来优化目标函数, 根据梯度下降法的运算规则, 需要求权重对于代价函数的梯度来进行权重的更新, 第<i>l</i>层权重的更新规则可表示为:</p>
                </div>
                <div class="p1">
                    <p id="141">Δ<b><i>W</i></b><sup><i>l</i></sup>=<i>δ</i><sup><i>l</i></sup>·<b><i>a</i></b><sup><i>l</i>-1</sup>      (15) </p>
                </div>
                <div class="p1">
                    <p id="142">其中:<i>δ</i><sup><i>l</i></sup>为第<i>l</i>层的误差系数, 其表示该层误差对总的误差的负责程度;<b><i>a</i></b><sup><i>l</i>-1</sup>为第<i>l</i>-1层的输出, 即第<i>l</i>层的输入。<i>δ</i><sup><i>l</i></sup>可用下面计算式求得:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msup><mrow></mrow><mi>l</mi></msup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">y</mi><mo>-</mo><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mi>l</mi></msup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>l</mi><mo>=</mo><mi>L</mi></mtd></mtr><mtr><mtd><mi>δ</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><msup><mo stretchy="false">) </mo><mo>′</mo></msup><mo>⋅</mo><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mi>l</mi></msup><mo>⋅</mo><mo stretchy="false"> (</mo><mi>l</mi><mo>-</mo><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>l</mi><mo>&lt;</mo><mi>L</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">其中“·”表示对应元素相乘。</p>
                </div>
                <div class="p1">
                    <p id="145">为方便直观, 采用以下形式表示CNN网络结构:3×32×32- 32C5- 3P2- 64C5- 3P2- 1 000N- 10N表示输入为3个尺寸为32×32的图像。C表示卷积, 32C5表示包含32个尺寸为5的卷积核的卷积层;P表示池化, 3P2表示池化层池化尺寸为3、步长为2;N表示全连接层, 1 000N表示包含1 000神经元的全连接层, 以此类推。</p>
                </div>
                <h3 id="146" name="146" class="anchor-tag">4 实验结果与分析</h3>
                <h4 class="anchor-tag" id="147" name="147">4.1 <i>MNIST</i><b>数据集</b></h4>
                <div class="p1">
                    <p id="148"><i>MNIST</i>数据集是计算机视觉和机器学习领域广泛使用的基准数据集, 可作为方法是否有效的一个衡量标准, 其包含60 000张训练和10 000张测试样本, 每个样本为28×28的灰度图像, 包含0～9中的一个数字, 在输入<i>CNN</i>前将样本归一化到[0, 1]。</p>
                </div>
                <div class="p1">
                    <p id="149">在训练阶段池化层引入<i>Dropout</i>, 测试阶段有与之对应的模型平均方法, 本文对文献<citation id="240" type="reference">[<a class="sup">11</a>]</citation>中测试阶段模型预测平均方法 (<i>Prob</i>.<i>weighted pooling</i>) 作出改进。将改进后的方法与原方法应用于同一结构<i>CNN</i>模型1×28×28- 12<i>C</i>5- 2<i>P</i>2- 24<i>C</i>5- 2<i>P</i>2- 1 000<i>N</i>- 10<i>N</i>, 在两种方法中均使用全连接层<i>Dropout</i>, 可消除全连接层对于结果的影响, 从而关注方法在池化层的效果。对比改进前后, 在测试阶段池化层上能够反映训练阶段池化层<i>Dropout</i>效果的优劣性。图5所示为使用三种方法在同一结构下训练迭代300次所得模型的测试错误率变化曲线。</p>
                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906009_150.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 MNIST相同结构下不同方法测试错误率" src="Detail/GetImg?filename=images/JSJY201906009_150.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>MNIST</i>相同结构下不同方法测试错误率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906009_150.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Testing error rates of different methods under</i><i>same structure on MNIST</i></p>

                </div>
                <div class="p1">
                    <p id="151">由图5可知, 错误率越低, 越能在测试时体现出训练阶段池化层<i>Dropout</i>的效果, 也就能越好地解决过拟合问题。</p>
                </div>
                <div class="p1">
                    <p id="152">在图5中, 使用全连接层<i>Dropout</i>错误率曲线作为参考, 可以看出:随着迭代的不断进行, 全连接层<i>Dropout</i>方法在收敛到一定程度时, 错误率不再下降;本文方法与改进前<i>Prob</i>.<i>weighted pooling</i>曲线能够收敛并不断下降, 且本文方法的错误率基本低于改进前方法的错误率, 所以本文方法在测试阶段池化层上能够更好地反映出训练阶段池化层引入<i>Dropout</i>后的效果, 能更有效避免过拟合问题。</p>
                </div>
                <div class="p1">
                    <p id="153">为了比较本文方法与随机池化方法在抑制过拟合性能上的表现, 在<i>CNN</i>模型1×28×28- 12<i>C</i>5- 2<i>P</i>2- 24<i>C</i>5- 2<i>P</i>2- 1 000<i>N</i>- 10<i>N</i>上对不同保留概率情况下本文方法与随机池化方法进行实验, 对于最大值池化<i>Dropout</i>训练的模型, 在测试时仅使用提出的双重概率加权的模型平均池化方法。图6所示为使用随机池化和不同保留概率下本文方法迭代300次的错误率变化曲线。</p>
                </div>
                <div class="area_img" id="154">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906009_154.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 MNIST不同保留概率p下测试错误率" src="Detail/GetImg?filename=images/JSJY201906009_154.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 <i>MNIST</i>不同保留概率<i>p</i>下测试错误率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906009_154.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Testing error rate with different retention probability <i>p</i> on MNIST</p>

                </div>
                <div class="p1">
                    <p id="155">从图6可以看出, 随机池化的测试错误率介于本文方法在<i>p</i>=0.7和<i>p</i>=0.9两者之间, 与本文方法 (<i>p</i>=0.5) 相差较多, <i>p</i>较大时 (<i>p</i>=0.9) , 本文方法的抑制过拟合性能不如随机池化, 表明保留概率<i>p</i>与本文方法的性能有较大关系。这是因为加权概率受保留概率<i>p</i>的影响, 在<i>p</i>较大时相当于大多数神经元都得到保留, Dropout稀疏化效果不明显, 因此不能很好抑制过拟合, 错误率较高。虽然随机池化没有超参数且不受保留概率<i>p</i>的影响, 但具有典型值<i>p</i> (<i>p</i>=0.5) 的本文方法在抑制过拟合问题上很大程度优于随机池化。</p>
                </div>
                <div class="p1">
                    <p id="156">训练一个不同结构模型采用不同的方法和不同保留概率<i>p</i>, 只在池化层使用Dropout, 以比较不同的<i>p</i>值对不同方法的影响, 实验在1×28×28- 20C5- 2P2- 40C5- 2P2- 1 000N- 10N的网络上迭代1 000次, 图7给出了测试阶段不同池化方法的错误率与保留概率<i>p</i>之间的关系。由图7可以看出, 不同的<i>p</i>值下, 本文方法表现较好, 且在<i>p</i>=0.5时的结果最优;Max pooling方法在较小的<i>p</i>值下, 错误率比较高, 表现较差;Prob.weighted pooling方法和本文方法, 在不同<i>p</i>值下的趋势呈U型, 且本文方法错误率低于Prob.weighted pooling方法, 在<i>p</i>值较大或较小时, Prob.weighted pooling方法和本文方法错误率均高于随机池化 (Stochastic Pooling) , 与图6内容相对应, 再次验证了图6所得结论的正确性。随着<i>p</i>值的增加, 不同方法之间的性能差异越来越小, 特殊的情况当<i>p</i>=1时, 差异将缩减为0。</p>
                </div>
                <div class="p1">
                    <p id="157">同样在1×28×28- 20C5- 2P2- 40C5- 2P2- 1 000N- 10N结构上, 利用MNIST数据集对使用全连接层Dropout的本文方法与其他方法的错误率进行记录比较。在不使用Dropout时的错误率为0.81%, 错误率最高, 因为受过拟合的影响, 在全连接层使用Dropout后的错误率降为0.56%;文献<citation id="241" type="reference">[<a class="sup">7</a>]</citation> 的Maxout+Dropout方法错误率约为0.45%, 文献<citation id="242" type="reference">[<a class="sup">10</a>]</citation>的 Stochastic Pooling方法错误率又降到0.47%, 文献<citation id="243" type="reference">[<a class="sup">11</a>]</citation>结合全连接层Dropout的最大值池化Dropout (Prob.weighted pooling) 方法错误率降为0.39%;同样结合全连接层Dropout, 而本文的方法错误率约为0.31%, 相比于其他方法在过拟合问题上表现要好, 表明了本文方法具有更好的抑制作用。</p>
                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906009_158.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 MNIST错误率与保留概率p的关系" src="Detail/GetImg?filename=images/JSJY201906009_158.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 MNIST错误率与保留概率<i>p</i>的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906009_158.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Relationship between error rate and retention probability <i>p</i> on MNIST</p>

                </div>
                <h4 class="anchor-tag" id="159" name="159">4.2  CIFAR- 10<b>数据集</b></h4>
                <div class="p1">
                    <p id="160"><i>CIFAR</i>- 10数据集<citation id="244" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>是一个包含10类共60 000张自然图像的数据集, 其中, 训练集包含50 000个样本, 测试集包含10 000个样本, 每个样本为一个32×32大小的<i>RGB</i>图像。对数据集中的样本像素值归一化在[0, 1], 并减去每个图像在数据集上计算的每个通道的平均值作为预处理。</p>
                </div>
                <div class="p1">
                    <p id="161">相对于<i>MNIST</i>数据集, <i>CIFAR</i>-10数据集在每个类别中的差异更大, 因此使用更深更宽的网络结构:3×32×32- 96<i>C</i>5- 3<i>P</i>2- 128<i>C</i>3- 3<i>P</i>2- 256<i>C</i>3- 3<i>P</i>2- 2 000<i>N</i>- 2 000<i>N</i>- 10<i>N</i>, 来对复杂的非线性关系进行建模, 同样只在池化层使用<i>Dropout</i>, 比较在<i>CIFAR</i>-10数据集上不同方法取不同保留概率<i>p</i> (<i>p</i>=0.3, <i>p</i>=0.5和<i>p</i>=0.7) 的影响。在上述CNN网络结构上训练迭代1 000次, 得到不同池化方法错误率与保留概率<i>p</i>的关系如图8所示。由图8可以看出, 本文方法在不同保留概率<i>p</i>下依然表现最好, 在<i>p</i>=0.5时依旧最优, 并且随着<i>p</i>的增加不同方法之间的性能差异逐渐缩小。</p>
                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201906009_162.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 CIFAR- 10错误率与保留概率p的关系" src="Detail/GetImg?filename=images/JSJY201906009_162.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 CIFAR- 10错误率与保留概率<i>p</i>的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201906009_162.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Relationship between error rate and retention probability <i>p</i> on CIFAR- 10</p>

                </div>
                <div class="p1">
                    <p id="163">利用CIFAR- 10数据集在同一结构的上述CNN模型下, 记录使用全连接层Dropout的本文方法与其他方法的错误率:在不使用Dropout时, 错误率约为16.50%, 在全连接层使用Dropout、文献<citation id="245" type="reference">[<a class="sup">7</a>]</citation> 的Maxout+Dropout、文献<citation id="246" type="reference">[<a class="sup">10</a>]</citation> 的Stochastic Pooling、文献<citation id="247" type="reference">[<a class="sup">11</a>]</citation> 结合全连接层Dropout的最大池化Dropout (Prob.weighted pooling) 、结合全连接层Dropout的本文方法错误率分别为14.24%、11.68%、15.14%、11.29%、11.23%, 可以看出本文方法在较大型数据集上依然具有不错的抑制过拟合表现。</p>
                </div>
                <h3 id="164" name="164" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="165">本文利用<i>Dropout</i>, 提出了一种双重概率加权池化的模型预测平均方法, 用于解决<i>CNN</i>中的过拟合问题。通过将池化区域内各单元值概率引入到<i>Prob</i>.<i>weighted pooling</i>方法, 并探索合适的保留概率<i>p</i>使算法具有更强的泛化性, 能够有效抑制过拟合, 从而能够将测试阶段的错误率向训练阶段靠拢, 使得错误率更低。可见选择池化激活值时, 考虑池化值对整个池化区域的影响有助于降低网络的错误率。通过对本文改进的方法与同样解决过拟合问题的方法进行实验对比, 结果表明本文所提方法相较于其他已有解决过拟合方法的错误率更低, 且保留概率为0.5时效果较好, 能够有效降低错误率, 抑制过拟合, 应用于图像分类任务可使识别率得到提升。但是在引入Dropout机制后的一个普遍现象是收敛速度变慢, 考虑到不同目标函数优化器的收敛速度不同, 下一步研究可在本文方法中加入不同目标优化函数, 使得模型训练测试时收敛速度更快。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="188">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pattern Recognition and Machine Learning">

                                <b>[1]</b>BISHOP C.Pattern Recognition and Machine Learning[M].Berlin:Springer, 2006:560-571.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700296012&amp;v=MDE0MDRUTW53WmVadEZpbmxVcjNJSjFvU2J4UT1OaWZPZmJLOEg5RE1xSTlGWnVJSkRIMDdvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>SCHMIDHUBER J.Deep learning in neural networks:an overview[J].Neural Networks, 2015, 61:85-117.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning">

                                <b>[3]</b>LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521:436-444.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=Mjc2MTJxWTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFZick9MejdCZHJHNEg5Yk0=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251. (ZHOU F Y, JIN L P, DONG J.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251.) 
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving neural networks by preventing co-adaptation of feature detector">

                                <b>[5]</b>HINTON G E, SRIVASTAVA N, KRIZHEVSKY A, et al.Improving neural networks by preventing co-adaptation of feature detector[J].ar Xiv Preprint, 2012, 2012:ar Xiv.1207.0580.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">

                                <b>[6]</b>SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research, 2014, 15 (1) :1929-1958.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maxout networks">

                                <b>[7]</b>GOOGFELLOW I, WARDE-FARLEY D, MIRZA M, et al.Maxout networks[C]//Proceedings of the 30th International Conference on Machine Learning.Atlanta, Georgia:JMLR W&amp;CP, 2013:1319-1327.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Regularization of Neural Networks Using Ropconnect">

                                <b>[8]</b>WAN L, ZEILER M D, ZHANG S, et al.Regularization of neural networks using dropconnect[C]//Proceeding of the 30th International Conference on Machine Learning.Atlanta, Georgia:JMLRW&amp;CP, 2013:1058-1066.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning under privileged information using heteroscedastic dropout">

                                <b>[9]</b>LAMBERT J, SENER O, SAVARESE S.Deep learning under privileged information using heteroscedastic dropout[EB/OL].[2018-08-29].https://arxiv.org/pdf/1805.11614.pdf.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic Pooling for Regularization of Deep Convolutional Neural Networks">

                                <b>[10]</b>ZEILER M D, FERGUS R.Stochastic pooling for regularization of deep convolutional neural networks[EB/OL].[2018-09-17].https://arxiv.org/pdf/1301.3557.pdf.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6AC2EFB7CEA0C6A9FE9AF8F9C65A8CA3&amp;v=MTk3ODBOaWZPZmJYSmJkTzUyZjFDRjU1K0RBOC92aDlsbnpZTVBuZVVwV0V6Zk1PY05zdWNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh3N3k1eGE0PQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>WU H, GU X.Towards dropout training for convolutional neural networks[J].Neural Networks, 2015, 71:1-10.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network in Network[C/OL]">

                                <b>[12]</b>LIN M, CHEN Q, YAN S.Network in network[J/OL].ar Xiv Preprint, [2018-09-17].https://arxiv.org/pdf/1312.4400.pdf.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201707012&amp;v=MTUwMDZHNEg5Yk1xSTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnkvaFZick9NVFhUYkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>赵慧珍, 刘付显, 李龙跃, 等.基于混合maxout单元的卷积神经网络性能优化[J].通信学报, 2017, 38 (7) :105-114. (ZHAOH Z, LIU F X, LI L Y, et al.Improving deep convolutional neural networks with mixed maxout units[J].Journal on Communications, 2017, 38 (7) :105-114.) 
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Image Net classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multiple layers of features from tiny images">

                                <b>[15]</b>KRIZHEVSKY A.Learning multiple layers of features from tiny images[D].Toronto:University of Toronto, 2009:16-22.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Review of deep convolution neural network in image classification,&amp;quot;">

                                <b>[16]</b>AL-SAFFAR A A M, TAO H, TALAB M A.Review of deep convolution neural network in image classification[C]//Proceedings of the 2017 International Conference on Radar, Antenna, Microwave, Electronics, and Telecommunications.Piscataway, NJ:IEEE, 2017:26-31.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD133CEB751E7FAE945E2F067BF5D3440E&amp;v=MDQxNzJTSG5sM21Rd0RiR1FRYnJxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBodzd5NXhhND1OajdCYXJLN0hhSzUzWWhBWlo0SWVnMU14aElXbnowTA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>XIE L, WANG J, LIN W, et al.Towards reversal-invariant image representation[J].International Journal of Computer Vision, 2017, 123 (2) :226-250.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning">

                                <b>[18]</b>GOODFELLOW I, BENGIO Y, COURVILLE A, et al.Deep Learning[M].Cambridge, MA:MIT Press, 2016:11-12.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Study on the optimization of CNN based on image identification">

                                <b>[19]</b>FENG Y, ZENG S, YANG Y, et al.Study on the optimization of CNN based on image identification[C]//Proceedings of the 17th International Symposium on Distributed Computing and Applications for Business Engineering and Science.Piscataway, NJ:IEEE, 2018:123-126.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201906009" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201906009&amp;v=MTMzNjNVUjdxZlp1WnNGeS9oVmJyT0x6N0JkN0c0SDlqTXFZOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxSWmZRblJCWGhwbXU2MEpGM0M3ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="2" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
