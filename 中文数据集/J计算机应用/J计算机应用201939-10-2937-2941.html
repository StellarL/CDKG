<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136459395283750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910024%26RESULT%3d1%26SIGN%3d%252bIYFMUmGCvMKPhqGk1nlGR1j63M%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910024&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910024&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910024&amp;v=MTYxODl1WnNGeW5oV3J2TUx6N0JkN0c0SDlqTnI0OUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#45" data-title="1 端到端声纹模型——深度说话人嵌入 ">1 端到端声纹模型——深度说话人嵌入</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="2 具有角度区分性的深度说话人嵌入 ">2 具有角度区分性的深度说话人嵌入</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="2.1 A-Softmax&lt;b&gt;原理&lt;/b&gt;">2.1 A-Softmax<b>原理</b></a></li>
                                                <li><a href="#67" data-title="2.2 A-Softmax&lt;b&gt;角度间隔的性质&lt;/b&gt;">2.2 A-Softmax<b>角度间隔的性质</b></a></li>
                                                <li><a href="#90" data-title="2.3 &lt;b&gt;网络模型设计&lt;/b&gt;">2.3 <b>网络模型设计</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#130" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#131" data-title="3.1 &lt;b&gt;实验数据集&lt;/b&gt;">3.1 <b>实验数据集</b></a></li>
                                                <li><a href="#134" data-title="3.2 &lt;b&gt;模型训练方法&lt;/b&gt;">3.2 <b>模型训练方法</b></a></li>
                                                <li><a href="#137" data-title="3.3 &lt;b&gt;实验结果分析&lt;/b&gt;">3.3 <b>实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#149" data-title="4 结语 ">4 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="图1 深度说话人嵌入系统流程">图1 深度说话人嵌入系统流程</a></li>
                                                <li><a href="#80" data-title="图2 二分类中权重与特征夹角关系示意图">图2 二分类中权重与特征夹角关系示意图</a></li>
                                                <li><a href="#89" data-title="图3 多分类中权重与特征夹角关系示意图">图3 多分类中权重与特征夹角关系示意图</a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;端到端声纹识别网络结构&lt;/b&gt;"><b>表</b>1 <b>端到端声纹识别网络结构</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;说话人辨认实验结果比较&lt;/b&gt;"><b>表</b>2 <b>说话人辨认实验结果比较</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;说话人确认实验结果比较&lt;/b&gt;"><b>表</b>3 <b>说话人确认实验结果比较</b></a></li>
                                                <li><a href="#148" data-title="图4 不同损失函数构建模型的DET曲线">图4 不同损失函数构建模型的DET曲线</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="183">


                                    <a id="bibliography_1" title="KINNUNEN T,LI H.An overview of text-independent speaker recognition:from features to supervectors[J].Speech Communication,2010,52(1):12-40." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300465429&amp;v=MTE2ODJubFVyM0lJVm9kYmhZPU5pZk9mYks3SHRET3JJOUZZTzBLQ0g0d29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        KINNUNEN T,LI H.An overview of text-independent speaker recognition:from features to supervectors[J].Speech Communication,2010,52(1):12-40.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_2" title="DEHAK N,KENNY P J,DEHAK R,et al.Front-end factor analysis for speaker verification[J].IEEE Transactions on Audio,Speech,and Language Processing,2011,19(4):788-798." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Front-End Factor Analysis for Speaker Verification">
                                        <b>[2]</b>
                                        DEHAK N,KENNY P J,DEHAK R,et al.Front-end factor analysis for speaker verification[J].IEEE Transactions on Audio,Speech,and Language Processing,2011,19(4):788-798.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_3" title="LI C,MA X,JIANG B,et al.Deep speaker:an end-to-end neural speaker embedding system[EB/OL].[2019-01-10].https://arxiv.org/pdf/1705.02304.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep speaker:an end-to-end neural speaker embedding system">
                                        <b>[3]</b>
                                        LI C,MA X,JIANG B,et al.Deep speaker:an end-to-end neural speaker embedding system[EB/OL].[2019-01-10].https://arxiv.org/pdf/1705.02304.pdf.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_4" title="LEI Y,SCHEFFER N,FERRER L,et al.A novel scheme for speaker recognition using a phonetically-aware deep neural network[C]//Proceedings of the 2014 IEEE International Conference on A-coustics,Speech and Signal Processing.Piscataway:IEEE,2014:1695-1699." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel scheme for speaker recognition using a phonetically-aware deep neural network">
                                        <b>[4]</b>
                                        LEI Y,SCHEFFER N,FERRER L,et al.A novel scheme for speaker recognition using a phonetically-aware deep neural network[C]//Proceedings of the 2014 IEEE International Conference on A-coustics,Speech and Signal Processing.Piscataway:IEEE,2014:1695-1699.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_5" title="FU T,QIAN Y,LIU Y,et al.Tandem deep features for text-dependent speaker verification[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/archive_papers/interspeech_2014/i14_1327.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tandem deep features for textdependent speaker verification">
                                        <b>[5]</b>
                                        FU T,QIAN Y,LIU Y,et al.Tandem deep features for text-dependent speaker verification[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/archive_papers/interspeech_2014/i14_1327.pdf.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_6" title="TIAN Y,CAI M,HE L,et al.Investigation of bottleneck features and multilingual deep neural networks for speaker verification[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1151.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Investigation of bottleneck features and multilingual deep neural networks for speaker verification">
                                        <b>[6]</b>
                                        TIAN Y,CAI M,HE L,et al.Investigation of bottleneck features and multilingual deep neural networks for speaker verification[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1151.pdf.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_7" title="VARIANI E,LEI X,Mc DERMOTT E,et al.Deep neural networks for small footprint text-dependent speaker verification[C]//Proceedings of the 2014 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2014:4052-4056." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for small footprint text-dependent speaker verification">
                                        <b>[7]</b>
                                        VARIANI E,LEI X,Mc DERMOTT E,et al.Deep neural networks for small footprint text-dependent speaker verification[C]//Proceedings of the 2014 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2014:4052-4056.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_8" title="CAI W,CHEN J,LI M.Analysis of length normalization in end-toend speaker verification system[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.03209.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analysis of length normalization in end-toend speaker verification system">
                                        <b>[8]</b>
                                        CAI W,CHEN J,LI M.Analysis of length normalization in end-toend speaker verification system[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.03209.pdf.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_9" title="王昕,张洪冉.基于DNN处理的鲁棒性I-Vector说话人识别算法[J].计算机工程与应用,2018,54(22):167-172.(WANG X,ZHANG H R.Robust i-vector speaker recognition method based on DNN processing[J].Computer Engineering and Applications,2018,54(22):167-172.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201822027&amp;v=MTY1OTh0R0ZyQ1VSN3FmWnVac0Z5bmhXcnZNTHo3TWFiRzRIOW5Pclk5SFk0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        王昕,张洪冉.基于DNN处理的鲁棒性I-Vector说话人识别算法[J].计算机工程与应用,2018,54(22):167-172.(WANG X,ZHANG H R.Robust i-vector speaker recognition method based on DNN processing[J].Computer Engineering and Applications,2018,54(22):167-172.)
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_10" title="LIU W,WEN Y,YU Z,et al.Sphere Face:deep hypersphere embedding for face recognition[C]//Proceedings of the IEEE2017 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6738-6746." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SphereFace:Deep Hypersphere Embedding for Face Recognition">
                                        <b>[10]</b>
                                        LIU W,WEN Y,YU Z,et al.Sphere Face:deep hypersphere embedding for face recognition[C]//Proceedings of the IEEE2017 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6738-6746.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_11" title="HEIGOLD G,MORENO I,BENGIO S,et al.End-to-end textdependent speaker verification[C]//Proceedings of the 2016IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2016:5115-5119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end textdependent speaker verification">
                                        <b>[11]</b>
                                        HEIGOLD G,MORENO I,BENGIO S,et al.End-to-end textdependent speaker verification[C]//Proceedings of the 2016IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2016:5115-5119.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_12" title="SNYDER D,GHAHREMANI P,POVEY D,et al.Deep neural network-based speaker embeddings for end-to-end speaker verification[C]//Proceedings of the 2016 IEEE Spoken Language Technology Workshop.Piscataway:IEEE,2016:165-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep neural network-based speaker embeddings for end-to-end speaker verification,&amp;quot;">
                                        <b>[12]</b>
                                        SNYDER D,GHAHREMANI P,POVEY D,et al.Deep neural network-based speaker embeddings for end-to-end speaker verification[C]//Proceedings of the 2016 IEEE Spoken Language Technology Workshop.Piscataway:IEEE,2016:165-170.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_13" title="ZHANG Y,PEZESHKI M,BRAKEL P,et al.Towards end-toend speech recognition with deep convolutional neural networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1701.02720.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards end-toend speech recognition with deep convolutional neural networks">
                                        <b>[13]</b>
                                        ZHANG Y,PEZESHKI M,BRAKEL P,et al.Towards end-toend speech recognition with deep convolutional neural networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1701.02720.pdf.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_14" title="ZHANG C,KOISHIDA K.End-to-end text-independent speaker verification with triplet loss on short utterances[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1608.PDF." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances">
                                        <b>[14]</b>
                                        ZHANG C,KOISHIDA K.End-to-end text-independent speaker verification with triplet loss on short utterances[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1608.PDF.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_15" title="WEN Y,ZHANG K,LI Z,et al.A discriminative feature learning approach for deep face recognition[C]//Proceedings of the2016 European Conference on Computer Vision,LNCS 9911.Cham:Springer,2016:499-515." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A discriminative feature learning approach for deep face recognition">
                                        <b>[15]</b>
                                        WEN Y,ZHANG K,LI Z,et al.A discriminative feature learning approach for deep face recognition[C]//Proceedings of the2016 European Conference on Computer Vision,LNCS 9911.Cham:Springer,2016:499-515.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_16" title="LIU W,WEN Y,YU Z,et al.Large-margin softmax loss for convolutional neural networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1612.02295.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-margin softmax loss for convolutional neural networks">
                                        <b>[16]</b>
                                        LIU W,WEN Y,YU Z,et al.Large-margin softmax loss for convolutional neural networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1612.02295.pdf.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_17" title="HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[17]</b>
                                        HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_18" title="CHUNG J S,NAGRANI A,ZISSERMAN A.Vox Celeb2:deep speaker recognition[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.05622.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vox Celeb2:deep speaker recognition">
                                        <b>[18]</b>
                                        CHUNG J S,NAGRANI A,ZISSERMAN A.Vox Celeb2:deep speaker recognition[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.05622.pdf.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_19" title="NAGRANI A,CHUNG J S,ZISSERMAN A.Vox Celeb:a largescale speaker identification dataset[EB/OL].[2019-01-10].https://arxiv.org/pdf/1706.08612.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vox Celeb:a largescale speaker identification dataset">
                                        <b>[19]</b>
                                        NAGRANI A,CHUNG J S,ZISSERMAN A.Vox Celeb:a largescale speaker identification dataset[EB/OL].[2019-01-10].https://arxiv.org/pdf/1706.08612.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-19 16:38</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),2937-2941 DOI:10.11772/j.issn.1001-9081.2019040757            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于角度间隔嵌入特征的端到端声纹识别模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">王康</a>
                                <a href="javascript:;">董元菲</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E7%83%BD%E7%81%AB%E5%A4%A9%E5%9C%B0%E9%80%9A%E4%BF%A1%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=0197167&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京烽火天地通信科技有限公司</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E9%82%AE%E7%94%B5%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉邮电科学研究院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统身份认证矢量(i-vector)与概率线性判别分析(PLDA)结合的声纹识别模型步骤繁琐、泛化能力较弱等问题,构建了一个基于角度间隔嵌入特征的端到端模型。该模型特别设计了一个深度卷积神经网络,从语音数据的声学特征中提取深度说话人嵌入;选择基于角度改进的A-Softmax作为损失函数,在角度空间中使模型学习到的不同类别特征始终存在角度间隔并且同类特征间聚集更紧密。在公开数据集VoxCeleb2上进行的测试表明,与i-vector结合PLDA的方法相比,该模型在说话人辨认中的Top-1和Top-5上准确率分别提高了58.9%和30%;而在说话人确认中的最小检测代价和等错误率上分别减小了47.9%和45.3%。实验结果验证了所设计的端到端模型更适合在多信道、大规模的语音数据集上学习到有类别区分性的特征。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A3%B0%E7%BA%B9%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">声纹识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">端到端模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">损失函数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E8%AF%B4%E8%AF%9D%E4%BA%BA%E5%B5%8C%E5%85%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度说话人嵌入;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王康(1987—),男,江苏南京人,工程师,主要研究方向:视频目标跟踪及行为分析、图像识别、音频识别、高性能计算;电子邮箱wangkang1808@fiberhome.com;
                                </span>
                                <span>
                                    董元菲(1995—),女,湖北武汉人,硕士研究生,主要研究方向:语音信号处理、深度学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目(2017YFB1400704);</span>
                    </p>
            </div>
                    <h1><b>Angular interval embedding based end-to-end voiceprint recognition model</b></h1>
                    <h2>
                    <span>WANG Kang</span>
                    <span>DONG Yuanfei</span>
            </h2>
                    <h2>
                    <span>Nanjing Fiber Home World Communication Technology Company Limited</span>
                    <span>Wuhan Research Institute of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An end-to-end model with angular interval embedding was constructed to solve the problems of complicated multiple steps and weak generalization ability in the traditional voiceprint recognition model based on the combination of identity vector(i-vector) and Probabilistic Linear Discriminant Analysis(PLDA). A deep convolutional neural network was specially designed to extract deep speaker embedding from the acoustic features of voice data. The Angular Softmax(A-Softmax), which is based on angular improvement, was employed as the loss function to keep the angular interval between the different classes of features learned by the model and make the clustering of the similar features closer in the angle space. Compared with the method combining i-vector and PLDA, it shows that the proposed model has the identification accuracy of Top-1 and Top-5 increased by 58.9% and 30% respectively and has the minimum detection cost and equal error rate reduced by 47.9% and 45.3% respectively for speaker verification on the public dataset VoxCeleb2. The results verify that the proposed end-to-end model is more suitable for learning class-discriminating features from multi-channel and large-scale datasets.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=voiceprint%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">voiceprint recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=end-to-end%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">end-to-end model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=loss%20function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">loss function;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20speaker%20embedding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep speaker embedding;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Kang,born in 1987,engineer.His research interests include video target tracking and behavior analysis,image recognition,audio recognition,high performance computing.;
                                </span>
                                <span>
                                    DONG Yuanfei,born in 1995,M.S.candidate.Her research interests include speech signal processing,deep learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-05</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Key Research and Development Plan for China(2017YFB1400704);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">声纹识别是一种从语音信息中提取生物特征的识别技术<citation id="221" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在该技术发展的几十年中,由Dehak等<citation id="222" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出的身份认证矢量(identity vector, i-vector)方法一度成为声纹识别领域的主流研究技术之一。该方法主要有三个步骤:1)利用高斯混合模型-通用背景模型(Gaussian Mixture Model-Universal Background Model, GMM-UBM)计算充分统计量;2)在全因子空间上提取i-vector;3)利用概率线性判别分析(Probabilistic Linear Discriminant Analysis, PLDA)计算i-vector间的似然比分数并作出判断<citation id="223" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="43">深度神经网络(Deep Neural Network, DNN)在图像识别、机器翻译和语音识别等诸多领域取得了非常瞩目的成绩,所以声纹识别技术同样引入了这一思想。利用DNN对声纹建模一般有两种方式:一种是利用DNN取代i-vector框架中GMM-UBM来计算充分统计量<citation id="224" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>;另一种是从DNN的瓶颈层中提取帧级别的特征,利用这些特征获得话音级别表示<citation id="228" type="reference"><link href="191" rel="bibliography" /><link href="193" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>,Variani等<citation id="225" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将DNN最后一个隐藏层中提取的特征整体取平均来替代i-vector,是这种思想的典型代表。目前,将DNN和i-vector融合的技术已经较为成熟,并且在部分小规模的商业产品上得到了实现。但利用该方法仍然存在两大问题:1)提取i-vector作为话音级别的表示形式后,还需要长度标准化和后续分类器的步骤<citation id="226" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,比较繁琐;2)由于加性噪声的存在,利用i-vector构建的模型泛化能力较弱<citation id="227" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="44">基于上述研究背景,本文构建一个端到端的声纹识别模型,利用卷积神经网络(Convolutional Neural Network, CNN)和重新设计的度量方式,提取到类别区分能力更强的嵌入表示。这些方法在人脸识别领域得到了验证,但在声纹识别中比较少见。模型用于文本无关的开集识别任务,也就是训练和测试数据没有交集,并通过嵌入之间的余弦距离直接来比较说话人之间的相似性。为使模型学习到的深度说话人嵌入有足够的类别区分性,损失函数选择A-Softmax(Angular Softmax)来替代分类网络中最常使用的Softmax。A-Softmax损失函数能学习角度判别特征,将不同类别的特征映射到单位超球面上的不同区域内<citation id="229" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,更适合大规模数据集的训练,得到泛化能力更强的模型。</p>
                </div>
                <h3 id="45" name="45" class="anchor-tag">1 端到端声纹模型——深度说话人嵌入</h3>
                <div class="p1">
                    <p id="46">开集识别任务本质上是一种度量学习任务,其中的关键是使模型学习到类别间隔较大的特征,所形成的特征空间足以概括没训练过的说话人,所以模型训练过程中的目标是在特征空间中不断缩小同类距离的同时增大异类之间的距离。</p>
                </div>
                <div class="p1">
                    <p id="47">目前,已有一些研究通过改进主干神经网络结构来提升模型效果,如文献<citation id="230" type="reference">[<a class="sup">11</a>]</citation>利用循环神经网络(Recurrent Neural Network, RNN)提取话音级别特征作为说话人嵌入,文献<citation id="231" type="reference">[<a class="sup">12</a>]</citation>则利用NIN(Network In Network)建模。CNN最初在图像领域应用广泛,将其应用到语音分析中也能有效地在声学特征中减少谱之间的变化并对谱之间的相关性进行建模<citation id="232" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,故本文选择CNN从声学特征中提取语音数据帧级别的特征。从平衡训练时间和模型深度的角度来看,选取CNN提取特征也要优于语音识别中常用的长短期记忆(Long Short-Term Memory, LSTM)网络<citation id="233" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="48">另一方面,模型的度量方式也可以进行改进,基于这种改进思想一般有两种方式<citation id="234" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>:一种是训练分类网络作为深度说话人嵌入的提取器,在损失函数上加上限制条件约束网络学习方向,提取输出层前一层的特征作为深度说话人嵌入;另一种是直接在特征空间中训练,使不同类别说话人之间的欧氏距离有一定的间隔,并将归一化后的特征作为深度说话人嵌入,这样特征空间中的欧氏距离与余弦距离意义等价,测试阶段可以直接利用余弦相似性计算分数。直接度量特征之间距离最具代表性的是三元组损失<citation id="235" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,但三元组的挖掘非常复杂,导致模型训练非常耗时,且对性能敏感,所以本文主要研究分类网络,即输出层神经元的个数等于训练的说话人类别数,这种思想的系统流程如图1所示。</p>
                </div>
                <div class="area_img" id="49">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910024_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 深度说话人嵌入系统流程" src="Detail/GetImg?filename=images/JSJY201910024_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 深度说话人嵌入系统流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910024_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flow chart of deep speaker embedding system</p>

                </div>
                <div class="p1">
                    <p id="50">系统的整体流程分为训练过程和测试过程两个部分。在训练过程中,将从语音数据中提取的声学特征送入CNN生成帧级别的特征,帧级别的特征被激活后送入平均池化层得到话音级别的特征,再利用仿射层进行维度转换,得到固定维度的深度说话人嵌入,输出层将固定维度的说话人嵌入映射到训练说话人类别数。损失函数是构建的端到端网络训练过程的最后一步,通过不断减小网络预测值和实际标签的差距来提高网络性能。在测试阶段,先把语音数据送入已经训练好的网络模型,从仿射层中得到深度说话人嵌入,再计算每对嵌入之间的余弦距离,根据阈值即可判断该对语音数据是属于相同说话人还是不同说话人。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">2 具有角度区分性的深度说话人嵌入</h3>
                <div class="p1">
                    <p id="52">基于Softmax损失函数学习到的深度说话人嵌入在本质上就有一定的角度区分性<citation id="236" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,这一点在文献<citation id="237" type="reference">[<a class="sup">15</a>]</citation>中也得到了证实,但在由Softmax直接映射的角度空间中对异类说话人嵌入没有明确的限制条件,这样同时优化了特征之间的夹角和距离。A-Softmax损失函数将特征权值进行归一化,使CNN更集中于优化不同特征之间的夹角,学习到具有角度区分性的深度说话人嵌入<citation id="238" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,以提高模型性能。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53">2.1 A-Softmax<b>原理</b></h4>
                <div class="p1">
                    <p id="54">延用文献<citation id="239" type="reference">[<a class="sup">16</a>]</citation>中的定义,将分类网络的全连接输出层,Softmax函数以及交叉熵损失函数三个步骤联合定义为Softmax损失函数,表达式为:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>o</mtext><mtext>f</mtext><mtext>t</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></msub><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mrow><mo>(</mo><mrow><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">其中:<i><b>x</b></i><sub><i>i</i></sub>表示第<i>i</i>个训练样本的输入特征; <i>y</i><sub><i>i</i></sub>为其对应的类别标签;<i><b>W</b></i><sub><i>j</i></sub>、<i><b>W</b></i><sub><i>yi</i></sub>分别是全连接输出层权重矩阵<i><b>W</b></i>的第<i>j</i>列和第<i>y</i><sub><i>i</i></sub>列;<i>b</i><sub><i>j</i></sub>、<i>b</i><sub><i>yi</i></sub>为其对应的偏置。训练数据时一般会分批处理,<i>N</i>即为每一批次中的训练样本个数,<i>K</i>为所有训练样本中的类别数。</p>
                </div>
                <div class="p1">
                    <p id="57">将<i><b>W</b></i>与<i><b>x</b></i><sub><i>i</i></sub>展开成模长与夹角余弦的乘积,同时限制‖<i><b>W</b></i><sub><i>j</i></sub>‖=1和<i>b</i><sub><i>j</i></sub>=0,即在每次迭代中都将权重矩阵<i><b>W</b></i>每列的模进行归一化,并将偏置设为0,损失函数表达式转化为:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mrow><mo>(</mo><mrow><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中:<i>θ</i><sub><i>j</i></sub><sub>,</sub><sub><i>i</i></sub>(0≤<i>θ</i><sub><i>j</i></sub><sub>,</sub><sub><i>i</i></sub>≤π)为向量<i><b>W</b></i><sub><i>j</i></sub>与特征<i><b>x</b></i><sub><i>i</i></sub>间的夹角,式(2)表明了训练样本<i>i</i>被预测为类别<i>j</i>的概率仅与<i>θ</i><sub><i>j</i></sub><sub>,</sub><sub><i>i</i></sub>有关。A-Softmax不仅在角度空间上使不同类别的样本分离,同时利用倍角关系增大了类别之间的角度间隔<citation id="240" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,表达式为:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mrow><mo>(</mo><mrow><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>m</mi><mi>θ</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>m</mi><mi>θ</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo>≠</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>Κ</mi></munderover><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">为便于计算,令<i>m</i>≥2,且为整数。由于<i>θ</i><sub><i>yi</i></sub><sub>,</sub><sub><i>i</i></sub>的范围限制在[0,π/<i>m</i>]内,定义一个单调递减函数来解除这一限制:</p>
                </div>
                <div class="p1">
                    <p id="62"><i>φ</i>(<i>θ</i><sub><i>yi</i></sub><sub>,</sub><sub><i>i</i></sub>)=(-1)<sup><i>k</i></sup>cos(<i>mθ</i><sub><i>yi</i></sub><sub>,</sub><sub><i>i</i></sub>)-2<i>k</i>      (4)</p>
                </div>
                <div class="p1">
                    <p id="63">其中:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>i</mi></mrow></msub><mo>∈</mo><mfrac><mrow><mi>k</mi><mtext>π</mtext></mrow><mi>m</mi></mfrac><mo>,</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mtext>π</mtext></mrow><mi>m</mi></mfrac></mtd></mtr><mtr><mtd><mi>k</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>m</mi><mo>-</mo><mn>1</mn><mo stretchy="false">]</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">此时,A-Softmax损失函数的表达式转化为:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mo>-</mo><mtext>S</mtext><mtext>o</mtext><mtext>f</mtext><mtext>t</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></msub><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mrow><mo>(</mo><mrow><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>φ</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>φ</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo>≠</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>Κ</mi></munderover><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">2.2 A-Softmax<b>角度间隔的性质</b></h4>
                <div class="p1">
                    <p id="68"><i>A</i>-<i>Softmax</i>损失函数不仅通过角度间隔增加了特征之间的类别区分能力,同时能将学习到的特征映射到单位超球面上解释。权重<i><b>W</b></i><sub><i>yi</i></sub>与特征<i><b>x</b></i><sub><i>i</i></sub>之间的夹角对应于该单位超球面上的最短弧长,同一类别在超球面上形成一个类似于超圆的区域。通过角度间隔参数<i>m</i>的设定可以调节学习任务的难易程度,<i>m</i>越大,单个类别形成的超圆区域也就越小,学习任务也越困难。但<i>m</i>存在一个最小值<i>m</i><sub>min</sub>使同类特征之间最大角度间隔小于异类特征之间最小角度间隔,文献<citation id="241" type="reference">[<a class="sup">10</a>]</citation>中未给出推导过程,本文将在二维空间中定量分析<i>m</i><sub>min</sub>。</p>
                </div>
                <div class="p1">
                    <p id="69">二分类情况下不同类别之间的角度间隔如图2所示,其中<i><b>W</b></i><sub>1</sub>、<i><b>W</b></i><sub>2</sub>分别是类1、类2的权重向量,<i><b>W</b></i><sub>1</sub>与<i><b>W</b></i><sub>2</sub>之间的夹角为<i>θ</i><sub>12</sub>,令输入的特征<i><b>x</b></i>属于类1,则有cos(<i>mθ</i><sub>1</sub>)&gt;cos(<i>θ</i><sub>2</sub>),即<i>mθ</i><sub>1</sub>&lt;<i>θ</i><sub>2</sub>。当特征<i><b>x</b></i>在<i><b>W</b></i><sub>1</sub>、<i><b>W</b></i><sub>2</sub>之间时,<i>θ</i><sub>1</sub>存在一个属于类1的最大角<i>θ</i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mo>_</mo><mi>max</mi></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup></mrow></math></mathml>;当特征<i><b>x</b></i>在<i><b>W</b></i><sub>1</sub>、<i><b>W</b></i><sub>2</sub>之外时,<i>θ</i><sub>1</sub>存在一个属于类1的最大角<i>θ</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mo>_</mo><mi>max</mi></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup></mrow></math></mathml>,<i>θ</i><sub>1</sub>的范围即在<i>θ</i><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mo>_</mo><mi>max</mi></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup></mrow></math></mathml>与<i>θ</i><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mo>_</mo><mi>max</mi></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup></mrow></math></mathml>之间。如图2(a),当<i><b>x</b></i>在<i><b>W</b></i><sub>1</sub>、<i><b>W</b></i><sub>2</sub>之间时有:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msubsup><mrow></mrow><mrow><mn>1</mn><mo>_</mo><mi>max</mi></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">如图2(b),当<i><b>x</b></i>在<i><b>W</b></i><sub>1</sub>、<i><b>W</b></i><sub>2</sub>之外,且<i>θ</i><sub>1</sub>与<i>θ</i><sub>2</sub>有重叠部分时,可得<i>θ</i><sub>12</sub>≤(<i>m</i>-1)π/<i>m</i>,此时有:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msubsup><mrow></mrow><mrow><mn>1</mn><mo>_</mo><mi>max</mi></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">同理,如图2(c),即<i>θ</i><sub>12</sub>&gt;(<i>m</i>-1)π/<i>m</i>的情况下,此时有:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msubsup><mrow></mrow><mrow><mn>1</mn><mo>_</mo><mi>max</mi></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup><mo>=</mo><mfrac><mrow><mn>2</mn><mtext>π</mtext><mo>-</mo><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">类间最小角度示意图如图2(d),设<i><b>x</b></i>′属于类2,此时有:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mi>m</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">为满足同类特征之间最大角度间隔小于异类特征之间最小角度间隔,由图2中推出的4个角度可以得到不等式组:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mo>≤</mo><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub><mo>,</mo><mtext> </mtext><mtext> </mtext><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub><mo>≤</mo><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></mfrac><mtext>π</mtext></mtd></mtr><mtr><mtd><mfrac><mrow><mn>2</mn><mtext>π</mtext><mo>-</mo><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mo>≤</mo><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>θ</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub><mo>&gt;</mo><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></mfrac><mtext>π</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">解不等式(10)得到二分类情况下参数<i>m</i>满足<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>≥</mo><mn>2</mn><mo>+</mo><msqrt><mn>3</mn></msqrt></mrow></math></mathml>。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910024_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 二分类中权重与特征夹角关系示意图" src="Detail/GetImg?filename=images/JSJY201910024_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 二分类中权重与特征夹角关系示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910024_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Relationship of angles between weights and features in binary-class classification</p>

                </div>
                <div class="p1">
                    <p id="81">将二分类推广到多分类时如图3所示,其中<i><b>W</b></i><sub><i>i</i></sub>为类<i>i</i>的权重向量,<i><b>W</b></i><sub><i>i</i></sub>、<i><b>W</b></i><sub><i>i</i></sub><sub>+1</sub>间的夹角为<i>θ</i><mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>,令输入特征<i><b>x</b></i>属于类<i>i</i>,如图3(a),设<i><b>x</b></i>′属于类<i>i</i>+1,此时有:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>_</mo><mi>max</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>θ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mi>θ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">同理,在图3(b)中,设<i><b>x</b></i>′属于类<i>i</i>-1,此时有:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>_</mo><mi>max</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>θ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mi>θ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">为满足期望的特征分布,由以上4个角度同样得到不等式:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>θ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>θ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mo>≤</mo><mrow><mi>min</mi></mrow><mrow><mo>{</mo><mrow><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mi>θ</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>,</mo><mfrac><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mi>θ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">解不等式(15)得到多分类情况下参数<i>m</i>满足<i>m</i><sub>min</sub>≥3。</p>
                </div>
                <div class="p1">
                    <p id="88">选择满足期望特征分布的参数<i>m</i>,理论上可使所有训练特征按标准分布在单位超球面上,不同类别之间始终存在角度间隔,在此基础上训练尽可能多的类别数,则可以得到类别区分能力更强的深度说话人嵌入,提高模型的泛化能力。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910024_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 多分类中权重与特征夹角关系示意图" src="Detail/GetImg?filename=images/JSJY201910024_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 多分类中权重与特征夹角关系示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910024_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Relationship of angles between weights and features in multi-class classification</p>

                </div>
                <h4 class="anchor-tag" id="90" name="90">2.3 <b>网络模型设计</b></h4>
                <div class="p1">
                    <p id="91">本文设计的网络模型主要分为三个部分:首先是语音信号声学特征的提取;其次是主干神经网络的构建;最后,利用<i>A</i>-<i>Softmax</i>损失函数衡量模型预测值,并更新参数。</p>
                </div>
                <div class="p1">
                    <p id="92">在声学特征提取阶段,为保留更丰富的原始音频信息,将语音信号利用帧长25 <i>ms</i>、帧移10 <i>ms</i>的滑动窗口转化为64维<i>FBank</i>(<i>FilterBank</i>)特征。每个样本随机截取多个约0.6 <i>s</i>的语音段,生成64×64的特征矩阵,经过零均值,单位方差归一化后,转化为单通道的特征图送入构建好的<i>CNN</i>。</p>
                </div>
                <div class="p1">
                    <p id="93">主干网络是基于残差网络设计<citation id="242" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,网络层细节如表1所示。</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表</b>1 <b>端到端声纹识别网络结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><i>Tab</i>.1 <i>Architecture of end</i>-<i>to</i>-<i>end voiceprint recognition network</i></p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td><br />网络层名称</td><td>网络结构</td><td>步长</td><td>输出尺寸</td></tr><tr><td><br />卷积层-1</td><td>5×5, 64</td><td>2×2</td><td>32×32×64</td></tr><tr><td><br />残差块-1</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>〖</mo><mrow><mtable><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo></mtd><mtd><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo></mtd><mtd><mn>6</mn><mn>4</mn></mtd></mtr></mtable></mrow></mrow><mo>×</mo><mn>1</mn></mrow></math></td><td>1×1</td><td>32×32×64</td></tr><tr><td><br />卷积层-2</td><td>5×5, 128</td><td>2×2</td><td>16×16×128</td></tr><tr><td><br />残差块-2</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>〖</mo><mrow><mtable><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo></mtd><mtd><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo></mtd><mtd><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr></mtable></mrow></mrow><mo>×</mo><mn>1</mn></mrow></math></td><td>1×1</td><td>16×16×128</td></tr><tr><td><br />卷积层-3</td><td>5×5, 256</td><td>2×2</td><td>8×8×256</td></tr><tr><td><br />残差块-3</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>〖</mo><mrow><mtable><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo></mtd><mtd><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo></mtd><mtd><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr></mtable></mrow></mrow><mo>×</mo><mn>1</mn></mrow></math></td><td>1×1</td><td>8×8×256</td></tr><tr><td><br />时间池化层</td><td>8×1</td><td>8×1</td><td>1×8×256</td></tr><tr><td><br />尺度变换</td><td>—</td><td>—</td><td>2 048</td></tr><tr><td><br />仿射层</td><td>2 048×512</td><td>—</td><td>512</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="95">每个残差块由两个卷积核为3×3、步长为1×1的卷积层构成,包含低层输出到高层输入的直接连接,每一种残差块只有一个。当输出通道数增加时,利用一个卷积核为5×5、步长为2×2的卷积层使频域的维度保持不变,将经过多个卷积层和残差块提取到的帧级别特征送入时间平均池化层。时间平均池化层将特征在时域上整体取均值,得到话音级别的特征,使得构建的网络在时间位置上具有不变性,再经过仿射层将话音级别的特征映射成512维的深度说话人嵌入。</p>
                </div>
                <div class="p1">
                    <p id="96"><i>A</i>-<i>Softmax</i>损失函数中的角度间隔参数<i>m</i>设为3,利用反向传播更新模型参数。测试数据直接从仿射层提取512维深度说话人嵌入,通过L2归一化后直接计算余弦相似性,设置分数阈值评判一对嵌入属于相同说话人还是不同说话人,模型的训练算法和测试算法分别如算法1和算法2所示。</p>
                </div>
                <div class="p1">
                    <p id="97">算法1 模型训练算法。</p>
                </div>
                <div class="p1">
                    <p id="98">输入 训练样本<i>X</i>,训练样本类别数<i>K</i>,角度间隔参数<i>m</i>,模型训练轮数<i>num</i>_<i>epoch</i>,批处理样本数<i>batchsize</i>。</p>
                </div>
                <div class="p1">
                    <p id="99">输出 模型<i>model</i>。</p>
                </div>
                <div class="p1">
                    <p id="100"><i>model</i>←create_graph()  /* 根据主干网络结构构建计算图*/</p>
                </div>
                <div class="p1">
                    <p id="101">for <i>epoch</i>←0 to <i>num</i>_<i>epoch</i></p>
                </div>
                <div class="p1">
                    <p id="102">do for <i>batch</i>←0 to (num(<i>X</i>)/<i>batchsize</i>)  /*计算每轮处理批数*/</p>
                </div>
                <div class="p1">
                    <p id="103">do <i><b>audio</b></i><b>_</b><i><b>feature</b></i>←zero(<i>shape</i>=)  /* 初始化声学特征张量 */</p>
                </div>
                <div class="p1">
                    <p id="104">for <i>i</i>←0 to <i>batchsize</i></p>
                </div>
                <div class="p1">
                    <p id="105">do function extract_feature(<i>X</i>)</p>
                </div>
                <div class="p1">
                    <p id="106"><i><b>feature</b></i>←FBank(<i>X</i>, <i>nfilt</i>=64)</p>
                </div>
                <div class="p1">
                    <p id="107"><i><b>feature</b></i>←<i><b>feature</b></i>  /* 时域上随机截取64维 */</p>
                </div>
                <div class="p1">
                    <p id="108"><i><b>feature</b></i>←normalize(<i><b>feature</b></i>, <i>dim</i>=0)</p>
                </div>
                <div class="p1">
                    <p id="109"><i><b>feature</b></i>←reshape(<i><b>feature</b></i>,<i>shape</i>=[64, 64, 1])  /* 特征向量维度转换 */</p>
                </div>
                <div class="p1">
                    <p id="110">return <i><b>feature</b></i></p>
                </div>
                <div class="p1">
                    <p id="111">end function</p>
                </div>
                <div class="p1">
                    <p id="112"><i><b>audio</b></i><b>_</b><i><b>feature</b></i>←extract_feature(<i>X</i>)</p>
                </div>
                <div class="p1">
                    <p id="113">endfor</p>
                </div>
                <div class="p1">
                    <p id="114"><i><b>embedding</b></i>←<i>model</i>(<i><b>audio</b></i><b>_</b><i><b>feature</b></i>)</p>
                </div>
                <div class="p1">
                    <p id="115"><i>loss</i>←A-Softmax(<i><b>embedding</b></i>, <i>K</i>, <i>m</i>)</p>
                </div>
                <div class="p1">
                    <p id="116">最小化<i>loss</i>,反向传播更新模型参数</p>
                </div>
                <div class="p1">
                    <p id="117">endfor</p>
                </div>
                <div class="p1">
                    <p id="118">endfor</p>
                </div>
                <div class="p1">
                    <p id="119">return <i>model</i></p>
                </div>
                <div class="p1">
                    <p id="120">算法2 模型测试算法。</p>
                </div>
                <div class="p1">
                    <p id="121">输入 声纹识别模型<i>model</i>,测试样本对<i>test</i>1、<i>test</i>2。</p>
                </div>
                <div class="p1">
                    <p id="122">输出 相似性分数<i>score</i>。</p>
                </div>
                <div class="p1">
                    <p id="123"><i>model</i>←read_graph(<i>model</i>)  /* 读入模型计算图和参数 */</p>
                </div>
                <div class="p1">
                    <p id="124"><i><b>audio</b></i><b>_</b><i><b>feature</b></i>1←<i>extract</i>_<i>feature</i>(<i>test</i>1)  /* 与算法1特征提取方法相同 */</p>
                </div>
                <div class="p1">
                    <p id="125"><i><b>audio</b></i><b>_</b><i><b>feature</b></i>2←<i>extract</i>_<i>feature</i>(<i>test</i>2)</p>
                </div>
                <div class="p1">
                    <p id="126"><i><b>embedding</b></i>1←<i>model</i>(<i><b>audio</b></i><b>_</b><i><b>feature</b></i>1)</p>
                </div>
                <div class="p1">
                    <p id="127"><i><b>embedding</b></i>2←<i>model</i>(<i><b>audio</b></i><b>_</b><i><b>feature</b></i>2)</p>
                </div>
                <div class="p1">
                    <p id="128"><i>score</i>←cosine_similarity(<i><b>embedding</b></i>1, <i><b>embedding</b></i>2)  /* 计算一对嵌入的余弦相似性分数 */</p>
                </div>
                <div class="p1">
                    <p id="129">return <i>score</i></p>
                </div>
                <h3 id="130" name="130" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="131" name="131">3.1 <b>实验数据集</b></h4>
                <div class="p1">
                    <p id="132">为得到一个强鲁棒性模型,需要训练一个多类别、多信道的大规模数据集,本实验采用VoxCeleb2数据集进行验证。VoxCeleb是一个从YouTube网站的采访视频中提取的视听数据集,由人类语音的短片段组成,其中VoxCeleb2数据集的规模比目前任何一个公开的声纹识别数据集仍大数倍,包含近6 000个说话人产生的百万多条语音数据<citation id="243" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="133">VoxCeleb2中的语音数据包含不同种族、口音、职业和年龄的说话人演讲,数据在无任何约束条件下采集,背景有说话声、笑声、重叠的语音等符合实际环境的各种噪声<citation id="244" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,更适合训练端到端的神经网络模型。同时该数据集提供了几种不同方法在不同评价指标下的基础分数,本实验构建自己的网络模型与i-vector结合PLDA的方法进行比较。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">3.2 <b>模型训练方法</b></h4>
                <div class="p1">
                    <p id="135">模型共训练40轮,每轮每批处理的样本数为64个音频文件。每个卷积层后都加入批标准化(Batch Normalization, BN)和激活层,以提高模型训练速度,激活函数选择上限值为20的线性整流函数(Rectified Linear Unit, ReLU)。优化器选择动量为0.9的随机梯度下降法,权重衰减设为0。</p>
                </div>
                <div class="p1">
                    <p id="136">为防止训练过程中损失函数出现震荡,利用指数衰减法控制模型学习率,衰减系数设为0.98,每隔1 000个批处理步骤当前学习率乘以衰减系数,模型初始学习率为0.001。</p>
                </div>
                <h4 class="anchor-tag" id="137" name="137">3.3 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="138">训练好的端到端模型可以同时进行说话人辨认和说话人确认两个实验,前者是“多选一”问题,后者是“一对一判别”问题。VoxCeleb2中的测试集共有118类,36 237条语音,两个实验设计的方法均参考文献<citation id="245" type="reference">[<a class="sup">19</a>]</citation>。实验训练了基于Softmax和A-Softmax两种损失函数的模型,以验证本文模型的优势。</p>
                </div>
                <div class="p1">
                    <p id="139">对于说话人辨认实验,在118个说话人中每人选择5条语音数据构建声纹库,即声纹库中一共包含590条语音。再从每个说话人中选择一条不同于声纹库的测试语音与声纹库中的所有语音进行比对,按相似性分数从大到小排序,计算相似度最大匹配成功的概率Top-1和前5名匹配成功的概率Top-5,结果见表2。</p>
                </div>
                <div class="area_img" id="140">
                    <p class="img_tit"><b>表</b>2 <b>说话人辨认实验结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.2 Comparison of results in speaker identification experiments</p>
                    <p class="img_note">单位:%</p>
                    <table id="140" border="1"><tr><td><br />方法</td><td>Top-1</td><td>Top-5</td></tr><tr><td><br />i-vector+PLDA<sup>[19]</sup></td><td>60.8</td><td>75.6</td></tr><tr><td><br />Deep speaker embedding+Softmax</td><td>82.2</td><td>91.5</td></tr><tr><td><br />Deep speaker embedding+A-Softmax</td><td>96.6</td><td>98.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="141">由表2的实验结果可知,采用提取深度说话人嵌入的方式,模型性能要明显优于i-vector结合PLDA的方法。选择A-Softmax作为损失函数构建的模型与之相比,Top-1和Top-5准确率分别提高了58.9%和30%。原因在于传统方法提取的i-vector中既包含说话人信息又包含信道信息,利用信道补偿的方法来减少信道影响不能充分拟合多种信道下采集的训练数据集,CNN却可以很好地拟合这种多种信道样本和标签之间的关系。损失函数选择A-Softmax与Softmax相比,Top-1和Top-5准确率分别提高了17.53%和7.41%。原因在于A-Softmax能学习到具有角度区分性的特征,对于从大规模的数据集训练得到的说话人嵌入在单位超球面上聚集更集中,这使得采用A-Softmax的模型比采用Softmax的模型具有更强的泛化能力。</p>
                </div>
                <div class="p1">
                    <p id="142">在说话人确认实验中,测试集中共有36 237条语音,对于每条测试语音组成两对与该条语音属于同种说话人的语音数据和两对属于不同说话人的语音数据,实验一共组成了144 948对测试对。计算所有测试对中错误接受率(False Acceptance Rate, FAR)和错误拒绝率(False Rejection Rate, FRR)相等时等错误率(Equal Error Rate, EER)的值。同时该实验还有一个评价标准为最小检测代价函数(Minimum Detection Cost Function, minDCF),检测代价函数<i>DCF</i>的公式为:</p>
                </div>
                <div class="p1">
                    <p id="143"><i>DCF</i>=<i>C</i><sub>FR</sub>×<i>P</i><sub>FRR</sub>×<i>P</i><sub>target</sub>+</p>
                </div>
                <div class="p1">
                    <p id="144"><i>C</i><sub>FA</sub>×<i>P</i><sub>FAR</sub>×(1-<i>P</i><sub>target</sub>)      (16)</p>
                </div>
                <div class="p1">
                    <p id="145">其中:<i>C</i><sub>FR</sub>和<i>C</i><sub>FA</sub>分别表示错误拒绝和错误接受的惩罚代价;<i>P</i><sub>target</sub>和1-<i>P</i><sub>target</sub>分别为真实说话测试和冒认测试的先验概率。实验设定<i>C</i><sub>FR</sub>=<i>C</i><sub>FA</sub>=1,<i>P</i><sub>target</sub>=0.01<citation id="246" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,结果见表3。同时针对两种不同损失函数所构建的模型,绘制了检测错误权衡(Detection Error Tradeoff, DET)曲线图,结果如图4所示。</p>
                </div>
                <div class="p1">
                    <p id="146">由表3和图4的实验结果可知,采用提取深度说话人嵌入的方式,模型性能受损失函数的影响非常大。选择A-Softmax作为损失函数构建的模型与传统方法相比,minDCF和EER分别减小了47.9%和45.3%。原因是采用A-Softmax损失函数构建的模型增加了角度间隔,学习到的深度说话人嵌入有非常好的类别区分性。但选择Softmax作为损失函数构建的模型,minDCF反而大于传统方法,EER的减少程度也不明显,这也说明了Softmax并不适用于学习具有类别区分性的深度说话人嵌入。</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表</b>3 <b>说话人确认实验结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.3 Comparison of results in speaker verification experiments</p>
                    <p class="img_note"></p>
                    <table id="147" border="1"><tr><td><br />方法</td><td>minDCF</td><td>EER/%</td></tr><tr><td><br />i-vector+PLDA<sup>[19]</sup></td><td>0.73</td><td>8.8</td></tr><tr><td><br />Deep speaker embedding+Softmax</td><td>0.76</td><td>8.5</td></tr><tr><td><br />Deep speaker embedding+A-Softmax</td><td>0.38</td><td>4.81</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910024_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同损失函数构建模型的DET曲线" src="Detail/GetImg?filename=images/JSJY201910024_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同损失函数构建模型的DET曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910024_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 DET curves of models constructed with different loss functions</p>

                </div>
                <h3 id="149" name="149" class="anchor-tag">4 结语</h3>
                <div class="p1">
                    <p id="150">本文构建了一个端到端声纹识别模型,该模型利用类似于残差网络的卷积神经网络,从声学特征中提取深度说话人嵌入,选择A-Softmax作为损失函数来学习具有角度区分性的特征。通过对角度间隔参数<i>m</i>的分析,推导出满足期望的特征分布时<i>m</i>的最小值。本文从实验中得出,端到端的声纹模型能训练出结构更简单、泛化能力更强的模型,该模型在说话人辨认实验上有明显的优势,但在说话人确认实验中,模型性能受损失函数的影响较大。对于更大规模的数据集,本文构建的网络模型可能达不到更好的效果,需要构建更深的网络且减少过拟合对模型效果的影响,为保持特征在频域上的维度不变,可以对每一层的残差块个数进行增加。后续将会进一步研究在大规模数据集的条件下,所设计的模型中残差块的个数对声纹识别模型性能的影响。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="183">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300465429&amp;v=MDkwMjJud1plWnRGaW5sVXIzSUlWb2RiaFk9TmlmT2ZiSzdIdERPckk5RllPMEtDSDR3b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>KINNUNEN T,LI H.An overview of text-independent speaker recognition:from features to supervectors[J].Speech Communication,2010,52(1):12-40.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Front-End Factor Analysis for Speaker Verification">

                                <b>[2]</b>DEHAK N,KENNY P J,DEHAK R,et al.Front-end factor analysis for speaker verification[J].IEEE Transactions on Audio,Speech,and Language Processing,2011,19(4):788-798.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep speaker:an end-to-end neural speaker embedding system">

                                <b>[3]</b>LI C,MA X,JIANG B,et al.Deep speaker:an end-to-end neural speaker embedding system[EB/OL].[2019-01-10].https://arxiv.org/pdf/1705.02304.pdf.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel scheme for speaker recognition using a phonetically-aware deep neural network">

                                <b>[4]</b>LEI Y,SCHEFFER N,FERRER L,et al.A novel scheme for speaker recognition using a phonetically-aware deep neural network[C]//Proceedings of the 2014 IEEE International Conference on A-coustics,Speech and Signal Processing.Piscataway:IEEE,2014:1695-1699.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tandem deep features for textdependent speaker verification">

                                <b>[5]</b>FU T,QIAN Y,LIU Y,et al.Tandem deep features for text-dependent speaker verification[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/archive_papers/interspeech_2014/i14_1327.pdf.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Investigation of bottleneck features and multilingual deep neural networks for speaker verification">

                                <b>[6]</b>TIAN Y,CAI M,HE L,et al.Investigation of bottleneck features and multilingual deep neural networks for speaker verification[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1151.pdf.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for small footprint text-dependent speaker verification">

                                <b>[7]</b>VARIANI E,LEI X,Mc DERMOTT E,et al.Deep neural networks for small footprint text-dependent speaker verification[C]//Proceedings of the 2014 IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2014:4052-4056.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analysis of length normalization in end-toend speaker verification system">

                                <b>[8]</b>CAI W,CHEN J,LI M.Analysis of length normalization in end-toend speaker verification system[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.03209.pdf.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201822027&amp;v=MTcwMDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluaFdydk1MejdNYWJHNEg5bk9yWTlIWTRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>王昕,张洪冉.基于DNN处理的鲁棒性I-Vector说话人识别算法[J].计算机工程与应用,2018,54(22):167-172.(WANG X,ZHANG H R.Robust i-vector speaker recognition method based on DNN processing[J].Computer Engineering and Applications,2018,54(22):167-172.)
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SphereFace:Deep Hypersphere Embedding for Face Recognition">

                                <b>[10]</b>LIU W,WEN Y,YU Z,et al.Sphere Face:deep hypersphere embedding for face recognition[C]//Proceedings of the IEEE2017 Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2017:6738-6746.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end textdependent speaker verification">

                                <b>[11]</b>HEIGOLD G,MORENO I,BENGIO S,et al.End-to-end textdependent speaker verification[C]//Proceedings of the 2016IEEE International Conference on Acoustics,Speech and Signal Processing.Piscataway:IEEE,2016:5115-5119.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep neural network-based speaker embeddings for end-to-end speaker verification,&amp;quot;">

                                <b>[12]</b>SNYDER D,GHAHREMANI P,POVEY D,et al.Deep neural network-based speaker embeddings for end-to-end speaker verification[C]//Proceedings of the 2016 IEEE Spoken Language Technology Workshop.Piscataway:IEEE,2016:165-170.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards end-toend speech recognition with deep convolutional neural networks">

                                <b>[13]</b>ZHANG Y,PEZESHKI M,BRAKEL P,et al.Towards end-toend speech recognition with deep convolutional neural networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1701.02720.pdf.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances">

                                <b>[14]</b>ZHANG C,KOISHIDA K.End-to-end text-independent speaker verification with triplet loss on short utterances[EB/OL].[2019-01-10].https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1608.PDF.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A discriminative feature learning approach for deep face recognition">

                                <b>[15]</b>WEN Y,ZHANG K,LI Z,et al.A discriminative feature learning approach for deep face recognition[C]//Proceedings of the2016 European Conference on Computer Vision,LNCS 9911.Cham:Springer,2016:499-515.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-margin softmax loss for convolutional neural networks">

                                <b>[16]</b>LIU W,WEN Y,YU Z,et al.Large-margin softmax loss for convolutional neural networks[EB/OL].[2019-01-10].https://arxiv.org/pdf/1612.02295.pdf.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[17]</b>HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C]//Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,2016:770-778.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vox Celeb2:deep speaker recognition">

                                <b>[18]</b>CHUNG J S,NAGRANI A,ZISSERMAN A.Vox Celeb2:deep speaker recognition[EB/OL].[2019-01-10].https://arxiv.org/pdf/1806.05622.pdf.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vox Celeb:a largescale speaker identification dataset">

                                <b>[19]</b>NAGRANI A,CHUNG J S,ZISSERMAN A.Vox Celeb:a largescale speaker identification dataset[EB/OL].[2019-01-10].https://arxiv.org/pdf/1706.08612.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910024" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910024&amp;v=MTYxODl1WnNGeW5oV3J2TUx6N0JkN0c0SDlqTnI0OUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
