<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136466189033750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909004%26RESULT%3d1%26SIGN%3dHR14uXNxdkpLnGReEiTrdO%252blQvo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909004&amp;v=MTA0NzlHRnJDVVI3cWZadVpzRnlqbFVyL05MejdCZDdHNEg5ak1wbzlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="1.1 &lt;b&gt;三支决策&lt;/b&gt;">1.1 <b>三支决策</b></a></li>
                                                <li><a href="#82" data-title="1.2 &lt;b&gt;代价敏感主动学习&lt;/b&gt;">1.2 <b>代价敏感主动学习</b></a></li>
                                                <li><a href="#85" data-title="1.3 &lt;b&gt;标签均匀分布模型&lt;/b&gt;">1.3 <b>标签均匀分布模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="2 代价敏感主动学习问题描述 ">2 代价敏感主动学习问题描述</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#102" data-title="2.1 &lt;b&gt;数据模型&lt;/b&gt;">2.1 <b>数据模型</b></a></li>
                                                <li><a href="#108" data-title="2.2 &lt;b&gt;问题定义&lt;/b&gt;">2.2 <b>问题定义</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#118" data-title="3 CAFS算法 ">3 CAFS算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#120" data-title="3.1 &lt;b&gt;算法框架&lt;/b&gt;">3.1 <b>算法框架</b></a></li>
                                                <li><a href="#188" data-title="3.2 CAFS&lt;b&gt;时间复杂度分析&lt;/b&gt;">3.2 CAFS<b>时间复杂度分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#193" data-title="4 CAFS运行实例 ">4 CAFS运行实例</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#197" data-title="5 实验与结果分析 ">5 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#199" data-title="5.1 &lt;b&gt;实验数据集&lt;/b&gt;">5.1 <b>实验数据集</b></a></li>
                                                <li><a href="#203" data-title="5.2 &lt;b&gt;实验代价设置&lt;/b&gt;">5.2 <b>实验代价设置</b></a></li>
                                                <li><a href="#207" data-title="5.3 &lt;b&gt;与代价敏感学习算法的对比实验&lt;/b&gt;">5.3 <b>与代价敏感学习算法的对比实验</b></a></li>
                                                <li><a href="#210" data-title="5.4 &lt;b&gt;与其他代价敏感主动学习算法的对比&lt;/b&gt;">5.4 <b>与其他代价敏感主动学习算法的对比</b></a></li>
                                                <li><a href="#215" data-title="5.5 &lt;b&gt;与非代价敏感学习算法的对比实验&lt;/b&gt;">5.5 <b>与非代价敏感学习算法的对比实验</b></a></li>
                                                <li><a href="#218" data-title="5.6 &lt;b&gt;实验结果分析&lt;/b&gt;">5.6 <b>实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#223" data-title="6 结语 ">6 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#101" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;符号以及含义&lt;/b&gt;"><b>表</b>1 <b>符号以及含义</b></a></li>
                                                <li><a href="#190" data-title="&lt;b&gt;表&lt;/b&gt;2 CAFS&lt;b&gt;时间复杂度&lt;/b&gt;"><b>表</b>2 CAFS<b>时间复杂度</b></a></li>
                                                <li><a href="#196" data-title="图1 运行实例示意图">图1 运行实例示意图</a></li>
                                                <li><a href="#202" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;数据集信息&lt;/b&gt;"><b>表</b>3 <b>数据集信息</b></a></li>
                                                <li><a href="#209" data-title="&lt;b&gt;表&lt;/b&gt;4 CAFS&lt;b&gt;算法与其他代价敏感学习算法在不同数据集上的平均代价&lt;/b&gt;"><b>表</b>4 CAFS<b>算法与其他代价敏感学习算法在不同数据集上的平均代价</b></a></li>
                                                <li><a href="#214" data-title="&lt;b&gt;表&lt;/b&gt;5 CAFS&lt;b&gt;算法与其他代价敏感主动学习算法在不同数据集上的平均代价对比&lt;/b&gt;"><b>表</b>5 CAFS<b>算法与其他代价敏感主动学习算法在不同数据集上的平均代价对比</b></a></li>
                                                <li><a href="#217" data-title="&lt;b&gt;表&lt;/b&gt;6 CAFS&lt;b&gt;算法与其他非代价敏感学习算法在不同数据集上的平均代价的对比&lt;/b&gt;"><b>表</b>6 CAFS<b>算法与其他非代价敏感学习算法在不同数据集上的平均代价的对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="255">


                                    <a id="bibliography_1" title=" SETTLES B.Active Learning [M].San Rafael,CA:Morgan and Claypool Publishers,2012:1-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active Learning">
                                        <b>[1]</b>
                                         SETTLES B.Active Learning [M].San Rafael,CA:Morgan and Claypool Publishers,2012:1-114.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_2" title=" ZHU X,GOLDBERG A B.Introduction to Semi-Supervised Learning [M].San Rafael,CA:Morgan and Claypool Publishers,2009:130." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Introduction to Semi-Supervised Learning">
                                        <b>[2]</b>
                                         ZHU X,GOLDBERG A B.Introduction to Semi-Supervised Learning [M].San Rafael,CA:Morgan and Claypool Publishers,2009:130.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_3" title=" SEUNG H S,OPPER M,SOMPOLINSKY H.Query by committee [C]// COLT 1992:Proceedings of the 5th Annual ACM Conference on Computational Learning Theory.New York:ACM,1992:287-294." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Query by committee">
                                        <b>[3]</b>
                                         SEUNG H S,OPPER M,SOMPOLINSKY H.Query by committee [C]// COLT 1992:Proceedings of the 5th Annual ACM Conference on Computational Learning Theory.New York:ACM,1992:287-294.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_4" title=" COHN D A,GHAHRAMANI Z,JORDAN M I,et al.Active learning with statistical models [J].Journal of Artificial Intelligence Research,1996,4(1):129-145." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active Learning with Statistical Models">
                                        <b>[4]</b>
                                         COHN D A,GHAHRAMANI Z,JORDAN M I,et al.Active learning with statistical models [J].Journal of Artificial Intelligence Research,1996,4(1):129-145.
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_5" title=" WANG M,MIN F,ZHANG Z H,et al.Active learning through density clustering [J].Expert Systems with Applications,2017,85:305-317." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES63F8583EB1CA3324E0C158FE506680DC&amp;v=MjUyMDVLMD1OaWZPZmJXN2FObkpwNHd3RnVwOGZYODZ6UkptNmt4OFRYZVUyUmMxZjdTY1JjN3NDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TGkrdw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         WANG M,MIN F,ZHANG Z H,et al.Active learning through density clustering [J].Expert Systems with Applications,2017,85:305-317.
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_6" title=" TONG S,KOLLER D.Support vector machine active learning with applications to text classification [J].Journal of Machine Learning Research,2001,2(1):45-66." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support vector machine active learning with applications to text classification">
                                        <b>[6]</b>
                                         TONG S,KOLLER D.Support vector machine active learning with applications to text classification [J].Journal of Machine Learning Research,2001,2(1):45-66.
                                    </a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_7" title=" THOMPSON C A.Active learning for natural language parsing and information extraction[C]// ICML 1999:Proceeding of the 16th International Conference on Machine Learning.San Francisco,CA:Morgan Kaufmann Publishers,1999:406-414." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active learning for natural language parsing and information extraction">
                                        <b>[7]</b>
                                         THOMPSON C A.Active learning for natural language parsing and information extraction[C]// ICML 1999:Proceeding of the 16th International Conference on Machine Learning.San Francisco,CA:Morgan Kaufmann Publishers,1999:406-414.
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_8" title=" ZHANG C,CHEN T.An active learning framework for content-based information retrieval [J].IEEE Transactions on Multimedia,2002,4(2):260-268." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An active learning framework for content based information retrieval">
                                        <b>[8]</b>
                                         ZHANG C,CHEN T.An active learning framework for content-based information retrieval [J].IEEE Transactions on Multimedia,2002,4(2):260-268.
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_9" title=" YU D,VARADARAJAN B,DENG L,et al.Active learning and semi-supervised learning for speech recognition:a unified framework using the global entropy reduction maximization criterion [J].Computer Speech and Language,2010,24(3):433-444." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100176131&amp;v=MDEwOTFPZmJLN0h0RE9ybzlGWmV3SkRYODRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJRjRWYWhjPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         YU D,VARADARAJAN B,DENG L,et al.Active learning and semi-supervised learning for speech recognition:a unified framework using the global entropy reduction maximization criterion [J].Computer Speech and Language,2010,24(3):433-444.
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_10" title=" MARGINEANTU D D.Active cost-sensitive learning [C]// IJCAI 2005:Proceedings of the 19th International Joint Conference on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann Publishers,2005:1622-1623." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active cost-sensitive learning">
                                        <b>[10]</b>
                                         MARGINEANTU D D.Active cost-sensitive learning [C]// IJCAI 2005:Proceedings of the 19th International Joint Conference on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann Publishers,2005:1622-1623.
                                    </a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_11" title=" MIN F,LIU F L,WEN L Y,et al.Tri-partition cost-sensitive active learning through kNN [J].Soft Computing,2017,23(5):1557-1572." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tri-partition cost-sensitive active learning through kNN">
                                        <b>[11]</b>
                                         MIN F,LIU F L,WEN L Y,et al.Tri-partition cost-sensitive active learning through kNN [J].Soft Computing,2017,23(5):1557-1572.
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_12" title=" WU Y X,MIN X Y,MIN F,et al.Cost-sensitive active learning with a label uniform distribution model [J].International Journal of Approximate Reasoning,2019,105:49-65." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE7408523568E65361FD5DA88912D8F9E&amp;v=MDMxODBGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMaSt3SzA9TmlmT2ZjYS9HdEhFcW8xR1llMEhlWG84ekJBU25FdDRQQTdxcEJzMGU4YWNNN1BxQ09Odg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         WU Y X,MIN X Y,MIN F,et al.Cost-sensitive active learning with a label uniform distribution model [J].International Journal of Approximate Reasoning,2019,105:49-65.
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_13" title=" YAO Y.Three-way decision:an interpretation of rules in rough set theory [C]// Proceedings of the 2009 International Conference on Rough Sets and Knowledge Technology,LNCS 5589.Berlin:Springer,2009:642-649." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Three-way decisions:an interpretation of rules in rough set theory">
                                        <b>[13]</b>
                                         YAO Y.Three-way decision:an interpretation of rules in rough set theory [C]// Proceedings of the 2009 International Conference on Rough Sets and Knowledge Technology,LNCS 5589.Berlin:Springer,2009:642-649.
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_14" title=" 李华雄,周献中,黄兵,等.决策粗糙集与代价敏感分类[J].计算机科学与探索,2013,7(2):126-135.(LI H X,ZHOU X Z,HUANG B,et al.Decision-theoretic rough set and cost-sensitive classification [J].Journal of Frontiers of Computer Science and Technology,2013,7(2):126-135.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXTS201302005&amp;v=MjQ4NjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFVyL05MalhmZmJHNEg5TE1yWTlGWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         李华雄,周献中,黄兵,等.决策粗糙集与代价敏感分类[J].计算机科学与探索,2013,7(2):126-135.(LI H X,ZHOU X Z,HUANG B,et al.Decision-theoretic rough set and cost-sensitive classification [J].Journal of Frontiers of Computer Science and Technology,2013,7(2):126-135.)
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_15" title=" 刘盾,李天瑞,李华雄.粗糙集理论:基于三支决策视角[J].南京大学学报(自然科学版),2013,49(5):574-581.(LIU D,LI T R,LI H X.Rough set theory:a three-way decisions perspective [J].Journal of Nanjing University (Natural Science),2013,49(5):574-581)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJDZ201305006&amp;v=MTQ3NDZxZlp1WnNGeWpsVXIvTkt5ZlBkTEc0SDlMTXFvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         刘盾,李天瑞,李华雄.粗糙集理论:基于三支决策视角[J].南京大学学报(自然科学版),2013,49(5):574-581.(LIU D,LI T R,LI H X.Rough set theory:a three-way decisions perspective [J].Journal of Nanjing University (Natural Science),2013,49(5):574-581)
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_16" title=" 杨习贝,杨静宇.邻域系统粗糙集模型[J].南京理工大学报,2012,36(2):291-295.(YANG X B,YANG J Y.Rough set model based on neighborhood system [J].Journal of Nanjing University of Science and Technology,2012,36(2):291-295.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJLG201202020&amp;v=MDQ1NjNIYWJHNEg5UE1yWTlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFVyL05LeWY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         杨习贝,杨静宇.邻域系统粗糙集模型[J].南京理工大学报,2012,36(2):291-295.(YANG X B,YANG J Y.Rough set model based on neighborhood system [J].Journal of Nanjing University of Science and Technology,2012,36(2):291-295.)
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_17" title=" SETTLES B,CRAVEN M,Friedland L.Active learning with real annotation costs [EB/OL].[2018- 12- 13].https://www.researchgate.net/publication/228770726_Active_learning_with_real_annotation_costs." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active learning with real annotation costs">
                                        <b>[17]</b>
                                         SETTLES B,CRAVEN M,Friedland L.Active learning with real annotation costs [EB/OL].[2018- 12- 13].https://www.researchgate.net/publication/228770726_Active_learning_with_real_annotation_costs.
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_18" title=" LIU A,JUN G,GHOSH J.Spatially cost-sensitive active learning [C]// SDM 2009:Proceedings of the 2009 SIAM International Conference on Data Mining.Philadelphia,PA:SIAM,2009:814-825." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatially cost-sensitive active learning">
                                        <b>[18]</b>
                                         LIU A,JUN G,GHOSH J.Spatially cost-sensitive active learning [C]// SDM 2009:Proceedings of the 2009 SIAM International Conference on Data Mining.Philadelphia,PA:SIAM,2009:814-825.
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_19" title=" ZHAO P L,HOI S C H.Cost-sensitive online active learning with application to malicious URL detection [C]// KDD 2013:Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,2013:919-927." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cost-sensitive online active learning with application to malicious URL detection">
                                        <b>[19]</b>
                                         ZHAO P L,HOI S C H.Cost-sensitive online active learning with application to malicious URL detection [C]// KDD 2013:Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,2013:919-927.
                                    </a>
                                </li>
                                <li id="293">


                                    <a id="bibliography_20" title=" CHEN P-L,LIN H-T.Active learning for multiclass cost-sensitive classification using probabilistic models [C]// TAAI 2013:Proceedings of the 2013 Conference on Technologies and Applications of Artificial Intelligence.Washington,DC:IEEE Computer Society,2013:13-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active learning for multiclass cost-sensitive classification using probabilistic models">
                                        <b>[20]</b>
                                         CHEN P-L,LIN H-T.Active learning for multiclass cost-sensitive classification using probabilistic models [C]// TAAI 2013:Proceedings of the 2013 Conference on Technologies and Applications of Artificial Intelligence.Washington,DC:IEEE Computer Society,2013:13-18.
                                    </a>
                                </li>
                                <li id="295">


                                    <a id="bibliography_21" title=" DEMIR B,MINELLO L,BRUZZONE L.Definition of effective training sets for supervised classification of remote sensing images by a novel cost-sensitive active learning method [J].IEEE Transactions on Geoscience and Remote Sensing,2014,52(2):1272-1284." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Definition of effective training sets for supervised classification of remote sensing images by a novel cost-sensitive active learning method">
                                        <b>[21]</b>
                                         DEMIR B,MINELLO L,BRUZZONE L.Definition of effective training sets for supervised classification of remote sensing images by a novel cost-sensitive active learning method [J].IEEE Transactions on Geoscience and Remote Sensing,2014,52(2):1272-1284.
                                    </a>
                                </li>
                                <li id="297">


                                    <a id="bibliography_22" title=" HUANG K-H,LIN H-T.A novel uncertainty sampling algorithm for cost-sensitive multiclass active learning [C]// ICDM 2016:Proceedings of the 2016 IEEE 16th International Conference on Data Ming.Piscataway,NJ:IEEE,2016:925-930." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel uncertainty sampling algorithm for cost-sensitive multiclass active learning">
                                        <b>[22]</b>
                                         HUANG K-H,LIN H-T.A novel uncertainty sampling algorithm for cost-sensitive multiclass active learning [C]// ICDM 2016:Proceedings of the 2016 IEEE 16th International Conference on Data Ming.Piscataway,NJ:IEEE,2016:925-930.
                                    </a>
                                </li>
                                <li id="299">


                                    <a id="bibliography_23" title=" BAHNSEN A C,AOUADA D,OTTERSTEN B.Example-dependent cost-sensitive logistic regression for credit scoring [C]// ICMLA 2014:Proceedings of the 2014 13th International Conference on Machine Learning and Application.Washington,DC:IEEE Computer Society,2014:263-269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Example-Dependent Cost-Sensitive Logistic Regression for Credit Scoring">
                                        <b>[23]</b>
                                         BAHNSEN A C,AOUADA D,OTTERSTEN B.Example-dependent cost-sensitive logistic regression for credit scoring [C]// ICMLA 2014:Proceedings of the 2014 13th International Conference on Machine Learning and Application.Washington,DC:IEEE Computer Society,2014:263-269.
                                    </a>
                                </li>
                                <li id="301">


                                    <a id="bibliography_24" title=" BAHNSEN A C,AOUADA D,OTTERSTEN B.Example-dependent cost-sensitive decision trees[J].Expert Systems with Applications,2015,42(19):6609-6619." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES120D7CC8DF6B69F1927FF7C0270AF733&amp;v=Mjk2ODhhQnVIWWZPR1FsZkJyTFUwNXRwaHhMaSt3SzA9TmlmT2ZiSzZIcVhMM1B4TkVKMEpmbm93dVJjYTZEZ0xQbmlSckJBeWVjUGlRcm1jQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         BAHNSEN A C,AOUADA D,OTTERSTEN B.Example-dependent cost-sensitive decision trees[J].Expert Systems with Applications,2015,42(19):6609-6619.
                                    </a>
                                </li>
                                <li id="303">


                                    <a id="bibliography_25" title=" BAHNSEN A C,AOUADA D,OTTERSTEN B.Ensemble of example-dependent cost-sensitive decision trees [EB/OL].[2018- 12- 13].https://arxiv.org/pdf/1505.04637v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble of example-dependent cost-sensitive decision trees">
                                        <b>[25]</b>
                                         BAHNSEN A C,AOUADA D,OTTERSTEN B.Ensemble of example-dependent cost-sensitive decision trees [EB/OL].[2018- 12- 13].https://arxiv.org/pdf/1505.04637v1.pdf.
                                    </a>
                                </li>
                                <li id="305">


                                    <a id="bibliography_26" title=" QUINLAN J R.Induction of decision trees [J].Machine Learning,1986,1(1):81-106." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001337810&amp;v=MDc2OTJySXhDYk9vUFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFU3L0pJRnM9Tmo3QmFyTzRIdEhO&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         QUINLAN J R.Induction of decision trees [J].Machine Learning,1986,1(1):81-106.
                                    </a>
                                </li>
                                <li id="307">


                                    <a id="bibliography_27" title=" LIAW A,WIENER M.Classification and regression by random forest [J].R News,2002,2/3:18-22." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification and regression by random forest">
                                        <b>[27]</b>
                                         LIAW A,WIENER M.Classification and regression by random forest [J].R News,2002,2/3:18-22.
                                    </a>
                                </li>
                                <li id="309">


                                    <a id="bibliography_28" title=" CRISTIANINI N,SHAWE T J.An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods [M].Cambridge,Eng.:Cambridge University Press,2000:46-71." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An introduction to support vector machines and other kernel-based learning methods">
                                        <b>[28]</b>
                                         CRISTIANINI N,SHAWE T J.An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods [M].Cambridge,Eng.:Cambridge University Press,2000:46-71.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-03 16:37</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2499-2504 DOI:10.11772/j.issn.1001-9081.2019020763            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于最远总距离采样的代价敏感主动学习</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">任杰</a>
                                <a href="javascript:;">闵帆</a>
                                <a href="javascript:;">汪敏</a>
                </h2>
                    <h2>

                    <span>西南石油大学计算机科学学院</span>
                    <span>西南石油大学电气信息学院</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>主动学习旨在通过人机交互减少专家标注,代价敏感主动学习则致力于平衡标注与误分类代价。基于三支决策(3WD)和标签均匀分布(LUD)模型,提出一种基于最远总距离采样的代价敏感主动学习算法(CAFS)。首先,设计了最远总距离采样策略,以查询代表性样本的标签;其次,利用了LUD模型和代价函数,计算期望采样数目;最后,使用了<i>k</i>-Means聚类技术分裂已获得不同标签的块。CAFS算法利用三支决策思想迭代地进行标签查询、实例预测和块分裂,直至处理完所有实例。学习过程在代价最小化目标的控制下进行。在9个公开数据上比较,CAFS比11个主流的算法具有更低的平均代价。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主动学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3Ek%3C%2Fi%3E-Means%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>k</i>-Means聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E7%AD%BE%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标签均匀分布;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E6%94%AF%E5%86%B3%E7%AD%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三支决策;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    任杰(1996—),男,山西忻州人,硕士研究生,主要研究方向:主动学习;;
                                </span>
                                <span>
                                    *闵帆(1973—),男,重庆人,教授,博士,CCF会员,主要研究方向:粒计算、推荐系统、主动学习;电子邮箱minfanphd@163.om;
                                </span>
                                <span>
                                    汪敏(1980—),女,湖南邵阳人,副教授,硕士,CCF会员,主要研究方向:数据挖掘、主动学习。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>四川省青年科技创新团队专项(2019JDTD0017);</span>
                                <span>四川省应用基础研究项目(2019JDTD0017);</span>
                    </p>
            </div>
                    <h1><b>Cost-sensitive active learning through farthest distance sum sampling</b></h1>
                    <h2>
                    <span>REN Jie</span>
                    <span>MIN Fan</span>
                    <span>WANG Min</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Southwest Petroleum University</span>
                    <span>School of Electrical Engineering and Information, Southwest Petroleum University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Active learning aims to reduce expert labeling through man-machine interaction, while cost-sensitive active learning focuses on balancing labeling and misclassification costs. Based on Three-Way Decision(3 WD) methodology and Label Uniform Distribution(LUD) model, a Cost-sensitive Active learning through the Farthest distance sum Sampling(CAFS) algorithm was proposed. Firstly, the farthest total distance sampling strategy was designed to query the labels of representative samples. Secondly, LUD model and cost function were used to calculate the expected sampling number. Finally, <i>k</i>-Means algorithm was employed to split blocks obtained different labels. In CAFS, 3 WD methodology was adopted in the iterative process of label query, instance prediction, and block splitting, until all instances were processed. The learning process was controlled by the cost minimization objective. Results on 9 public datasets show that CAFS has lower average cost compared with 11 mainstream algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=active%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">active learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3Ek%3C%2Fi%3E-Means%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>k</i>-Means clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=label%20uniform%20distribution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">label uniform distribution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Three-Way%20Decision(3WD)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Three-Way Decision(3WD);</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    REN Jie, born in 1996, M. S. candidate. His research interests include active learning. ;
                                </span>
                                <span>
                                    MIN Fan, born in 1973, Ph. D. , professor. His research interests include granular computing, recommender system, active learning. ;
                                </span>
                                <span>
                                    WANG Min, born in 1980, M. S. , associate professor. Her research interests include data mining, active learning.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-22</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Scientific Innovation Group for Youths of Sichuan Province(2019JDTD0017);</span>
                                <span>the Applied Basic Research Project of Sichuan Province(2017JY0190);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="69" name="69" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="70">主动学习<citation id="311" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是半监督学习<citation id="312" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>的一种方式,旨在通过人机交互减少专家标注的工作量。常用方法大致分为两类:基于聚类的方法选择具有代表性的对象,基于委员会的方法<citation id="313" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>选择不确定性高的对象。Cohn等<citation id="314" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了一种基于高斯模型和局部加权回归模型的主动学习算法,应用模型以及回归使主动学习所需的训练样本急剧减少。Wang等<citation id="315" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了基于密度峰值聚类的主动学习算法,在相同的训练样本基础上使得算法的分类精度进一步提高。目前主动学习已广泛应用于文本分类<citation id="316" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、信息提取<citation id="317" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、图像分类<citation id="318" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、语音识别<citation id="319" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等领域。</p>
                </div>
                <div class="p1">
                    <p id="71">代价敏感主动学习<citation id="320" type="reference"><link href="273" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>致力于平衡标注与误分类代价。教师代价是专家标注样本标签的代价,误分类代价是指将样本错误分类的代价。该问题比经典的主动学习更有实际意义,也更具一般性。Min等<citation id="321" type="reference"><link href="275" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>利用<i>k</i>最近邻(<i>k</i>-Nearest Neighbors, <i>k</i>NN)将总体根据代价分成3个部分,提出了基于<i>k</i>NN的三分代价敏感主动学习算法,该算法重复三分区过程从而减少了总代价;但该算法并未考虑块内采样数目。Wu等<citation id="322" type="reference"><link href="277" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>建立了标签均匀分布模型,在代价的基础上利用标签均匀分布(Label Uniform Distribution, LUD)模型计算每块内最优的采样数目,进一步降低了代价;但其采样策略没有考虑样本点的信息量,使得代价依然有可优化的空间。</p>
                </div>
                <div class="p1">
                    <p id="72">本文提出一种基于最远总距离采样的代价敏感主动学习算法(Cost-sensitive Active learning through the Farthest distance sum Sampling, CAFS)。该算法有如下特点:</p>
                </div>
                <div class="p1">
                    <p id="73">1)利用三支决策(Three-Way Decision, 3WD)的思想,使学习过程更加完善。算法迭代地进行标签查询、实例预测和块分裂,直至处理完所有实例。方案在查询过程中进行分类,不需要引入其他的分类器。</p>
                </div>
                <div class="p1">
                    <p id="74">2)提出了最远总距离策略以获得需查询标签的样本。针对随机采样采样的不足,该策略综合考虑了某块内已查询的所有样本和信息量,可获得更具代表性样本。</p>
                </div>
                <div class="p1">
                    <p id="75">3)采用LUD模型计算块内需要查询的样本数,并设置阈值,对过小的块进行总体查询,使得采样数目在此情况下达到最优。该模型对不同的数据集有较好的适用性。</p>
                </div>
                <div class="p1">
                    <p id="76">4)采用了高效的<i>k</i>-Means聚类算法。该算法使用距离函数表达对象的相似性,与最远总距离采样策略配合可以获得很好效果。</p>
                </div>
                <div class="p1">
                    <p id="77">本文在9个数据集上与11个主流算法进行了比较,结果表明,CAFS算法在平均代价方面优于对比算法。</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="79" name="79">1.1 <b>三支决策</b></h4>
                <div class="p1">
                    <p id="80">三支决策(3WD)<citation id="323" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>是一种符合人类认知的决策模式。它是实现二支决策的一个中间步骤,在实际决策的过程中,对于具有充分把握接受或拒绝的事物能够立即作出快速的判断,对于那些不能立即作出决策的事件,则进行延迟决策。三支决策是一种包含三个部分或三个操作的分治方法,也是决策理论粗糙集的延伸。</p>
                </div>
                <div class="p1">
                    <p id="81">很多理论和应用使用了三支决策的方法及思想。其中三支形式概念分析和三支认知计算衍生出了概念学习和多粒度认识操作。通过决策粗糙集理论和属性约简方法将三支决策理论粗糙集与代价敏感相结合<citation id="324" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,在样本上得出最优测试属性,并依据最优测试属性在测试集上计算,使得分类结果具有最小误分类代价和测试代价。基于三支决策的多粒度粗糙集理论<citation id="325" type="reference"><link href="283" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>通过分析三支决策与概率粗糙集、决策粗糙集间的关系以及在属性约简的相关知识,给出了在医学、工程方向的应用和三支决策未来的发展方向。三支邻域粗糙集模型<citation id="326" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>根据错误率和多粒度构建不同的邻域系统,证明了可变精度粗糙集和多粒度粗糙集是邻域系统粗糙集模型的特例。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">1.2 <b>代价敏感主动学习</b></h4>
                <div class="p1">
                    <p id="83">代价敏感主动学习在主动学习的基础上,考虑了在学习过程中的代价敏感性,为不同的类别提供了不同的代价权重以及教师代价,在代价函数的约束下进行学习。</p>
                </div>
                <div class="p1">
                    <p id="84">由于代价敏感学习更具实际意义,从而受到很多学者的关注,如文献<citation id="327" type="reference">[<a class="sup">10</a>]</citation>中引入了代价敏感主动学习,并提出在未标记数据下的分类概率和基于分类概率的抽样和决策。Settles等<citation id="328" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>分析了4个真实的文字和图像领域的教师代价,给出了某些具体领域的教师代价的特征。Liu等<citation id="329" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>将联系教师代价与距离,使代价敏感主动学习在空间数据上展开。Zhao等<citation id="330" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>通过优化两种代价处理不平衡URL检测任务的问题,使代价敏感主动学习在URL检测问题上优于一般检测学习算法。Chen等<citation id="331" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出了最大预期代价和代价加权边际最小策略,使多类代价敏感主动学习表现更加突出。Demir等<citation id="332" type="reference"><link href="295" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>通过在遥感图像分类中,使用成本函构建教师代价利用了遥感图像的特性,使代价的定义更为全面。Huang等<citation id="333" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>通过非度量多位缩放将代价信息嵌入到特殊隐藏空间中的距离中,从隐藏空间的距离定义样本的不确定性,使学习过程选择更有效的样本。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">1.3 <b>标签均匀分布模型</b></h4>
                <div class="p1">
                    <p id="86">目前,数据集中大量标签未知是造成多种学习任务结果不理想的重要原因之一,主动学习算法正是此类问题的合理解决方案。对于大量标签未知的数据,我们很迫切地需要知道数据的结构以及分布,所以很容易基于现实模型或者简单的理论分析来假设一种分布模型,应用数据本身的结构在满足任务目标的前提下降低学习过程中的代价。</p>
                </div>
                <div class="p1">
                    <p id="87">基于最远总距离采样的代价敏感主动学习CAFS算法应用简单的均匀分布统计模型,利用概率和均值估计二分类数据中的正反例的个数。同时为了减少总教师代价,在均匀分布的基础上,利用期望数目和代价函数计算最优采样数目。</p>
                </div>
                <div class="p1">
                    <p id="88">CAFS算法采用标签均匀分布模型,即在总体分布未知的情况下,假设二分类总体中抽到正反例的概率相同。其概率如下:</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mfrac></mrow></math></mathml>; ∀0≤<i>i</i>≤<i>n</i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="91">在标签均匀分布模型中,如果在总体<i>X</i>中随机选取<i>R</i>个正例和<i>B</i>个反例,那么在总体中有<i>R</i><sup>*</sup>个正例的概率则为:</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula"><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>b</mi><mo stretchy="true">¯</mo></mover><mo stretchy="false">(</mo><mrow><mrow><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow><mi>R</mi><mo>,</mo><mi>B</mi></mrow></math></mathml>;<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>A</mi><msubsup><mrow></mrow><mrow><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mi>R</mi></msubsup><mo>⋅</mo><mi>A</mi><msubsup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mi>B</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>R</mi></mrow><mi>n</mi></munderover><mi>A</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>R</mi></msubsup><mi>A</mi><msubsup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mi>i</mi></mrow><mi>B</mi></msubsup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="95">在上述假设以及概率公式成立的情况下,正反例在总体<i>X</i>中期望的数目为:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>b</mi><mo stretchy="true">¯</mo></mover><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>R</mi><mo>,</mo><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mover accent="true"><mi>r</mi><mo>¯</mo></mover><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>B</mi><mo>,</mo><mi>R</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>R</mi></mrow><mrow><mi>n</mi><mo>-</mo><mi>R</mi></mrow></munderover><mi>i</mi></mstyle><mi>A</mi><msubsup><mrow></mrow><mi>i</mi><mi>R</mi></msubsup><mi>A</mi><msubsup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mi>i</mi></mrow><mi>B</mi></msubsup></mrow><mrow><mi>n</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>R</mi></mrow><mrow><mi>n</mi><mo>-</mo><mi>R</mi></mrow></munderover><mi>A</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>R</mi></msubsup><mi>A</mi><msubsup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mi>i</mi></mrow><mi>B</mi></msubsup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">当在连续抽出正例或反例时候,出现另一个对立的实例对于期望的影响很大,有如下公式成立:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>r</mi><mo>¯</mo></mover><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>R</mi><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo><mo>&gt;</mo><mover accent="true"><mi>r</mi><mo>¯</mo></mover><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mn>2</mn><mi>R</mi><mo>-</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="99" name="99" class="anchor-tag">2 代价敏感主动学习问题描述</h3>
                <div class="p1">
                    <p id="100">为介绍CAFS算法,表1列出了本文使用的符号以及含义。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表</b>1 <b>符号以及含义</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 Symbols and meanings</p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td><br />变量符号</td><td>含义</td></tr><tr><td><br /><i>U</i></td><td>样本总体</td></tr><tr><td><br /><i>C</i></td><td>条件属性集</td></tr><tr><td><br /><i>d</i></td><td>决策属性</td></tr><tr><td><br /><i>V</i><sub><i>a</i></sub></td><td>属性<i>a</i>的值域,<i>a</i>∈<i>C</i>∪ <i>D</i></td></tr><tr><td><br /><i>V</i></td><td><i>V</i><sub><i>a</i></sub>的集合</td></tr><tr><td><br /><i>I</i></td><td>信息函数</td></tr><tr><td><br /><i>m</i></td><td>误分类代价</td></tr><tr><td><br /><i>t</i></td><td>教师代价</td></tr><tr><td><br /><i>x</i><sub><i>i</i></sub></td><td><i>U</i>中第<i>i</i>个实例</td></tr><tr><td><br /><i>y</i><sub><i>i</i></sub></td><td><i>x</i><sub><i>i</i></sub>的实际标签</td></tr><tr><td><br /><i>l</i><sub><i>i</i></sub></td><td><i>x</i><sub><i>i</i></sub>的预测标签</td></tr><tr><td><br /><i>SL</i><sub><i>i</i></sub></td><td>选择的代表点<i>i</i></td></tr><tr><td><br /><i>N</i></td><td><i>U</i>中的实例个数</td></tr><tr><td><br /><i>X</i></td><td><i>U</i>的一个子集</td></tr><tr><td><br /><i>n</i></td><td><i>X</i>中的实例个数</td></tr><tr><td><br /><i>R</i></td><td><i>X</i>中已经被查询过的正样本个数</td></tr><tr><td><br /><i>B</i></td><td><i>X</i>中已经被查询过的负样本个数</td></tr><tr><td><br /><i>A</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></td><td>从<i>i</i>个实例中取<i>j</i>个做排列数<i>A</i></td></tr><tr><td><br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup><mrow><mo>|</mo><mrow><mi>R</mi><mo>,</mo><mi>B</mi><mo>;</mo><mi>n</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></math></td><td>由<i>X</i>中抽出<i>R</i>个正例,<i>B</i>个反例,则<i>X</i>中恰好含有<i>R</i><sup>*</sup>个正例的概率</td></tr><tr><td><br /><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>r</mi><mo>¯</mo></mover></math>(<i>n</i>,<i>R</i>,<i>B</i>)</td><td><i>X</i>中正实例的期望比例</td></tr><tr><td><br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>b</mi><mo stretchy="true">¯</mo></mover><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>R</mi><mo>,</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></math></td><td><i>X</i>中负实例的期望比例</td></tr><tr><td><br /><i>σ</i>(<i>n</i>,<i>R</i>,<i>B</i>)</td><td>正实例比例的标准差</td></tr><tr><td><br /><i>f</i></td><td><i>X</i>中期望采样数目</td></tr><tr><td><br /><i>s</i></td><td><i>X</i>中最优采样数目</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">2.1 <b>数据模型</b></h4>
                <div class="p1">
                    <p id="103">CAFS算法使用如下数据模型。</p>
                </div>
                <div class="p1">
                    <p id="104"><b>定义</b>1 教师误分类代价敏感决策系统(Teacher-and-Misclassification-Cost-sensitive Decision System, TMC-DS),是七元组:</p>
                </div>
                <div class="p1">
                    <p id="105"><i>S</i>=(<i>U</i>,<i>C</i>,<i>d</i>,<i>V</i>,<i>I</i>,<i>m</i>,<i>t</i>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="106">其中:<i>U</i>是有限的实例集合,<i>C</i>是条件属性的集合,<i>d</i>是代价属性,<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∪</mo><mrow><mi>a</mi><mo>∈</mo><mi>C</mi><mstyle displaystyle="true"><mo>∪</mo><mo stretchy="false">{</mo></mstyle><mi>d</mi><mo stretchy="false">}</mo></mrow></munder><mi>V</mi></mstyle><msub><mrow></mrow><mrow><msub><mrow></mrow><mi>a</mi></msub></mrow></msub><mo>,</mo><mi>V</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></math></mathml>是属性<i>a</i>的属性值,<i>I</i>:<i>U</i>×(<i>C</i>∪{<i>d</i>})→ <i>V</i>是信息函数,<i>m</i>:<i>V</i><sub><i>d</i></sub>×<i>V</i><sub><i>d</i></sub> → <i>R</i><sup>+</sup>∪{0}是误分类代价函数,<i>t</i>∈<i>R</i><sup>+</sup>∪{0}是教师代价。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">2.2 <b>问题定义</b></h4>
                <div class="p1">
                    <p id="109">问题1 代价敏感主动学习。</p>
                </div>
                <div class="p1">
                    <p id="110">输入:一个代价敏感决策系统七元组TMC-DS;</p>
                </div>
                <div class="p1">
                    <p id="111">输出:专家查询的实例集合<i>U</i><sub><i>t</i></sub>,预测标签<i>l</i><sub><i>U</i>-<i>U</i><sub><i>t</i></sub></sub>。</p>
                </div>
                <div class="p1">
                    <p id="112">优化目标:<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mspace width="0.25em" /><mi>c</mi><mi>o</mi><mi>s</mi><mi>t</mi><mo>=</mo><mo stretchy="false">(</mo><mi>t</mi><mrow><mo>|</mo><mrow><mi>U</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>U</mi><mo>|</mo></mrow></mrow></munderover><mi>m</mi></mstyle><mo stretchy="false">(</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>/</mo><mrow><mo>|</mo><mi>U</mi><mo>|</mo></mrow></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="114">输入的是不含标记的代价敏感决策系统TMC-DS。输出包含两个部分:其一是实例子集<i>U</i><sub><i>t</i></sub>其中的标签是查询或者由专家给出;其二是剩余实例的预测标签<i>l</i><sub><i>U</i>-<i>U</i><sub><i>t</i></sub></sub>。</p>
                </div>
                <div class="p1">
                    <p id="115">优化目标是通过减少教师代价和误分类代价使平均代价达到最小,其中<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi><mo>×</mo><mrow><mo>|</mo><mrow><mi>U</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>是总教师代价,<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>U</mi><mo>|</mo></mrow></mrow></munderover><mi>m</mi></mstyle><mo stretchy="false">(</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>是总的误分类代价。其中教师代价和误分类代价是在获得<i>U</i><sub><i>t</i></sub>之后计算得到的,<i>U</i><sub><i>t</i></sub>并不是用户指定的。而随着<i>U</i><sub><i>t</i></sub>大小的增加,教师代价呈线性增长,误分类代价可能会减少,本文的CAFS算法找到了一个教师代价与误分类代价的相对平衡点。</p>
                </div>
                <h3 id="118" name="118" class="anchor-tag">3 CAFS算法</h3>
                <div class="p1">
                    <p id="119">本章将详细介绍CAFS算法的执行过程,其中包括CAFS算法总体流程、根据LUD模型以及代价函数计算出最优采样数目的lookup方法、根据最远总距离采样策略利用<i>k</i>-Means聚类对块进行分裂并迭代学习的splitAndLearn方法。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">3.1 <b>算法框架</b></h4>
                <div class="p1">
                    <p id="121">基于最远总距离采样的代价敏感主动学习CAFS的算法框架如算法1所示,其中第2)行是为了在块中寻找最远总距离的代表点,之后的步骤会确定当前块是否需要分块迭代学习。</p>
                </div>
                <div class="p1">
                    <p id="122">算法1 基于最远总距离采样的代价敏感主动学习算法(CAFS)。</p>
                </div>
                <div class="p1">
                    <p id="123">输入:样本总体<i>U</i>,算法2(lookup)最优采样数目<i>s</i>;</p>
                </div>
                <div class="p1">
                    <p id="124">输出:预测标签集合<i>l</i><sub><i>U</i>-<i>U</i><sub><i>t</i></sub></sub>。</p>
                </div>
                <div class="p1">
                    <p id="125">1)for(<i>x</i><sub><i>i</i></sub>∈<i>U</i> &amp;&amp; (<i>R</i> or <i>B</i>)&lt;<i>s</i>)</p>
                </div>
                <div class="p1">
                    <p id="127">2)<i>SL</i><sub><i>f</i></sub> ← findFarthest(<i>U</i><sub><i>t</i></sub>);  /*结合已经查询的实例结合中寻找最远距离点*/</p>
                </div>
                <div class="p1">
                    <p id="129">3)if(<i>y</i><sub><i>SLf</i></sub>== <i>y</i><sub>0</sub>)  /*判断最远的代表样本点是否与初始样本点的标签相同*/</p>
                </div>
                <div class="p1">
                    <p id="131">4)<i>U</i><sub><i>t</i></sub> ← <i>SL</i><sub><i>f</i></sub></p>
                </div>
                <div class="p1">
                    <p id="133">5)continue</p>
                </div>
                <div class="p1">
                    <p id="135">6)else</p>
                </div>
                <div class="p1">
                    <p id="137">7)splitAndLearn  /*分裂迭代学习算法3 */</p>
                </div>
                <div class="p1">
                    <p id="139">8)end if</p>
                </div>
                <div class="p1">
                    <p id="141">9)end for</p>
                </div>
                <div class="p1">
                    <p id="143">10)return <i>l</i><sub><i>U</i></sub>-<i>U</i><sub><i>t</i></sub></p>
                </div>
                <div class="p1">
                    <p id="145">算法2是CAFS算法中根据LUD模型计算要查询标签个数的lookup方法,其中<i>f</i>是根据LUD模型以及代价函数所确定的正反例期望查询数目,如式(6)所示:</p>
                </div>
                <div class="p1">
                    <p id="146" class="code-formula">
                        <mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><mo>=</mo></mtd></mtr><mtr><mtd><mspace width="0.25em" /><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>m</mi><mo stretchy="false">(</mo><mo>-</mo><mo>,</mo><mo>+</mo><mo stretchy="false">)</mo><mi>Ν</mi><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>r</mi><mo>¯</mo></mover><mo stretchy="false">(</mo><mi>Ν</mi><mo>,</mo><mi>R</mi><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>t</mi><mi>R</mi><mo>,</mo><mtext> </mtext><mtext>已</mtext><mtext>标</mtext><mtext>记</mtext><mtext>实</mtext><mtext>例</mtext><mtext>为</mtext><mtext>正</mtext><mtext>例</mtext></mtd></mtr><mtr><mtd><mi>m</mi><mo stretchy="false">(</mo><mo>+</mo><mo>,</mo><mo>-</mo><mo stretchy="false">)</mo><mi>Ν</mi><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>b</mi><mo stretchy="true">¯</mo></mover><mo stretchy="false">(</mo><mi>Ν</mi><mo>,</mo><mn>0</mn><mo>,</mo><mi>B</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>t</mi><mi>B</mi><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>已</mtext><mtext>标</mtext><mtext>记</mtext><mtext>实</mtext><mtext>例</mtext><mtext>为</mtext><mtext>反</mtext><mtext>例</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="147">算法2 最优标签查询数目计算算法(lookup)。</p>
                </div>
                <div class="p1">
                    <p id="148">输入:数据块的大小<i>n</i>,第一个抽出的样本标签<i>y</i><sub>0</sub>;</p>
                </div>
                <div class="p1">
                    <p id="149">输出:最优采样数<i>s</i>。</p>
                </div>
                <div class="p1">
                    <p id="150">1)for(<i>x</i><sub><i>i</i></sub>∈<i>X</i>)</p>
                </div>
                <div class="p1">
                    <p id="152">2)<i>SL</i><sub><i>f</i></sub> ← <i>bought</i><sub><i>i</i></sub>  /*记录已购买的标签*/</p>
                </div>
                <div class="p1">
                    <p id="154">3)<i>i</i>(<i>r</i><sup>*</sup>,<i>b</i><sup>*</sup>)← <i>f</i>  /*根据式(6)计算期望查询数目 */</p>
                </div>
                <div class="p1">
                    <p id="156">4)end for</p>
                </div>
                <div class="p1">
                    <p id="158">5)<i>i</i><sup>*</sup> ← lookup(<i>y</i><sub>0</sub>)</p>
                </div>
                <div class="p1">
                    <p id="160">6)<i>s</i> ← (<i>i</i><sup>*</sup>-<i>SL</i>.<i>legth</i>)</p>
                </div>
                <div class="p1">
                    <p id="162">7)return <i>s</i></p>
                </div>
                <div class="p1">
                    <p id="164">算法3介绍块分裂条件以及如何迭代学习的过程。在选取最远总距离代表点后,需要得知该代表点与之前查询的块标签是否一致。如果一致,继续利用最远距离采样策略采样直至达到最优采样数<i>s</i>,否则利用<i>k</i>-Means聚类算法分裂该块并迭代学习的过程。</p>
                </div>
                <div class="p1">
                    <p id="165">算法3 块分裂迭代学习算法(splitAndLearn)。</p>
                </div>
                <div class="p1">
                    <p id="166">输入:数据块<i>X</i>;</p>
                </div>
                <div class="p1">
                    <p id="167">输出:数据块的<i>X</i>的预测标签合集<i>l</i><sub><i>i</i>∈<i>X</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="168">1)if(<i>SL</i>.<i>length</i>&lt;<i>s</i>)</p>
                </div>
                <div class="p1">
                    <p id="170">2)<i>SL</i><sub>new</sub> ← findFarthest(<i>U</i><sub><i>t</i></sub>)</p>
                </div>
                <div class="p1">
                    <p id="172">3)if(<i>y</i><sub><i>SL</i></sub><sub>new</sub>≠<i>y</i><sub><i>X</i></sub><sub>0</sub>)</p>
                </div>
                <div class="p1">
                    <p id="174">4)<i>X</i><sub>1</sub>,<i>X</i><sub>2</sub> ← kMeansCluter(<i>X</i>)</p>
                </div>
                <div class="p1">
                    <p id="176">5)<i>l</i><sub><i>X</i></sub><sub>1</sub> ← CAFS(<i>X</i><sub>1</sub>)</p>
                </div>
                <div class="p1">
                    <p id="178">6)<i>l</i><sub><i>X</i></sub><sub>2</sub> ← CAFS(<i>X</i><sub>2</sub>)</p>
                </div>
                <div class="p1">
                    <p id="180">7)end if</p>
                </div>
                <div class="p1">
                    <p id="182">8)else</p>
                </div>
                <div class="p1">
                    <p id="184">9)end if</p>
                </div>
                <div class="p1">
                    <p id="186">10)return <i>l</i><sub><i>i</i></sub>∈<i>X</i></p>
                </div>
                <h4 class="anchor-tag" id="188" name="188">3.2 CAFS<b>时间复杂度分析</b></h4>
                <div class="p1">
                    <p id="189">基于最远总距离采样的代价敏感主动学习算法(CAFS)的时间复杂度如表2所示。</p>
                </div>
                <div class="area_img" id="190">
                    <p class="img_tit"><b>表</b>2 CAFS<b>时间复杂度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Time complexity of CAFS</p>
                    <p class="img_note"></p>
                    <table id="190" border="1"><tr><td><br />步骤描述</td><td>时间复杂度</td></tr><tr><td><br />计算最优代表点个数</td><td><i>O</i>(<i>n</i> log <i>n</i>)</td></tr><tr><td><br />寻找代表点</td><td><i>Θ</i>(<i>n</i> log <i>n</i>)</td></tr><tr><td><br />分裂数据块</td><td><i>O</i>(<i>n</i>)</td></tr><tr><td><br />预测其他实例标签</td><td><i>Θ</i>(<i>n</i>)</td></tr><tr><td><br />扫描数据</td><td><i>Θ</i>(<i>n</i>)</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="191">在实际算法过程中,时间复杂度会随着数据集变化而变化。在最优情况下,数据集中的实例个数趋于无穷时且为同一标签时,时间复杂度为<i>Ο</i>(<i>n</i> log <i>n</i>)。在最坏的情况下,且不同标签数据分布极为密集时,算法需要递归循环log <i>n</i>次。即:</p>
                </div>
                <div class="p1">
                    <p id="192">log <i>n</i>×(<i>Ο</i>(<i>n</i> log <i>n</i>)+<i>Θ</i>(<i>n</i> log <i>n</i>)+<i>Ο</i>(<i>n</i>))=<i>Ο</i>(<i>n</i><sup>2</sup>)</p>
                </div>
                <h3 id="193" name="193" class="anchor-tag">4 CAFS运行实例</h3>
                <div class="p1">
                    <p id="194">CAFS算法首先扫描块内已经标注的实例,查看该块是否需要分裂,之后计算需要查询的个数,以最远总距离采样策略选取代表点并查询标签,在满足最优采样数目<i>s</i>后预测其他未标记数据。为了更好地展示CAFS算法的学习过程,以下将利用小型的数据集描述CAFS算法的学习过程。</p>
                </div>
                <div class="p1">
                    <p id="195">如图1运行实例,首先,对数据进行初始化图1(a),并记录数据中的第一个实例,然后如图1(b)采用最远总距离样本采样策略选取代表性样本点查询。如图1(c),很明显两个实例的标签不同,此时利用<i>k</i>-Means聚类对数据块进行分裂处理。对分裂后的数据块采样同样的策略迭代学习,如图1(d) (e)。由于本次运行实例采用极具代表性的数据,所以在数据块的最优查询数目<i>s</i>的前提下两个块中的样本标签相同,根据CAFS算法的策略,如图1(f)会对剩余的实例进行预测,即完成本次过程。</p>
                </div>
                <div class="area_img" id="196">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909004_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 运行实例示意图" src="Detail/GetImg?filename=images/JSJY201909004_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 运行实例示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909004_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Running example</p>

                </div>
                <h3 id="197" name="197" class="anchor-tag">5 实验与结果分析</h3>
                <div class="p1">
                    <p id="198">实验运行在64位16 GB RAM的Windows10的个人电脑上,其中处理器为i7-7700HQ 2.80 GHz,并利用Java在Eclipse上实现。</p>
                </div>
                <h4 class="anchor-tag" id="199" name="199">5.1 <b>实验数据集</b></h4>
                <div class="p1">
                    <p id="200">实验数据集来源于UCI机器学习仓库和IDA基准仓库,表3列出了数据集的基本信息,这些数据集一部分是人造数据集,大部分来源于现实生活,涵盖了生物学、金融学、计算机、通信、植物学、医疗和质谱分析等领域。</p>
                </div>
                <div class="p1">
                    <p id="201">实验选取11个相关的算法进行了对比,并根据CAFS算法特点分成了三组进行了相关实验:1)与同类的代价敏感学习算法进行对比;2)与代价敏感的主动学习算法进行对比;3)代价敏感学习与非代价敏感学习算法对比,而且为了将非代价敏感学习与代价敏感学习进行代价方面的对比,利用实验中的代价误分类代价设置,将非代价敏感学习的结果统一成代价进行比较。</p>
                </div>
                <div class="area_img" id="202">
                    <p class="img_tit"><b>表</b>3 <b>数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Dataset information</p>
                    <p class="img_note"></p>
                    <table id="202" border="1"><tr><td><br />编号</td><td>名称</td><td>实例个数</td><td>实例维度</td><td>来源</td></tr><tr><td><br />1</td><td>Allmal</td><td>72</td><td>7 129</td><td>生物学</td></tr><tr><td><br />2</td><td>Arcene</td><td>200</td><td>1 000</td><td>质谱</td></tr><tr><td><br />3</td><td>Banana</td><td>5 300</td><td>2</td><td>植物学</td></tr><tr><td><br />4</td><td>Credit6000</td><td>5 987</td><td>65</td><td>金融学</td></tr><tr><td><br />5</td><td>Heart</td><td>270</td><td>13</td><td>医疗</td></tr><tr><td><br />6</td><td>Ionosphere</td><td>351</td><td>34</td><td>物理学</td></tr><tr><td><br />7</td><td>Madelon</td><td>2 600</td><td>500</td><td>人造</td></tr><tr><td><br />8</td><td>Sonar</td><td>208</td><td>60</td><td>通信</td></tr><tr><td><br />9</td><td>Spambase</td><td>4 207</td><td>57</td><td>计算机</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="203" name="203">5.2 <b>实验代价设置</b></h4>
                <div class="p1">
                    <p id="204"><i>m</i>表示误分类代价矩阵,<i>m</i>(+,-)=4表示将正例预测成反例的代价为4,<i>m</i>(-,+)=2表示将反例预测成正例的代价为2。另外设置<i>t</i>=1是指查询一个实例的教师代价是1。实验中的平均代价计算公式则为:</p>
                </div>
                <div class="p1">
                    <p id="205"><i>AverageCost</i>=(<i>M</i><sub>1</sub><i>m</i>(+,-)+<i>M</i><sub>2</sub><i>m</i>(-,+)+<i>tT</i>)/<i>n</i>      (7)</p>
                </div>
                <div class="p1">
                    <p id="206">其中:<i>M</i><sub>1</sub>实验结果中将正例预测成反例的个数,<i>M</i><sub>2</sub>为将反例预测成正例的个数,<i>T</i>为向专家查询实例的个数。</p>
                </div>
                <h4 class="anchor-tag" id="207" name="207">5.3 <b>与代价敏感学习算法的对比实验</b></h4>
                <div class="p1">
                    <p id="208">本节将CAFS算法与代价敏感逻辑回归算法(Cost Sensitive Logistic Regression algorithm, CSLR)<citation id="334" type="reference"><link href="299" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、代价敏感决策树算法(Cost Sensitive Decision Tree algorithm, CSDT)<citation id="335" type="reference"><link href="301" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>和代价敏感随机森林算法(Cost Sensitive Random Forest algorithm, CSRF)<citation id="336" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>在9个公开数据集上进行了对比,并以平均代价(根据式(7)计算)为唯一参照,结果如表4所示。其中“—”表示CSLR在Arcene数据集上运行超过5 h也没有产生结果;平均排名则指算法在所有数据集上表现排名的均值。从表4中看出,CAFS的平均代价相对于CSLR、CSDT、CSRF分别降低了56%、27%、32%。</p>
                </div>
                <div class="area_img" id="209">
                    <p class="img_tit"><b>表</b>4 CAFS<b>算法与其他代价敏感学习算法在不同数据集上的平均代价</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Comparison of average cost of CAFS algorithm and other cost-sensitive learning algorithms on different datasets</p>
                    <p class="img_note"></p>
                    <table id="209" border="1"><tr><td>算法</td><td>Allmal</td><td>Arcene</td><td>Banana</td><td>Credit6000</td><td>Heart</td><td>Ionosphere</td><td>Madelon</td><td>Sonar</td><td>Spambase</td><td>平均排名</td></tr><tr><td>CSLR</td><td>1.194 4</td><td>—</td><td>1.837 7</td><td>1.635 9</td><td>1.240 7</td><td>0.884 3</td><td>1.752 3</td><td>1.364 4</td><td>1.646 3</td><td>3.888 9</td></tr><tr><td><br />CSDT</td><td>1.138 9</td><td>1.035 0</td><td>0.716 3</td><td>0.592 7</td><td>0.959 3</td><td>0.607 4</td><td>1.307 7</td><td>1.064 4</td><td>0.483 8</td><td>2.666 7</td></tr><tr><td><br />CSRF</td><td>1.416 7</td><td>1.015 0</td><td>1.555 9</td><td>0.510 0</td><td>0.889 6</td><td>0.543 6</td><td>1.199 2</td><td>1.037 5</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>4</mn><mn>2</mn><mtext> </mtext><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>2.222 2</td></tr><tr><td><br />CAFS</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>7</mn><mn>5</mn><mn>0</mn><mtext> </mtext><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>7</mn><mn>0</mn><mn>5</mn><mtext> </mtext><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>4</mn><mn>2</mn><mn>2</mn><mtext> </mtext><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>4</mn><mn>8</mn><mn>0</mn><mtext> </mtext><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>2</mn><mn>5</mn><mtext> </mtext><mn>9</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>5</mn><mn>1</mn><mn>2</mn><mtext> </mtext><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>8</mn><mn>4</mn><mn>8</mn><mtext> </mtext><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>8</mn><mn>2</mn><mtext> </mtext><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.688 1</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mo>.</mo><mn>2</mn><mn>2</mn><mn>2</mn><mtext> </mtext><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="210" name="210">5.4 <b>与其他代价敏感主动学习算法的对比</b></h4>
                <div class="p1">
                    <p id="211">本节实验选取了5个代价敏感主动学习算法进行比较。其中:ALCE(Active Learning Embed Cost algorithm)<citation id="337" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>为代价嵌入主动学习算法,CWMM(Cost Weight Minimum Margin algorithm)为代价权重最小边缘算法,MEC(Maximum Expected Cost algorithm)为最大期望代价算法,TALK(Tri-partition Active Learning through <i>K</i>-nearest neighbors algorithm)为基于<i>k</i>近邻的三支决策主动学习算法,CADU(Cost-sensitive Active learning algorithm with a label Uniform Distribution model)为基于密度聚类的代价敏感主动学习算法。</p>
                </div>
                <div class="p1">
                    <p id="212">对ALCE、CWMM和MEC进行了5次重复实验,以保证实验结果的准确性;而且由于数据顺序不影响TALK、CADU和CAFS的结果,即实验的结果稳定,所以只进行1次实验。其中CAFS和CADU不需要已经标记的初始训练集;而且采样数目是CWMM和MEC的参数,为了保证实验结果的有效性,将采样数目设置为CAFS、TALK CADU的计算值。</p>
                </div>
                <div class="p1">
                    <p id="213">表5显示在9个数据集上,CAFS在大部分数据集上表现优异,其中平均代价相对于ALCE,CWMM,MEC,TALK,CADU算法分别降低了30%、37%、35%、27%、10%,在平均排名上也取得了最好的成绩。</p>
                </div>
                <div class="area_img" id="214">
                    <p class="img_tit"><b>表</b>5 CAFS<b>算法与其他代价敏感主动学习算法在不同数据集上的平均代价对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Comparison of average cost of CAFS algorithm and other cost-sensitive active learning algorithms on different datasets</p>
                    <p class="img_note"></p>
                    <table id="214" border="1"><tr><td>算法</td><td>Allmal</td><td>Arcene</td><td>Banana</td><td>Credit6000</td><td>Heart</td><td>Ionosphere</td><td>Madelon</td><td>Sonar</td><td>Spambase</td><td>平均排名</td></tr><tr><td>ALCE</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>9</mn><mn>4</mn><mtext> </mtext><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.925 0</td><td>0.454 6</td><td>0.648 6</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>5</mn><mn>5</mn><mn>3</mn><mtext> </mtext><mn>3</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>1.407 3</td><td>1.463 4</td><td>1.122 1</td><td>0.987 7</td><td>3.222 2</td></tr><tr><td><br />CWMM</td><td>1.911 1</td><td>0.925 0</td><td>0.631 4</td><td>0.664 9</td><td>0.665 9</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>5</mn><mn>0</mn><mn>9</mn><mtext> </mtext><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>1.463 4</td><td>1.299 0</td><td>1.088 0</td><td>4.444 4</td></tr><tr><td><br />MEC</td><td>1.511 1</td><td>0.925 0</td><td>0.673 5</td><td>0.655 8</td><td>0.569 5</td><td>0.654 1</td><td>1.463 4</td><td>1.270 2</td><td>1.089 7</td><td>4.222 2</td></tr><tr><td><br />TALK</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>9</mn><mn>4</mn><mtext> </mtext><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>1.120 0</td><td>0.896 6</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>2</mn><mn>0</mn><mtext> </mtext><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.888 9</td><td>1.282 1</td><td>1.000 0</td><td>0.932 7</td><td>0.798 2</td><td>3.888 9</td></tr><tr><td><br />CADU</td><td>0.833 3</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>8</mn><mn>5</mn><mtext> </mtext><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>3</mn><mn>0</mn><mtext> </mtext><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.415 4</td><td>0.663 0</td><td>0.774 9</td><td>0.977 7</td><td>0.908 7</td><td>0.766 6</td><td>2.444 4</td></tr><tr><td><br />CAFS</td><td>0.750 0</td><td>0.705 0</td><td>0.422 6</td><td>0.480 8</td><td>0.625 9</td><td>0.512 8</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>8</mn><mn>4</mn><mn>8</mn><mtext> </mtext><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>8</mn><mn>2</mn><mtext> </mtext><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>8</mn><mn>8</mn><mtext> </mtext><mn>1</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>2</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>0</mn><mtext> </mtext><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="215" name="215">5.5 <b>与非代价敏感学习算法的对比实验</b></h4>
                <div class="p1">
                    <p id="216">最后,为了实验的完整性,CAFS与3个非代价敏感学习算法——投票熵采样算法(Vote Entropy Sampling algorithm, VES)、一致熵采样算法(Consensus Entropy Sampling algorithm, CES)和最大分歧采样算法(Max Disagreement Sampling algorithm, MDS)进行对比。三种算法选取了不同的采样方案,并且有3个基本分类器组成,分别是决策树<citation id="338" type="reference"><link href="305" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、随机森林<citation id="339" type="reference"><link href="307" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>和带径向基函数(Radial Basis Function, RBF)内核的支持向量机(Support Vector Machine, SVM)<citation id="340" type="reference"><link href="309" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>。因为某些算法在单次实验中会有结果的偏差,所以进行了5次实验。实验结果如表6所示,由于非代价敏感学习算法不考虑代价因素,所以在为保证实验结果的统一性,计算平均代价时会根据学习结果与代价设置进行代价计算。由表6可以看出,CAFS算法在4个算法中平均排名最好,并且平均代价对应于VES、CES、MDS算法分别降低了13.8%、14.34%、19.67%。</p>
                </div>
                <div class="area_img" id="217">
                    <p class="img_tit"><b>表</b>6 CAFS<b>算法与其他非代价敏感学习算法在不同数据集上的平均代价的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 6 Comparison of average cost of CAFS algorithm and other cost-insensitive active learning algorithms on different datasets</p>
                    <p class="img_note"></p>
                    <table id="217" border="1"><tr><td>算法</td><td>Allmal</td><td>Arcene</td><td>Banana</td><td>Credit6000</td><td>Heart</td><td>Ionosphere</td><td>Madelon</td><td>Sonar</td><td>Spambase</td><td>平均排名</td></tr><tr><td>VES</td><td>1.033 3</td><td>1.097 0</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>9</mn><mn>9</mn><mtext> </mtext><mn>9</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.590 0</td><td>0.689 6</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>4</mn><mn>1</mn><mn>4</mn><mtext> </mtext><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>1.078 2</td><td>0.895 2</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>9</mn><mn>4</mn><mtext> </mtext><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>2.000 0</td></tr><tr><td><br />CES</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>7</mn><mn>4</mn><mn>4</mn><mtext> </mtext><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.999 0</td><td>0.583 8</td><td>0.618 2</td><td>0.719 3</td><td>0.514 0</td><td>1.125 5</td><td>0.945 2</td><td>0.425 3</td><td>2.888 9</td></tr><tr><td><br />MDS</td><td>1.150 0</td><td>1.037 0</td><td>0.447 8</td><td>0.601 6</td><td>0.784 4</td><td>0.547 0</td><td>1.170 2</td><td>0.918 3</td><td>0.460 6</td><td>4.000 0</td></tr><tr><td><br />CAFS</td><td>0.750 0</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>7</mn><mn>0</mn><mn>5</mn><mtext> </mtext><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.422 6</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>4</mn><mn>8</mn><mn>0</mn><mtext> </mtext><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>2</mn><mn>5</mn><mtext> </mtext><mn>9</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.512 8</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>8</mn><mn>4</mn><mn>8</mn><mtext> </mtext><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>8</mn><mn>2</mn><mtext> </mtext><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.688 1</td><td>1.666 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="218" name="218">5.6 <b>实验结果分析</b></h4>
                <div class="p1">
                    <p id="219">综合以上实验结果,有如下结论:</p>
                </div>
                <div class="p1">
                    <p id="220">1)CAFS算法与主流的代价敏感学习CSLR、CSDT和CSRF相比,平均代价是最低的。</p>
                </div>
                <div class="p1">
                    <p id="221">2)CAFS与同类的代价敏感主动学习算法CWMM、MEC、TALK和CADU相比,实验结果是最优的。</p>
                </div>
                <div class="p1">
                    <p id="222">实验结果表明CAFS算法能够有效地降低总代价。</p>
                </div>
                <h3 id="223" name="223" class="anchor-tag">6 结语</h3>
                <div class="p1">
                    <p id="224">本文提出的基于最远总距离采样的主动学习算法,建立了LUD模型,并提出了最远总距离采样的策略。利用3WD思想使得学习的过程更加完善。标签均匀分布模型在给定的代价以及假设的均匀分布的条件下,可获得最优的采样数目。最远总距离采样策略,综合考虑了信息量和样本的总体特性,使得选择的样本更具代表性。下一步的主要工作包含两个方面:其一是将LUD模型推广到多类别的学习任务中;其二是设计更加合适的样本采样策略,进一步减小算法的代价,提高预测精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="255">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active Learning">

                                <b>[1]</b> SETTLES B.Active Learning [M].San Rafael,CA:Morgan and Claypool Publishers,2012:1-114.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Introduction to Semi-Supervised Learning">

                                <b>[2]</b> ZHU X,GOLDBERG A B.Introduction to Semi-Supervised Learning [M].San Rafael,CA:Morgan and Claypool Publishers,2009:130.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Query by committee">

                                <b>[3]</b> SEUNG H S,OPPER M,SOMPOLINSKY H.Query by committee [C]// COLT 1992:Proceedings of the 5th Annual ACM Conference on Computational Learning Theory.New York:ACM,1992:287-294.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active Learning with Statistical Models">

                                <b>[4]</b> COHN D A,GHAHRAMANI Z,JORDAN M I,et al.Active learning with statistical models [J].Journal of Artificial Intelligence Research,1996,4(1):129-145.
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES63F8583EB1CA3324E0C158FE506680DC&amp;v=MTUyMjVjN3NDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TGkrd0swPU5pZk9mYlc3YU5uSnA0d3dGdXA4Zlg4NnpSSm02a3g4VFhlVTJSYzFmN1NjUg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> WANG M,MIN F,ZHANG Z H,et al.Active learning through density clustering [J].Expert Systems with Applications,2017,85:305-317.
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support vector machine active learning with applications to text classification">

                                <b>[6]</b> TONG S,KOLLER D.Support vector machine active learning with applications to text classification [J].Journal of Machine Learning Research,2001,2(1):45-66.
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active learning for natural language parsing and information extraction">

                                <b>[7]</b> THOMPSON C A.Active learning for natural language parsing and information extraction[C]// ICML 1999:Proceeding of the 16th International Conference on Machine Learning.San Francisco,CA:Morgan Kaufmann Publishers,1999:406-414.
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An active learning framework for content based information retrieval">

                                <b>[8]</b> ZHANG C,CHEN T.An active learning framework for content-based information retrieval [J].IEEE Transactions on Multimedia,2002,4(2):260-268.
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100176131&amp;v=MTA0MDFoYz1OaWZPZmJLN0h0RE9ybzlGWmV3SkRYODRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJRjRWYQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> YU D,VARADARAJAN B,DENG L,et al.Active learning and semi-supervised learning for speech recognition:a unified framework using the global entropy reduction maximization criterion [J].Computer Speech and Language,2010,24(3):433-444.
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active cost-sensitive learning">

                                <b>[10]</b> MARGINEANTU D D.Active cost-sensitive learning [C]// IJCAI 2005:Proceedings of the 19th International Joint Conference on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann Publishers,2005:1622-1623.
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tri-partition cost-sensitive active learning through kNN">

                                <b>[11]</b> MIN F,LIU F L,WEN L Y,et al.Tri-partition cost-sensitive active learning through kNN [J].Soft Computing,2017,23(5):1557-1572.
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE7408523568E65361FD5DA88912D8F9E&amp;v=MDE5MzZmY2EvR3RIRXFvMUdZZTBIZVhvOHpCQVNuRXQ0UEE3cXBCczBlOGFjTTdQcUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMaSt3SzA9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> WU Y X,MIN X Y,MIN F,et al.Cost-sensitive active learning with a label uniform distribution model [J].International Journal of Approximate Reasoning,2019,105:49-65.
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Three-way decisions:an interpretation of rules in rough set theory">

                                <b>[13]</b> YAO Y.Three-way decision:an interpretation of rules in rough set theory [C]// Proceedings of the 2009 International Conference on Rough Sets and Knowledge Technology,LNCS 5589.Berlin:Springer,2009:642-649.
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXTS201302005&amp;v=MjEzNzVGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFVyL05MalhmZmJHNEg5TE1yWTk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 李华雄,周献中,黄兵,等.决策粗糙集与代价敏感分类[J].计算机科学与探索,2013,7(2):126-135.(LI H X,ZHOU X Z,HUANG B,et al.Decision-theoretic rough set and cost-sensitive classification [J].Journal of Frontiers of Computer Science and Technology,2013,7(2):126-135.)
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJDZ201305006&amp;v=MjUwMDNVUjdxZlp1WnNGeWpsVXIvTkt5ZlBkTEc0SDlMTXFvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 刘盾,李天瑞,李华雄.粗糙集理论:基于三支决策视角[J].南京大学学报(自然科学版),2013,49(5):574-581.(LIU D,LI T R,LI H X.Rough set theory:a three-way decisions perspective [J].Journal of Nanjing University (Natural Science),2013,49(5):574-581)
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJLG201202020&amp;v=MDczNDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFVyL05LeWZIYWJHNEg5UE1yWTlIWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 杨习贝,杨静宇.邻域系统粗糙集模型[J].南京理工大学报,2012,36(2):291-295.(YANG X B,YANG J Y.Rough set model based on neighborhood system [J].Journal of Nanjing University of Science and Technology,2012,36(2):291-295.)
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active learning with real annotation costs">

                                <b>[17]</b> SETTLES B,CRAVEN M,Friedland L.Active learning with real annotation costs [EB/OL].[2018- 12- 13].https://www.researchgate.net/publication/228770726_Active_learning_with_real_annotation_costs.
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatially cost-sensitive active learning">

                                <b>[18]</b> LIU A,JUN G,GHOSH J.Spatially cost-sensitive active learning [C]// SDM 2009:Proceedings of the 2009 SIAM International Conference on Data Mining.Philadelphia,PA:SIAM,2009:814-825.
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cost-sensitive online active learning with application to malicious URL detection">

                                <b>[19]</b> ZHAO P L,HOI S C H.Cost-sensitive online active learning with application to malicious URL detection [C]// KDD 2013:Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York:ACM,2013:919-927.
                            </a>
                        </p>
                        <p id="293">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active learning for multiclass cost-sensitive classification using probabilistic models">

                                <b>[20]</b> CHEN P-L,LIN H-T.Active learning for multiclass cost-sensitive classification using probabilistic models [C]// TAAI 2013:Proceedings of the 2013 Conference on Technologies and Applications of Artificial Intelligence.Washington,DC:IEEE Computer Society,2013:13-18.
                            </a>
                        </p>
                        <p id="295">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Definition of effective training sets for supervised classification of remote sensing images by a novel cost-sensitive active learning method">

                                <b>[21]</b> DEMIR B,MINELLO L,BRUZZONE L.Definition of effective training sets for supervised classification of remote sensing images by a novel cost-sensitive active learning method [J].IEEE Transactions on Geoscience and Remote Sensing,2014,52(2):1272-1284.
                            </a>
                        </p>
                        <p id="297">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel uncertainty sampling algorithm for cost-sensitive multiclass active learning">

                                <b>[22]</b> HUANG K-H,LIN H-T.A novel uncertainty sampling algorithm for cost-sensitive multiclass active learning [C]// ICDM 2016:Proceedings of the 2016 IEEE 16th International Conference on Data Ming.Piscataway,NJ:IEEE,2016:925-930.
                            </a>
                        </p>
                        <p id="299">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Example-Dependent Cost-Sensitive Logistic Regression for Credit Scoring">

                                <b>[23]</b> BAHNSEN A C,AOUADA D,OTTERSTEN B.Example-dependent cost-sensitive logistic regression for credit scoring [C]// ICMLA 2014:Proceedings of the 2014 13th International Conference on Machine Learning and Application.Washington,DC:IEEE Computer Society,2014:263-269.
                            </a>
                        </p>
                        <p id="301">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES120D7CC8DF6B69F1927FF7C0270AF733&amp;v=MTg2MjRlY1BpUXJtY0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMaSt3SzA9TmlmT2ZiSzZIcVhMM1B4TkVKMEpmbm93dVJjYTZEZ0xQbmlSckJBeQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> BAHNSEN A C,AOUADA D,OTTERSTEN B.Example-dependent cost-sensitive decision trees[J].Expert Systems with Applications,2015,42(19):6609-6619.
                            </a>
                        </p>
                        <p id="303">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble of example-dependent cost-sensitive decision trees">

                                <b>[25]</b> BAHNSEN A C,AOUADA D,OTTERSTEN B.Ensemble of example-dependent cost-sensitive decision trees [EB/OL].[2018- 12- 13].https://arxiv.org/pdf/1505.04637v1.pdf.
                            </a>
                        </p>
                        <p id="305">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001337810&amp;v=MTc0NTM3cWRaK1p1RmlybFU3L0pJRnM9Tmo3QmFyTzRIdEhOckl4Q2JPb1BZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdS&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> QUINLAN J R.Induction of decision trees [J].Machine Learning,1986,1(1):81-106.
                            </a>
                        </p>
                        <p id="307">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification and regression by random forest">

                                <b>[27]</b> LIAW A,WIENER M.Classification and regression by random forest [J].R News,2002,2/3:18-22.
                            </a>
                        </p>
                        <p id="309">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An introduction to support vector machines and other kernel-based learning methods">

                                <b>[28]</b> CRISTIANINI N,SHAWE T J.An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods [M].Cambridge,Eng.:Cambridge University Press,2000:46-71.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909004&amp;v=MTA0NzlHRnJDVVI3cWZadVpzRnlqbFVyL05MejdCZDdHNEg5ak1wbzlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
