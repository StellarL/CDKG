<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136463611221250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201910043%26RESULT%3d1%26SIGN%3dbbRK95A83bKPV79pAAaAX1tee2g%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910043&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201910043&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910043&amp;v=MTIyMDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1Y3dlBMejdCZDdHNEg5ak5yNDlCWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 图像配准 ">1 图像配准</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="2 最优拼接缝的自适应消除图像融合 ">2 最优拼接缝的自适应消除图像融合</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="3 图像矫直 ">3 图像矫直</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#117" data-title="4 实验与结果分析 ">4 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#118" data-title="4.1 &lt;b&gt;主观评价&lt;/b&gt;">4.1 <b>主观评价</b></a></li>
                                                <li><a href="#129" data-title="4.2 &lt;b&gt;实验参数及客观评价&lt;/b&gt;">4.2 <b>实验参数及客观评价</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#148" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="图1 序列图像提取特征点">图1 序列图像提取特征点</a></li>
                                                <li><a href="#61" data-title="图2 改进SIFT特征提取">图2 改进SIFT特征提取</a></li>
                                                <li><a href="#65" data-title="图3 图像配准结果">图3 图像配准结果</a></li>
                                                <li><a href="#81" data-title="图4 像素点能量值的占比参数的自适应关系">图4 像素点能量值的占比参数的自适应关系</a></li>
                                                <li><a href="#89" data-title="图5 权值图像&lt;i&gt;I&lt;/i&gt;&lt;sub&gt;&lt;i&gt;L&lt;/i&gt;&lt;/sub&gt;">图5 权值图像<i>I</i><sub><i>L</i></sub></a></li>
                                                <li><a href="#91" data-title="图6 图2(a)和图2(b)的融合结果">图6 图2(a)和图2(b)的融合结果</a></li>
                                                <li><a href="#96" data-title="图7 全景图边缘提取">图7 全景图边缘提取</a></li>
                                                <li><a href="#102" data-title="图8 全景图矫直">图8 全景图矫直</a></li>
                                                <li><a href="#116" data-title="图9 算法流程">图9 算法流程</a></li>
                                                <li><a href="#122" data-title="图10 两幅原始图像序列">图10 两幅原始图像序列</a></li>
                                                <li><a href="#123" data-title="图11 对图10的拼接算法结果对比">图11 对图10的拼接算法结果对比</a></li>
                                                <li><a href="#124" data-title="图12 10幅原始图像序列">图12 10幅原始图像序列</a></li>
                                                <li><a href="#125" data-title="图13 对图12的拼接算法结果对比">图13 对图12的拼接算法结果对比</a></li>
                                                <li><a href="#126" data-title="图14 16幅原始图像序列">图14 16幅原始图像序列</a></li>
                                                <li><a href="#127" data-title="图15 对图14的拼接算法结果对比">图15 对图14的拼接算法结果对比</a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;参数&lt;/b&gt;&lt;i&gt;k&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;、&lt;i&gt;k&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;b&gt;取值对应的图像清晰度对比&lt;/b&gt;"><b>表</b>1 <b>参数</b><i>k</i><sub>1</sub>、<i>k</i><sub>2</sub><b>取值对应的图像清晰度对比</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;全景图清晰度及质量评价指标对比&lt;/b&gt;"><b>表</b>2 <b>全景图清晰度及质量评价指标对比</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;图&lt;/b&gt;12&lt;b&gt;图像提取特征点实验数据对比&lt;/b&gt;"><b>表</b>3 <b>图</b>12<b>图像提取特征点实验数据对比</b></a></li>
                                                <li><a href="#147" data-title="图16 算法时间对比">图16 算法时间对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="173">


                                    <a id="bibliography_1" title="ALCANTARILLA P F,NUEVO J,BARTOLI A.Fast explicit diffusion for accelerated features in nonlinear scale spaces[C]//Proceedings of the 24th British Machine Vision Conference.Durham:BMVA Press,2013:1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast explicit diffusion for accelerated features in nonlinear scale spaces">
                                        <b>[1]</b>
                                        ALCANTARILLA P F,NUEVO J,BARTOLI A.Fast explicit diffusion for accelerated features in nonlinear scale spaces[C]//Proceedings of the 24th British Machine Vision Conference.Durham:BMVA Press,2013:1-11.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_2" title="郑悦,程红,孙文邦.邻域最短距离法寻找最佳拼接缝[J].中国图象图形学报,2014,19(2):227-233.(ZHENG Y,CHENG H,SUN W B.Finding an optimal seam-line through the shorted distance in the neighborhood[J].Journal of Image and Graphics,2014,19(2):227-233.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201402008&amp;v=MjkxNjhIOVhNclk5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtWN3ZQUHlyZmJMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        郑悦,程红,孙文邦.邻域最短距离法寻找最佳拼接缝[J].中国图象图形学报,2014,19(2):227-233.(ZHENG Y,CHENG H,SUN W B.Finding an optimal seam-line through the shorted distance in the neighborhood[J].Journal of Image and Graphics,2014,19(2):227-233.)
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_3" title="WEI Q,DOBIGEON N,TOURNERET J.Fast fusion of multi-band images based on solving a Sylvester equation[J].IEEE Transactions on Image Processing,2015,24(11):4109-4119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast fusion of multi-band images based on solving a sylvester equation">
                                        <b>[3]</b>
                                        WEI Q,DOBIGEON N,TOURNERET J.Fast fusion of multi-band images based on solving a Sylvester equation[J].IEEE Transactions on Image Processing,2015,24(11):4109-4119.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_4" title="TONG Y,CHEN J.Compressive sensing image fusion in heterogeneous sensor networks based on shearlet and wavelet transform[J].EURASIP Journal on Wireless Communications and Networking,2017,2017:52." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Compressive sensing image fusion in heterogeneous sensor networks based on shearlet and wavelet transform">
                                        <b>[4]</b>
                                        TONG Y,CHEN J.Compressive sensing image fusion in heterogeneous sensor networks based on shearlet and wavelet transform[J].EURASIP Journal on Wireless Communications and Networking,2017,2017:52.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_5" title="LARAQUI A,SAAIDI A,SATORI K.MSIP:multi-scale image pre-processing method applied in image mosaic[J].Multimedia Tools and Applications,2018,77(6):7517-7537." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MSIP multi-scale image pre-processing method applied in image mosaic">
                                        <b>[5]</b>
                                        LARAQUI A,SAAIDI A,SATORI K.MSIP:multi-scale image pre-processing method applied in image mosaic[J].Multimedia Tools and Applications,2018,77(6):7517-7537.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_6" title="邹佳彬,孙伟.基于提升静态小波变换与联合结构组稀疏表示的多聚焦图像融合[J].计算机应用,2018,38(3):859-865.(ZOU J B,SUN W.Multi-focus image fusion based on lifting stationary wavelet transform and joint structural group sparse representation[J].Journal of Computer Applications,2018,38(3):859-865.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201803044&amp;v=MjgzNzc3dlBMejdCZDdHNEg5bk1ySTlCWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        邹佳彬,孙伟.基于提升静态小波变换与联合结构组稀疏表示的多聚焦图像融合[J].计算机应用,2018,38(3):859-865.(ZOU J B,SUN W.Multi-focus image fusion based on lifting stationary wavelet transform and joint structural group sparse representation[J].Journal of Computer Applications,2018,38(3):859-865.)
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_7" title="BROWN M,LOWE D G.Automatic panoramic image stitching using invariant features[J].International Journal of Computer Vision 2007,74(1):59-73." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831131&amp;v=MzAwMDJadUZpcmxVNzdNSkZrPU5qN0Jhck80SHRIT3A0eEVaZWdPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZFor&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        BROWN M,LOWE D G.Automatic panoramic image stitching using invariant features[J].International Journal of Computer Vision 2007,74(1):59-73.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_8" title="MIKOLAJCZYK K,SCHMID C.An affine invariant interest point detector[C]//Proceedings of the 2002 European Conference on Computer Vision,LNCS 2350.Berlin:Springer,2002:128-142." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An affine invariant interest point detector">
                                        <b>[8]</b>
                                        MIKOLAJCZYK K,SCHMID C.An affine invariant interest point detector[C]//Proceedings of the 2002 European Conference on Computer Vision,LNCS 2350.Berlin:Springer,2002:128-142.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_9" title="FISCHLER M A,BOLLES R C.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM 1981,24(6):381-395." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000025011&amp;v=MjEwODU3SzdIdGpOcjQ5RlpPa0tESDA0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSUY4UWJoVT1OaWZJWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        FISCHLER M A,BOLLES R C.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM 1981,24(6):381-395.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_10" title="WAINE M,ROSSA C,SLOBODA R,et al.3D shape visualization of curved needles in tissue from 2D ultrasound images using RANSAC[C]//Proceedings of the 2015 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2015:4723-4728." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D shape visualization of curved needle s in tissue from 2D ultrasound images using RANSAC">
                                        <b>[10]</b>
                                        WAINE M,ROSSA C,SLOBODA R,et al.3D shape visualization of curved needles in tissue from 2D ultrasound images using RANSAC[C]//Proceedings of the 2015 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2015:4723-4728.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_11" title="TANG Y,SHIN J,LIAO H C.De-ghosting method for image stitching[J].International Journal of Digital Content Technology&amp;amp;Its Applications,2012,6:17-24." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD4027BC0AFC6B45FE839A954F46CF9998&amp;v=Mjk2NzNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMbTd4Szg9TmozYWFyZTRITmErM0k4MEVwZ0pmbmc4dVdNYjZUWU1RWHJtMmhZekNzU2RUTE9YQw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        TANG Y,SHIN J,LIAO H C.De-ghosting method for image stitching[J].International Journal of Digital Content Technology&amp;amp;Its Applications,2012,6:17-24.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_12" title="ZHANG B,LIU Q,IKENAGA T.Ghost-free high dynamic range imaging via moving objects detection and extension[C]//Proceedings of the 2015 Asia-Pacific Signal and Information Processing Association Summit and Conference.Piscataway:IEEE,2015:459-462." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ghost-free high dynamic range imaging via moving objects detection and extension">
                                        <b>[12]</b>
                                        ZHANG B,LIU Q,IKENAGA T.Ghost-free high dynamic range imaging via moving objects detection and extension[C]//Proceedings of the 2015 Asia-Pacific Signal and Information Processing Association Summit and Conference.Piscataway:IEEE,2015:459-462.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_13" title="HUANG C,LIN S,CHEN J.Efficient image stitching of continuous image sequence with image and seam selections[J].IEEESensors Journal,2015,15(10):5910-5918." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Image Stitching of Continuous Image Sequence With Image and Seam Selections">
                                        <b>[13]</b>
                                        HUANG C,LIN S,CHEN J.Efficient image stitching of continuous image sequence with image and seam selections[J].IEEESensors Journal,2015,15(10):5910-5918.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_14" title="方贤勇,潘志庚,徐丹.图像拼接的改进算法[J].计算机辅助设计与图形学学报,2003,15(11):1362-1365.(FANG XY,PAN Z G,XU D.An improved algorithm for image mosaics[J].Journal of Computer-Aided Design&amp;amp;Computer Graphics,2003,15(1):1362-1365.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF200311006&amp;v=MDE4MDhIdExOcm85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtWN3ZQTHo3QmFMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        方贤勇,潘志庚,徐丹.图像拼接的改进算法[J].计算机辅助设计与图形学学报,2003,15(11):1362-1365.(FANG XY,PAN Z G,XU D.An improved algorithm for image mosaics[J].Journal of Computer-Aided Design&amp;amp;Computer Graphics,2003,15(1):1362-1365.)
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_15" title="AHONEN T,HADID A,PIETIKINEN M.Face recognition with local binary patterns[C]//Proceedings of the 2004 European Conference on Computer Vision,LNCS 3021.Berlin:Springer,2004:469-481." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition with local binary patterns">
                                        <b>[15]</b>
                                        AHONEN T,HADID A,PIETIKINEN M.Face recognition with local binary patterns[C]//Proceedings of the 2004 European Conference on Computer Vision,LNCS 3021.Berlin:Springer,2004:469-481.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_16" title="CHOI H C,AHN B H.Image alignment by parameter hypersurface learning[J].Electronics Letters,2016,52(18):1526-1527." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image alignment by parameter hypersurface learning">
                                        <b>[16]</b>
                                        CHOI H C,AHN B H.Image alignment by parameter hypersurface learning[J].Electronics Letters,2016,52(18):1526-1527.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_17" title="CANNY J.A computational approach to edge detection[J].IEEETransactions on Pattern Analysis and Machine Intelligence,1986,8(6):679-698." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computational approach to edge detection">
                                        <b>[17]</b>
                                        CANNY J.A computational approach to edge detection[J].IEEETransactions on Pattern Analysis and Machine Intelligence,1986,8(6):679-698.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_18" title="QU Z,WANG T,AN S Q,et al.Image seamless stitching and straightening based on image block[J].IET Image Processing,2018,12(8):1361-1369." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image seamless stitching and straightening based on image block">
                                        <b>[18]</b>
                                        QU Z,WANG T,AN S Q,et al.Image seamless stitching and straightening based on image block[J].IET Image Processing,2018,12(8):1361-1369.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_19" title="QU Z,WEI X,CHEN S.An algorithm of image mosaic based on binary tree and eliminating distortion error[J].PLo S One,2019,14(1):e0210354." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An algorithm of image mosaic based on binary tree and eliminating distortion error">
                                        <b>[19]</b>
                                        QU Z,WEI X,CHEN S.An algorithm of image mosaic based on binary tree and eliminating distortion error[J].PLo S One,2019,14(1):e0210354.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_20" title="张小利,李雄飞,李军.融合图像质量评价指标的相关性分析及性能评估[J].自动化学报,2014,40(2):306-315.(ZHANG X L,LI X F,LI J.Validation and correlation analysis of metrics for evaluating performance of image fusion[J].Acta Automatica Sinica,2014,40(2):306-315.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201402014&amp;v=MTU3MjBLQ0xmWWJHNEg5WE1yWTlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1Y3dlA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                        张小利,李雄飞,李军.融合图像质量评价指标的相关性分析及性能评估[J].自动化学报,2014,40(2):306-315.(ZHANG X L,LI X F,LI J.Validation and correlation analysis of metrics for evaluating performance of image fusion[J].Acta Automatica Sinica,2014,40(2):306-315.)
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_21" title="黄学优,张长江.双树复小波域的MRI图像去噪[J].中国图象图形学报,2016,21(1):104-113.(HUANG X Y,ZHANG C J.MRI denoising based on dual-tree complex wavelet transform[J].Journal of Image and Graphics,2016,21(1):104-113.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201601013&amp;v=MDEzODc0SDlmTXJvOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWprVjd2UFB5cmZiTEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        黄学优,张长江.双树复小波域的MRI图像去噪[J].中国图象图形学报,2016,21(1):104-113.(HUANG X Y,ZHANG C J.MRI denoising based on dual-tree complex wavelet transform[J].Journal of Image and Graphics,2016,21(1):104-113.)
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-12 15:26</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(10),3053-3059 DOI:10.11772/j.issn.1001-9081.2019030544            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于拼接缝自适应消除和全景图矫直的快速图像拼接算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%98%A5%E5%BE%B7&amp;code=10625479&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨春德</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%88%90%E7%87%95%E8%8F%B2&amp;code=42897048&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">成燕菲</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对图像拼接存在色差过渡不均匀、图像倾斜扭曲及拼接效率低的现象,提出一种基于拼接缝自适应消除和全景图矫直的快速图像拼接算法。首先,用尺度不变特征变换(SIFT)提取图像指定区域特征点并用双向<i>K</i>近邻(KNN)算法进行图像配准,有效提高算法效率;其次,利用动态规划思想提出自适应公式找到最优拼接缝并用图像融合算法对其自适应消除,解决拼接缝色差过渡不均匀问题;最后,针对累积拼接误差形成全景图倾斜的现象,利用边缘检测算法提出自适应拟合四边形矫直模型,把原始全景图矫直为一个全新的全景图。所提算法与分块图像拼接和二叉树图像拼接算法相比,图像质量提升了5.84%～7.83%,拼接时间仅为原来的50%～70%。实验结果表明,该算法不仅通过自适应更新机制减少不同图像背景下拼接缝色差过渡不均匀的现象,从而提高了图像质量;而且提高了拼接效率,降低了全景图倾斜扭曲程度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%BA%E5%BA%A6%E4%B8%8D%E5%8F%98%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尺度不变特征变换;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E4%BC%98%E6%8B%BC%E6%8E%A5%E7%BC%9D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最优拼接缝;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E6%B6%88%E9%99%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应消除;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E7%9F%AB%E7%9B%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像矫直;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    杨春德(1964—),男,重庆人,教授,硕士,主要研究方向:数字图像处理、信息与计算理论;;
                                </span>
                                <span>
                                    *成燕菲(1995—),女,重庆人,硕士研究生,主要研究方向:数字图像处理。电子邮箱271962728@qq.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>重庆市基础与前沿计划项目(cstc2014jcyjA40033,cstc2015jcyjA40034,cstc2014jcyjA10051);</span>
                    </p>
            </div>
                    <h1><b>Fast image mosaic algorithm based on adaptive elimination of stitching seam and panorama alignment</b></h1>
                    <h2>
                    <span>YANG Chunde</span>
                    <span>CHENG Yanfei</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Technology, Chongqing University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the phenomenon that image mosaic, to a certain extent, has uneven chromatic aberration, distortion and low efficiency, an adaptive elimination of image stitching seam and panorama alignment based fast image mosaic algorithm was proposed. Firstly, the Scale-Invariant Feature Transform(SIFT) was used to extract feature points of the specified area of the image and image registration was performed by using bidirectional <i>K</i>-Nearest Neighbor(KNN) algorithm, effectively improving the algorithm efficiency. Secondly, focusing on the uneven chromatic aberration transition of stitching seam, an adaptive formula for finding the optimal stitching seam was proposed based on dynamic programming, and then the seam was adaptively eliminated by image fusion. Finally, for the phenomenon of panoramic tilt caused by accumlated stitching error, an adaptive fitting quadrilateral alignment model based on edge detection algorithm was proposed to make the original panorama into a completely new panorama. Compared with the image mosaic algorithm based on block and the image mosaic algorithm based on binary tree, the proposed algorithm has the image quality improved by 5.84%-7.83% and the stitching time shortened to only 50%-70% of the original. Experimental results show that the proposed algorithm not only reduces the unevenness of chromatic aberration transition of stitching seam in different image backgrounds through adaptive update mechanism, so as to improve the image quality, but also increases the stitching efficiency and reduces the distortion degree of panorama.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Scale-Invariant%20Feature%20Transform(SIFT)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Scale-Invariant Feature Transform(SIFT);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optimal%20stitching%20seam&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optimal stitching seam;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20elimination&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive elimination;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20alignment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image alignment;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YANG Chunde,born in 1964,M.S.,professor.His research interests include digital image processing,information and computing theory.;
                                </span>
                                <span>
                                    CHENG Yanfei,born in 1995,M.S.candidate.Her research interests include digital image processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-03</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the Basic and Frontier Program of Chongqing(cstc2014jcyj A40033,cstc2015jcyj A40034,cstc2014jcyj A10051);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="48">图像拼接技术<citation id="215" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是将同一场景下数幅有序且有重复区域的图像拼接成一幅全景图,在无人机航拍、遥感图像等领域应用广泛。图像拼接一般分为特征提取、图像配准和图像融合三个部分。</p>
                </div>
                <div class="p1">
                    <p id="49">目前,在图像处理领域,国内外专家都做了大量的研究。其中,郑悦等<citation id="216" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出邻域最短距离法寻找最佳拼接缝算法,仍没能解决拼接缝两侧过渡不均的问题;Wei等<citation id="217" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出快速多波段融合算法,实现对高空间低光谱分辨率图像和低空间高光谱分辨率图像的融合,仍存在拼接缝明显问题;Tong等<citation id="218" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出基于剪切和小波变换的异构传感器网络压缩感知图像融合算法,可以在低采样率下获得理想融合结果;Laraqui等<citation id="219" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出应用于图像拼接的多尺度图像预处理算法,可以校正图像之间的尺度差异,以减少异常值和对齐误差;邹佳彬等<citation id="220" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出的基于提升静态小波变换与联合结构组稀疏表示的多聚焦图像融合算法,可以较好保留图像的纹理与边缘等细节信息,融合图像效果比较好。</p>
                </div>
                <div class="p1">
                    <p id="50">本文为提高图像质量和拼接效率,同时降低图像扭曲误差,提出一种基于最优拼接缝自适应消除和全景图矫直的快速图像拼接算法。该算法改进了SIFT(Scale Invariant Feature Transform, SIFT)算法<citation id="221" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,只提取图像指定区域的特征,并使用双向<i>K</i>近邻(<i>K</i>-Nearest Neighbor, KNN)算法剔除误匹配特征点,完成图像配准;然后,在图像重叠区域运用动态规划思想寻找最优拼接缝,再对最优拼接缝限定区域进行多分辨率拉普拉斯融合;最后,建立自适应拟合四边形矫直模型,可以自适应地对全景图进行误差矫直,有效消除扭曲现象,完成图像拼接。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 图像配准</h3>
                <div class="p1">
                    <p id="52">特征配准是两幅含有重叠区域图像的特征点进行配准,首先要提取特征点。传统SIFT算法提取特征点由四个关键步骤组成:尺度空间极值检测<citation id="222" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>;定位关键点;确定关键点方向;生成特征描述子。基于传统SIFT算法因提取整幅图像特征导致算法计算量大,并易造成后续匹配错误和计算冗余。本文改进SIFT算法,在尺度空间进行极值检测时,以数像素点的方式只检测图像指定区域特征点,缩短图像检测范围,在保证特征提取准确度的同时,有效提高时间效率。</p>
                </div>
                <div class="p1">
                    <p id="53">设<i>n</i>幅分辨率<i>i</i>*<i>j</i>序列图像,判断当前图像是参考图像还是目标图像,特征点提取如图1所示。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 序列图像提取特征点" src="Detail/GetImg?filename=images/JSJY201910043_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 序列图像提取特征点  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Feature points extraction of image sequence</p>

                </div>
                <div class="p1">
                    <p id="55">图1中的序列图像除第一幅和最后一幅图像外,其余图像需要提取两次特征点。设提取单位像素特征的时间为<i>t</i>,则理论上提取整幅图像特征的时间为(<i>i</i>×<i>j</i>)×<i>t</i>。设<i>T</i>(<i>i</i>, <i>j</i>)是提取<i>n</i>幅序列图像特征的时间,如式(1):</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Τ</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>≈</mo><mo stretchy="false">(</mo><mi>i</mi><mo>×</mo><mi>j</mi><mo stretchy="false">)</mo><mo>×</mo><mi>t</mi><mo>+</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mn>2</mn><mo stretchy="false">(</mo><mi>i</mi><mo>×</mo><mi>j</mi><mo stretchy="false">)</mo><mo>×</mo><mi>t</mi><mo>+</mo><mo>⋯</mo><mo>+</mo><mn>2</mn><mo stretchy="false">(</mo><mi>i</mi><mo>×</mo><mi>j</mi><mo stretchy="false">)</mo><mo>×</mo><mi>t</mi></mrow></mstyle><mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></munder><mo>+</mo></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mi>i</mi><mo>×</mo><mi>j</mi><mo stretchy="false">)</mo><mo>×</mo><mi>t</mi><mo>≈</mo><mn>2</mn><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><mi>i</mi><mo>×</mo><mi>j</mi><mo stretchy="false">)</mo><mo>×</mo><mi>t</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">限定提取特征区域,如图1所示。若是参考图像,限定区域为从右向左数<i>m</i>个像素点所在区域;若是目标图像,则为从左向右数<i>m</i>个像素点所在区域。限定区域为,则提取特征时间为(<i>i</i>×<i>m</i>)×<i>t</i>,提取<i>n</i>幅图像特征的时间为<i>T</i>(<i>i</i>,<i>m</i>)。设<i>λ</i>表示本文提取特征点的时间缩短效率,如式(2):</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>λ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>-</mo><mi>Τ</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><mrow><mi>Τ</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mn>1</mn><mo>-</mo><mfrac><mrow><mn>2</mn><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><mi>i</mi><mo>×</mo><mi>m</mi><mo stretchy="false">)</mo><mo>×</mo><mi>t</mi></mrow><mrow><mn>2</mn><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><mi>i</mi><mo>×</mo><mi>j</mi><mo stretchy="false">)</mo><mo>×</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mi>m</mi><mi>j</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中:<i>λ</i>与<i>m</i>的取值有关,而<i>m</i>&lt;<i>j</i>。因此理论上,限定提取特征点区域会减少提取特征点数量,降低算法计算量,提高时间效率。</p>
                </div>
                <div class="p1">
                    <p id="60">改进SIFT特征提取结果如图2所示。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 改进SIFT特征提取" src="Detail/GetImg?filename=images/JSJY201910043_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 改进SIFT特征提取  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Feature extraction by improved SIFT</p>

                </div>
                <div class="p1">
                    <p id="62">SIFT特征提取后,分别对参考图像和目标图像建立K-D树(<i>K</i>-Dimensional Tree),并分别取其中一个为参考进行KNN匹配,提取两次匹配操作的公共匹配对作为初始匹配。获得初始匹配结果后,采用随机采样一致性(RANdom SAmple Consensus, RANSAC)算法<citation id="223" type="reference"><link href="189" rel="bibliography" /><link href="191" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>剔除外点,保留内点,降低误匹配率,并估算图像之间的仿射变换矩阵。初始匹配得到的匹配对中随机选取<i>a</i>对点对集合,<i>θ</i>′为图像旋转角度,<i>u</i>、<i>v</i>为图像沿<i>x</i>轴与<i>y</i>轴方向的水平位移。经过多次迭代,估算出矩阵模型<i><b>H</b></i>的参数,如式(3):</p>
                </div>
                <div class="area_img" id="63">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201910043_06300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="64">特征点配准结果如图3所示。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 图像配准结果" src="Detail/GetImg?filename=images/JSJY201910043_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 图像配准结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Image matching result</p>

                </div>
                <h3 id="66" name="66" class="anchor-tag">2 最优拼接缝的自适应消除图像融合</h3>
                <div class="p1">
                    <p id="67">传统的图像融合算法对配准后的多幅有序图像直接融合,会产生十分明显的拼接缝并且出现色差过渡不连续和伪影的现象<citation id="224" type="reference"><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。因此,本文提出一种最优拼接缝自适应消除的图像融合算法。</p>
                </div>
                <div class="p1">
                    <p id="68">寻找最优拼接缝<citation id="225" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的常用算法有狄克斯特拉算法、贪婪算法等。这些算法不能自适应地应用于各种场景,并且时间复杂度较高无法满足实时拼接的需求。本文提出一种基于动态规划思想寻找最优拼接缝的算法,可以有效解决这个问题。本文算法核心是扫描图像重叠区域的像素信息,找到每行像素点在重叠部分图像色差最小且结构与周围像素点最相似的位置,即计算当前像素点的能量值是否为每行像素点的最小值。能量值的取值与重叠部分图像的色差和纹理差有关。这些位置的像素点集合,就是要找的最优拼接缝。</p>
                </div>
                <div class="p1">
                    <p id="69">本文改进方贤勇等<citation id="226" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的拼接缝寻找算法,设<i>r</i>和<i>c</i>分别表示拼接图像重叠区域的行和列。设<i>E</i><sub>color</sub>(<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>j</i></sub>)表示重叠区域第<i>i</i>行<i>j</i>列像素点周围5×5区域的色差:</p>
                </div>
                <div class="area_img" id="172">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJY201910043_17200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="72">考虑图像像素点周围特征的灰度不变性和旋转不变性,本文使用圆形LBP(Local Binary Pattern)算子<citation id="227" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>描述图像局部特征。设一个中心像素点(<i>x</i><sub><i>c</i></sub>, <i>y</i><sub><i>c</i></sub>),<i>R</i>是半径,<i>P</i>是样本点的个数,通过式(5)计算它的邻近点(<i>x</i><sub><i>p</i></sub>, <i>y</i><sub><i>p</i></sub>), <i>p</i>∈{1,2,…,<i>P</i>}。</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>x</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mi>x</mi><msub><mrow></mrow><mi>c</mi></msub><mo>+</mo><mi>R</mi><mspace width="0.25em" /><mi>cos</mi><mo stretchy="false">(</mo><mn>2</mn><mtext>π</mtext><mi>p</mi><mo>/</mo><mi>Ρ</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>y</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>c</mi></msub><mo>+</mo><mi>R</mi><mspace width="0.25em" /><mi>sin</mi><mo stretchy="false">(</mo><mn>2</mn><mtext>π</mtext><mi>p</mi><mo>/</mo><mi>Ρ</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">设<i>i</i><sub><i>c</i></sub>是当前像素点(<i>x</i><sub><i>c</i></sub>, <i>y</i><sub><i>c</i></sub>)的灰度值,<i>i</i><sub><i>p</i></sub>是相邻像素点(<i>x</i><sub><i>p</i></sub>, <i>y</i><sub><i>p</i></sub>)的灰度值,<i>s</i>是符号函数。设<i>E</i><sub>geometry</sub>(<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>j</i></sub>)表示重叠区域第<i>i</i>行<i>j</i>列像素点周围区域的纹理差:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>e</mtext><mtext>o</mtext><mtext>m</mtext><mtext>e</mtext><mtext>t</mtext><mtext>r</mtext><mtext>y</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>L</mi><mi>B</mi><mi>Ρ</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>c</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></munderover><mn>2</mn></mstyle><msup><mrow></mrow><mi>p</mi></msup><mi>s</mi><mo stretchy="false">(</mo><mi>i</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>i</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">设<i>α</i><sup>*</sup><sub><i>ij</i></sub>表示当前像素点的色差与重叠区域当前行色差均值的比值, <i>β</i><sup>*</sup><sub><i>ij</i></sub>表示当前像素点的纹理差与重叠区域当前行纹理差均值的比值,如式(7)所示:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>α</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mo>*</mo></msubsup><mo>=</mo><mfrac><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>l</mtext><mtext>o</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><mi>c</mi><mo stretchy="false">)</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>E</mi></mstyle><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>l</mtext><mtext>o</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr><mtr><mtd><mi>β</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mo>*</mo></msubsup><mo>=</mo><mfrac><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>e</mtext><mtext>o</mtext><mtext>m</mtext><mtext>e</mtext><mtext>t</mtext><mtext>r</mtext><mtext>y</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><mi>c</mi><mo stretchy="false">)</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>E</mi></mstyle><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>e</mtext><mtext>o</mtext><mtext>m</mtext><mtext>e</mtext><mtext>t</mtext><mtext>r</mtext><mtext>y</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">设<i>α</i><sub><i>ij</i></sub>′和<i>β</i><sub><i>ij</i></sub>′分别表示计算当前像素点能量值的两个因素色差和纹理差的占比参数,其自适应公式为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>´</mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>α</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>)</mo></mrow></mrow><mo>|</mo></mrow></mtd></mtr><mtr><mtd><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>´</mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>)</mo></mrow></mrow><mo>|</mo></mrow></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">式(8)是根据sigmod函数得到的,如图4所示。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 像素点能量值的占比参数的自适应关系" src="Detail/GetImg?filename=images/JSJY201910043_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 像素点能量值的占比参数的自适应关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Adaptive relationship of the ratio parameters of pixel energy value</p>

                </div>
                <div class="p1">
                    <p id="82">图4中:<i>α</i><sup>*</sup><sub><i>ij</i></sub>越接近于1,表示当前像素点色差与重叠区域当前行色差相差不大,相应的<i>α</i><sub><i>ij</i></sub>′越小,即计算能量值时色差因素占比越小;同理, <i>β</i><sup>*</sup><sub><i>ij</i></sub>越接近于1,相应的<i>β</i><sub><i>ij</i></sub>′取值越小,纹理差因素占比越小。相反地,<i>α</i><sup>*</sup><sub><i>ij</i></sub>和<i>β</i><sup>*</sup><sub><i>ij</i></sub>越远离1,相应的<i>α</i><sub><i>ij</i></sub>′和<i>β</i><sub><i>ij</i></sub>′取值越大,即计算能量值时相应的因素占比就越大。</p>
                </div>
                <div class="p1">
                    <p id="83">为保证色差占比参数<i>α</i><sub><i>ij</i></sub>′和纹理差占比参数<i>β</i><sub><i>ij</i></sub>′在计算能量值时的和恒为1,对其进行归一化处理,得到最后的色差因素占比参数<i>α</i><sub><i>ij</i></sub>和纹理差因素占比参数<i>β</i><sub><i>ij</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>´</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>´</mo><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac></mtd></mtr><mtr><mtd><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>´</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>´</mo><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">最终遍历重叠区域像素点,通过能量式(10)计算像素点的能量值,可以定位重叠区域中每行能量值最小的像素点。</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>r</mi></mtd></mtr><mtr><mtd><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>c</mi></mtd></mtr></mtable></mrow></munder><mspace width="0.25em" /><mi>E</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>E</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>l</mtext><mtext>o</mtext><mtext>r</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>E</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>e</mtext><mtext>o</mtext><mtext>m</mtext><mtext>e</mtext><mtext>t</mtext><mtext>r</mtext><mtext>y</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mspace width="0.25em" /><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">找到最优拼接缝后对待拼接图像直接拼接,会出现拼接缝的痕迹。为了消除拼接缝,需要对拼接图像进行图像融合。本文使用改进的最优拼接缝拉普拉斯图像融合算法,对融合区域<i>ξ</i>进行限制,消除伪影和拼接缝,提高融合效率和图像质量。</p>
                </div>
                <div class="p1">
                    <p id="88">图像融合前首先建立拼接缝图像的权值图像<i>I</i><sub><i>L</i></sub>,拼接缝左侧用像素值255填充,右侧用像素值0填充,如图5所示。然后求出最优拼接缝最小外接矩形左边界<i>x</i><sub>min</sub>和右边界<i>x</i><sub>max</sub>。取一个最优拼接缝的外接矩形<i>L</i>,使其左边界为<i>x</i><sub>min</sub>-<i>ξ</i>,右边界为<i>x</i><sub>min</sub>+<i>ξ</i>。图像融合的具体步骤如下:首先,对目标图像和配准后的参考图像扩充至权值图像<i>I</i><sub><i>L</i></sub>大小并限定融合区域;其次,对融合图像限定区域进行拉普拉斯分解,获得拉普拉斯金字塔;接着,对权值图像<i>I</i><sub><i>L</i></sub>采用高斯扩展处理,使最优拼接缝处像素值从255到0缓慢过渡,进行平滑处理;最后,根据融合准则对分解的拉普拉斯金字塔对应的层进行融合,再进行重构,得到最终的融合图像。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 权值图像IL" src="Detail/GetImg?filename=images/JSJY201910043_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 权值图像<i>I</i><sub><i>L</i></sub>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Weight image <i>I</i><sub><i>L</i></sub></p>

                </div>
                <div class="p1">
                    <p id="90">图6(a)是原始图像图2(a)、2(b)的拼接缝掩膜,图6(b)是最优拼接缝自适应消除的拉普拉斯图像融合算法结果。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 图2(a)和图2(b)的融合结果" src="Detail/GetImg?filename=images/JSJY201910043_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 图2(a)和图2(b)的融合结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Image fusion result for Fig.2(a) and Fig.2(b)</p>

                </div>
                <h3 id="92" name="92" class="anchor-tag">3 图像矫直</h3>
                <div class="p1">
                    <p id="93">图像的采集和配准过程存在不可避免的扭曲误差现象<citation id="228" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。因此,本文提出一个自适应拟合四边形矫直模型,对拼接后全景图进行自适应矫直,降低全景图的扭曲程度。</p>
                </div>
                <div class="p1">
                    <p id="94">本文使用<i>Canny</i>边缘检测算法<citation id="229" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>检测全景图二值图像轮廓。该算法基于梯度,可以精确定位边沿位置。通过沿幅角方向检测模值的极大值点,即边缘点,置像素灰度值为0,使边沿非常细。<i>Canny</i>算法使用双阈值检测,一个较小阈值会保留很多边沿,其中一部分是没有用的,而一个较大阈值则会保留主要的边沿,同时也可能会丢失一些边沿信息,<i>Canny</i>边缘检测算法一定程度上可以克服这类困难。</p>
                </div>
                <div class="p1">
                    <p id="95">图7(<i>a</i>)是拼接后的全景图,图7(<i>b</i>)是提取待矫直全景图的边缘并绘制在同等大小的空白图像上的边缘图。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 全景图边缘提取" src="Detail/GetImg?filename=images/JSJY201910043_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 全景图边缘提取  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.7 <i>Edge extraction of panorama</i></p>

                </div>
                <div class="p1">
                    <p id="97">将提取的边缘拟合成四条直线,形成一个近似四边形,如图8(<i>a</i>)是基于图7(<i>b</i>)全景图拟合的四边形。设<i>θ</i>是图像矫直角度,<i>θ</i><sub><i>i</i></sub>(<i>i</i>=1,2)是拟合四边形中两条水平向边的倾斜角度,<i>l</i><sub><i>i</i></sub>.<i>x</i>、<i>l</i><sub><i>i</i></sub>.<i>y</i>是两条水平向边上的任意一点坐标值。<i>θ</i>的取值为:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false">(</mo><mrow><mi>arctan</mi></mrow><mo stretchy="false">(</mo><mfrac><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>.</mo><mi>x</mi></mrow><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>.</mo><mi>y</mi></mrow></mfrac><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">设(<i>a</i>.<i>x</i>,<i>a</i>.<i>y</i>)、(<i>b</i>.<i>x</i>,<i>b</i>.<i>y</i>)、(<i>c</i>.<i>x</i>,<i>c</i>.<i>y</i>)和(<i>d</i>.<i>x</i>,<i>d</i>.<i>y</i>)是拟合四边形的左上、左下、右上和右下四个顶点坐标。矫直后四个顶点坐标,其左上和左下坐标不变,设其右上和右下坐标为(<i>M</i>,<i>a</i>.<i>y</i>),(<i>N</i>,<i>b</i>.<i>y</i>),<i>height</i>为待矫直图像的高度。其中<i>M</i>和<i>N</i>的取值为:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>Μ</mi><mo>=</mo><msqrt><mrow><mi>c</mi><mo>.</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>c</mi><mo>.</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mtd></mtr><mtr><mtd><mi>Ν</mi><mo>=</mo><msqrt><mrow><mi>d</mi><mo>.</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false">|</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>-</mo><mo stretchy="false">|</mo><mi>c</mi><mo>.</mo><mi>y</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">利用矫直后的四对坐标,计算仿射变换矩阵,作用于整个全景图,并进行双线性插值完成图像矫直,如图8(b)所示。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 全景图矫直" src="Detail/GetImg?filename=images/JSJY201910043_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 全景图矫直  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Panorama alignment</p>

                </div>
                <div class="p1">
                    <p id="103">本文具体算法步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="104">输入 有重叠区域的有序图像序列<i>S</i>(<i>S</i><sub>1</sub>,<i>S</i><sub>2</sub>,…,<i>S</i><sub><i>N</i></sub>)。</p>
                </div>
                <div class="p1">
                    <p id="105">输出 一幅拼接并矫直的全景图。</p>
                </div>
                <div class="p1">
                    <p id="106">步骤1 针对拼接图像<i>S</i><sub><i>i</i></sub>和<i>S</i><sub><i>i</i></sub><sub>+1</sub>(<i>i</i>∈[1,<i>N</i>-1]),图像配准。</p>
                </div>
                <div class="p1">
                    <p id="107">1)判断相邻图像是目标图像还是参考图像,并确定<i>m</i>的取值0.4<i>j</i>,得到限定区域。</p>
                </div>
                <div class="p1">
                    <p id="108">2)对限定区域提取特征点,使用双向KNN算法,计算欧氏距离,将初始匹配特征点集合和数目保存在数组<i>FeatureSet</i>和<i>NumF</i>中。</p>
                </div>
                <div class="p1">
                    <p id="109">3)使用RANSAC算法剔除误匹配后,计算<i>S</i><sub><i>i</i></sub>和<i>S</i><sub><i>i</i></sub><sub>+1</sub>之间的仿变换模型,保存在数组<i>HF</i>中。</p>
                </div>
                <div class="p1">
                    <p id="110">4)返回步骤1)继续下一组相邻图像的图像配准,直到完成所有图像序列间的配准为止。</p>
                </div>
                <div class="p1">
                    <p id="111">步骤2 根据数组<i>HF</i>通过式(3)计算仿射变换矩阵<i><b>H</b></i>,使目标图像与参考图像在相同坐标系下。</p>
                </div>
                <div class="p1">
                    <p id="112">步骤3 利用式(4)找到最优拼接缝并使用多分辨率拉普拉斯融合算法进行图像融合。</p>
                </div>
                <div class="p1">
                    <p id="113">步骤4 转到步骤2执行下一幅图像拼接直到图像序列<i>S</i>只有一幅图像为止。</p>
                </div>
                <div class="p1">
                    <p id="114">步骤5 对拼接后全景图使用边缘检测算法,并利用式(11)和(12)对全景图进行矫直,得到矫直后的全景图。</p>
                </div>
                <div class="p1">
                    <p id="115">算法流程如图9所示。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 算法流程" src="Detail/GetImg?filename=images/JSJY201910043_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Algorithm flowchart</p>

                </div>
                <h3 id="117" name="117" class="anchor-tag">4 实验与结果分析</h3>
                <h4 class="anchor-tag" id="118" name="118">4.1 <b>主观评价</b></h4>
                <div class="p1">
                    <p id="119">本文算法对多个场景图像进行图像拼接和矫直实验,实验在<i>CPU</i>为<i>Intel i</i>5处理器、内存为4 <i>GB</i>的<i>Windows</i> 7系统下完成,实验环境是<i>Visual Studio</i> (<i>OpenCV</i> 2.4.9)。实验中的参数a=3。</p>
                </div>
                <div class="p1">
                    <p id="120">图10是两张原始图像序列,图12、14是多序列原始图像。图11、13、15是对应原图像的三种拼接算法结果。</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 两幅原始图像序列" src="Detail/GetImg?filename=images/JSJY201910043_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 两幅原始图像序列  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.10 <i>Two original image sequences</i></p>

                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 对图10的拼接算法结果对比" src="Detail/GetImg?filename=images/JSJY201910043_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 对图10的拼接算法结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.11 <i>Comparison of image mosaic algorithm results for Fig</i>.10</p>

                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 10幅原始图像序列" src="Detail/GetImg?filename=images/JSJY201910043_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 10幅原始图像序列  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.12 <i>Ten original image sequences</i></p>

                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 对图12的拼接算法结果对比" src="Detail/GetImg?filename=images/JSJY201910043_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 对图12的拼接算法结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.13 <i>Comparison of image mosaic algorithm results for Fig</i>.12</p>

                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 16幅原始图像序列" src="Detail/GetImg?filename=images/JSJY201910043_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 16幅原始图像序列  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.14 <i>Sixteen original image sequences</i></p>

                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 对图14的拼接算法结果对比" src="Detail/GetImg?filename=images/JSJY201910043_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图15 对图14的拼接算法结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>.15 <i>Comparison of image mosaic algorithm results for Fig</i>.14</p>

                </div>
                <div class="p1">
                    <p id="128">图11(<i>a</i>)、图13(<i>a</i>)和15(<i>a</i>)是基于分块图像拼接算法的结果<citation id="230" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,图11(<i>b</i>)、13(<i>b</i>)和15(<i>b</i>)是基于二叉树图像拼接算法的结果<citation id="231" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,图11(<i>c</i>)、13(<i>c</i>)和15(<i>c</i>)是本文改进算法的拼接结果。从图11的实线方框以及放大细节图中的实线圆框可以明显看出,其他两种算法得到的拼接图像仍存在明显的拼接痕迹,甚至出现图像扭曲、失真的现象,而本文算法对拼接缝周围的色差过渡有显著效果。因此,从视觉上明显看出,本文算法得到的全景图更能够满足人们对图像的视觉要求。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">4.2 <b>实验参数及客观评价</b></h4>
                <div class="p1">
                    <p id="130">为了更客观地反映本文算法的优越性,通过计算图像的统计参数获得相对客观的量化指标,对图像拼接算法进行客观评价<citation id="232" type="reference"><link href="211" rel="bibliography" /><link href="213" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="131">设<i>w</i>、<i>h</i>分别表示全景图的高度和宽度,<i>s</i>(<i>x</i>, <i>y</i>)是图像<i>I</i>(<i>x</i>, <i>y</i>)在点(<i>x</i>, <i>y</i>)处的梯度。图像的清晰度可以通过<i>Tenengrad</i>梯度函数衡量,如式(13):</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>w</mi><mo>×</mo><mi>h</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><mi>w</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><mi>h</mi></munderover><mi>s</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mspace width="0.25em" /><mi>y</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">其中:<mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mspace width="0.25em" /><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><msqrt><mrow><mi>G</mi><msub><mrow></mrow><mi>x</mi></msub><mo>*</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mspace width="0.25em" /><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>G</mi><msub><mrow></mrow><mi>y</mi></msub><mo>*</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mspace width="0.25em" /><mi>y</mi><mo stretchy="false">)</mo></mrow></msqrt><mo>,</mo><mi>G</mi><msub><mrow></mrow><mi>x</mi></msub></mrow></math></mathml>和<i>G</i><sub><i>y</i></sub>是Sobel卷积核,Sobel算子可以计算水平和垂直方向的梯度。同时,图像的清晰度也可用全景图方差<i>σ</i><sup>2</sup>衡量,如式(14):</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>w</mi><mo>×</mo><mi>h</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>h</mi></munderover><mo stretchy="false">(</mo></mstyle></mrow></mstyle><mi>Ι</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo><mo>-</mo><mi>Μ</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135">其中:<mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>w</mi><mo>×</mo><mi>h</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>h</mi></munderover><mi>Ι</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo stretchy="false">)</mo></mrow></math></mathml>。梯度值和方差值越大,图像越清晰。</p>
                </div>
                <div class="p1">
                    <p id="136">定义全景图信息比近似反映全景扭曲程度,如式(15):</p>
                </div>
                <div class="p1">
                    <p id="137">信息比<mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mtext>黑</mtext><mtext>色</mtext><mtext>背</mtext><mtext>景</mtext><mtext>像</mtext><mtext>素</mtext><mtext>总</mtext><mtext>数</mtext></mrow><mrow><mi>w</mi><mo>×</mo><mi>h</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="138">全景图扭曲程度越大,全景图中无用黑色像素随之增多,信息比参数越小。</p>
                </div>
                <div class="p1">
                    <p id="139">要使式(8)中的函数在[0,2]以<i>α</i><sup>*</sup><sub><i>ij</i></sub>(<i>β</i><sup>*</sup><sub><i>ij</i></sub>)=1为对称轴,就要使式(8)中的参数<i>p</i><sub>1</sub>=4, <i>p</i><sub>2</sub>=4,如图4所示。同时,经过多次实验,参数<i>k</i><sub>1</sub>=1.8、<i>k</i><sub>2</sub>=1.7时图像清晰度最好,结果对比如表1所示。</p>
                </div>
                <div class="p1">
                    <p id="140">从表2可明显看出,本文算法在全景图的清晰度、倾斜角度和信息比方面都有较为明显的进步,而且随着图像数据集的增加,这些优势会越明显。在本文图像数据集中,与基于分块的图像拼接算法和基于二叉树的图像拼接算法对比,清晰度比前者平均提高7.83%,比后者平均提高5.84%;倾斜度比前者平均降低1.57°,比后者平均提稿1.14°;信息比比前者平均提升2.54%,比后者平均提高2.87%。因此,本文的拼接算法改善了图像拼接中的扭曲现象,同时提高了拼接后全景图的质量。</p>
                </div>
                <div class="p1">
                    <p id="141">本文算法的运行时间由三个阶段组成:图像配准、图像融合和图像矫直。由式(2)可知,图像配准的时间效率<i>λ</i>与图像分辨率和限定提取特征的区域参数<i>m</i>有关;由式(10)以及融合范围可知,图像融合的时间效率与图像分辨率和限定融合范围参数<i>ξ</i>有关;由式(11)和(12)可知,图像矫直的时间效率与全景图分辨率有关。因此,本文算法的时间效率是由图像分辨率以及参数<i>m</i>和<i>ξ</i>决定的,<i>ξ</i>的经验阈值为30。而本文算</p>
                </div>
                <div class="p1">
                    <p id="142">法是与其他两种算法的时间进行比对,因此,针对同样的图像数据集,图像分辨率的因素不考虑在内,<i>m</i>的取值在参考了黄金分割比0.618以及对本文图像数据集的分析,考虑<i>m</i>的取值范围在0.35<i>j</i>～0.55<i>j</i>,经过多次实验,取得最优经验阈值0.4<i>j</i>,实验效果最好,见表3。同时,通过式(2)计算得到的<i>λ</i>可知,配准算法的时间可以缩短近60%。图16是本文算法与其他两种对比算法的时间对比。</p>
                </div>
                <div class="area_img" id="143">
                    <p class="img_tit"><b>表</b>1 <b>参数</b><i>k</i><sub>1</sub>、<i>k</i><sub>2</sub><b>取值对应的图像清晰度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.1 Clarity comparison of images corresponding to <i>k</i><sub>1</sub> and <i>k</i><sub>2</sub></p>
                    <p class="img_note"></p>
                    <table id="143" border="1"><tr><td rowspan="2" colspan="2"><br />参数</td><td colspan="5"><br />场景</td></tr><tr><td colspan="2"><br />图6(b)</td><td rowspan="2"></td><td colspan="2"><br />图8(b)</td></tr><tr><td><br /><i>k</i><sub>1</sub></td><td><i>k</i><sub>2</sub></td><td><br />梯度</td><td>方差</td><td><br />梯度</td><td>方差</td></tr><tr><td>1.7</td><td>1.7</td><td>7.53</td><td>6 294</td><td></td><td>10.6</td><td>2 765</td></tr><tr><td><br />1.7</td><td>1.8</td><td>7.65</td><td>6 374</td><td></td><td>11.3</td><td>2 832</td></tr><tr><td><br />1.7</td><td>1.9</td><td>7.79</td><td>6 396</td><td></td><td>11.1</td><td>2 835</td></tr><tr><td><br />1.8</td><td>1.7</td><td>7.83</td><td>6 402</td><td></td><td>11.5</td><td>2 847</td></tr><tr><td><br />1.8</td><td>1.8</td><td>7.81</td><td>6 389</td><td></td><td>10.9</td><td>2 793</td></tr><tr><td><br />1.8</td><td>1.9</td><td>7.78</td><td>6 388</td><td></td><td>10.7</td><td>2 812</td></tr><tr><td><br />1.9</td><td>1.7</td><td>7.68</td><td>6 375</td><td></td><td>11.2</td><td>2 815</td></tr><tr><td><br />1.9</td><td>1.8</td><td>7.55</td><td>6 324</td><td></td><td>10.8</td><td>2 796</td></tr><tr><td><br />1.9</td><td>1.9</td><td>7.88</td><td>6 384</td><td></td><td>11.2</td><td>2 785</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="144">
                    <p class="img_tit"><b>表</b>2 <b>全景图清晰度及质量评价指标对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.2 Comparison of panorama clarity and quality evaluation indicators</p>
                    <p class="img_note"></p>
                    <table id="144" border="1"><tr><td rowspan="2">图号</td><td rowspan="2">尺寸</td><td colspan="4"><br />基于分块图像拼接算法</td><td rowspan="2"></td><td colspan="4"><br />基于二叉树图像拼接算法</td><td rowspan="2"></td><td colspan="4"><br />基于本文算法</td></tr><tr><td><br />倾斜度/(°)</td><td>信息比/%</td><td>梯度</td><td>方差</td><td><br />倾斜度/(°)</td><td>信息比/%</td><td>梯度</td><td>方差</td><td><br />倾斜度/(°)</td><td>信息比/%</td><td>梯度</td><td>方差</td></tr><tr><td>图10</td><td>375×389×2</td><td>0.23</td><td>98.54</td><td>2.37</td><td>3 805.9</td><td></td><td>0.26</td><td>98.51</td><td>2.51</td><td><b>3</b><b>902.3</b></td><td></td><td><b>0.18</b></td><td><b>99.24</b></td><td><b>2.69</b></td><td>3 859.4</td></tr><tr><td><br />图12</td><td>375×500×10</td><td>3.89</td><td>94.90</td><td>7.14</td><td><b>4</b><b>268.7</b></td><td></td><td>1.84</td><td>94.01</td><td>7.21</td><td>3 984.6</td><td></td><td><b>0.08</b></td><td><b>99.52</b></td><td><b>7.92</b></td><td>4 180.1</td></tr><tr><td><br />图14</td><td>500×747×16</td><td>1.92</td><td>95.28</td><td>6.72</td><td>7 759.3</td><td></td><td>1.73</td><td>94.87</td><td>7.13</td><td>7 905.4</td><td></td><td><b>0.16</b></td><td><b>96.65</b></td><td><b>7.51</b></td><td><b>8</b><b>117.1</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:加粗数据为最优值。</p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表</b>3 <b>图</b>12<b>图像提取特征点实验数据对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab.3 Comparison of experimental data of image feature points extraction in Figure 12</p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td rowspan="2">图像</td><td colspan="2"><br />特征点对数</td><td rowspan="2"></td><td colspan="2"><br />特征匹配对数</td><td rowspan="2"></td><td colspan="2"><br />特征匹配率/%</td></tr><tr><td><br />整幅<br />区域</td><td>限定<br />区域</td><td><br />整幅<br />区域</td><td>限定<br />区域</td><td><br />整幅<br />区域</td><td>限定<br />区域</td></tr><tr><td><i>I</i><sub>1</sub>和<i>I</i><sub>2</sub></td><td>1 560</td><td>912</td><td></td><td>132</td><td>156</td><td></td><td>73.90</td><td>85.20</td></tr><tr><td><br /><i>I</i><sub>2</sub>和<i>I</i><sub>3</sub></td><td>1 789</td><td>903</td><td></td><td>128</td><td>131</td><td></td><td>86.70</td><td>89.00</td></tr><tr><td><br /><i>I</i><sub>3</sub>和<i>I</i><sub>4</sub></td><td>1 796</td><td>846</td><td></td><td>140</td><td>129</td><td></td><td>76.00</td><td>79.00</td></tr><tr><td><br /><i>I</i><sub>4</sub>和<i>I</i><sub>5</sub></td><td>1 767</td><td>864</td><td></td><td>134</td><td>156</td><td></td><td>66.70</td><td>84.20</td></tr><tr><td><br /><i>I</i><sub>5</sub>和<i>I</i><sub>6</sub></td><td>1 814</td><td>841</td><td></td><td>70</td><td>95</td><td></td><td>66.30</td><td>87.60</td></tr><tr><td><br /><i>I</i><sub>6</sub>和<i>I</i><sub>7</sub></td><td>1 636</td><td>757</td><td></td><td>47</td><td>63</td><td></td><td>86.70</td><td>91.40</td></tr><tr><td><br /><i>I</i><sub>7</sub>和<i>I</i><sub>8</sub></td><td>1 535</td><td>729</td><td></td><td>19</td><td>54</td><td></td><td>61.50</td><td>93.30</td></tr><tr><td><br /><i>I</i><sub>8</sub>和 <i>I</i><sub>9</sub></td><td>1 499</td><td>697</td><td></td><td>32</td><td>41</td><td></td><td>82.30</td><td>89.60</td></tr><tr><td><br /><i>I</i><sub>9</sub>和<i>I</i><sub>10</sub></td><td>1 238</td><td>574</td><td></td><td>21</td><td>24</td><td></td><td>73.50</td><td>84.70</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="146">从表3可以看到,虽然提取整幅图像的SIFT特征点个数比较多,但对比提取指定区域图像的SIFT特征点,它们的匹配对数是相似的,但时间花销明显少很多,甚至经过RANSAC剔除误匹配后本文算法的匹配率更高,平均在87%左右。结合图16的全景图拼接时间的对比,可以明显看出,虽然本文算法的图像融合时间对比其他算法会有小幅度增加,但图像配准的时间在拼接算法中的时间占比是最重的。因此,本文算法比传统的SIFT特征匹配方法具有更强的鲁棒性。拼接算法消耗的时间是分块图像拼接算法的70%,是二叉树图像拼接算法的50%。</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201910043_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图16 算法时间对比" src="Detail/GetImg?filename=images/JSJY201910043_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图16 算法时间对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201910043_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.16 Algorithm time comparison</p>

                </div>
                <h3 id="148" name="148" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="149">在传统的图像拼接算法基础上,本文算法首先限定了SIFT特征的提取区域,在保证特征匹配准确率的同时,大幅缩短了算法的时间。同时,在此基础上提出了基于最优拼接缝的自适应消除图像融合算法,通过提出的自适应能量公式找到最优拼接缝并进行消除,有效地解决了伪影以及模糊像素点导致的色差问题。最后,对拼接后的全景图进行拟合四边形矫直,进一步改善图像失真问题。实验结果表明,本文算法有效提高了图像拼接质量和图像清晰度,有效保留了全景图信息;同时,提高了算法效率,实现了序列图像快速拼接。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="173">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast explicit diffusion for accelerated features in nonlinear scale spaces">

                                <b>[1]</b>ALCANTARILLA P F,NUEVO J,BARTOLI A.Fast explicit diffusion for accelerated features in nonlinear scale spaces[C]//Proceedings of the 24th British Machine Vision Conference.Durham:BMVA Press,2013:1-11.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201402008&amp;v=MzE5ODJyZmJMRzRIOVhNclk5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtWN3ZQUHk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>郑悦,程红,孙文邦.邻域最短距离法寻找最佳拼接缝[J].中国图象图形学报,2014,19(2):227-233.(ZHENG Y,CHENG H,SUN W B.Finding an optimal seam-line through the shorted distance in the neighborhood[J].Journal of Image and Graphics,2014,19(2):227-233.)
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast fusion of multi-band images based on solving a sylvester equation">

                                <b>[3]</b>WEI Q,DOBIGEON N,TOURNERET J.Fast fusion of multi-band images based on solving a Sylvester equation[J].IEEE Transactions on Image Processing,2015,24(11):4109-4119.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Compressive sensing image fusion in heterogeneous sensor networks based on shearlet and wavelet transform">

                                <b>[4]</b>TONG Y,CHEN J.Compressive sensing image fusion in heterogeneous sensor networks based on shearlet and wavelet transform[J].EURASIP Journal on Wireless Communications and Networking,2017,2017:52.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MSIP multi-scale image pre-processing method applied in image mosaic">

                                <b>[5]</b>LARAQUI A,SAAIDI A,SATORI K.MSIP:multi-scale image pre-processing method applied in image mosaic[J].Multimedia Tools and Applications,2018,77(6):7517-7537.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201803044&amp;v=MTM3NDhIOW5Nckk5QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5amtWN3ZQTHo3QmQ3RzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>邹佳彬,孙伟.基于提升静态小波变换与联合结构组稀疏表示的多聚焦图像融合[J].计算机应用,2018,38(3):859-865.(ZOU J B,SUN W.Multi-focus image fusion based on lifting stationary wavelet transform and joint structural group sparse representation[J].Journal of Computer Applications,2018,38(3):859-865.)
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831131&amp;v=MTczMTNYcVJyeG94Y01IN1I3cWRaK1p1RmlybFU3N01KRms9Tmo3QmFyTzRIdEhPcDR4RVplZ09ZM2s1ekJkaDRqOTlT&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>BROWN M,LOWE D G.Automatic panoramic image stitching using invariant features[J].International Journal of Computer Vision 2007,74(1):59-73.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An affine invariant interest point detector">

                                <b>[8]</b>MIKOLAJCZYK K,SCHMID C.An affine invariant interest point detector[C]//Proceedings of the 2002 European Conference on Computer Vision,LNCS 2350.Berlin:Springer,2002:128-142.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000025011&amp;v=MjgyOTZIdGpOcjQ5RlpPa0tESDA0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNJSUY4UWJoVT1OaWZJWTdLNw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>FISCHLER M A,BOLLES R C.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM 1981,24(6):381-395.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D shape visualization of curved needle s in tissue from 2D ultrasound images using RANSAC">

                                <b>[10]</b>WAINE M,ROSSA C,SLOBODA R,et al.3D shape visualization of curved needles in tissue from 2D ultrasound images using RANSAC[C]//Proceedings of the 2015 IEEE International Conference on Robotics and Automation.Piscataway:IEEE,2015:4723-4728.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD4027BC0AFC6B45FE839A954F46CF9998&amp;v=MTQyMDFYcm0yaFl6Q3NTZFRMT1hDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TG03eEs4PU5qM2FhcmU0SE5hKzNJODBFcGdKZm5nOHVXTWI2VFlNUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>TANG Y,SHIN J,LIAO H C.De-ghosting method for image stitching[J].International Journal of Digital Content Technology&amp;Its Applications,2012,6:17-24.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ghost-free high dynamic range imaging via moving objects detection and extension">

                                <b>[12]</b>ZHANG B,LIU Q,IKENAGA T.Ghost-free high dynamic range imaging via moving objects detection and extension[C]//Proceedings of the 2015 Asia-Pacific Signal and Information Processing Association Summit and Conference.Piscataway:IEEE,2015:459-462.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Image Stitching of Continuous Image Sequence With Image and Seam Selections">

                                <b>[13]</b>HUANG C,LIN S,CHEN J.Efficient image stitching of continuous image sequence with image and seam selections[J].IEEESensors Journal,2015,15(10):5910-5918.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF200311006&amp;v=MjY3OTA1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1Y3dlBMejdCYUxHNEh0TE5ybzlGWW9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>方贤勇,潘志庚,徐丹.图像拼接的改进算法[J].计算机辅助设计与图形学学报,2003,15(11):1362-1365.(FANG XY,PAN Z G,XU D.An improved algorithm for image mosaics[J].Journal of Computer-Aided Design&amp;Computer Graphics,2003,15(1):1362-1365.)
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition with local binary patterns">

                                <b>[15]</b>AHONEN T,HADID A,PIETIKINEN M.Face recognition with local binary patterns[C]//Proceedings of the 2004 European Conference on Computer Vision,LNCS 3021.Berlin:Springer,2004:469-481.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image alignment by parameter hypersurface learning">

                                <b>[16]</b>CHOI H C,AHN B H.Image alignment by parameter hypersurface learning[J].Electronics Letters,2016,52(18):1526-1527.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computational approach to edge detection">

                                <b>[17]</b>CANNY J.A computational approach to edge detection[J].IEEETransactions on Pattern Analysis and Machine Intelligence,1986,8(6):679-698.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image seamless stitching and straightening based on image block">

                                <b>[18]</b>QU Z,WANG T,AN S Q,et al.Image seamless stitching and straightening based on image block[J].IET Image Processing,2018,12(8):1361-1369.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An algorithm of image mosaic based on binary tree and eliminating distortion error">

                                <b>[19]</b>QU Z,WEI X,CHEN S.An algorithm of image mosaic based on binary tree and eliminating distortion error[J].PLo S One,2019,14(1):e0210354.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201402014&amp;v=MTM5MDZxZlp1WnNGeWprVjd2UEtDTGZZYkc0SDlYTXJZOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b>张小利,李雄飞,李军.融合图像质量评价指标的相关性分析及性能评估[J].自动化学报,2014,40(2):306-315.(ZHANG X L,LI X F,LI J.Validation and correlation analysis of metrics for evaluating performance of image fusion[J].Acta Automatica Sinica,2014,40(2):306-315.)
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201601013&amp;v=MjMwODJDVVI3cWZadVpzRnlqa1Y3dlBQeXJmYkxHNEg5Zk1ybzlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>黄学优,张长江.双树复小波域的MRI图像去噪[J].中国图象图形学报,2016,21(1):104-113.(HUANG X Y,ZHANG C J.MRI denoising based on dual-tree complex wavelet transform[J].Journal of Image and Graphics,2016,21(1):104-113.)
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201910043" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201910043&amp;v=MTIyMDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqa1Y3dlBMejdCZDdHNEg5ak5yNDlCWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
