<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136466563877500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909015%26RESULT%3d1%26SIGN%3dAdznd12%252fMW4JCwOXFE5uUK2rbV0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909015&amp;v=MzA1NTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXJ2SUx6N0JkN0c0SDlqTXBvOUVZWVE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#81" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#86" data-title="1 实时人脸表情识别系统概述 ">1 实时人脸表情识别系统概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="2 快速稳定的人脸检测 ">2 快速稳定的人脸检测</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="2.1 MSSD&lt;b&gt;人脸检测网络&lt;/b&gt;">2.1 MSSD<b>人脸检测网络</b></a></li>
                                                <li><a href="#94" data-title="2.2 &lt;b&gt;结合跟踪模型的人脸检测&lt;/b&gt;">2.2 <b>结合跟踪模型的人脸检测</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#97" data-title="3 多尺度核特征人脸表情识别网络 ">3 多尺度核特征人脸表情识别网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#98" data-title="3.1 &lt;b&gt;深度可分离卷积&lt;/b&gt;">3.1 <b>深度可分离卷积</b></a></li>
                                                <li><a href="#109" data-title="3.2 &lt;b&gt;多尺度核卷积单元&lt;/b&gt;">3.2 <b>多尺度核卷积单元</b></a></li>
                                                <li><a href="#123" data-title="3.3 &lt;b&gt;多尺度核特征网络&lt;/b&gt;">3.3 <b>多尺度核特征网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="4 实验结果及分析 ">4 实验结果及分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#129" data-title="4.1 &lt;b&gt;数据集&lt;/b&gt;">4.1 <b>数据集</b></a></li>
                                                <li><a href="#134" data-title="4.2 &lt;b&gt;数据增强&lt;/b&gt;">4.2 <b>数据增强</b></a></li>
                                                <li><a href="#137" data-title="4.3 &lt;b&gt;人脸检测实验结果&lt;/b&gt;">4.3 <b>人脸检测实验结果</b></a></li>
                                                <li><a href="#142" data-title="4.4 &lt;b&gt;人脸表情识别实验结果&lt;/b&gt;">4.4 <b>人脸表情识别实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#151" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="图1 实时人脸表情识别系统总体流程">图1 实时人脸表情识别系统总体流程</a></li>
                                                <li><a href="#93" data-title="图2 MSSD网络模型">图2 MSSD网络模型</a></li>
                                                <li><a href="#96" data-title="图3 结合跟踪的人脸检测流程">图3 结合跟踪的人脸检测流程</a></li>
                                                <li><a href="#100" data-title="图4 两种卷积核对比">图4 两种卷积核对比</a></li>
                                                <li><a href="#111" data-title="图5 改进的线性瓶颈层">图5 改进的线性瓶颈层</a></li>
                                                <li><a href="#113" data-title="图6 多尺度核卷积单元">图6 多尺度核卷积单元</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表&lt;/b&gt;1 FER-2013&lt;b&gt;上的多尺度核特征有效性评估&lt;/b&gt;"><b>表</b>1 FER-2013<b>上的多尺度核特征有效性评估</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同激活函数在&lt;/b&gt;FER-2013&lt;b&gt;上的识别率&lt;/b&gt;"><b>表</b>2 <b>不同激活函数在</b>FER-2013<b>上的识别率</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;多尺度核特征网络结构&lt;/b&gt;"><b>表</b>3 <b>多尺度核特征网络结构</b></a></li>
                                                <li><a href="#136" data-title="图7 数据增强效果">图7 数据增强效果</a></li>
                                                <li><a href="#139" data-title="图8 WIDER FACE测试结果示例">图8 WIDER FACE测试结果示例</a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同方法人脸检测速度对比&lt;/b&gt;"><b>表</b>4 <b>不同方法人脸检测速度对比</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;不同数据集上的识别率对比&lt;/b&gt;"><b>表</b>5 <b>不同数据集上的识别率对比</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;表&lt;/b&gt;6 FER-2013&lt;b&gt;识别结果混淆矩阵&lt;/b&gt;"><b>表</b>6 FER-2013<b>识别结果混淆矩阵</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表&lt;/b&gt;7 CK+&lt;b&gt;识别结果混淆矩阵&lt;/b&gt;"><b>表</b>7 CK+<b>识别结果混淆矩阵</b></a></li>
                                                <li><a href="#150" data-title="图9 FER-2013中的易混表情对比">图9 FER-2013中的易混表情对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="186">


                                    <a id="bibliography_1" title=" EKMAN P.Contacts across cultures in the face and emotion [J].Journal of Personality and Social Psychology,1971,17(2):124-129." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Constants across cultures in the face and emotion">
                                        <b>[1]</b>
                                         EKMAN P.Contacts across cultures in the face and emotion [J].Journal of Personality and Social Psychology,1971,17(2):124-129.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_2" title=" ZHAO X,ZHANG S.Facial expression recognition based on local binary patterns and kernel discriminant isomap [J].Sensors,2011,11(10):9573-9588." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition based on local binary patterns and kernel discriminant isomap">
                                        <b>[2]</b>
                                         ZHAO X,ZHANG S.Facial expression recognition based on local binary patterns and kernel discriminant isomap [J].Sensors,2011,11(10):9573-9588.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_3" title=" KUMAR P,HAPPY S L,ROUTRAY A.A real-time robust facial expression recognition system using HOG features [C]// CAST 2016:Proceedings of the 2016 International Conference on Computing,Analytics and Security Trends.Piscataway,NJ:IEEE,2016:289-293." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A real-time robust facial expression recognition system using HOG features">
                                        <b>[3]</b>
                                         KUMAR P,HAPPY S L,ROUTRAY A.A real-time robust facial expression recognition system using HOG features [C]// CAST 2016:Proceedings of the 2016 International Conference on Computing,Analytics and Security Trends.Piscataway,NJ:IEEE,2016:289-293.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_4" title=" 刘帅师,田彦涛,万川.基于Gabor多方向特征融合与分块直方图的人脸表情识别方法[J].自动化学报,2011,37(12):1455-1463.(LIU S S,TIAN Y T,WAN C.Facial expression recognition method based on gabor multi-orientation features fusion and block histogram [J].Acta Automatica Sinica,2011,37(12):1455-1463.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201112006&amp;v=MDg1OTNZOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXJ2TEtDTGZZYkc0SDlETnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         刘帅师,田彦涛,万川.基于Gabor多方向特征融合与分块直方图的人脸表情识别方法[J].自动化学报,2011,37(12):1455-1463.(LIU S S,TIAN Y T,WAN C.Facial expression recognition method based on gabor multi-orientation features fusion and block histogram [J].Acta Automatica Sinica,2011,37(12):1455-1463.)
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_5" title=" BERRETTI S,del BIMBO A,PALA P,et al.A set of selected SIFT features for 3D facial expression recognition [C]// ICPR 2010:Proceedings of the 2010 20th International Conference on Pattern Recognition.Piscataway,NJ:IEEE,2010:4125-4128." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A set of selected SIFT features for3D facial expression recognition">
                                        <b>[5]</b>
                                         BERRETTI S,del BIMBO A,PALA P,et al.A set of selected SIFT features for 3D facial expression recognition [C]// ICPR 2010:Proceedings of the 2010 20th International Conference on Pattern Recognition.Piscataway,NJ:IEEE,2010:4125-4128.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_6" title=" CHEON Y,KIM D.Natural facial expression recognition using differential-AAM and manifold learning [J].Pattern Recognition,2009,42(7):1340-1350." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738893&amp;v=MDU5MTA0VmJoRT1OaWZPZmJLN0h0RE5xWTlGWStnSEJIVTZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJRg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         CHEON Y,KIM D.Natural facial expression recognition using differential-AAM and manifold learning [J].Pattern Recognition,2009,42(7):1340-1350.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_7" title=" 尹星云,王洵,董兰芳,等.用隐马尔可夫模型设计人脸表情识别系统[J].电子科技大学学报,2003,32(6):725-728.(YIN X Y,WANG X,DONG L F,et al.Design of recognition for facial expression by hidden markov model [J].Journal of University of Electronic Science and Technology of China,2003,32(6):725-728.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DKDX200306037&amp;v=MTM2MTFzRnlqbFVydkxJU2JQZHJHNEh0TE1xWTlHWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         尹星云,王洵,董兰芳,等.用隐马尔可夫模型设计人脸表情识别系统[J].电子科技大学学报,2003,32(6):725-728.(YIN X Y,WANG X,DONG L F,et al.Design of recognition for facial expression by hidden markov model [J].Journal of University of Electronic Science and Technology of China,2003,32(6):725-728.)
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_8" title=" VAPNIK V N,LERNER A Y.Recognition of patterns with help of generalized portraits [J].Avtomatika I Telemekhanika,1963,24(6):774-780." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recognition of Patterns with help of Generalized Portraits">
                                        <b>[8]</b>
                                         VAPNIK V N,LERNER A Y.Recognition of patterns with help of generalized portraits [J].Avtomatika I Telemekhanika,1963,24(6):774-780.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     ROWEIS S T.Nonlinear dimensionality reduction by locally linear embedding [J].Science,2000,290(5500):2323-2326.</a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_10" title=" HART P E.The condensed nearest neighbor rule [J].IEEE Transactions on Information Theory,1968,14(3):515-516." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The condensed nearest neighbor rule">
                                        <b>[10]</b>
                                         HART P E.The condensed nearest neighbor rule [J].IEEE Transactions on Information Theory,1968,14(3):515-516.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_11" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks [C]// NIPS &#39;12:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[11]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks [C]// NIPS &#39;12:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_12" title=" LYONS M J,AKAMATSU S,KAMACHI M G,et al.Coding facial expressions with Gabor wavelets[C]// AFGR 1998:Proceedings of the 3rd IEEE International Conference on Automatic Face and Gesture Recognition.Piscataway,NJ:IEEE,1998:200-205." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coding facial expressions with Gabor wavelets">
                                        <b>[12]</b>
                                         LYONS M J,AKAMATSU S,KAMACHI M G,et al.Coding facial expressions with Gabor wavelets[C]// AFGR 1998:Proceedings of the 3rd IEEE International Conference on Automatic Face and Gesture Recognition.Piscataway,NJ:IEEE,1998:200-205.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_13" title=" LUCEY P,COHN J F,KANADE T,et al.The extended Cohn-Kanade dataset (CK+):a complete dataset for action unit and emotion-specified expression [C]// CVPRW 2010:Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2010:94-101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The extended cohn-kanade dataset(CK+):A complete dataset for action unit and emotion-specified expression">
                                        <b>[13]</b>
                                         LUCEY P,COHN J F,KANADE T,et al.The extended Cohn-Kanade dataset (CK+):a complete dataset for action unit and emotion-specified expression [C]// CVPRW 2010:Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2010:94-101.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_14" title=" GOODFELLOW I J,ERHAN D,CARRIER P L,et al.Challenges in representation learning:a report on three machine learning contests [J].Neural Networks,2013,64:59-63." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF26D71F1425206A2ABE3B94D4D9CA9E2&amp;v=MDM2MzdPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMaSt4S3M9TmlmT2ZjVzZHS1hMcnZsRVlPa0tEbncvdmhSaW1FcCtPbmJtMkJaQmNNSGxUTStkQw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         GOODFELLOW I J,ERHAN D,CARRIER P L,et al.Challenges in representation learning:a report on three machine learning contests [J].Neural Networks,2013,64:59-63.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_15" title=" DHALL A,GOECKE R,LUCEY S,et al.Static facial expression analysis in tough conditions:data,evaluation protocol and benchmark [C]// ICCVW 2011:Proceedings of the 2011 IEEE International Conference on Computer Vision Workshops.Piscataway,NJ:IEEE,2011:2106-2112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Static facial expression analysis in tough conditions:data,evaluation protocol and benchmark">
                                        <b>[15]</b>
                                         DHALL A,GOECKE R,LUCEY S,et al.Static facial expression analysis in tough conditions:data,evaluation protocol and benchmark [C]// ICCVW 2011:Proceedings of the 2011 IEEE International Conference on Computer Vision Workshops.Piscataway,NJ:IEEE,2011:2106-2112.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_16" title=" TANG Y.Deep learning using linear support vector machines [EB/OL].[2018- 12- 21].https://arxiv.org/pdf/1306.0239.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning using linear support vector machines">
                                        <b>[16]</b>
                                         TANG Y.Deep learning using linear support vector machines [EB/OL].[2018- 12- 21].https://arxiv.org/pdf/1306.0239.pdf.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_17" title=" AL-SHABI M,CHEAH W P,CONNIE T.Facial expression recognition using a hybrid CNN-SIFT aggregator [EB/OL].[2018- 08- 17].https://arxiv.org/ftp/arxiv/papers/1608/1608.02833.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition using a hybrid CNN-SIFT aggregator">
                                        <b>[17]</b>
                                         AL-SHABI M,CHEAH W P,CONNIE T.Facial expression recognition using a hybrid CNN-SIFT aggregator [EB/OL].[2018- 08- 17].https://arxiv.org/ftp/arxiv/papers/1608/1608.02833.pdf.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_18" title=" FANG H,PARTHAL&#193;IN N M,AUBREY A J,et al.Facial expression recognition in dynamic sequences:an integrated approach [J].Pattern Recognition,2014,47(3):1271-1281." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200017901&amp;v=MDcwODE5UE5yWTlGWk9vSUJYdzRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJRjRWYmhFPU5pZk9mYks4SA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         FANG H,PARTHAL&#193;IN N M,AUBREY A J,et al.Facial expression recognition in dynamic sequences:an integrated approach [J].Pattern Recognition,2014,47(3):1271-1281.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_19" title=" JEON J,PARK J-C,JO Y J,et al.A real-time facial expression recognizer using deep neural network [C]// IMCOM &#39;16:Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication.New York:ACM,2016:Article No.94." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Real-Time Facial Expression Recognizer using Deep Neural Network">
                                        <b>[19]</b>
                                         JEON J,PARK J-C,JO Y J,et al.A real-time facial expression recognizer using deep neural network [C]// IMCOM &#39;16:Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication.New York:ACM,2016:Article No.94.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_20" title=" NEHAL O,NOHA A,FAYEZ W.Intelligent real-time facial expression recognition from video sequences based on hybrid feature tracking algorithms [J].International Journal of Advanced Computer Science and Applications,2017,8(1):245-260." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJSI&amp;filename=SJSI05D893B4348F5CD10FA7FC734EBF9907&amp;v=MjEwNjMreEtzPU5pZllaN085YXRuRnJQMUJaKzhIZW5sS3V4Y1RuRTU2UGd6bHJ4WkFDOFNkVExxWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         NEHAL O,NOHA A,FAYEZ W.Intelligent real-time facial expression recognition from video sequences based on hybrid feature tracking algorithms [J].International Journal of Advanced Computer Science and Applications,2017,8(1):245-260.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_21" title=" LIU W,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9905.Berlin:Springer,2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ssd:Single shot multibox detector">
                                        <b>[21]</b>
                                         LIU W,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9905.Berlin:Springer,2016:21-37.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                     HENRIQUES J F,CASEIRO R,MARTINS,et al.High-speed tracking with kernelized correlation filters [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):583-596.</a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_23" title=" SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[23]</b>
                                         SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_24" title=" HOWARD A G,ZHU M,CHEN B.et al.MobileNets:efficient convolutional neural networks for mobile vision applications [EB/OL].[2018- 12- 17].https://arxiv.org/pdf/1704.04861.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobile Nets:Efficient convolutional neural networks for mobile vision applications">
                                        <b>[24]</b>
                                         HOWARD A G,ZHU M,CHEN B.et al.MobileNets:efficient convolutional neural networks for mobile vision applications [EB/OL].[2018- 12- 17].https://arxiv.org/pdf/1704.04861.pdf.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_25" title=" SANDLER M,HOWARD A,ZHU M,et al.Inverted residuals and linear bottlenecks:mobile networks for classification,detection and segmentation [EB/OL].[2018- 12- 16].https://arxiv.org/pdf/1801.04381v2.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverted residuals and linear bottlenecks:mobile networks for classification,detection and segmentation">
                                        <b>[25]</b>
                                         SANDLER M,HOWARD A,ZHU M,et al.Inverted residuals and linear bottlenecks:mobile networks for classification,detection and segmentation [EB/OL].[2018- 12- 16].https://arxiv.org/pdf/1801.04381v2.pdf.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_26" title=" HE K,ZHANG X,REN S,et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification [EB/OL].[2018- 12- 06].https://arxiv.org/pdf/1502.01852.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Delving deep into rectifiers:surpassing human-level performance on ImageNet classification">
                                        <b>[26]</b>
                                         HE K,ZHANG X,REN S,et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification [EB/OL].[2018- 12- 06].https://arxiv.org/pdf/1502.01852.pdf.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_27" title=" JARRETT K,KAVUKCUOGLU K,RANZATO M,et al.What is the best multi-stage architecture for object recognition?[C]// ICCV 2009:Proceedings of the IEEE 12th International Conference on Computer Vision.Piscataway,NJ:IEEE,2009:2146-2153." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What is the best multi-stage architecture for object recognition?">
                                        <b>[27]</b>
                                         JARRETT K,KAVUKCUOGLU K,RANZATO M,et al.What is the best multi-stage architecture for object recognition?[C]// ICCV 2009:Proceedings of the IEEE 12th International Conference on Computer Vision.Piscataway,NJ:IEEE,2009:2146-2153.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_28" title=" LIEW S S,KHALIL-HANI M,BAKHTERI R.Bounded activation functions for enhanced training stability of deep neural networks on visual pattern recognition problems [J].Neurocomputing,2016,216(C):718-734." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1F6DF12D7DDA6D97BE8878905C9F0873&amp;v=MjIzODZkMVQzZnJyQmRHY01TVVRiMmNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TGkreEtzPU5pZk9mYkxPR0tXNnJvMHhZNTk3ZlhwTnhoRmhueg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         LIEW S S,KHALIL-HANI M,BAKHTERI R.Bounded activation functions for enhanced training stability of deep neural networks on visual pattern recognition problems [J].Neurocomputing,2016,216(C):718-734.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_29" title=" DJORK-ARN&#201; C,UNTERTHINER T,HOCHREITER S.Fast and accurate deep network learning by Exponential Linear Units (ELUs) [EB/OL].[2019- 01- 22].https://arxiv.org/pdf/1511.07289.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [C/OL]">
                                        <b>[29]</b>
                                         DJORK-ARN&#201; C,UNTERTHINER T,HOCHREITER S.Fast and accurate deep network learning by Exponential Linear Units (ELUs) [EB/OL].[2019- 01- 22].https://arxiv.org/pdf/1511.07289.pdf.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_30" title=" YANG S,LUO P,LOY C C,et al.WIDER FACE:a face detection benchmark [C]// CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:5525-5533." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=WIDER FACE: A Face Detection Benchmark">
                                        <b>[30]</b>
                                         YANG S,LUO P,LOY C C,et al.WIDER FACE:a face detection benchmark [C]// CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:5525-5533.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_31" title=" DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database [C]// CVPR 2009:Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:a large-scale hierarchical image database">
                                        <b>[31]</b>
                                         DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database [C]// CVPR 2009:Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2009:248-255.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_32" title=" YANG S,LUO P,LOY C C,et al.From facial parts responses to face detection:a deep learning approach [C]// ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway,NJ:IEEE,2015:3676-3684." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=From Facial Parts Responses to Face Detection:A Deep Learning Approach">
                                        <b>[32]</b>
                                         YANG S,LUO P,LOY C C,et al.From facial parts responses to face detection:a deep learning approach [C]// ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway,NJ:IEEE,2015:3676-3684.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_33" title=" ZHANG K,ZHANG Z,LI Z,et al.Joint face detection and alignment using multitask cascaded convolutional networks [J].IEEE Signal Processing Letters,2016,23(10):1499-1503." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks">
                                        <b>[33]</b>
                                         ZHANG K,ZHANG Z,LI Z,et al.Joint face detection and alignment using multitask cascaded convolutional networks [J].IEEE Signal Processing Letters,2016,23(10):1499-1503.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_34" title=" SZEGEDY C,IOFFE S,VANHOUCKE V,et al.Inception-v4,Inception-ResNet and the impact of residual connections on learning [C]// AAAI 2017:Proceedings of the 31st AAAI Conference on Artificial Intelligence.Menlo Park,CA:AAAI Press,2017:23-38." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning">
                                        <b>[34]</b>
                                         SZEGEDY C,IOFFE S,VANHOUCKE V,et al.Inception-v4,Inception-ResNet and the impact of residual connections on learning [C]// AAAI 2017:Proceedings of the 31st AAAI Conference on Artificial Intelligence.Menlo Park,CA:AAAI Press,2017:23-38.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_35" title=" GUO Y,TAO D,YU J,et al.Deep neural networks with relativity learning for facial expression recognition [C]// ICMEW 2016:Proceedings of the 2016 IEEE International Conference on Multimedia and Expo Workshops.Piscataway,NJ:IEEE,2016:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks with Relativity Learning for facial expression recognition">
                                        <b>[35]</b>
                                         GUO Y,TAO D,YU J,et al.Deep neural networks with relativity learning for facial expression recognition [C]// ICMEW 2016:Proceedings of the 2016 IEEE International Conference on Multimedia and Expo Workshops.Piscataway,NJ:IEEE,2016:1-6.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_36" title=" YAN J,ZHENG W,CUI Z,et al.A joint convolutional bidirectional LSTM framework for facial expression recognition [J].IEICE Transactions on Information and Systems,2018,101(4):1217-1220." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A joint convolutional bidirectional LSTM framework for facial expression recognition">
                                        <b>[36]</b>
                                         YAN J,ZHENG W,CUI Z,et al.A joint convolutional bidirectional LSTM framework for facial expression recognition [J].IEICE Transactions on Information and Systems,2018,101(4):1217-1220.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_37" title=" FERNANDEZ P D M,PE&#209;A F A G,REN T I,et al.FERAtt:facial expression recognition with attention net [EB/OL].[2019- 02- 08].https://arxiv.org/pdf/1902.03284.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FERAtt:facial expression recognition with attention net">
                                        <b>[37]</b>
                                         FERNANDEZ P D M,PE&#209;A F A G,REN T I,et al.FERAtt:facial expression recognition with attention net [EB/OL].[2019- 02- 08].https://arxiv.org/pdf/1902.03284.pdf.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_38" title=" SONG X,BAO H.Facial expression recognition based on video [C]// AIPR 2017:Proceedings of the 2016 IEEE Applied Imagery Pattern Recognition Workshop.Washington,DC:IEEE Computer Society,2016,1:1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition based on video">
                                        <b>[38]</b>
                                         SONG X,BAO H.Facial expression recognition based on video [C]// AIPR 2017:Proceedings of the 2016 IEEE Applied Imagery Pattern Recognition Workshop.Washington,DC:IEEE Computer Society,2016,1:1-5.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_39" title=" ZHANG K,HUANG Y,DU Y,et al.Facial expression recognition based on deep evolutional spatial-temporal networks [J].IEEE Transactions on Image Processing,2017,26(9):4193-4203." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition based on deep evolutional spatial-temporal networks">
                                        <b>[39]</b>
                                         ZHANG K,HUANG Y,DU Y,et al.Facial expression recognition based on deep evolutional spatial-temporal networks [J].IEEE Transactions on Image Processing,2017,26(9):4193-4203.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-06-13 12:51</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2568-2574 DOI:10.11772/j.issn.1001-9081.2019030540            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多尺度核特征卷积神经网络的实时人脸表情识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%97%BB%E6%8B%A9&amp;code=39651662&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李旻择</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%B0%8F%E9%9C%9E&amp;code=14182157&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李小霞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AD%A6%E6%B8%8A&amp;code=10224596&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王学渊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E7%BB%B4&amp;code=42779641&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙维</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0189969&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南科技大学信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%89%B9%E6%AE%8A%E7%8E%AF%E5%A2%83%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%9B%9B%E5%B7%9D%E7%9C%81%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E8%A5%BF%E5%8D%97%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特殊环境机器人技术四川省重点实验室(西南科技大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对人脸表情识别的泛化能力不足、稳定性差以及速度慢难以满足实时性要求的问题,提出了一种基于多尺度核特征卷积神经网络的实时人脸表情识别方法。首先,提出改进的MobileNet结合单发多盒检测器(MSSD)轻量化人脸检测网络,并利用核相关滤波(KCF)模型对检测到的人脸坐标信息进行跟踪来提高检测速度和稳定性;然后,使用三种不同尺度卷积核的线性瓶颈层构成三条支路,用通道合并的特征融合方式形成多尺度核卷积单元,利用其多样性特征来提高表情识别的精度;最后,为了提升模型泛化能力和防止过拟合,采用不同的线性变换方式进行数据增强来扩充数据集,并将FER-2013人脸表情数据集上训练得到的模型迁移到小样本CK+数据集上进行再训练。实验结果表明,所提方法在FER-2013数据集上的识别率达到73.0%,较Kaggle表情识别挑战赛冠军提高了1.8%,在CK+数据集上的识别率高达99.5%。对于640×480的视频,人脸检测速度达到每秒158帧,是主流人脸检测网络多任务级联卷积神经网络(MTCNN)的6.3倍,同时人脸检测和表情识别整体速度达到每秒78帧。因此所提方法能够实现快速精确的人脸表情识别。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸表情识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%B8%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">核相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李旻择(1992—),男,四川南充人,硕士研究生,CCF会员,主要研究方向:深度学习、计算机视觉;;
                                </span>
                                <span>
                                    *李小霞(1976—),女,四川安岳人,教授,博士,主要研究方向:模式识别、计算机视觉;电子邮箱664368504@qq.com;
                                </span>
                                <span>
                                    王学渊(1974—),男,四川绵阳人,副教授,博士,主要研究方向:图像处理;;
                                </span>
                                <span>
                                    孙维(1995—),男,四川达州人,硕士研究生,主要研究方向:图像处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61771411);</span>
                                <span>四川省科技计划项目(2019YJ0449);</span>
                                <span>西南科技大学研究生创新基金资助项目(18ycx123);</span>
                    </p>
            </div>
                    <h1><b>Real-time facial expression recognition based on convolutional neural network with multi-scale kernel feature</b></h1>
                    <h2>
                    <span>LI Minze</span>
                    <span>LI Xiaoxia</span>
                    <span>WANG Xueyuan</span>
                    <span>SUN Wei</span>
            </h2>
                    <h2>
                    <span>School of Information Engineering, Southwest University of Science and Technology</span>
                    <span>Key Laboratory of Special Environmental Robotics in Sichuan Province ( Southwest University of Science and Technology)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problems of insufficient generalization ability, poor stability and difficulty in meeting the real-time requirement of facial expression recognition, a real-time facial expression recognition method based on multi-scale kernel feature convolutional neural network was proposed. Firstly, an improved MSSD(MobileNet+Single Shot multiBox Detector) lightweight face detection network was proposed, and the detected face coordinates information was tracked by Kernel Correlation Filter(KCF) model to improve the detection speed and stability. Then, three linear bottlenecks of three different scale convolution kernels were used to form three branches. The multi-scale kernel convolution unit was formed by the feature fusion of channel combination, and the diversity feature was used to improve the accuracy of expression recognition. Finally, in order to improve the generalization ability of the model and prevent over-fitting, different linear transformation methods were used for data enhancement to augment the dataset, and the model trained on the FER-2013 facial expression dataset was migrated to the small sample CK+ dataset for retraining. The experimental results show that the recognition rate of the proposed method on the FER-2013 dataset reaches 73.0%, which is 1.8% higher than that of the Kaggle Expression Recognition Challenge champion, and the recognition rate of the proposed method on the CK+ dataset reaches 99.5%. For 640×480 video, the face detection speed of the proposed method reaches 158 frames per second, which is 6.3 times of that of the mainstream face detection network MTCNN(MultiTask Cascaded Convolutional Neural Network). At the same time, the overall speed of face detection and expression recognition of the proposed method reaches 78 frames per second. It can be seen that the proposed method can achieve fast and accurate facial expression recognition.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Facial%20Expression%20Recognition(FER)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Facial Expression Recognition(FER);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Kernel%20Correlation%20Filter(KCF)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Kernel Correlation Filter(KCF);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=transfer%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">transfer learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Minze, born in 1992, M. S. candidate. His research interests include deep learning, computer vision. ;
                                </span>
                                <span>
                                    LI Xiaoxia, born in 1976, Ph. D. , professor. Her research interests include pattern recognition, computer vision. ;
                                </span>
                                <span>
                                    WANG Xueyuan, born in 1974, Ph. D. , associate professor. His research interests include image processing. ;
                                </span>
                                <span>
                                    SUN Wei, born in 1995, M. S. candidate. His research interests include image processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-03</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61771411);</span>
                                <span>the Sichuan Science and Technology Project(2019YJ0449);</span>
                                <span>the Graduate Innovation Fund of Southwest University of Science and Technology(18ycx123);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="81" name="81" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="82">人的脸部表情不仅可以传达人的情绪,而且还能够传递人的丰富情感信息。在当下的人工智能时代,人机交互在日常生活中越来越普及,想要让机器更好地理解人类,人脸表情的识别是必不可少的途径。人脸表情识别(Facial Expression Recognition, FER)是计算机视觉、人工智能等领域的重要研究方向,它是一个有趣且具有挑战性的问题,在教育、医疗、心理分析等领域均具有重要的研究价值与意义。</p>
                </div>
                <div class="p1">
                    <p id="83">1971年,著名的心理学家Ekman<citation id="264" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>将人脸部表情划分为六类基本表情:愤怒、厌恶、恐惧、高兴、悲伤和惊讶,并提出表情可以通过观察面部信号来识别。例如“高兴”是看到令人高兴的事情或者听到好的消息,一般会通过嘴角抬高、眼睛变小来表达。此后的研究均是在这六种基本表情基础上展开的,用于人脸表情识别的各种特征提取算法和分类器被相继开发出来。典型的表情特征提取方法有局部二值模式(Local Binary Pattern, LBP)<citation id="265" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、方向梯度直方图(Histograms of Oriented Gradients, HOG)<citation id="266" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Gabor小波变换<citation id="267" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、尺度不变的特征变换(Scale Invariant Feature Transform, SIFT)<citation id="268" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、主动外观模型(Active Appearance Model, AAM)<citation id="269" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等,典型的表情分类方法有隐马尔可夫模型(Hidden Markov Model, HMM)法<citation id="270" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、支持向量机(Support Vector Machine, SVM)<citation id="271" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、局部线性嵌入(Local Linear Embedding, LLE)<citation id="272" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、<i>K</i>最近邻(<i>K</i>-Nearest Neighbors, <i>K</i>NN)算法<citation id="273" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>等。这些研究大多都是人工提取特征,因此效果优劣依赖于前期的特征提取,人为干扰因素较多,且泛化能力不足。</p>
                </div>
                <div class="p1">
                    <p id="84">2012年,Krizhevsky等<citation id="274" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>在ILSVRC-2012中使用AlexNet卷积神经网络(Convolutional Neural Network, CNN)取得了惊人成绩,其识别率远超其他人工特征的传统方法。随后深度神经网络使得人脸表情识别得到了进一步的发展,用于人脸表情识别的各种数据集也日益增多,常见的有JAFFE<citation id="275" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、Extensive Cohn-Kanade(CK+)<citation id="276" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、FER-2013<citation id="277" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、SFEW2.0<citation id="278" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>等。2013年,Tang<citation id="279" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出将CNN与SVM相结合,并且放弃了普通CNN所使用的交叉熵损失最小化方法,而是用标准的铰链损失来最小化基于边际的损失。他的方法在私有测试集上实现了71.2%的识别率,获得了FER-2013人脸表情识别挑战赛的冠军。2017年,Al-Shabi等<citation id="280" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>通过合并CNN和SIFT特征,建立了一个混合CNN-SIFT分类器,使得小样本数据也能够有较好的识别效果,在CK+数据集上的识别率达到了99.4%。人脸表情的识别率虽然在逐步升高,但识别速度却很低,想要实际应用还很难实现。2014年,Fang等<citation id="281" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了一种新的人脸表情自动分析框架,选择具有峰值表情的帧来提取突出信息,实现了3.5帧/s(Frame Per Second, FPS)的识别速度。2016年,Jeon等<citation id="282" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>使用HOG特征来检测人脸,CNN来提取特征,在FER-2013数据集上实现了70.7%的识别率,6.5帧/s的识别速度。2017年,Nehal等<citation id="283" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出了一种智能层次支持向量机,用多级的SVM来减少混淆表情间的相互关系,获得了10.8帧/s的识别速度。</p>
                </div>
                <div class="p1">
                    <p id="85">用深度神经网络来进行人脸表情识别,虽然能够减少人为干扰因素、提高稳定性,但是要拥有较高的识别率,网络模型一般都比较大,参数量太大导致速度很慢,使得其难以满足实时性需求。针对这个问题,以单发多盒检测器(Single Shot multibox Detector, SSD)网络<citation id="284" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>为基础,改进的MobileNet-SSD(MSSD)轻量化人脸检测网络被提出,来进行人脸的检测,结合核相关滤波(Kernel Correlation Filter, KCF)算法<citation id="285" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>进行人脸的跟踪,可大幅提高人脸的检测速度并且提高多角度和遮挡的人脸检测的稳定性;然后利用迁移学习方法,将两个数据集进行联合训练,并且使用多尺度核特征CNN来进行人脸表情特征提取与识别,进一步提高识别率;最后将以上两个模型相融合,实现快速精确的实时人脸表情识别。</p>
                </div>
                <h3 id="86" name="86" class="anchor-tag">1 实时人脸表情识别系统概述</h3>
                <div class="p1">
                    <p id="87">一个完整的实时人脸表情识别系统包括:人脸检测与定位、表情特征提取和表情分类。针对实际应用中需要兼顾识别速度与精度的问题,首先将改进的MSSD人脸检测网络和KCF快速跟踪模型相结合,进行人脸目标的快速稳定检测;然后进行人脸的切片,再将人脸切片(即单纯人脸图片)输入已经训练好的多尺度核特征CNN进行表情识别;最后,将以上两个网络进行串联融合,整个过程形成了检测-跟踪-识别模式,构成了一个完整的实时人脸表情识别系统。图1是实时人脸表情识别系统总体流程。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 实时人脸表情识别系统总体流程" src="Detail/GetImg?filename=images/JSJY201909015_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 实时人脸表情识别系统总体流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Overall process of real-time facial expression recognition system</p>

                </div>
                <h3 id="89" name="89" class="anchor-tag">2 快速稳定的人脸检测</h3>
                <h4 class="anchor-tag" id="90" name="90">2.1 MSSD<b>人脸检测网络</b></h4>
                <div class="p1">
                    <p id="91">目标检测网络一般由基础网络进行特征提取,元结构进行分类回归和边界框回归。以SSD目标检测网络为基础,首先将原基础网络VGG-16<citation id="286" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>改为轻量化网络MobileNet<citation id="287" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>,然后将其中的第7个深度可分离卷积层(浅层特征)与最后5层(深层特征)的特征图进行融合,改进为MSSD网络,网络模型如图2所示。MobileNet中最大的亮点就是深度可分离卷积,它由深度卷积和点卷积组成,极大地加快了训练与识别的速度,因此采用深度可分离卷积来构建网络。在MSSD网络中,输入端通过1个卷积核大小为3×3、步长为2的标准卷积层,再经过13个深度可分离卷积层,后面输出端连接了4个卷积核分别为1×1、3×3交替组合的标准卷积层和1个最大池化层,考虑到池化层会损失一部分有效特征,因此在网络的标准卷积层中使用了步长为2的卷积核替代池化层。</p>
                </div>
                <div class="p1">
                    <p id="92">网络浅层特征的感受野较小,拥有更多的细节信息,对小目标的检测更具优势,因此MSSD人脸检测网络采用浅层与深层特征融合的方式。经实验分析,将第7层的浅层特征与深层特征融合时效果最好,因此网络采用第7、15、16、17、18、19层的融合特征。网络先将这六层的特征图分别重新调整为一维向量,再进行串联融合,实现多尺度人脸检测。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 MSSD网络模型" src="Detail/GetImg?filename=images/JSJY201909015_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 MSSD网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 MSSD network model</p>

                </div>
                <h4 class="anchor-tag" id="94" name="94">2.2 <b>结合跟踪模型的人脸检测</b></h4>
                <div class="p1">
                    <p id="95">为了进一步地提高检测速度,将人脸检测网络和跟踪模型相结合,形成检测-跟踪-检测的模式。这样的结合方式不仅有效地提高了人脸检测的速度,还可处理多角度、有遮挡的人脸检测问题。跟踪模型是基于统计学习的跟踪算法KCF,该算法主要使用了轮转矩阵对样本进行采集,然后使用快速傅里叶变换对其进行加速运算,这使得该算法的跟踪效果和速度都大大提升。先利用MSSD模型对人脸进行检测,并进行KCF跟踪模型更新;然后,将检测到的人脸坐标信息输入跟踪模型KCF中,以此作为人脸基础样本框并采用检测1帧跟踪10帧的策略来进行跟踪;最后,为了防止跟踪丢失,再次进行MSSD模型更新,重新对人脸进行检测。图3为结合跟踪的人脸检测流程。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 结合跟踪的人脸检测流程" src="Detail/GetImg?filename=images/JSJY201909015_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 结合跟踪的人脸检测流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Face detection flow chart combined with tracking</p>

                </div>
                <h3 id="97" name="97" class="anchor-tag">3 多尺度核特征人脸表情识别网络</h3>
                <h4 class="anchor-tag" id="98" name="98">3.1 <b>深度可分离卷积</b></h4>
                <div class="p1">
                    <p id="99">Howard等<citation id="288" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>在2017年提出MobileNet,对标准卷积进行了分解,分为了深度卷积和点卷积两个部分,共同构成深度可分离卷积,标准卷积核与深度可分离卷积核的对比如图4所示。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 两种卷积核对比" src="Detail/GetImg?filename=images/JSJY201909015_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 两种卷积核对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of two convolution kernels</p>

                </div>
                <div class="p1">
                    <p id="101">假设输入特征图尺寸为<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>,通道数为<i>M</i>,卷积核大小为<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>,卷积核个数为<i>N</i>。</p>
                </div>
                <div class="p1">
                    <p id="102">对于同样的输入和输出,标准卷积过程计算量为:<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×<i>M</i>×<i>N</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>,深度可分离卷积过程计算量为:<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×1×<i>M</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>+1×1×<i>M</i>×<i>N</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="103">通过以上可知深度可分离卷积方式与标准卷积方式的计算量比例为:</p>
                </div>
                <div class="p1">
                    <p id="104">(<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×1×<i>M</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>+</p>
                </div>
                <div class="p1">
                    <p id="105">1×1×<i>M</i>×<i>N</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>)/</p>
                </div>
                <div class="p1">
                    <p id="106">(<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×<i>M</i>×<i>N</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>)=(1/<i>N</i>)+(1/<i>D</i><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Κ</mi><mn>2</mn></msubsup></mrow></math></mathml>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="108">对于卷积核大小为3×3的卷积过程,计算量可减少至原来1/9。可见这样的结构使其极大地减少了计算量,有效提高了训练与识别的速度。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">3.2 <b>多尺度核卷积单元</b></h4>
                <div class="p1">
                    <p id="110">多尺度核卷积单元主要以深度可分离卷积为基础,分支中采用了MobileNetV2<citation id="289" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>的线性瓶颈层结构并对其进行了改进,将其中的非线性激活函数改为PReLU<citation id="290" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>,图5是改进的线性瓶颈层(bottleneck_p)结构。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 改进的线性瓶颈层" src="Detail/GetImg?filename=images/JSJY201909015_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 改进的线性瓶颈层  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Improved linear bottleneck</p>

                </div>
                <div class="p1">
                    <p id="112">深度卷积(图中为Dw_Conv)作为特征提取部分,点卷积(图中为Conv 1×1)作为瓶颈层进行通道数的缩放,并且输出端的点卷积采用的是线性结构,因为该处点卷积是用于通道数的压缩,若再进行非线性操作,则会损失大量有用特征。图6是多尺度核卷积单元结构图,它包含了三条分支,每个分支均采用步长为2的改进的线性瓶颈层结构。通过三个不同深度卷积核大小的分支并联形成的多尺度核卷积单元,融合了不同卷积核大小提取的多样性特征,进而有效地提高人脸表情的识别率。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 多尺度核卷积单元" src="Detail/GetImg?filename=images/JSJY201909015_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 多尺度核卷积单元  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Multi-scale kernel convolution unit</p>

                </div>
                <div class="p1">
                    <p id="114">为了说明多尺度核特征的有效性以及卷积核大小的选取,用表1所示网络结构进行了10组对比实验。表1是在FER-2013上的多尺度核特征有效性评估结果。实验1是将多尺度核卷积单元改为核大小为3×3的标准卷积进行的实验,实验2～6是将多尺度核卷积单元的三条支路均使用同一大小的卷积核进行的实验,实验7～10是改变多尺度核卷积单元三条支路的卷积核大小进行的实验。实验1～6表明网络使用适当卷积核大小的单一尺度核卷积单元比不使用的识别率更高;实验2～6表明具有单一尺度核卷积单元的网络使用3×3卷积核的效果比其他卷积核大小更好;实验2～10表明除了实验9的情况外,多尺度核卷积单元比单一尺度核卷积单元更有效,同时实验9的情况说明了多尺度核卷积单元的三个卷积核不能都取比较大的尺寸。</p>
                </div>
                <div class="p1">
                    <p id="115">通过以上分析,多尺度核卷积单元的核大小选取了3×3、11×11、19×19三种最优尺度,使用多尺度核卷积比标准卷积的识别率提升了3.2%。</p>
                </div>
                <div class="area_img" id="116">
                                            <p class="img_tit">
                                                <b>表</b>1 FER-2013<b>上的多尺度核特征有效性评估</b>
                                                    <br />
                                                Tab. 1 Effectiveness evaluation of multi-scale kernel feature on FER-2013
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_11600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909015_11600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_11600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 FER-2013上的多尺度核特征有效性评估" src="Detail/GetImg?filename=images/JSJY201909015_11600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="117">在多尺度核卷积单元中,除了用于压缩的点卷积不使用非线性激活函数外,其他卷积层均使用PReLU激活函数。式(2)、式(3)分别是激活函数ReLU<citation id="291" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>和PReLU的表达式,<i>i</i>表示不同通道。</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>R</mtext><mtext>e</mtext><mtext>L</mtext><mtext>U</mtext><mspace width="0.25em" /><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false">)</mo><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mo>,</mo><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Ρ</mtext><mtext>R</mtext><mtext>e</mtext><mtext>L</mtext><mtext>U</mtext><mspace width="0.25em" /><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mo>,</mo><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mo>,</mo><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">ReLU激活函数是将所有负值都设为0,其余保持不变。当训练过程中有较大梯度经过ReLU时,会引起输入数据产生巨大变化,会出现大多数输入是负数的情况,这种情况下会导致神经元永久性失活,梯度永远为0,无法继续进行网络权重的更新。然而在PReLU中修正了数据的分布,使得一部分负值也能够得以保留,很好地解决了ReLU中存在的问题,并且式(3)中的参数<i>a</i><sub><i>i</i></sub>可以通过训练得到,能够根据数据的变化而变化,灵活性与适应性更强。</p>
                </div>
                <div class="p1">
                    <p id="121">通过以上分析,将不同激活函数对多尺度核特征人脸表情识别效果进行了对比,表2是不同激活函数在FER-2013数据集上的识别率,可知使用PReLU比ReLU的识别率高1.8个百分点,因此选择PReLU作为激活函数。</p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表</b>2 <b>不同激活函数在</b>FER-2013<b>上的识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 2 Recognition rate of different activation functions on FER-2013</p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td><br />激活函数</td><td>识别率/%</td><td></td><td>激活函数</td><td>识别率/%</td></tr><tr><td><br />ReLU<sup>[27]</sup></td><td>71.2</td><td rowspan="3"><br /></td><td><br />ELU<sup>[29]</sup></td><td>72.5</td></tr><tr><td><br />LeakyReLU<sup>[28]</sup></td><td>71.4</td><td><br />PReLU<sup>[26]</sup></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>7</mn><mn>3</mn><mo>.</mo><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />ReLU6<sup>[25]</sup></td><td>71.8</td><td><br /></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="123" name="123">3.3 <b>多尺度核特征网络</b></h4>
                <div class="p1">
                    <p id="124">用于人脸表情识别的多尺度核特征网络结构如表3所示。表中multi_conv2d、bottleneck_p(1～5)分别表示3.2节介绍的多尺度核卷积单元和改进的线性瓶颈层。网络的输入首先经过一个多尺度核卷积单元(multi_conv2d),采用6倍的扩张系数,每个分支采用16个卷积核进行卷积,输出通道数为16,步长为2,再将三分支特征进行融合,输出通道数变为48;然后经过12个改进的线性瓶颈层,每层的深度卷积核大小均使用3×3,并且在训练期间进行数据的批量归一化;最后会通过一个卷积核大小为1×1、步长为1的标准卷积层和一个核大小为3×3的平均池化层;输出端的分类器设计采用了全卷积神经网络的分类策略,使用了步长为1、核大小为1×1、输出通道数为7(7类表情)的标准卷积层来替代全连接层,加快表情识别速度。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表</b>3 <b>多尺度核特征网络结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Network structure of multi-scale kernel feature</p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td><br />输入<br />尺寸</td><td>操作</td><td>扩张<br />系数</td><td>输出<br />通道数</td><td>重复<br />次数</td><td>步长</td></tr><tr><td>48×48×1</td><td>multi_conv2d</td><td>6</td><td>48</td><td>1</td><td>2</td></tr><tr><td><br />24×24×48</td><td>bottleneck_p1</td><td>1</td><td>32</td><td>1</td><td>1</td></tr><tr><td><br />24×24×32</td><td>bottleneck_p2</td><td>6</td><td>64</td><td>3</td><td>2</td></tr><tr><td><br />12×12×64</td><td>bottleneck_p3</td><td>6</td><td>96</td><td>4</td><td>2</td></tr><tr><td><br />6×6×96</td><td>bottleneck_p4</td><td>6</td><td>160</td><td>3</td><td>2</td></tr><tr><td><br />3×3×160</td><td>bottleneck_p5</td><td>6</td><td>320</td><td>1</td><td>1</td></tr><tr><td><br />3×3×320</td><td>conv2d 1×1</td><td></td><td>1 280</td><td>1</td><td>1</td></tr><tr><td><br />3×3×1 280</td><td>avg_pool 3×3</td><td></td><td>1 280</td><td>1</td><td></td></tr><tr><td><br />1×1×1 280</td><td>conv2d 1×1</td><td></td><td>7</td><td>1</td><td>1</td></tr><tr><td><br />1×1×7</td><td>Reshape</td><td></td><td>7</td><td>1</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="126" name="126" class="anchor-tag">4 实验结果及分析</h3>
                <div class="p1">
                    <p id="127">实验配置如下:</p>
                </div>
                <div class="p1">
                    <p id="128">中央处理器(Central Processing Unit, CPU):Inter Core i7-7700K,主频为4.20 GHz,内存为16 GB;图像处理器(Graphic Processing Unit, GPU):GeForce GTX 1080Ti,显存为12 GB。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">4.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="130">实验中用到了三种数据集:WIDER FACE<citation id="292" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>、CK+<citation id="293" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和FER-2013<citation id="294" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="131">WIDER FACE数据集为人脸检测基准数据集,共包含了32 203张图像,并对393 703个面部进行了标记,具有不同的尺寸、姿势、遮挡、表情、光照以及化妆的人脸。所有的图像被分为61类,每类随机选择40%作为训练集、10%作为验证集、50%作为测试集,即训练集12 881张、验证集3 220张、测试集16 102张。</p>
                </div>
                <div class="p1">
                    <p id="132">CK+人脸表情数据集包括123个人,593个图像序列,每个图像序列的最后一张都有动作单元标签,而其中327个图像序列有表情标签,被标注为七类表情标签:愤怒、鄙视、厌恶、恐惧、高兴、悲伤和惊讶。但是在其他的表情数据集中没有鄙视这类表情,为了和其他数据集能够相互兼容,因此去掉了鄙视这类表情。</p>
                </div>
                <div class="p1">
                    <p id="133">FER-2013是Kaggle人脸表情识别挑战赛提供的一个人脸表情数据集。该数据集总共包含35 887张表情图像,分为7类基本表情:愤怒、厌恶、恐惧、高兴、悲伤、惊讶和中性。FER2013已被挑战赛举办方分为了三部分:训练集28 709张、公共测试集3 589张和私有测试集3 589张。在训练时将公共测试集作为验证集,私有测试集作为最终指标判断的测试集,该数据集包含了不同年龄、不同角度的人脸表情,并且分辨率也相对较低,很多图片还有手、头发和围巾等的遮挡,非常具有挑战性,很符合真实环境中的条件。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">4.2 <b>数据增强</b></h4>
                <div class="p1">
                    <p id="135">为了增强人脸表情识别模型对噪声和角度变换等干扰的稳定性,对实验数据集进行了数据增强,对每张图像都使用了不同的线性变换方式进行增强,如图7所示。进行数据增强的变换有随机水平翻转、比例为0.1的水平和竖直方向偏移、比例为0.1的随机缩放、在(-10,10)进行随机转动角度、归一化为零均值和单位方差向量,并对变换过程中出现的空白区域按照最近像素点进行填充。</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 数据增强效果" src="Detail/GetImg?filename=images/JSJY201909015_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 数据增强效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Data enhancement effect</p>

                </div>
                <h4 class="anchor-tag" id="137" name="137">4.3 <b>人脸检测实验结果</b></h4>
                <div class="p1">
                    <p id="138">对于结合跟踪的MSSD人脸检测网络,先将MSSD的基础网络MobileNet在ImageNet<citation id="295" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>1000分类的大型图像数据库上进行预训练;然后再将预训练好的模型迁移到MSSD网络中,用人脸检测基准数据库WIDER FACE进行微调;最后用WIDER FACE的测试集进行测试。图8是测试集中部分图片检测结果,可知MSSD人脸检测网络对多尺寸、多角度和遮挡等均具有较好的检测效果,稳定性强。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 WIDER FACE测试结果示例" src="Detail/GetImg?filename=images/JSJY201909015_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 WIDER FACE测试结果示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 WIDER FACE test result examples</p>

                </div>
                <div class="p1">
                    <p id="140">在检测速度方面,使用大小为640×480的视频进行测试,取视频的前3 000帧来计算平均处理速度,并与主流的人脸检测网络模型进行了对比实验。表4是不同方法人脸检测速度对比结果。MSSD网络人脸检测速度为63帧/s,再结合KCF跟踪器,速度可达158帧/s。多任务级联卷积神经网络(MultiTask Cascaded Convolutional Neural Network, MTCNN)是主流的人脸检测网络,本文方法的检测速度是它的6.3倍,优势非常明显。</p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit"><b>表</b>4 <b>不同方法人脸检测速度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Comparison of face detection speeds by different methods</p>
                    <p class="img_note"></p>
                    <table id="141" border="1"><tr><td><br />方法</td><td>平均速度/<br />FPS</td><td></td><td>方法</td><td>平均速度/<br />FPS</td></tr><tr><td>Faceness<sup>[32]</sup></td><td>10</td><td></td><td>MSSD</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mspace width="0.25em" /><mn>6</mn><mn>3</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />MTCNN<sup>[33]</sup></td><td>25</td><td></td><td>MSSD+KCF</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mn>5</mn><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />SSD<sup>[21]</sup></td><td>37</td><td></td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="142" name="142">4.4 <b>人脸表情识别实验结果</b></h4>
                <div class="p1">
                    <p id="143">人脸表情识别实验主要是在FER-2013和CK+两个数据上进行训练和测试,在训练过程中均采用随机初始化权重和偏置,批量大小为16,初始学习率为0.01,并且采用了训练自动停止策略,即出现过拟合现象时,训练经过20个循环后自动停止并保存模型。</p>
                </div>
                <div class="p1">
                    <p id="144">模型训练过程使用FER-2013的训练集(28 709张)进行训练,公共测试集(3 589张)作为验证集来调整模型的权重参数,最后用私有测试集(3 589张)进行最后的测试。然后与目前先进的表情识别网络进行了对比。表5第一部分是不同方法在FER-2013上的识别率对比结果。可知本文方法优于其他主流方法,达到了73.0%的识别率,比Kaggle人脸表情识别挑战赛冠军Tang<citation id="296" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的识别率提高了1.8个百分点,同时识别速度达到了154帧/s。</p>
                </div>
                <div class="p1">
                    <p id="145">在CK+数据集上的实验采用了迁移学习方法,将模型在FER-2013上训练得到的权重参数作为预训练结果,然后在CK+上进行微调,并采用10折交叉验证对模型性能进行评估。表5第二部分是不同方法在CK+数据集上的识别率对比,本文方法取得了99.5%的最高识别率。</p>
                </div>
                <div class="area_img" id="146">
                                            <p class="img_tit">
                                                <b>表</b>5 <b>不同数据集上的识别率对比</b>
                                                    <br />
                                                Tab. 5 Comparison of recognition rate on different datasets
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909015_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表5 不同数据集上的识别率对比" src="Detail/GetImg?filename=images/JSJY201909015_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="147">表6和表7分别是在FER-2013和CK+两个数据集上的识别结果混淆矩阵。在数据集FER-2013中,高兴的识别率最高为90.0%,其次是惊讶和厌恶,对恐惧和悲伤的识别率相对较低。从表7可看出造成这两者识别率较低的原因是这两类表情容易相互混淆。为了更直观地对这两类表情进行分析,图9给出了FER-2013中的恐惧和悲伤两类表情图像,可知在该数据集中恐惧和悲伤两类表情极易混淆,人工都很难进行准确判断。在数据集CK+中,其数据集较小并且没有FER-2013中那么多的标签噪声,同时又全是清晰的正面表情照片,因此本文方法在该数据集中除了厌恶之外的各类表情识别率均为100%,仅将厌恶表情中的3%识别为了愤怒,整体识别率高达99.5%。</p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit"><b>表</b>6 FER-2013<b>识别结果混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 6 Confusion matrix of FER-2013 recognition result</p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td rowspan="2"><br />真实表情<br />类别</td><td colspan="7"><br />预测表情类别</td></tr><tr><td><br />愤怒</td><td>厌恶</td><td>恐惧</td><td>高兴</td><td>悲伤</td><td>惊讶</td><td>中性</td></tr><tr><td><br />愤怒</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.02</td><td>0.10</td><td>0.03</td><td>0.11</td><td>0.02</td><td>0.06</td></tr><tr><td><br />厌恶</td><td>0.11</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>7</mn><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.04</td><td>0.01</td><td>0.02</td><td>0.04</td><td>0.01</td></tr><tr><td><br />恐惧</td><td>0.08</td><td>0.00</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>3</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.02</td><td>0.13</td><td>0.08</td><td>0.05</td></tr><tr><td><br />高兴</td><td>0.01</td><td>0.00</td><td>0.01</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>9</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.02</td><td>0.02</td><td>0.03</td></tr><tr><td><br />悲伤</td><td>0.08</td><td>0.00</td><td>0.14</td><td>0.02</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.01</td><td>0.11</td></tr><tr><td><br />惊讶</td><td>0.02</td><td>0.00</td><td>0.08</td><td>0.04</td><td>0.02</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>8</mn><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.03</td></tr><tr><td><br />中性</td><td>0.07</td><td>0.00</td><td>0.06</td><td>0.06</td><td>0.15</td><td>0.01</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表</b>7 CK+<b>识别结果混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 7 Confusion matrix of CK+ recognition result</p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td rowspan="2"><br />真实表情<br />类别</td><td colspan="6"><br />预测表情类别</td></tr><tr><td><br />愤怒</td><td>厌恶</td><td>恐惧</td><td>高兴</td><td>悲伤</td><td>惊讶</td></tr><tr><td><br />愤怒</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mo>.</mo><mn>0</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td><br />厌恶</td><td>0.03</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>9</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td><br />恐惧</td><td>0.00</td><td>0.00</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mo>.</mo><mn>0</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td><br />高兴</td><td>0.00</td><td>0.00</td><td>0.00</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mo>.</mo><mn>0</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.00</td><td>0.00</td></tr><tr><td><br />悲伤</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mo>.</mo><mn>0</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>0.00</td></tr><tr><td><br />惊讶</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mo>.</mo><mn>0</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909015_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 FER-2013中的易混表情对比" src="Detail/GetImg?filename=images/JSJY201909015_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 FER-2013中的易混表情对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909015_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Confusing expression contrast in FER-2013</p>

                </div>
                <h3 id="151" name="151" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="152">针对人脸表情识别的泛化能力不足、稳定性差以及速度难以达到实时性要求的问题,提出了一种基于多尺度核特征卷积神经网络的实时稳定人脸表情识别方法。用检测加跟踪的模式进行人脸检测,实现了158帧/s的快速稳定人脸检测,而且多尺度核特征表情识别网络在FER-2013和CK+数据集上分别达到了73.0%和99.5%的高识别率。整个系统采用轻量化网络结构,总体处理速度高达78帧/s。精度和速度都能满足实际需求。在后续的研究中,可以利用反卷积等方法可视化各层特征,结合高低层有效特征进一步提高网络的精度。另外,可以采用更加接近真实环境的表情数据集进行训练,并且增加疼痛之类的表情类别,使得理论研究能够与实际相结合,将该方法使用在医疗监护等的实际场景中。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="186">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Constants across cultures in the face and emotion">

                                <b>[1]</b> EKMAN P.Contacts across cultures in the face and emotion [J].Journal of Personality and Social Psychology,1971,17(2):124-129.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition based on local binary patterns and kernel discriminant isomap">

                                <b>[2]</b> ZHAO X,ZHANG S.Facial expression recognition based on local binary patterns and kernel discriminant isomap [J].Sensors,2011,11(10):9573-9588.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A real-time robust facial expression recognition system using HOG features">

                                <b>[3]</b> KUMAR P,HAPPY S L,ROUTRAY A.A real-time robust facial expression recognition system using HOG features [C]// CAST 2016:Proceedings of the 2016 International Conference on Computing,Analytics and Security Trends.Piscataway,NJ:IEEE,2016:289-293.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201112006&amp;v=MTUxMTh2TEtDTGZZYkc0SDlETnJZOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 刘帅师,田彦涛,万川.基于Gabor多方向特征融合与分块直方图的人脸表情识别方法[J].自动化学报,2011,37(12):1455-1463.(LIU S S,TIAN Y T,WAN C.Facial expression recognition method based on gabor multi-orientation features fusion and block histogram [J].Acta Automatica Sinica,2011,37(12):1455-1463.)
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A set of selected SIFT features for3D facial expression recognition">

                                <b>[5]</b> BERRETTI S,del BIMBO A,PALA P,et al.A set of selected SIFT features for 3D facial expression recognition [C]// ICPR 2010:Proceedings of the 2010 20th International Conference on Pattern Recognition.Piscataway,NJ:IEEE,2010:4125-4128.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738893&amp;v=MTEzNDNsVXIzSUlGNFZiaEU9TmlmT2ZiSzdIdEROcVk5RlkrZ0hCSFU2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> CHEON Y,KIM D.Natural facial expression recognition using differential-AAM and manifold learning [J].Pattern Recognition,2009,42(7):1340-1350.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DKDX200306037&amp;v=MjU2MDBGckNVUjdxZlp1WnNGeWpsVXJ2TElTYlBkckc0SHRMTXFZOUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 尹星云,王洵,董兰芳,等.用隐马尔可夫模型设计人脸表情识别系统[J].电子科技大学学报,2003,32(6):725-728.(YIN X Y,WANG X,DONG L F,et al.Design of recognition for facial expression by hidden markov model [J].Journal of University of Electronic Science and Technology of China,2003,32(6):725-728.)
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recognition of Patterns with help of Generalized Portraits">

                                <b>[8]</b> VAPNIK V N,LERNER A Y.Recognition of patterns with help of generalized portraits [J].Avtomatika I Telemekhanika,1963,24(6):774-780.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 ROWEIS S T.Nonlinear dimensionality reduction by locally linear embedding [J].Science,2000,290(5500):2323-2326.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The condensed nearest neighbor rule">

                                <b>[10]</b> HART P E.The condensed nearest neighbor rule [J].IEEE Transactions on Information Theory,1968,14(3):515-516.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[11]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks [C]// NIPS '12:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012:1097-1105.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coding facial expressions with Gabor wavelets">

                                <b>[12]</b> LYONS M J,AKAMATSU S,KAMACHI M G,et al.Coding facial expressions with Gabor wavelets[C]// AFGR 1998:Proceedings of the 3rd IEEE International Conference on Automatic Face and Gesture Recognition.Piscataway,NJ:IEEE,1998:200-205.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The extended cohn-kanade dataset(CK+):A complete dataset for action unit and emotion-specified expression">

                                <b>[13]</b> LUCEY P,COHN J F,KANADE T,et al.The extended Cohn-Kanade dataset (CK+):a complete dataset for action unit and emotion-specified expression [C]// CVPRW 2010:Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2010:94-101.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF26D71F1425206A2ABE3B94D4D9CA9E2&amp;v=MTUyMzVXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRwaHhMaSt4S3M9TmlmT2ZjVzZHS1hMcnZsRVlPa0tEbncvdmhSaW1FcCtPbmJtMkJaQmNNSGxUTStkQ09OdkZTaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> GOODFELLOW I J,ERHAN D,CARRIER P L,et al.Challenges in representation learning:a report on three machine learning contests [J].Neural Networks,2013,64:59-63.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Static facial expression analysis in tough conditions:data,evaluation protocol and benchmark">

                                <b>[15]</b> DHALL A,GOECKE R,LUCEY S,et al.Static facial expression analysis in tough conditions:data,evaluation protocol and benchmark [C]// ICCVW 2011:Proceedings of the 2011 IEEE International Conference on Computer Vision Workshops.Piscataway,NJ:IEEE,2011:2106-2112.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning using linear support vector machines">

                                <b>[16]</b> TANG Y.Deep learning using linear support vector machines [EB/OL].[2018- 12- 21].https://arxiv.org/pdf/1306.0239.pdf.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition using a hybrid CNN-SIFT aggregator">

                                <b>[17]</b> AL-SHABI M,CHEAH W P,CONNIE T.Facial expression recognition using a hybrid CNN-SIFT aggregator [EB/OL].[2018- 08- 17].https://arxiv.org/ftp/arxiv/papers/1608/1608.02833.pdf.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200017901&amp;v=MDU3MzZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyM0lJRjRWYmhFPU5pZk9mYks4SDlQTnJZOUZaT29JQlh3NA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> FANG H,PARTHALÁIN N M,AUBREY A J,et al.Facial expression recognition in dynamic sequences:an integrated approach [J].Pattern Recognition,2014,47(3):1271-1281.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Real-Time Facial Expression Recognizer using Deep Neural Network">

                                <b>[19]</b> JEON J,PARK J-C,JO Y J,et al.A real-time facial expression recognizer using deep neural network [C]// IMCOM '16:Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication.New York:ACM,2016:Article No.94.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJSI&amp;filename=SJSI05D893B4348F5CD10FA7FC734EBF9907&amp;v=MDI1MjNBQzhTZFRMcVlDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TGkreEtzPU5pZllaN085YXRuRnJQMUJaKzhIZW5sS3V4Y1RuRTU2UGd6bHJ4Wg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> NEHAL O,NOHA A,FAYEZ W.Intelligent real-time facial expression recognition from video sequences based on hybrid feature tracking algorithms [J].International Journal of Advanced Computer Science and Applications,2017,8(1):245-260.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ssd:Single shot multibox detector">

                                <b>[21]</b> LIU W,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector [C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9905.Berlin:Springer,2016:21-37.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                 HENRIQUES J F,CASEIRO R,MARTINS,et al.High-speed tracking with kernelized correlation filters [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):583-596.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[23]</b> SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition [EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobile Nets:Efficient convolutional neural networks for mobile vision applications">

                                <b>[24]</b> HOWARD A G,ZHU M,CHEN B.et al.MobileNets:efficient convolutional neural networks for mobile vision applications [EB/OL].[2018- 12- 17].https://arxiv.org/pdf/1704.04861.pdf.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverted residuals and linear bottlenecks:mobile networks for classification,detection and segmentation">

                                <b>[25]</b> SANDLER M,HOWARD A,ZHU M,et al.Inverted residuals and linear bottlenecks:mobile networks for classification,detection and segmentation [EB/OL].[2018- 12- 16].https://arxiv.org/pdf/1801.04381v2.pdf.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Delving deep into rectifiers:surpassing human-level performance on ImageNet classification">

                                <b>[26]</b> HE K,ZHANG X,REN S,et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification [EB/OL].[2018- 12- 06].https://arxiv.org/pdf/1502.01852.pdf.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What is the best multi-stage architecture for object recognition?">

                                <b>[27]</b> JARRETT K,KAVUKCUOGLU K,RANZATO M,et al.What is the best multi-stage architecture for object recognition?[C]// ICCV 2009:Proceedings of the IEEE 12th International Conference on Computer Vision.Piscataway,NJ:IEEE,2009:2146-2153.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1F6DF12D7DDA6D97BE8878905C9F0873&amp;v=MjcwODRyQmRHY01TVVRiMmNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0cGh4TGkreEtzPU5pZk9mYkxPR0tXNnJvMHhZNTk3ZlhwTnhoRmhuemQxVDNmcg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> LIEW S S,KHALIL-HANI M,BAKHTERI R.Bounded activation functions for enhanced training stability of deep neural networks on visual pattern recognition problems [J].Neurocomputing,2016,216(C):718-734.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [C/OL]">

                                <b>[29]</b> DJORK-ARNÉ C,UNTERTHINER T,HOCHREITER S.Fast and accurate deep network learning by Exponential Linear Units (ELUs) [EB/OL].[2019- 01- 22].https://arxiv.org/pdf/1511.07289.pdf.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=WIDER FACE: A Face Detection Benchmark">

                                <b>[30]</b> YANG S,LUO P,LOY C C,et al.WIDER FACE:a face detection benchmark [C]// CVPR 2016:Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:5525-5533.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:a large-scale hierarchical image database">

                                <b>[31]</b> DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database [C]// CVPR 2009:Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2009:248-255.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=From Facial Parts Responses to Face Detection:A Deep Learning Approach">

                                <b>[32]</b> YANG S,LUO P,LOY C C,et al.From facial parts responses to face detection:a deep learning approach [C]// ICCV 2015:Proceedings of the 2015 IEEE International Conference on Computer Vision.Piscataway,NJ:IEEE,2015:3676-3684.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks">

                                <b>[33]</b> ZHANG K,ZHANG Z,LI Z,et al.Joint face detection and alignment using multitask cascaded convolutional networks [J].IEEE Signal Processing Letters,2016,23(10):1499-1503.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning">

                                <b>[34]</b> SZEGEDY C,IOFFE S,VANHOUCKE V,et al.Inception-v4,Inception-ResNet and the impact of residual connections on learning [C]// AAAI 2017:Proceedings of the 31st AAAI Conference on Artificial Intelligence.Menlo Park,CA:AAAI Press,2017:23-38.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks with Relativity Learning for facial expression recognition">

                                <b>[35]</b> GUO Y,TAO D,YU J,et al.Deep neural networks with relativity learning for facial expression recognition [C]// ICMEW 2016:Proceedings of the 2016 IEEE International Conference on Multimedia and Expo Workshops.Piscataway,NJ:IEEE,2016:1-6.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A joint convolutional bidirectional LSTM framework for facial expression recognition">

                                <b>[36]</b> YAN J,ZHENG W,CUI Z,et al.A joint convolutional bidirectional LSTM framework for facial expression recognition [J].IEICE Transactions on Information and Systems,2018,101(4):1217-1220.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FERAtt:facial expression recognition with attention net">

                                <b>[37]</b> FERNANDEZ P D M,PEÑA F A G,REN T I,et al.FERAtt:facial expression recognition with attention net [EB/OL].[2019- 02- 08].https://arxiv.org/pdf/1902.03284.pdf.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition based on video">

                                <b>[38]</b> SONG X,BAO H.Facial expression recognition based on video [C]// AIPR 2017:Proceedings of the 2016 IEEE Applied Imagery Pattern Recognition Workshop.Washington,DC:IEEE Computer Society,2016,1:1-5.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition based on deep evolutional spatial-temporal networks">

                                <b>[39]</b> ZHANG K,HUANG Y,DU Y,et al.Facial expression recognition based on deep evolutional spatial-temporal networks [J].IEEE Transactions on Image Processing,2017,26(9):4193-4203.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909015&amp;v=MzA1NTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeWpsVXJ2SUx6N0JkN0c0SDlqTXBvOUVZWVE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
