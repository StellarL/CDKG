<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136450777940000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJY201911041%26RESULT%3d1%26SIGN%3dqtKnQrX%252bksJ22Wz%252bV69urucrtHw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201911041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911041&amp;v=MjAwMzdyM0tMejdCZDdHNEg5ak5ybzlCWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluZ1Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="1 算法的整体框架 ">1 算法的整体框架</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="2 数据预处理 ">2 数据预处理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="2.1 RGB&lt;b&gt;视频预处理&lt;/b&gt;">2.1 RGB<b>视频预处理</b></a></li>
                                                <li><a href="#48" data-title="2.2 &lt;b&gt;关节点数据预处理&lt;/b&gt;">2.2 <b>关节点数据预处理</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="3 模型结构 ">3 模型结构</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="3.1 &lt;i&gt;CNN&lt;/i&gt;&lt;b&gt;概述&lt;/b&gt;">3.1 <i>CNN</i><b>概述</b></a></li>
                                                <li><a href="#78" data-title="3.2 &lt;i&gt;VGG&lt;/i&gt;&lt;b&gt;网络&lt;/b&gt;">3.2 <i>VGG</i><b>网络</b></a></li>
                                                <li><a href="#81" data-title="3.3 &lt;b&gt;融合结构&lt;/b&gt;">3.3 <b>融合结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="4 实验测试与分析 ">4 实验测试与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#85" data-title="4.1 &lt;b&gt;数据库与测试环境介绍&lt;/b&gt;">4.1 <b>数据库与测试环境介绍</b></a></li>
                                                <li><a href="#89" data-title="4.2 &lt;i&gt;SBU&lt;/i&gt;&lt;b&gt;数据库实验测试结果&lt;/b&gt;">4.2 <i>SBU</i><b>数据库实验测试结果</b></a></li>
                                                <li><a href="#105" data-title="4.3 &lt;i&gt;NTU&lt;/i&gt;&lt;b&gt;数据库实验测试结果&lt;/b&gt;">4.3 <i>NTU</i><b>数据库实验测试结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="5 结语 ">5 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="图1 本文算法整体框架流程">图1 本文算法整体框架流程</a></li>
                                                <li><a href="#50" data-title="图2 &lt;i&gt;CD&lt;/i&gt;和&lt;i&gt;NM&lt;/i&gt;特征提取流程">图2 <i>CD</i>和<i>NM</i>特征提取流程</a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表&lt;/b&gt;1 VGG&lt;b&gt;网络结构及参数&lt;/b&gt;"><b>表</b>1 VGG<b>网络结构及参数</b></a></li>
                                                <li><a href="#83" data-title="图3 融合结构整体流程">图3 融合结构整体流程</a></li>
                                                <li><a href="#92" data-title="图4 &lt;i&gt;SBU&lt;/i&gt;上训练和测试数据的识别准确率">图4 <i>SBU</i>上训练和测试数据的识别准确率</a></li>
                                                <li><a href="#94" data-title="图5 &lt;i&gt;SBU&lt;/i&gt;上&lt;i&gt;RGB&lt;/i&gt;视频数据识别结果混淆矩阵">图5 <i>SBU</i>上<i>RGB</i>视频数据识别结果混淆矩阵</a></li>
                                                <li><a href="#99" data-title="图6 &lt;i&gt;SBU&lt;/i&gt;上关节点数据识别结果混淆矩阵">图6 <i>SBU</i>上关节点数据识别结果混淆矩阵</a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;本文方法与其他方法的识别结果对比&lt;/b&gt;(&lt;i&gt;SBU&lt;/i&gt;&lt;b&gt;数据库&lt;/b&gt;)"><b>表</b>2 <b>本文方法与其他方法的识别结果对比</b>(<i>SBU</i><b>数据库</b>)</a></li>
                                                <li><a href="#108" data-title="图7 &lt;i&gt;NTU&lt;/i&gt;上数据训练准确率和模型损失值">图7 <i>NTU</i>上数据训练准确率和模型损失值</a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;本文方法与其他方法的识别结果对比&lt;/b&gt;(&lt;i&gt;NTU&lt;/i&gt;&lt;b&gt;数据库&lt;/b&gt;)"><b>表</b>3 <b>本文方法与其他方法的识别结果对比</b>(<i>NTU</i><b>数据库</b>)</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="149">


                                    <a id="bibliography_1" title=" 王世刚,孙爱朦,赵文婷,等.基于时空兴趣点的单人行为及交互行为识别[J].吉林大学学报(工学版),2015,45(1):304-308.(WANG S G,SUN A M,ZHAO W T,et al.Single and interactive human behavior recognition algorithm based on spatio-temporal interest point [J].Journal of Jilin University (Engineering and Technology Edition),2015,45(1):304-308.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201501044&amp;v=MTI4NTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5nVnIzS0x5SE1kN0c0SDlUTXJvOUJZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         王世刚,孙爱朦,赵文婷,等.基于时空兴趣点的单人行为及交互行为识别[J].吉林大学学报(工学版),2015,45(1):304-308.(WANG S G,SUN A M,ZHAO W T,et al.Single and interactive human behavior recognition algorithm based on spatio-temporal interest point [J].Journal of Jilin University (Engineering and Technology Edition),2015,45(1):304-308.)
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_2" title=" GAVRILA D M,DAVIS L S.3-D model-based tracking of humans in action:a multi-view approach[C]// Proceedings of the 1996 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,1996:73-80." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3-D model-based tracking of humans in action:a multi-view approach">
                                        <b>[2]</b>
                                         GAVRILA D M,DAVIS L S.3-D model-based tracking of humans in action:a multi-view approach[C]// Proceedings of the 1996 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,1996:73-80.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_3" title=" 赵海勇,刘志镜,张浩.基于轮廓特征的人体行为识别[J].光电子&#183;激光,2010,21(10):1547-1551.(ZHAO H Y,LIU Z J,ZHANG H.Human action recognition based on image contour [J].Journal of Photoelectron&#183;Laser,2010,21(10):1547-1551)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDZJ201010031&amp;v=MDg3MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmdWcjNLSWluUlpMRzRIOUhOcjQ5R1pZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         赵海勇,刘志镜,张浩.基于轮廓特征的人体行为识别[J].光电子&#183;激光,2010,21(10):1547-1551.(ZHAO H Y,LIU Z J,ZHANG H.Human action recognition based on image contour [J].Journal of Photoelectron&#183;Laser,2010,21(10):1547-1551)
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_4" title=" 韩磊,李军峰,贾云得.基于时空单词的双人交互行为识别方法[J].计算机学报,2010,33(4):776-784.(HAN L,LI J F,JIA Y D.Human interaction recognition method using spatio-temporal words[J].Chinese Journal of Computers,2010,33(4):776-784.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201004014&amp;v=MDk3NDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluZ1ZyM0tMejdCZHJHNEg5SE1xNDlFWUlRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         韩磊,李军峰,贾云得.基于时空单词的双人交互行为识别方法[J].计算机学报,2010,33(4):776-784.(HAN L,LI J F,JIA Y D.Human interaction recognition method using spatio-temporal words[J].Chinese Journal of Computers,2010,33(4):776-784.)
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_5" title=" LI N,CHENG X,GUO H,et al.Recognizing human interactions by genetic algorithm-based random forest spatio-temporal correlation[J].Pattern Analysis and Applications,2016,19(1):267-282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recognizing human interactions by genetic algorithm-based random forest spatio-temporal correlation">
                                        <b>[5]</b>
                                         LI N,CHENG X,GUO H,et al.Recognizing human interactions by genetic algorithm-based random forest spatio-temporal correlation[J].Pattern Analysis and Applications,2016,19(1):267-282.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_6" title=" YUN K,HONORIO J,CHATTOPADHYAY D,et al.Two-person interaction detection using body-pose features and multiple instance learning[C]// Proceedings of the 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2012:28-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two-Person Interaction Detection Using Body-Pose Features and Multiple Instance Learning">
                                        <b>[6]</b>
                                         YUN K,HONORIO J,CHATTOPADHYAY D,et al.Two-person interaction detection using body-pose features and multiple instance learning[C]// Proceedings of the 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2012:28-35.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_7" title=" SLAMA R,WANNOUS H,DAOUDI M,et al.Accurate 3D action recognition using learning on the Grassmann manifold[J].Pattern Recognition,2015,48(2):556-567." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700101400&amp;v=Mjc1MjlyM0lJVnNSYUJBPU5pZk9mYks4SDlETXFJOUZaZXNPQ0h3NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         SLAMA R,WANNOUS H,DAOUDI M,et al.Accurate 3D action recognition using learning on the Grassmann manifold[J].Pattern Recognition,2015,48(2):556-567.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_8" title=" GHORBEL E,BOUTTEAU R,BOONAERT J,et al.3D real-time human action recognition using a spline interpolation approach[C]// Proceedings of the 2015 International Conference on Image Processing Theory,Tools and Applications.Piscataway:IEEE,2015:61-66." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D real-time human action recognition using a spline interpolation approach">
                                        <b>[8]</b>
                                         GHORBEL E,BOUTTEAU R,BOONAERT J,et al.3D real-time human action recognition using a spline interpolation approach[C]// Proceedings of the 2015 International Conference on Image Processing Theory,Tools and Applications.Piscataway:IEEE,2015:61-66.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_9" title=" SIMONYAN K,ZISSERMAN A.Two-stream convolutional networks for action recognition in videos[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:568-576." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">
                                        <b>[9]</b>
                                         SIMONYAN K,ZISSERMAN A.Two-stream convolutional networks for action recognition in videos[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:568-576.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_10" title=" LI C,ZHONG Q,XIE D,et al.Skeleton-based action recognition with convolutional neural networks[C]// Proceedings of the 2017 IEEE International Conference on Multimedia &amp;amp; Expo Workshops.Piscataway:IEEE,2017:597-600." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Skeleton-based action recognition with convolutional neural networks">
                                        <b>[10]</b>
                                         LI C,ZHONG Q,XIE D,et al.Skeleton-based action recognition with convolutional neural networks[C]// Proceedings of the 2017 IEEE International Conference on Multimedia &amp;amp; Expo Workshops.Piscataway:IEEE,2017:597-600.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_11" title=" LIU J,WANG G,DUAN L,et al.Skeleton-based human action recognition with global context-aware attention LSTM networks[J].IEEE Transactions on Image Processing,2018,27(4):1586-1599." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Skeleton-based human action recognition with global context-aware attention LSTM networks">
                                        <b>[11]</b>
                                         LIU J,WANG G,DUAN L,et al.Skeleton-based human action recognition with global context-aware attention LSTM networks[J].IEEE Transactions on Image Processing,2018,27(4):1586-1599.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_12" title=" KE Q,BENNAMOUN M,AN S,et al.Learning clip representations for skeleton-based 3D action recognition[J].IEEE Transactions on Image Processing,2018,27(6):2842-2855." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning clip representations for skeleton-based 3D action recognition">
                                        <b>[12]</b>
                                         KE Q,BENNAMOUN M,AN S,et al.Learning clip representations for skeleton-based 3D action recognition[J].IEEE Transactions on Image Processing,2018,27(6):2842-2855.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_13" title=" LIU J,SHAHROUDY A,XU D,et al.Spatio-temporal LSTM with trust gates for 3D human act in recognition[C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9907.Berlin:Springer,2016:816-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition">
                                        <b>[13]</b>
                                         LIU J,SHAHROUDY A,XU D,et al.Spatio-temporal LSTM with trust gates for 3D human act in recognition[C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9907.Berlin:Springer,2016:816-833.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_14" title=" LI C,ZHONG Q,XIE D,et al.Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation[EB/OL].[2019- 03- 20].http://arxiv.org/pdf/1804.06055." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation">
                                        <b>[14]</b>
                                         LI C,ZHONG Q,XIE D,et al.Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation[EB/OL].[2019- 03- 20].http://arxiv.org/pdf/1804.06055.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_15" title=" SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[15]</b>
                                         SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-08-22 09:59</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(11),3349-3354 DOI:10.11772/j.issn.1001-9081.2019040633            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于RGB和关节点数据融合模型的双人交互行为识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%AC%E6%99%93%E9%A3%9E&amp;code=28602650&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姬晓飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A7%A6%E7%90%B3%E7%90%B3&amp;code=43224188&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">秦琳琳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%89%AC%E6%89%AC&amp;code=26953453&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王扬扬</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%88%E9%98%B3%E8%88%AA%E7%A9%BA%E8%88%AA%E5%A4%A9%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0272020&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">沈阳航空航天大学自动化学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于RGB视频序列的双人交互行为识别已经取得了重大进展,但因缺乏深度信息,对于复杂的交互动作识别不够准确。深度传感器(如微软Kinect)能够有效提高全身各关节点的跟踪精度,得到准确的人体运动及变化的三维关节点数据。依据RGB视频和关节点数据的各自特性,提出一种基于RGB和关节点数据双流信息融合的卷积神经网络(CNN)结构模型。首先,利用Vibe算法获得RGB视频在时间域的感兴趣区域,之后提取关键帧映射到RGB空间,以得到表示视频信息的时空图,并把图送入CNN提取特征;然后,在每帧关节点序列中构建矢量,以提取余弦距离(CD)和归一化幅值(NM)特征,将单帧中的余弦距离和关节点特征按照关节点序列的时间顺序连接,馈送入CNN学习更高级的时序特征;最后,将两种信息源的softmax识别概率矩阵进行融合,得到最终的识别结果。实验结果表明,将RGB视频信息和关节点信息结合可以有效地提高双人交互行为识别结果,在国际公开的SBU Kinect interaction数据库和NTU RGB+D数据库中分别达到92.55%和80.09%的识别率,证明了提出的模型对双人交互行为识别的有效性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=RGB%E8%A7%86%E9%A2%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">RGB视频;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%B3%E8%8A%82%E7%82%B9%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">关节点数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网路;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=softmax&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">softmax;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E4%BA%BA%E4%BA%A4%E4%BA%92%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双人交互行为识别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *姬晓飞(1978—),女,辽宁鞍山人,副教授,博士,主要研究方向:视频分析与处理、模式识别,电子邮箱,jixiaofei7804@126.com;
                                </span>
                                <span>
                                    秦琳琳(1994—),女,山东菏泽人,硕士研究生,主要研究方向:视频分析与处理、生物特征与行为分析;;
                                </span>
                                <span>
                                    王扬扬(1979—),女,辽宁沈阳人,工程师,博士,主要研究方向:视频分析与处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金资助项目(61602321);</span>
                                <span>辽宁省教育厅科学研究服务地方项目(L201708);辽宁省教育厅科学研究青年项目(L201745);</span>
                    </p>
            </div>
                    <h1><b>Human interaction recognition based on RGB and skeleton data fusion model</b></h1>
                    <h2>
                    <span>JI Xiaofei</span>
                    <span>QIN Linlin</span>
                    <span>WANG Yangyang</span>
            </h2>
                    <h2>
                    <span>College of Automation, Shenyang Aerospace University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In recent years, significant progress has been made in human interaction recognition based on RGB video sequences. Due to its lack of depth information, it cannot obtain accurate recognition results for complex interactions. The depth sensors(such as Microsoft Kinect) can effectively improve the tracking accuracy of the joint points of the whole body and obtain three-dimensional data that can accurately track the movement and changes of the human body. According to the respective characteristics of RGB and joint point data, a convolutional neural network structure model based on RGB and joint point data dual-stream information fusion was proposed. Firstly, the region of interest of the RGB video in the time domain was obtained by using the Vibe algorithm, and the key frames were extracted and mapped to the RGB space to obtain the spatial-temporal map representing the video information. The map was sent to the convolutional neural network to extract features. Then, a vector was constructed in each frame of the joint point sequence to extract the Cosine Distance(CD) and Normalized Magnitude(NM) features. The cosine distance and the characteristics of the joint nodes in each frame were connected in time order of the joint point sequence, and were fed into the convolutional neural network to learn more advanced temporal features. Finally, the softmax recognition probability matrixes of the two information sources were fused to obtain the final recognition result. The experimental results show that combining RGB video information with joint point information can effectively improve the recognition result of human interaction behavior, and achieves 92.55% and 80.09% recognition rate on the international public SBU Kinect interaction database and NTU RGB+D database respectively, verifying the effectiveness of the proposed model for the identification of interaction behaviour between two people.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=RGB%20video&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">RGB video;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=skeleton%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">skeleton data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=softmax&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">softmax;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=human%20interaction%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">human interaction recognition;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JI Xiaofei, born in 1978, Ph. D., associate professor. Her research interests include video analysis and processing, pattern recognition. ;
                                </span>
                                <span>
                                    QIN Linlin, born in 1994, M. S. candidate. Her research interests include video analysis and processing, biological characteristics and behavior analysis. ;
                                </span>
                                <span>
                                    WANG Yangyang, born in 1979, Ph. D., engineer. Her research interests include video analysis and processing.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-15</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by National Natural Science Foundation of China(61602321);</span>
                                <span>the Local Project of Scientific Research Service of Liaoning Education Department(L201708);the Scientific Research Youth Project of Liaoning Education Department(L201745);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="34">基于视频的交互行为识别具有较高的实用价值和广阔的应用前景<citation id="179" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。根据原始数据的不同,对于双人交互行为识别的分析方法可以分为基于RGB视频和基于关节点数据两类。基于RGB视频的研究开展比较早,Gavrila等<citation id="180" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出用时空体来描述人的行为,即利用人体行为的轮廓随时间变化的过程来识别行为的类别。赵海勇等<citation id="181" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>将时变轮廓形状转换为对应的一维距离向量并提取行为序列的关键姿态,将关键姿态编码为行为字符串进行交互行为识别; 韩磊等<citation id="182" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出一种基于时空单词的双人交互行为识别方法,该方法从包含双人交互的视频中提取时空兴趣点,并通过投票生成单人原子行为的时空单词,采用条件随机场模型建模单人原子行为模型。在此基础上并训练马尔可夫逻辑网用于双人交互行为的推理。 Li等<citation id="183" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出一种多特征结合的描述方法,提取时空兴趣点,并采用一系列描述子对其进行表示,采用时空匹配法和遗传算法训练随机森林实现动作识别。这类基于RGB的算法对于简单的双人交互行为得到了较好的识别效果,但由于缺乏深度信息,对于复杂多变的交互动作识别不够准确。</p>
                </div>
                <div class="p1">
                    <p id="35">近几年,随着深度传感器(如微软Kinect)的快速发展,大幅提高了全身各关节点数据的跟踪精度, Kinect相机共包括彩色摄像头、深度摄像头和红外摄像机三个摄像头,其中彩色摄像头拍摄视角范围内的彩色视频图像,同时深度摄像头通过分析红外光谱,创建可视范围内的物体的深度图像,利用深度图转化得到3D关节点数据。基于关节点的双人交互识别获得了越来越多研究者的关注。Yun等<citation id="184" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>利用当前帧中所有关节对的距离、当前帧中关节与前一帧中关节之间的距离以及当前帧中各关节点与中心点之间的距离来描述身体姿态,通过多实例学习方法得到每个动作的姿势描述符。这种特征描述简单易获取,但缺少了上下文时序关系的描述。Slama等<citation id="185" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将一个动作描述为时间序列中关节点三维坐标的集合,每个动作序列被表示为产生三维关节轨迹的线性动力系统,采用自回归滑动平均模型来表示序列, 最后采用线性支持向量机(Support Vector Machine, SVM)进行分类。这种描述符同时包括时间和空间信息,但是对于相似动作的识别效果较差。</p>
                </div>
                <div class="p1">
                    <p id="36">目前基于RGB视频和关节点数据的双人交互行为识别研究中,多数是依赖于低级或中级的手动获取特征,在处理复杂数据时能力有限,适应性不强且动作识别准确率提升空间不大。近几年,随着卷积神经网络(Convolutional Neural Network, CNN)在静态图像分类中获得成功,其已经扩展到用于解决动作识别的研究中<citation id="186" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。Simonyan等<citation id="187" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出基于RGB视频时间-空间结合的双流卷积神经网络模型,其中空间流是利用带有视频场景和对象的静态视频帧进行卷积,时间流是利用光流堆积法与轨迹追踪法获得光流图进行卷积,最后将两流做softmax的分数融合。分类结果表明,识别率较传统传统特征明显提升,但基于多帧获得的光流图计算量较大。Li等<citation id="188" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出一种新颖的双流卷积网络结构,首先将原始的骨架信息直接送入CNN提取特征,另外将连续两帧的骨架关节运动也送入网络提取特征,将两种特征连接并经过softmax融合获得识别结果。该方法仅使用关节数据进行识别,计算量低,但是没有很好地利用特征的时序关系。为了更好地建模关节点特征的时序关系,Liu 等<citation id="189" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出全局感知注意力长短期记忆(Long Short-Term Memory, LSTM)网络的动作识别方法。该网络包括两个LSTM网络,第一个LSTM层用于编码原始骨架序列并初始化全局存储单元,然后将全局存储单元的表示送入第二LSTM层,以选择性地关注每个帧的信息性关节,经过多次迭代优化全局存储信息,最后将精简的全局信息送入softmax分类器识别动作类。Ke等<citation id="190" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将骨架序列3D坐标的每个通道转化为一个时空信息的片段,每个骨架序列转换为三个片段,表示整个骨架序列的时间信息和骨架关节之间特定的空间关系,同时提出多任务卷积神经网络(Multi-Task Convolutional Neural Network, MTCNN),并行处理每个片段所有帧以学习骨架序列的时间和空间信息。Liu等<citation id="191" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出一个关节点序列的时空LSTM网络,将LSTM的学习扩展到时空域,每个关节从相邻关节以及前一帧接受信息编码时空特征,采用树状结构表示关节点之间的相邻特性和运动关系,最后将骨架数据的结果送入LSTM网络进行建模与识别。Li等<citation id="192" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出基于骨架端到端的卷积共生特征学习框架,首先对每个关节点的点级信息独立编码,将骨架序列表示为张量,使用卷积和独立学习点级别特征,然后转换卷积层的输出,分层聚合来自关节的全局特征,得到时间和空间域的语义表示,最后送入分层式共现网络(Hierarchical Co-occurrence Network,HCN)学习。利用CNN在关节点和RGB视频的双人交互行为识别中均取得了良好的效果,识别的准确率较手动提取特征有了大幅度的提升。但将CNN应用在两种特征互补的数据源结合中,还处于初始阶段。因此,本文提出了一种RGB视频和关节点数据双流信息融合的CNN识别框架,该框架较好地利用了RGB信息和关节点信息的互补性,进一步提高了对于复杂交互行为识别的准确性。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag">1 算法的整体框架</h3>
                <div class="p1">
                    <p id="38">本文算法的整体框架如图1所示,具体实现步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="39">1)基于RGB视频的处理过程:首先判断两个交互个体的质心之间的距离,获取交互帧的执行阶段,从得到的RGB视频帧中等间距选出三帧,利用Vibe算法做背景减除,分别得到三帧不包括背景信息的二值图像,将代表视频的三张图片映射到RGB空间,并对三张图片压缩,得到表示视频信息的时空图。</p>
                </div>
                <div class="p1">
                    <p id="40">2)基于关节点数据的处理过程:首先,构造交互个体及交互双方之间的关节点矢量,然后构造基于矢量的具有平移、旋转、缩放不变形的余弦距离(Cosine Distance, CD)和归一化幅值(Normalized Magnitude, NM)特征表示骨架序列的空间结构信息,将两种基础特征分别连接起来并构造成灰度图像,送入CNN用于提取更高级的时序特征和动作识别。</p>
                </div>
                <div class="p1">
                    <p id="41">3)基于RGB视频和关节点数据融合的识别过程:将处理好的RGB视频数据和关节点数据分别送入深度学习网络中,将各自得到的识别概率矩阵做权值融合送入softmax分类器得到最后的识别分数。</p>
                </div>
                <div class="area_img" id="42">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法整体框架流程" src="Detail/GetImg?filename=images/JSJY201911041_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法整体框架流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Overall framework process of the proposed algorithm</p>

                </div>
                <h3 id="43" name="43" class="anchor-tag">2 数据预处理</h3>
                <h4 class="anchor-tag" id="44" name="44">2.1 RGB<b>视频预处理</b></h4>
                <div class="p1">
                    <p id="45">对于动作序列,一个动作流程可以分为准备、执行和结束阶段,但有的动作如“握手”与“靠近”,在准备和结束有较大的相似性,为增加两个动作的可区分性,通过测量交互动作双方的质心距离<i>D</i>来得到动作视频的执行阶段。</p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">c</mi><mo>=</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mfrac><mn>1</mn><mrow><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mi>b</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">d</mi><mo>=</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mi>b</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>)</mo></mrow></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>L</mi><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">c</mi><mo>-</mo><mi mathvariant="bold-italic">d</mi><mo stretchy="false">∥</mo><mo>-</mo><mi>D</mi><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mo>≥</mo><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"><mspace width="0.25em" /><mtext>保</mtext><mtext>留</mtext><mtext>帧</mtext></mtd></mtr><mtr><mtd columnalign="left"><mo>&lt;</mo><mn>0</mn><mo>,</mo></mtd><mtd columnalign="left"><mspace width="0.25em" /><mtext>去</mtext><mtext>除</mtext><mtext>帧</mtext></mtd></mtr></mtable></mrow></mrow><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">其中:<i><b>c</b></i>、<i><b>d</b></i>分别表示交互行为双方的体心;<i>m</i><sub>1</sub>和<i>n</i><sub>1</sub>分别表示单个个体所包含的像素点个数;(<i>a</i><sub><i>i</i></sub>,<i>b</i><sub><i>i</i></sub>)和(<i>a</i><sub><i>j</i></sub>,<i>b</i><sub><i>j</i></sub>)表示单人的像素点坐标;<i>D</i>为设定的阈值;<i>L</i>为判别量。通过以上预处理过程得到更为精简的RGB视频信息,利用Vibe算法做背景减除,从去除背景的视频帧中等间距选出三帧,并将这三帧图像映射到RGB空间,得到表示视频信息的时空图。</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48">2.2 <b>关节点数据预处理</b></h4>
                <div class="p1">
                    <p id="49">在双人交互识别过程中,获得动作序列中完整的空间位置信息和时序关系,对识别结果十分重要。因此,在单帧关节点中构造关节点向量,并提取余弦距离(<i>CD</i>)和归一化幅值(<i>NM</i>)特征,分别按照关节点序列的时间信息连接,关节点序列转换为基于图像的表示,则可以使用<i>CNN</i>学习序列中更高级的时间结构。首先获取具有旋转、缩放、平移不变性的余弦距离和归一化幅值特征,具体算法如图2所示。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CD和NM特征提取流程" src="Detail/GetImg?filename=images/JSJY201911041_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>CD</i>和<i>NM</i>特征提取流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 2 <i>Extraction process of CD and NM features</i></p>

                </div>
                <div class="p1">
                    <p id="51">关节点向量表示的计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="52">将单帧中的关节点数据定义为:</p>
                </div>
                <div class="p1">
                    <p id="53"><i>Ω</i>={<i><b>P</b></i><sub><i>i</i></sub>∈<b>R</b><sup>3</sup>:<i>i</i>= 1,2,…,<i>n</i>}      (3)</p>
                </div>
                <div class="p1">
                    <p id="54">其中:<i>n</i>表示单帧中所包含的关节点数,<i><b>P</b></i><sub><i>i</i></sub>=[<i>x</i><sub><i>i</i></sub>,<i>y</i><sub><i>i</i></sub>,<i>z</i><sub><i>i</i></sub>]代表第<i>i</i>关节点的3D坐标。所有帧的关节点按交互行为双方分为两部分,分别表示为:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ω</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∪</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><mrow></mrow></mstyle><mi mathvariant="bold-italic">Ω</mi><msub><mrow></mrow><mi>k</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">其中:<i>Ω</i><sub>1</sub>代表左侧行为者,<i>Ω</i><sub>2</sub>表示右侧行为者。</p>
                </div>
                <div class="p1">
                    <p id="57">对于不同的行为者<i>Ω</i><sub>1,2</sub>,选择一个初始关节点<i><b>p</b></i><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>0</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>,其余关节点定义为一个集合<i><b>p</b></i>,本文定义单人内的关节点向量为:</p>
                </div>
                <div class="p1">
                    <p id="58"><i>υ</i><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>={<i><b>p</b></i>-<i><b>p</b></i><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>0</mn><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>:<i><b>p</b></i>∈<i>Ω</i><sub><i>k</i></sub>}      (5)</p>
                </div>
                <div class="p1">
                    <p id="59">双人之间的关节点向量为:</p>
                </div>
                <div class="p1">
                    <p id="60"><i>υ</i><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>={<i><b>p</b></i>-<i><b>p</b></i><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>0</mn><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>:<i><b>p</b></i>∈<i>Ω</i>\<i>Ω</i><sub><i>k</i></sub>}      (6)</p>
                </div>
                <div class="p1">
                    <p id="61">选择脊柱根部的关节点作为原点,更能反映其他关节的运动。</p>
                </div>
                <div class="p1">
                    <p id="62">余弦距离和归一化幅值特征表示过程如下:</p>
                </div>
                <div class="p1">
                    <p id="63">设定<i><b>v</b></i>∈<i>υ</i><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>,<i><b>u</b></i>∈<i>υ</i><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>∪<i>υ</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>,本文定义余弦距离为:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi mathvariant="bold-italic">v</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">u</mi></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">v</mi><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">单帧中得到的14×28=392维余弦距离特征。</p>
                </div>
                <div class="p1">
                    <p id="66">定义归一化幅度为:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mn>0</mn><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中<i><b>u</b></i><sub>0</sub>为选择的参考向量,将颈部和脊柱根部构成的向量作为参考向量,得到28维归一化幅值特征。</p>
                </div>
                <div class="p1">
                    <p id="69">将所有视频帧的上述特征按照时间关系连接,每个关节点序列共包含<i>n</i>帧,则得到的余弦距离维数为14×28×<i>n</i>,归一化幅值特征维数为28×<i>n</i>,其中每列表示单帧的空间结构特征,初步提取所有帧的信息。然后将得到的余弦距离和归一化幅值矩阵归一化至0～255,成为一幅灰度图像,由于相邻关节点和相邻帧中相同关节点的变化是连续的,因此图像中的像素不会急剧变化。为了减少不同关节点帧数造成的差异,将所有关节点序列得到的灰度图像调整至相同大小。最后馈送入CNN学习更高级的特征,获得最后的识别结果。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">3 模型结构</h3>
                <div class="p1">
                    <p id="71">将数据集中每类动作的<i>RGB</i>视频和关节点数据按照8∶2 分为训练集和测试集,在<i>TensorFlow</i>平台下使用<i>Keras</i>框架对深度卷积神经网络<i>VGGNet</i>16<citation id="193" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>模型迁移学习,实现人体动作识别。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">3.1 <i>CNN</i><b>概述</b></h4>
                <div class="p1">
                    <p id="73"><i>CNN</i>由输入层和输出层及多个隐藏层组成,隐含层包括卷积层、池化层及全连接层。</p>
                </div>
                <div class="p1">
                    <p id="74">卷积层(<i>Convolutional layer</i>) 卷积运算的目的是提取输入的不同特征,第一层卷积层可能只能提取到一些低级的如边缘、线条和角等特征,多层的卷积网络能从低级的特征中迭代提取更复杂的特征。</p>
                </div>
                <div class="p1">
                    <p id="75">池化层(<i>Pooling layer</i>) 池化即降采样,目的是减少特征图,主要是通过减少网络的参数来减少计算量,并且在一定程度上能够控制过拟合。</p>
                </div>
                <div class="p1">
                    <p id="76">全连接层(<i>Fully</i>-<i>Connected layer</i>) 全连接层的每一个节点都与上一层的所有节点相连,把前边提取到的特征综合起来。由于其全相连的特性,一般全连接层的参数也是最多的。</p>
                </div>
                <div class="p1">
                    <p id="77"><i>CNN</i>与传统的神经网络相比不同之处,主要有局部感知、权值共享和多卷积核三点。局部感知就是卷积核和图像卷积时,每次卷积核所覆盖的像素只是一小部分,感知的是局部特征,<i>CNN</i>是一个从局部到整体的过程。传统的神经网络参数量非常巨大,而卷积层的参数完全取决于滤波器的设置大小,整个图片共享一组滤波器的参数,通过权值共享降低参数量。一种卷积核代表一种特征,为了获取更多不同特征的集合,卷积层会有多个卷积核,来得到不同的特征。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">3.2 <i>VGG</i><b>网络</b></h4>
                <div class="p1">
                    <p id="79"><i>VGG</i>是牛津大学计算机视觉组和<i>Google DeepMind</i>公司一起研发的深度卷积神经网络,该网络主要是泛化性能很好。<i>VGG</i>探索了<i>CNN</i>的深度与其性能之间的关系,通过反复堆叠3×3的小型卷积核和2×2的最大池化层,成功地构筑了16～19层深的CNN。同时将卷积层提升到卷积块,使网络有更大的感受野同时也降低网络参数,学习能力更强。在训练过程中使用Multi-Scale 做数据增强,将同一张图片缩放到不同的尺寸,增加数据量。本文选择层数为16的VGG作为CNN模型。如表1所示为VGG网络的结构及参数。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表</b>1 VGG<b>网络结构及参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 1 VGG network structure and parameters</p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />Layers</td><td>Output</td></tr><tr><td><br />Conv3- 64×2</td><td>224×224×64</td></tr><tr><td><br />maxpool</td><td>112×112×64</td></tr><tr><td><br />Conv3- 128×2</td><td>11×112×128</td></tr><tr><td><br />maxpool</td><td>56×56×128</td></tr><tr><td><br />Conv3- 256×3</td><td>56×56×256</td></tr><tr><td><br />maxpool</td><td>28×28×256</td></tr><tr><td><br />Conv3- 512×3</td><td>28×28×512</td></tr><tr><td><br />maxpool</td><td>14×14×512</td></tr><tr><td><br />Conv3- 512×3</td><td>14×14×512</td></tr><tr><td><br />maxpool</td><td>7×7×512</td></tr><tr><td><br />FC- 4096</td><td>4 096</td></tr><tr><td><br />FC- 4096</td><td>4 096</td></tr><tr><td><br />FC- 1000</td><td>1 000</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">3.3 <b>融合结构</b></h4>
                <div class="p1">
                    <p id="82">针对可视范围内发生的动作,<i>Kinect</i>相机能够同时获取<i>RGB</i>视频和3<i>D</i>关节点数据。将传统<i>RGB</i>数据与3<i>D</i>关节点数据相结合,两者信息互补,经过预处理,<i>RGB</i>获得更精简的信息,3<i>D</i>关节点数据得到具有旋转、平移、缩放不变性的基础特征。分别将处理得到的<i>RGB</i>视频和关节点数据送入<i>VGG</i>16网络模型,最终利用<i>softmax</i>分类器得到基于<i>RGB</i>视频和关节点数据各动作类别的识别分数概率矩阵。然后,针对不同的原始数据流给予不同的权值融合<i>softmax</i>值,将融合得到的结果再次经过<i>softmax</i>分类器,最终得到融合识别结果分数矩阵,实现<i>RGB</i>视频和3<i>D</i>关节点数据的决策集融合。总体流程如图3所示。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 融合结构整体流程" src="Detail/GetImg?filename=images/JSJY201911041_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 融合结构整体流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 3 <i>Flow chart of fusion structure</i></p>

                </div>
                <h3 id="84" name="84" class="anchor-tag">4 实验测试与分析</h3>
                <h4 class="anchor-tag" id="85" name="85">4.1 <b>数据库与测试环境介绍</b></h4>
                <div class="p1">
                    <p id="86">为证明提出方法的有效性,采用国际标准的<i>SBU Kinect</i>数据库和<i>NTU RGB</i>+<i>D</i>数据库进行验证。<i>SBU Kinect</i>交互数据集共有7名动作行为人,组成21对动作执行者,包括8个动作类别,分别为靠近、离开、踢腿、打、推、拥抱、握手和传递物品。在大多数的互动行为中,一个人做出动作另一个人做出反应动作,且均采用相同的室内背景录制。每个人由15个关节点表示,每帧的关节点数据维度为15×3×2=90。该数据集包含的动作大多为非周期性行为,且包含相似动作,准确实现动作识别具有一定的难度。</p>
                </div>
                <div class="p1">
                    <p id="87"><i>NTU RGB</i>+<i>D Dateset</i>数据集是目前包括双人交互的<i>RGB</i>+<i>D</i>视频和关节点数据最大的数据库,包括56 880个视频片段。本数据库共有40名行为动作者,包括60个动作类别,包括日常动作、与健康相关的动作和双人交互行为。本数据库采用三个高度相同但角度不同的摄像机采集图片。动作行为人执行两次动作,一次面向左侧摄像头一次面向右侧摄像头。本数据库提供两种识别评估标准<i>CS</i>和<i>CV</i>, 本文采用<i>CS</i>的评估方式。</p>
                </div>
                <div class="p1">
                    <p id="88">本实验基于<i>Tensorflow</i>平台利用<i>keras</i>深度学习库在<i>GPU</i>处理器下进行,操作系统为<i>Ubuntu</i>16.04,内存和硬盘参数分别为32 <i>GB</i>、256 <i>GB</i>+2 <i>TB</i>,编程环境为<i>Python</i>3.6,程序框架<i>Keras</i>2.1.3。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">4.2 <i>SBU</i><b>数据库实验测试结果</b></h4>
                <h4 class="anchor-tag" id="90" name="90">1)<i>RGB</i>视频和关节点数据测试结果分析。</h4>
                <div class="p1">
                    <p id="91">本实验在国际公开的<i>SBU</i>数据集中的<i>RGB</i>视频和关节点数据上分别做了测试,把每个动作按8∶2 的比例划分为训练集和测试集,用80%的数据训练模型,将训练好的模型用20%的视频做测试。本实验共采用200次迭代训练,每次迭代训练中训练数据与测试数据对应的准确率如图4所示。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 SBU上训练和测试数据的识别准确率" src="Detail/GetImg?filename=images/JSJY201911041_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 <i>SBU</i>上训练和测试数据的识别准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 4 <i>Recognition accuracy of</i><i>training and testing data on SBU</i></p>

                </div>
                <div class="p1">
                    <p id="93">从图4中可知,随着训练次数增加,准确率不断增加,模型的损失值不断降低。利用<i>RGB</i>视频数据在此模型下测试,得到的最优识别准确率为87.5%,将最终的识别结果用混淆矩阵表示如图5所示。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 SBU上RGB视频数据识别结果混淆矩阵" src="Detail/GetImg?filename=images/JSJY201911041_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>SBU</i>上<i>RGB</i>视频数据识别结果混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>RGB video data recognition result confusion matrix on SBU</i></p>

                </div>
                <div class="p1">
                    <p id="95">从图5混淆矩阵分析可知,误识别动作主要为“推”和“握手”两个动作,通过分析可知,这两类动作在视频的阶段帧与帧之间的变化幅度小,导致<i>Vibe</i>背景减除后得到的动作区分性不足,导致模型的误识别。</p>
                </div>
                <div class="p1">
                    <p id="96">将关节点数据在构建的模型下进行测试,得到识别准确率为91.87%,把识别结果用归一化混淆矩阵表示,如图6。</p>
                </div>
                <div class="p1">
                    <p id="97">从上述混淆矩阵分析可知,应用关节点数据进行识别,8种行为动作中有6种行为能达到准确识别,错误识别主要发生在握手(<i>shaking hand</i>)和传递物品(<i>exchanging</i>),因为关节点数据只对行为人的动作变化作出精确描述,不包含环境中的其他事物,对行为的外观描述信息较少,因此容易造成包含环境中其他事物的动作识别不准确。</p>
                </div>
                <div class="p1">
                    <p id="98">从图5和图6的分析可以看出,<i>RGB</i>视频信息与关节点数据具有较好的互补性,为下一步的融合提供了依据。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 SBU上关节点数据识别结果混淆矩阵" src="Detail/GetImg?filename=images/JSJY201911041_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 <i>SBU</i>上关节点数据识别结果混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>Joint point data recognition result confusion matrix on SBU</i></p>

                </div>
                <h4 class="anchor-tag" id="100" name="100">2)<i>RGB</i>视频和关节点数据信息融合。</h4>
                <div class="p1">
                    <p id="101">本文将<i>RGB</i>视频和关节点数据得到的识别分数作决策级融合,将两种信息源得到的识别概率矩阵加权融合送入<i>softmax</i>分类器得到最终的识别分数,得到最终比较理想的识别结果,识别率为92.55%。</p>
                </div>
                <div class="p1">
                    <p id="102">为验证本文提出模型的有效性,本文将同样在<i>SBU Kinect interaction</i> 数据库上进行算法测试的结果与本文所得的实验结果相比较,如表2所示。</p>
                </div>
                <div class="area_img" id="103">
                                            <p class="img_tit">
                                                <b>表</b>2 <b>本文方法与其他方法的识别结果对比</b>(<i>SBU</i><b>数据库</b>)
                                                    <br />
                                                <i>Tab</i>. 2 <i>Recognition result comparison of</i><i>the proposed method with other methods on SBU</i>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201911041_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 本文方法与其他方法的识别结果对比(SBU数据库)" src="Detail/GetImg?filename=images/JSJY201911041_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="104">从表2中可知,本文提出的基于<i>CNN</i>的<i>RGB</i>和关节点数据融合的双人交互行为识别框架获得了良好的识别结果。与文献<citation id="194" type="reference">[<a class="sup">5</a>]</citation>和文献<citation id="195" type="reference">[<a class="sup">6</a>]</citation>中利用单一数据源和手动提取特征相结合的处理方法相比较,识别准确率得到了大幅度的提升。本文的识别结果与文献<citation id="196" type="reference">[<a class="sup">13</a>]</citation>相当,但文献<citation id="197" type="reference">[<a class="sup">13</a>]</citation>中引入一个信任门消除关节点数据的噪声,而本文的方法对原始含有噪声的数据没有作任何处理,采用原始的关节点数据构造基础特征。文献<citation id="198" type="reference">[<a class="sup">12</a>]</citation>是将每个关节点序列转化为三个片段,采用多任务卷积神经网络识别分类,但训练多任务并行的<i>CNN</i>模型复杂度高,训练过程复杂,而本实验中采用16层的卷积网络,迭代一次的训练时长仅为2 <i>s</i>,识别过程中处理一帧数据的时间约为27 <i>ms</i>, 具有较好的实时性。本文采用的方法避免对原始的关节点数据进行处理,算法相对简单,具有一定的实际应用前景。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">4.3 <i>NTU</i><b>数据库实验测试结果</b></h4>
                <h4 class="anchor-tag" id="106" name="106">1)<i>RGB</i>视频和关节点数据测试结果分析。</h4>
                <div class="p1">
                    <p id="107">本实验在<i>NTU RGB</i>+<i>D</i>数据库的<i>RGB</i>视频和关节点数据上进行分别训练与测试,采用原数据库提供的<i>Cross</i>-<i>Subject</i>测试模式,将动作行为人分为两组,得到的行为动作分别作为训练集和测试集。针对不同的数据源,<i>RGB</i>视频数据采用100次的迭代训练,关节点数据采用200次迭代训练每次迭代训练中训练数据与测试数据对应的准确率和模型损失值如图7所示。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 NTU上数据训练准确率和模型损失值" src="Detail/GetImg?filename=images/JSJY201911041_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 <i>NTU</i>上数据训练准确率和模型损失值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 7 <i>Data training accuracy and model loss value on NTU database</i></p>

                </div>
                <div class="p1">
                    <p id="109">从图7中可知,随着训练次数增加,准确率增加,由于<i>NTU</i>数据库很大且相机的变化角度和参与动作的人数较多,且在训练时利用<i>batch size</i>调整一次学习的信息量,导致模型存在一些震荡。利用<i>RGB</i>视频数据在此模型下测试,得到的最优识别准确率为75.82%。利用关节点数据在此模型下得到的最优识别结果为74.37%。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">2)<i>RGB</i>视频和关节点数据信息融合。</h4>
                <div class="p1">
                    <p id="111">单独利用<i>RGB</i>视频和关节点数据分别进行测试,结果发现“摸口袋”这个动作,在<i>RGB</i>视频识别过程中得到的结果较差,而在关节点数据识别过程中得到了较为理想的识别结果。本文将<i>RGB</i>视频和关节点数据得到的识别分数作决策级融合,将两种信息源得到的识别概率矩阵加权融合送入<i>softmax</i>分类器得到最终的识别分数,得到的正确识别率为80.09%,较单一数据源的结果有了较大的提升。</p>
                </div>
                <div class="p1">
                    <p id="112">为验证本文提出模型的有效性,本文将同样在<i>NTU RGB</i>+<i>D</i>数据库<i>Cross</i>-<i>Subject</i>测试模式下验证的其他文献得到的测试结果与本文所得的实验结果相比较,如表3所示。</p>
                </div>
                <div class="area_img" id="113">
                                            <p class="img_tit">
                                                <b>表</b>3 <b>本文方法与其他方法的识别结果对比</b>(<i>NTU</i><b>数据库</b>)
                                                    <br />
                                                <i>Tab</i>. 3 <i>Recognition result comparison of</i><i>the proposed method with other methods on NTU</i>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201911041_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201911041_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201911041_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 本文方法与其他方法的识别结果对比(NTU数据库)" src="Detail/GetImg?filename=images/JSJY201911041_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="114">由表3可知,本文提出的<i>RGB</i>视频和关节点数据融合并与<i>CNN</i>结合的网络模型结构,比文献<citation id="199" type="reference">[<a class="sup">11</a>]</citation>和文献<citation id="200" type="reference">[<a class="sup">13</a>]</citation>中采用的方法得到的识别结果大幅度提高。文献<citation id="201" type="reference">[<a class="sup">10</a>]</citation>采用双流并行的<i>CNN</i>,模型复杂度高且训练时间长; 文献<citation id="202" type="reference">[<a class="sup">14</a>]</citation>使用<i>CNN</i>模型学习共生特征,并设计一种端到端的分层式学习网络,获得了较高的识别结果,但它将骨架表示为张量的过程计算量较大,同时使用卷积层独立地为每个关节学习点层面的特征,卷积网络设计复杂; 本文采用16层卷积结构,网络模型简单且参数较少,模型训练时间短,每帧的处理时间约为27 <i>ms</i>,同时也得到了较为理想的实验结果。</p>
                </div>
                <h3 id="115" name="115" class="anchor-tag">5 结语</h3>
                <div class="p1">
                    <p id="116">本文根据<i>RGB</i>视频和关节点数据各自的优缺点,提出将<i>RGB</i>视频和关节点数据在决策级中有效结合起来的双人交互行为识别算法。本文充分地利用<i>RGB</i>视频和关节点数据互补的特性,在对两种原始数据作出合理的预处理的前提下,采用<i>CNN</i>的框架,进行更高级的特征提取与分类。本文采用国际公认的<i>SBU Kinect</i>深度数据库和<i>NTU RGB</i>+<i>D</i>数据库进行训练与测试,结果表明识别结果良好,同时避免了复杂的预处理。下一步研究重点是在当前模型的基础上引入时序建模,将两种数据源更好地结合,进一步提高双人交互行为识别的准确性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="149">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201501044&amp;v=MDA3NzgzS0x5SE1kN0c0SDlUTXJvOUJZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5nVnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 王世刚,孙爱朦,赵文婷,等.基于时空兴趣点的单人行为及交互行为识别[J].吉林大学学报(工学版),2015,45(1):304-308.(WANG S G,SUN A M,ZHAO W T,et al.Single and interactive human behavior recognition algorithm based on spatio-temporal interest point [J].Journal of Jilin University (Engineering and Technology Edition),2015,45(1):304-308.)
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3-D model-based tracking of humans in action:a multi-view approach">

                                <b>[2]</b> GAVRILA D M,DAVIS L S.3-D model-based tracking of humans in action:a multi-view approach[C]// Proceedings of the 1996 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Piscataway:IEEE,1996:73-80.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDZJ201010031&amp;v=MTIzMTZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGeW5nVnIzS0lpblJaTEc0SDlITnI0OUc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 赵海勇,刘志镜,张浩.基于轮廓特征的人体行为识别[J].光电子·激光,2010,21(10):1547-1551.(ZHAO H Y,LIU Z J,ZHANG H.Human action recognition based on image contour [J].Journal of Photoelectron·Laser,2010,21(10):1547-1551)
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201004014&amp;v=MDMwMzVxcUJ0R0ZyQ1VSN3FmWnVac0Z5bmdWcjNLTHo3QmRyRzRIOUhNcTQ5RVlJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 韩磊,李军峰,贾云得.基于时空单词的双人交互行为识别方法[J].计算机学报,2010,33(4):776-784.(HAN L,LI J F,JIA Y D.Human interaction recognition method using spatio-temporal words[J].Chinese Journal of Computers,2010,33(4):776-784.)
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recognizing human interactions by genetic algorithm-based random forest spatio-temporal correlation">

                                <b>[5]</b> LI N,CHENG X,GUO H,et al.Recognizing human interactions by genetic algorithm-based random forest spatio-temporal correlation[J].Pattern Analysis and Applications,2016,19(1):267-282.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two-Person Interaction Detection Using Body-Pose Features and Multiple Instance Learning">

                                <b>[6]</b> YUN K,HONORIO J,CHATTOPADHYAY D,et al.Two-person interaction detection using body-pose features and multiple instance learning[C]// Proceedings of the 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops.Piscataway:IEEE,2012:28-35.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700101400&amp;v=MjIxOTB3NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXIzSUlWc1JhQkE9TmlmT2ZiSzhIOURNcUk5Rlplc09DSA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> SLAMA R,WANNOUS H,DAOUDI M,et al.Accurate 3D action recognition using learning on the Grassmann manifold[J].Pattern Recognition,2015,48(2):556-567.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D real-time human action recognition using a spline interpolation approach">

                                <b>[8]</b> GHORBEL E,BOUTTEAU R,BOONAERT J,et al.3D real-time human action recognition using a spline interpolation approach[C]// Proceedings of the 2015 International Conference on Image Processing Theory,Tools and Applications.Piscataway:IEEE,2015:61-66.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">

                                <b>[9]</b> SIMONYAN K,ZISSERMAN A.Two-stream convolutional networks for action recognition in videos[C]// Proceedings of the 27th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:568-576.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Skeleton-based action recognition with convolutional neural networks">

                                <b>[10]</b> LI C,ZHONG Q,XIE D,et al.Skeleton-based action recognition with convolutional neural networks[C]// Proceedings of the 2017 IEEE International Conference on Multimedia &amp; Expo Workshops.Piscataway:IEEE,2017:597-600.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Skeleton-based human action recognition with global context-aware attention LSTM networks">

                                <b>[11]</b> LIU J,WANG G,DUAN L,et al.Skeleton-based human action recognition with global context-aware attention LSTM networks[J].IEEE Transactions on Image Processing,2018,27(4):1586-1599.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning clip representations for skeleton-based 3D action recognition">

                                <b>[12]</b> KE Q,BENNAMOUN M,AN S,et al.Learning clip representations for skeleton-based 3D action recognition[J].IEEE Transactions on Image Processing,2018,27(6):2842-2855.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition">

                                <b>[13]</b> LIU J,SHAHROUDY A,XU D,et al.Spatio-temporal LSTM with trust gates for 3D human act in recognition[C]// Proceedings of the 2016 European Conference on Computer Vision,LNCS 9907.Berlin:Springer,2016:816-833.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation">

                                <b>[14]</b> LI C,ZHONG Q,XIE D,et al.Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation[EB/OL].[2019- 03- 20].http://arxiv.org/pdf/1804.06055.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[15]</b> SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2019- 01- 10].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201911041" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201911041&amp;v=MjAwMzdyM0tMejdCZDdHNEg5ak5ybzlCWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnluZ1Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
