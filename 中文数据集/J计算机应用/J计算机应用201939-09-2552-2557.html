<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136471056846250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJY201909013%26RESULT%3d1%26SIGN%3dCK1ZK%252fWxKU4Jhp%252bpZxmnn1ohbJ0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJY201909013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909013&amp;v=MDQwNzFzRnlqbFdyeklMejdCZDdHNEg5ak1wbzlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 引言 ">0 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="1 多尺度编解码深度卷积网络 ">1 多尺度编解码深度卷积网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="1.1 &lt;b&gt;网络结构&lt;/b&gt;">1.1 <b>网络结构</b></a></li>
                                                <li><a href="#63" data-title="1.2 &lt;b&gt;损失函数&lt;/b&gt;">1.2 <b>损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="2.1 &lt;b&gt;数据集&lt;/b&gt;">2.1 <b>数据集</b></a></li>
                                                <li><a href="#73" data-title="2.2 &lt;b&gt;实验细节&lt;/b&gt;">2.2 <b>实验细节</b></a></li>
                                                <li><a href="#75" data-title="2.3 &lt;b&gt;结果与分析&lt;/b&gt;">2.3 <b>结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="3 结语 ">3 结语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="图1 多尺度编解码深度卷积网络结构">图1 多尺度编解码深度卷积网络结构</a></li>
                                                <li><a href="#54" data-title="图2 快速多尺度残差块示意图">图2 快速多尺度残差块示意图</a></li>
                                                <li><a href="#56" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;网络结构参数配置表&lt;/b&gt;"><b>表</b>1 <b>网络结构参数配置表</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;两个测试集上质量评估结果&lt;/b&gt;"><b>表</b>2 <b>两个测试集上质量评估结果</b></a></li>
                                                <li><a href="#81" data-title="图3 GOPRO测试集去模糊结果可视化对比">图3 GOPRO测试集去模糊结果可视化对比</a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同尺度性能比较&lt;/b&gt;"><b>表</b>3 <b>不同尺度性能比较</b></a></li>
                                                <li><a href="#86" data-title="图4 不同尺度可视化对比">图4 不同尺度可视化对比</a></li>
                                                <li><a href="#89" data-title="图5 不同模块可视化">图5 不同模块可视化</a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;单尺度下各模块性能比较&lt;/b&gt;"><b>表</b>4 <b>单尺度下各模块性能比较</b></a></li>
                                                <li><a href="#94" data-title="图6 单尺度下跳跃连接比较">图6 单尺度下跳跃连接比较</a></li>
                                                <li><a href="#97" data-title="图7 自行拍摄图像去模糊">图7 自行拍摄图像去模糊</a></li>
                                                <li><a href="#102" data-title="图8 离焦模糊图像复原">图8 离焦模糊图像复原</a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;大尺度图像去模糊结果&lt;/b&gt;"><b>表</b>5 <b>大尺度图像去模糊结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="141">


                                    <a id="bibliography_1" title=" NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017,1:257-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep multi-scale convolutional neural network for dynamic scene deblurring">
                                        <b>[1]</b>
                                         NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017,1:257-265.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_2" title=" PAN J,HU Z,SU Z,et al.Deblurring text images via l0-regularized intensity and gradient prior [C]// CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2014:2901-2908." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deblurring text images via L0-regularized intensity and gradient prior">
                                        <b>[2]</b>
                                         PAN J,HU Z,SU Z,et al.Deblurring text images via l0-regularized intensity and gradient prior [C]// CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2014:2901-2908.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_3" title=" LI L,PAN J,LAI W-S,et al.Learning a discriminative prior for blind image deblurring [C]// CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:6616-6625." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Prior for Blind Image Deblurring">
                                        <b>[3]</b>
                                         LI L,PAN J,LAI W-S,et al.Learning a discriminative prior for blind image deblurring [C]// CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:6616-6625.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_4" title=" ZHANG X,DONG H,HU Z,et al.Gated fusion network for joint image deblurring and super-resolution [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1807.10806.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gated fusion network for joint image deblurring and super-resolution">
                                        <b>[4]</b>
                                         ZHANG X,DONG H,HU Z,et al.Gated fusion network for joint image deblurring and super-resolution [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1807.10806.pdf.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_5" title=" SUN J,CAO W,XU Z,et al.Learning a convolutional neural network for non-uniform motion blur removal [C]// CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:769-777." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a convolutional neural network for non-uniform motion blur removal">
                                        <b>[5]</b>
                                         SUN J,CAO W,XU Z,et al.Learning a convolutional neural network for non-uniform motion blur removal [C]// CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:769-777.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_6" title=" KUPYN O,BUDZAN V,MYKHAILYCH M,et al.DeblurGAN:blind motion deblurring using conditional adversarial networks [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1711.07064.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeblurGAN:blind motion deblurring using conditional adversarial networks">
                                        <b>[6]</b>
                                         KUPYN O,BUDZAN V,MYKHAILYCH M,et al.DeblurGAN:blind motion deblurring using conditional adversarial networks [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1711.07064.pdf.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_7" title=" TAO X,GAO H,WANG Y,et al.Scale-recurrent network for deep image deblurring [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1802.01770.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scale-recurrent network for deep image deblurring">
                                        <b>[7]</b>
                                         TAO X,GAO H,WANG Y,et al.Scale-recurrent network for deep image deblurring [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1802.01770.pdf.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_8" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G,et al.ImageNet classification with deep convolution neural network [C]// NIPS &#39;12:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012,1:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with DeepConvolutional Neural Networks">
                                        <b>[8]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G,et al.ImageNet classification with deep convolution neural network [C]// NIPS &#39;12:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012,1:1097-1105.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_9" title=" LOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// ICML &#39;15:Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:JMLR.org,2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[9]</b>
                                         LOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// ICML &#39;15:Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:JMLR.org,2015:448-456.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_10" title=" LI J,FANG F,MEI K,et al.Multi-scale residual network for image super-resolution [C]// Proceedings of the 2018 European Conference on Computer Vision,LNCS 11212.Berlin:Springer,2018:527-542." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale residual network for image super-resolution">
                                        <b>[10]</b>
                                         LI J,FANG F,MEI K,et al.Multi-scale residual network for image super-resolution [C]// Proceedings of the 2018 European Conference on Computer Vision,LNCS 11212.Berlin:Springer,2018:527-542.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_11" title=" K&#214;HLER R,HIRSCH M,MOHLER B,et al.Recording and playback of camera shake:benchmarking blind deconvolution with a real-world database [EB/OL].[2019- 01- 05].http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=4F605BF966AB6B236B6591E377AC8243 doi=10.1.1.379.1398&amp;amp;rep=rep1&amp;amp;type=pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recording and playback of camera shake:benchmarking blind deconvolution with a real-world database">
                                        <b>[11]</b>
                                         K&#214;HLER R,HIRSCH M,MOHLER B,et al.Recording and playback of camera shake:benchmarking blind deconvolution with a real-world database [EB/OL].[2019- 01- 05].http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=4F605BF966AB6B236B6591E377AC8243 doi=10.1.1.379.1398&amp;amp;rep=rep1&amp;amp;type=pdf.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_12" title=" LAI W,HUANG J,AHUJA N,et al.Deep Laplacian pyramid networks for fast and accurate super-resolution [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:5835-5843." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution">
                                        <b>[12]</b>
                                         LAI W,HUANG J,AHUJA N,et al.Deep Laplacian pyramid networks for fast and accurate super-resolution [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:5835-5843.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_13" title=" MAO X-J,SHEN C,YANG Y-B,et al.Image restoration using convolutional auto-encoders with symmetric skip connections [EB/OL].[2019- 01- 07].https://arxiv.org/pdf/1606.08921.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image restoration using convolutional auto-encoders with symmetric skip connections">
                                        <b>[13]</b>
                                         MAO X-J,SHEN C,YANG Y-B,et al.Image restoration using convolutional auto-encoders with symmetric skip connections [EB/OL].[2019- 01- 07].https://arxiv.org/pdf/1606.08921.pdf.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_14" title=" SU S,DELBRACIO M,WANG J,et al.Deep video deblurring for hand-held cameras [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:237-246." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep video deblurring for hand-held cameras">
                                        <b>[14]</b>
                                         SU S,DELBRACIO M,WANG J,et al.Deep video deblurring for hand-held cameras [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:237-246.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_15" title=" RONNEBERGER O,FISCHER P,BROX T.U-net:convolutional networks for biomedical image segmentation [C]// Proceedings of the 2015 International Conference on Medical Image Computing and Computer-Assisted Intervention,LNCS 9351.Berlin :Springer,2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-net:Convolutional networks for biomedical image segmentation">
                                        <b>[15]</b>
                                         RONNEBERGER O,FISCHER P,BROX T.U-net:convolutional networks for biomedical image segmentation [C]// Proceedings of the 2015 International Conference on Medical Image Computing and Computer-Assisted Intervention,LNCS 9351.Berlin :Springer,2015:234-241.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_16" title=" KIM T H,LEE K M.Segmentation-free dynamic scene deblurring [C]// CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2014,1:2766-2773." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segmentationfree Dynamic Scene Deblurring">
                                        <b>[16]</b>
                                         KIM T H,LEE K M.Segmentation-free dynamic scene deblurring [C]// CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2014,1:2766-2773.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_17" title=" GONG D,YANG J,LIU L,et al.From motion blur to motion flow:a deep learning solution for removing heterogeneous motion blur [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:3806-3815." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=From motion blur to motion flow:a deep learning solution for removing heterogeneous motion blur">
                                        <b>[17]</b>
                                         GONG D,YANG J,LIU L,et al.From motion blur to motion flow:a deep learning solution for removing heterogeneous motion blur [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:3806-3815.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_18" title=" ZHANG J,PAN J,REN J,et al.Dynamic scene deblurring using spatially variant recurrent neural networks [C]// CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:2521-2529." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic scene deblurring using spatially variant recurrent neural networks">
                                        <b>[18]</b>
                                         ZHANG J,PAN J,REN J,et al.Dynamic scene deblurring using spatially variant recurrent neural networks [C]// CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:2521-2529.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_19" title=" 于春和,祁奇.离焦模糊图像复原技术综述[J].沈阳航空航天大学学报,2018,35(5):57-63.(YU C H,QI Q.A survey of defocusing image restoration techniques [J].Journal of Shenyang Aerospace University,2018,35(5):57-63.)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HKGX201805009&amp;v=MTc2MjZxZlp1WnNGeWpsV3J6SUxTYk1kckc0SDluTXFvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         于春和,祁奇.离焦模糊图像复原技术综述[J].沈阳航空航天大学学报,2018,35(5):57-63.(YU C H,QI Q.A survey of defocusing image restoration techniques [J].Journal of Shenyang Aerospace University,2018,35(5):57-63.)
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-05-23 15:45</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJY" target="_blank">计算机应用</a>
                2019,39(09),2552-2557 DOI:10.11772/j.issn.1001-9081.2019020373            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>盲去模糊的多尺度编解码深度卷积网络</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BE%E7%91%9E%E6%98%8E&amp;code=32048677&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贾瑞明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B1%E6%A1%A2%E8%8A%9D&amp;code=42779638&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邱桢芝</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E5%AE%B6%E7%A4%BC&amp;code=32048675&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔家礼</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%B8%80%E4%B8%81&amp;code=22161831&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王一丁</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E6%96%B9%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0226398&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北方工业大学信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对拍摄场景中物体运动不一致所带来的非均匀模糊,为提高复杂运动场景中去模糊的效果,提出一种多尺度编解码深度卷积网络。该网络采用“从粗到细”的多尺度级联结构,在模糊核未知条件下,实现盲去模糊;其中,在该网络的编解码模块中,提出一种快速多尺度残差块,使用两个感受野不同的分支增强网络对多尺度特征的适应能力;此外,在编解码之间增加跳跃连接,丰富解码端信息。与2018年国际计算机视觉与模式识别会议(CVPR)上提出的多尺度循环网络相比,峰值信噪比(PSNR)高出0.06 dB;与2017年CVPR上提出的深度多尺度卷积网络相比,峰值信噪比和平均结构相似性(MSSIM)分别提高了1.4%和3.2%。实验结果表明,该网络能快速去除图像模糊,恢复出图像原有的边缘结构和纹理细节。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B2%E5%8E%BB%E6%A8%A1%E7%B3%8A&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">盲去模糊;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度结构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%B3%E8%B7%83%E8%BF%9E%E6%8E%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跳跃连接;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BC%96%E8%A7%A3%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">编解码;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *贾瑞明(1978—),男,山东青岛人,助理研究员,博士,主要研究方向:计算机视觉、深度学习、模式识别;电子邮箱jiaruiming@ncut.edu.cn;
                                </span>
                                <span>
                                    邱桢芝(1994—),女,山西长治人,硕士研究生,主要研究方向:计算机视觉、深度学习;;
                                </span>
                                <span>
                                    崔家礼(1975—),男,山东枣庄人,助理研究员,博士,主要研究方向:图像处理、模式识别;;
                                </span>
                                <span>
                                    王一丁(1967—),男,辽宁沈阳人,教授,博士,主要研究方向:图像处理、图像分析与识别。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金面上项目(61673021);</span>
                    </p>
            </div>
                    <h1><b>Deep multi-scale encoder-decoder convolutional network for blind deblurring</b></h1>
                    <h2>
                    <span>JIA Ruiming</span>
                    <span>QIU Zhenzhi</span>
                    <span>CUI Jiali</span>
                    <span>WANG Yiding</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Technology, North China University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the heterogeneous blur of images caused by inconsistent motion of objects in the shooting scene, a deep multi-scale encoder-decoder convolutional network was proposed to improve the deblurring effect in complex motion scenes. A multi-scale cascade structure named “from coarse to fine” was applied to this network, and blind deblurring was achieved with the blur kernel unknown. In the encoder-decoder module of the network, a fast multi-scale residual block was proposed, which used two branches with different receptive fields to enhance the adaptability of the network to multi-scale features. In addition, skip connections were added between the encoder and the decoder to enrich the information of the decoder. The Peak Signal-to-Noise Ratio(PSNR) value pf this network is 0.06 dB higher than that of the Scale-recurrent Network proposed on CVPR(Conference on Computer Vision and Pattern Recognition)2018; the PSNR and Mean Structural Similarity(MSSIM) values are increased by 1.4% and 3.2% respectively compared to those of the deep multi-scale convolution network proposed on CVPR2017. The experimental results show that the proposed network can deblur the image quickly and restore the edge structure and texture details of the image.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=blind%20deblurring&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">blind deblurring;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20structure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale structure;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=skip%20connection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">skip connection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=encoder-decoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">encoder-decoder;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network(CNN);</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JIA Ruiming, born in 1978, Ph. D. , research assistant. His research interests include computer vision, deep learning, pattern recognition. ;
                                </span>
                                <span>
                                    QIU Zhenzhi, born in 1994, M. S. candidate. Her research interests include computer vision, deep learning. ;
                                </span>
                                <span>
                                    CUI Jiali, born in 1975, Ph. D. , research assistant. His research interests include image processing, pattern recognition. ;
                                </span>
                                <span>
                                    WANG Yiding, born in 1967, Ph. D. , professor. His research interests include image processing, image analysis and recognition.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-07</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>partially supported by the National Natural Science Foundation of China(61673021);</span>
                    </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 引言</h3>
                <div class="p1">
                    <p id="42">图像去模糊是计算机视觉及图像处理中一个重要任务,在交通安全、医学图像、军事侦察等领域都有广泛应用。图像模糊中运动模糊是最见的一种,由相机晃动、多个目标物体的运动等造成,具有重要的现实研究意义。去模糊的目的是从退化的模糊图像中恢复出其对应的清晰图像。数学上模糊图像由清晰图像和模糊核卷积加上噪声形成,根据模糊核是否已知,去模糊可分成非盲去模糊和盲去模糊<citation id="179" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。多数情况下模糊核是无法提早获得的,所以盲去模糊应用更广泛也更具挑战性。盲去模糊是一个不适定的逆问题,为了解决这个问题,许多学者将模糊核和清晰图像的信息作为先验知识来提高复原图像的质量,其中包括正则化强度和梯度先验<citation id="180" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、数据驱动判别先验<citation id="181" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等。上述方法能够改善去模糊质量,但都需要复杂的模糊核估计步骤,模糊核估计不正确会使恢复的图像存在肉眼可见的伪影。</p>
                </div>
                <div class="p1">
                    <p id="43">近年来,人们将基于卷积神经网络的方法应用于图像去模糊任务,采用端到端的方式直接恢复清晰图像,避免了模糊核估计带来的相关问题,取得很好的去模糊效果。大多数卷积神经网络的方法主要消除由简单的平移或相机旋转引起的运动模糊,这些方法假设模糊是均匀的或在整张图片中是空间不变的。然而,由于场景深度的变化和目标物体的运动,真实图片通常为非均匀模糊<citation id="182" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。Sun等<citation id="183" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>用卷积神经网络去除非均匀运动模糊。但所用数据集中的模糊图像是由模糊核与清晰图像卷积合成,这与真实场景中的模糊有很大不同,当把它们应用在真实去模糊问题中,不能得到很好的恢复结果。为此,Nah等<citation id="184" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出一个大规模更接近真实的模糊-清晰图像数据集对,并用一种深度多尺度卷积网络,以端到端的方式直接去除动态场景模糊。Kupyn等<citation id="185" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>用条件生成对抗网络和内容损失函数去除运动产生的图像模糊。Tao等<citation id="186" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>用多尺度循环网络以“从粗到细”的方式逐渐恢复清晰图像,实现了更好的去模糊结果。然而,这些方法仍然存在速度慢、恢复图像纹理不清晰等问题。</p>
                </div>
                <div class="p1">
                    <p id="44">针对上述研究,本文提出一种多尺度编解码深度卷积网络,以端到端的方式直接去除动态场景中的运动模糊,实现快速、高效的图像复原。本文主要工作包括以下三个方面:</p>
                </div>
                <div class="p1">
                    <p id="45">1)提出快速多尺度残差块(Fast Multi-scale Residual Block, FMRB),作为网络的重要组成模块,使网络对不同尺度的模糊输入有更强的适应能力。同残差块<citation id="187" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Inception模块<citation id="188" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、多尺度残差块(Multi-scale Residual Block, MSRB)<citation id="189" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>等相比,FMRB去模糊效果更优。</p>
                </div>
                <div class="p1">
                    <p id="46">2)提出新的多尺度编解码深度卷积网络用于非均匀图像盲去模糊。网络采用“从粗到细”的多尺度结构逐渐恢复清晰图像,每个尺度均使用相同的参数设置。在不同尺度间进行参数共享,不仅能够降低参数量,还能防止过拟合。同时各尺度均使用编解码加跳跃连接的结构,能加速网络收敛,更好地恢复图像的纹理信息。</p>
                </div>
                <div class="p1">
                    <p id="47">3)将提出的网络模型在GOPRO数据集<citation id="190" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和Köhler数据集<citation id="191" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>上进行实验,并与最先进的去模糊算法比较,得到了更优的去模糊效果。用自行拍摄的模糊图像实验,恢复出可视化效果较好的清晰图像。同时,验证了本文方法除运动模糊外,对其他类型模糊的去模糊效果,并对大尺图像去模糊的运算速度和性能进行实验分析。</p>
                </div>
                <h3 id="48" name="48" class="anchor-tag">1 多尺度编解码深度卷积网络</h3>
                <h4 class="anchor-tag" id="49" name="49">1.1 <b>网络结构</b></h4>
                <div class="p1">
                    <p id="50">本文将提出的模型称为多尺度编解码深度卷积网络,如图1所示。网络整体为“从粗到细”的多尺度结构,即从粗糙的低分辨率模糊图像逐渐恢复精细的高分辨率清晰图像。各尺度使用相同的编解码器,如图1(b)所示。编解码模块由多个快速多尺度残差块(FMRB),卷积层和反卷积层堆叠而成。FMRB如图2所示,是一种局部多尺度结构。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 多尺度编解码深度卷积网络结构" src="Detail/GetImg?filename=images/JSJY201909013_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 多尺度编解码深度卷积网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Deep multi-scale encoder-decoder convolutional network structure</p>

                </div>
                <h4 class="anchor-tag" id="52" name="52">1.1.1 多尺度结构</h4>
                <div class="p1">
                    <p id="53">多尺度结构在传统最大后验概率优化的方法<citation id="192" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和近期深度学习的方法<citation id="193" type="reference"><link href="141" rel="bibliography" /><link href="153" rel="bibliography" /><link href="163" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">7</a>,<a class="sup">12</a>]</sup></citation>等都有广泛应用,并表现出突出性能。本文提出的多尺度结构如图1(a)所示,网络的输入为3个分辨率不同的模糊图像<i>X</i><sub><i>k</i></sub>(<i>k</i>=1,2,3),它们由原始模糊图像下采样形成。下标<i>k</i>代表尺度层级,图像的分辨率随着<i>k</i>的增大顺序增加,尺度间隔比率为0.5。首先将尺度最小最容易恢复的模糊图像<i>X</i><sub>1</sub>输入,恢复出其对应大小估计的清晰图像<i>Y</i><sub>1</sub>。然后将估计的中间清晰图像上采样到更大的尺度,与模糊图像<i>X</i><sub>2</sub>一同作为下一尺度的输入,进一步引导更大尺寸的图像恢复。同理,可以获得最后的高分辨率输出图像<i>Y</i><sub>3</sub>。这种直接学习由输入到输出端到端的映射方式,避免了模糊核估计带来的相关问题。此外,多尺度结构可以降低网络的训练难度,使恢复的图像更清晰。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 快速多尺度残差块示意图" src="Detail/GetImg?filename=images/JSJY201909013_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 快速多尺度残差块示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Schematic diagram of fast multi-scale residual block</p>

                </div>
                <div class="p1">
                    <p id="55">各尺度具有完全相同的网络结构和参数配置,这样在不同尺度间可以共享网络的权重,不仅可以减少训练的参数,而且能提升网络性能,防止过拟合。网络具体的参数配置如表1所示。</p>
                </div>
                <div class="area_img" id="56">
                                            <p class="img_tit">
                                                <b>表</b>1 <b>网络结构参数配置表</b>
                                                    <br />
                                                Tab. 1 Parameter configuration of model structure
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_05600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909013_05600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_05600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 网络结构参数配置表" src="Detail/GetImg?filename=images/JSJY201909013_05600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>
                                <p class="img_note">注:↑表示将特征图通过步长为2的反卷积层进行上采样;↓表示将特征图通过步长为2的卷积层进行下采样。</p>

                </div>
                <h4 class="anchor-tag" id="57" name="57">1.1.2 编解码结构</h4>
                <div class="p1">
                    <p id="58">编解码结构在计算机视觉任务<citation id="195" type="reference"><link href="153" rel="bibliography" /><link href="165" rel="bibliography" /><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>中的有效性已被证明。本文提出一种新的编解码结构,如图1(b)所示,由多个FMRB以及卷积层和反卷积层组成。步长为2的卷积层将特征图尺寸降为原来的一半,同时将通道数增加一倍;相反,步长为2的反卷积层则将特征通道数减半,特征图尺寸提升一倍。编码块的主要任务是进行特征提取,抽象图像的内容信息,并且消除模糊。解码块的作用是恢复图像的高频细节信息。图像去模糊任务中,需要足够大的感受野来恢复严重模糊的图像。目前加深、加宽网络已经成为一种增加网络感受野的设计趋势,然而简单地增加深度会使网络变得更难训练,出现梯度消失、梯度爆炸等现象<citation id="194" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。本文网络较深,为了防止上述问题的出现,在编码部分和对称的解码部分添加了跳跃连接,每隔1个FMRB添加1条跳跃连接。这种连接不仅可以将编码部分的图像信息传递到解码部分,帮助恢复原始的清晰图像。在反向传播中还有助于将梯度传到底层,加速网络收敛,提升去模糊性能。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.1.3 快速多尺度残差块</h4>
                <div class="p1">
                    <p id="60">快速多尺度残差块(FMRB)如图2所示。模块的输入传给两个分支,右边的分支首先经过一个3×3卷积层,左边的分支经过两个3×3的卷积层。两个3×3卷积层与一个5×5卷积层的感受野相同<citation id="196" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,但是计算量更小非线性变换更强。由于两个分支的感受野不同,所以能检测不同尺度的信息。最后拼接意味着不同尺度的特征融合,这种信息交互可以使后面的层共享两个分支之间的信息,使网络对多尺度输入有更强的适应性。</p>
                </div>
                <div class="p1">
                    <p id="61">图2中<i>S</i>为相邻卷积层卷积核的数量,<i>C</i><sub><i>n</i>-1</sub>和<i>C</i><sub><i>n</i></sub>分别为模块输入、输出的通道数,这里<i>C</i><sub><i>n</i>-1</sub>=<i>C</i><sub><i>n</i></sub>=<i>S</i>。尽管两个3×3卷积与一个5×5卷积的感受野相同,但FMRB计算量(由于其他部分计算量相同,为了简便,只考虑虚线框内的计算量)由170<i>MNS</i><sup>2</sup>减少为135<i>MNS</i><sup>2</sup>(这里通过像素填充使模块输入到输出的特征图分辨率保持<i>M</i>×<i>N</i>不变,为了方便,省略了偏置计算)。FMRB在深度加深的同时,时间复杂度更低,运算速度更快。</p>
                </div>
                <div class="p1">
                    <p id="62">由于深度网络能提取更丰富的特征,所以网络深度对其性能有至关重要的影响。但是深度网络很难训练,为了缓解这个问题,FMRB采用了残差学习,在模块的输入到输出之间添加1个恒等映射。这种局部残差学习能加速网络收敛,防止梯度消失,提升网络性能。FMRB中的1×1的卷积层,实现特征通道降维,使模块的输出和输入保持相同的维度,同时可以保留有用的图像信息,剔除冗余信息。</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63">1.2 <b>损失函数</b></h4>
                <div class="p1">
                    <p id="64">本文将均方误差(Mean Squared Error, MSE)作为该网络的损失函数。如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>Θ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Κ</mi><mi>Ν</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">∥</mo></mstyle></mrow></mstyle><mi>F</mi><mo stretchy="false">(</mo><mi>X</mi><msubsup><mrow></mrow><mi>k</mi><mi>i</mi></msubsup><mo>,</mo><mi>Θ</mi><mo stretchy="false">)</mo><mo>-</mo><mi>Y</mi><msubsup><mrow></mrow><mi>k</mi><mi>i</mi></msubsup><mo stretchy="false">∥</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">其中:<i>N</i>为训练样本对的个数,<i>K</i>为网络最大尺度层级,<i>Θ</i>为网络权重。通过训练实现该损失函数最小化,使网络在尺度层级<i>k</i>对第<i>i</i>张图像的去模糊结果<i>F</i>(<i>X</i><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>i</mi></msubsup></mrow></math></mathml>,<i>Θ</i>)与真实清晰图像<i>Y</i><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>i</mi></msubsup></mrow></math></mathml>的欧几里得距离最小。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">2 实验结果与分析</h3>
                <h4 class="anchor-tag" id="70" name="70">2.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="71">GOPRO数据集<citation id="197" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是2017年提出的大型去模糊数据集,与以往数据集中用模糊核与清晰图像卷积合成模糊图像不同,它是用高速摄像机捕捉连续短曝光的清晰帧,并进行整合平均来模拟长曝光的模糊帧。这样形成的图像更接近真实,能够模拟复杂的相机抖动和场景中多个目标运动带来的非均匀模糊。GOPRO数据集总共包含3 214对模糊-清晰图像,图片大小为720×1 280,其中2 103对图像用来训练,其余1 111对图像用来测试。</p>
                </div>
                <div class="p1">
                    <p id="72">Köhler数据集<citation id="198" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是一个评估和比较盲去模糊算法的基准数据集。作者通过记录和分析真实相机的运动,然后用机器人载体进行回放,通过在6D相机的运动轨迹上留下一连串清晰的图像,形成数据集。Köhler数据集由4张图片组成,对每张图片用12个不同的模糊核对进行模糊,最后形成48张模糊图像。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">2.2 <b>实验细节</b></h4>
                <div class="p1">
                    <p id="74">本文实验在CPU为i5-3470,内存16 GB,GPU为NVIDIA 1080Ti的计算机上进行。除FMRB中的卷积层,网络中其他卷积层均使用5×5大小的卷积核,反卷积核尺寸为4×4,在卷积层和反卷积层后面均使用ReLU激活函数。此外,使用像素填充保持特征图的输出和输入尺度不变。训练时将训练集中的模糊-清晰图像对随机裁剪成256×256大小的图像块,测试时保持图片原有大小不变。训练阶段多尺度层级的输入/输出分辨率为{64×64,128×128,256×256}的图像块,本文用双线性插值来采样图片。初始学习率设置为 1E-4,然后使用指数衰减法逐步减小学习率,衰减系数为0.3。用Adam优化器来优化损失函数,实验中批尺寸设为2,网络训练直至收敛。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75">2.3 <b>结果与分析</b></h4>
                <h4 class="anchor-tag" id="76" name="76">2.3.1 质量评估</h4>
                <div class="p1">
                    <p id="77">传统去模糊算法通常假设整幅图像的模糊是均匀和不变的。然而,运动模糊图像的模糊通常是动态变化的和非均匀的。为了公平比较,本文没有与传统的均匀去模糊算法对比,只与最先进的非均匀去模糊算法进行比较。表2为不同去模糊方法在GOPRO测试集上的质量评估结果。用峰值信噪比(Peak Signal-to-Noise Ratio, PSNR)和结构相似性(Structural SIMilarity index, SSIM)两项指标作为性能衡量的标准。图3是在GOPRO测试集上可视化对比的示例图。在Köhler数据集上,本文与多种方法的客观质量评估对比结果见表3,用PSNR和平均结构相似性(Mean Structural SIMilarity, MSSIM)作为评价算法性能的标准。</p>
                </div>
                <div class="area_img" id="78">
                                            <p class="img_tit">
                                                <b>表</b>2 <b>两个测试集上质量评估结果</b>
                                                    <br />
                                                Tab. 2 Quality evaluations on two testsets
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_07800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJY201909013_07800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_07800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 两个测试集上质量评估结果" src="Detail/GetImg?filename=images/JSJY201909013_07800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="79">从表2的GOPRO测试集部分可以看出,本文去模糊图像得到了最高的PSNR和SSIM值。表中前三种方法<citation id="204" type="reference"><link href="149" rel="bibliography" /><link href="171" rel="bibliography" /><link href="173" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>不能有效去除图像中的模糊。Nah等<citation id="199" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和Zhang等<citation id="200" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>的去模糊效果相较前面三种方法有了很大的提升,但还没达到最佳。Tao等<citation id="201" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation><sup></sup>在2018年CVPR上提出的多尺度循环网络(Scale-Recurrent Network, SRN-DeblurNet),被证明是有效的去模糊方法。本文结果较Tao等<citation id="202" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>的PSNR值提高了0.06 dB,SSIM也有所提高,说明本文算法复原图像质量要优于其他方法。从图3(b)左图可以看出,Tao等<citation id="203" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>去模糊结果的车牌出现多余信息,圆圈中的数字存在结构变化。对于图3(b)右图复原后的图像存在严重伪影,且没能恢复出车完整的轮廓。本文去模糊的可视化图像没有明显的伪影,不仅去除了图像的模糊,还保持了清晰的纹理和边缘细节,更接近真清晰图像。</p>
                </div>
                <div class="p1">
                    <p id="80">各算法在Köhler数据集上的测试结果见表2Köhler测试集部分,本文的PSNR和MSSIM均高于其他算法,与Nah等<citation id="205" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>的深度多尺度卷积网络相比,两项指标分别提高1.4%和3.2%。 综上可得,本文的去模糊效果要优于其他方法。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 GOPRO测试集去模糊结果可视化对比" src="Detail/GetImg?filename=images/JSJY201909013_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 GOPRO测试集去模糊结果可视化对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Visual comparisons on GOPRO testset</p>

                </div>
                <h4 class="anchor-tag" id="82" name="82">2.3.2 多尺度结构分析</h4>
                <div class="p1">
                    <p id="83">为了检验多尺度结构的有效性,本文分别对单一尺度、2个和3个尺度模型进行测试,即式(1)中的最大尺度层级<i>K</i>分别为1、2和3。均在GOPRO数据集上进行训练和测试。以PSNR、SSIM以及在测试集上的平均测试时间作为图像质量的客观评价标准。不同尺度性能比较如表3所示,在测试集上去模糊结果的局部可视化效果为图4。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表</b>3 <b>不同尺度性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 3 Performance comparison of different scales</p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td><br /><i>K</i></td><td>PSNR/dB</td><td>SSIM</td><td>测试时间/s</td></tr><tr><td><br />1</td><td>29.55</td><td>0.925 5</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mo>.</mo><mn>1</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />2</td><td>30.04</td><td>0.931 1</td><td>1.32</td></tr><tr><td><br />3</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>3</mn><mn>0</mn><mo>.</mo><mn>3</mn><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>9</mn><mn>3</mn><mn>5</mn><mtext> </mtext><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>1.37</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="85">在表3中,当<i>K</i>=2时,PSNR和SSIM值比<i>K</i>=1分别提高0.49 dB和0.005 6。从图4(左)也可以看出网络为1个尺度时,去模糊效果不好,恢复的字体扭曲并严重失真。2个尺度较单一尺度有较大的提升,能改善图像模糊,且没有出现扭曲变形现象。<i>K</i>=3与<i>K</i>=2比较,平均的PSNR和SSIM值分别提高0.28 dB和0.003 9,进一步提升了去模糊效果。从图4(右)能看出,尺度层级为2的去模糊图像地面纹理不清晰,而3个尺度的复原图像边缘更清晰,细节较丰富,与清晰图像最为接近。<i>K</i>为3性能更好,测试时间与<i>K</i>为2相近,所以本文的网络结构使用3个尺度。综上,多尺度结构对于图像去模糊任务是非常有效的。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同尺度可视化对比" src="Detail/GetImg?filename=images/JSJY201909013_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同尺度可视化对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Visual comparison of different scales</p>

                </div>
                <h4 class="anchor-tag" id="87" name="87">2.3.3 快速多尺度残差块分析</h4>
                <div class="p1">
                    <p id="88">本文提出一个新的模块——快速多尺度残差块(FMRB)。为了验证FMRB的有效性,基于本文的网络结构,分别用经典的残差块(Res-block)<citation id="206" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Inception V2模块<citation id="207" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、MSRB<citation id="208" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>以及FMRB-s(FMRB的简化版本)替换 FMRB,在相同的平台下实验,不同模块结构如图5所示。为了快速验证,我们使用单一尺度结构在GOPRO数据集上进行实验。测试结果见表4。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同模块可视化" src="Detail/GetImg?filename=images/JSJY201909013_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同模块可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Visualization of different modules</p>

                </div>
                <div class="area_img" id="90">
                    <p class="img_tit"><b>表</b>4 <b>单尺度下各模块性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 4 Performance comparison of different modules at single scale</p>
                    <p class="img_note"></p>
                    <table id="90" border="1"><tr><td><br />模块</td><td>PSNR/dB</td><td>SSIM</td><td>测试时间/s</td></tr><tr><td><br />Res-block</td><td>28.99</td><td>0.917 6</td><td>0.85</td></tr><tr><td><br />Inception V2</td><td>28.10</td><td>0.901 9</td><td>0.74</td></tr><tr><td><br />FMRB-s</td><td>28.79</td><td>0.913 8</td><td>0.83</td></tr><tr><td><br />MSRB</td><td>29.53</td><td>0.925 2</td><td>1.40</td></tr><tr><td><br />FMRB</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>2</mn><mn>9</mn><mo>.</mo><mn>5</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>9</mn><mn>2</mn><mn>5</mn><mtext> </mtext><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>1.17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="91">表4中结果表明,使用FMRB模块的PSNR和SSIM值最高,在所有对比模块中,本模块有一定优势。FMRB较MSRB模块计算量更少,在测试时间上,速度提升16%,实时性更好。综上,模块FMRB对于图像去模糊任务是非常有效的,它用更少的计算量实现了更优的性能。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">2.3.4 跳跃连接分析</h4>
                <div class="p1">
                    <p id="93">随着网络深度的增加,图像信息会丢失。本文的网络中,编码部分特征图的空间信息可以通过跳跃连接传送到解码部分,帮助解码器恢复更清晰的图像。本文分别对有跳跃连接和无跳跃连接的两种网络结构(网络其他结构保持不变)在GOPRO数据集上实验,为了简便,两种结构都使用单一尺度。图6给出两种模型的盲去模糊质量评估散点图。由图可以看出,在100～600个Epoch之间,有跳跃连接的曲线均落在无跳跃连接曲线的上方,即有跳跃连接模型的去模糊结果得到了更高的PSNR值。所以,对于深度编解码网络,跳跃连接具有较大的优势。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 单尺度下跳跃连接比较" src="Detail/GetImg?filename=images/JSJY201909013_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 单尺度下跳跃连接比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparison of skip connections at single scale</p>

                </div>
                <h4 class="anchor-tag" id="95" name="95">2.3.5 自行拍摄图像去模糊</h4>
                <div class="p1">
                    <p id="96">GOPRO数据集的模糊图像是合成的,与真实模糊图像有很大不同。许多方法将训练好的模型运用到实际情况中,不能得到很好的去模糊结果。为了验证本文模型的实用性,将手机自行拍摄的模糊图片在训练好的模型上进行去模糊实验,结果如图7所示。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 自行拍摄图像去模糊" src="Detail/GetImg?filename=images/JSJY201909013_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 自行拍摄图像去模糊  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Real-world blurred images</p>

                </div>
                <div class="p1">
                    <p id="98">从图7的两组图像可以看出,多尺度编解码深度卷积网络能够去除实际拍摄图片的运动模糊,去模糊图像得到了较高质量的视觉效果。本文所提网络的泛化能力较强,能广泛应用于日常生活中。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">2.3.6 其他模糊图像复原</h4>
                <div class="p1">
                    <p id="100">模糊图像类型不同,但其形成过程都可以通过清晰图像与模糊核的卷积来描述,去模糊过程具有相似性,本质上都是一个求反卷积的过程。理论上,本文用于处理运动模糊的方法也能用来去除其他模糊。</p>
                </div>
                <div class="p1">
                    <p id="101">物体运动、镜头聚焦不准、光学系统的衍射等都会造成图像模糊,但最常见的模糊分为运动模糊和离焦模糊两大类型。本文所用GOPRO数据集中图像的模糊是由相机抖动和拍摄场景中多个物体的快速运动共同产生的,包含全局模糊和局部模糊,所以本文不只是处理简单的匀速直线运动模糊,本文的模糊核是空间变换和非均匀的,覆盖范围更广;离焦模糊是由聚焦不准、景深、成像设备质量等形成,目前有高斯模型和圆盘模型来近似离焦模糊的模糊核<citation id="209" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,高斯模型具有规律性,圆盘离焦模型为弥散状且均匀分布的圆盘形,与运动模糊的模糊核相比更为简单。为了验证本文方法的对于去除其他模糊的可行性,对离焦模糊的图像进行测试,实验结果见图8。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJY201909013_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 离焦模糊图像复原" src="Detail/GetImg?filename=images/JSJY201909013_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 离焦模糊图像复原  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJY201909013_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Defocus blurred image restoration</p>

                </div>
                <div class="p1">
                    <p id="103">从图8可以看出去模糊图像中铁架和台阶的纹理比模糊图像更为清晰,本文方法能够改善离焦模糊图像的质量。验证了上述理论的正确性,所提方法具有普适性,对于其他模糊图像的复原也具有较好的效果。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">2.3.7 大尺度图像去模糊</h4>
                <div class="p1">
                    <p id="105">本文的多尺度编解码深度卷积网络在图像去模糊性能和实时性方面都具有一定的优势,但是随着图片尺度的增加,去模糊的速度逐渐减慢。为提升大尺度图像在所提方法中的计算速度,作了以下处理。取1 000对GOPRO测试集中的图像拼接成250对空间分辨率为1 440×2 560的图像,称为GOPRO-L数据集。分别用直接输入法和三种不同处理方法在此数据集上进行测试:第一种是下采样法,即用双三次插值将模糊图像下采样成低分辨率图像作为网络的输入,然后将去模糊结果用同样的方法上采样到原图大小;第二种方法是分块法,将模糊图像分成多个小块分别去模糊,然后将测试后的图像拼成输入对应的图像;第三种方法是将前两种方法结合使用,将分块后的图像下采样进行去模糊。表5为不同方法对大尺度图像的去模糊结果,括号中的值分别为下采样后图像与原输入图像的尺寸比例和分块个数,用PSNR和平均测试时间作为评价标准。</p>
                </div>
                <div class="area_img" id="106">
                    <p class="img_tit"><b>表</b>5 <b>大尺度图像去模糊结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Tab. 5 Large-scale image deblurring results</p>
                    <p class="img_note"></p>
                    <table id="106" border="1"><tr><td><br />方法</td><td>PSNR/dB</td><td>测试时间/s</td></tr><tr><td><br />直接输入</td><td>30.15</td><td>5.02</td></tr><tr><td><br />下采样(比例0.8)</td><td>29.82</td><td>3.19</td></tr><tr><td><br />下采样(比例0.6)</td><td>28.94</td><td>1.85</td></tr><tr><td><br />下采样(比例0.4)</td><td>27.97</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>8</mn><mn>3</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />分块(2×2)</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>3</mn><mn>0</mn><mo>.</mo><mn>4</mn><mn>1</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>5.25</td></tr><tr><td><br />分块(4×4)</td><td>30.24</td><td>5.58</td></tr><tr><td><br />分块(8×8)</td><td>29.89</td><td>6.86</td></tr><tr><td><br />分块(2×2)+下采样(0.4)</td><td>28.24</td><td>0.90</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="107">由表5可知,在下采样法中,随着输入图像尺寸的减小,测试时间大幅降低,但是PSNR值逐渐减小。下采样图像尺寸为原图0.4倍时的测试效率比直接输入法提升83.47%,但PSNR值降低7.23%。随着分块数量的增加,PSNR值逐渐降低,测试时间也在增加。当分块数为2×2个小块时,测试时间比直接输入法增加4.58%,但PSNR值提升了0.86%。下采样法对于提高大尺度图像的计算速度是非常有效的,在去模糊性能的要求不是太高时此方法非常有效。分块数量较少的分块法能提升去模糊的性能。将两种方法结合使用, 如表5中的最后一种方法,较单一分块法(2×2块)测试效率提升82.86%,较单一下采样法(0.4倍)的PSNR值提升0.97%,在提升性能的同时也保证了运算速度。</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">3 结语</h3>
                <div class="p1">
                    <p id="109">为去除非均匀运动模糊,并解决早期去模糊算法计算复杂、运算速度慢、恢复图像存在重影等问题,本文提出一种多尺度编解码深度卷积网络,以端到端的方式快速实现模糊图像的盲去模糊。实验结果表明本文方法优于目前先进的去模糊方法,恢复图像纹理更清晰;同时,多尺度结构能减少图像重影,保持图像的边缘结构;编解码加跳跃连接的结构对于提升去模糊性能是有效的;提出的快速多尺度残差块用更少的计算量实现了更优的性能。实验还表明本文方法对去除真实拍摄图像的模糊和其他类型的模糊也均是有效的。对于大尺度图像去模糊,在保持运算速度的同时,如何进一步提升图像复原的质量,并将提出的网络用于其他图像复原任务是接下来的研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="141">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep multi-scale convolutional neural network for dynamic scene deblurring">

                                <b>[1]</b> NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017,1:257-265.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deblurring text images via L0-regularized intensity and gradient prior">

                                <b>[2]</b> PAN J,HU Z,SU Z,et al.Deblurring text images via l0-regularized intensity and gradient prior [C]// CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2014:2901-2908.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Prior for Blind Image Deblurring">

                                <b>[3]</b> LI L,PAN J,LAI W-S,et al.Learning a discriminative prior for blind image deblurring [C]// CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:6616-6625.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gated fusion network for joint image deblurring and super-resolution">

                                <b>[4]</b> ZHANG X,DONG H,HU Z,et al.Gated fusion network for joint image deblurring and super-resolution [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1807.10806.pdf.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a convolutional neural network for non-uniform motion blur removal">

                                <b>[5]</b> SUN J,CAO W,XU Z,et al.Learning a convolutional neural network for non-uniform motion blur removal [C]// CVPR 2015:Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:769-777.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeblurGAN:blind motion deblurring using conditional adversarial networks">

                                <b>[6]</b> KUPYN O,BUDZAN V,MYKHAILYCH M,et al.DeblurGAN:blind motion deblurring using conditional adversarial networks [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1711.07064.pdf.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scale-recurrent network for deep image deblurring">

                                <b>[7]</b> TAO X,GAO H,WANG Y,et al.Scale-recurrent network for deep image deblurring [EB/OL].[2019- 01- 05].https://arxiv.org/pdf/1802.01770.pdf.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with DeepConvolutional Neural Networks">

                                <b>[8]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G,et al.ImageNet classification with deep convolution neural network [C]// NIPS '12:Proceedings of the 25th International Conference on Neural Information Processing Systems.North Miami Beach,FL,USA:Curran Associates,2012,1:1097-1105.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[9]</b> LOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift [C]// ICML '15:Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:JMLR.org,2015:448-456.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale residual network for image super-resolution">

                                <b>[10]</b> LI J,FANG F,MEI K,et al.Multi-scale residual network for image super-resolution [C]// Proceedings of the 2018 European Conference on Computer Vision,LNCS 11212.Berlin:Springer,2018:527-542.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recording and playback of camera shake:benchmarking blind deconvolution with a real-world database">

                                <b>[11]</b> KÖHLER R,HIRSCH M,MOHLER B,et al.Recording and playback of camera shake:benchmarking blind deconvolution with a real-world database [EB/OL].[2019- 01- 05].http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=4F605BF966AB6B236B6591E377AC8243 doi=10.1.1.379.1398&amp;rep=rep1&amp;type=pdf.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution">

                                <b>[12]</b> LAI W,HUANG J,AHUJA N,et al.Deep Laplacian pyramid networks for fast and accurate super-resolution [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:5835-5843.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image restoration using convolutional auto-encoders with symmetric skip connections">

                                <b>[13]</b> MAO X-J,SHEN C,YANG Y-B,et al.Image restoration using convolutional auto-encoders with symmetric skip connections [EB/OL].[2019- 01- 07].https://arxiv.org/pdf/1606.08921.pdf.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep video deblurring for hand-held cameras">

                                <b>[14]</b> SU S,DELBRACIO M,WANG J,et al.Deep video deblurring for hand-held cameras [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:237-246.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-net:Convolutional networks for biomedical image segmentation">

                                <b>[15]</b> RONNEBERGER O,FISCHER P,BROX T.U-net:convolutional networks for biomedical image segmentation [C]// Proceedings of the 2015 International Conference on Medical Image Computing and Computer-Assisted Intervention,LNCS 9351.Berlin :Springer,2015:234-241.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segmentationfree Dynamic Scene Deblurring">

                                <b>[16]</b> KIM T H,LEE K M.Segmentation-free dynamic scene deblurring [C]// CVPR 2014:Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2014,1:2766-2773.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=From motion blur to motion flow:a deep learning solution for removing heterogeneous motion blur">

                                <b>[17]</b> GONG D,YANG J,LIU L,et al.From motion blur to motion flow:a deep learning solution for removing heterogeneous motion blur [C]// CVPR 2017:Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition.Washington,DC:IEEE Computer Society,2017,1:3806-3815.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic scene deblurring using spatially variant recurrent neural networks">

                                <b>[18]</b> ZHANG J,PAN J,REN J,et al.Dynamic scene deblurring using spatially variant recurrent neural networks [C]// CVPR 2018:Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:2521-2529.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HKGX201805009&amp;v=MTM1NjNNZHJHNEg5bk1xbzlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRnlqbFdyeklMU2I=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 于春和,祁奇.离焦模糊图像复原技术综述[J].沈阳航空航天大学学报,2018,35(5):57-63.(YU C H,QI Q.A survey of defocusing image restoration techniques [J].Journal of Shenyang Aerospace University,2018,35(5):57-63.)
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJY201909013" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201909013&amp;v=MDQwNzFzRnlqbFdyeklMejdCZDdHNEg5ak1wbzlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c0dERktQNlNvSG5VV2tqWXgrbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
